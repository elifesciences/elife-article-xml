<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108915</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108915</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108915.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>The time course of visuo-semantic representations in the human brain is captured by combining vision and language models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Rong</surname>
<given-names>Boyan</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A3">3</xref>
<email xlink:href="mailto:boyanr.nj@gmail.com">boyanr.nj@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gifford</surname>
<given-names>Alessandro Thomas</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Düzel</surname>
<given-names>Emrah</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
<xref ref-type="aff" rid="A3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cichy</surname>
<given-names>Radoslaw Martin</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046ak2485</institution-id><institution>Department of Education and Psychology, Freie Universität Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Institute of Cognitive Neurology and Dementia Research, Otto-von-Guericke-Universität Magdeburg</institution></institution-wrap>, <city>Magdeburg</city>, <country country="DE">Germany</country></aff>
<aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>German Center for Neurodegenerative Diseases (DZNE), Otto-von-Guericke-Universität Magdeburg</institution></institution-wrap>, <city>Magdeburg</city>, <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Zhang</surname>
<given-names>Xilin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="FN1" fn-type="coi-statement"><p><bold>Competing interests</bold> The authors declare no competing interests.</p></fn>
</author-notes>
<pub-date pub-type="epub">
<day>24</day>
<month>06</month>
<year>2025</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2025-11-05">
<day>05</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108915</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2025-09-01">
<day>01</day>
<month>09</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-06-24">
<day>24</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2506.19497"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Rong et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Rong et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108915-v1.pdf"/>
<abstract>
<p>The human visual system provides us with a rich and meaningful percept of the world, transforming retinal signals into visuo-semantic representations. For a model of these representations, here we leveraged a combination of two currently dominating approaches: vision deep neural networks (DNNs) and large language models (LLMs). Using large-scale human electroencephalography (EEG) data recorded during object image viewing, we built encoding models to predict EEG responses using representations from a vision DNN, an LLM, and their fusion. We show that the fusion encoding model outperforms encoding models based on either the vision DNN or the LLM alone, as well as previous modelling approaches, in predicting neural responses to visual stimulation. The vision DNN and the LLM complemented each other in explaining stimulus-related signal in the EEG responses. The vision DNN uniquely captured earlier and broadband EEG signals, whereas the LLM uniquely captured later and low frequency signals, as well as detailed visuo-semantic stimulus information. Together, this provides a more accurate model of the time course of visuo-semantic processing in the human brain.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>1. Introduction</title>
<p>Visual perception provides us with a rich and detailed understanding of the world around us. This fundamental cognitive process is mediated by the rapid transformation of neural representations across the visual processing hierarchy from simple visual features such as edges into complex visuo-semantic formats carrying meaning <sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c8">8</xref></sup>. Providing a model predictive of these representations is a key step towards understanding vision. Over the last decade, deep neural networks (DNNs) trained on vision tasks such as object categorization have emerged as powerful models to predict visual brain activity <sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c12">12</xref></sup>. However, while the representations of these vision DNNs best predict early and intermediate visual processing stages in the brain <sup><xref ref-type="bibr" rid="c13">13</xref>–<xref ref-type="bibr" rid="c19">19</xref></sup>, later stages are less well captured, thus necessitating models with a different type of representational format.</p>
<p>In response to this shortcoming, recent progress came from computational models of human language that capture visuo-semantic information <sup><xref ref-type="bibr" rid="c20">20</xref>–<xref ref-type="bibr" rid="c23">23</xref></sup>. Both vision DNNs trained with language co-supervision (called multimodal DNNs <sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>), as well as large language models (LLMs) <sup><xref ref-type="bibr" rid="c26">26</xref>–<xref ref-type="bibr" rid="c28">28</xref></sup>, surpass the predictive power of vision DNNs in accounting for brain responses to visual stimulation, particularly for higher visual cortical areas <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c29">29</xref>–<xref ref-type="bibr" rid="c31">31</xref></sup>. However, these studies focused on functional magnetic resonance imaging (fMRI) data that lacks the millisecond precision at which visual processing occurs. Thus, they were agnostic to the dynamics of visual processing that are uniquely explained by either vision DNNs or LLMs.</p>
<p>Based on these recent developments, here we hypothesized that combining a vision DNN with a LLM would yield better neural predictions than either model component alone, and that the two components would uniquely predict earlier and later stages of visual processing, respectively.</p>
<p>To test these hypotheses, we predicted temporally resolved human electroencephalography (EEG) responses for thousands of naturalistic images of objects 32 using encoding models 9,33,34 that fused the representations of a vision DNN and an LLM. Confirming our first hypothesis, we found that the fusion encoding model outperformed encoding models trained on the vision DNN or LLM in isolation in predicting EEG responses to visual stimulation. Confirming our second hypothesis, we found that the vision DNN component of the fusion encoding model uniquely captured earlier and broadband EEG signals, whereas the LLM component uniquely captured later and low frequency signals, as well as detailed visuo-semantic stimulus information.</p>
<p>Together, our results provide a modelling approach accounting for the time course of visuo-semantic processing in the human brain.</p>
</sec>
<sec id="s2" sec-type="results">
<title>2. Results</title>
<p>Our core hypotheses were that human neural responses to visual stimulation are better modelled by combining representations from a vision DNN and an LLM than by the representations from either of the two components alone, and that the vision DNN and LLM component would uniquely predict earlier and later stages of visual processing, respectively. The rationale is that vision DNNs and LLMs take complementary roles in predicting neural responses. Vision DNNs capture visual characteristics of visual stimuli, such as color and object shape, and are thus good predictors of earlier stages of neural processing <sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c37">37</xref></sup> Instead, LLMs capture semantic characteristics of visual stimuli such as conceptual and functional meaning, and are thus good predictors of later stages of neural processing <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup>.</p>
<p>To test these hypotheses, we combined the vision DNN and the LLM into a fusion encoding model <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup> that predicted neural responses from a large-scale dataset of human EEG recordings to 16,740 naturalistic stimulus images <sup><xref ref-type="bibr" rid="c32">32</xref></sup>. The vision DNN consisted in CORnet-S <sup><xref ref-type="bibr" rid="c40">40</xref></sup>, a recurrent convolutional DNN trained on object categorization; the LLM consisted in OpenAI’s text-embedding-3-large, an LLM trained on vast text corpora.</p>
<p>As a precondition for the fusion encoding model to better predict neural responses than encoding models based on the vision DNN or the LLM components alone, these two components should differently represent information about the same stimulus images. To ascertain this, we extracted stimulus-wise representations by feeding the stimulus images to the vision DNN, and the stimulus image text description (generated by GPT-4V; number of words per image: mean = 26.22, SD = 6.92) to the LLM (<xref ref-type="fig" rid="fig1">Fig. 1A</xref>). Next, taking 1,000,000 random draws of stimulus pairs from the full stimulus set, we computed the pairwise cosine similarity of the corresponding vision DNN or LLM representations (<xref ref-type="fig" rid="fig1">Fig. 1B</xref>) <sup><xref ref-type="bibr" rid="c29">29</xref></sup>. The stimulus pairwise cosine similarities of the vision DNN and LLM representations were moderately correlated (Pearson’s <italic>r</italic> = 0.24). This partial representational overlap is expected, as visually similar objects tend to be semantically similar, too <sup><xref ref-type="bibr" rid="c41">41</xref>–<xref ref-type="bibr" rid="c43">43</xref></sup>. However, the moderate correlation indicates that most of the representational content differed between vision DNN and the LLM. To illustrate this, we consider outliers in the correlation plot (i.e., the upper left and lower right quadrant), identifying cases of image pairs that were similarly represented in only the visual DNN or LLM, indicating differences in representational content between the two. Image pairs with low similarity in the vision DNN but high similarity in the LLM (upper left quadrant, e.g., twig and wooden peg, needle and knitting), were visually dissimilar, but conceptually similar. In contrast, image pairs with high similarity in the vision DNN but low similarity in the LLM (lower right quadrant, e.g., streetlight and scalpel, granite and mud) were visually similar, but conceptually dissimilar. Together this shows that the vision DNN and the LLM differ in their representational content, warranting an investigation of their combined representations to predict neural responses to visual stimuli.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Combining a vision DNN with an LLM improves the prediction of neural responses to visual stimulation.</title>
<p><bold>A,</bold> Extraction of vision DNN and LLM representations for each of the 16,740 stimulus images. To obtain the vision DNN representations, we fed each image to the vision DNN and extracted its activations. To obtain the LLM representations, for each image we generated five text descriptions using GPT-4V, independently fed these descriptions to the LLM, and averaged the resulting five embedding instances. <bold>B,</bold> Scatterplot of pairwise cosine similarities between pairs of stimulus representations of either the vision DNN or the LLM. The gray line indicates the linear fit between the vision DNN and LLM pairwise similarities. Inset images are illustrative examples of image pair outliers. <bold>C,</bold> Encoding models training pipeline. We trained encoding models to predict empirically-recorded EEG responses based on representations from vision DNNs, LLM, and their combination. This resulted in three types of encoding models: vision, language, and fusion. <bold>D,</bold> Encoding models testing pipeline. We used the trained encoding models to predict EEG responses to the test stimulus images, and compared (Pearson’s <italic>r</italic>) these predictions to the corresponding empirically-recorded EEG responses, resulting in prediction accuracy time courses. <bold>E,</bold> Prediction accuracy (Pearson’s <italic>r</italic>) timecourse for the vision, language, and fusion encoding models. The prediction accuracies are averaged across all participants and EEG channels. In gray is the area between the noise ceiling lower and upper bounds. <bold>F,</bold> Difference in prediction accuracy between the fusion and the vision or language encoding model. <bold>E-F,</bold> The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. Rows of asterisks at the bottom of the plots indicate significant time points (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR corrected across 180 time points, <italic>N</italic> = 10 participants). <bold>G,</bold> Partial correlations between the recorded EEG test responses and the predicted EEG test responses from the fusion encoding model, controlling for the variance explained by the predicted EEG test responses from either the language encoding model (thus isolating the unique contribution of the vision DNN), or from the vision encoding model (thus isolating the unique contribution of the LLM). <bold>H,</bold> EEG topography of partial correlation results, indicating the unique prediction accuracy contribution of the vision DNN. <bold>I,</bold> EEG topography of partial correlation results, indicating the unique prediction accuracy contribution of the LLM. <bold>H-I,</bold> The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05 FDR-corrected across 63 channels and 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<sec id="s2-1">
<title>2.1. Combining a vision DNN with an LLM improves the prediction of neural responses to visual stimuli</title>
<p>To predict human neural responses to visual stimuli from model representations, we trained encoding models using time-resolved EEG responses to over 16,000 images recorded in 10 human participants <sup><xref ref-type="bibr" rid="c32">32</xref></sup> (<xref ref-type="fig" rid="fig1">Fig. 1C</xref>). We build three types of encoding models that differed in the format of the stimulus representations used to predict the EEG responses: i) the vision DNN representations (resulting in the vision encoding model, henceforth color-coded in blue); ii) the LLM representations (resulting in the language encoding model, in orange); and iii) the combination of the vision DNN and LLM representations (resulting in the fusion encoding model, in green). We then evaluated the performance of each encoding model type using a set of 200 test images not used for model training, resulting in time courses of prediction accuracy (<xref ref-type="fig" rid="fig1">Fig. 1D</xref>).</p>
<p>Here and throughout the manuscript, for the participant-averaged time courses of prediction accuracy we assessed significance using <italic>t</italic>-tests (one sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected for 180 time points, <italic>N</italic> = 10 participants). We reported peak latencies with 95% confidence intervals in parenthesis (determined by bootstrapping the participant samples through 10,000 iterations).</p>
<p>All three encoding model types significantly predicted EEG responses to visual stimuli, albeit with different dynamics (<xref ref-type="fig" rid="fig1">Fig. 1E</xref>). The vision encoding model’s prediction accuracy (blue curve) peaked at 110 ms (105 – 115 ms) while the language encoding model’s accuracy (orange curve) peaked at 365 ms (185 – 370 ms), with a significant peak latency difference of 255 ms (75 – 265 ms, <italic>p</italic> &lt; 10<sup>−3</sup>). This suggests that the vision and language encoding models predict earlier and later stages of visual processing, respectively. At their respective peaks, the encoding models cross the lower bounds of the noise ceilings (gray area) indicating that, at these time points, they predict all the explainable signal in the EEG responses.</p>
<p>The fusion encoding model (green curve) outperformed both vision and language encoding models. To ascertain this, we subtracted their prediction accuracies (<xref ref-type="fig" rid="fig1">Fig. 1F</xref>). The difference in prediction accuracy between the fusion and the language encoding models (purple curve), which isolates the effect of the vision DNN, peaked at 90 ms (90 – 105 ms). In contrast, the difference in prediction accuracy between the fusion and the vision encoding models (red curve), which isolates the effect of the LLM, peaked at 365 ms (360 – 400 ms), with a significant peak latency difference of 275 ms (265 – 310 ms, <italic>p</italic> &lt; 10<sup>−4</sup>). This confirms our hypotheses that the fusion encoding model outperforms both the vision and the language encoding models, and that the vision and language encoding models predict earlier and later stages of visual processing, respectively.</p>
<p>We established the robustness of these findings through several complementary analyses. First, the result pattern replicated in every single participant (<xref ref-type="fig" rid="fig5">Suppl. Fig. 1</xref>). Second, we obtained similar results when assessing the encoding models’ prediction accuracies with pairwise decoding (<xref ref-type="fig" rid="fig6">Suppl. Fig. 2</xref>). Third, we demonstrated the independent contribution of vision and language encoding models in predicting EEG responses also using partial correlation (<xref ref-type="fig" rid="fig7">Suppl. Fig. 3</xref>) and variance partitioning (<xref ref-type="fig" rid="fig8">Suppl. Fig. 4</xref>) (see <xref ref-type="table" rid="tbl1">Suppl. Table 1</xref> for a detailed comparison of onset and peak latency between models).</p>
<p>To estimate the cortical sources uniquely predicted by the vision DNN or the LLM we partialled out the contribution of the language or vision encoding models on the fusion encoding model, respectively, and inspected the resulting partial correlation topographies in EEG sensor space (<xref ref-type="fig" rid="fig1">Fig. 1G</xref>). The unique contribution of the vision DNN consisted in early prediction accuracy peak at 90 ms (85 – 90 ms) for medial occipito-parietal electrodes (i.e., Oz, POz, O1) (<xref ref-type="fig" rid="fig1">Fig. 1H</xref>), whereas the unique contribution of the LLM consisted in later prediction accuracies that extended more anteriorly, with bilateral peaks in bilateral occipito-temporal electrodes (i.e., TP9, TP10, P9, P10) (<xref ref-type="fig" rid="fig1">Fig. 1I</xref>). A closer inspection of the topography of the unique contribution of the language encoding model (<xref ref-type="fig" rid="fig1">Fig. 1I</xref>) revealed a finer picture, suggesting two distinct processing stages: an early stage with a peak at 200 ms (195 – 260 ms) limited to bilateral temporo-occipital electrodes, and a second stage with a second peak at 365 ms (360 – 400 ms) additionally involving parieto-frontal electrodes. Both peaks occurred later than the peak of the vision encoding model, with a peak difference of 110 ms (105 – 175 ms, <italic>p</italic> &lt; 10<sup>−3</sup>) and 275 ms (275 – 310 ms, <italic>p</italic> &lt; 10<sup>−4</sup>), respectively. This suggests two cortically distinct stages of processing captured by the language encoding model. We observed an equivalent topological pattern when using subtraction (<xref ref-type="fig" rid="fig9">Suppl. Fig. 5</xref>) or variance partitioning (<xref ref-type="fig" rid="fig10">Suppl. Fig. 6</xref>), further demonstrating the robustness of these findings.</p>
<p>Together, these results confirm our core hypotheses that combining a vision DNN with an LLM improves the prediction of neural responses to visual stimuli compared to using each component in isolation, and that the vision DNN or LLM components uniquely capture earlier versus later stages of EEG responses to visual stimuli, respectively.</p>
</sec>
<sec id="s2-2">
<title>2.2. The improvement in neural prediction of the fusion encoding model is due to differences between the vision DNN and LLM representations</title>
<p>Why are the LLM representations of the fusion encoding model leading to an increase in prediction accuracy compared to the vision encoding model based solely on visual DNN representations?</p>
<p>Above we showed that the vision DNN and the LLM differently represent stimulus information (<xref ref-type="fig" rid="fig1">Fig. 1B</xref>), suggesting that the fusion model may leverage these differences for improved neural predictivity. If so, the stimulus images that most benefit from the fusion encoding model in terms of EEG prediction accuracy, should also be most differently represented between the vision DNN and the LLM. To test this, we calculated the reduction in the mean squared error (MSE) -computed between the predicted and recorded EEG responses for each of the 200 images of the test set- for the fusion encoding model compared to the vision encoding model (i.e., Δ<italic>MSE</italic> = <italic>MSE<sub>vision</sub></italic> − <italic>MSE<sub>fusion</sub></italic>). We then determined the pairwise cosine similarities of either the vision DNN or LLM representations for the group of 25 stimulus images that most benefited from the fusion model (highest Δ<italic>MSE</italic> values), and for the group of 25 stimulus images that least benefited from the fusion model (lowest Δ<italic>MSE</italic> values). Finally, we correlated the pairwise cosine similarities of the vision DNN and the LLM, independently for the stimulus pairs from each of the two stimulus groups. As expected, we observed a lower correlation for the group of stimulus images that most benefited from the fusion model (red dots, <italic>r</italic> = 0.17), than for the group of stimuli benefiting the least (blue dots, <italic>r</italic> = 0.60), with a significant difference (Δ<italic>r</italic> = 0.43, <italic>p</italic> &lt; 10<sup>−10</sup>) (<xref ref-type="fig" rid="fig2">Fig. 2A</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Factors determining the prediction performance of the fusion encoding model.</title>
<p><bold>A,</bold> Scatterplot of pairwise cosine similarity between pairs of stimulus representations of either the vision DNN or the LLM. The stimuli were the 25 stimulus images that most benefited from the fusion compared to the vision encoding model in terms of EEG prediction accuracy (color coded in red), and the 25 stimulus images that least benefitted (color coded in blue). The red and blue lines indicate the linear fit between the vision DNN and LLM pairwise similarities for the 25 most benefitting and 25 least benefitting images, respectively. <bold>B,</bold> Visualization of the 25 stimulus images that most or least benefited from the fusion compared to the vision encoding model in terms of EEG prediction accuracy. <bold>C,</bold> Prediction accuracy (Pearson’s <italic>r</italic>) timecourse for the vision encoding model, and the fusion encoding models trained on full descriptions, object category labels, and descriptions randomly assigned to stimulus images. <bold>D,</bold> Prediction accuracy improvement over the vision encoding model for fusion encoding models trained on full descriptions, object category labels, and image descriptions randomly assigned to stimulus images. <bold>E,</bold> Prediction accuracy time course for the vision encoding model, and the fusion encoding models trained on full descriptions and on parts of speech (nouns, adjectives, and verbs from the full descriptions). <bold>F,</bold> Prediction accuracy improvement over the vision encoding model for fusion encoding models trained on full descriptions, nouns, adjectives, and verbs. <bold>C-F,</bold> The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. Rows of asterisks at the bottom of the plots indicate significant time points (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR corrected across 180 time points, <italic>N</italic> = 10 participants). In gray is the area between the noise ceiling lower and upper bounds. <bold>G,</bold> Comparison of stimulus image object category labels (both human-annotated and DNN-generated) and nouns from the full image descriptions (excluding the nouns that overlap with the object category labels), for two illustrative examples.</p></caption>
<graphic xlink:href="2506.19497v1_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Together, this indicates that the improved neural prediction performance of the fusion compared to the vision encoding model is indeed due to differences between the vision DNN and LLM representations.</p>
<p>Upon visual inspection, we did not observe qualitative differences between the images that most and least benefited from the fusion model (<xref ref-type="fig" rid="fig2">Fig. 2B</xref>), suggesting that the improvement in prediction accuracy is due to the language-aligned representations capturing various types of stimulus information rather than a single, qualitatively discernible, factor.</p>
</sec>
<sec id="s2-3">
<title>2.3. The LLM component of the fusion encoding model captures detailed visuo-semantic stimulus information</title>
<p>What type of information is captured by LLM representations so as to improve the performance of the fusion over the vision encoding model? To investigate this, we built fusion encoding models using LLM representations for different types of textual input, and compared their performance to the vision encoding model and to the fusion encoding model using LLM representations for the full image descriptions.</p>
<p>We first tested the relevance of object category information – a key information encoded in the brain <sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>. In detail, we built fusion encoding models using LLM representations for the stimulus image object category annotations, generated in two ways: by humans through crowdsourcing <sup><xref ref-type="bibr" rid="c46">46</xref></sup>, and for the top-5 category labels generated by DNN models trained on object categorization (<xref ref-type="fig" rid="fig2">Fig. 2C-D</xref>; <xref ref-type="fig" rid="fig11">Suppl. Fig. 7A</xref>). As expected, object category information improved EEG prediction over the vision encoding model (<xref ref-type="fig" rid="fig2">Fig. 2D</xref>, pink and purple curves), but less so than the fusion encoding model based on the full image descriptions (green curve). Thus, while object category information is an important predictive factor <sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>, full descriptions contain additional visuo-semantic information predictive of neural responses.</p>
<p>Next, we dissected this visuo-semantic information by assessing the role of parts of speech. We reasoned that nouns reflect object-related information, adjectives reflect properties of these objects, and verbs reflect action-related information. We thus built several fusion encoding models using LLM representations for different parts of speech-nouns, adjectives, and verbs-derived from the full image descriptions (<xref ref-type="fig" rid="fig2">Fig. 2E-F</xref>; <xref ref-type="fig" rid="fig11">Suppl. Fig. 7A</xref>). We found that nouns (beige curve) and adjectives (brown curve) improved EEG prediction over the vision encoding model-although this improvement was lower than the one obtained with the full image descriptions (green curve)-and no improvement for verbs (yellow curve) (see <xref ref-type="table" rid="tbl2">Suppl. Table 2</xref> for a detailed comparison of onset and peak latency between models). This shows that the nouns and adjectives in the full image descriptions are driving the prediction accuracy improvement of the fusion over the vision encoding model, suggesting that neural responses for visual stimulation encode detailed information about objects and their properties.</p>
<p>The fusion encoding models trained on full image description nouns led to higher prediction accuracies compared to fusion encoding models trained on object category information (<xref ref-type="fig" rid="fig11">Suppl. Fig. 7B</xref>). To determine what additional information nouns add beyond object category labels (<xref ref-type="fig" rid="fig2">Fig. 2C-D</xref>), we compared the object category labels with the nouns from the full image description. While category labels mostly named the main object in the image, the image description nouns provided more detailed information. For example, for the image of a golden retriever, the object category labels consisted of dog breeds, while the image description nouns captured the scene context (ocean, sky, waves, shoreline), object details (paw), relations (backdrop, angle), abstract concepts (expression), and emotions (joy) (<xref ref-type="fig" rid="fig2">Fig. 2G</xref>, left). Similarly, for the image of a snowman, the image description nouns provided detailed information about object parts (scarf, buttons, coal, carrot), background elements (landscape, trees, forest, winter), and associated emotional and abstract concepts (fun, childhood, nostalgia), that were absent from the object category labels (<xref ref-type="fig" rid="fig2">Fig. 2G</xref>, right).</p>
<p>Finally, as a control we ascertained that our results are not trivially driven by adding LLM representations per se, independent of their meaning. For this we built a fusion encoding model based on LLM representations for full image descriptions that were randomly assigned to images. This led to no significant improvement in prediction accuracy compared to the vision encoding model (<xref ref-type="fig" rid="fig2">Fig. 2C-D</xref>, gray curve). Thus, the improvement in prediction accuracy of the fusion encoding model (with image descriptions correctly assigned to stimulus images) over the vision encoding model is driven by meaningful information contained in the image descriptions.</p>
<p>Together, this shows that LLM representations capture detailed visuo-semantic information about the stimulus images, accounting for the improvement in prediction accuracy of the fusion over the vision encoding model.</p>
</sec>
<sec id="s2-4">
<title>2.4. The fusion encoding model outperforms multimodal models and is robust to model choice</title>
<p>Previous research established that multimodal DNNs trained on combined visual and linguistic input result in more accurate encoding models compared to vision encoding models, especially beyond the first stages of neural responses to visual stimulation <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c47">47</xref>–<xref ref-type="bibr" rid="c50">50</xref></sup>. We thus benchmarked our fusion encoding model against encoding models built using two multimodal DNNs: CLIP <sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c51">51</xref></sup> and VisualBERT <sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c52">52</xref></sup>. We found that our fusion encoding model outperformed both multimodal models (<xref ref-type="fig" rid="fig3">Fig. 3A-B</xref>), demonstrating the advantage of the fusion model approach for neural prediction.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Model comparisons and generalizability analysis.</title>
<p><bold>A,</bold> Prediction accuracy timecourse for the fusion encoding model, and the encoding models trained on representations from multimodal DNNs (CLIP and VisualBERT). <bold>B,</bold> Difference in prediction accuracy between the fusion encoding model, and the encoding models trained on representations from multimodal DNNs (CLIP and VisualBERT). <bold>C,</bold> Prediction accuracy timecourse for fusion encoding models trained using different vision DNNs. <bold>D,</bold> Prediction accuracy improvement over the vision encoding model for fusion encoding models trained using different vision DNNs. <bold>E,</bold> Prediction accuracy timecourse for fusion encoding models trained using different LLMs. <bold>F,</bold> Prediction accuracy improvement over the vision encoding model for fusion encoding models trained using different LLMs. <bold>A-F,</bold> The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. Rows of asterisks at the bottom of the plots indicate significant time points (onesided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR corrected across 180 time points, <italic>N</italic> = 10 participants). In gray is the area between the noise ceiling lower and upper bounds.</p></caption>
<graphic xlink:href="2506.19497v1_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Finally, to ascertain that our results do not depend on the exact instantiation of the vision DNN and LLM used, we built fusion encoding models using different vision DNNs and LLMs (<xref ref-type="fig" rid="fig3">Fig. 3C-E</xref>). Comparing the prediction accuracy of these alternative fusion encoding model instantiations against the prediction accuracy of the respective vision encoding models revealed a significant improvement in all cases, demonstrating the generality of our findings (<xref ref-type="fig" rid="fig3">Fig. 3D,F</xref>; see <xref ref-type="table" rid="tbl2">Suppl. Table 2</xref> for a detailed comparison of onset and peak latency between models).</p>
</sec>
<sec id="s2-5">
<title>2.5. The vision DNN and LLM capture distinct spectral signatures of neural activity</title>
<p>Our results suggested that the contributions of the vision DNN and LLM to the fusion encoding model depend on distinct cortical circuits active at different time points. Based on this, we hypothesized that the vision DNN and the LLM also differently relate to the spectral basis of the EEG response <sup><xref ref-type="bibr" rid="c53">53</xref>–<xref ref-type="bibr" rid="c55">55</xref></sup>. Specifically, since early neural responses with respect to image onset relate to broadband neural activity <sup><xref ref-type="bibr" rid="c56">56</xref>–<xref ref-type="bibr" rid="c61">61</xref></sup>, we hypothesized that the vision DNN component of the fusion model would accurately predict the full frequency spectrum of EEG responses. Furthermore, since later neural responses relate to low-frequency neural activity <sup><xref ref-type="bibr" rid="c62">62</xref>–<xref ref-type="bibr" rid="c64">64</xref></sup>, we further hypothesized that the LLM component of the fusion model would accurately predict low-frequencies of EEG responses.</p>
<p>To test this, we decomposed the EEG signals into time-frequency representations from 2 to 70 Hz (<xref ref-type="fig" rid="fig4">Fig. 4A</xref>), and trained encoding models to predict the EEG responses in this time-frequency domain.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Time-frequency resolved analysis and results.</title>
<p><bold>A,</bold> We decomposed the EEG responses into the timefrequency domain using Morlet wavelets. <bold>B-D,</bold> Prediction accuracy (Pearson’s <italic>r</italic>) of the EEG time-frequency data of the vision (<bold>B</bold>), language (<bold>C</bold>), and fusion (<bold>D</bold>) encoding models. <bold>E,</bold> Difference in prediction accuracy between the fusion and the language encoding models, which isolated the effect of the vision DNN. <bold>F,</bold> Difference in prediction accuracy between the fusion and the vision encoding models, which isolated the effect of the LLM.. <bold>BF,</bold> The prediction accuracies are averaged across all participants and EEG channels. The gray dashed lines indicate latency and frequency peaks of prediction accuracy with 95% confidence intervals. Cyan contour lines delineate clusters of significant effects (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 30 frequency points and 180 time points, <italic>N</italic> = 10 participants). <bold>G,</bold> EEG topography of partial correlation results, indicating the unique prediction accuracy contribution of the vision DNN. <bold>H,</bold> EEG topography of partial correlation results, indicating the unique prediction accuracy contribution of the LLM. <bold>G-H,</bold> The highlighted black dots indicate significant channels (onesided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 63 channels and 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>We found that the vision (<xref ref-type="fig" rid="fig4">Fig. 4B</xref>), language (<xref ref-type="fig" rid="fig4">Fig. 4C</xref>), and fusion (<xref ref-type="fig" rid="fig4">Fig. 4D</xref>) encoding models predicted neural activity across all investigated frequencies (cyan contours delineate significant effects. To directly assess differences between models, we subtracted the prediction accuracy of the vision (<xref ref-type="fig" rid="fig4">Fig. 4E</xref>) or language (<xref ref-type="fig" rid="fig4">Fig. 4F</xref>) encoding models from the prediction accuracy of the fusion encoding model . The difference between the fusion and the language encoding models, which isolated the effect of the vision DNN, peaked at 80 ms (80 − 80 ms), with broadband effect across the whole investigated frequency range peaking at 6.82 Hz (6.82 − 7.70 Hz) (<xref ref-type="fig" rid="fig4">Fig. 4E</xref>). In contrast, the difference between the fusion and the vision encoding models, which isolated the effect of the LLM, peaked at 360 ms (360 − 400 ms), with effects limited to frequencies below 12Hz peaking at 2.26 Hz (2.00 − 4.72 Hz) (<xref ref-type="fig" rid="fig4">Fig. 4F</xref>). These peaks had a significant latency difference of 280 ms (280 − 320 ms, <italic>p</italic> &lt; 10<sup>−5</sup>), and a significant frequency difference of 4.55 Hz (2.10 − 5.70 Hz, <italic>p</italic> &lt; 10<sup>−5</sup>) (for statistical details see <xref ref-type="table" rid="tbl3">Suppl. Table 3</xref>). Together, this further confirms that the vision DNN and LLM components uniquely capture earlier or later stages of EEG responses to visual stimuli, respectively, and additionally reveals that these two components capture different spectral bases of the EEG response. We obtained an equivalent results pattern when using partial correlation (<xref ref-type="fig" rid="fig12">Suppl. Fig. 8</xref>) and variance partitioning (<xref ref-type="fig" rid="fig13">Suppl. Fig. 9</xref>) instead of subtraction, thus substantiating the results.</p>
<p>To estimate the cortical sources uniquely predicted by the vision DNN or the LLM, we partialled out the contribution of the language or vision encoding model on the fusion encoding model, respectively, and inspected the resulting partial correlation topographies in EEG sensor space (<xref ref-type="fig" rid="fig4">Fig. 4G-H</xref>) (<italic>N</italic> = 10 participants, one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 63 channels and 180 time points). The earlier and broadband frequency effects (from 2 Hz to 70 Hz) for the vision model peaked at occipital electrodes (e.g., Oz, POz), whereas the later and lower frequency effects (from 2 Hz to 12 Hz) for the language model peaked in bilateral occipito-temporal electrodes (e.g., TP9, TP10), suggesting occipital and temporal cortical regions as sources of broad- and low-frequency bands, respectively (for statistical details see <xref ref-type="table" rid="tbl3">Suppl. Table 3</xref>, for visualization across the full frequency range see <xref ref-type="fig" rid="fig14">Suppl. Fig. 10</xref>–<xref ref-type="fig" rid="fig15">11</xref>). Equivalent results pattern emerged using variance partitioning (<xref ref-type="fig" rid="fig16">Suppl. Fig. 12</xref>–<xref ref-type="fig" rid="fig17">13</xref>) and subtraction (<xref ref-type="fig" rid="fig18">Suppl. Fig. 14</xref>–<xref ref-type="fig" rid="fig19">15</xref>).</p>
<p>Together, this revealed that the vision DNN and LLM components of the fusion encoding model capture neural dynamics with distinct spectral signatures and cortical origin: the vision DNN component captured early, broadband responses from occipital electrodes, whereas the LLM component captured later, low-frequency responses from occipito-temporal electrodes.</p>
</sec>
</sec>
<sec id="s3" sec-type="discussion">
<title>3. Discussion</title>
<p>Our goal was to provide a modelling approach accounting for the time course of visuo-semantic processing in the human brain. Towards this goal, we trained encoding models on a large datasets of EEG responses to naturalistic images to test the hypothesis that human representations across the visual processing hierarchy are better modelled by a fusion of representations from a vision DNN and an LLM, than by the representations of either model alone. Our results confirmed this hypothesis, and also showed that the fusion encoding model outperformed previous multimodal approaches. This predictive benefit was due to the vision DNN and LLM components capturing complementary neural representations: the vision DNN captured earlier and broadband signals, whereas the LLM captured later and low frequency signals. Investigation into the factors responsible for the prediction improvement by the LLM component revealed a role for detailed visuo-semantic stimulus information beyond the category of the objects viewed. The overall results pattern was robust across different instantiations of vision DNNs and LLMs, demonstrating the generalizability of our approach.</p>
<sec id="s3-1">
<title>3.1. Combining a vision DNN with an LLM improves the prediction of neural responses to visual stimulation</title>
<p>Building an encoding model of human visual processing is an integral part in the quest of understanding human vision <sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c65">65</xref></sup>. While, in the last decade, the modelling of human visual processing has been dominated by vision DNNs <sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup>, recent research started exploring language models that capture visuo-semantic information – both LLMs <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c49">49</xref>,<xref ref-type="bibr" rid="c66">66</xref></sup> and multimodal DNNs that combine visual and linguistic information <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref>,<xref ref-type="bibr" rid="c51">51</xref>,<xref ref-type="bibr" rid="c52">52</xref>,<xref ref-type="bibr" rid="c67">67</xref></sup>. Here we showed that fusing a vision DNN and an LLM outperformed encoding models trained on the two unimodal components in isolation in predicting human EEG responses to visual stimulation.The improvement over the unimodal encoding models lies in fusing the predictive power of vision DNNs and LLMs in two ways. First, the fusion encoding model reaches the same prediction accuracy as the vision or language encoding models at their respective peak latencies (i.e., 105 ms and 365 ms, respectively). Second, during the interim processing period between peaks, the fusion encoding model outperforms vision and language encoding models. Thus, the fusion encoding model predicts the temporal dynamics of neural responses to visual stimuli either equally well, or better, than vision or language encoding models.</p>
<p>Our fusion encoding model also outperformed previous approaches focussing on multimodal architectures, in particular VisualBERT <sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c52">52</xref></sup> and CLIP <sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c51">51</xref></sup>. This offers exciting venues for future research. CLIP, VisualBERT, and similar multimodal architectures provide state-of-the-art applications in a wide range of cognitive scenarios such as neural predictions with audiovisual stimuli <sup><xref ref-type="bibr" rid="c49">49</xref>,<xref ref-type="bibr" rid="c67">67</xref></sup>, cross-species neural predictions <sup><xref ref-type="bibr" rid="c66">66</xref>,<xref ref-type="bibr" rid="c68">68</xref></sup>, and prediction of behavioral patterns <sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>. Given that our fusion encoding model outperformed these multimodal approaches, this invites direct extension of the fusion model to the aforementioned applications for further increases in neural prediction.</p>
<p>Rather than providing one, unified model of neural processing, our fusion encoding model divides and conquers the neural prediction challenge in two parts, carried out by the vision DNN and LLM components, respectively. This allows to quantify how much do vision DNN and LLM representations contribute to neural prediction, in contrast to multimodal architectures where these representations are highly entangled <sup><xref ref-type="bibr" rid="c69">69</xref></sup>. We leveraged this interpretability to determine the factors determining the added predictive benefit of the LLM representations over the vision DNN representations. We found that these factors consist of detailed visuo-semantic stimulus information, including context, object parts, material properties, relations, and emotional connotations. This supports the idea that the objective of the ventral visual stream goes beyond the categorization of the major objects present in a scene <sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>, instead including a detailed semantic analysis of visual scenes <sup><xref ref-type="bibr" rid="c70">70</xref>–<xref ref-type="bibr" rid="c73">73</xref></sup>.</p>
</sec>
<sec id="s3-2">
<title>3.2. The vision DNN and LLM characterize distinct neural processing stages</title>
<p>The vision DNN and LLM representations revealed distinct neural spectro-temporal signatures. As expected, the vision DNN uniquely accounted for early stages of visual processing (i.e., with a prediction accuracy peak at 110 ms over occipital electrodes), and for a broadband spectral basis (2 − 70 Hz). This timing is consistent with an early processing stage in hierarchical models of vision <sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>, and matches the spectral signatures of processing in early visual cortex <sup><xref ref-type="bibr" rid="c55">55</xref>,<xref ref-type="bibr" rid="c74">74</xref>,<xref ref-type="bibr" rid="c75">75</xref></sup>.</p>
<p>In contrast, the LLM uniquely accounted for later stages of neural processing, revealing <sup><xref ref-type="bibr" rid="c31">31</xref></sup> two processing stages mostly in the delta (2 – 4 Hz) and theta (4 – 8 Hz) frequency range defined by two peaks: an earlier peak at 200 ms overlying temporal cortex, and a later peak at 365 ms.</p>
<p>We hypothesize that the first peak at 200 ms coincides with the first emergence of conceptual object representations <sup><xref ref-type="bibr" rid="c88">88</xref></sup>, suggestive of early semantic processing driven by theta oscillations in the anterior temporal lobe (ATL) <sup><xref ref-type="bibr" rid="c77">77</xref>–<xref ref-type="bibr" rid="c80">80</xref></sup>. According to the hub-and-spoke theory <sup><xref ref-type="bibr" rid="c81">81</xref>–<xref ref-type="bibr" rid="c83">83</xref></sup> of semantic representation, the ATL serves as a transmodal semantic hub that rapidly activates conceptual knowledge based on incoming low-level information.</p>
<p>The second peak at 365 ms had a more widespread topography, including electrodes overlying the frontal cortex. In both timing and topography, this second peak parallels the N400 component <sup><xref ref-type="bibr" rid="c84">84</xref>,<xref ref-type="bibr" rid="c85">85</xref></sup>, which indexes semantic memory access and integration through an extensive frontotemporal network <sup><xref ref-type="bibr" rid="c86">86</xref>,<xref ref-type="bibr" rid="c87">87</xref></sup> linked to the theta frequency band <sup><xref ref-type="bibr" rid="c88">88</xref>–<xref ref-type="bibr" rid="c91">91</xref></sup>.</p>
</sec>
<sec id="s3-3">
<title>3.3. Conclusion</title>
<p>Explaining human vision involves providing an encoding model of the neural representations that mediate our rich and detailed perception of the world. Our results indicate that combining vision DNNs with LLMs yields encoding models that outperform previous approaches in the prediction accuracy of time-resolved human EEG responses to visual stimuli. This opens a new direction for modelling and characterizing the time course of visuo-semantic processing in the human brain.</p>
</sec>
</sec>
<sec id="s4" sec-type="methods">
<title>4. Methods</title>
<sec id="s4-1">
<title>4.1. Data</title>
<p>For all our analyses we used the THINGS EEG2 dataset <sup><xref ref-type="bibr" rid="c32">32</xref></sup>, a large-scale dataset of EEG responses to naturalistic images optimized to build encoding models of neural responses to visual stimulation. The THINGS EEG2 dataset contains EEG recordings of 10 participants viewing images of objects on natural backgrounds from the THINGS database <sup><xref ref-type="bibr" rid="c46">46</xref></sup>. Each participant completed 82,160 trials, over 4 data collection sessions, spanning 16,740 distinct images. The dataset is split into a training partition of 16,540 images each presented 4 times (i.e., 4 repetitions per image), and a test image partition of 200 images each presented 80 times (i.e., 80 repetitions per image). The images in the training partition belong to 1,654 object categories (i.e., 10 exemplar images per category), whereas the images in the test partition belong to 200 object categories that do not overlap with the categories from the training partition.</p>
<p>The EEG recordings were collected using a 64-channel EASYCAP with electrodes arranged in the standard 10-10 system <sup><xref ref-type="bibr" rid="c92">92</xref></sup>, and a Brainvision actiCHamp amplifier at a sampling rate of 1,000 Hz. The data were online filtered between 0.03 and 100 Hz and referenced to the Fz electrode.</p>
<sec id="s4-1-1">
<title>4.1.1. EEG preprocessing</title>
<p>We epoched the EEG trials from −100 ms to 800 ms with respect to stimulus onset. To sharpen the spatial resolution of the EEG signal, we applied a current source density (CSD) transform to the epoched data. The CSD transform computes the spatial Laplacian of the sensor signal, which reduces volume conduction effects and minimizes the spatial blurring of the EEG responses <sup><xref ref-type="bibr" rid="c93">93</xref>–<xref ref-type="bibr" rid="c95">95</xref></sup>. Next, we downsampled the data to 200 Hz and baseline-corrected it by subtracting the mean of the pre-stimulus interval for each trial and channel separately. Finally, to correct for session-specific effects we <italic>z</italic>-scored the data within each recording session.</p>
</sec>
<sec id="s4-1-2">
<title>4.1.2. EEG time-frequency decomposition</title>
<p>For the EEG time-frequency domain analysis, we computed the time-frequency decomposition on the EEG dataset using complex Morlet wavelets <sup><xref ref-type="bibr" rid="c96">96</xref>,<xref ref-type="bibr" rid="c97">97</xref></sup>, resulting in a complex vector at each selected time-frequency point and channel. The frequencies ranged from 2 to 70 Hz in 50 logarithmically spaced increments. The time intervals ranged from −80 ms to 800 ms with respect to stimulus onset, in steps of 40 ms. To ensure a uniform temporal smoothing, we set a fixed number of cycles (i.e., 5 cycles) across all frequencies.</p>
</sec>
</sec>
<sec id="s4-2">
<title>4.4. Encoding models</title>
<p>We trained encoding models of EEG responses to images using image feature representations from vision DNNs, LLMs, and their combination.</p>
<sec id="s4-2-1">
<title>4.4.1. Vision DNN image feature representations</title>
<p>As the vision DNN we used CORnet-S <sup><xref ref-type="bibr" rid="c40">40</xref></sup>, a recurrent convolutional neural network composed of four sequential, anatomically-mapped areas (V1, V2, V4 and IT) with within-area recurrence, trained on object categorization on ImageNet <sup><xref ref-type="bibr" rid="c98">98</xref></sup>.</p>
<p>For the main analyses, we extracted the image feature representations of CORnet-S from the last layers of areas V1, V2, V4, IT, and from the decoder layer. We then <italic>z</italic>-scored the feature maps of the training partition images to zero mean and unit variance for each feature across the image sample dimension, and applied nonlinear principal component analysis (PCA) (polynomial kernel of degree 4) to reduce the features to 1,000 components. Finally, we applied the same <italic>z</italic>-score and PCA steps to the test partition images using the mean, standard deviation, and PCA weights fit with the training partition images.</p>
<p>For the control analyses, we built encoding models using image feature representations extracted from three additional vision DNNs: (1) AlexNet <sup><xref ref-type="bibr" rid="c99">99</xref></sup>, a supervised feedforward DNN with 5 convolutional layers followed by 3 fully-connected layers, from which we extracted feature maps from layers maxpool1, maxpool2, ReLU3, ReLU4, maxpool5, ReLU6, ReLU7, and fc8; (2) ResNet-50 <sup><xref ref-type="bibr" rid="c100">100</xref></sup>, a supervised model of 50 layers incorporating residual connections, from which we extracted feature maps from the last layer of each of its four main blocks and from the decoder layer; and (3) MoCo <sup><xref ref-type="bibr" rid="c101">101</xref></sup>, which consists in the same architecture as ResNet-50 trained in a self-supervised manner, from which we extracted feature maps from the same layers as ResNet-50. We applied the same <italic>z</italic>-score and PCA steps described above to the image feature maps of these additional models.</p>
</sec>
<sec id="s4-2-2">
<title>4.4.2. Image description generation and LLM feature representations</title>
<p>We generated five distinct text descriptions for each image in the dataset, by feeding the images to GPT-4V <sup><xref ref-type="bibr" rid="c102">102</xref></sup> with the prompt: <italic>“Describe the image in five different ways”</italic>. Because 45 train images did not pass the safety check of GPT-4V, we excluded those images and trained the models with the remaining 16,495 image conditions. The generated descriptions formed the input to the LLMs.</p>
<p>For the main analyses, we extracted the LLM feature representations using OpenAI’s textembedding-3-large. Specifically, we fed the image descriptions to text-embedding-3-large, averaged the resulting embeddings across the five description versions (i.e., resulting in one LLM feature representation for each image), and applied the same <italic>z</italic>-score and PCA steps described above to the LLM feature representations.</p>
<p>For the control analyses, we built encoding models using LLM feature representations extracted from three additional LLMs from the MTEB leaderboard <sup><xref ref-type="bibr" rid="c103">103</xref></sup>: NV-Embed-v2, all-MiniLM-L12-v2 and stella-en-1.5B-v5. We applied the same <italic>z</italic>-score and PCA steps described above to the LLM feature representations of these additional models.</p>
<p>Finally, to determine the type of semantic information relevant for predicting neural activity, we passed the following types of text into the OpenAI’s text-embedding-3-large model: human-annotated object categories, DNN-generated object categories, permuted descriptions, nouns from the descriptions, adjectives from the descriptions, and verbs from the descriptions.</p>
</sec>
<sec id="s4-2-3">
<title>4.4.3. Encoding model training</title>
<p>The encoding models consisted in ridge regressions that mapped vision DNN and/or LLM representations onto EEG responses. Using the training image partitions we trained independent ridge regression for each subject, EEG channel, and EEG time point. We started by averaging the EEG responses for each training image across the 4 repetitions. Next, we trained three distinct encoding models:
<list list-type="order">
<list-item><p>A vision encoding model, trained using the vision DNN representations as predictosrs.</p></list-item>
<list-item><p>A language encoding model, trained using LLM representations as predictors.</p></list-item>
<list-item><p>A fusion encoding model, trained using a combination of vision DNN and LLM representations as predictors. To combine the vision DNN representations (X) with the LLM representations (Y), we used a convex combination. Specifically, we computed the fused representations as a concatenation of weighted X and Y: [(1-α)X, αY], where α is a weighting coefficient constrained to 0 ≤ α ≤ 1.</p></list-item>
</list>
</p>
<p>For model training, we used a grid search algorithm <sup><xref ref-type="bibr" rid="c104">104</xref></sup> with 5-fold cross-validation to optimize the ridge regularization strength λ (in the range [10<sup>−3</sup>, 10<sup>4</sup>] with 100 log-spaced steps), and the weighting coefficient α for the fusion model (in the range [0, 1] with a step size of 0.05), independently for each subject, EEG channel, and EEG time point.</p>
</sec>
<sec id="s4-2-4">
<title>4.4.4. Encoding model testing</title>
<p>We tested the encoding models’ performance by comparing the predicted EEG responses for the test images to the recorded EEG responses for the same test images. In detail, we first randomly selected and averaged 40 repetitions of the recorded EEG responses for each test image (we used the other 40 repetitions to estimate the noise ceiling, see “Noise ceiling calculation” section). Then, for each subject, EEG channel, and EEG time point, we calculated the Pearson’s correlation coefficient between the 200-dimensional vectors of the predicted and averaged recorded EEG responses for the 200 test images. We iterated this process 100 times, each time averaging across a different random split of recorded EEG data repetitions, and averaged the correlation coefficients across the 100 iterations.</p>
</sec>
<sec id="s4-2-5">
<title>4.4.5. Noise ceiling calculation</title>
<p>We calculated the noise ceiling lower and upper bounds to estimate the theoretical maximal encoding predictions performance given the noise in the recorded EEG data. We randomly divided the recorded EEG responses for the test images into two non-overlapping partitions of 40 image condition repetitions each, where the first partition corresponded to the 40 repetitions of recorded EEG responses described in section “4.4.4 Encoding model testing”. To obtain the noise ceiling lower bound estimate, we averaged the recorded EEG responses of each partition across repetitions, and correlated the resulting averaged responses between the two partitions. To obtain the noise ceiling upper bound estimate, we correlated the recorded EEG responses of the first partition averaged across the 40 repetitions, with the recorded EEG responses averaged across all 80 test image repetitions. To avoid the results being biased by one specific configuration of repetitions, we iterated the correlation analyses 100 times, while always randomly assigning different repetitions to the two recorded EEG response partitions data partitions, and then averaged the results across the 100 iterations.</p>
</sec>
<sec id="s4-2-6">
<title>4.4.6. Partial correlation analysis</title>
<p>We performed a partial correlation analysis to isolate the individual contribution of vision DNNs and LLMs to the prediction accuracy of the fusion model. We conducted two types of analyses for each subject, EEG channel, and EEG time point:
<list list-type="order">
<list-item><p>To determine the individual contribution of the vision DNN, we calculated the partial correlation between recorded EEG responses and EEG responses predicted by the fusion encoding model, while partialling out (i.e., controlling) the EEG variance explained by the language encoding model.</p></list-item>
<list-item><p>To determine the individual contribution of the LLM, we calculated the partial correlation between recorded EEG responses and EEG responses predicted by the fusion encoding model, while partialling out the EEG variance explained by the vision encoding model.</p></list-item>
</list>
</p>
<p>In each case the partial correlation analysis consisted in the following steps:
<list list-type="order">
<list-item><p>Using ordinary least squares (OLS) we regressed the variance of the recorded EEG responses and of the EEG responses predicted by the fusion encoding model, that is explained by the control encoding model (i.e., the vision or language encoding model).</p></list-item>
<list-item><p>We computed the residuals of both regressions in 1.</p></list-item>
<list-item><p>We computed the Pearson’s correlation coefficients between the two sets of residuals.</p></list-item>
</list>
</p>
<p>Here too we repeated the analysis across 100 iterations, and averaged the results across iterations, as described above.</p>
</sec>
<sec id="s4-2-7">
<title>4.4.7. Encoding models training and testing for time-frequency EEG data</title>
<p>To train and test encoding models of the EEG responses in the time-frequency domain we followed the same analysis rationale as described above, with the following two differences. First, we additionally trained and tested a separate encoding model for each frequency. Second, as the time-frequency decomposed EEG responses are complex values with a real part and an imaginary part, for all analysis steps involving correlations (i.e., during encoding model testing) we used complex Pearson correlation coefficients <sup><xref ref-type="bibr" rid="c105">105</xref>,<xref ref-type="bibr" rid="c106">106</xref></sup>.</p>
</sec>
</sec>
<sec id="s4-3">
<title>4.5. Statistical testing</title>
<p>To establish the statistical significance of the correlation, decoding, and variance partitioning analyses, we tested all results against chance using one-sided <italic>t</italic>-tests. The rationale was to reject the null hypothesis H0 of the analyses results being at chance level with a confidence of 95% or higher (i.e., with a <italic>p</italic> value of <italic>p</italic> &lt; 0.05), thus supporting the experimental hypothesis H1 of the results being significantly higher than chance. The chance level differed across analyses: 0 Pearson’s <italic>r</italic> in the correlation analyses; 0 % accuracy in the pairwise decoding analyses; 0 explained variance in the variance partitioning analysis; a difference of 0 in the comparisons between the different encoding models.</p>
<p>We controlled familywise error rate by applying a false discovery rate (FDR) correction <sup><xref ref-type="bibr" rid="c107">107</xref></sup> to the resulting P-values to correct for the number of EEG time points (<italic>N=</italic> 180) in all analyses, and additionally for the number of EEG channels (<italic>N</italic> = 63) in the EEG topoplot visualizations.</p>
<p>To calculate the 95% confidence intervals of the peak latencies and peak latency differences, we created 10,000 bootstrapped samples by resampling the subject-specific results with replacement. This yielded empirical distributions of the results, from which we took the 95% confidence intervals.</p>
</sec>
</sec>
</body>
<back>
<sec id="s8">
<title>Supplementary Figures</title>
<fig id="fig5" position="float" fig-type="figure">
<label>Supplementary Figure 1:</label>
<caption><title>Encoding models’ single participant prediction accuracy (correlation)</title>
<p><bold>A,</bold> Prediction accuracy (Pearson’s <italic>r</italic>) correlations time course for the visual, language, and fusion encoding models on single-subject level. In gray is the area between the noise ceiling lower and upper bounds. <bold>B,</bold> Difference in prediction accuracy between the fusion and the vision or language encoding model on single-subject level. <bold>A-B,</bold> The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. In gray is the area between the noise ceiling lower and upper bounds.</p></caption>
<graphic xlink:href="2506.19497v1_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig6" position="float" fig-type="figure">
<label>Supplementary Figure 2:</label>
<caption><title>Encoding models’ prediction accuracy (pairwise decoding)</title>
<p>The rationale of this analysis was to see if a classifier trained on the recorded EEG responses is capable of generalizing its performance to the predicted EEG responses from the encoding models. This is a complementary way (to the correlation analysis, see <xref ref-type="fig" rid="fig1">Fig. 1E-F</xref>, <xref ref-type="fig" rid="fig5">Suppl. Fig. 1</xref>) to assess the similarity between the recorded and predicted EEG responses, hence the encoding models’ predictive power. We performed the pairwise decoding analysis using the recorded and predicted EEG responses for the 200 test images. We started by averaging 40 recorded EEG image condition repetitions (we used the other 40 repetitions to estimate the noise ceiling; see the “Noise ceiling calculation” paragraph in the Methods section) into 10 pseudo-trials of 4 repeats each. Next, we used the pseudo-trials for training linear support vector machines (SVMs) to perform binary classification between each pair of the 200 recorded EEG image conditions using their EEG channel activity. We then tested the trained classifiers on the corresponding pairs of predicted EEG image conditions. We performed the pairwise decoding analysis independently for each EEG time point, and then averaged decoding accuracy scores across image condition pairs, obtaining a time course of decoding accuracies. <bold>A,</bold> Pairwise decoding accuracy time course between the recorded and predicted EEG responses from visual, language, and fusion encoding models. The decoding accuracy using the vision encoding model peaks at 105 ms (100-110 ms); the decoding accuracy using the language encoding model peaks 190 ms (110-200 ms); the decoding accuracy using the fusion encoding model peaks 105 ms (100-110 ms). There is no peak-to-peak latency difference between the fusion encoding model and vision encoding model (0 ms (0-5 ms), <italic>p</italic> &gt; 0.05). The peak-to-peak latency difference between the fusion and language encoding models is 85 ms (0-95 ms), <italic>p</italic> = 0.06). <bold>B,</bold> Difference in decoding accuracy between the fusion encoding model and the vision or language encoding models. The difference between the fusion and vision encoding models peaks at 365 ms (210-365 ms); the difference between the fusion and language encoding models peaks at 95 ms (90-100 ms). Their peak-to-peak latency difference is 270 ms (115-270 ms,<italic>p</italic> &lt; 10<sup>−4</sup>). <bold>A-B</bold>, Colored dots below plots indicate significant points (one sided t-test, p &lt; 0.05, FDR-corrected for 180 time points, N = 10 participants). <bold>C,</bold> Decoding accuracy time course for visual, language, and fusion encoding models on single-subject level. <bold>D,</bold> Difference in decoding accuracy between the fusion encoding model and the vision or language encoding models on single-subject level. <bold>A and C,</bold> In gray is the area between the noise ceiling lower and upper bounds. <bold>A-D,</bold> The decoding accuracies are baseline corrected by subtracting the 50% chance level. The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. In gray is the area between the noise ceiling lower and upper bounds.</p></caption>
<graphic xlink:href="2506.19497v1_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig7" position="float" fig-type="figure">
<label>Supplementary Figure 3:</label>
<caption><title>Encoding models’ prediction accuracy (partial correlation)</title>
<p>Partial correlation time courses between the recorded EEG test responses and the predicted EEG test responses from the fusion encoding model, controlling for the variance explained by the predicted EEG test responses from either the vision or language encoding models, indicating the unique prediction accuracy contribution of the vision DNN. The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. In gray is the area between the noise ceiling lower and upper bounds. Colored dots below plots indicate significant points (one sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected for 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig8" position="float" fig-type="figure">
<label>Supplementary Figure 4:</label>
<caption><title>Encoding models’ prediction accuracy (variance partitioning)</title>
<p>We used variance partitioning to assess the unique variance of EEG responses uniquely predicted by either the vision DNN or the LLM. We trained the vision, language, and fusion encoding models, we computed their explained variance (<italic>R<sup>2</sup></italic>) using the test split, and we adjusted the resulting explained variance scores using the formula <inline-formula id="ID1">
<alternatives>
<mml:math display="inline" id="I1"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2506.19497v1_ieq1.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula>, where <italic>R<sup>2</sup></italic> is the unadjusted coefficient of determination, <italic>n</italic> is the number of test split samples, and <italic>k</italic> is the number of predictors in the model. To compute the unique variance explained by the vision encoding model we subtracted the <inline-formula id="ID2">
<alternatives>
<mml:math display="inline" id="I2"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math>
<inline-graphic xlink:href="2506.19497v1_ieq2.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula> of the language encoding model from the <inline-formula id="ID3">
<alternatives>
<mml:math display="inline" id="I3"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math>
<inline-graphic xlink:href="2506.19497v1_ieq3.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula> of the fusion encoding model. Similarly, to compute the unique variance explained by the language encoding model we subtracted the <inline-formula id="ID4">
<alternatives>
<mml:math display="inline" id="I4"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math>
<inline-graphic xlink:href="2506.19497v1_ieq4.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula> of the vision encoding model from the <inline-formula id="ID5">
<alternatives>
<mml:math display="inline" id="I5"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math>
<inline-graphic xlink:href="2506.19497v1_ieq5.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</inline-formula> of the fusion encoding model. <bold>A,</bold> Variance explained by the vision, language, and fusion encoding models. <bold>B,</bold> Unique variance explained by the vision and language encoding models. <bold>A-B,</bold> The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. Colored dots below plots indicate significant points (one sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected for 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig8.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig9" position="float" fig-type="figure">
<label>Supplementary Figure 5:</label>
<caption><title>Topography plots of encoding models’ prediction accuracy (correlation)</title>
<p><bold>A-C,</bold> Prediction accuracy (Pearson’s <italic>r</italic>) topoplots over time, averaged across participants. <bold>A,</bold> Vision encoding model. <bold>B,</bold> Language encoding model. <bold>C,</bold> Fusion encoding model. <bold>D,</bold> Difference in prediction accuracy between the fusion and the language encoding models, which isolated the effect of the vision DNN. <bold>E,</bold> Difference in prediction accuracy between the fusion and the vision encoding models, which isolated the effect of the LLM. <bold>A-E,</bold> The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig9.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig10" position="float" fig-type="figure">
<label>Supplementary Figure 6:</label>
<caption><title>Topography plots of encoding models’ prediction accuracy (variance partitioning)</title>
<p><bold>A</bold>, Variance of the recorded EEG responses explained by vision encoding model. <bold>B</bold>, Variance of the recorded EEG responses explained by the language encoding model. <bold>C</bold>, Unique variance of the recorded EEG responses explained by the fusion encoding model over the language encoding model (thus isolating the effect of the vision DNN). <bold>D</bold>, Unique variance of the recorded EEG responses explained by the fusion encoding model over the vision encoding model (thus isolating the effect of the LLM). <bold>A-D,</bold> Results are averaged across participants. The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. Colored dots below plots indicate significant points (one sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected for 180 time points, <italic>N</italic> = 10 participants). The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 63 channels and 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig10.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig11" position="float" fig-type="figure">
<label>Supplementary Figure 7:</label>
<caption><title>Prediction accuracy of fusion encoding models trained using different linguistic input</title>
<p><bold>A,</bold> Difference in prediction accuracy (Pearson’s <italic>r</italic>) between the fusion encoding model trained on full image descriptions, and the fusion encoding models trained on object category labels, nouns, adjectives, verbs, and image descriptions randomly assigned to stimulus images. <bold>B,</bold> Difference in prediction accuracy between the fusion encoding model trained on the full image description nouns, and the fusion encoding models trained on object category labels. <bold>A-B,</bold> The black dashed vertical lines indicate the onset of stimulus presentation, and the black dashed horizontal lines indicate the chance level of no experimental effect. Rows of asterisks at the bottom of the plots indicate significant time points (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR corrected across 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig11.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig12" position="float" fig-type="figure">
<label>Supplementary Figure 8:</label>
<caption><title>Time-frequency EEG prediction accuracy (partial correlation)</title>
<p><bold>A</bold>, Partial correlation between the recorded EEG test responses and the predicted EEG test responses from the fusion encoding model, controlling for the variance explained by the predicted EEG test responses from the language encoding model (thus isolating the effect of the vision DNN). <bold>B</bold>, Partial correlation between the recorded EEG test responses and the predicted EEG test responses from the fusion encoding model, controlling for the variance explained by the predicted EEG test responses from the vision encoding model (thus isolating the effect of the LLM). <bold>A-B</bold>, The dashed gray lines indicate latency and frequency peaks of prediction accuracy. Cyan contour lines delineate significant effects (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR corrected across all time and frequency points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig12.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig13" position="float" fig-type="figure">
<label>Supplementary Figure 9:</label>
<caption><title>Time-frequency EEG prediction accuracy (variance partitioning)</title>
<p><bold>A</bold>, Variance of the recorded EEG responses explained by vision encoding model <bold>B</bold>, Variance of the recorded EEG responses explained by the language encoding model. <bold>C</bold>, Unique variance of the recorded EEG responses explained by the fusion encoding model over the language encoding model (thus isolating the effect of the vision DNN). <bold>D</bold>, Unique variance of the recorded EEG responses explained by the fusion encoding model over the vision encoding model (thus isolating the effect of the LLM). <bold>A-D</bold>, The dashed gray lines indicate latency and frequency peaks of prediction accuracy. Cyan contour lines delineate significant effects (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR corrected across all time and frequency points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig13.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig14" position="float" fig-type="figure">
<label>Supplementary Figure 10:</label>
<caption><title>Time-frequency EEG prediction accuracy (partial correlation; unique contribution of the vision DNN)</title>
<p>EEG topography of partial correlation results, indicating the unique prediction accuracy contribution of the vision DNN. The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 63 channels and 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig14.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig15" position="float" fig-type="figure">
<label>Supplementary Figure 11:</label>
<caption><title>Time-frequency EEG prediction accuracy (partial correlation; unique contribution of the LLM)</title>
<p>EEG topography of partial correlation results, indicating the unique prediction accuracy contribution of the LLM. The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 63 channels and 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig15.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig16" position="float" fig-type="figure">
<label>Supplementary Figure 12:</label>
<caption><title>Time-frequency EEG prediction accuracy (variance partitioning; unique variance explained by the vision DNN)</title>
<p>EEG topography of variance partitioning results, indicating the variance uniquely explained by the vision DNN. We performed the variance partitioning analysis using the same rationale detailed in <xref ref-type="fig" rid="fig8">Suppl. Fig. 4</xref>. The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 63 channels and 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig16.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig17" position="float" fig-type="figure">
<label>Supplementary Figure 13:</label>
<caption><title>Time-frequency EEG prediction accuracy (variance partitioning; unique variance explained by the LLM)</title>
<p>EEG topography of variance partitioning results, indicating the variance uniquely explained by the LLM. We performed the variance partitioning analysis using the same rationale detailed in <xref ref-type="fig" rid="fig8">Suppl. Fig. 4</xref>. The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig17.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig18" position="float" fig-type="figure">
<label>Supplementary Figure 14:</label>
<caption><title>Time-frequency EEG prediction accuracy (subtraction; improvement of the vision DNN over the LLM)</title>
<p>EEG topography of the difference between the fusion and language encoding models prediction accuracies, indicating the improvement of the vision DNN over the LLM. The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 63 channels and 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig18.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig19" position="float" fig-type="figure">
<label>Supplementary Figure 15:</label>
<caption><title>Time-frequency EEG prediction accuracy (subtraction; improvement of the LLM over the vision DNN)</title>
<p>EEG topography of the difference between the fusion and vision encoding models prediction accuracies, indicating the improvement of the LLM over the vision DNN. The highlighted black dots indicate significant channels (one-sided <italic>t</italic>-test, <italic>p</italic> &lt; 0.05, FDR-corrected across 63 channels and 180 time points, <italic>N</italic> = 10 participants).</p></caption>
<graphic xlink:href="2506.19497v1_fig19.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s9">
<title>Supplementary Tables</title>
<table-wrap id="tbl1" position="float" orientation="portrait">
<label>Supplementary Table 1:</label>
<caption><title>Bootstrapped onset and peak latency</title></caption>
<alternatives>
<graphic xlink:href="2506.19497v1_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top" colspan="5">A. Bootstrapped onset and peak latency for main conditions</th>
</tr>
<tr>
<th align="left" valign="top">Method</th>
<th align="left" valign="top">Encoding model</th>
<th align="left" valign="top">Onset latency (ms)</th>
<th align="left" valign="top" colspan="2">Peak latency (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Correlation</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">40 (15-45)</td>
<td align="left" valign="top" colspan="2">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">50 (10-60)</td>
<td align="left" valign="top" colspan="2">365 (185-370)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Fusion</td>
<td align="left" valign="top">45 (10-45)</td>
<td align="left" valign="top" colspan="2">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">Subtraction</td>
<td align="left" valign="top">Fusion minus language</td>
<td align="left" valign="top">40 (15-45)</td>
<td align="left" valign="top" colspan="2">90 (90-105)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Fusion minus vision</td>
<td align="left" valign="top">160 (150-165)</td>
<td align="left" valign="top" colspan="2">365 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top">Partial correlation</td>
<td align="left" valign="top">Fusion (vision)</td>
<td align="left" valign="top">35 (5-45)</td>
<td align="left" valign="top" colspan="2">90 (85-90)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Fusion (language)</td>
<td align="left" valign="top">145 (140-155)</td>
<td align="left" valign="top" colspan="2">365 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top">Variance explained</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">35 (35-45)</td>
<td align="left" valign="top" colspan="2">105 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">50 (25-60)</td>
<td align="left" valign="top" colspan="2">365 (180-370)</td>
</tr>
<tr>
<td align="left" valign="top">Unique variance</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">40 (35-45)</td>
<td align="left" valign="top">90 (90-110)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">300 (300-300)</td>
<td align="left" valign="top">365 (365-370)</td>
</tr>
</tbody>
</table>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top" colspan="5">B. Bootstrapped onset and peak latency difference</th>
</tr>
<tr>
<th align="left" valign="top">Method</th>
<th align="left" valign="top">Encoding model 1</th>
<th align="left" valign="top">Encoding model 2</th>
<th align="left" valign="top">Onset latency difference (ms)</th>
<th align="left" valign="top">Peak latency difference (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Correlation</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">10 (-25–45)</td>
<td align="left" valign="top">255 (75–265) <xref ref-type="table-fn" rid="TFN4">***</xref></td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Fusion</td>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">5 (-25–45)</td>
<td align="left" valign="top">255 (75–265) <xref ref-type="table-fn" rid="TFN4">***</xref></td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">Fusion</td>
<td align="left" valign="top">5 (-20–25)</td>
<td align="left" valign="top">0 (0–5)</td>
</tr>
<tr>
<td align="left" valign="top">Subtraction</td>
<td align="left" valign="top">Fusion minus language</td>
<td align="left" valign="top">Fusion minus vision</td>
<td align="left" valign="top">120 (105–145) <xref ref-type="table-fn" rid="TFN3">****</xref></td>
<td align="left" valign="top">275 (265–310) <xref ref-type="table-fn" rid="TFN3">****</xref></td>
</tr>
<tr>
<td align="left" valign="top">Partial correlation</td>
<td align="left" valign="top">Fusion (vision)</td>
<td align="left" valign="top">Fusion (language)</td>
<td align="left" valign="top">110(100–140) <xref ref-type="table-fn" rid="TFN4">***</xref></td>
<td align="left" valign="top">275 (275–310) <xref ref-type="table-fn" rid="TFN3">****</xref></td>
</tr>
<tr>
<td align="left" valign="top">Variance explained</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">15 (-5–25)</td>
<td align="left" valign="top">260 (70–265) <xref ref-type="table-fn" rid="TFN4">***</xref></td>
</tr>
<tr>
<td align="left" valign="top">Unique variance</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">260 (255–265) <xref ref-type="table-fn" rid="TFN3">****</xref></td>
<td align="left" valign="top">275 (255–280) <xref ref-type="table-fn" rid="TFN3">****</xref></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="TFN1"><p>All differences are calculated using model 2 minus model 1</p></fn>
<fn id="TFN2"><p>The model names in the partial correlation, e.g. “Fusion (vision)”, indicate the unique contribution of the model component within the parentheses.</p></fn>
<fn id="TFN3"><label>****</label><p>indicate significant at <italic>p &lt;</italic> 0.0001.</p></fn>
<fn id="TFN4"><label>***</label><p>indicate significant at <italic>p &lt;</italic> 0.001.</p></fn>
<fn id="TFN5"><label>**</label><p>indicate significant at <italic>p &lt;</italic> 0.01.</p></fn>
<fn id="TFN6"><label>*</label><p>indicates significant at <italic>p &lt;</italic> 0.05.</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="tbl2" position="float" orientation="portrait">
<label>Supplementary Table 2:</label>
<caption><title>Bootstrapped onset and peak latency comparison across different encoding model types</title></caption>
<alternatives>
<graphic xlink:href="2506.19497v1_tbl2.tif" mime-subtype="tif" mimetype="image"/>
<graphic xlink:href="2506.19497v1_tbl3.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Encoding model</th>
<th align="left" valign="top">Onset latency (ms)</th>
<th align="left" valign="top">Peak latency (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top" colspan="3"><bold>A. Bootstrapped peak latency for linguistic factor comparison</bold></td>
</tr>
<tr>
<td align="left" valign="top">Full description</td>
<td align="left" valign="top">45 (10-45)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">Category label (human)</td>
<td align="left" valign="top">45 (20-45)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">Category label (DNN)</td>
<td align="left" valign="top">15 (5-40)</td>
<td align="left" valign="top">105 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">Random description</td>
<td align="left" valign="top">40 (35-45)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">Noun only</td>
<td align="left" valign="top">40 (-5-45)</td>
<td align="left" valign="top">105 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">Adjective only</td>
<td align="left" valign="top">45 (25-50)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">Verb only</td>
<td align="left" valign="top">35 (0-45)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">Full description minus vision</td>
<td align="left" valign="top">160 (150-170)</td>
<td align="left" valign="top">365 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top">Category label (human) minus vision</td>
<td align="left" valign="top">180 (155-295)</td>
<td align="left" valign="top">380 (355-380)</td>
</tr>
<tr>
<td align="left" valign="top">Category label (DNN) minus vision</td>
<td align="left" valign="top">160 (155-230)</td>
<td align="left" valign="top">365 (345-395)</td>
</tr>
<tr>
<td align="left" valign="top">Random description minus vision</td>
<td align="left" valign="top">-</td>
<td align="left" valign="top">-30 (-100-45)</td>
</tr>
<tr>
<td align="left" valign="top">Noun only minus vision</td>
<td align="left" valign="top">160 (-70-170)</td>
<td align="left" valign="top">385 (355-395)</td>
</tr>
<tr>
<td align="left" valign="top">Adjective only minus vision</td>
<td align="left" valign="top">230 (230-345)</td>
<td align="left" valign="top">385 (-5-410)</td>
</tr>
<tr>
<td align="left" valign="top">Verb only minus vision</td>
<td align="left" valign="top">-</td>
<td align="left" valign="top">0 (-85-750)</td>
</tr>
<tr>
<td align="left" valign="top">Full description minus category label (human)</td>
<td align="left" valign="top">150 (145-170)</td>
<td align="left" valign="top">365 (365-400)</td>
</tr>
<tr>
<td align="left" valign="top">Full description minus category label (DNN)</td>
<td align="left" valign="top">150 (115-165)</td>
<td align="left" valign="top">385 (365-405)</td>
</tr>
<tr>
<td align="left" valign="top">Full description minus random description</td>
<td align="left" valign="top">145 (130-155)</td>
<td align="left" valign="top">365 (365-400)</td>
</tr>
<tr>
<td align="left" valign="top">Full description minus verb only</td>
<td align="left" valign="top">85 (80-160)</td>
<td align="left" valign="top">385 (365-400)</td>
</tr>
<tr>
<td align="left" valign="top">Full description minus adjective only</td>
<td align="left" valign="top">145 (120-150)</td>
<td align="left" valign="top">365 (360-380)</td>
</tr>
<tr>
<td align="left" valign="top">Full description minus noun only</td>
<td align="left" valign="top">155 (25-270)</td>
<td align="left" valign="top">365 (355-385)</td>
</tr>
<tr>
<td align="left" valign="top" colspan="3"><bold>B. Bootstrapped onset and peak latency for multimodal model comparison</bold></td>
</tr>
<tr>
<td align="left" valign="top">Fusion model</td>
<td align="left" valign="top">45 (10-45)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">CLIP image encoder</td>
<td align="left" valign="top">20 (10-45)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">CLIP text encoder</td>
<td align="left" valign="top">55 (45-60)</td>
<td align="left" valign="top">365 (115-370)</td>
</tr>
<tr>
<td align="left" valign="top">CLIP fusion</td>
<td align="left" valign="top">45 (15-50)</td>
<td align="left" valign="top">110 (105-120)</td>
</tr>
<tr>
<td align="left" valign="top">VisualBERT</td>
<td align="left" valign="top">50 (15-60)</td>
<td align="left" valign="top">365 (180-370)</td>
</tr>
<tr>
<td align="left" valign="top">Fusion minus CLIP image encoder</td>
<td align="left" valign="top">55 (40-65)</td>
<td align="left" valign="top">365 (350-395)</td>
</tr>
<tr>
<td align="left" valign="top">Fusion minus CLIP text encoder</td>
<td align="left" valign="top">25 (15-45)</td>
<td align="left" valign="top">90 (85-100)</td>
</tr>
<tr>
<td align="left" valign="top">Fusion minus CLIP fusion</td>
<td align="left" valign="top">25 (5-50)</td>
<td align="left" valign="top">100 (90-205)</td>
</tr>
<tr>
<td align="left" valign="top">Fusion minus VisualBERT</td>
<td align="left" valign="top">45 (5-50)</td>
<td align="left" valign="top">90 (85-100)</td>
</tr>
<tr>
<td align="left" valign="top" colspan="3"><bold>C. Bootstrapped onset and peak latency for vision DNN comparison</bold></td>
</tr>
<tr>
<td align="left" valign="top">CORnet (fusion)</td>
<td align="left" valign="top">45 (10-45)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">AlexNet (fusion)</td>
<td align="left" valign="top">45 (0-55)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">MoCo (fusion)</td>
<td align="left" valign="top">45 (15-45)</td>
<td align="left" valign="top">110 (105-120)</td>
</tr>
<tr>
<td align="left" valign="top">ResNet-50 (fusion)</td>
<td align="left" valign="top">45 (15-45)</td>
<td align="left" valign="top">110 (105-120)</td>
</tr>
<tr>
<td align="left" valign="top">CORnet (fusion minus vision)</td>
<td align="left" valign="top">160 (150-170)</td>
<td align="left" valign="top">365 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top">AlexNet (fusion minus vision)</td>
<td align="left" valign="top">150 (145-170)</td>
<td align="left" valign="top">370 (350-400)</td>
</tr>
<tr>
<td align="left" valign="top">MoCo (fusion minus vision)</td>
<td align="left" valign="top">150 (145-160)</td>
<td align="left" valign="top">370 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top">ResNet-50 (fusion minus vision)</td>
<td align="left" valign="top">150 (145-160)</td>
<td align="left" valign="top">370 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top" colspan="3"><bold>D. Bootstrapped onset and peak latency for LLM comparison</bold></td>
</tr>
<tr>
<td align="left" valign="top">text-embedding-3-large (fusion)</td>
<td align="left" valign="top">45 (10-45)</td>
<td align="left" valign="top">110 (105-115)</td>
</tr>
<tr>
<td align="left" valign="top">all-MiniLM-L12-v2 (fusion)</td>
<td align="left" valign="top">35 (-25-45)</td>
<td align="left" valign="top">110 (100-120)</td>
</tr>
<tr>
<td align="left" valign="top">stella-en-1.5B-v5 (fusion)</td>
<td align="left" valign="top">45 (-25-45)</td>
<td align="left" valign="top">110 (100-120)</td>
</tr>
<tr>
<td align="left" valign="top">NV-Embed-v2 (fusion)</td>
<td align="left" valign="top">45 (0-45)</td>
<td align="left" valign="top">110 (100-120)</td>
</tr>
<tr>
<td align="left" valign="top">text-embedding-3-large (fusion minus vision)</td>
<td align="left" valign="top">150 (150-165)</td>
<td align="left" valign="top">365 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top">all-MiniLM-L12-v2 (fusion minus vision)</td>
<td align="left" valign="top">150 (140-165)</td>
<td align="left" valign="top">370 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top">stella-en-1.5B-v5 (fusion minus vision)</td>
<td align="left" valign="top">160 (155-235)</td>
<td align="left" valign="top">370 (360-400)</td>
</tr>
<tr>
<td align="left" valign="top">NV-Embed-v2 (fusion minus vision)</td>
<td align="left" valign="top">160 (150-185)</td>
<td align="left" valign="top">370 (350-400)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl3" position="float" orientation="portrait">
<label>Supplementary Table 3:</label>
<caption><title>Bootstrapped peak latency and peak frequency of prediction accuracy in the EEG time-frequency domain</title></caption>
<alternatives>
<graphic xlink:href="2506.19497v1_tbl4.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top" colspan="4">A. Bootstrapped peak latency and frequency in the time-frequency domain</th>
</tr>
<tr>
<th align="left" valign="top">Method</th>
<th align="left" valign="top">Encoding model</th>
<th align="left" valign="top">Peak latency (ms)</th>
<th align="left" valign="top">Peak frequency (Hz)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Correlation</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">120 (120-120)</td>
<td align="left" valign="top">6.03 (5.33-6.82)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">240 (200-360)</td>
<td align="left" valign="top">4.72 (4.72-5.33)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Fusion</td>
<td align="left" valign="top">120 (120-120)</td>
<td align="left" valign="top">6.03 (5.33-6.82)</td>
</tr>
<tr>
<td align="left" valign="top">Subtraction</td>
<td align="left" valign="top">Fusion minus language</td>
<td align="left" valign="top">80 (80-80)</td>
<td align="left" valign="top">6.82 (6.82-7.70)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Fusion minus vision</td>
<td align="left" valign="top">360 (360-400)</td>
<td align="left" valign="top">2.26 (2.00-4.72)</td>
</tr>
<tr>
<td align="left" valign="top">Partial correlation</td>
<td align="left" valign="top">Fusion (vision)</td>
<td align="left" valign="top">120 (120-120)</td>
<td align="left" valign="top">6.03 (6.03-6.82)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Fusion (language)</td>
<td align="left" valign="top">360 (360-400)</td>
<td align="left" valign="top">4.17 (3.69-4.72)</td>
</tr>
<tr>
<td align="left" valign="top">Variance explained</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">120 (120-120)</td>
<td align="left" valign="top">6.03 (5.33-6.82)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">240 (200-360)</td>
<td align="left" valign="top">4.72 (4.72-5.33)</td>
</tr>
<tr>
<td align="left" valign="top">Unique variance</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">80 (80-120)</td>
<td align="left" valign="top">6.82 (6.03-6.82)</td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">240 (200-360)</td>
<td align="left" valign="top">5.33(2.56-6.82)</td>
</tr>
</tbody>
</table>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top" colspan="5">B. Bootstrapped peak latency and frequency difference</th>
</tr>
<tr>
<th align="left" valign="top">Method</th>
<th align="left" valign="top">Encoding model 1</th>
<th align="left" valign="top">Encoding model 2</th>
<th align="left" valign="top">Peak latency difference (ms) (model 2 minus model 1)</th>
<th align="left" valign="top">Peak frequency difference (Hz) (model 1 minus model 2)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Correlation</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">120 (80–240) <xref ref-type="table-fn" rid="TFN8">****</xref></td>
<td align="left" valign="top">1.31 (0.62–2.10) <xref ref-type="table-fn" rid="TFN9">**</xref></td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Fusion</td>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">120(80–240) <xref ref-type="table-fn" rid="TFN8">****</xref></td>
<td align="left" valign="top">1.31 (0.62–2.10) <xref ref-type="table-fn" rid="TFN9">**</xref></td>
</tr>
<tr>
<td align="left" valign="top"/>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">Fusion</td>
<td align="left" valign="top">0 (0–0)</td>
<td align="left" valign="top">0.00 (0.00–0.00)</td>
</tr>
<tr>
<td align="left" valign="top">Subtraction</td>
<td align="left" valign="top">Fusion minus language</td>
<td align="left" valign="top">Fusion minus vision 280 (280–320) <xref ref-type="table-fn" rid="TFN8">****</xref></td>
<td align="left" valign="top">4.55 (2.10–5.70) <xref ref-type="table-fn" rid="TFN8">****</xref></td>
</tr>
<tr>
<td align="left" valign="top">Partial correlation</td>
<td align="left" valign="top">Fusion (vision)</td>
<td align="left" valign="top">Fusion (language)</td>
<td align="left" valign="top">240 (240–280) <xref ref-type="table-fn" rid="TFN8">****</xref></td>
<td align="left" valign="top">1.86 (1.31–3.12) <xref ref-type="table-fn" rid="TFN8">****</xref></td>
</tr>
<tr>
<td align="left" valign="top">Variance explained</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">120 (80–240) <xref ref-type="table-fn" rid="TFN8">****</xref></td>
<td align="left" valign="top">1.31 (0.00–2.10) <xref ref-type="table-fn" rid="TFN10">*</xref></td>
</tr>
<tr>
<td align="left" valign="top">Unique variance</td>
<td align="left" valign="top">Vision</td>
<td align="left" valign="top">Language</td>
<td align="left" valign="top">120(80–240) <xref ref-type="table-fn" rid="TFN8">****</xref></td>
<td align="left" valign="top">1.31 (0.00–2.10) <xref ref-type="table-fn" rid="TFN10">*</xref></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="TFN7"><p>The model names in the partial correlation, e.g. “Fusion (vision)”, indicate the unique contribution of the model component within the parentheses.</p></fn>
<fn id="TFN8"><label>****</label><p>indicate significant at <italic>p &lt;</italic> 0.0001.</p></fn>
<fn id="TFN9"><label>**</label><p>indicate significant at <italic>p &lt;</italic> 0.01.</p></fn>
<fn id="TFN10"><label>*</label><p>indicates significant at <italic>p &lt;</italic> 0.05.</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<ack>
<title>Acknowledgements</title>
<p>R.M.C. is supported by German Research Council (DFG) grants (CI 241/1-3, CI 241/1-7, INST 272/297-2), European Research Council (ERC) Consolidator Grant (ERC-CoG-2024101123101). We thank the HPC Service of FUB-IT, Freie Universität Berlin, for computing time (DOI: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.17169/refubium-26754">http://dx.doi.org/10.17169/refubium-26754</ext-link>).</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5" sec-type="data-availability">
<title>Data availability</title>
<p>Our analyses were based on THINGS EEG2, a freely available dataset of EEG recordings of 10 participants viewing images of 16,740 objects on natural backgrounds 32. THINGS EEG2’s raw EEG data is available on OSF at <ext-link ext-link-type="uri" xlink:href="https://osf.io/3jk45/">https://osf.io/3jk45/</ext-link> and the preprocessed EEG data is available on OSF at <ext-link ext-link-type="uri" xlink:href="https://osf.io/3eayd/">https://osf.io/3eayd/</ext-link>.</p>
</sec>
<sec id="s6" sec-type="code-availability">
<title>Code availability</title>
<p>Our code is available as a public Github repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/rbybryan/EEG_fusion_encoding">https://github.com/rbybryan/EEG_fusion_encoding</ext-link>.</p>
</sec>
<sec id="s7">
<title>Author contributions</title>
<p>E.D. and R.M.C. acquired funding. B.R. and R.M.C. designed research. B.R. and A.T.G. preprocessed EEG data. B.R. modelled and analyzed data. B.R., A.T.G. and R.M.C. interpreted results. B.R. prepared figures. B.R. drafted the manuscript. B.R., A.T.G., E.D. and R.M.C. edited and revised the manuscript. All authors approved the final version of the manuscript.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marr</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Visual information processing: the structure and creation of visual representations</article-title>. <source>Philos. Trans. R. Soc. Lond. B. Biol. Sci.</source> <volume>290</volume>, <fpage>199</fpage>–<lpage>218</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name><surname>Sheinberg</surname>, <given-names>D. L.</given-names></string-name></person-group> <article-title>Visual object recognition</article-title>. <source>Annu. Rev. Neurosci.</source> <volume>19</volume>, <fpage>577</fpage>–<lpage>621</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanaka</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>Inferotemporal cortex and object vision</article-title>. <source>Annu. Rev. Neurosci.</source> <volume>19</volume>, <fpage>109</fpage>–<lpage>139</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thorpe</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fize</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Marlot</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source> <volume>381</volume>, <fpage>520</fpage>–<lpage>522</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riesenhuber</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Poggio</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nat. Neurosci.</source> <volume>2</volume>, <fpage>1019</fpage>–<lpage>1025</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hung</surname>, <given-names>C. P.</given-names></string-name>, <string-name><surname>Kreiman</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Poggio</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> <article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title>. <source>Science</source> <volume>310</volume>, <fpage>863</fpage>–<lpage>866</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title>. <source>Neuron</source> <volume>60</volume>, <fpage>1126</fpage>–<lpage>1141</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Oliva</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Resolving human object recognition in space and time</article-title>. <source>Nat. Neurosci.</source> <volume>17</volume>, <fpage>455</fpage>–<lpage>462</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name> &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nat. Neurosci.</source> <volume>19</volume>, <fpage>356</fpage>–<lpage>365</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Kaiser</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Deep Neural Networks as Scientific Models</article-title>. <source>Trends Cogn. Sci.</source> <volume>23</volume>, <fpage>305</fpage>–<lpage>317</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name> &amp; others</person-group>. <article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title>. <source>Proc. Natl. Acad. Sci.</source> <volume>116</volume>, <fpage>21854</fpage>–<lpage>21863</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The neuroconnectionist research programme</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>24</volume>, <fpage>431</fpage>–<lpage>450</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name> &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLOS Comput. Biol.</source> <volume>10</volume>, <elocation-id>e1003915</elocation-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proc. Natl. Acad. Sci.</source> <volume>111</volume>, <fpage>8619</fpage>–<lpage>8624</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Güçlü</surname>, <given-names>U.</given-names></string-name> &amp; <string-name><surname>Gerven</surname>, <given-names>M. A. J. van</given-names></string-name></person-group>. <article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title>. <source>J. Neurosci.</source> <volume>35</volume>, <fpage>10005</fpage>–<lpage>10014</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Khosla</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Torralba</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Oliva</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Sci. Rep.</source> <volume>6</volume>, <fpage>27755</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Seeing it all: Convolutional network layers map the function of the human visual system</article-title>. <source>NeuroImage</source> <volume>152</volume>, <fpage>184</fpage>–<lpage>194</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Schrimpf</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like?</article-title> <source>407007 Preprint at</source> <pub-id pub-id-type="doi">10.1101/407007</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Storrs</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mehrer</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>Diverse Deep Neural Networks All Predict Human Inferior Temporal Cortex Well, After Training and Fitting</article-title>. <source>J. Cogn. Neurosci.</source> <volume>33</volume>, <fpage>2044</fpage>–<lpage>2064</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Marjieh</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sucholutsky</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Sumers</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Jacoby</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name></person-group> <article-title>Predicting Human Similarity Judgments Using Large Language Models</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.2202.04728</pub-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Visual representations in the human brain are aligned with large language models</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.2209.11737</pub-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Demircan</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Evaluating alignment between humans and neural network representations in image-based learning tasks</article-title>. <source>Adv. Neural Inf. Process. Syst.</source> <volume>37</volume>, <fpage>122406</fpage>–<lpage>122433</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walbrin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sossounov</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mahdiani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vaz</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Almeida</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Fine-grained knowledge about manipulable objects is well-predicted by contrastive language image pre-training</article-title>. <source>iScience</source> <volume>27</volume>, (<year>2024</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>L. H.</given-names></string-name>, <string-name><surname>Yatskar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Yin</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hsieh</surname>, <given-names>C.-J.</given-names></string-name> &amp; <string-name><surname>Chang</surname>, <given-names>K.-W.</given-names></string-name></person-group> <article-title>VisualBERT: A Simple and Performant Baseline for Vision and Language</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.1908.03557</pub-id> (<year>2019</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Radford</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Learning Transferable Visual Models From Natural Language Supervision</article-title>. in <conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name> <fpage>8748</fpage>–<lpage>8763</lpage> (<conf-sponsor>PMLR</conf-sponsor>, <year>2021</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Brown</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <chapter-title>Language Models are Few-Shot Learners</chapter-title>. in <source>Advances in Neural Information Processing Systems</source> vol. <volume>33</volume> <fpage>1877</fpage>–<lpage>1901</lpage> (<publisher-name>Curran Associates, Inc.</publisher-name>, <year>2020</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Rae</surname>, <given-names>J. W.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.2112.11446</pub-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Touvron</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>LLaMA: Open and Efficient Foundation Language Models</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.2302.13971</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>A. Y.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Wehbe</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Better models of human high-level visual cortex emerge from natural language supervision with a large and diverse dataset</article-title>. <source>Nat. Mach. Intell.</source> <volume>5</volume>, <fpage>1415</fpage>–<lpage>1426</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Du</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vo</surname>, <given-names>V. A.</given-names></string-name>, <string-name><surname>Lal</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name></person-group> <article-title>Brain encoding models based on multimodal transformers can transfer across language and vision</article-title>. <source>Adv. Neural Inf. Process. Syst.</source> <volume>36</volume>, <fpage>29654</fpage>–<lpage>29666</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Grosbard</surname>, <given-names>I. D.</given-names></string-name> &amp; <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name></person-group> <article-title>Self-supervision deep learning models are better models of human high-level visual cortex: The roles of multi-modality and dataset training size</article-title>. <source>2025.01.09.632216 Preprint at</source> <pub-id pub-id-type="doi">10.1101/2025.01.09.632216</pub-id> (<year>2025</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gifford</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Dwivedi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Roig</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name></person-group> <article-title>A large and rich EEG dataset for modeling human visual object recognition</article-title>. <source>NeuroImage</source> <volume>264</volume>, <fpage>119754</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Prenger</surname>, <given-names>R. J.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source> <volume>452</volume>, <fpage>352</fpage>–<lpage>355</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Encoding and decoding in fMRI</article-title>. <source>NeuroImage</source> <volume>56</volume>, <fpage>400</fpage>–<lpage>410</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Rafegas</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Vanrell</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Color Representation in CNNs: Parallelisms with Biological Vision</article-title>. <conf-name>2017 IEEE Int. Conf. Comput. Vis. Workshop ICCVW</conf-name> <fpage>2697</fpage>–<lpage>2705</lpage> (<year>2017</year>) doi:<pub-id pub-id-type="doi">10.1109/ICCVW.2017.318</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baker</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Erlikhman</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Kellman</surname>, <given-names>P. J.</given-names></string-name></person-group> <article-title>Local features and global shape information in object classification by deep convolutional neural networks</article-title>. <source>Vision Res.</source> <volume>172</volume>, <fpage>46</fpage>–<lpage>61</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeman</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Ritchie</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Bracci</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Op de Beeck</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>Orthogonal Representations of Object Shape and Category in Deep Convolutional Neural Networks and Human Visual Cortex</article-title>. <source>Sci. Rep.</source> <volume>10</volume>, <fpage>2453</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Piantadosi</surname>, <given-names>S. T.</given-names></string-name> &amp; <string-name><surname>Hill</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Meaning without reference in large language models</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.2208.02957</pub-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Alrefaie</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Salem</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Morsy</surname>, <given-names>N. E.</given-names></string-name>, <string-name><surname>Samir</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Gaber</surname>, <given-names>M. M.</given-names></string-name></person-group> <article-title>The dynamics of meaning through time: Assessment of Large Language Models</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.2501.05552</pub-id> (<year>2025</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kubilius</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Brain-like object recognition with high-performing shallow recurrent ANNs</article-title>. in <conf-name>Proceedings of the 33rd International Conference on Neural Information Processing Systems</conf-name> <fpage>12805</fpage>–<lpage>12816</lpage> (<publisher-name>Curran Associates Inc.</publisher-name>, <publisher-loc>Red Hook, NY, USA</publisher-loc>, <year>2019</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nosofsky</surname>, <given-names>R. M.</given-names></string-name></person-group> <article-title>Choice, similarity, and the context theory of classification</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn.</source> <volume>10</volume>, <fpage>104</fpage>–<lpage>114</lpage> (<year>1984</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shepard</surname>, <given-names>R. N.</given-names></string-name></person-group> <article-title>Toward a Universal Law of Generalization for Psychological Science</article-title>. <source>Science</source> <volume>237</volume>, <fpage>1317</fpage>–<lpage>1323</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roads</surname>, <given-names>B. D.</given-names></string-name> &amp; <string-name><surname>Love</surname>, <given-names>B. C.</given-names></string-name></person-group> <article-title>Modeling Similarity and Psychological Space</article-title>. <source>Annu. Rev. Psychol.</source> <volume>75</volume>, <fpage>215</fpage>–<lpage>240</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jozwik</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Storrs</surname>, <given-names>K. R.</given-names></string-name> &amp; <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Deep Convolutional Neural Networks Outperform Feature-Based But Not Categorical Models in Explaining Object Similarity Judgments</article-title>. <source>Front. Psychol.</source> <volume>8</volume>, <fpage>1726</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devereux</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Clarke</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Tyler</surname>, <given-names>L. K.</given-names></string-name></person-group> <article-title>Integrated deep visual and semantic attractor neural networks predict fMRI pattern-information along the ventral object processing pathway</article-title>. <source>Sci. Rep.</source> <volume>8</volume>, <fpage>10636</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title>. <source>PLOS One</source> <volume>14</volume>, <elocation-id>e0223792</elocation-id> (<year>2019</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Bavaresco</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kloots</surname>, <given-names>M. de H.</given-names></string-name>, <string-name><surname>Pezzelle</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Fernández</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Modelling Multimodal Integration in Human Concept Processing with Vision-and-Language Models</article-title>. <source>Preprint at</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2407.17914">http://arxiv.org/abs/2407.17914</ext-link> (<year>2024</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liao</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sawayama</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Xiao</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Probing the link between vision and language in material perception using psychophysics and unsupervised learning</article-title>. <source>PLOS Comput. Biol.</source> <volume>20</volume>, <elocation-id>e1012481</elocation-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Nakagi</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Unveiling Multi-level and Multi-modal Semantic Representations in the Human Brain using Large Language Models</article-title>. in <conf-name>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</conf-name> <fpage>20313</fpage>–<lpage>20338</lpage> <year>2024</year>. doi:<pub-id pub-id-type="doi">10.18653/v1/2024.emnlp-main.1133</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Subramaniam</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Revealing vision-language integration in the brain with multimodal networks</article-title>. in <conf-name>Proceedings of the 41st International Conference on Machine Learning</conf-name> vol. <volume>235</volume> <fpage>46868</fpage>–<lpage>46890</lpage> (<publisher-name>JMLR.org</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc>, <year>2024</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conwell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Prince</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name> &amp; <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name></person-group> <article-title>A large-scale examination of inductive biases shaping high-level visual representation in brains and machines</article-title>. <source>Nat. Commun.</source> <volume>15</volume>, <fpage>9383</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Oota</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Arora</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rowtula</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Gupta</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Bapi</surname>, <given-names>R. S.</given-names></string-name></person-group> <article-title>Visio-Linguistic Brain Encoding</article-title>. in <conf-name>Proceedings of the 29th International Conference on Computational Linguistics</conf-name> <fpage>116</fpage>–<lpage>133</lpage> <year>2022</year>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engel</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Singer</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Dynamic predictions: Oscillations and synchrony in top-down processing</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>2</volume>, <fpage>704</fpage>–<lpage>716</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Rhythms for Cognition: Communication through Coherence</article-title>. <source>Neuron</source> <volume>88</volume>, <fpage>220</fpage>–<lpage>235</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Michalareas</surname>, <given-names>G.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Alpha-beta and gamma rhythms subserve feedback and feedforward influences among human visual cortical areas</article-title>. <source>Neuron</source> <volume>89</volume>, <fpage>384</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Asynchronous broadband signals are the principal source of the BOLD response in human visual cortex</article-title>. <source>Curr. Biol. CB</source> <volume>23</volume>, <fpage>1145</fpage>–<lpage>1153</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Kerkoerle</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A.</source> <volume>111</volume>, <fpage>14332</fpage>–<lpage>14341</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bastos</surname>, <given-names>A. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Visual areas exert feedforward and feedback influences through distinct frequency channels</article-title>. <source>Neuron</source> <volume>85</volume>, <fpage>390</fpage>–<lpage>401</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuzovkin</surname>, <given-names>I.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex</article-title>. <source>Commun. Biol.</source> <volume>1</volume>, <fpage>1</fpage>–<lpage>12</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mohsenzadeh</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Qin</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</article-title>. <source>eLife</source> <volume>7</volume>, <elocation-id>e36329</elocation-id> (<year>2018</year>). <pub-id pub-id-type="doi">10.7554/eLife.36329</pub-id></mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khalaf</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Early neural activity changes associated with stimulus detection during visual conscious perception</article-title>. <source>Cereb. Cortex</source> <volume>33</volume>, <fpage>1347</fpage>–<lpage>1360</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bastos</surname>, <given-names>A. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Canonical Microcircuits for Predictive Coding</article-title>. <source>Neuron</source> <volume>76</volume>, <fpage>695</fpage>–<lpage>711</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bastos</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Lundqvist</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Waite</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Kopell</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>E. K.</given-names></string-name></person-group> <article-title>Layer and rhythm specificity for predictive routing</article-title>. <source>Proc. Natl. Acad. Sci.</source> <volume>117</volume>, <fpage>31459</fpage>–<lpage>31469</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xie</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kaiser</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name></person-group> <article-title>Visual Imagery and Perception Share Neural Representations in the Alpha Frequency Band</article-title>. <source>Curr. Biol.</source> <volume>30</volume>, <elocation-id>2621–2627.e5</elocation-id> (<year>2020</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Rust</surname>, <given-names>N. C.</given-names></string-name></person-group> <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source> <volume>73</volume>, <fpage>415</fpage>–<lpage>434</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Conwell</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Monkey See, Model Knew: Large Language Models Accurately Predict Visual Brain Responses in Humans and Non-Human Primates</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.1101/2025.03.05.641284</pub-id> (<year>2025</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Small</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lee Masson</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Isik</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Vision and language representations in multimodal AI models and human social brain regions during natural movie viewing</article-title>. <conf-name>UniReps: Unifying Representations in Neural Models Workshop, 38th Conference on Neural Information Processing Systems (NeurIPS)</conf-name>. <year>2024</year></mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conwell</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Is visual cortex really “language-aligned”? Perspectives from Model-to-Brain Comparisons in Human and Monkeys on the Natural Scenes Dataset</article-title>. <source>J. Vis.</source> <volume>24</volume>, <fpage>1288</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Dang</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.2412.02104</pub-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Russell</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Torralba</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fergus</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Freeman</surname>, <given-names>W.</given-names></string-name></person-group> <chapter-title>Object Recognition by Scene Alignment</chapter-title>. in <source>Advances in Neural Information Processing Systems</source> vol. <volume>20</volume> (<publisher-name>Curran Associates, Inc.</publisher-name>, <year>2007</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brandman</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name></person-group> <article-title>Interaction between Scene and Object Processing Revealed by Human fMRI and MEG Decoding</article-title>. <source>J. Neurosci.</source> <volume>37</volume>, <fpage>7700</fpage>–<lpage>7710</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Võ</surname>, <given-names>M. L.-H.</given-names></string-name>, <string-name><surname>Boettcher</surname>, <given-names>S. E.</given-names></string-name> &amp; <string-name><surname>Draschkow</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Reading scenes: how scene grammar guides attention and aids perception in real-world environments</article-title>. <source>Curr. Opin. Psychol.</source> <volume>29</volume>, <fpage>205</fpage>–<lpage>210</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Võ</surname>, <given-names>M. L.-H.</given-names></string-name></person-group> <article-title>The meaning and structure of scenes</article-title>. <source>Vision Res.</source> <volume>181</volume>, <fpage>10</fpage>–<lpage>20</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buffalo</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Landman</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Buschman</surname>, <given-names>T. J.</given-names></string-name> &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Laminar differences in gamma and alpha coherence in the ventral stream</article-title>. <source>Proc. Natl. Acad. Sci.</source> <volume>108</volume>, <fpage>11262</fpage>–<lpage>11267</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Decoding the orientation of contrast edges from MEG evoked and induced responses</article-title>. <source>NeuroImage</source> <volume>180</volume>, <fpage>267</fpage>–<lpage>279</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bankson</surname>, <given-names>B. B.</given-names></string-name>, <string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Groen</surname>, <given-names>I. I. A.</given-names></string-name> &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name></person-group> <article-title>The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks</article-title>. <source>NeuroImage</source> <volume>178</volume>, <fpage>172</fpage>–<lpage>182</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schendan</surname>, <given-names>H. E.</given-names></string-name> &amp; <string-name><surname>Maher</surname>, <given-names>S. M.</given-names></string-name></person-group> <article-title>Object knowledge during entry-level categorization is activated and modified by implicit memory after 200 ms</article-title>. <source>NeuroImage</source> <volume>44</volume>, <fpage>1423</fpage>–<lpage>1438</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clarke</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>K. I.</given-names></string-name> &amp; <string-name><surname>Tyler</surname>, <given-names>L. K.</given-names></string-name></person-group> <article-title>The Evolution of Meaning: Spatio-temporal Dynamics of Visual Object Recognition</article-title>. <source>J. Cogn. Neurosci.</source> <volume>23</volume>, <fpage>1887</fpage>–<lpage>1899</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clarke</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Devereux</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Randall</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Tyler</surname>, <given-names>L. K.</given-names></string-name></person-group> <article-title>Predicting the Time Course of Individual Objects with MEG</article-title>. <source>Cereb. Cortex N. Y. NY</source> <volume>25</volume>, <fpage>3602</fpage>–<lpage>3612</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clarke</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Devereux</surname>, <given-names>B. J.</given-names></string-name> &amp; <string-name><surname>Tyler</surname>, <given-names>L. K.</given-names></string-name></person-group> <article-title>Oscillatory Dynamics of Perceptual to Conceptual Transformations in the Ventral Visual Pathway</article-title>. <source>J. Cogn. Neurosci.</source> <volume>30</volume>, <fpage>1590</fpage>–<lpage>1605</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Nestor</surname>, <given-names>P. J.</given-names></string-name> &amp; <string-name><surname>Rogers</surname>, <given-names>T. T.</given-names></string-name></person-group> <article-title>Where do you know what you know? The representation of semantic knowledge in the human brain</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>8</volume>, <fpage>976</fpage>–<lpage>987</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lambon Ralph</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Sage</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>R. W.</given-names></string-name> &amp; <string-name><surname>Mayberry</surname>, <given-names>E. J.</given-names></string-name></person-group> <article-title>Coherent concepts are computed in the anterior temporal lobes</article-title>. <source>Proc. Natl. Acad. Sci.</source> <volume>107</volume>, <fpage>2717</fpage>–<lpage>2722</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ralph</surname>, <given-names>M. A. L.</given-names></string-name>, <string-name><surname>Jefferies</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Rogers</surname>, <given-names>T. T.</given-names></string-name></person-group> <article-title>The neural and computational bases of semantic cognition</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>18</volume>, <fpage>42</fpage>–<lpage>55</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kutas</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name></person-group> <article-title>Reading senseless sentences: brain potentials reflect semantic incongruity</article-title>. <source>Science</source> <volume>207</volume>, <fpage>203</fpage>–<lpage>205</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kutas</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Federmeier</surname>, <given-names>K. D.</given-names></string-name></person-group> <article-title>Thirty years and counting: Finding meaning in the N400 component of the event related brain potential (ERP)</article-title>. <source>Annu. Rev. Psychol.</source> <volume>62</volume>, <fpage>621</fpage>–<lpage>647</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Petten</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Luka</surname>, <given-names>B. J.</given-names></string-name></person-group> <article-title>Neural localization of semantic context effects in electromagnetic and hemodynamic studies</article-title>. <source>Brain Lang.</source> <volume>97</volume>, <fpage>279</fpage>–<lpage>293</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lau</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Phillips</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>A cortical network for semantics: (de)constructing the N400</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>9</volume>, <fpage>920</fpage>–<lpage>933</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sederberg</surname>, <given-names>P. B.</given-names></string-name>, <string-name><surname>Kahana</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Howard</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Donner</surname>, <given-names>E. J.</given-names></string-name> &amp; <string-name><surname>Madsen</surname>, <given-names>J. R.</given-names></string-name></person-group> <article-title>Theta and gamma oscillations during encoding predict subsequent recall</article-title>. <source>J. Neurosci. Off. J. Soc. Neurosci.</source> <volume>23</volume>, <fpage>10809</fpage>–<lpage>10814</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watrous</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Tandon</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Conner</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Pieters</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Ekstrom</surname>, <given-names>A. D.</given-names></string-name></person-group> <article-title>Frequencyspecific network connectivity increases underlie accurate spatiotemporal memory retrieval</article-title>. <source>Nat. Neurosci.</source> <volume>16</volume>, <fpage>349</fpage>–<lpage>356</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Helfrich</surname>, <given-names>R. F.</given-names></string-name> &amp; <string-name><surname>Knight</surname>, <given-names>R. T.</given-names></string-name></person-group> <article-title>Oscillatory Dynamics of Prefrontal Cognitive Control</article-title>. <source>Trends Cogn. Sci.</source> <volume>20</volume>, <fpage>916</fpage>–<lpage>930</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Theta phase precession supports memory formation and retrieval of naturalistic experience in humans</article-title>. <source>Nat. Hum. Behav.</source> <volume>8</volume>, <fpage>2423</fpage>–<lpage>2436</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nuwer</surname>, <given-names>M. R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>IFCN standards for digital recording of clinical EEG. International Federation of Clinical Neurophysiology</article-title>. <source>Electroencephalogr. Clin. Neurophysiol.</source> <volume>106</volume>, <fpage>259</fpage>–<lpage>261</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrin</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Pernier</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Scalp Current Density Mapping: Value and Estimation from Potential Data</article-title>. <source>IEEE Trans. Biomed. Eng.</source> <volume>BME-34</volume>, <fpage>283</fpage>–<lpage>288</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrin</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Pernier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Echallier</surname>, <given-names>J. F.</given-names></string-name></person-group> <article-title>Spherical splines for scalp potential and current density mapping</article-title>. <source>Electroencephalogr. Clin. Neurophysiol.</source> <volume>72</volume>, <fpage>184</fpage>–<lpage>187</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kayser</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Tenke</surname>, <given-names>C. E.</given-names></string-name></person-group> <article-title>On the benefits of using surface Laplacian (Current Source Density) methodology in electrophysiology</article-title>. <source>Int. J. Psychophysiol. Off. J. Int. Organ. Psychophysiol.</source> <volume>97</volume>, <fpage>171</fpage>–<lpage>173</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tallon-Baudry</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Delpuech</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Permier</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Oscillatory gamma-band (30-70 Hz) activity induced by a visual search task in humans</article-title>. <source>J. Neurosci. Off. J. Soc. Neurosci.</source> <volume>17</volume>, <fpage>722</fpage>–<lpage>734</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>M. X.</given-names></string-name></person-group> <article-title>A better way to define and describe Morlet wavelets for time-frequency analysis</article-title>. <source>NeuroImage</source> <volume>199</volume>, <fpage>81</fpage>–<lpage>86</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russakovsky</surname>, <given-names>O.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>Int. J. Comput. Vis.</source> <volume>115</volume>, <fpage>211</fpage>–<lpage>252</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name></person-group> <chapter-title>ImageNet Classification with Deep Convolutional Neural Networks</chapter-title>. in <source>Advances in Neural Information Processing Systems</source> vol. <volume>25</volume> (<publisher-name>Curran Associates, Inc.</publisher-name>, <year>2012</year>).</mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Deep Residual Learning for Image Recognition</article-title>. in <conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name> <fpage>770</fpage>–<lpage>778</lpage> (<publisher-name>IEEE, Las Vegas</publisher-name>, <publisher-loc>NV, USA</publisher-loc>, <year>2016</year>). doi:<pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>.</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Fan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Girshick</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Momentum Contrast for Unsupervised Visual Representation Learning</article-title>. in <conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name> <fpage>9726</fpage>–<lpage>9735</lpage> (<year>2020</year>). doi:<pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00975</pub-id>.</mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><collab>OpenAI</collab> <etal>et al.</etal></person-group> <article-title>GPT-4 Technical Report</article-title>. <source>Preprint at</source> <pub-id pub-id-type="doi">10.48550/arXiv.2303.08774</pub-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Muennighoff</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Tazi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Magne</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Reimers</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>MTEB: Massive Text Embedding Benchmark</article-title>. in <conf-name>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</conf-name> <fpage>2014</fpage>–<lpage>2037</lpage> (<year>2023</year>). doi:<pub-id pub-id-type="doi">10.18653/v1/2023.eacl-main.148</pub-id>.</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>LaValle</surname>, <given-names>S. M.</given-names></string-name> &amp; <string-name><surname>Branicky</surname>, <given-names>M. S.</given-names></string-name></person-group> <chapter-title>On the Relationship between Classical Grid Search and Probabilistic Roadmaps</chapter-title>. in <source>Algorithmic Foundations of Robotics V</source> (eds. <person-group person-group-type="editor"><string-name><surname>Boissonnat</surname>, <given-names>J.-D.</given-names></string-name>, <string-name><surname>Burdick</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Goldberg</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Hutchinson</surname>, <given-names>S.</given-names></string-name></person-group>) vol. <volume>7</volume> <fpage>59</fpage>–<lpage>75</lpage> (<publisher-name>Springer Berlin Heidelberg</publisher-name>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <year>2004</year>).</mixed-citation></ref>
<ref id="c105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schreier</surname>, <given-names>P. J.</given-names></string-name></person-group> <article-title>A Unifying Discussion of Correlation Analysis for Complex Random Vectors</article-title>. <source>IEEE Trans. Signal Process.</source> <volume>56</volume>, <fpage>1327</fpage>–<lpage>1336</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c106"><label>106.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Šverko</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Vrankić</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vlahinić</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Rogej</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Complex Pearson Correlation Coefficient for EEG Connectivity Analysis</article-title>. <source>Sensors</source> <volume>22</volume>, <fpage>1477</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Hochberg</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title>. <source>J. R. Stat. Soc. Ser. B Stat. Methodol.</source> <volume>57</volume>, <fpage>289</fpage>–<lpage>300</lpage> (<year>1995</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108915.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Xilin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study provides a large-scale EEG investigation into how visual deep neural networks (DNNs) and large language models (LLMs) differentially explain the temporal dynamics of visuo-semantic processing in the human brain. Although evidence <bold>convincingly</bold> shows that DNNs account for early perceptual responses, while LLMs capture later, low-frequency activity associated with semantic integration, the theoretical interpretation of LLM contributions and methodological aspects - including task engagement, justification of model choices, and dimensionality reduction - requires further clarification. The work will be of broad interest to fields of psychology, cognitive neuroscience, and artificial intelligence.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108915.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors provide a compelling case that the unique variance explained by LLMs is different (and later) than the unique variance explained by DNNs. This characterises when, and to some extent where, these differences occur, and for LLMs, why. The authors also probe what in the sentences is driving the brain alignment.</p>
<p>Strengths:</p>
<p>(1) The study is timely.</p>
<p>(2) There is a robust dataset and results.</p>
<p>(3) There is compelling separation between unique responses related to LLMs and DNNs.</p>
<p>(4) The paper is well-written.</p>
<p>Weaknesses:</p>
<p>The authors could explore more of what the overlap between the LLM and DNN means, and in general, how this relates to untrained networks.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108915.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study provides an investigation into the temporal dynamics of visuo-semantic processing in the human brain, leveraging both deep neural networks (DNNs) and large language models (LLMs). By developing encoding models based on vision DNNs, LLMs, and their fusion, the authors demonstrate that vision DNNs preferentially account for early, broadband EEG responses, while LLMs capture later, low-frequency signals and more detailed visuo-semantic information. It is shown that the parietal cortex shows responses during visuo-semantic processing that can be partially accounted for by language features, highlighting the role of higher-level areas in encoding abstract semantic information.</p>
<p>Strengths:</p>
<p>The study leverages a very large EEG dataset with tens of thousands of stimulus presentations, which provides an unusually strong foundation for benchmarking a variety of vision DNNs and LLMs. This scale not only increases statistical power but also allows robust comparison across model architectures, ensuring that the conclusions are not idiosyncratic to a particular dataset or stimulus set.</p>
<p>By using high-density EEG, the authors are able to capture the fine-grained temporal dynamics of visuo-semantic processing, going beyond the coarse temporal resolution of fMRI-based studies. This enables the authors to disentangle early perceptual encoding from later semantic integration, and to characterize how different model types map onto these stages of brain activity. The temporal dimension provides a particularly valuable complement to previous fMRI-based model-to-brain alignment studies.</p>
<p>The encoding models convincingly show that vision DNNs and LLMs play complementary roles in predicting neural responses. The vision DNNs explain earlier broadband responses related to perceptual processing, while LLMs capture later, lower-frequency signals that reflect higher-order semantic integration. This dual contribution provides new mechanistic insights into how visual and semantic information unfold over time in the brain, and highlights the utility of combining unimodal models rather than relying on multimodal networks alone.</p>
<p>Weaknesses:</p>
<p>(1) The experimental design is insufficiently described, particularly regarding whether participants were engaged in a behavioral task or simply passively viewing images. Task demands are known to strongly influence neural coding and representations, and without this information, it is difficult to interpret the nature of the EEG responses reported.</p>
<p>(2) The description of the encoding model lacks precision and formalization. It is not entirely clear what exactly is being predicted, how the model weights are structured across time points, or the dimensionality of the inputs and outputs. A more formal mathematical formulation would improve clarity and reproducibility.</p>
<p>(3) The selected vision DNNs (CORnet-S, ResNet, AlexNet, MoCo) have substantially lower ImageNet classification accuracies than current state-of-the-art models, with gaps of at least 10%. Referring to these models collectively as &quot;vision DNNs&quot; may overstate their representational adequacy. This performance gap raises concerns about whether the chosen models can fully capture the visual and semantic features needed for comparison with brain data. Clarification of the rationale for choosing these particular networks, and discussion of how this limitation might affect the conclusions, is needed.</p>
<p>(4) The analytic framework treats &quot;vision&quot; and &quot;language&quot; as strictly separate representational domains. However, semantics are known to emerge in many state-of-the-art visual models, with different layers spanning a gradient from low-level visual features to higher-level semantic representations. Some visual layers may be closer to LLM-derived representations than others. By not examining this finer-grained representational structure within vision DNNs, the study may oversimplify the distinction between vision- and language-based contributions.</p>
<p>(5) The study uses static images, which restricts the scope of the findings to relatively constrained visual semantics. This limitation may explain why nouns and adjectives improved predictions over vision DNNs, but verbs did not. Verbs often require dynamic information about actions or events, which static images cannot convey.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108915.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Rong et al., compare EEG image responses from a large-scale dataset to state-of-the-art vision and language models, as well as their fusion. They find that the fusion of models provides the best predictivity, with early contribution from vision models and later predictivity from language models. The paper has several strengths: high temporal resolution data (though at the expense of spatial resolution), detailed comparison of alignment (and differences) between vision and language model embeddings, and comparison of &quot;fusion&quot; of different DNN models.</p>
<p>Despite the paper's strengths, it is not clear what is at stake with these findings or how they advance our knowledge beyond other recent studies showing vision versus language model predictions of visual cortex responses with fMRI.</p>
<p>Strengths:</p>
<p>The authors use a large-scale EEG dataset and a comprehensive modeling approach. The methods are sound and involve multiple model comparisons. In particular, the disentangling of vision and language model features is something that has been largely ignored in prior related studies.</p>
<p>Weaknesses:</p>
<p>(1) The authors state their main hypothesis (lines 48-51) that human neural responses to visual stimulation are better modelled by combining representations from a vision DNN and an LLM than by the representations from either of the two components alone, and that the vision DNN and LLM components would uniquely predict earlier and later stages of visual processing, respectively.</p>
<p>While they confirm this hypothesis in largely compelling ways, it is not clear whether these results tell us something about the brain beyond how to build the most predictive model.</p>
<p>In particular, why do language models offer advantages over vision models, and what does this tell us about human visual processing? In several places, the discussion of advantages for the language model felt somewhat trivial and did not seem to advance our understanding of human vision, e.g., &quot;responses for visual stimulation encode detailed information about objects and their properties&quot; (lines 266-270) and &quot;LLM representations capture detailed visuo-semantic information about the stimulus images&quot; (line 293).</p>
<p>(2) It is not clear what the high temporal resolution EEG data tell us that the whole-brain fMRI data do not. The latency results seem to be largely in line with fMRI findings, where the early visual cortex is better predicted by vision models, and the language model is better in later/more anterior regions. In addition, it would help to discuss whether the EEG signals are likely to be restricted to the visual cortex, or could the LLM predictivity explain downstream processing captured by whole-brain EEG signals?</p>
<p>Relatedly, it would help the authors to expand on the implications of the frequency analysis.</p>
<p>(3) While the authors test many combinations of vision and language models and show their &quot;fusion&quot; advantages are largely robust to these changes, it is still hard to ignore the vast differences between vision and language models, in terms of architecture and how they are trained. Two studies (Wang et al., 2023, and Conwell et al., 2024) have now shown that when properly controlling for architecture and dataset, there is little to no advantage of language alignment in predicting visual cortex responses. It would help for the authors to both discuss this aspect of the prior literature and to try to address the implications for their own findings (related to pt 1 about what, if anything, is &quot;special&quot; about language models).</p>
<p>(4) Model features - it would help to state the dimensionality of the input embeddings for each model and how much variance is explained and preserved after the PCA step? I wonder how sensitive the findings are to this choice of dimensionality reduction, and whether an approach that finds the optimal model layer (in a cross-validated way) would show less of a difference between vision/language models (I realize this is not feasible with models like GPT-3).</p>
<p>(5) To better understand the fusion advantage, it would help to look at the results, look for a pair of vision models and a pair of language models. Can a similar advantage be found by combining models from the same modality?</p>
</body>
</sub-article>
</article>