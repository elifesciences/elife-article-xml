<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">100830</article-id>
<article-id pub-id-type="doi">10.7554/eLife.100830</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100830.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Minimal background noise enhances neural speech tracking: Evidence of stochastic resonance</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Herrmann</surname>
<given-names>Björn</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>bherrmann@reseasrch.baycrest.org</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03gp5b411</institution-id><institution>Rotman Research Institute, Baycrest Academy for Research and Education</institution></institution-wrap>, <city>North York</city>, <country>Canada</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>Department of Psychology, University of Toronto</institution></institution-wrap>, <city>Toronto</city>, <country>Canada</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>King</surname>
<given-names>Andrew J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="coi-statement"><p>Conflict of interest statement: The author declares no competing interests.</p></fn>
<fn id="n2" fn-type="con"><p>Author contributions: Björn Herrmann: Conceptualization, methodology, formal analysis, investigation, data curation, writing - original draft, writing - review and editing, visualization, supervision, project administration, funding acquisition.</p>
</fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-12-11">
<day>11</day>
<month>12</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP100830</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-10">
<day>10</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-24">
<day>24</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.19.599692"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Herrmann</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Herrmann</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-100830-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Neural activity in auditory cortex tracks the amplitude envelope of continuous speech, but recent work counter-intuitively suggests that neural tracking increases when speech is masked by background noise, despite reduced speech intelligibility. Noise-related amplification could indicate that stochastic resonance – the response facilitation through noise – supports neural speech tracking. However, a comprehensive account of the sensitivity of neural tracking to background noise and of the role cognitive investment is lacking. In five electroencephalography (EEG) experiments (N=109; box sexes), the current study demonstrates a generalized enhancement of neural speech tracking due to minimal background noise. Results show that a) neural speech tracking is enhanced for speech masked by background noise at very high SNRs (∼30 dB SNR) where speech is highly intelligible; b) this enhancement is independent of attention; c) it generalizes across different stationary background maskers, but is strongest for 12-talker babble; and d) it is present for headphone and free-field listening, suggesting that the neural-tracking enhancement generalizes to real-life listening. The work paints a clear picture that minimal background noise enhances the neural representation of the speech envelope, suggesting that stochastic resonance contributes to neural speech tracking. The work further highlights non-linearities of neural tracking induced by background noise that make its use as a biological marker for speech processing challenging.</p>
</abstract>
<abstract abstract-type="teaser">
<title>Significance statement</title>
<p>The current study demonstrates a generalized enhancement of neural speech tracking due to minimal background noise. Results show that a) neural tracking is enhanced for speech masked by noise at high SNRs (∼30 dB) where speech is highly intelligible; b) this enhancement is independent of attention; c) it generalizes across stationary background maskers, but is strongest for 12-talker babble; and d) it is present for headphone and free-field listening, indicating that the neural-tracking enhancement generalizes to real-life listening. The work suggests that stochastic resonance – the amplification of neural activity through noise – contributes to neural speech tracking. The work further highlights non-linearities of neural tracking induced by noise that make using neural tracking as a biological marker for speech processing challenging.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Electroencephalography</kwd>
<kwd>speech encoding</kwd>
<kwd>story listening</kwd>
<kwd>temporal response function</kwd>
<kwd>background noise</kwd>
<kwd>attention</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Speech in everyday life is often masked by background sound, such as music or speech by other people, making speech comprehension challenging, especially for older adults (<xref ref-type="bibr" rid="c76">Pichora-Fuller et al., 2016</xref>; <xref ref-type="bibr" rid="c37">Herrmann and Johnsrude, 2020</xref>). Having challenges with speech comprehension is a serious barrier to social participation (<xref ref-type="bibr" rid="c66">Nachtegaal et al., 2009</xref>; <xref ref-type="bibr" rid="c35">Heffernan et al., 2016</xref>) and can have long-term negative health consequences, such as cognitive decline (<xref ref-type="bibr" rid="c55">Lin and Albert, 2014</xref>; <xref ref-type="bibr" rid="c74">Panza et al., 2019</xref>). Understanding how individuals encode speech in the presence of background sound is thus an important area of research and clinical application. One successful approach to characterize speech encoding in the brain is to quantify how well neural activity tracks relevant speech features, such as the amplitude envelope, of continuous speech (<xref ref-type="bibr" rid="c17">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Crosse et al., 2021</xref>). Greater speech tracking has been associated with higher speech intelligibility (<xref ref-type="bibr" rid="c26">Ding et al., 2014</xref>; <xref ref-type="bibr" rid="c93">Vanthornhout et al., 2018</xref>; <xref ref-type="bibr" rid="c54">Lesenfants et al., 2019</xref>), leading to the suggestion that speech tracking could be a useful clinical biomarker, for example, in individuals with hearing loss (<xref ref-type="bibr" rid="c32">Gillis et al., 2022</xref>; <xref ref-type="bibr" rid="c72">Palana et al., 2022</xref>; <xref ref-type="bibr" rid="c83">Schmitt et al., 2022</xref>). Counterintuitively, however, neural speech tracking has been shown to increase in the presence of background masking sound even at masking levels for which speech intelligibility is decreased (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). The causes of this increase in speech tracking under background masking are unclear.</p>
<p>In many everyday situations, a listener must block out ambient, stationary background noise, such as multi-talker babble in a busy restaurant. For low signal-to-noise ratios (SNRs) between speech and a masker, neural speech tracking decreases relative to high SNRs (<xref ref-type="bibr" rid="c25">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>), possibly reflecting the decreased speech understanding under speech masking. In contrast, for moderate masking levels, at which listeners can understand most words, if not all, neural speech tracking can be increased relative to clear speech (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). This has been interpreted to reflect the increased attention required to understand speech (<xref ref-type="bibr" rid="c34">Hauswald et al., 2022</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). However, the low-to-moderate SNRs (e.g., &lt;10 dB) typically used to study neural speech tracking require listeners to invest cognitively and thus do not allow distinguishing a cognitive mechanism from the possibility that the noise per se leads to an increase in neural tracking, for example, through stochastic resonance, where background noise amplifies the response of a system to an input (<xref ref-type="bibr" rid="c51">Kitajo et al., 2007</xref>; <xref ref-type="bibr" rid="c62">McDonnell and Ward, 2011</xref>; <xref ref-type="bibr" rid="c52">Krauss et al., 2016</xref>). Examining neural speech tracking under high SNRs (e.g., &gt;20 dB SNR), for which individuals can understand speech with ease, is needed to understand the noise-related tracking enhancement.</p>
<p>A few previous studies suggest that noise per se may be a relevant factor. For example, the neural-tracking increase has been observed for speech masked by multi-talker background babble (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>), but appears to be less present for noise that spectrally matches the speech signal (<xref ref-type="bibr" rid="c25">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c88">Synigal et al., 2023</xref>), suggesting that the type of noise is important. Other research indicates that neural responses to tone bursts can increase in the presence of minimal noise (i.e., high SNRs) relative to clear conditions (<xref ref-type="bibr" rid="c3">Alain et al., 2009</xref>; <xref ref-type="bibr" rid="c1">Alain et al., 2012</xref>; <xref ref-type="bibr" rid="c2">Alain et al., 2014</xref>), pointing to the critical role of noise in amplifying neural responses independent of speech.</p>
<p>Understanding the relationship between neural speech tracking and background noise is critical because neural tracking is frequently used to investigate consequences of hearing loss for speech processing (<xref ref-type="bibr" rid="c79">Presacco et al., 2019</xref>; <xref ref-type="bibr" rid="c21">Decruy et al., 2020</xref>; <xref ref-type="bibr" rid="c92">Van Hirtum et al., 2023</xref>). Moreover, older adults often exhibit enhanced neural speech tracking (<xref ref-type="bibr" rid="c78">Presacco et al., 2016</xref>; <xref ref-type="bibr" rid="c11">Brodbeck et al., 2018</xref>; <xref ref-type="bibr" rid="c13">Broderick et al., 2021</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>) which is thought to be due to a loss of inhibition and increased internal noise (<xref ref-type="bibr" rid="c102">Zeng, 2013</xref>; <xref ref-type="bibr" rid="c5">Auerbach et al., 2014</xref>; <xref ref-type="bibr" rid="c52">Krauss et al., 2016</xref>; <xref ref-type="bibr" rid="c103">Zeng, 2020</xref>; <xref ref-type="bibr" rid="c38">Herrmann and Butler, 2021</xref>). The age-related neural tracking enhancement may be harder to understand if external, sound-based noise also drives increases in neural speech tracking.</p>
<p>The current study comprises 5 EEG experiments in younger adults that aim to investigate how neural speech tracking is affected by different degrees of background masking (Experiment 1), whether neural tracking enhancements are due to attention investment (Experiment 2), the generalizability of changes in neural speech tracking for different masker types (Experiments 3 and 4), and whether effects generalize from headphone to free-field listening (Experiment 5). The results point to a highly generalizable enhancement in neural speech tracking at minimal background masking levels that is independent of attention, suggesting that stochastic resonance plays a critical role.</p>
</sec>
<sec id="s2">
<title>General methods</title>
<sec id="s2a">
<title>Participants</title>
<p>The current study comprised 5 experiments. Participants were native English speakers or grew up in English-speaking countries (mostly Canada) and have been speaking English since early childhood (&lt;5 years of age). Participants reported having normal hearing abilities and no neurological disease (one person reported having ADHD, but this did not affect their participation).</p>
<p>Detailed demographic information is provided in the respective participant section for each experiment. Participants gave written informed consent prior to the experiment and were compensated for their participation. The study was conducted in accordance with the Declaration of Helsinki, the Canadian Tri-Council Policy Statement on Ethical Conduct for Research Involving Humans (TCPS2-2014), and was approved by the Research Ethics Board of the Rotman Research Institute at Baycrest Academy for Research and Education.</p>
</sec>
<sec id="s2b">
<title>Sound environment and stimulus presentation</title>
<p>Data collection was carried out in a sound-attenuating booth. Sounds were presented via Sennheiser (HD 25-SP II) headphones and computer loudspeakers (Experiment 5) through an RME Fireface 400 external sound card. Stimulation was run using Psychtoolbox in MATLAB (v3.0.14; MathWorks Inc.) on a Lenovo T480 laptop with Microsoft Windows 7. Visual stimulation was projected into the sound booth via screen mirroring. All sounds were presented at about 65 dB SPL.</p>
</sec>
<sec id="s2c">
<title>Story materials</title>
<p>In each of the 5 experiments, participants listened to 24 stories of about 1:30 to 2:30 min duration each. OpenAI’s GPT3.5 (OpenAI et al., 2023) was used to generate each story, five corresponding comprehension questions, and four associated multiple-choice answer options (1 correct, 3 incorrect). Each story was on a different topic (e.g., a long-lost friend, a struggling artist). GPT stories, questions, and answer choices were manually edited wherever needed to ensure accuracy. Auditory story files were generated using Google’s modern artificial intelligence (AI)-based speech synthesizer using the male “en-US-Neural2-J” voice with default pitch and speed parameters (<ext-link ext-link-type="uri" xlink:href="https://cloud.google.com/text-to-speech/docs/voices">https://cloud.google.com/text-to-speech/docs/voices</ext-link>). Modern AI speech is experienced as very naturalistic and speech perception is highly similar for AI and human speech (<xref ref-type="bibr" rid="c36">Herrmann, 2023</xref>). A new set of 24 stories and corresponding comprehension questions and multiple-choice options were generated for each of the 5 experiments.</p>
<p>After each story in Experiments 1, 3, 4, and 5, participants answered the 5 comprehension questions about the story. Each comprehension question comprised four response options (chance level = 25%). Participants further rated the degree to which they understood the gist of what was said in the story, using a 9-point scale that ranged from 1 (strongly disagree) to 9 (strongly agree; the precise wording was: “I understood the gist of the story” and “(Please rate this statement independently of how you felt about the other stories)”). Gist ratings were linearly scaled to range between 0 and 1 to facilitate interpretability similar to the proportion correct responses (<xref ref-type="bibr" rid="c57">Mathiesen et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). For short sentences, gist ratings have been shown to highly correlated with speech intelligibility scores (<xref ref-type="bibr" rid="c19">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="c80">Ritz et al., 2022</xref>). In Experiment 2, participants performed a visual n-back task (detailed below) while stories were presented and no comprehension questions nor the gist rating were administered.</p>
</sec>
<sec id="s2d">
<title>Electroencephalography recordings and preprocessing</title>
<p>Electroencephalographical signals were recorded from 16 scalp electrodes (Ag/Ag–Cl-electrodes; 10-20 placement) and the left and right mastoids using a BioSemi system (Amsterdam, The Netherlands). The sampling frequency was 1024 Hz with an online low-pass filter of 208 Hz. Electrodes were referenced online to a monopolar reference feedback loop connecting a driven passive sensor and a common-mode-sense (CMS) active sensor, both located posteriorly on the scalp.</p>
<p>Offline analysis was conducted using MATLAB software. An elliptic filter was used to suppress power at the 60-Hz line frequency. Data were re-referenced by averaging the signal from the left and right mastoids and subtracting the average separately from each of the 16 channels. Rereferencing to the averaged mastoids was calculated to gain high signal-to-noise ratio for auditory responses at fronto-central-parietal electrodes (<xref ref-type="bibr" rid="c82">Ruhnau et al., 2012</xref>; <xref ref-type="bibr" rid="c39">Herrmann et al., 2013</xref>). Data were filtered with a 0.7-Hz high-pass filter (length: 2449 samples, Hann window) and a 22-Hz lowpass filter (length: 211 samples, Kaiser window).</p>
<p>EEG data were segmented into time series time-locked to story onset and down-sampled to 512 Hz. Independent components analysis was used to remove signal components reflecting blinks, eye movement, and noise artifacts (<xref ref-type="bibr" rid="c6">Bell and Sejnowski, 1995</xref>; <xref ref-type="bibr" rid="c56">Makeig et al., 1995</xref> <xref ref-type="bibr" rid="c70">Oostenveld et al., 2011</xref>). After the independent components analysis, remaining artifacts were removed by setting the voltage for segments in which the EEG amplitude varied more than 80 µV within a 0.2-s period in any channel to 0 µV (cf. <xref ref-type="bibr" rid="c27">Dmochowski et al., 2012</xref>; <xref ref-type="bibr" rid="c28">Dmochowski et al., 2014</xref>; <xref ref-type="bibr" rid="c16">Cohen and Parra, 2016</xref>; <xref ref-type="bibr" rid="c45">Irsik et al., 2022</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). Data were low-pass filtered at 10 Hz (251 points, Kaiser window) because neural signals in the low-frequency range are most sensitive to acoustic features (Di Liberto et al., 2015; <xref ref-type="bibr" rid="c104">Zuk et al., 2021</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>).</p>
</sec>
<sec id="s2e">
<title>Calculation of amplitude-onset envelopes</title>
<p>For each clear story (i.e., without background noise or babble), a cochleogram was calculated using a simple auditory-periphery model with 30 auditory filters (McDermott and Simoncelli, 2011). The resulting amplitude envelope for each auditory filter was compressed by 0.6 to simulate inner ear compression (McDermott and Simoncelli, 2011). Such a computationally simple peripheral model has been shown to be sufficient, as compared to complex, more realistic models, for envelopetracking approaches (<xref ref-type="bibr" rid="c8">Biesmans et al., 2017</xref>). Amplitude envelopes were averaged across auditory filters and low-pass filtered at 40-Hz filter (Butterworth filter). To obtain the amplitude-onset envelope, the first derivative was calculated and all negative values were set to zero (<xref ref-type="bibr" rid="c41">Hertrich et al., 2012</xref>; <xref ref-type="bibr" rid="c30">Fiedler et al., 2017</xref>; <xref ref-type="bibr" rid="c29">Fiedler et al., 2019</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). The onset-envelope was down-sampled to match the sampling of the EEG data.</p>
</sec>
<sec id="s2f">
<title>EEG temporal response function and prediction accuracy</title>
<p>A forward model based on the linear temporal response function (TRF; <xref ref-type="bibr" rid="c17">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Crosse et al., 2021</xref>) was used to quantify the relationship between the amplitude-onset envelope of a story and EEG activity (note that cross-correlation led to very similar results; cf. <xref ref-type="bibr" rid="c41">Hertrich et al., 2012</xref>). The ridge regularization parameter lambda (λ), which prevents overfitting, was set to 10 based on previous work (<xref ref-type="bibr" rid="c30">Fiedler et al., 2017</xref>; <xref ref-type="bibr" rid="c29">Fiedler et al., 2019</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). Pre-selection of λ based on previous work avoids extremely low and high λ on some cross-validation iterations and avoids substantially longer computational time. Pre-selection of λ also avoids issues if limited data per condition are available, as in the current study (<xref ref-type="bibr" rid="c18">Crosse et al., 2021</xref>).</p>
<p>For each story, 50 25-s data snippets (<xref ref-type="bibr" rid="c17">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Crosse et al., 2021</xref>) were extracted randomly from the EEG data and corresponding onset-envelope (<xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). Each of the 50 EEG and onset-envelope snippets were held out once as a test dataset, while the remaining non-overlapping EEG and onset-envelope snippets were used as training datasets. That is, for each training dataset, linear regression with ridge regularization was used to map the onset-envelope onto the EEG activity to obtain a TRF model for lags ranging from 0 to 0.4 s (<xref ref-type="bibr" rid="c42">Hoerl and Kennard, 1970</xref>; <xref ref-type="bibr" rid="c17">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Crosse et al., 2021</xref>). The TRF model calculated for the training data was used to predict the EEG signal for the held-out test dataset. The Pearson correlation between the predicted and the observed EEG data of the test dataset was used as a measure of EEG prediction accuracy (<xref ref-type="bibr" rid="c17">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Crosse et al., 2021</xref>). Model estimation and prediction accuracy were calculated separately for each of the 50 data snippets per story, and prediction accuracies were averaged across the 50 snippets.</p>
<p>To investigate the neural-tracking response directly, we calculated TRFs for each training dataset for a broader set of lags, ranging from -0.15 to 0.5 s, to enable similar analyses as for traditional event-related potentials (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). TRFs were averaged across the 50 training datasets and the mean in the time window -0.15 to 0 s was subtracted from the data at each time point (baseline correction).</p>
<p>Data analyses focused on a fronto-central electrode cluster (F3, Fz, F4, C3, Cz, C4) known to be sensitive to neural activity originating from auditory cortex (<xref ref-type="bibr" rid="c65">Näätänen and Picton, 1987</xref>; <xref ref-type="bibr" rid="c77">Picton et al., 2003</xref>; <xref ref-type="bibr" rid="c40">Herrmann et al., 2018</xref>; <xref ref-type="bibr" rid="c46">Irsik et al., 2021</xref>). Prediction accuracies and TRFs were averaged across the electrodes of this fronto-central electrode cluster prior to further analysis.</p>
<p>Analyses of the TRF focused on the P1-N1 and the P2-N1 amplitude differences. The amplitude of individual TRF components (P1, N1, P2) was not analyzed because the TRF time courses for the clear condition had an overall positive shift (see also <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>) that could bias analyses more favorably towards response differences which may, however, be harder to interpret. The P1, N1, and P2 latencies were estimated from the averaged time courses across participants, separately for each SNR. P1, N1, and P2 amplitudes were calculated for each participant and condition as the mean amplitude in the 0.02 s time window centered on the peak latency. The P1-minus-N1 and P2-minus-N1 amplitude differences were calculated.</p>
</sec>
<sec id="s2g">
<title>Statistical analyses</title>
<p>All statistical analyses were carried out using MATLAB (MathWorks) and JASP software (<xref ref-type="bibr" rid="c47">JASP, 2023</xref>; version 0.18.3.0). Details about the specific tests used are provided in the relevant sections for each experiment.</p>
</sec>
</sec>
<sec id="s3">
<title>Experiment 1: Enhanced neural speech tracking due to minimal background babble</title>
<sec id="s3a">
<title>Methods and materials</title>
<sec id="s3a1">
<title>Participants</title>
<p>Twenty-two adults (median: 23.5 years; range: 18–35 years; 12 male, 9 female, 1 transgender) participated in Experiment 1.</p>
</sec>
<sec id="s3a2">
<title>Stimuli and procedures</title>
<p>Participants listened to 24 stories in 6 blocks (4 stories per block). Three of the 24 stories were played under clear conditions (i.e., without background noise). Twelve-talker babble was added to the other 21 stories (<xref ref-type="bibr" rid="c9">Bilger, 1984</xref>; <xref ref-type="bibr" rid="c10">Bilger et al., 1984</xref>; <xref ref-type="bibr" rid="c100">Wilson et al., 2012b</xref>). Twelve-talker babble is a standardized masker in speech-in-noise tests (<xref ref-type="bibr" rid="c9">Bilger, 1984</xref>; <xref ref-type="bibr" rid="c10">Bilger et al., 1984</xref>) that simulates a crowded restaurant, while not permitting the identification of individual words in the masker (<xref ref-type="bibr" rid="c59">Mattys et al., 2012</xref>). The babble masker was added at SNRs ranging from +30 to –2 dB in 21 steps of 1.6 dB SNR. Speech in background babble at 15 to 30 dB SNR is highly intelligible (<xref ref-type="bibr" rid="c43">Holder et al., 2018</xref>; <xref ref-type="bibr" rid="c85">Spyridakou et al., 2020</xref>; <xref ref-type="bibr" rid="c45">Irsik et al., 2022</xref>). No difference in speech intelligibility during story listening has been found between clear speech and speech masked by a 12-talker babble at +12 dB SNR (<xref ref-type="bibr" rid="c45">Irsik et al., 2022</xref>). Intelligibility typically drops below 90% of correctly reported words for +7 dB and lower SNR levels (<xref ref-type="bibr" rid="c45">Irsik et al., 2022</xref>). Hence, listeners have no trouble understanding speech at the highest SNRs used in the current study. All speech stimuli were normalized to the same root-mean-square amplitude and presented at about 65 dB SPL. Participants listened to each story, and after each story rated gist understanding and answered comprehension questions. Stories were presented in random order. Assignment of speech-clarity levels (clear speech and SNRs) to specific stories was randomized across participants.</p>
</sec>
<sec id="s3a3">
<title>Analyses</title>
<p>Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs for the three clear stories were averaged. For the stories in babble, a sliding average across SNR levels was calculated for behavioral data, EEG prediction accuracy, and TRFs, such that data for three neighboring SNR levels were averaged. Averaging across three stories was calculated to reduce noise in the data and match the averaging of three stories for the clear condition. For TRFs, analyses focused on the P1-N1 and the P2-N1 amplitude differences. For the statistical analyses, the clear condition was compared to each SNR level (resulting from the sliding average) using a paired samples t-test. False discovery rate (FDR) was used to account for multiple comparisons (<xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c31">Genovese et al., 2002</xref>). In cases where the data indicated a breaking point in behavior or brain response as a function of SNR, an explorative piece-wise regression (broken-stick analysis) with two linear pieces was calculated (<xref ref-type="bibr" rid="c63">McZgee and Carleton, 1970</xref>; <xref ref-type="bibr" rid="c94">Vieth, 1989</xref>; <xref ref-type="bibr" rid="c90">Toms and Lesperance, 2003</xref>). Identification of the breaking point and two pieces was calculated on the across-participant average as the minimum root mean squared error. A linear function was then fit to each participant’s data as a function of SNR and the estimated slope was tested against zero using a one-sample t-test, separately for each of the two pieces.</p>
</sec>
</sec>
<sec id="s3b">
<title>Results and discussion</title>
<p>Comprehension accuracy and gist ratings did not differ significantly between clear speech and any of the SNR levels (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig1" ref-type="fig">Figure 1A</xref>). Because gist ratings appeared to change somewhat with SNR (<xref rid="fig1" ref-type="fig">Figure 1A</xref>, right), an explorative piece-wise regression was calculated. The piece-wise regression revealed a breaking point at +15.6 dB SNR, such gist ratings decreased for +15.6 dB SNR and lower (t<sub>21</sub> = 3.008, p = 0.007, d = 0.641; right to left in <xref rid="fig1" ref-type="fig">Figure 1A</xref>), whereas gist ratings did not linearly change for +15.6 dB SNR and above (t<sub>21</sub> = 0.214, p = 0.832, d = 0.046).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Results for Experiment 1.</title>
<p><bold>A:</bold> Accuracy of story comprehension (left) and gist ratings (right). <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for different speech-clarity conditions. Topographical distributions reflect the average across all speech-clarity conditions. The black asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The absence of an asterisk indicates that there was no significant difference. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>EEG prediction accuracy did not differ between clear speech and any SNR level (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig1" ref-type="fig">Figure 1B</xref>). In contrast, the P1-N1 amplitude of the TRF was significantly greater for all SNR levels relative to clear speech (p<sub>FDR</sub> ≤ 0.05; <xref rid="fig1" ref-type="fig">Figure 1C, D</xref>).</p>
<p>Explorative piece-wise regression revealed a breaking point at +9.2 dB SNR, showing a significant linear increase in P1-N1 amplitude from +28.4 dB to +9.2 dB SNR (right to left in <xref rid="fig1" ref-type="fig">Figure 1D</xref>; t<sub>21</sub> = -5.131, p = 4.4 · 10<sup>-5</sup>, d = 1.094), whereas no significant trend was observed for SNRs from about +9.2 dB to -0.4 dB SNR (t<sub>21</sub> = 1.001, p = 0.328, d = 0.214). No differences were found for the P2-N1 amplitude (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig1" ref-type="fig">Figure 1C, D</xref>). Experiment 1 replicates the previously observed enhancement in the neural tracking of the speech onset-envelope for speech presented in moderate background babble (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). Critically, Experiment 1 expands the previous work by showing that background babble also leads to enhanced neural speech tracking for very minimal background masking levels (∼30 dB SNR). The noise-related enhancement at moderate babble levels has previously been interpreted to result from increased attention or effort to understand speech (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). However, speech for the very high SNR levels (&gt;15 dB SNR) used in Experiment 1 is highly intelligible and thus should require little or no effort to understand. It is therefore unlikely that increased attention or effort drive the increase in neural speech tracking in background babble. Nonetheless, participants attended to the speech in Experiment 1, and it can thus not be fully excluded that attention investment played a role in the noise-related enhancement. To investigate the role of attention further, participants in Experiment 2 listened to stories under the same speech-clarity conditions as for Experiment 1 while performing a visual task and ignoring the stories.</p>
</sec>
</sec>
<sec id="s4">
<title>Experiment 2: Noise-related enhancement in speech tracking is unrelated to attention</title>
<sec id="s4a">
<title>Methods and materials</title>
<sec id="s4a1">
<title>Participants</title>
<p>Twenty-two adults participated in Experiment 2 (median: 23 years; range: 18–31 years; 11 male, 10 female, 1 transgender). Data from 5 additional participants were recorded but excluded from the analysis because they performed below 60% in the visual n-back task that was used to distract participants from listening to the concurrently presented speech. A low performance could mean that the participants did not fully attend to the visual task and instead attended to the spoken speech. To avoid this possibility, data from low performers were excluded.</p>
</sec>
<sec id="s4a2">
<title>Stimuli and procedure</title>
<p>A new set of 24 stories was generated and participants were presented with 4 stories in each of 6 blocks. Speech-clarity levels were the same as in Experiment 1 (i.e., clear speech and SNRs ranging from +30 to –2 dB SNR). In Experiment 2, participants were instructed to ignore the stories and instead perform a visual 1-back task. In no part of the experiment were participants instructed to attend to the speech.</p>
<p>For the visual 1-back task, images of white digits (0 to 9) on black background were taken from the MNIST Handwritten Digit Classification Dataset (<xref ref-type="bibr" rid="c22">Deng, 2012</xref>). The digit images were selected, because different images of the same digit differ visually and thus make it challenging to use a simple feature-matching strategy to solve the 1-back task. A new digit image was presented every 0.5 seconds throughout the time over which a story was played (1:30 to 2:30 min). A digit image was presented for 0.25 s followed by a 0.25 s black screen before the next image was presented. The continuous stream of digits contained a digit repetition (albeit a different image sample) every 6 to 12 digits. Participants were tasked with pressing a button on a keyboard as soon as they detected a repetition. We did not include comprehension questions or gist ratings in Experiment 2 to avoid that participants feel they should pay attention to the speech materials. Hit rate and response time were used as behavioral measures.</p>
</sec>
<sec id="s4a3">
<title>Analyses</title>
<p>Analyses examining the effects of SNR in Experiment 2 were similar to the analyses in Experiment 1. Behavioral data (hit rate, response time in 1-back task) and EEG data (prediction accuracy, TRFs) for the three clear stories were averaged and a sliding average procedure across SNRs (three neighbors) was used for stories in babble. Statistical tests compared SNR conditions to the clear condition, including FDR-thresholding. An explorative piece-wise regression with two linear pieces was calculated in cases where the data indicated a breaking point in behavior or brain response as a function of SNR.</p>
</sec>
</sec>
<sec id="s4b">
<title>Results</title>
<p>The behavioral results showed no significant differences for hit rate or response time in the visual 1-back task between the clear condition and any of the SNR levels (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig2" ref-type="fig">Figure 2A</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Results for Experiment 2.</title>
<p><bold>A:</bold> Hit rate (left) and response times (right) for the visual n-back task. <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for different speech-clarity conditions. Topographical distributions reflect the average across all speech-clarity conditions. The black asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The absence of an asterisk indicates that there was no significant difference. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>EEG prediction accuracy did not differ between clear speech and any SNR level (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig3" ref-type="fig">Figure 3B</xref>). In contrast, and similar to Experiment 1, the P1-N1 amplitude of the TRF was significantly greater for all SNR levels relative to clear speech (p<sub>FDR</sub> ≤ 0.05; <xref rid="fig3" ref-type="fig">Figure 3C, D</xref>). An explorative piece-wise regression revealed a breaking point at +4.4 dB SNR, showing a significant linear increase in P1-N1 amplitude from +28.4 dB to +4.4 dB SNR (t<sub>21</sub> = -3.506, p = 0.002, d = 0.747; right to left in <xref rid="fig3" ref-type="fig">Figure 3D</xref>), whereas the P1-N1 amplitude decreased from +4.4 dB to -0.4 dB SNR (t<sub>21</sub> = 2.416, p = 0.025, d = 0.515). No differences were found for the P2-N1 amplitude (p<sub>FDR</sub> &gt; 0.05; <xref rid="fig3" ref-type="fig">Figure 3C, D</xref>)</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Depiction of stimulus samples.</title>
<p><bold>A:</bold> Time courses for clear speech and speech to which background babble of speech-matched noise was added at 20 dB SNR (all sound mixtures were normalized to the same root-mean-square amplitude). The first 6 s of a story are shown. <bold>B:</bold> Spectrograms of the samples in Panel A. <bold>C:</bold> Power spectra for clear speech, babble, and speech-matched noise. In C, only background babble/noise is displayed, without added speech.</p></caption>
<graphic xlink:href="599692v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The results of Experiment 2 show that, under diverted attention, neural speech tracking is enhanced for speech presented in background babble at very high SNR levels (∼30 dB SNR) relative to clear speech. Because participants did not pay attention to the speech in Experiment 2, the results indicate that attention is unlikely to drive the masker-related enhancement in neural tracking observed here and previously (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). The enhancement may thus be exogenously rather endogenously driven. Previous work suggests the type of masker may play an important role in whether speech tracking is enhanced by background sound. A masker-related enhancement was reported for a 12-talker babble masker (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>), whereas the effect was absent, or relatively small, for a stationary noise that spectrally matched the speech signal (<xref ref-type="bibr" rid="c25">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c88">Synigal et al., 2023</xref>). Sound normalization also differed. The work observing the enhancement normalized all SNR conditions to the same overall amplitude (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). This leads to a reduction in the speech-signal amplitude as SNR decreases, which, in fact, works against the observation that neural speech tracking is enhanced for speech in babble. In the other studies, the level of the speech signal was kept the same across SNRs and background noise was added at across SNRs (<xref ref-type="bibr" rid="c25">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c88">Synigal et al., 2023</xref>). Using this normalization approach, the overall amplitude of the sound mixture increases with decreasing SNR. Experiment 3 was conducted to disentangle these different potential contributions to the masker-related enhancement in neural speech tracking.</p>
</sec>
</sec>
<sec id="s5">
<title>Experiment 3: Masker-related enhancement of neural tracking is greater for babble than speech-matched noise</title>
<sec id="s5a">
<title>Methods and materials</title>
<sec id="s5a1">
<title>Participants</title>
<p>Twenty-three people (median: 25 years; range: 19–33 years; 7 male, 15 female, 1 transgender) participated in Experiment 3. Data from one additional person were recorded, but due to technical issues no triggers were recorded. The data could thus not be analyzed and were excluded.</p>
</sec>
<sec id="s5a2">
<title>Stimuli and procedures</title>
<p>A new set of 24 stories, comprehension questions, and multiple-choice options were generated. Participants listened to 4 stories in each of 6 blocks. Four of the 24 stories were presented under clear conditions. Ten stories were masked by 12-talker babble at 5 different SNRs (two stories each: +5, +10, +15, +20, +25 dB SNR), whereas the other 10 stories were masked by a stationary noise that spectrally matched the speech signal at 5 different SNRs (two stories each: +5, +10, +15, +20, +25 dB SNR). To obtain the spectrally matched noise, the long-term spectrum of the clear speech signal was calculated using a fast Fourier transform (FFT). The inverted FFT was calculated using the frequency-specific amplitudes estimated from the speech signal jointly with randomly selected phases for each frequency. <xref rid="fig4" ref-type="fig">Figure 4</xref> shows a time course snippet, a power spectrum, and a short snippet of the spectrogram for clear speech, speech masked by babble, and speech masked by the spectrally matched noise. Twelve-talker babble and speech-matched noise spectrally overlap extensively (<xref rid="fig4" ref-type="fig">Figure 4</xref>), but 12-talker babble recognizably contains speech (although individual elements cannot be identified), whereas the speech-matched noise does not contain recognizable speech elements.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Results for Experiment 3.</title>
<p><bold>A:</bold> Accuracy of story comprehension (left) and gist ratings (right). Higher vs. lower intensity refers to the two sound-level normalization types, one resulting a slightly lower intensity of the speech signal in the sound mixture than the other. <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for clear speech and different speech-masking and sound normalization conditions. In panels A, B, and D, a colored asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The specific color of the asterisk – blue vs red – indicates the normalization type (higher vs lower speech level, respectively). The absence of an asterisk indicates that there was no significant difference relative to clear speech. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For five of the 10 stories per masker type (babble, noise), the speech-plus-masker sound signal (mixed at a specific SNR) was normalized to the same root-mean-square (RMS) amplitude as the clear speech stories. This normalization results in a decreasing level of the speech signal in the speech-plus-masker mixture as SNR decreases. For the other five stories, the RMS amplitude of the speech signal was kept the same for all stories and a masker was added at the specific SNRs. This normalization results in an increasing sound level (RMS) of the sound mixture as SNR decreases. In other words, the speech signal in the sound mixture is played at a slightly lower intensity for the former than for the latter normalization type. We thus refer to this manipulation as Speech Level with lower and higher levels for the two normalization types, respectively. Please note that these differences in speech level are very minor due to the high SNRs used here. Stories were presented in randomized order and the assignment of stories to conditions was randomized across participants.</p>
</sec>
<sec id="s5a3">
<title>Analysis</title>
<p>Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs for the four clear stories were averaged. For the stories in babble and speech-matched noise, a sliding average across SNR levels was calculated for behavioral data, EEG prediction accuracy, and TRFs, such that data for four neighboring SNR levels were averaged, separately for the two masker types (babble, noise) and normalization types (adjusted speech level, non-adjusted speech level). Averaging across four stories was calculated to reduce noise in the data and match the number of stories included in the average for the clear condition. For TRFs, analyses focused on the P1-N1 and the P2-N1 amplitude differences. For the statistical analyses, the clear condition was compared to each SNR level (resulting from the sliding average) using a paired samples t-test. False discovery rate (FDR) was used to account for multiple comparisons (<xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c31">Genovese et al., 2002</xref>). Differences between masker types and normalization types were examined using a repeated measures analysis of variance (rmANOVA) with the within-participant factors SNR (10, 15, 20 dB SNR), Masker Type (babble, speech-match noise), and Normalization Type (lower vs higher speech levels). Post hoc tests were calculated for significant main effects and interactions. Holm’s methods was used to correct for multiple comparisons (<xref ref-type="bibr" rid="c44">Holm, 1979</xref>). Statistical analyses were carried out in JASP software (v0.18.3; <xref ref-type="bibr" rid="c47">JASP, 2023</xref>). Note that JASP uses pooled error terms and degrees of freedom from an rmANOVA for the corresponding post hoc effects. The reported degrees of freedom are thus higher than for direct contrasts had they been calculated independently from the rmANOVA.</p>
</sec>
</sec>
<sec id="s5b">
<title>Results</title>
<p>The analysis of behavioral performance revealed significantly lower comprehension performance and gist ratings, compared to clear speech, for babble-masked speech that was normalized such that the speech level was slightly lower than for clear speech (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). The rmANOVA for the proportion of correctly answered comprehension questions did not reveal any effects or interactions (for all p &gt; 0.1). The rmANOVA for gist ratings revealed an effect of Normalization Type (F<sub>1,22</sub> = 4.300, p = 0.05, ω<sup>2</sup> = 0.008), showing higher gist ratings when the speech signal had a ‘higher’ compared to a ‘lower’ intensity, whereas the other effects and interactions were not significant (for all p &gt; 0.05).</p>
<p>The analysis of EEG prediction accuracy revealed no differences between clear speech and any of the masked speech (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig4" ref-type="fig">Figure 4B</xref>). The rmANOVA did not reveal any significant effects nor interactions (for all p &gt; 0.25).</p>
<p>For the analysis of the TRF revealed the following results. For the babble masker, the P1-N1 amplitudes were larger for all SNR levels compared to clear speech, for both sound-level normalization types (for all p<sub>FDR</sub> ≤ 0.05). For the speech-matched noise, P1-N1 amplitudes were larger for all SNR levels compared to clear speech for the normalization resulting in lower speech intensity (for all p<sub>FDR</sub> ≤ 0.05), but only for 10 dB SNR for the normalization resulting in higher speech intensity (15 dB SNR was significant for an uncorrected t-tests). The rmANOVA revealed larger P1-N1 amplitudes for the 12-talker babble compared to the speech-matched noise masker (effect of Masker Type: F<sub>1,22</sub> = 32.849, p = 9.1 · 10<sup>-6</sup>, ω<sup>2</sup> = 0.162) and larger amplitudes for lower SNRs (effect of SNR: F<sub>2,44</sub> = 22.608, p = 1.8 · 10<sup>-7</sup>, ω<sup>2</sup> = 0.049). None of the interactions nor the effect of Normalization Type were significant (for all p &gt; 0.05).</p>
<p>Analysis of the P2-N1 revealed larger amplitudes for speech masked by speech-matched noise at 10 dB and 15 dB SNR, for both normalization types (for all p<sub>FDR</sub> ≤ 0.05). None of the other masked speech conditions differed from clear speech. The rmANOVA revealed an effect of SNR (F<sub>2,44</sub> = 26.851, p = 2.7 · 10<sup>-8</sup>, ω<sup>2</sup> = 0.024), Masker Type (F<sub>1,22</sub> = 5.859, p = 0.024, ω<sup>2</sup> = 0.023), and a SNR × Masker Type interaction (F<sub>2,44</sub> = 7.684, p = 0.001, ω<sup>2</sup> = 0.007). The interaction was due to an increase in P2-N1 amplitude with decreasing SNR for the speech-matched noise (all p<sub>Holm</sub> ≤ 0.05), whereas P2-N1 amplitudes for babble did not differ significantly between SNR conditions (all p<sub>Holm</sub> &gt; 0.05).</p>
<p>The results of Experiment 3 show that neural speech tracking increases for babble and speech-matched noise maskers compared to clear speech, but that the 12-talker babble masker leads to a greater enhancement compared to the speech-matched noise. Slight variations in the level of the speech signal in the sound mixture (resulting from different sound-level normalization procedures) do not seem to overly impact the results. Because Experiment 3 indicates that the type of background noise may affect the degree of masker-related enhancement, we conducted Experiment 4 to investigate whether different types of commonly used noises lead to similar enhancements in neural speech tracking.</p>
</sec>
</sec>
<sec id="s6">
<title>Experiment 4: Neural-tracking enhancements generalize across different masker types</title>
<sec id="s6a">
<title>Methods and materials</title>
<sec id="s6a1">
<title>Participants</title>
<p>Twenty individuals participated in Experiment 4 (median: 25.5 years; range: 19–34 years; 4 male, 15 female, 1 transgender). Data from one additional person were recorded, but due to technical issues no triggers were recorded. The data could thus not be analyzed and were excluded.</p>
</sec>
<sec id="s6a2">
<title>Stimuli and procedures</title>
<p>A new set of 24 stories, corresponding comprehension questions, and multiple-choice options were generated. Participants listened to 4 stories in each of 6 blocks. After each story, they answered 5 comprehension questions and rated gist understanding. Three stories were presented in each of 8 conditions: clear speech (no masker), speech with added white noise, pink noise, stationary noise that spectrally matched the speech signal (Experiment 3), 12-talker babble (Experiments 1-3), and three additional 12-talker babbles. The additional babble maskers were created to ensure there is nothing specific about the babble masker used in our previous work (<xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>) and in Experiments 1-3 that could lead to an enhanced tracking response and to vary spectral properties of the babble associated with different voice genders (male, female). For the three additional babble maskers, 24 text excerpts of about 800 words each were taken from Wikipedia (e.g., about flowers, forests, insurance, etc.). The 24 text excerpts were fed into Google’s AI speech synthesizer to generate 24 continuous speech materials (∼5 min) of which 12 were from male voices and 12 from female voices. The three 12-talker babble maskers were created by adding speech from 6 male and 6 female voices (mixed gender 12-talker babble), speech from the 12 male voices (male gender 12-talker babble), and speech from the 12 female voices (female gender 12-talker babble). Maskers were added to the speech signal at 20 dB SNR and all acoustic stimuli were normalized to the same root-mean-square amplitude. A power spectrum for each of the 7 masker types is displayed in <xref rid="fig5" ref-type="fig">Figure 5</xref>. Stories were presented in randomized order and assignment of stories to the 8 different conditions were randomized across participants.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Spectra for clear speech and different background noises.</title></caption>
<graphic xlink:href="599692v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s6a3">
<title>Analysis</title>
<p>Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs were averaged across the three stories for each condition. For TRFs, analyses focused on the P1-N1 and the P2-N1 amplitude differences. For the statistical analyses, the clear condition was compared to the masker conditions using a paired samples t-test. FDR was used to account for multiple comparisons (<xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c31">Genovese et al., 2002</xref>). Differences between the four different babble maskers, between babble and noise maskers, and between the three noise maskers were also investigated using paired samples t-tests.</p>
</sec>
</sec>
<sec id="s6b">
<title>Results and discussion</title>
<p>Comprehension accuracy and gist ratings for the clear story did not significantly differ from the data for masked speech (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig6" ref-type="fig">Figure 6A</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Results for Experiment 4.</title>
<p><bold>A:</bold> Accuracy of story comprehension (left) and gist ratings (right). <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for clear speech and different speech-masking conditions. Topographical distributions reflect the average across all conditions. In panels A, B, and D, the black asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The absence of an asterisk indicates that there was no significant difference relative to clear speech. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v1_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>EEG prediction accuracy did not significantly differ between clear speech and any of the masker types (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig6" ref-type="fig">Figure 6B</xref>). In contrast, the TRF P1-N1 amplitude was larger for all masker types, expect for white noise, compared to clear speech (for all p<sub>FDR</sub> ≤ 0.05; <xref rid="fig6" ref-type="fig">Figure 6D</xref>; the difference between clear speech and speech masked by white noise was significant when uncorrected; p = 0.05). There were no differences among the four different babble maskers (for all p &gt; 0.6), indicating that different voice genders of the 12-talker babble do not differentially affect the masker-related enhancement in neural speech tracking. However, the P1-N1 amplitude was larger for speech masked by the babble maskers (collapsed across the four babble maskers) compared to speech masked white noise (t<sub>19</sub> = 4.133, p = 5.7 · 10<sup>-4</sup>, d = 0.68), pink noise (t<sub>19</sub> = 5.355, p = 3.6 · 10<sup>-5</sup>, d = 1.197), and the noise spectrally matched to speech (t<sub>19</sub> = 3.039, p = 0.007, d = 0.68). There were no differences among the three noise maskers (for all p &gt; 0.05). The P2-N1 amplitude for clear speech did not different from the P2-N1 amplitude for masked speech (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig6" ref-type="fig">Figure 6D</xref>, right).</p>
<p>The results of Experiment 4 replicate the results from Experiments 1-3 by showing that babble noise at a high SNR (20 dB) increases neural speech tracking. Experiment 4 further shows that the neural-tracking enhancement generalizes across different noises, albeit a bit less for white noise (significant only when uncorrected for multiple comparisons). Results from Experiment 4 also replicate the larger tracking enhancement for speech in babble noise compared to speech in speech-matched noise observed in Experiment 3. Sounds in Experiments 1-4 were presented via headphones, which is comparable to previous work using headphones or in-ear phones (<xref ref-type="bibr" rid="c3">Alain et al., 2009</xref>; <xref ref-type="bibr" rid="c1">Alain et al., 2012</xref>; <xref ref-type="bibr" rid="c25">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c2">Alain et al., 2014</xref>; <xref ref-type="bibr" rid="c12">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="c20">Decruy et al., 2019</xref>; <xref ref-type="bibr" rid="c91">Tune et al., 2021</xref>; <xref ref-type="bibr" rid="c88">Synigal et al., 2023</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). However, headphones or in-ear phones attenuate external sound sources such that clear speech is arguably presented in ‘unnatural’ quiet. In everyday life, speech typically reaches our ears in free-field space. Experiment 5 examines whether the noise-related enhancement in neural speech tracking also generalizes to free-field listening.</p>
</sec>
</sec>
<sec id="s7">
<title>Experiment 5: Neural-tracking enhancement generalizes to free-field listening</title>
<sec id="s7a">
<title>Methods and materials</title>
<sec id="s7a1">
<title>Participants</title>
<p>Twenty-two adults participated in Experiment 5 (median: 26 years; range: 19–34 years; 10 male, 11 female, 1 transgender).</p>
</sec>
<sec id="s7a2">
<title>Stimuli and procedures</title>
<p>A new set of 24 stories, comprehension questions, and multiple-choice options were generated. Participants listened to 4 stories in each of 6 blocks. After each story, they answered 5 comprehension questions and rated gist understanding. For 3 of the 6 blocks, participants listened to the stories through headphones, as for Experiments 1-4, whereas for the other 3 blocks, participants listened to the stories via computer loudspeakers placed in front of them. Blocks for different sound-delivery conditions (headphones, loudspeakers) alternated within each participant’s session, and the starting condition was counter-balanced across participants. For each sound-delivery condition, participants listened to three stories each under clear conditions, +10 dB, +15 dB, and +20 dB SNR (12 talker babble, generated using Google’s AI voices as described for Experiment 4). Stories were distributed such that the four speech-clarity conditions were presented in each block in randomized order. All acoustic stimuli were normalized to the same root-mean-square amplitude. The sound level of the headphones and the sound level of the loudspeakers (at the location of a participant’s head) were matched.</p>
</sec>
<sec id="s7a3">
<title>Analysis</title>
<p>Behavioral data (comprehension accuracy, gist ratings), EEG prediction accuracy, and TRFs were averaged across the three stories for each speech-clarity and sound-delivery condition. For TRFs, analyses focused on the P1-N1 and the P2-N1 amplitude differences. For the statistical analyses, the clear condition was compared to the masker conditions using a paired samples t-test. FDR was used to account for multiple comparisons (<xref ref-type="bibr" rid="c7">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="c31">Genovese et al., 2002</xref>). To test for differences between sound-delivery types, a rmANOVA was calculated, using the within-participant factors SNR (10, 15, 20 dB SNR; clear speech was not included, because a difference to clear speech was tested directly as just described) and Sound Delivery (headphones, loudspeakers). Post hoc tests were calculated using dependent samples t-tests, and Holm’s methods was used to correct for multiple comparisons (<xref ref-type="bibr" rid="c44">Holm, 1979</xref>).</p>
</sec>
</sec>
<sec id="s7b">
<title>Results and discussion</title>
<p>Comprehension accuracy and gist ratings are shown in <xref rid="fig7" ref-type="fig">Figure 7A</xref>. There were no differences between clear speech and speech masked by background babble for any of the conditions, with the exception of a lower gist rating for the 20 dB SNR loudspeaker condition (<xref rid="fig7" ref-type="fig">Figure 7A</xref>, right).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Results for Experiment 5.</title>
<p><bold>A:</bold> Accuracy of story comprehension (left) and gist ratings (right). <bold>B:</bold> EEG prediction accuracy. <bold>C:</bold> Temporal response functions (TRFs). <bold>D:</bold> P1-N1 and P2-N1 amplitude difference for clear speech and different speech-masking and sound-delivery conditions. In panels A, B, and D, a colored asterisk close to the x-axis indicates a significant difference relative to the clear condition (FDR-thresholded). The specific color of the asterisk – blue vs red – indicates the sound-delivery type. The absence of an asterisk indicates that there was no significant difference relative to clear speech. Error bars reflect the standard error of the mean.</p></caption>
<graphic xlink:href="599692v1_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For the EEG prediction accuracy, there were no differences between the clear conditions and any of the masked speech conditions (<xref rid="fig7" ref-type="fig">Figure 7B</xref>). The rmANOVA did not reveal effects or interactions involving SNR or Sound Delivery (ps &gt; 0.15).</p>
<p>The analysis of TRF amplitudes revealed significantly larger P1-N1 amplitudes for all masked speech conditions compared to clear speech (for all p<sub>FDR</sub> ≤ 0.05; <xref rid="fig7" ref-type="fig">Figure 7D</xref>, left). A rmANOVA did not reveal any effects or interaction involving SNR or Sound Delivery (ps &gt; 0.1). For the P2-N1 amplitude, there were no differences between clear speech and any of the masked speech conditions (for all p<sub>FDR</sub> &gt; 0.05; <xref rid="fig7" ref-type="fig">Figure 7D</xref>, right). The rmANOVA revealed an effect of SNR (F<sub>2,42</sub> = 3.953, p = 0.027, ω<sup>2</sup> = 0.005), caused by the lower P2-N1 amplitude for the +15 dB SNR conditions compared to the +10 dB SNR (t<sub>42</sub> = 2.493, p<sub>Holm</sub> = 0.05, d = 0.171) and the +20 dB SNR condition (t<sub>42</sub> = 2.372, p<sub>Holm</sub> = 0.05, d = 0.162). There was no effect of Sound Delivery nor a SNR × Sound Delivery interaction (ps &gt; 0.5).</p>
<p>The results of Experiment 5 show that the enhanced neural tracking of speech associated with minor background babble is unspecific to delivering sounds via headphones (which typically attenuate sounds in the environment). Instead, minor background babble at +20 dB SNR also increased the neural tracking of speech under free-field (loudspeaker) conditions, thus pointing to the generalizable nature of the phenomenon to conditions more akin to naturalistic listening scenarios.</p>
</sec>
</sec>
<sec id="s8">
<title>Discussion</title>
<p>In 5 EEG experiments, the current study investigated the degree to which background masking sounds at high SNRs, for which speech is highly intelligible, affect neural speech tracking. The results show that 12-talker babble enhances neural tracking at very high SNRs (∼30 dB) relative to clear speech (Experiment 1) and that this enhancement is present even when participants carry out an unrelated visual task (Experiment 2), suggesting that attention or effort do not cause the noise-related neural-tracking enhancement. The results further show that the enhancement of neural speech tracking is greater for speech in the presence of 12-talker babble compared to a stationary noise that spectrally matched the speech (Experiments 3 and 4), although both masker types spectrally overlap. The enhancement was also greater for 12-talker babble compared to pink noise and white noise (Experiment 4). Finally, the enhanced neural speech tracking generalizes from headphone to free-field listening (Experiment 5), pointing to the real-world nature of the tracking enhancement. Overall, the current study paints a clear picture of a highly generalized enhancement of neural speech tracking in the presence of minimal background noise, making links to speech intelligibility and listening challenges in noise challenging.</p>
<sec id="s8a">
<title>Enhanced neural tracking of speech under minimal background noise</title>
<p>Across all five experiments of the current study, speech masked by background noise at high SNRs (up to 30 dB SNR) led to an enhanced neural tracking of the amplitude-onset envelope of speech. The enhancement was present for different background maskers, but most prominently for 12-talker babble. Previous work on neural speech tracking also observed enhanced neural tracking for speech masked by 12-talker babble at moderate SNRs (∼12 dB; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>), consistent with the current study. The current results are also consistent with studies showing a noise-related enhancement to tone bursts (<xref ref-type="bibr" rid="c3">Alain et al., 2009</xref>; <xref ref-type="bibr" rid="c1">Alain et al., 2012</xref>; <xref ref-type="bibr" rid="c2">Alain et al., 2014</xref>), syllable onsets (<xref ref-type="bibr" rid="c75">Parbery-Clark et al., 2011</xref>), and high-frequency temporal modulations in sounds (<xref ref-type="bibr" rid="c96">Ward et al., 2010</xref>; <xref ref-type="bibr" rid="c84">Shukla and Bidelman, 2021</xref>). Other work, using a noise masker that spectrally matches the target speech, have not reported tracking enhancements (<xref ref-type="bibr" rid="c25">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c88">Synigal et al., 2023</xref>). However, in these works, SNRs have been lower (&lt;10 dB) to investigate neural tracking under challenging listening conditions. At low SNRs, neural speech tracking decreases (<xref ref-type="bibr" rid="c25">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref rid="fig1" ref-type="fig">Figures 1</xref> and <xref rid="fig2" ref-type="fig">2</xref>), thus resulting in an inverted u-shape in relation to SNR for attentive and passive listening (Experiments 1 and 2). Moreover, the speech-tracking enhancement was smaller for speech-matched noise compared to babble noise (<xref rid="fig4" ref-type="fig">Figures 4</xref> and <xref rid="fig6" ref-type="fig">6</xref>), potentially explaining the absence of the enhancement for speech-matched noise at low SNRs in previous work (<xref ref-type="bibr" rid="c25">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="c88">Synigal et al., 2023</xref>).</p>
<p>The noise-related enhancement in the neural tracking of the speech envelope was greatest for 12-talker babble, but it was also present for speech-matched noise, pink noise, and, to some extent, white noise. The latter three noises bare no perceptional relation to speech, but resemble stationary, background buzzing from industrial noise, heavy rain, waterfalls, wind, or ventilation. Twelve-talker babble – which is also a stationary masker – is clearly recognizable as overlapping speech, but words or phonemes cannot be identified (<xref ref-type="bibr" rid="c9">Bilger, 1984</xref>; <xref ref-type="bibr" rid="c10">Bilger et al., 1984</xref>; <xref ref-type="bibr" rid="c98">Wilson, 2003</xref>; <xref ref-type="bibr" rid="c100">Wilson et al., 2012b</xref>). There may thus be something about the naturalistic, speech nature of the background babble that facilitates neural speech tracking.</p>
<p>The spectral power for both the 12-talker babble and the speech-matched noise overlaps strongly with the spectral properties of the speech signal, although the speech-matched noise most closely resembles the spectrum of speech. Spectral overlap of the background sound and the speech signal could cause energetic masking in the auditory periphery and degrade accurate neural speech representations (<xref ref-type="bibr" rid="c14">Brungart et al., 2006</xref>; <xref ref-type="bibr" rid="c59">Mattys et al., 2012</xref>; <xref ref-type="bibr" rid="c99">Wilson et al., 2012a</xref>; Kidd et al., 2019). Although peripheral masking would not explain why neural speech tracking is enhanced in the first place, more peripheral masking for the speech-matched noise compared to the 12-talker babble would be consistent with a reduced enhancement for the former compared to the latter masker. However, pink noise and white noise overlap spectrally much less with speech than the other two background maskers, but the neural tracking enhancement did not differ between the speech-matched noise, pink noise, and white noise maskers. This again suggests that there may be something about the speech-nature of the babble masker that drives the larger neural tracking enhancement.</p>
<p>Critically, the current results have implications for research and clinical applications. The neural tracking of the speech envelope has been linked to speech intelligibility (<xref ref-type="bibr" rid="c26">Ding et al., 2014</xref>; <xref ref-type="bibr" rid="c93">Vanthornhout et al., 2018</xref>; <xref ref-type="bibr" rid="c54">Lesenfants et al., 2019</xref>) and has been proposed to be a useful clinical biomarker for speech encoding in the brain (<xref ref-type="bibr" rid="c24">Dial et al., 2021</xref>; <xref ref-type="bibr" rid="c32">Gillis et al., 2022</xref>; <xref ref-type="bibr" rid="c83">Schmitt et al., 2022</xref>; <xref ref-type="bibr" rid="c53">Kries et al., 2024</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). However, speech intelligibility, assessed here via gist ratings (<xref ref-type="bibr" rid="c19">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="c80">Ritz et al., 2022</xref>), only declines for SNRs below 15 dB SNR (consistent with intelligibility scores; <xref ref-type="bibr" rid="c45">Irsik et al., 2022</xref>), whereas the neural tracking enhancement is already present for ∼30 dB SNR. This result questions the link between neural envelope tracking and speech intelligibility, or at least makes the relationship non-linear (cf. <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>). Research on the neural tracking of speech using background noise must thus consider that the noise itself may enhance the tracking.</p>
</sec>
<sec id="s8b">
<title>Potential mechanisms associated with enhanced neural speech tracking</title>
<p>Enhanced neural tracking associated with a stationary background masker or noise-vocoded speech has been interpreted to reflect an attentional gain when listeners must invest cognitively to understand speech (<xref ref-type="bibr" rid="c34">Hauswald et al., 2022</xref>; <xref ref-type="bibr" rid="c101">Yasmin et al., 2023</xref>; <xref ref-type="bibr" rid="c73">Panela et al., 2024</xref>). However, these works used moderate to low SNRs or moderate speech degradations, making it challenging to distinguish between attentional mechanisms and mechanisms driven by background noise per se. The current study demonstrates that attention unlikely causes the enhanced neural tracking of the speech envelope. First, the tracking enhancement was observed for very high SNRs (∼30 dB) at which speech is highly intelligible (<xref ref-type="bibr" rid="c43">Holder et al., 2018</xref>; <xref ref-type="bibr" rid="c85">Spyridakou et al., 2020</xref>; <xref ref-type="bibr" rid="c45">Irsik et al., 2022</xref>). Arguably, little or no effort is needed to understand speech at ∼30 dB SNR, making attentional gain an unlikely explanation. Importantly, neural speech tracking was enhanced even when participants performed a visual task and passively listened to speech (<xref rid="fig2" ref-type="fig">Figure 2</xref>). Participants are unlikely to invest effort to understand speech when performing an attention-demanding visual task. Taken together, the current study provides little evidence that noise-related enhancements of neural speech tracking are due to attention or effort investment.</p>
<p>Another possibility put forward in the context of enhanced neural responses to tone bursts in noise (<xref ref-type="bibr" rid="c3">Alain et al., 2009</xref>; <xref ref-type="bibr" rid="c1">Alain et al., 2012</xref>; <xref ref-type="bibr" rid="c2">Alain et al., 2014</xref>) is that background noise increases arousal, which, in turn, amplifies the neural response to sound. However, a few pieces of evidence are inconsistent with this hypothesis. Arousal to minimal background noise habituates quickly (<xref ref-type="bibr" rid="c4">Alvar and Francis, 2024</xref>) and arousal does not appear to affect early sensory responses but rather later, non-sensory responses in EEG (&gt;150 ms; <xref ref-type="bibr" rid="c33">Han et al., 2013</xref>). Moreover, pupil dilation – a measure of arousal (<xref ref-type="bibr" rid="c58">Mathôt, 2018</xref>; <xref ref-type="bibr" rid="c48">Joshi and Gold, 2020</xref>; <xref ref-type="bibr" rid="c15">Burlingham et al., 2022</xref>) – is similar for speech in noise at SNRs ranging from +16 dB to +4 dB SNR (<xref ref-type="bibr" rid="c69">Ohlenforst et al., 2017</xref>; <xref ref-type="bibr" rid="c68">Ohlenforst et al., 2018</xref>), for which neural tracking increases (<xref rid="fig1" ref-type="fig">Figure 1</xref>). Hence, arousal is unlikely to explain the noise-related enhancement in neural speech tracking, but more direct research is needed to clarify this further.</p>
<p>A third potential explanation of enhanced neural tracking is stochastic resonance, reflecting an automatic mechanism in neural circuits (<xref ref-type="bibr" rid="c86">Stein et al., 2005</xref>; <xref ref-type="bibr" rid="c61">McDonnell and Abbott, 2009</xref>; <xref ref-type="bibr" rid="c62">McDonnell and Ward, 2011</xref>). Stochastic resonance has been described as the facilitation of a near-threshold input through background noise. That is, a near-threshold stimulus or neuronal input, that alone may not be sufficient to drive a neuron beyond its firing threshold, can lead to neuronal firing if noise is added, because the noise increases the stimulus or neuronal input for brief periods, causing a response in downstream neurons (<xref ref-type="bibr" rid="c95">Ward et al., 2002</xref>; <xref ref-type="bibr" rid="c64">Moss et al., 2004</xref>). However, the term is now used more broadly to describe any phenomenon where the presence of noise in a nonlinear system improves the quality of the output signal than when noise is absent (<xref ref-type="bibr" rid="c61">McDonnell and Abbott, 2009</xref>). Stochastic resonance has been observed in humans in several domains, such as in tactile, visual, and auditory perception (<xref ref-type="bibr" rid="c50">Kitajo et al., 2003</xref>; <xref ref-type="bibr" rid="c97">Wells et al., 2005</xref>; <xref ref-type="bibr" rid="c89">Tabarelli et al., 2009</xref>, but see <xref ref-type="bibr" rid="c81">Rufener et al., 2020</xref>). In the current study, speech was presented at suprathreshold levels, but stochastic resonance may still play a role at the neuronal level (<xref ref-type="bibr" rid="c87">Stocks, 2000</xref>; <xref ref-type="bibr" rid="c61">McDonnell and Abbott, 2009</xref>). EEG signals reflect the synchronized activity of more than 10,000 neurons (<xref ref-type="bibr" rid="c67">Niedermeyer and da Silva, 2005</xref>). Some neurons may not receive sufficiently strong input to elicit a response when a person listens to clear speech but may be pushed beyond their firing threshold by the additional, acoustically elicited noise in the neural system. Twelve-talker babble was associated with the greatest noise-related enhancement in neural tracking, possibly because the 12-talker babble facilitated neuronal activity in speech-relevant auditory regions, where the other, non-speech noises were less effective.</p>
</sec>
<sec id="s9">
<title>Conclusions</title>
<p>The current study provides a comprehensive account of a generalized increase in the neural tracking of the amplitude-onset envelope of speech due to minimal background noise. The results show that a) neural speech tracking is enhanced for speech masked by background noise at very high SNRs (∼30 dB), b) this enhancement is independent of attention, c) it generalizes across different stationary background maskers, although being strongest for 12-talker babble, and d) it is present for headphone and free-field listening, suggesting the neural-tracking enhancement generalizes to real-life situations. The work paints a clear picture that minimal background noise enhances the neural representation of the speech envelope. The work further highlights the non-linearities of neural speech tracking as a function of background noise, challenging the feasibility of neural speech tracking as a biological marker for speech processing.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Priya Pandey, Tiffany Lao, and Saba Junaid for their help with data collection. The research was supported by the Canada Research Chair program (CRC-2019-00156) and the Natural Sciences and Engineering Research Council of Canada (Discovery Grant: RGPIN-2021-02602).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alain</surname> <given-names>C</given-names></string-name>, <string-name><surname>McDonald</surname> <given-names>K</given-names></string-name>, <string-name><surname>Van Roon</surname> <given-names>P</given-names></string-name></person-group> (<year>2012</year>) <article-title>Effects of age and background noise on processing a mistuned harmonic in an otherwise periodic complex sound</article-title>. <source>Hearing Research</source> <volume>283</volume>:<fpage>126</fpage>–<lpage>135</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alain</surname> <given-names>C</given-names></string-name>, <string-name><surname>Roye</surname> <given-names>A</given-names></string-name>, <string-name><surname>Salloum</surname> <given-names>C</given-names></string-name></person-group> (<year>2014</year>) <article-title>Effects of age-related hearing loss and background noise on neuromagnetic activity from auditory cortex</article-title>. <source>Frontiers in Systems Neuroscience</source> <volume>8</volume><elocation-id>8</elocation-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alain</surname> <given-names>C</given-names></string-name>, <string-name><surname>Quan</surname> <given-names>J</given-names></string-name>, <string-name><surname>McDonald</surname> <given-names>K</given-names></string-name>, <string-name><surname>Van Roon</surname> <given-names>P</given-names></string-name></person-group> (<year>2009</year>) <article-title>Noise-induced increase in human auditory evoked neuromagnetic fields</article-title>. <source>European Journal of Neuroscience</source> <volume>30</volume>:<fpage>132</fpage>–<lpage>142</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alvar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Francis</surname> <given-names>AL</given-names></string-name></person-group> (<year>2024</year>) <article-title>Effects of background noise on autonomic arousal (skin conductance level)</article-title>. <source>JASA Express Letters</source> <volume>4</volume>:<fpage>013601</fpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Auerbach</surname> <given-names>BD</given-names></string-name>, <string-name><surname>Rodrigues</surname> <given-names>PV</given-names></string-name>, <string-name><surname>Salvi</surname> <given-names>RJ</given-names></string-name></person-group> (<year>2014</year>) <article-title>Central gain control in tinnitus and hyperacusis</article-title>. <source>Frontiers in Neurology</source> <volume>5</volume><elocation-id>206</elocation-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name></person-group> (<year>1995</year>) <article-title>An information maximization approach to blind separation and blind deconvolution</article-title>. <source>Neural Computation</source> <volume>7</volume>:<fpage>1129</fpage>–<lpage>1159</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benjamini</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hochberg</surname> <given-names>Y</given-names></string-name></person-group> (<year>1995</year>) <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal Statistical Society Series B</source> <volume>57</volume>:<fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biesmans</surname> <given-names>W</given-names></string-name>, <string-name><surname>Das</surname> <given-names>N</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bertrand</surname> <given-names>A</given-names></string-name></person-group> (<year>2017</year>) <article-title>Auditory-Inspired Speech Envelope Extraction Methods for Improved EEG-Based Auditory Attention Detection in a Cocktail Party Scenario</article-title>. <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source> <volume>25</volume>:<fpage>402</fpage>–<lpage>412</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bilger</surname> <given-names>RC</given-names></string-name></person-group> (<year>1984</year>) <source>Manual for the clinical use of the revised SPIN Test</source>. <publisher-loc>Champaign, IL, USA</publisher-loc>: <publisher-name>The University of Illinois</publisher-name>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bilger</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Nuetzel</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Rabinowitz</surname> <given-names>WM</given-names></string-name>, <string-name><surname>Rzeczkowski</surname> <given-names>C</given-names></string-name></person-group> (<year>1984</year>) <article-title>Standardization of a Test of Speech Perception in Noise</article-title>. <source>Journal of Speech, Language, and Hearing Research</source> <volume>27</volume>:<fpage>32</fpage>–<lpage>48</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brodbeck</surname> <given-names>C</given-names></string-name>, <string-name><surname>Presacco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>S</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2018</year>) <article-title>Over-representation of speech in older adults originates from early response in higher order auditory cortex</article-title>. <source>Acta Acust United Acust</source> <volume>104</volume>:<fpage>774</fpage>–<lpage>777</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Broderick</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2018</year>) <article-title>Electrophysiological Correlates of Semantic Dissimilarity Reflect the Comprehension of Natural, Narrative Speech</article-title>. <source>Current Biology</source> <volume>28</volume>:<fpage>803</fpage>–<lpage>809</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Broderick</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Rofes</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2021</year>) <article-title>Dissociable electrophysiological measures of natural language processing reveal differences in speech comprehension strategy in healthy ageing</article-title>. <source>Scientific Reports</source> <volume>11</volume>:<fpage>4963</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brungart</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>PS</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>BD</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>D</given-names></string-name></person-group> (<year>2006</year>) <article-title>Isolating the energetic component of speech-on-speech masking with ideal time-frequency segregation</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>120</volume>:<fpage>4007</fpage>–<lpage>4018</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burlingham</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Mirbagheri</surname> <given-names>S</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name></person-group> (<year>2022</year>) <article-title>A unified model of the task-evoked pupil response</article-title>. <source>Science Advances</source> <volume>8</volume>:<fpage>eabi9979</fpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Parra</surname> <given-names>LC</given-names></string-name></person-group> (<year>2016</year>) <article-title>Memorable Audiovisual Narratives Synchronize Sensory and Supramodal Neural Responses</article-title>. <source>eNeuro</source> <volume>3</volume>:<fpage>e0203</fpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Bednar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2016</year>) <article-title>The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli</article-title>. <source>Frontiers in human neuroscience</source> <volume>10</volume>:<fpage>604</fpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Zuk</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Nidiffer</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Molholm</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2021</year>) <article-title>Linear Modeling of Neurophysiological Responses to Speech and Other Continuous Stimuli: Methodological Considerations for Applied Research</article-title>. <source>Frontiers in Neuroscience</source> <volume>15</volume>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davis</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name></person-group> (<year>2003</year>) <article-title>Hierarchical Processing in Spoken Language Comprehension</article-title>. <source>The Journal of Neuroscience</source> <volume>23</volume>:<fpage>3423</fpage>–<lpage>3431</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2019</year>) <article-title>Evidence for enhanced neural tracking of the speech envelope underlying age-related speech-in-noise difficulties</article-title>. <source>Journal of Neurophysiology</source> <volume>122</volume>:<fpage>601</fpage>–<lpage>615</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2020</year>) <article-title>Hearing impairment is associated with enhanced neural tracking of the speech envelope</article-title>. <source>Hearing Research</source> <volume>393</volume>:<fpage>107961</fpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deng</surname> <given-names>L</given-names></string-name></person-group> (<year>2012</year>) <article-title>The mnist database of handwritten digit images for machine learning research</article-title>. <source>IEEE Signal Processing Magazine</source> <volume>29</volume>:<fpage>141</fpage>–<lpage>142</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Di Liberto</given-names> <surname>Giovanni M</surname></string-name>, <string-name><surname>O’Sullivan James</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lalor Edmund</surname> <given-names>C</given-names></string-name></person-group> (<year>2015</year>) <article-title>Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing</article-title>. <source>Current Biology</source> <volume>25</volume>:<fpage>2457</fpage>–<lpage>2465</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dial</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Gnanateja</surname> <given-names>GN</given-names></string-name>, <string-name><surname>Tessmer</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Gorno-Tempini</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Chandrasekaran</surname> <given-names>B</given-names></string-name>, <string-name><surname>Henry</surname> <given-names>ML</given-names></string-name></person-group> (<year>2021</year>) <article-title>Cortical Tracking of the Speech Envelope in Logopenic Variant Primary Progressive Aphasia</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>14</volume>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2013</year>) <article-title>Adaptive temporal encoding leads to a background-insensitive cortical representation of speech</article-title>. <source>The Journal of Neuroscience</source> <volume>33</volume>:<fpage>5728</fpage>–<lpage>5735</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name><surname>Chatterjee</surname> <given-names>M</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2014</year>) <article-title>Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure</article-title>. <source>NeuroImage</source> <volume>88</volume>:<fpage>41</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dmochowski</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Sajda</surname> <given-names>P</given-names></string-name>, <string-name><surname>Dias</surname> <given-names>J</given-names></string-name>, <string-name><surname>Parra</surname> <given-names>LC</given-names></string-name></person-group> (<year>2012</year>) <article-title>Correlated components of ongoing EEG point to emotionally laden attention – a possible marker of engagement?</article-title> <source>Frontiers in Human Neuroscience</source> <volume>6</volume><elocation-id>112</elocation-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dmochowski</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Bezdek</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Abelson</surname> <given-names>BP</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Schumacher</surname> <given-names>EH</given-names></string-name>, <string-name><surname>Parra</surname> <given-names>LC</given-names></string-name></person-group> (<year>2014</year>) <article-title>Audience preferences are predicted by temporal reliability of neural processing</article-title>. <source>Nature Communications</source> <volume>29</volume>:<fpage>4567</fpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiedler</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wöstmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Herbst</surname> <given-names>SK</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2019</year>) <article-title>Late cortical tracking of ignored speech facilitates neural selectivity in acoustically challenging conditions</article-title>. <source>Neuroimage</source> <volume>186</volume>:<fpage>33</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiedler</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wöstmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Graversen</surname> <given-names>C</given-names></string-name>, <string-name><surname>Brandmeyer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2017</year>) <article-title>Single-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech</article-title>. <source>Journal of Neural Engineering</source> <volume>14</volume>:<fpage>036020</fpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Genovese</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Lazar</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Nichols</surname> <given-names>T</given-names></string-name></person-group> (<year>2002</year>) <article-title>Thresholding of statistical maps in functional neuroimaging using the false discovery rate</article-title>. <source>NeuroImage</source> <volume>15</volume>:<fpage>870</fpage>–<lpage>878</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gillis</surname> <given-names>M</given-names></string-name>, <string-name><surname>Van Canneyt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name></person-group> (<year>2022</year>) <article-title>Neural tracking as a diagnostic tool to assess the auditory pathway</article-title>. <source>Hearing Research</source> <volume>426</volume>:<fpage>108607</fpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname> <given-names>L</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Jin</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Luo</surname> <given-names>Y</given-names></string-name></person-group> (<year>2013</year>) <article-title>Low-Arousal Speech Noise Improves Performance in N-Back Task: An ERP Study</article-title>. <source>PLOS One</source> <volume>8</volume>:<fpage>e76261</fpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hauswald</surname> <given-names>A</given-names></string-name>, <string-name><surname>Keitel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>Y-P</given-names></string-name>, <string-name><surname>Rösch</surname> <given-names>S</given-names></string-name>, <string-name><surname>Weisz</surname> <given-names>N</given-names></string-name></person-group> (<year>2022</year>) <article-title>Degradation levels of continuous speech affect neural speech tracking and alpha power differently</article-title>. <source>European Journal of Neuroscience</source> <volume>55</volume>:<fpage>3288</fpage>–<lpage>3302</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heffernan</surname> <given-names>E</given-names></string-name>, <string-name><surname>Coulson</surname> <given-names>NS</given-names></string-name>, <string-name><surname>Henshaw</surname> <given-names>H</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Ferguson</surname> <given-names>MA</given-names></string-name></person-group> (<year>2016</year>) <article-title>Understanding the psychosocial experiences of adults with mild-moderate hearing loss: An application of Leventhal’s self-regulatory model</article-title>. <source>International Journal of Audiology</source> <volume>55</volume>:<fpage>S3</fpage>–<lpage>S12</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2023</year>) <article-title>The perception of artificial-intelligence (AI) based synthesized speech in younger and older adults</article-title>. <source>International Journal of Speech Technology</source> <volume>26</volume>:<fpage>395</fpage>–<lpage>415</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name></person-group> (<year>2020</year>) <article-title>A Model of Listening Engagement (MoLE)</article-title>. <source>Hearing Research</source> <volume>397</volume>:<fpage>108016</fpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Butler</surname> <given-names>BE</given-names></string-name></person-group> (<year>2021</year>) <article-title>Hearing Loss and Brain Plasticity: The Hyperactivity Phenomenon</article-title>. <source>Brain Structure &amp; Function</source> <volume>226</volume>:<fpage>2019</fpage>–<lpage>2039</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Henry</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2013</year>) <article-title>Frequency-specific adaptation in human auditory cortex depends on the spectral variance in the acoustic stimulation</article-title>. <source>Journal of Neurophysiology</source> <volume>109</volume>:<fpage>2086</fpage>–<lpage>2096</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Maess</surname> <given-names>B</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name></person-group> (<year>2018</year>) <article-title>Aging Affects Adaptation to Sound-Level Statistics in Human Auditory Cortex</article-title>. <source>The Journal of Neuroscience</source> <volume>38</volume>:<fpage>1989</fpage>–<lpage>1999</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hertrich</surname> <given-names>I</given-names></string-name>, <string-name><surname>Dietrich</surname> <given-names>S</given-names></string-name>, <string-name><surname>Trouvain</surname> <given-names>J</given-names></string-name>, <string-name><surname>Moos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ackermann</surname> <given-names>H</given-names></string-name></person-group> (<year>2012</year>) <article-title>Magnetic brain activity phase-locked to the envelope, the syllable onsets, and the fundamental frequency of a perceived speech signal</article-title>. <source>Psychophysiology</source> <volume>49</volume>:<fpage>322</fpage>–<lpage>334</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoerl</surname> <given-names>AE</given-names></string-name>, <string-name><surname>Kennard</surname> <given-names>RW</given-names></string-name></person-group> (<year>1970</year>) <article-title>Ridge Regression: Biased Estimation for Nonorthogonal Problems</article-title>. <source>Technometrics</source> <volume>12</volume>:<fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holder</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Levin</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Gifford</surname> <given-names>RH</given-names></string-name></person-group> (<year>2018</year>) <article-title>Speech Recognition in Noise for Adults With Normal Hearing: Age-Normative Performance for AzBio, BKB-SIN, and QuickSIN</article-title>. <source>Otology &amp; Neurotology</source> <volume>39</volume>:<fpage>e972</fpage>–<lpage>e978</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holm</surname> <given-names>S</given-names></string-name></person-group> (<year>1979</year>) <article-title>A Simple Sequentially Rejective Multiple Test Procedure</article-title>. <source>Scandinavian Journal of Statistics</source> <volume>6</volume>:<fpage>65</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irsik</surname> <given-names>VC</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2022</year>) <article-title>Neural activity during story listening is synchronized across individuals despite acoustic masking</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>34</volume>:<fpage>933</fpage>–<lpage>950</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irsik</surname> <given-names>VC</given-names></string-name>, <string-name><surname>Almanaseer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2021</year>) <article-title>Cortical Responses to the Amplitude Envelopes of Sounds Change with Age</article-title>. <source>The Journal of Neuroscience</source> <volume>41</volume>:<fpage>5045</fpage>–<lpage>5055</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>JASP</collab> () <collab>JASP [Computer software]</collab></person-group><year>2023</year>) . In: <ext-link ext-link-type="uri" xlink:href="https://jasp-stats.org/">https://jasp-stats.org/</ext-link>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joshi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gold</surname> <given-names>JI</given-names></string-name></person-group> (<year>2020</year>) <article-title>Pupil Size as a Window on Neural Substrates of Cognition</article-title>. <source>Trends in Cognitive Sciences</source> <volume>24</volume>:<fpage>466</fpage>–<lpage>480</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kidd G</surname>, <given-names>Jr</given-names></string-name>., <string-name><surname>Mason</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Best</surname> <given-names>V</given-names></string-name>, <string-name><surname>Roverud</surname> <given-names>E</given-names></string-name>, <string-name><surname>Swaminathan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jennings</surname> <given-names>T</given-names></string-name>, <string-name><surname>Clayton</surname> <given-names>K</given-names></string-name>, <string-name><surname>Steven Colburn</surname> <given-names>H</given-names></string-name></person-group> (<year>2019</year>) <article-title>Determining the energetic and informational components of speech-on-speech masking in listeners with sensorineural hearing loss</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>145</volume>:<fpage>440</fpage>–<lpage>457</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kitajo</surname> <given-names>K</given-names></string-name>, <string-name><surname>Nozaki</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Yamamoto</surname> <given-names>Y</given-names></string-name></person-group> (<year>2003</year>) <article-title>Behavioral Stochastic Resonance within the Human Brain</article-title>. <source>Physical Review Letters</source> <volume>90</volume>:<fpage>218103</fpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kitajo</surname> <given-names>K</given-names></string-name>, <string-name><surname>Doesburg</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Yamanaka</surname> <given-names>K</given-names></string-name>, <string-name><surname>Nazaki</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Yamamoto</surname> <given-names>Y</given-names></string-name></person-group> (<year>2007</year>) <article-title>Noise-induced large-scale phase synchronization of human-brain activity associated with behavioural stochastic resonance</article-title>. <source>Europhysics Letters</source> <volume>80</volume>:<fpage>40009</fpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krauss</surname> <given-names>P</given-names></string-name>, <string-name><surname>Tziridis</surname> <given-names>K</given-names></string-name>, <string-name><surname>Metzner</surname> <given-names>C</given-names></string-name>, <string-name><surname>Schilling</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hoppe</surname> <given-names>U</given-names></string-name>, <string-name><surname>Schulze</surname> <given-names>H</given-names></string-name></person-group> (<year>2016</year>) <article-title>Stochastic Resonance Controlled Upregulation of Internal Noise after Hearing Loss as a Putative Cause of Tinnitus-Related Neuronal Hyperactivity</article-title>. <source>Frontiers in Neuroscience</source> <volume>10</volume><elocation-id>597</elocation-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kries</surname> <given-names>J</given-names></string-name>, <string-name><surname>De Clercq</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gillis</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lemmens</surname> <given-names>R</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vandermosten</surname> <given-names>M</given-names></string-name></person-group> (<year>2024</year>) <article-title>Exploring neural tracking of acoustic and linguistic speech representations in individuals with post-stroke aphasia</article-title>. <source>Human Brain Mapping</source> <volume>45</volume>:<fpage>e26676</fpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lesenfants</surname> <given-names>D</given-names></string-name>, <string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Verschueren</surname> <given-names>E</given-names></string-name>, <string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2019</year>) <article-title>Predicting individual speech intelligibility from the cortical tracking of acoustic- and phonetic-level speech representations</article-title>. <source>Hearing Research</source> <volume>380</volume>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname> <given-names>FR</given-names></string-name>, <string-name><surname>Albert</surname> <given-names>M</given-names></string-name></person-group> (<year>2014</year>) <article-title>Hearing loss and dementia – who is listening?</article-title> <source>Aging &amp; Mental Health</source> <volume>18</volume>:<fpage>671</fpage>–<lpage>673</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Makeig</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bell</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Jung</surname> <given-names>T-P</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name></person-group> (<year>1995</year>) <article-title>Independent component analysis of electroencephalographic data</article-title>. In: <conf-name>Advances in Neural Information Processing Systems</conf-name>, pp <fpage>145</fpage>–<lpage>151</lpage>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Mathiesen</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Van Hedger</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Irsik</surname> <given-names>VC</given-names></string-name>, <string-name><surname>Bain</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2023</year>) <article-title>Exploring age differences in absorption and enjoyment during story listening</article-title>. <source>PsyArXiv</source>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathôt</surname> <given-names>S</given-names></string-name></person-group> (<year>2018</year>) <article-title>Pupillometry: Psychology, physiology, and function</article-title>. <source>Journal of Cognition</source> <volume>1</volume>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mattys</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Davis</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Bradlow</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Scott</surname> <given-names>SK</given-names></string-name></person-group> (<year>2012</year>) <article-title>Speech recognition in adverse conditions: A review</article-title>. <source>Language and Cognitive Processes</source> <volume>27</volume>:<fpage>953</fpage>–<lpage>978</lpage>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>McDermott</given-names> <surname>Josh H</surname></string-name>, <string-name><given-names>Simoncelli</given-names> <surname>Eero P</surname></string-name></person-group> (<year>2011</year>) <article-title>Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis</article-title>. <source>Neuron</source> <volume>71</volume>:<fpage>926</fpage>–<lpage>940</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonnell</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>D</given-names></string-name></person-group> (<year>2009</year>) <article-title>What Is Stochastic Resonance? Definitions, Misconceptions, Debates, and Its Relevance to Biology</article-title>. <source>PLOS Computational Biology</source> <volume>5</volume>:<fpage>e1000348</fpage>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonnell</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name></person-group> (<year>2011</year>) <article-title>The benefits of noise in neural systems: bridging theory and experiment</article-title>. <source>Nature Reviews Neuroscience</source> <volume>12</volume>:<fpage>415</fpage>–<lpage>425</lpage>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McZgee</surname> <given-names>VE</given-names></string-name>, <string-name><surname>Carleton</surname> <given-names>WT</given-names></string-name></person-group> (<year>1970</year>) <article-title>Piecewise Regression</article-title>. <source>Journal of the American Statistical Association</source> <volume>65</volume>:<fpage>1109</fpage>–<lpage>1124</lpage>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moss</surname> <given-names>F</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Sannita</surname> <given-names>WG</given-names></string-name></person-group> (<year>2004</year>) <article-title>Stochastic resonance and sensory information processing: a tutorial and review of application</article-title>. <source>Clinical Neurophysiology</source> <volume>115</volume>:<fpage>267</fpage>–<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Näätänen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Picton</surname> <given-names>TW</given-names></string-name></person-group> (<year>1987</year>) <article-title>The N1 wave of the human electric and magnetic response to sound: a review and an analysis of the component structure</article-title>. <source>Psychophysiology</source> <volume>24</volume>:<fpage>375</fpage>–<lpage>425</lpage>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nachtegaal</surname> <given-names>J</given-names></string-name>, <string-name><surname>Smit</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Smits</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bezemer</surname> <given-names>PD</given-names></string-name>, <string-name><surname>van Beek</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Festen</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>SE</given-names></string-name></person-group> (<year>2009</year>) <article-title>The association between hearing status and psychosocial health before the age of 70 years: results from an internet-based national survey on hearing</article-title>. <source>Ear &amp; Hearing</source> <volume>30</volume>:<fpage>302</fpage>–<lpage>312</lpage>.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niedermeyer</surname> <given-names>E</given-names></string-name>, <string-name><surname>da Silva</surname> <given-names>FHL</given-names></string-name></person-group> (<year>2005</year>) <article-title>Electroencephalography: Basic Principles, Clinical Applications, and Related Fields</article-title>: <source>Lippincott Williams &amp; Wilkins</source>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ohlenforst</surname> <given-names>B</given-names></string-name>, <string-name><surname>Wendt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Naylor</surname> <given-names>G</given-names></string-name>, <string-name><surname>Zekveld</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name></person-group> (<year>2018</year>) <article-title>Impact of SNR, masker type and noise reduction processing on sentence recognition performance and listening effort as indicated by the pupil dilation response</article-title>. <source>Hearing Research</source>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ohlenforst</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zekveld</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wendt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Naylor</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Versfeld</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>SE</given-names></string-name></person-group> (<year>2017</year>) <article-title>Impact of stimulus-related factors and hearing impairment on listening effort as indicated by pupil dilation</article-title>. <source>Hearing Research</source> <volume>351</volume>:<fpage>68</fpage>–<lpage>79</lpage>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fries</surname> <given-names>P</given-names></string-name>, <string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name></person-group> (<year>2011</year>) <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Computational Intelligence and Neuroscience</source> <volume>2011</volume> <elocation-id>156869</elocation-id>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><collab>OpenAI</collab>, <etal>et al.</etal></person-group> (<year>2023</year>) <article-title>GPT-4 Technical Report</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Palana</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tager-Flusberg</surname> <given-names>H</given-names></string-name></person-group> (<year>2022</year>) <article-title>Evaluating the use of cortical entrainment to measure atypical speech processing: A systematic review</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source> <volume>133</volume>:<fpage>104506</fpage>.</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panela</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Copelli</surname> <given-names>F</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2024</year>) <article-title>Reliability and generalizability of neural speech tracking in younger and older adults</article-title>. <source>Neurobiology of Aging</source> <volume>134</volume>:<fpage>165</fpage>–<lpage>180</lpage>.</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panza</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lozupone</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sardone</surname> <given-names>R</given-names></string-name>, <string-name><surname>Battista</surname> <given-names>P</given-names></string-name>, <string-name><surname>Piccininni</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dibello</surname> <given-names>V</given-names></string-name>, <string-name><surname>La Montagna</surname> <given-names>M</given-names></string-name>, <string-name><surname>Stallone</surname> <given-names>R</given-names></string-name>, <string-name><surname>Venezia</surname> <given-names>P</given-names></string-name>, <string-name><surname>Liguori</surname> <given-names>A</given-names></string-name>, <string-name><surname>Giannelli</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bellomo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Greco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Daniele</surname> <given-names>A</given-names></string-name>, <string-name><surname>Seripa</surname> <given-names>D</given-names></string-name>, <string-name><surname>Nicola</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Logroscino</surname> <given-names>G</given-names></string-name></person-group> (<year>2019</year>) <article-title>Sensorial frailty: age-related hearing loss and the risk of cognitive impairment and dementia in later life</article-title>. <source>Therapeutic Advances in Chronic Disease</source> <volume>10</volume>:<fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parbery-Clark</surname> <given-names>A</given-names></string-name>, <string-name><surname>Marmel</surname> <given-names>F</given-names></string-name>, <string-name><surname>Bair</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kraus</surname> <given-names>N</given-names></string-name></person-group> (<year>2011</year>) <article-title>What subcortical–cortical relationships tell us about processing speech in noise</article-title>. <source>European Journal of Neuroscience</source> <volume>33</volume>:<fpage>549</fpage>–<lpage>557</lpage>.</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pichora-Fuller</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Eckert</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Edwards</surname> <given-names>B</given-names></string-name>, <string-name><surname>Hornsby</surname> <given-names>BWY</given-names></string-name>, <string-name><surname>Humes</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Lemke</surname> <given-names>U</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Matthen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Mackersie</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Naylor</surname> <given-names>G</given-names></string-name>, <string-name><surname>Phillips</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Richter</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rudner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sommers</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Tremblay</surname> <given-names>KL</given-names></string-name>, <string-name><surname>Wingfield</surname> <given-names>A</given-names></string-name></person-group> (<year>2016</year>) <article-title>Hearing Impairment and Cognitive Energy: The Framework for Understanding Effortful Listening (FUEL)</article-title>. <source>Ear &amp; Hearing</source> <volume>37</volume> <issue>Suppl 1</issue>:<fpage>5S</fpage>–<lpage>27S</lpage>.</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Picton</surname> <given-names>TW</given-names></string-name>, <string-name><surname>John</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Dimitrijevic</surname> <given-names>A</given-names></string-name>, <string-name><surname>Purcell</surname> <given-names>DW</given-names></string-name></person-group> (<year>2003</year>) <article-title>Human auditory steady-state responses</article-title>. <source>International Journal of Audiology</source> <volume>42</volume>:<fpage>177</fpage>–<lpage>219</lpage>.</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Presacco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>S</given-names></string-name></person-group> (<year>2016</year>) <article-title>Evidence of degraded representation of speech in noise, in the aging midbrain and cortex</article-title>. <source>Journal of Neurophysiology</source> <volume>116</volume>:<fpage>2346</fpage>–<lpage>2355</lpage>.</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Presacco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>S</given-names></string-name></person-group> (<year>2019</year>) <article-title>Speech-in-noise representation in the aging midbrain and cortex: Effects of hearing loss</article-title>. <source>PLoS ONE</source> <volume>14</volume>:<fpage>e0213899</fpage>.</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ritz</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wild</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name></person-group> (<year>2022</year>) <article-title>Parametric cognitive load reveals hidden costs in the neural processing of perfectly intelligible degraded speech</article-title>. <source>The Journal of Neuroscience</source> <volume>42</volume>:<fpage>4619</fpage>–<lpage>4628</lpage>.</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rufener</surname> <given-names>KS</given-names></string-name>, <string-name><surname>Kauk</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ruhnau</surname> <given-names>P</given-names></string-name>, <string-name><surname>Repplinger</surname> <given-names>S</given-names></string-name>, <string-name><surname>Heil</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zaehle</surname> <given-names>T</given-names></string-name></person-group> (<year>2020</year>) <article-title>Inconsistent effects of stochastic resonance on human auditory processing</article-title>. <source>Scientific Reports</source> <volume>10</volume>:<fpage>6419</fpage>.</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruhnau</surname> <given-names>P</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Schröger</surname> <given-names>E</given-names></string-name></person-group> (<year>2012</year>) <article-title>Finding the right control: The mismatch negativity under investigation</article-title>. <source>Clinical Neurophysiology</source> <volume>123</volume>:<fpage>507</fpage>–<lpage>512</lpage>.</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmitt</surname> <given-names>R</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Giroud</surname> <given-names>N</given-names></string-name></person-group> (<year>2022</year>) <article-title>Better speech-in-noise comprehension is associated with enhanced neural speech tracking in older adults with hearing impairment</article-title>. <source>Cortex</source> <volume>151</volume>:<fpage>133</fpage>–<lpage>146</lpage>.</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shukla</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bidelman</surname> <given-names>GM</given-names></string-name></person-group> (<year>2021</year>) <article-title>Enhanced brainstem phase-locking in low-level noise reveals stochastic resonance in the frequency-following response (FFR)</article-title>. <source>Brain Research</source> <volume>1771</volume>:<fpage>147643</fpage>.</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spyridakou</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rosen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Dritsakis</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bamiou</surname> <given-names>D-E</given-names></string-name></person-group> (<year>2020</year>) <article-title>Adult normative data for the speech in babble (SiB) test</article-title>. <source>International Journal of Audiology</source> <volume>59</volume>:<fpage>33</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stein</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Gossen</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>KE</given-names></string-name></person-group> (<year>2005</year>) <article-title>Neuronal variability: noise or part of the signal?</article-title> <source>Nature Reviews Neuroscience</source> <volume>6</volume>:<fpage>389</fpage>–<lpage>397</lpage>.</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stocks</surname> <given-names>NG</given-names></string-name></person-group> (<year>2000</year>) <article-title>Suprathreshold Stochastic Resonance in Multilevel Threshold Systems</article-title>. <source>Physical Review Letters</source> <volume>84</volume>:<fpage>2310</fpage>–<lpage>2313</lpage>.</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Synigal</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2023</year>) <article-title>Electrophysiological indices of hierarchical speech processing differentially reflect the comprehension of speech in noise</article-title>. <source>BioRxiv</source>.</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tabarelli</surname> <given-names>D</given-names></string-name>, <string-name><surname>Vilardi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Begliomini</surname> <given-names>C</given-names></string-name>, <string-name><surname>Pavani</surname> <given-names>F</given-names></string-name>, <string-name><surname>Turatto</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ricci</surname> <given-names>L</given-names></string-name></person-group> (<year>2009</year>) <article-title>Statistically robust evidence of stochastic resonance in human auditory perceptual system</article-title>. <source>The European Physical Journal B</source> <volume>69</volume>:<fpage>155</fpage>–<lpage>159</lpage>.</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Toms</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Lesperance</surname> <given-names>ML</given-names></string-name></person-group> (<year>2003</year>) <article-title>Piecewise regression: A tool for identifying ecological thresholds</article-title>. <source>Ecology</source> <volume>84</volume>:<fpage>2034</fpage>–<lpage>2041</lpage>.</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tune</surname> <given-names>S</given-names></string-name>, <string-name><surname>Alavash</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fiedler</surname> <given-names>L</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2021</year>) <article-title>Neural attentional-filter mechanisms of listening success in middle-aged and older individuals</article-title>. <source>Nature Communications</source> <volume>12</volume>:<fpage>4533</fpage>.</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Hirtum</surname> <given-names>T</given-names></string-name>, <string-name><surname>Somers</surname> <given-names>B</given-names></string-name>, <string-name><surname>Dieudonné</surname> <given-names>B</given-names></string-name>, <string-name><surname>Verschueren</surname> <given-names>E</given-names></string-name>, <string-name><surname>Wouters</surname> <given-names>J</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2023</year>) <article-title>Neural envelope tracking predicts speech intelligibility and hearing aid benefit in children with hearing loss</article-title>. <source>Hearing Research</source> <volume>439</volume>:<fpage>108893</fpage>.</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wouters</surname> <given-names>J</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2018</year>) <article-title>Speech Intelligibility Predicted from Neural Entrainment of the Speech Envelope</article-title>. <source>Journal of the Association for Research in Otolaryngology</source> <volume>19</volume>:<fpage>181</fpage>–<lpage>191</lpage>.</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vieth</surname> <given-names>E</given-names></string-name></person-group> (<year>1989</year>) <article-title>Fitting piecewise linear regression functions to biological responses</article-title>. <source>Journal of Applied Physiology</source> <volume>67</volume>:<fpage>390</fpage>–<lpage>396</lpage>.</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Neiman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Moss</surname> <given-names>F</given-names></string-name></person-group> (<year>2002</year>) <article-title>Stochastic resonance in psychophysics and in animal behavior</article-title>. <source>Biological Cybernetics</source> <volume>87</volume>:<fpage>91</fpage>–<lpage>101</lpage>.</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>MacLean</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Kirschner</surname> <given-names>A</given-names></string-name></person-group> (<year>2010</year>) <article-title>Stochastic Resonance Modulates Neural Synchronization within and between Cortical Sources</article-title>. <source>PLoS ONE</source> <volume>5</volume>:<fpage>e14371</fpage>.</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wells</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Chua</surname> <given-names>R</given-names></string-name>, <string-name><surname>Inglis</surname> <given-names>JT</given-names></string-name></person-group> (<year>2005</year>) <article-title>Touch Noise Increases Vibrotactile Sensitivity in Old and Young</article-title>. <source>Psychological Science</source> <volume>16</volume>:<fpage>313</fpage>–<lpage>320</lpage>.</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname> <given-names>RH</given-names></string-name></person-group> (<year>2003</year>) <article-title>Development of a speech-in-multitalker-babble paradigm to assess word-recognition performance</article-title>. <source>Journal of the American Academy of Audiology</source> <volume>14</volume>:<fpage>453</fpage>–<lpage>470</lpage>.</mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Trivette</surname> <given-names>CP</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Watts</surname> <given-names>KL</given-names></string-name></person-group> (<year>2012a</year>) <article-title>The Effects of Energetic and Informational Masking on the Words-in-Noise Test (WIN)</article-title>. <source>J Am Acad Audiol</source> <volume>23</volume>:<fpage>522</fpage>–<lpage>533</lpage>.</mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname> <given-names>RH</given-names></string-name>, <string-name><surname>McArdle</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Watts</surname> <given-names>KL</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SL</given-names></string-name></person-group> (<year>2012b</year>) <article-title>The Revised Speech Perception in Noise Test (R-SPIN)in a Multiple Signal-to-Noise Ratio Paradigm</article-title>. <source>Journal of the American Academy of Audiology</source> <volume>23</volume>:<fpage>590</fpage>–<lpage>605</lpage>.</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yasmin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Irsik</surname> <given-names>VC</given-names></string-name>, <string-name><surname>Johnsrude</surname> <given-names>IS</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>B</given-names></string-name></person-group> (<year>2023</year>) <article-title>The effects of speech masking on neural tracking of acoustic and semantic features of natural speech</article-title>. <source>Neuropsychologia</source> <volume>186</volume>:<fpage>108584</fpage>.</mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname> <given-names>F-G</given-names></string-name></person-group> (<year>2013</year>) <article-title>An active loudness model suggesting tinnitus as increased central noise and hyperacusis as increased nonlinear gain</article-title>. <source>Hearing Research</source> <volume>295</volume>:<fpage>172</fpage>–<lpage>179</lpage>.</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname> <given-names>F-G</given-names></string-name></person-group> (<year>2020</year>) <article-title>Tinnitus and hyperacusis: central noise, gain and variance</article-title>. <source>Current Opinion in Physiology</source> <volume>18</volume>:<fpage>123</fpage>–<lpage>129</lpage>.</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zuk</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Reilly</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2021</year>) <article-title>Envelope reconstruction of speech and music highlights stronger tracking of speech at low frequencies</article-title>. <source>PLOS Computational Biology</source> <volume>17</volume>:<fpage>e1009358</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100830.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents an <bold>important</bold> contribution to the understanding of neural speech tracking, demonstrating how minimal background noise can enhance the neural tracking of the amplitude-onset envelope. The evidence supporting the claims of the author is <bold>solid</bold>, through a well-designed series of EEG experiments. This work will be of interest to auditory scientists, particularly those investigating biological markers of speech processing.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100830.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper presents a comprehensive study of how neural tracking of speech is affected by background noise. Using five EEG experiments and Temporal response function (TRF), it investigates how minimal background noise can enhance speech tracking even when speech intelligibility remains very high. The results suggest that this enhancement is not attention-driven but could be explained by stochastic resonance. These findings generalize across different background noise types and listening conditions, offering insights into speech processing in real-world environments.</p>
<p>I find this paper well-written, the experiments and results are clearly described. However, I have a few comments that may be useful to address.</p>
<p>(1) The behavioral accuracy and EEG results for clear speech in Experiment 4 differ from those of Experiments 1-3. Could the author provide insights into the potential reasons for this discrepancy? Might it be due to linguistic/ acoustic differences between the passages used in experiments? If so, what was the rationale behind using different passages across different experiments?</p>
<p>(2) Regarding peak amplitude extraction, why were the exact peak amplitudes and latencies of the TRFs for each subject not extracted, and instead, an amplitude average within a 20 ms time window based on the group-averaged TRFs used? Did the latencies significantly differ across different SNR conditions?</p>
<p>(3) How is neural tracking quantified in the current study? Does improved neural tracking correlate with EEG prediction accuracy or individual peak amplitudes? Given the differing trends between N1 and P2 peaks in babble and speech-matched noise in experiment 3, how is it that babble results in greater envelope tracking compared to speech-matched noise?</p>
<p>(4) The paper discusses how speech envelope-onset tracking varies with different background noises. Does the author expect similar trends for speech envelope tracking as well? Additionally, could you explain why envelope onsets were prioritized over envelope tracking in this analysis?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100830.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The author investigates the role of background noise on EEG-assessed speech tracking in a series of five experiments. In the first experiment, the influence of different degrees of background noise is investigated and enhanced speech tracking for minimal noise levels is found. The following four experiments explore different potential influences on this effect, such as attentional allocation, different noise types, and presentation mode.</p>
<p>The step-wise exploration of potential contributors to the effect of enhanced speech tracking for minimal background noise is compelling. The motivation and reasoning for the different studies are clear and logical and therefore easy to follow. The results are discussed in a concise and clear way. While I specifically like the conciseness, one inevitable consequence is that not all results are equally discussed in depth.</p>
<p>Based on the results of the five experiments, the author concludes that the enhancement of speech tracking for minimal background noise is likely due to stochastic resonance. Given broad conceptualizations of stochastic resonance as a noise benefit this is a reasonable conclusion.</p>
<p>This study will likely impact the field as it provides compelling support questioning the relationship between speech tracking and speech processing.</p>
</body>
</sub-article>
</article>