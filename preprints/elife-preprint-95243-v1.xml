<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95243</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95243</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95243.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Embedding stochastic dynamics of the environment in spontaneous activity by prediction-based plasticity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0951-5791</contrib-id>
<name>
<surname>Asabuki</surname>
<given-names>Toshitake</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4507-8648</contrib-id>
<name>
<surname>Clopath</surname>
<given-names>Claudia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Bioengineering, Imperial College London</institution>, London, <country>UK</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gjorgjieva</surname>
<given-names>Julijana</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Technical University of Munich</institution>
</institution-wrap>
<city>Freising</city>
<country>Germany</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>To whom correspondence should be addressed. E-mail: <email>c.clopath@imperial.ac.uk</email> (C.C.); <email>t.asabuki@imperial.ac.uk</email> (T.A.)</corresp>
<fn fn-type="con"><p><bold>Author Contributions:</bold> T.A. and C.C. conceived the study and wrote the paper. T.A. performed the simulations and data analyses.</p></fn>
<fn fn-type="others"><p><bold>Competing Interest Statement:</bold> The authors declare no competing interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-04-12">
<day>12</day>
<month>04</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95243</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-17">
<day>17</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-05-01">
<day>01</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.05.01.538909"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Asabuki &amp; Clopath</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Asabuki &amp; Clopath</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95243-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>The brain learns an internal model of the environment through sensory experiences, which is essential for high-level cognitive processes. Recent studies show that spontaneous activity reflects such learned internal model. Although computational studies have proposed that Hebbian plasticity can learn the switching dynamics of replayed activities, it is still challenging to learn dynamic spontaneous activity that obeys the statistical properties of sensory experience. Here, we propose a pair of biologically plausible plasticity rules for excitatory and inhibitory synapses in a recurrent spiking neural network model to embed stochastic dynamics in spontaneous activity. The proposed synaptic plasticity rule for excitatory synapses seeks to minimize the discrepancy between stimulus-evoked and internally predicted activity, while inhibitory plasticity maintains the excitatory-inhibitory balance. We show that the spontaneous reactivation of cell assemblies follows the transition statistics of the model’s evoked dynamics. We also demonstrate that simulations of our model can replicate recent experimental results of spontaneous activity in songbirds, suggesting that the proposed plasticity rule might underlie the mechanism by which animals learn internal models of the environment.</p></abstract>
<abstract abstract-type="teaser">
<title>Significance Statement</title>
<p>While spontaneous activity in the brain is often seen as simple background noise, recent work has hypothesized that spontaneous activity instead reflects the brain’s learnt internal model. While several studies have proposed synaptic plasticity rules to generate structured spontaneous activities, the mechanism of learning and embedding transition statistics in spontaneous activity is still unclear. Using a computational model, we investigate the synaptic plasticity rules that learn dynamic spontaneous activity obeying appropriate transition statistics. Our results shed light on the learning mechanism of the brain’s internal model, which is a crucial step towards a better understanding of the role of spontaneous activity as an internal generative model of stochastic processes in complex environments.</p></abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>synaptic plasticity</kwd>
<kwd>recurrent spiking neural network</kwd>
<kwd>internal model</kwd>
<kwd>spontaneous activity</kwd>
<kwd>transition statistics</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The brain is thought to use its sensory experience to learn an appropriate internal model of the environment, which can improve perception and behavioral performance. (<xref ref-type="bibr" rid="c35">Merfeld et al., 1999</xref>; <xref ref-type="bibr" rid="c31">Lewald and Ehrenstein, 1998</xref>; <xref ref-type="bibr" rid="c5">Bell et al., 1997</xref>; <xref ref-type="bibr" rid="c51">Yasui and Young, 1975</xref>; <xref ref-type="bibr" rid="c49">Wolpert et al., 1995</xref>). Such learning is thought to be fundamental to higher-order cognitive processes such as perception, decision making, and prediction of sensory stimuli. Recent computational and experimental evidence suggests that the brain’s learned internal model may be reflected in spontaneous activity. For example, in the visual cortex of awake ferrets, spontaneous activity shows spatial similarity to activity elicited by natural scenes (<xref ref-type="bibr" rid="c6">Berkes et al., 2011</xref>). Furthermore, hippocampus generates sequential replay of place fields during rest and sleep (<xref ref-type="bibr" rid="c48">Wilson and McNaughton, 1994</xref>; <xref ref-type="bibr" rid="c42">Skaggs and McNaughton, 1996</xref>; <xref ref-type="bibr" rid="c29">Lee and Wilson, 2002</xref>). Such hippocampal replay occurs in a highly stereotyped temporal order, with the same sequence of replayed activities often observed across multiple events (<xref ref-type="bibr" rid="c12">Davidson et al., 2009</xref>; <xref ref-type="bibr" rid="c13">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="c21">Gupta et al., 2010</xref>; <xref ref-type="bibr" rid="c50">Wu and Foster, 2014</xref>).</p>
<p>Several computational studies have proposed variants of Hebbian plasticity rules for learning deterministic or even stochastic switching dynamics of replayed activities (<xref ref-type="bibr" rid="c30">Levy et al., 2001</xref>; <xref ref-type="bibr" rid="c32">Litwin-Kumar and Doiron, 2014</xref>; <xref ref-type="bibr" rid="c45">Triplett et al., 2018</xref>; <xref ref-type="bibr" rid="c18">Ocker and Doiron, 2019</xref>; <xref ref-type="bibr" rid="c44">Asabuki and Fukai, 2023</xref>). However, it has been challenging to extend these results to generate dynamic spontaneous activity obeying appropriate transition probabilities learned through sensory experience. Finding a plasticity rule which is capable of learning structured transitions in spontaneous activity could be instrumental for understanding the mechanism underlying cognitive processes in the brain.</p>
<p>In this paper, we propose a local biologically-plausible plasticity rule for learning the statistical transitions between assemblies in spontaneous activity. We use a recurrent spiking neural network model consisting of distinct excitatory and inhibitory populations. The proposed synaptic plasticity rule for excitatory synapses seeks to minimize the discrepancy between stimulus-evoked and internally predicted activity, while inhibitory plasticity maintains the excitatory-inhibitory balance. We explore the potential performance of our model by learning the Markovian transition statistics of evoked network states. Our results show that the trained model exhibits spontaneous stochastic transitions of cell assemblies, even after structured external inputs are removed. We show that the transition statistics of spontaneous activity show a striking similarity to those of the evoked dynamics.</p>
<p>To further validate our model, we compare the model behavior with recent experimental results in songbirds (<xref ref-type="bibr" rid="c8">Bouchard and Brainard, 2016</xref>), which show that the uncertainty of upcoming states in a bird song modulates the degree of neural predictability. Our model replicates this experimental result, suggesting that the connectivity structure learned via the proposed plasticity mechanism could plausibly underly the songbird’s learned internal model.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Spontaneous replay of learnt stochastic sequences</title>
<p>While most studies have investigated plasticity mechanisms for learning random switching (<xref ref-type="bibr" rid="c32">Litwin-Kumar and Doiron, 2014</xref>; <xref ref-type="bibr" rid="c45">Triplett et al., 2018</xref>; <xref ref-type="bibr" rid="c18">Ocker and Doiron, 2019</xref>; <xref ref-type="bibr" rid="c44">Asabuki and Fukai, 2023</xref>) or deterministic transitions (<xref ref-type="bibr" rid="c10">Chadwick et al., 2015</xref>) between cell assemblies, our objective is to create a network model that spontaneously generates stochastic sequences of assemblies following synaptic plasticity. To that end, we first design a simple task whereby stimuli undergo stochastic transitions over time, and presentation of each stimulus increases excitatory drive to neurons targeted by that pattern (<xref rid="fig1" ref-type="fig">Fig.1a</xref>, top). We assume that a non-overlapping subset of excitatory network neurons receive its preferred stimulus (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). After learning, the network should replay stochastic sequences of assemblies with transitions that are statistically consistent with evoked dynamics, without relying on external stimuli (<xref rid="fig1" ref-type="fig">Fig.1a</xref>, bottom).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Task to be learned (a, top) An example of a task used to test the model. Stimulus patterns evolve in time according to structured transition probabilities. The presentation of each stimulus pattern activates the corresponding group of neurons. Recurrent connections are learned by synaptic plasticity (a, bottom). The learned network should replay assemblies spontaneously, where the transition statistics are consistent with the evoked stimuli. (b) A network model with distinct excitatory and inhibitory populations. Only excitatory populations are driven by external inputs. Only synapses that project to excitatory neurons are assumed to be plastic. (c) A schematic of the proposed plasticity rules. Excitatory (blue) and inhibitory (orange) synapses projecting to an excitatory neuron (triangle) obey different plasticity rules. For excitatory synapses, errors between internally driven excitation (blue sigmoid) and the output of the cell provide feedback to the synapses and modulate plasticity (blue square). All excitatory connections seek to minimize such errors. For inhibitory synapses, the error between internally driven excitation (blue sigmoid) and inhibition (orange sigmoid) should be minimized to maintain excitatory inhibitory balance (orange square).</p></caption>
<graphic xlink:href="538909v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We examined the possible learning mechanisms of stochastic neural sequences with a recurrent spiking network. Our network model consists of excitatory (E) and inhibitory (I) model neurons (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Only excitatory neurons are driven by external stochastic sequences. Initially, neurons in the network have random recurrent connections.</p>
<p>To learn a network model to obtain transition statistics of evoked dynamics, we proposed different local plasticity mechanisms for excitatory and inhibitory synapses. We assumed that only connections onto excitatory neurons were plastic (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>), while all others (i.e., connections onto inhibitory neurons) were fixed. In the excitatory recurrent connectivity, all synaptic weights were modified to reduce the error between internally generated and stimulus-evoked activities (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, blue square). This plasticity rule is mathematically similar to that proposed in (<xref ref-type="bibr" rid="c38">Pfister et al., 2006</xref>; <xref ref-type="bibr" rid="c46">Urbanczik and Senn, 2014</xref>). Through this process, excitatory synapses that contribute to predicting neural activity will be strengthened, thereby increasing the similarity between spontaneous and evoked activity. Instead of predicting the firing rate of neurons, the inhibitory synapses were modified to predict the recurrent excitatory potential (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, orange square). This inhibitory plasticity is crucial for the network to maintain excitatory-inhibitory balance and generate spontaneous replay of stochastic assembly sequences, as we will see later. All feedforward connections were fixed and receptive fields were preconfigured. Finally, as in previous studies (<xref ref-type="bibr" rid="c44">Asabuki and Fukai, 2023</xref>), parameters of the response function are regulated according to the activity history of individual neurons (Methods). This regulation maintains the appropriate dynamic range of activities irrespective of the strength of external stimuli.</p>
<p>To examine how external stochastic sequences can influence network wiring, we trained a network model driven by stochastic external inputs. These inputs were generated by first-order Markovian chains with three 200ms long states, governed by fixed transition probabilities (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). During training, excitatory synapses were modified much quicker than inhibitory synapses (<xref rid="fig2" ref-type="fig">Fig.2b</xref>). This difference in plasticity timescales follows from the nature of our learning rules: the wiring of excitatory synapses is reorganized by external stimuli, while inhibitory synapses only change to rebalance excitation. As such, excitatory plasticity in our model occurs before inhibitory plasticity, consistent with the experimental results (<xref ref-type="bibr" rid="c11">D’amour JA and Froemke, 2015</xref>). We then asked how plasticity affects the neural dynamics by comparing the spontaneous activities of the network before and after learning. Here, we simulated spontaneous activity by replacing the temporally structured stimulation (i.e., the Markovian chain in <xref rid="fig2" ref-type="fig">Fig.2a</xref>) with constant background input. Further, all synapses were kept fixed during spontaneous activity.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Spontaneous replay of stochastic transition of assemblies. (a) First, we considered a simple stochastic transition between three stimulus patterns. (b) Dynamics of weight change via plasticity. Excitatory synapses (blue) converged quicker than inhibitory synapses (orange). (c) Example spontaneous assembly reactivations (top) and raster plot (bottom) of the learned network are shown. Colors indicate the corresponding stimulus patterns shown in a. (d) Distribution of assembly reactivations. (e, left) The network currents to assembly 1 (green) and assembly 2 (orange) immediately after the reactivation of assembly 3 ceased. Both currents were similar in magnitude. (e, right) Currents to assembly 2 (orange) and assembly 3 (blue) immediately after the reactivation of assembly 1 ceased. The current to assembly 3 was stronger than that to assembly 2. (f) Relationship between the transition statistics of stimulus patterns and that of replayed assemblies. The spontaneous activity reproduced transition statistics of external stimulus patterns.</p></caption>
<graphic xlink:href="538909v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Before learning, due to uniform initial connectivity, all excitatory neurons showed synchronous and spatially unstructured spontaneous activity (Supplementary Fig.1). However, after learning, three cell assemblies emerged in the network, each of which encoded one external stimulus. Sequences of these cell assemblies were replayed stochastically in spontaneous activity (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>), and durations of given assembly reactivations were biased toward shorter durations but distributed broadly (<xref rid="fig2" ref-type="fig">Fig.2d</xref>).</p>
<p>We next asked whether or not the statistics of assembly switching were influenced by the temporal structure of the external sequence received by the network while it was learning. Since each assembly reactivation was contingent upon previous assembly, statistics of external sequence may influence the strength of synaptic currents via recurrent connectivity. To test this prediction, we first investigated how spontaneous reactivation of assembly 3 drives the subsequent assemblies (i.e., assemblies 1 and 2). Immediately after the reactivation of assembly 3 ceased, currents onto both subsequent assemblies increased gradually, without showing significant difference (<xref rid="fig2" ref-type="fig">Fig.2e</xref>, left). This is due to the fact that state 1 and 2 are structurally symmetrical in our setting (<xref rid="fig2" ref-type="fig">Fig.2a</xref>). We then asked how reactivation of assembly 1 drives the subsequent assemblies (i.e., assemblies 2 and 3). We note that the transition probabilities in the stimulus patterns were biased towards state 3 in this case (<xref rid="fig2" ref-type="fig">Fig.2a</xref>). Consistent with this bias between transition probabilities, we found that assembly 3 was driven much strongly than assembly 2 (<xref rid="fig2" ref-type="fig">Fig.2e</xref>, right). These results suggest that the temporal statistics of the trained external sequence influence the strength of synaptic currents that drive each assembly. We then quantified the similarity between the transition statistics of stimulus patterns and that of the replayed assemblies. We defined the transition probabilities between assemblies by simply counting the occurrence of switching events over all possible pairs of assemblies (Methods). Comparison between transition probabilities of stimulus patterns and that of the reactivated assemblies revealed a clear alignment of temporal statistics (<xref rid="fig2" ref-type="fig">Fig.2f</xref>).</p>
<p>In summary, the plasticity rules in our model learn the transition statistics of evoked patterns while maintaining excitation-inhibition balance. Our results show that the this prediction-based plasticity rule allows the model to learn and spontaneously replays the transition statistics of evoked patterns.</p>
</sec>
<sec id="s2b">
<title>Learned excitatory synapses encode transition statistics</title>
<p>To further understand the mechanism underlying the statistical similarity between the evoked patterns and spontaneous activity, we next asked how the transition statistics of stimulus patterns can influence network wiring. Over the course of training, the average weights of connections in each of the 3 cell assemblies increased gradually and converged to a strong value (<xref rid="fig3" ref-type="fig">Fig.3a</xref> middle and <xref rid="fig3" ref-type="fig">Fig.3b</xref>, top), indicating the formation of assemblies. On the other hand, we found that the average weights between each pair of assemblies decreased settled at different stationary values (<xref rid="fig3" ref-type="fig">Fig.3a</xref>, right and <xref rid="fig3" ref-type="fig">Fig.3b</xref>, bottom). After training, we reasoned that the transition probabilities between states should be encoded exclusively via between-assembly connections, as none of the states in the Markovian chain have self-transitions. To test this prediction, we first compared the average between-assembly connection matrix (<xref rid="fig3" ref-type="fig">Fig.3a</xref>, right) and the ground truth transition aligned well to the ground truth probabilities (<xref rid="fig3" ref-type="fig">Fig.3d</xref>). These results indicate that the network learns the temporal statistics of sequences by modifying the structure of inter-assembly excitatory connections.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Learned excitatory synapses encode transition statistics. (a) A 3 by 3 matrix of excitatory connections, learned with the task in <xref rid="fig2" ref-type="fig">Fig.2a</xref> (left). The matrix can be decomposed to within- (middle) and between-assembly connections (right). (b) Strength of within- (top) and that of betweenassembly excitatory synapses (bottom) during learning are shown. (c) True transition matrix of stimulus patterns. (d) Relationship between the strength of excitatory synapses between assemblies and true transition probabilities between patterns.</p></caption>
<graphic xlink:href="538909v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The above analysis of excitatory weights revealed its crucial role in learning transition probabilities. Next, we examined the role of inhibitory plasticity in our model’s function. To do so, we first simulated the network with fixed inhibitory weights performing the same task shown in <xref rid="fig2" ref-type="fig">Figure 2</xref>. We found that such a model exhibited spontaneous activity with blurred assembly structures compared to the original model (Supplementary Fig.2a). Furthermore, the transition probabilities between replayed assemblies in this case did not show clear alignment with true transition (Supplementary Fig. 2b), though the excitatory weights reached values which did encode transitions (Supplementary Fig. 2c, d). These results suggest that maintenance of EI balance through inhibitory plasticity is necessary for generating structured spontaneous activity, even if excitatory connections learn transition probabilities.</p>
</sec>
<sec id="s2c">
<title>Network can adapt fast to task switching</title>
<p>In the above results, transitions between stimulus patterns obeyed fixed transition probabilities. We then wondered how the network learning would be affected if transition structures of stimulus patterns changed over time. To test such a scenario, we considered a case where the transition matrix in a Markovian chain switches between the first half and the second half of the learning phase (Supplementary Fig. 3a). We will refer these matrices as task1- and task2-matrix, respectively, and examine whether switching of transition matrixes influences the connectivity. During the first half of learning phase, between-assembly connections converged to certain values to encode task1-matrix (Supplementary Fig. 3b, bottom, 0-500 seconds). However, such stable connectivity reorganized quickly once the imposed task was switched to task2-matrix (Supplementary Fig. 3b, bottom, 500-1,000 seconds). Note that in contrast to between-assemblies connections, within-assembly connections did not show such reorganization (Supplementary Fig. 3b, top). These results indicate that our model adapted to the second task even if distinct assembly structures were already formed during the first task.</p>
<p>To further understand how the model adapts to the new task, we next asked how error terms in excitatory and inhibitory plasticity (<xref ref-type="disp-formula" rid="eqn11">Eqs.11</xref> and <xref ref-type="disp-formula" rid="eqn13">13</xref>) change through learning. As expected, the low-pass filtered errors PE<sup>exc</sup> and PE<sup>inh</sup> decreased as the network trained on task1 (Supplementary Fig.3c, 0-500 seconds). However, once the task was switched, errors showed an abrupt increase followed by a gradual decrease as the network learned the second task (Supplementary Fig.3c, 500-1,000 seconds; Supplementary Fig.3d). Consistent with the previous result (<xref rid="fig2" ref-type="fig">Figure 2b</xref>), the peak of inhibitory error occurred delayed after that of excitatory one in each task (<xref ref-type="bibr" rid="c11">D’amour JA and Froemke, 2015</xref>; <xref ref-type="bibr" rid="c47">Vogels et al., 2011</xref>) (Supplementary Fig. 3d). In summary, our model is also capable of task switching, via the reorganization of its weight structures through continuing plasticity.</p>
</sec>
<sec id="s2d">
<title>The network can learn complex stochastic sequences</title>
<p>So far, we have considered the capabilities of our model in regard to the relatively simple class of stochastic dynamics. In particular, the task we considered above contains only three states, and the transition structure was symmetric. In a realistic sequence, like the song of a bird, transition statistics are typically heterogeneous and more structured. To evaluate the model performance over a wide variety of structures, we now consider a transition diagram with more complex structure (<xref rid="fig4" ref-type="fig">Fig.4a</xref>). Despite its complex structure, the learned network showed spontaneous reactivations of all assemblies evoked during learning (<xref rid="fig4" ref-type="fig">Fig.4b</xref>), and the transition dynamics between these assemblies were governed by learned transition probabilities (<xref rid="fig4" ref-type="fig">Fig.4c</xref>). Indeed, the learned weight structures were consistent with the transition probabilities between states as we have seen in simpler task (Supplementary Fig.4).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Learning complex structures. (a) Transition diagram of complex task. (b) Spontaneous activity of learned network. (c) Transition statistics of assemblies reproduce true statistics. (d) Transition diagram of temporal community structure. (e) Raster plot of spontaneous activity of the network trained over structure shown in (d). (f) Structure of learned excitatory synapses encode the community structure. (g) Spontaneous transition between assemblies connected in the diagram shown in d occurs much frequent than disconnected case. (h) Low dimensional representation of evoked activity patterns shows high similarity with community structure. (i) Time courses of replayed activities transitioning within (red) and between (blue) communities. (j) Comparison of mean durations in (i). P-value was calculated by two-sided Welch’s t-test.</p></caption>
<graphic xlink:href="538909v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Recent experimental studies which examined temporal community structure (i.e., highly structured graph structure consisting of clusters of densely interconnected nodes; <xref rid="fig4" ref-type="fig">Fig.4d</xref>) found that human subjects tend to associate a given visual stimulus with other stimuli within the same “community” (<xref ref-type="bibr" rid="c40">Schapiro et al., 2013</xref>; <xref ref-type="bibr" rid="c39">Pudhiyidath et al., 2022</xref>). To investigate whether the model can learn to associate states within a stimulated community, we first trained the network with a stochastic sequence of inputs, generated by a random walk over graph with temporal community structure (<xref rid="fig4" ref-type="fig">Fig.4d</xref>). The learned model showed stochastic assembly transition during spontaneous activity (<xref rid="fig4" ref-type="fig">Fig.4e</xref>) relying on the appropriate weight structure (<xref rid="fig4" ref-type="fig">Fig.4f</xref>). Although transition occurred between all pairs of assemblies, transitions between connected states in the diagram occurred much frequently than transitions between disconnected states (<xref rid="fig4" ref-type="fig">Fig.4g</xref>). This is because plasticity formed strong excitatory connections between assemblies with nonzero transition probabilities, as shown in <xref rid="fig4" ref-type="fig">Figure 4f</xref>.</p>
<p>In human participants, low-dimensional representations of evoked activities in different cortical regions have been reported to show clusters consistent with the structure of communities (<xref ref-type="bibr" rid="c40">Schapiro et al., 2013</xref>). To test whether our model could reproduce such representation of communities, we analyzed the low-dimensional representation of evoked activities in our model by applying principal component analysis (PCA) (see Methods). Such analysis revealed that the representation of stimulus patterns were grouped together into clusters or communities of mutually predictive stimuli, consistent with the experimental results (<xref rid="fig4" ref-type="fig">Fig.4h</xref>). We found that the clustered representations still exist even if the input sequences were scrambled after learning (Supplementary Fig.5), indicating that this result does not rely on the stimulus protocol, but instead on the learned weights.</p>
<p>We further asked whether withinand between-community reactivations showed any differences in terms of their behavior. To this end, we perturbed an assembly corresponding to non-boundary states in the first community (states 2-4 in the transition diagram shown in <xref rid="fig4" ref-type="fig">Fig.4d</xref>) and monitored the behavior of subsequent autonomous network activities. According to the above results, we expect that within-community reactivations should occur quicker than between-community assemblies, due to strong within-community coupling. To test this hypothesis, we calculated the duration from the end of the perturbation until subsequent activity reached a certain threshold (<xref rid="fig4" ref-type="fig">Fig.4i</xref>). As expected, the transition to within-community states showed much shorter durations than to between-community case (<xref rid="fig4" ref-type="fig">Fig.4j</xref>), indicating that between-community transition occurred with much slower time scale compared to within-community case. Together, these results indicate that our network can learn complex temporal structures in spontaneous activity and reproduce the neural representation of the temporal community structure observed in the experiment.</p>
</sec>
<sec id="s2e">
<title>Network dynamics consistent with recorded neural data of songbird</title>
<p>Finally, we tested whether the spontaneous activity in our model resembles recorded neural activity of HVC in Bengalese finch (Bf). Bf learns songs composed of multiple stereotyped short sequences, or syllables. The transitions between these syllables can be described via Markovian process with varying levels of certainty. Intuitively, given one syllable in a bird song, precise prediction about the neural response to the next syllable can be made if the transition from that syllable is highly certain, while imprecise transitions will lead to imprecise predictions about the neural response. Indeed, recent experimental study reported that uncertainty of upcoming syllables in a Bf song modulates the degree of predictability of subsequent neural activation (poststimulus activity; PSA) in HVC (<xref ref-type="bibr" rid="c8">Bouchard and Brainard, 2016</xref>). We sought to test whether our model would exhibit a similar property. To this end, we analyzed the behavior of a network model that had already learned the task (shown in <xref rid="fig4" ref-type="fig">Figure 4a</xref>). The transition structure we chose is relatively simple compared to the real song of a Bf, yet captures measured features of bird songs (i.e., both structures consist of highly certain and less-certain transitions). In the experiment, similarities were calculated between the trial-averaged PSA following a short sequence of stimuli, and the response to an isolated stimulus. To mimic this experimental design, we measured stimulustriggered averages of our autonomous network activity as a proxy for PSA (<xref rid="fig5" ref-type="fig">Fig.5a</xref>). To examine how uncertainty of state transitions in a sequence influence predictive strength in network activity, we first calculated the Pearson correlation coefficient between PSA and responses to next states in a sequence. We will refer to such correlations as “next-state correlations”. Note that if there were multiple next-states from a given state, all correlations corresponding to that state were averaged. We further calculated the correlation between PSA and responses to other states that did not follow the given state (“other-state correlations”). Similar to the next-state correlations, other-state correlations were averaged over all disconnected states from each state. We then compared next-state correlations and other-state correlations between highly certain (<xref rid="fig5" ref-type="fig">Fig.5b, left</xref>) and less-certain (<xref rid="fig5" ref-type="fig">Fig.5b</xref>, right) transitions. Here, highly certain transitions refer to those which have a transition probability greater than 1/2. Other transitions were classified as less-certain transitions. Consistent with experimental results, nextstate correlations were significantly greater than other-state correlations in the highly certain case (<xref rid="fig5" ref-type="fig">Fig.5b, left</xref>). This correlation difference was less significant in less-certain case (<xref rid="fig5" ref-type="fig">Fig.5b</xref>, right). These results indicate that transition uncertainty modulated the degree to which PSA is predictive of upcoming states.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Network dynamics consistent with recorded neural data of songbird (a) Example poststimulus activity (PSA) for low- (left) and high-entropy (right) transition cases. (b) Comparison of correlation coefficients between PSA and evoked single-syllable responses for next syllables and other syllables. For low entropy transition case, the next-syllables correlations were significantly higher than other-syllables correlations (p &lt; 0.01, Wilcoxon signed-rank test) (left). In contrast, such correlation coefficients showed no significant difference for high entropy transition case (p &gt; 0.3, Wilcoxon signed-rank test) (right). Red crosses are mean. (c) The difference in correlation coefficients between next and other syllables (ΔR) was significantly greater for low entropy transitions than for high entropy transitions (p &lt; 0.01, two-sided Welch’s t-test).</p></caption>
<graphic xlink:href="538909v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We performed a more direct comparison of predictive strength by measuring the difference between two types of correlations (i.e., next- and other-state correlations) over multiple levels of transition uncertainty. Here, for each state, next-state correlation was subtracted by other-state correlation. Transition uncertainties were quantified by calculating the conditional entropy of transition probabilities of stimulus patterns. Note that a higher value of entropy indicates less-certain transition, and vice versa. As expected, correlation differences increased as entropy decreased (<xref rid="fig5" ref-type="fig">Fig.5c</xref>), indicating that the predictive strength of network PSA was larger for low-entropy transitions (i.e., highly certain transitions) than for high-entropy transitions (i.e., less-certain transitions). What is the underlying mechanism of such predictability differences? Although each trial of assembly perturbation lead to subsequent reactivation of one of the assemblies, trial-averaged activities (i.e., PSAs) marginalized all possible transitions in the transition diagram (<xref rid="fig5" ref-type="fig">Fig.5a</xref>). Due to this averaging process, similarities between PSA and stimulus-evoked activities increases if conditional entropy is low (i.e., certain transition), and vice versa. Overall, our results suggest that our model learns transition statistics of stimulus patterns, with transition uncertainty influencing predictive strength in the network activity.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Understanding how the brain learns internal models of the environment is a challenging problem in neuroscience. In this study, we proposed synaptic plasticity rules for learning assembly transitions via sensory experiences. Our excitatory plasticity aims at minimizing the error between sensory-evoked and internally generated predictions of upcoming activity. We showed that the network learns the appropriate wiring patterns to encode the transition structure of states, and thus exhibits stochastic transitions between assemblies in spontaneous activity. We further showed that appropriate replay of stochastic transitions requires both excitatory and inhibitory plasticity. These plasticity rules showed a clear division of labor. For excitatory synapses, the connectivity learns transition probabilities during the evoked phase, and inhibitory plasticity seeks to maintain the excitatoryinhibitory balance. We showed that network excitatory plasticity alone cannot account for stochastic replay of learned activity, even if excitatory synapses learn an appropriate structure.</p>
<p>Variants of the Hebbian plasticity rule have been widely used to learn the precise order of sequential reactivations. For example, a rate-based Hebbian rule has been proposed to generate trajectories along a chain of metastable attractors, each corresponding to a reactivation of a single network state (<xref ref-type="bibr" rid="c15">Fonollosa et al., 2015</xref>). Another proposed mechanism is that the transitions are governed by theta oscillations, which form a temporal backbone of the sequential reactivation of assemblies (<xref ref-type="bibr" rid="c10">Chadwick et al., 2015</xref>). Despite the successes of these Hebbian rules in learning precise order in sequences, plasticity rules that learn structured transition probabilities and replay them in spontaneous activity were still unknown.</p>
<p>How does our plasticity mechanism differ from the Hebbian rule? In the Hebbian rule, synaptic strength is potentiated as long as pre- and postsynaptic neurons show correlated activities. Due to this nature of the Hebbian rule, after sufficient potentiation, synapses reach a predefined upper limit, making the strength uniform among strong synapses (<xref ref-type="bibr" rid="c27">Kempter et al., 1999</xref>; <xref ref-type="bibr" rid="c43">Song et al., 2000</xref>; <xref ref-type="bibr" rid="c34">Masquelier et al., 2008</xref>). Such connectivity is useful when the network learns deterministic sequences, but it alone is insufficient to learn transition probabilities. In contrast, our proposed model aims at predicting the evoked activities by internally generated dynamics, so that learning ceases when the prediction error is sufficiently minimized. This mechanism results in learned synaptic distributions that are not uniform as observed in STDP, but rather converge to values proportional to the transition probabilities between assemblies (as shown in <xref rid="fig3" ref-type="fig">Figure 3b</xref>).</p>
<p>The proposed mechanism of learning stochastic transitions between cell assemblies may offer several advantages over deterministic transitions, as suggested by previous studies. One possibility is that the internal dynamics of stochastic transitions can be used as prior knowledge about the structure of the world. In particular, the learned information about the transition statistics can be used to make probabilistic predictions about upcoming sensory events. It may also provide a flexible representation of the environment. In a deterministic case, assemblies are replayed in a fixed temporal order, which may make the network susceptible to noise or unexpected changes in the environment. In contrast, stochastic transitions may allow the network to generate rich repertoires of representations that could provide flexible computation against an uncertain environment.</p>
<p>In reinforcement learning (RL), balancing the tradeoff between exploration and exploitation to maximize a long-term reward signal is one of the most challenging problems. While both exploration and exploitation phases are crucial in RL, exploration is often much more difficult. This difficulty arises from the fact that exploration is especially important when the agent does not have an optimal policy. One way in which the agent might bypass or speed up this exploration phase is through prior knowledge of the environment’s transition statistics. Furthermore, learning transition statistics as an internal model may be beneficial when an agent solves a task in an environment where the reward distribution is sparse. Having an internal model of the transition statistics may allow an agent to predict the expected value of the future reward for taking a particular action in a given state. However, the relationship between the reward-based plasticity rule and our proposed rule still needs further study.</p>
<p>Our model results were also compared to experimental results of sequence predictability in a songbird. Recent experiments have shown that the predictive uncertainty of the upcoming stimulus modulates the degree of similarity between stimulus-evoked and post-stimulus autonomous activity in the HVC of the Bengal finch (<xref ref-type="bibr" rid="c8">Bouchard and Brainard, 2016</xref>). However, the underlying mechanism is still unknown. Here, we have shown that a stochastic state transition in spontaneous activity can explain such a dependence of activity similarity on stimulus uncertainty. Our model predicts that the PSA reflects a trial average of stochastic transitions of evoked activity from a given stimulus. Trial-averaged neural activity washes out the variability of all possible realizations of the stochastic transition. Thus, PSA of an uncertain stimulus results in a combination of multiple transitions, leading to activity less similar than that evoked by a single stimulus. Several studies have shown that Hidden Markov Models or other statistical methods could account for the transition statistics in bird song (<xref ref-type="bibr" rid="c28">Kogan and Margoliash, 1998</xref>; <xref ref-type="bibr" rid="c25">Katahira et al., 2011</xref>). However, our study suggests that trial averaging operations can influence the degree of similarity between stimulus-evoked and post-stimulus activity.</p>
<p>Although we have shown that the proposed model can learn Markovian transitions, several studies suggest that animals often exhibit behaviors with non-Markovian or hierarchical statistics (<xref ref-type="bibr" rid="c41">Seeds et al., 2014</xref>; <xref ref-type="bibr" rid="c7">Berman et al., 2016</xref>; <xref ref-type="bibr" rid="c23">Jovanic et al., 2016</xref>; <xref ref-type="bibr" rid="c22">Jin and Costa, 2015</xref>; <xref ref-type="bibr" rid="c19">Geddes et al., 2018</xref>; <xref ref-type="bibr" rid="c33">Markowitz et al., 2018</xref>; <xref ref-type="bibr" rid="c26">Kato et al., 2015</xref>; <xref ref-type="bibr" rid="c24">Kaplan et al., 2020</xref>). In principle, our learning rule cannot be applied to learning non-Markovian transitions, since it only learns local transitions between states (<xref ref-type="bibr" rid="c9">Brea et al., 2013</xref>). Another limitation of our model is that it cannot learn transition statistics if the states are separated in time. Both of these problems could be solved by considering working memory (WM) (<xref ref-type="bibr" rid="c3">Baddeley, 1992</xref>; <xref ref-type="bibr" rid="c36">Miller and Cohen, 2001</xref>) in an activity-dependent (<xref ref-type="bibr" rid="c16">Funahashi et al., 1989</xref>; <xref ref-type="bibr" rid="c20">Goldman-Rakic, 1995</xref>; <xref ref-type="bibr" rid="c17">Fuster and Alexander, 1971</xref>; <xref ref-type="bibr" rid="c1">Amit and Brunel, 1997</xref>) or activitysilent manner (<xref ref-type="bibr" rid="c37">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="c4">Barak and Tsodyks, 2014</xref>; <xref ref-type="bibr" rid="c52">Zucker and Regehr, 2002</xref>; <xref ref-type="bibr" rid="c14">Erickson et al., 2010</xref>). Clarifying the relationship between the proposed prediction-based plasticity rule and plasticity rules that support memory traces, such as short-term plasticity, will warrant future computational studies.</p>
<p>Our work sheds light on the learning mechanism of the brain’s internal model, which is a crucial step towards a better understanding of the role of spontaneous activity as an internal generative model of stochastic processes in complex environments.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<p>Our recurrent neural networks consist of <italic>N</italic><sub><italic>E</italic></sub> excitatory and <italic>N</italic><sub><italic>I</italic></sub> inhibitory neurons. During learning, the membrane potentials of neuron at time <italic>t</italic> with external current <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> were calculated as follows:
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="538909v1_eqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="538909v1_eqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are the membrane potential of <italic>i</italic>-th excitatory and inhibitory neuron, respectively (see <xref rid="tbl1" ref-type="table">Table 1</xref> for the list of variables and functions). The strength of external input <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> takes the value 1 if stimulus pattern targets neuron <italic>i</italic> was presented and 0 otherwise. This structured external input was replaced to constant inputs <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> of value 0.3 during spontaneous activity. We will describe the details of stimulus patterns later. <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is a recurrent connection weight from <italic>j</italic>-th neuron in population <italic>b</italic> to <italic>i</italic>-th neuron in population <italic>a</italic>. All neurons were connected with a coupling probability of <italic>p</italic> = 0.5. Initial value of synaptic weights <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> were uniformly set to <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> if <italic>a</italic> = <italic>E</italic> and <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline9.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> if <italic>a</italic> = <italic>I</italic>. <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline10.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is a postsynaptic potential evoked by <italic>i</italic>-th neuron in population <italic>a</italic>, which will be described later.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Definition of variables and functions.</p></caption>
<graphic xlink:href="538909v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="538909v1_tbl1a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Spiking of each neuron model in population <italic>E</italic> was modeled as an inhomogeneous Poisson process with instantaneous firing rate <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline11.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> with a dynamic sigmoidal response function <italic>φ</italic> with parameters of slope <italic>β</italic> and threshold <italic>θ</italic> as:
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="538909v1_eqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>φ</italic><sub>0</sub> is the maximum instantaneous firing rate of 50 Hz and <italic>g</italic> = 2. The slope <italic>β</italic> and threshold <italic>θ</italic> of sigmoidal function of population <italic>E</italic> was regulated by the memory trace <italic>h</italic><sub><italic>i</italic></sub> as:
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="538909v1_eqn4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="538909v1_eqn5.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where the values of constant parameters are <italic>β</italic><sub>0</sub> = 5 and <italic>θ</italic><sub>0</sub> =1. The memory trace tracks the maximum value of the short history of membrane potential <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline12.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> as
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="538909v1_eqn6.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>τ</italic><sub><italic>h</italic></sub> = 10 s is a time scale of memory trace. Inhibitory neurons’ firing rate were assumed to be calculated with static sigmoidal function as:
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="538909v1_eqn7.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where the maximum instantaneous firing rate <italic>φ</italic><sub>0</sub> was assumed to be same with that of excitatory neurons (i.e., 50 Hz). The parameters <italic>β</italic><sub>0</sub> and <italic>θ</italic><sub>0</sub> are the constant values already appeared in <xref ref-type="disp-formula" rid="eqn4">Eqs. (4)</xref> and <xref ref-type="disp-formula" rid="eqn5">(5)</xref>.</p>
<p>Neuron <italic>i</italic> in population <italic>a</italic> generates a Poisson spike train at the instantaneous firing rate of <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline13.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Let us describe the generated Poisson spike trains as:
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="538909v1_eqn8.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>δ</italic> is the Dirac’s delta function and <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline14.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the set of time of the spikes of the neuron. The postsynaptic potential evoked by the neuron (i.e., <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline15.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>) was then calculated as:
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="538909v1_eqn9.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="538909v1_eqn10.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>τ</italic><sub><italic>s</italic></sub> = 5ms, τ = 15 ms, and <italic>x</italic><sub>0</sub> = 25.</p>
<sec id="s4a">
<title>The learning rules</title>
<p>All excitatory synaptic connections onto excitatory neurons obeyed the following plasticity rule to predict the activity of postsynaptic neurons as:
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="538909v1_eqn11.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline16.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is a recurrent prediction of a firing rate, defined as:
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="538909v1_eqn12.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where the function <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline17.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the static sigmoid function defined in <xref ref-type="disp-formula" rid="eqn7">Eq.7</xref>. In this study, the learning rate was set to <italic>ϵ</italic> = 10<sup>−4</sup> in all simulations.</p>
<p>The inhibitory synapses onto excitatory neurons were plastic according to the following rule:
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="538909v1_eqn13.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline18.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> was the total inhibitory input onto postsynaptic neuron:
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="538909v1_eqn14.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Through this inhibitory plasticity, inhibitory synapses were modified to maintain excitatory-inhibitory balance in all excitatory neurons.</p>
</sec>
<sec id="s4b">
<title>Simulation details</title>
<p>The parameters used in the simulations are summarized in <xref rid="tbl2" ref-type="table">Table 2</xref>. All simulations were performed in customized Python3 code written by TA with numpy 1.17.3 and scipy 0.18. Differential equations were numerically integrated using a Euler method with integration time steps of 1 ms.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Parameter settings.</p></caption>
<graphic xlink:href="538909v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s4c">
<title>Stimulation protocols</title>
<p>In all simulations, each stimulus patterns had a duration of 200 ms and were presented without inter-pattern interval. We assumed each neuron in a network was stimulated by one of stimulus patterns and targeted assemblies were not overlapped. Presentation of each pattern triggers excitatory current to its targeted neurons of strength 1 and zero otherwise. During spontaneous activity, stimulus patterns were replaced with constant background input <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline19.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for all excitatory neurons. In <xref rid="fig5" ref-type="fig">Figure 5</xref>, we assumed all excitatory neurons receive both structured and constant background inputs over whole period.</p>
</sec>
<sec id="s4d">
<title>Calculation of transition probabilities in spontaneous activity</title>
<p>In <xref rid="fig2" ref-type="fig">Figs. 2f</xref>, <xref rid="fig4" ref-type="fig">4c</xref>, and <xref rid="fig4" ref-type="fig">4g</xref>, we first calculated the population average of the instantaneous firing rates of all neurons in each assembly, during spontaneous activity. We will term such activities as assembly activities. We then defined the assembly reactivations by events that the assembly activities exceeded the threshold of which the value 50% of maximum value of each assembly activities. Transition probabilities between assemblies across all possible pairs were then calculated by counting the occurrences of reactivation of the subsequent assembly within 100 ms of the end time of reactivation of the preceding assembly. In <xref rid="fig2" ref-type="fig">Fig. 2d</xref>, durations of each assembly reactivation event were defined as a period during each assembly activation exceeded threshold.</p>
</sec>
<sec id="s4e">
<title>Calculation of weight changes</title>
<p>In <xref rid="fig2" ref-type="fig">Fig.2b</xref>, the weight changes were calculated every 2 s for excitatory and inhibitory synapses as:
<disp-formula id="eqn15">
<alternatives><graphic xlink:href="538909v1_eqn15.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn16">
<alternatives><graphic xlink:href="538909v1_eqn16.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline20.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is a synapse at time <italic>t</italic> and <italic>dt</italic> is a simulation time step of 1 ms.</p>
</sec>
<sec id="s4f">
<title>Calculation of error dynamics in task switching</title>
<p>In Supplementary Figures 3c and 3d, two types of prediction errors for excitatory and inhibitory plasticity were calculated as follows. First, we obtained the lowpass filtered errors <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline21.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="538909v1_inline22.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> calculated by instantaneous error values in the plasticity rules (i.e., <xref ref-type="disp-formula" rid="eqn11">Eqs. 11</xref> and <xref ref-type="disp-formula" rid="eqn13">13</xref>) as:
<disp-formula id="eqn17">
<alternatives><graphic xlink:href="538909v1_eqn17.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn18">
<alternatives><graphic xlink:href="538909v1_eqn18.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>τ</italic><sub><italic>avg</italic></sub> = 30 s is a time constant for low-pass filter and <italic>i</italic> is a neuron index. We then calculated the averaged errors PE<sup>exc</sup> and PE<sup>inh</sup> as:
<disp-formula id="eqn19">
<alternatives><graphic xlink:href="538909v1_eqn19.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn20">
<alternatives><graphic xlink:href="538909v1_eqn20.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where |•| is an absolute value.</p>
</sec>
<sec id="s4g">
<title>Analysis of the low-dimensional representation in network</title>
<p>In <xref rid="fig4" ref-type="fig">Fig.4h</xref>, we first obtained matrix of network responses <italic>U</italic> = (<bold><italic>r</italic></bold><sub>1</sub>, …, <bold><italic>r</italic></bold><sub>15</sub>), where <bold><italic>r</italic></bold><sub><italic>i</italic></sub> (<italic>i</italic> = 1, …, 15) is a trial-averaged response of a whole network to one of 15 stimulus patterns shown in <xref rid="fig4" ref-type="fig">Fig.4d</xref>. Trial averaging was performed over multiple presentations of each stimulus. We then applied the principal component analysis (PCA) to matrix <italic>U</italic> and visualized the low dimensional representation of multiple stimulus in the learned network.</p>
</sec>
<sec id="s4h">
<title>Correlation measure for comparison with a songbird</title>
<p>In <xref rid="fig5" ref-type="fig">Fig 5</xref>, we calculated stimulus-triggered averages of autonomous network activity to obtain poststimulus activity (PSA) of a network model. In <xref rid="fig5" ref-type="fig">Figs 5b</xref> and <xref rid="fig5" ref-type="fig">5c</xref>, correlation between PSA and evoked activity triggered by one stimulus pattern was calculated neuron-wise and then averaged over all neurons.</p>
</sec>
</sec>
<sec id="d1e1248" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1327">
<label>Supplemental figures</label>
<media xlink:href="supplements/538909_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by BBSRC (BB/N013956/1), Wellcome Trust (200790/Z/16/Z), the Simons Foundation (564408) and EPSRC(EP/R035806/1). The authors also thank Ian Cone for his comments on the manuscript and technical assistance.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Amit</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Brunel</surname> <given-names>N.</given-names></string-name> <article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title>. <source>Cereb Cortex</source>. <year>1997</year> <month>Apr</month>-May;<volume>7</volume>(<issue>3</issue>):<fpage>237</fpage>–<lpage>52</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/7.3.237</pub-id>. PMID: <pub-id pub-id-type="pmid">9143444</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Asabuki</surname> <given-names>T</given-names></string-name>, <string-name><surname>Fukai</surname> <given-names>T.</given-names></string-name> <article-title>Somatodendritic consistency check for temporal feature segmentation</article-title>. <source>Nat Commun</source>. <year>2020</year> <month>Mar</month> 25;<volume>11</volume>(<issue>1</issue>):<fpage>1554</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41467-020-15367-w</pub-id>. PMID: <pub-id pub-id-type="pmid">32214100</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC7096495</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Baddeley</surname> <given-names>A.</given-names></string-name> <article-title>Working memory</article-title>. <source>Science</source>. <year>1992</year> <month>Jan</month> 31;<volume>255</volume>(<issue>5044</issue>):<fpage>556</fpage>–<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1736359</pub-id>. PMID: <pub-id pub-id-type="pmid">1736359</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Barak</surname> <given-names>O</given-names></string-name>, <string-name><surname>Tsodyks</surname> <given-names>M.</given-names></string-name> <article-title>Working models of working memory</article-title>. <source>Curr Opin Neurobiol</source>. <year>2014</year> <month>Apr</month>;<volume>25</volume>:<fpage>20</fpage>–<lpage>4</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conb.2013.10.008</pub-id>. Epub 2013 Dec 4. PMID: <pub-id pub-id-type="pmid">24709596</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Bell</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bodznick</surname> <given-names>D</given-names></string-name>, <string-name><surname>Montgomery</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bastian</surname> <given-names>J.</given-names></string-name> <article-title>The generation and subtraction of sensory expectations within cerebellum-like structures</article-title>. <source>Brain Behav Evol</source>. <year>1997</year>;<volume>50</volume> <issue>Suppl 1</issue>:<fpage>17</fpage>–<lpage>31</lpage>. doi:<pub-id pub-id-type="doi">10.1159/000113352</pub-id>. PMID: <pub-id pub-id-type="pmid">9217991</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Berkes</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Orban</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lengyel</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Fiser</surname>, <given-names>J.</given-names></string-name> <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title>. <source>Science</source> <volume>331</volume>, <fpage>83</fpage>–<lpage>87</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Berman</surname> <given-names>GJ</given-names></string-name>, <string-name><surname>Bialek</surname> <given-names>W</given-names></string-name>, <string-name><surname>Shaevitz</surname> <given-names>JW</given-names></string-name>. <article-title>Predictability and hierarchy in Drosophila behavior</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2016</year>; <volume>113</volume>(<issue>42</issue>):<fpage>11943</fpage>–<lpage>11948</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1607601113</pub-id> PMID: <pub-id pub-id-type="pmid">27702892</pub-id></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Bouchard</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Brainard</surname> <given-names>MS</given-names></string-name>. <article-title>Auditory-induced neural dynamics in sensorymotor circuitry predict learned temporal and sequential statistics of birdsong</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2016</year> <month>Aug</month> 23;<volume>113</volume>(<issue>34</issue>):<fpage>9641</fpage>–<lpage>6</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1606725113</pub-id>. Epub 2016 Aug 9. PMID: <pub-id pub-id-type="pmid">27506786</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC5003256</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Brea</surname> <given-names>J</given-names></string-name>, <string-name><surname>Senn</surname> <given-names>W</given-names></string-name>, <string-name><surname>Pfister</surname> <given-names>JP</given-names></string-name>. <article-title>Matching Recall and Storage in Sequence Learning with Spiking Neural Networks</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>; <volume>33</volume>(<issue>23</issue>):<fpage>9565</fpage>–<lpage>9575</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI</pub-id>. 4098-12.2013 PMID: <pub-id pub-id-type="pmid">23739954</pub-id></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Chadwick</surname> <given-names>A</given-names></string-name>, <string-name><surname>van Rossum</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Nolan</surname> <given-names>MF</given-names></string-name>. <article-title>Independent theta phase coding accounts for CA1 population sequences and enables flexible remapping</article-title>. <source>Elife</source>. <year>2015</year> <month>Feb</month> 2;<volume>4</volume>:<fpage>e03542</fpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.03542</pub-id>. PMID: <pub-id pub-id-type="pmid">25643396</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC4383210</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>D’amour</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Froemke</surname> <given-names>RC</given-names></string-name>. <article-title>Inhibitory and excitatory spike-timing-dependent plasticity in the auditory cortex</article-title>. <source>Neuron</source>. <year>2015</year> <month>Apr</month> 22;<volume>86</volume>(<issue>2</issue>):<fpage>514</fpage>–<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.014</pub-id>. Epub 2015 Apr 2. PMID: <pub-id pub-id-type="pmid">25843405</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC4409545</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Davidson</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Kloosterman</surname> <given-names>F</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>MA</given-names></string-name>. <article-title>Hippocampal replay of extended experience</article-title>. <source>Neuron</source>. <year>2009</year> <month>Aug</month> 27;<volume>63</volume>(<issue>4</issue>):<fpage>497</fpage>–<lpage>507</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.027</pub-id>. PMID: <pub-id pub-id-type="pmid">19709631</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC4364032</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Diba</surname> <given-names>K</given-names></string-name>, <string-name><surname>Buzsáki</surname> <given-names>G.</given-names></string-name> <article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title>. <source>Nat Neurosci</source>. <year>2007</year> <month>Oct</month>;<volume>10</volume>(<issue>10</issue>):<fpage>1241</fpage>–<lpage>2</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1961</pub-id>. Epub 2007 Sep 2. PMID: <pub-id pub-id-type="pmid">17828259</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC2039924</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Erickson</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Maramara</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Lisman</surname> <given-names>J.</given-names></string-name> <article-title>A single brief burst induces GluR1-dependent associative short-term potentiation: a potential mechanism for short-term memory</article-title>. <source>J Cogn Neurosci</source>. <year>2010</year> <month>Nov</month>;<volume>22</volume>(<issue>11</issue>):<fpage>2530</fpage>–<lpage>40</lpage>. doi:<pub-id pub-id-type="doi">10.1162/jocn.2009.21375</pub-id>. PMID: <pub-id pub-id-type="pmid">19925206</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC3195522</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Fonollosa</surname> <given-names>J</given-names></string-name>, <string-name><surname>Neftci</surname> <given-names>E</given-names></string-name>, <string-name><surname>Rabinovich</surname> <given-names>M.</given-names></string-name> <article-title>Learning of Chunking Sequences in Cognition and Behavior</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year> <month>Nov</month> 19;<volume>11</volume>(<issue>11</issue>):<fpage>e1004592</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1004592</pub-id>. PMID: <pub-id pub-id-type="pmid">26584306</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC4652905</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Funahashi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bruce</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>. <article-title>Mnemonic coding of visual space in the monkey’s dorsolateral prefrontal cortex</article-title>. <source>J Neurophysiol</source>. <year>1989</year> <month>Feb</month>;<volume>61</volume>(<issue>2</issue>):<fpage>331</fpage>–<lpage>49</lpage>. doi:<pub-id pub-id-type="doi">10.1152/jn.1989.61.2.331</pub-id>. PMID: <pub-id pub-id-type="pmid">2918358</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Fuster</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Alexander</surname> <given-names>GE</given-names></string-name>. <article-title>Neuron activity related to short-term memory</article-title>. <source>Science</source>. <year>1971</year> <month>Aug</month> 13;<volume>173</volume>(<issue>3997</issue>):<fpage>652</fpage>–<lpage>4</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.173.3997.652</pub-id>. PMID: <pub-id pub-id-type="pmid">4998337</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>Gabriel Koch</given-names> <surname>Ocker</surname></string-name>, <string-name><given-names>Brent</given-names> <surname>Doiron</surname></string-name>, <article-title>Training and Spontaneous Reinforcement of Neuronal Assemblies by Spike Timing Plasticity</article-title>, <source>Cerebral Cortex</source>, Volume <volume>29</volume>, Issue <issue>3</issue>, <month>March</month> <year>2019</year>, Pages <fpage>937</fpage>–<lpage>951</lpage>, <pub-id pub-id-type="doi">10.1093/cercor/bhy001</pub-id></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Geddes</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Li</surname> <given-names>H</given-names></string-name>, <string-name><surname>Jin</surname> <given-names>X.</given-names></string-name> <article-title>Optogenetic Editing Reveals the Hierarchical Organization of Learned Action Sequences</article-title>. <source>Cell</source>. <year>2018</year>; <volume>174</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>43</lpage>.e15. <pub-id pub-id-type="doi">10.1016/j.cell.2018.06.012</pub-id> PMID: <pub-id pub-id-type="pmid">29958111</pub-id></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>. <article-title>Cellular basis of working memory</article-title>. <source>Neuron</source>. <year>1995</year> <month>Mar</month>;<volume>14</volume>(<issue>3</issue>):<fpage>477</fpage>–<lpage>85</lpage>. doi:<pub-id pub-id-type="doi">10.1016/0896-6273(95)90304-6</pub-id>. PMID: <pub-id pub-id-type="pmid">7695894</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Gupta</surname> <given-names>AS</given-names></string-name>, <string-name><surname>van der Meer</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Touretzky</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Redish</surname> <given-names>AD</given-names></string-name>. <article-title>Hippocampal replay is not a simple function of experience</article-title>. <source>Neuron</source>. <year>2010</year> <month>Mar</month> 11;<volume>65</volume>(<issue>5</issue>):<fpage>695</fpage>–<lpage>705</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.034</pub-id>. PMID: <pub-id pub-id-type="pmid">20223204</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC4460981</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Jin</surname> <given-names>X</given-names></string-name>, <string-name><surname>Costa</surname> <given-names>RM</given-names></string-name>. <article-title>Shaping action sequences in basal ganglia circuits</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2015</year>; <volume>33</volume>:<fpage>188</fpage>–<lpage>196</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2015.06.011</pub-id> PMID: <pub-id pub-id-type="pmid">26189204</pub-id></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Jovanic</surname> <given-names>T</given-names></string-name>, <string-name><surname>Schneider-Mizell</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Shao</surname> <given-names>M</given-names></string-name>, <string-name><surname>Masson</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Denisov</surname> <given-names>G</given-names></string-name>, <string-name><surname>Fetter</surname> <given-names>RD</given-names></string-name>, <etal>et al.</etal> <article-title>Competitive Disinhibition Mediates Behavioral Choice and Sequences in Drosophila</article-title>. <source>Cell</source>. <year>2016</year>; <volume>167</volume>(<issue>3</issue>):<fpage>858</fpage>–<lpage>870</lpage>.e19. <pub-id pub-id-type="doi">10.1016/j.cell.2016.09.009</pub-id> PMID: <pub-id pub-id-type="pmid">27720450</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Kaplan</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Salazar Thula</surname> <given-names>O</given-names></string-name>, <string-name><surname>Khoss</surname> <given-names>N</given-names></string-name>, <string-name><surname>Zimmer</surname> <given-names>M.</given-names></string-name> <article-title>Nested Neuronal Dynamics Orchestrate a Behavioral Hierarchy across Timescales</article-title>. <source>Neuron</source>. <year>2020</year>; <volume>105</volume>(<issue>3</issue>):<fpage>562</fpage>–<lpage>576</lpage>.e9. <pub-id pub-id-type="doi">10.1016/j.neuron</pub-id>. 2019.10.037 PMID: <pub-id pub-id-type="pmid">31786012</pub-id></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Katahira</surname> <given-names>K</given-names></string-name>, <string-name><surname>Suzuki</surname> <given-names>K</given-names></string-name>, <string-name><surname>Okanoya</surname> <given-names>K</given-names></string-name>, <string-name><surname>Okada</surname> <given-names>M.</given-names></string-name> <article-title>Complex sequencing rules of birdsong can be explained by simple hidden Markov processes</article-title>. <source>PLoS One</source>. <year>2011</year>;<volume>6</volume>(<issue>9</issue>):<fpage>e24516</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0024516</pub-id>. Epub 2011 Sep 7. PMID: <pub-id pub-id-type="pmid">21915345</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC3168521</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Kato</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Schrodel</surname> <given-names>T</given-names></string-name>, <string-name><surname>Skora</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lindsay</surname> <given-names>TH</given-names></string-name>, <string-name><surname>Yemini</surname> <given-names>E</given-names></string-name>, <etal>et al.</etal> <article-title>Global Brain Dynamics Embed the Motor Command Sequence of Caenorhabditis elegans</article-title>. <source>Cell</source>. <year>2015</year>; <volume>163</volume>(<issue>3</issue>):<fpage>656</fpage>–<lpage>669</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2015.09.034</pub-id> PMID: <pub-id pub-id-type="pmid">26478179</pub-id></mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Kempter</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gerstner</surname> <given-names>W</given-names></string-name>, <string-name><surname>van Hemmen</surname> <given-names>JL</given-names></string-name> (<year>1999</year>) <article-title>Hebbian learning and spiking neurons</article-title>. <source>Physical Review E</source> <volume>59</volume>: <fpage>4498</fpage>–<lpage>4514</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Kogan</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Margoliash</surname> <given-names>D.</given-names></string-name> <article-title>Automated recognition of bird song elements from continuous recordings using dynamic time warping and hidden Markov models: a comparative study</article-title>. <source>J Acoust Soc Am</source>. <year>1998</year> <month>Apr</month>;<volume>103</volume>(<issue>4</issue>):<fpage>2185</fpage>–<lpage>96</lpage>. doi:<pub-id pub-id-type="doi">10.1121/1.421364</pub-id>. PMID: <pub-id pub-id-type="pmid">9566338</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Lee</surname> <given-names>A.K.</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>M.A.</given-names></string-name> <article-title>Memory of sequential experience in the hippocampus during slow wave sleep</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>:<fpage>1183</fpage>–<lpage>1194</lpage>. [PubMed] [Google Scholar]</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Levy</surname> <given-names>N</given-names></string-name>, <string-name><surname>Horn</surname> <given-names>D</given-names></string-name>, <string-name><surname>Meilijson</surname> <given-names>I</given-names></string-name>, <string-name><surname>Ruppin</surname> <given-names>E.</given-names></string-name> <article-title>Distributed synchrony in a cell assembly of spiking neurons</article-title>. <source>Neural Netw</source>. <year>2001</year> <month>Jul</month>-Sep;<volume>14</volume>(<issue>6-7</issue>):<fpage>815</fpage>–<lpage>24</lpage>. doi:<pub-id pub-id-type="doi">10.1016/s0893-6080(01)00044-2</pub-id>. PMID: <pub-id pub-id-type="pmid">11665773</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Lewald</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ehrenstein</surname> <given-names>WH</given-names></string-name>. <article-title>Influence of head-to-trunk position on sound lateralization</article-title>. <source>Exp Brain Res</source>. <year>1998</year> <month>Aug</month>;<volume>121</volume>(<issue>3</issue>):<fpage>230</fpage>–<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s002210050456</pub-id>. PMID: <pub-id pub-id-type="pmid">9746129</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Litwin-Kumar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Doiron</surname>, <given-names>B.</given-names></string-name> <article-title>Formation and maintenance of neuronal assemblies through synaptic plasticity</article-title>. <source>Nat Commun</source> <volume>5</volume>, <fpage>5319</fpage> (<year>2014</year>). <pub-id pub-id-type="doi">10.1038/ncomms6319</pub-id></mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Markowitz</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Gillis</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Beron</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Neufeld</surname> <given-names>SQ</given-names></string-name>, <string-name><surname>Robertson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bhagat</surname> <given-names>ND</given-names></string-name>, <etal>et al.</etal> <article-title>The Striatum Organizes 3D Behavior via Moment-to-Moment Action Selection</article-title>. <source>Cell</source>. <year>2018</year>; <volume>174</volume>(<issue>1</issue>):<fpage>44</fpage>–<lpage>58</lpage>.e17. <pub-id pub-id-type="doi">10.1016/j.cell.2018.04.019</pub-id> PMID: <pub-id pub-id-type="pmid">29779950</pub-id></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Masquelier</surname> <given-names>T</given-names></string-name>, <string-name><surname>Guyonneau</surname> <given-names>R</given-names></string-name>, <string-name><surname>Thorpe</surname> <given-names>SJ</given-names></string-name> (<year>2008</year>) <article-title>Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains</article-title>. <source>PLoS One</source> <volume>3</volume>: <fpage>e1377</fpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Merfeld</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Zupan</surname> <given-names>L</given-names></string-name>, <string-name><surname>Peterka</surname> <given-names>RJ</given-names></string-name>. <article-title>Humans use internal models to estimate gravity and linear acceleration</article-title>. <source>Nature</source>. <year>1999</year> <month>Apr</month> 15;<volume>398</volume>(<issue>6728</issue>):<fpage>615</fpage>–<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1038/19303</pub-id>. PMID: <pub-id pub-id-type="pmid">10217143</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>JD</given-names></string-name>. <article-title>An integrative theory of prefrontal cortex function</article-title>. <source>Annu Rev Neurosci</source>. <year>2001</year>;<volume>24</volume>:<fpage>167</fpage>–<lpage>202</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id>. PMID: <pub-id pub-id-type="pmid">11283309</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Mongillo</surname> <given-names>G</given-names></string-name>, <string-name><surname>Barak</surname> <given-names>O</given-names></string-name>, <string-name><surname>Tsodyks</surname> <given-names>M.</given-names></string-name> <article-title>Synaptic theory of working memory</article-title>. <source>Science</source>. <year>2008</year> <month>Mar</month> 14;<volume>319</volume>(<issue>5869</issue>):<fpage>1543</fpage>–<lpage>6</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1150769</pub-id>. PMID: <pub-id pub-id-type="pmid">18339943</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Pfister</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Toyoizumi</surname> <given-names>T</given-names></string-name>, <string-name><surname>Barber</surname> <given-names>D</given-names></string-name>, <string-name><surname>Gerstner</surname> <given-names>W.</given-names></string-name> <article-title>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning</article-title>. <source>Neural Comput</source>. <year>2006</year> <month>Jun</month>;<volume>18</volume>(<issue>6</issue>):<fpage>1318</fpage>–<lpage>48</lpage>. doi:<pub-id pub-id-type="doi">10.1162/neco.2006.18.6.1318</pub-id>. PMID: <pub-id pub-id-type="pmid">16764506</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Pudhiyidath</surname> <given-names>A</given-names></string-name>, <string-name><surname>Morton</surname> <given-names>NW</given-names></string-name>, <string-name><surname>Viveros Duran</surname> <given-names>R</given-names></string-name>, <string-name><surname>Schapiro</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Momennejad</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hinojosa-Rowland</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Molitor</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Preston</surname> <given-names>AR</given-names></string-name>. <article-title>Representations of Temporal Community Structure in Hippocampus and Precuneus Predict Inductive Reasoning Decisions</article-title>. <source>J Cogn Neurosci</source>. <year>2022</year> <month>Sep</month> 1;<volume>34</volume>(<issue>10</issue>):<fpage>1736</fpage>–<lpage>1760</lpage>. doi:<pub-id pub-id-type="doi">10.1162/jocn_a_01864</pub-id>. PMID: <pub-id pub-id-type="pmid">35579986</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Schapiro</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name>, <string-name><surname>Cordova</surname> <given-names>NI</given-names></string-name>, <string-name><surname>Turk-Browne</surname> <given-names>NB</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>. <article-title>Neural representations of events arise from temporal community structure</article-title>. <source>Nat Neurosci</source>. <year>2013</year> <month>Apr</month>;<volume>16</volume>(<issue>4</issue>):<fpage>486</fpage>–<lpage>92</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3331</pub-id>. Epub 2013 Feb 17. PMID: <pub-id pub-id-type="pmid">23416451</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC3749823</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Seeds</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Ravbar</surname> <given-names>P</given-names></string-name>, <string-name><surname>Chung</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hampel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Midgley</surname> <given-names>FM</given-names></string-name>, <string-name><surname>Mensh</surname> <given-names>BD</given-names></string-name>, <etal>et al.</etal> <article-title>A suppression hierarchy among competing motor programs drives sequential grooming in Drosophila</article-title>. <source>eLife</source>. <year>2014</year>; <volume>3</volume>:<fpage>e02951</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.02951</pub-id> PMID: <pub-id pub-id-type="pmid">25139955</pub-id></mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Skaggs</surname> <given-names>W.E.</given-names></string-name>, <string-name><surname>McNaughton</surname> <given-names>B.L.</given-names></string-name> <article-title>Replay of neuronal firing sequences in rat hippocampus during sleep following spatial experience</article-title>. <source>Science</source>. <year>1996</year>;<volume>271</volume>:<fpage>1870</fpage>–<lpage>1873</lpage>. [PubMed] [Google Scholar]</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Song</surname> <given-names>S</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>KD</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>LF</given-names></string-name> (<year>2000</year>) <article-title>Competitive Hebbian learning through spike-timing dependent synaptic plasticity</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>919</fpage>–<lpage>926</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="web"><string-name><given-names>Toshitake</given-names> <surname>Asabuki</surname></string-name> and <string-name><given-names>Tomoki</given-names> <surname>Fukai</surname></string-name>. <article-title>Learning rules for cortical-like spontaneous replay of an internal model</article-title>. <source>bioRxiv</source> <year>2023</year>.02.17.528958; doi: <pub-id pub-id-type="doi">10.1101/2023.02.17.528958</pub-id></mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Triplett</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Avitan</surname> <given-names>L</given-names></string-name>, <string-name><surname>Goodhill</surname> <given-names>GJ</given-names></string-name> (<year>2018</year>) <article-title>Emergence of spontaneous assembly activity in developing neural networks without afferent input</article-title>. <source>PLOS Computational Biology</source> <volume>14</volume>(<issue>9</issue>): <fpage>e1006421</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006421</pub-id></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Urbanczik</surname> <given-names>R</given-names></string-name>, <string-name><surname>Senn</surname> <given-names>W.</given-names></string-name> <article-title>Learning by the dendritic prediction of somatic spiking</article-title>. <source>Neuron</source>. <year>2014</year> <month>Feb</month> 5;<volume>81</volume>(<issue>3</issue>):<fpage>521</fpage>–<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.030</pub-id>. PMID: <pub-id pub-id-type="pmid">24507189</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Vogels</surname> <given-names>TP</given-names></string-name>, <string-name><surname>Sprekeler</surname> <given-names>H</given-names></string-name>, <string-name><surname>Zenke</surname> <given-names>F</given-names></string-name>, <string-name><surname>Clopath</surname> <given-names>C</given-names></string-name>, <string-name><surname>Gerstner</surname> <given-names>W.</given-names></string-name> <article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title>. <source>Science</source>. <year>2011</year> <month>Dec</month> 16;<volume>334</volume>(<issue>6062</issue>):<fpage>1569</fpage>–<lpage>73</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1211095</pub-id>. Epub 2011 Nov 10. Erratum in: Science. 2012 May 18;336(6083):802. PMID: <pub-id pub-id-type="pmid">22075724</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname> <given-names>M.A.</given-names></string-name>, <string-name><surname>McNaughton</surname> <given-names>B.L.</given-names></string-name> <article-title>Reactivation of hippocampal ensemble memories during sleep</article-title>. <source>Science</source>. <year>1994</year>;<volume>265</volume>:<fpage>676</fpage>–<lpage>679</lpage>. [PubMed] [Google Scholar]</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Ghahramani</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Jordan</surname> <given-names>MI</given-names></string-name>. <article-title>An internal model for sensorimotor integration</article-title>. <source>Science</source>. <year>1995</year> <month>Sep</month> 29;<volume>269</volume>(<issue>5232</issue>):<fpage>1880</fpage>–<lpage>2</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.7569931</pub-id>. PMID: <pub-id pub-id-type="pmid">7569931</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname> <given-names>X</given-names></string-name>, <string-name><surname>Foster</surname> <given-names>DJ</given-names></string-name>. <article-title>Hippocampal replay captures the unique topological structure of a novel environment</article-title>. <source>J Neurosci</source>. <year>2014</year> <month>May</month> 7;<volume>34</volume>(<issue>19</issue>):<fpage>6459</fpage>–<lpage>69</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3414-13.2014</pub-id>. PMID: <pub-id pub-id-type="pmid">24806672</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC4012305</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Yasui</surname> <given-names>S</given-names></string-name>, <string-name><surname>Young</surname> <given-names>LR</given-names></string-name>. <article-title>Perceived visual motion as effective stimulus to pursuit eye movement system</article-title>. <source>Science</source>. <year>1975</year> <month>Nov</month> 28;<volume>190</volume>(<issue>4217</issue>):<fpage>906</fpage>–<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1188373</pub-id>. PMID: <pub-id pub-id-type="pmid">1188373</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Zucker</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Regehr</surname> <given-names>WG</given-names></string-name>. <article-title>Short-term synaptic plasticity</article-title>. <source>Annu Rev Physiol</source>. <year>2002</year>;<volume>64</volume>:<fpage>355</fpage>–<lpage>405</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev.physiol.64.092501.114547</pub-id>. PMID: <pub-id pub-id-type="pmid">11826273</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95243.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gjorgjieva</surname>
<given-names>Julijana</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Technical University of Munich</institution>
</institution-wrap>
<city>Freising</city>
<country>Germany</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This is an <bold>important</bold> study that investigates how neural networks can learn to stochastically replay presented sequences of activity according to learned transition probabilities. The authors use error-based excitatory plasticity to minimize the difference between internally predicted activity and stimulus-driven activity, and inhibitory plasticity to maintain E-I balance. The approach is <bold>solid</bold> but the choice of learning rules and parameters is not always always justified, lacking a formal derivation and concrete experimental predictions.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95243.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In the presented manuscript, the authors investigate how neural networks can learn to replay presented sequences of activity. Their focus lies on the stochastic replay according to learned transition probabilities. They show that based on error-based excitatory and balance-based inhibitory plasticity networks can self-organize towards this goal. Finally, they demonstrate that these learning rules can recover experimental observations from song-bird song learning experiments.</p>
<p>Overall, the study appears well-executed and coherent, and the presentation is very clear and helpful. However, it remains somewhat vague regarding the novelty. The authors could elaborate on the experimental and theoretical impact of the study, and also discuss how their results relate to those of Kappel et al, and others (e.g., Kappel et al (doi.org/10.1371/journal.pcbi.1003511)). Overall, the work could benefit if there was either (A) a formal analysis or derivation of the plasticity rules involved and a formal justification of the usefulness of the resulting (learned) neural dynamics; and/or (B) a clear connection of the employed plasticity rules to biological plasticity and clear testable experimental predictions. Thus, overall, this is a good work with some room for improvement.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95243.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work proposes a synaptic plasticity rule that explains the generation of learned stochastic dynamics during spontaneous activity. The proposed plasticity rule assumes that excitatory synapses seek to minimize the difference between the internal predicted activity and stimulus-evoked activity, and inhibitory synapses try to maintain the E-I balance by matching the excitatory activity. By implementing this plasticity rule in a spiking recurrent neural network, the authors show that the state-transition statistics of spontaneous excitatory activity agree with that of the learned stimulus patterns, which are reflected in the learned excitatory synaptic weights. The authors further demonstrate that inhibitory connections contribute to well-defined state transitions matching the transition patterns evoked by the stimulus. Finally, they show that this mechanism can be expanded to more complex state-transition structures including songbird neural data.</p>
<p>Strengths:</p>
<p>This study makes an important contribution to computational neuroscience, by proposing a possible synaptic plasticity mechanism underlying spontaneous generations of learned stochastic state-switching dynamics that are experimentally observed in the visual cortex and hippocampus. This work is also very clearly presented and well-written, and the authors conducted comprehensive simulations testing multiple hypotheses. Overall, I believe this is a well-conducted study providing interesting and novel aspects of the capacity of recurrent spiking neural networks with local synaptic plasticity.</p>
<p>Weaknesses:</p>
<p>This study is very well-thought-out and theoretically valuable to the neuroscience community, and I think the main weaknesses are in regard to how much biological realism is taken into account. For example, the proposed model assumes that only synapses targeting excitatory neurons are plastic, and uses an equal number of excitatory and inhibitory neurons.</p>
<p>The model also assumes Markovian state dynamics while biological systems can depend more on history. This limitation, however, is acknowledged in the Discussion.</p>
<p>
Finally, to simulate spontaneous activity, the authors use a constant input of 0.3 throughout the study. Different amplitudes of constant input may correspond to different internal states, so it will be more convincing if the authors test the model with varying amplitudes of constant inputs.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95243.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Asabuki and Clopath study stochastic sequence learning in recurrent networks of Poisson spiking neurons that obey Dale's law. Inspired by previous modeling studies, they introduce two distinct learning rules, to adapt excitatory-to-excitatory and inhibitory-to-excitatory synaptic connections. Through a series of computer experiments, the authors demonstrate that their networks can learn to generate stochastic sequential patterns, where states correspond to non-overlapping sets of neurons (cell assemblies) and the state-transition conditional probabilities are first-order Markov, i.e., the transition to a given next state only depends on the current state. Finally, the authors use their model to reproduce certain experimental songbird data involving highly-predictable and highly-uncertain transitions between song syllables.</p>
<p>Strengths:</p>
<p>This is an easy-to-follow, well-written paper, whose results are likely easy to reproduce. The experiments are clear and well-explained. The study of songbird experimental data is a good feature of this paper; finches are classical model animals for understanding sequence learning in the brain. I also liked the study of rapid task-switching, it's a good-to-know type of result that is not very common in sequence learning papers.</p>
<p>Weaknesses:</p>
<p>While the general subject of this paper is very interesting, I missed a clear main result. The paper focuses on a simple family of sequence learning problems that are well-understood, namely first-order Markov sequences and fully visible (no-hidden-neuron) networks, studied extensively in prior work, including with spiking neurons. Thus, because the main results can be roughly summarized as examples of success, it is not entirely clear what the main point of the authors is.</p>
<p>Going into more detail, the first major weakness I see in this paper is the heuristic choice of learning rules. The paper studies Poisson spiking neurons (I return to this point below), for which learning rules can be derived from a statistical objective, typically maximum likelihood. For fully-visible networks, these rules take a simple form, similar in many ways to the E-to-E rule introduced by the authors. This more principled route provides quite a lot of additional understanding on what is to be expected from the learning process. For instance, should maximum likelihood learning succeed, it is not surprising that the statistics of the training sequence distribution are reproduced. Moreover, given that the networks are fully visible, I think that the maximum likelihood objective is a convex function of the weights, which then gives hope that the learning rule does succeed. And so on. This sort of learning rule has been studied in a series of papers by David Barber and colleagues [refs. 1, 2 below], who applied them to essentially the same problem of reproducing sequence statistics in recurrent fully-visible nets. It seems to me that one key difference is that the authors consider separate E and I populations, and find the need to introduce a balancing I-to-E learning rule.</p>
<p>Because the rules here are heuristic, a number of questions come to mind. Why these rules and not others - especially, as the authors do not discuss in detail how they could be implemented through biophysical mechanisms? When does learning succeed or fail? What is the main point being conveyed, and what is the contribution on top of the work of e.g. Barber, Brea, et al. (2013), or Pfister et al. (2004)?</p>
<p>The use of a Poisson spiking neuron model is the second major weakness of the study. A chief challenge in much of the cited work is to generate stochastic transitions from recurrent networks of deterministic neurons. The task the authors set out to do is much easier with stochastic neurons; it is reasonable that the network succeeds in reproducing Markovian sequences, given an appropriate learning rule. I believe that the main point comes from mapping abstract Markov states to assemblies of neurons. If I am right, I missed more analyses on this point, for instance on the impact that varying cell assembly size would have on the findings reported by the authors.</p>
<p>Finally, it was not entirely clear to me what the main fundamental point in the HVC data section was. Can the findings be roughly explained as follows: if we map syllables to cell assemblies, for high-uncertainty syllable-to-syllable transitions, it becomes harder to predict future neural activity? In other words, is the main point that the HVC encodes syllables by cell assemblies?</p>
<p>(1) Learning in Spiking Neural Assemblies, David Barber, 2002. URL: <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2002/file/619205da514e83f869515c782a328d3c-Paper.pdf">https://proceedings.neurips.cc/paper/2002/file/619205da514e83f869515c782a328d3c-Paper.pdf</ext-link></p>
<p>(2) Correlated sequence learning in a network of spiking neurons usingmaximum likelihood, David Barber, Felix Agakov, 2002. URL: <ext-link ext-link-type="uri" xlink:href="http://web4.cs.ucl.ac.uk/staff/D.Barber/publications/barber-agakov-TR0149.pdf">http://web4.cs.ucl.ac.uk/staff/D.Barber/publications/barber-agakov-TR0149.pdf</ext-link></p>
</body>
</sub-article>
</article>