<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108795</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108795</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108795.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Sensitivity of the human temporal voice areas to nonhuman primate vocalizations</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0638-3981</contrib-id>
<name>
<surname>Ceravolo</surname>
<given-names>Leonardo</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<email>leonardo.ceravolo@unige.ch</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Debracque</surname>
<given-names>Coralie</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gruber</surname>
<given-names>Thibaud</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n2">‡</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Grandjean</surname>
<given-names>Didier</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n2">‡</xref>
</contrib>
    <aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>University of Geneva</institution></institution-wrap>, <city>Geneva</city>, <country country="CH">Switzerland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="others" id="n1"><label>*</label><p>shared first authors</p>
</fn><fn fn-type="others" id="n2"><label>‡</label><p>shared last authors</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-11-25">
<day>25</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108795</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-09-19">
<day>19</day>
<month>09</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-09-22">
<day>22</day>
<month>09</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.09.19.677258"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Ceravolo et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Ceravolo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108795-v1.pdf"/>
<abstract><p>In recent years, research on voice processing in the human brain—particularly the study of temporal voice areas (TVA)—was dedicated almost exclusively to conspecific vocalizations. To characterize commonalities and differences regarding primate vocalization representations in the human brain, the inclusion of closely related nonhuman primates—namely chimpanzees and bonobos—is needed. We hypothesized that neural commonalities would depend on both phylogenetic and acoustic proximities, with chimpanzees ranking closest to <italic>Homo</italic>. Presenting human participants (N=23) with the vocalizations of four primate species (rhesus macaques, chimpanzees, bonobos and humans) and regressing-out relevant acoustic parameters using three distinct analyses, we observed within-TVA, sample-specific, bilateral anterior superior temporal gyrus activity for chimpanzee vocalizations compared to: all other species; nonhuman primates; human vocalizations. Within-TVA activity was also observed for macaque vocalizations. Our results provide evidence for subregions of the TVA that respond principally—but not exclusively—to phylogenetically and acoustically close nonhuman primate vocalizations, namely those of chimpanzees.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Remove color highlights related to revisions
Include supplementary material</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The study of cerebral mechanisms underlying speech and voice processing has gained importance since the early 2000s with the advent of functional magnetic resonance imaging (fMRI) [<xref ref-type="bibr" rid="c1">1</xref>]. Voice-sensitive areas, commonly referred to as “temporal voice areas” (TVA) or simply “voice areas”, have been highlighted along the upper, superior portion of the temporal cortex [<xref ref-type="bibr" rid="c2">2</xref>]. Since then, great efforts have been made to better characterize these TVA, with particular attention to their spatial division into functional subregions [<xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c5">5</xref>]. A fairly large body of literature points to the critical role of the TVA in voice perception and processing in healthy participants [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c8">8</xref>] as well as in lesioned patients [<xref ref-type="bibr" rid="c9">9</xref>]. Subregions of the TVA have also been directly linked to social perception [<xref ref-type="bibr" rid="c10">10</xref>], vocal emotion processing [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>], voice identity [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>], and gender perception [<xref ref-type="bibr" rid="c15">15</xref>]. The developmental axis of voice processing has also been studied in infants, demonstrating the existence of the TVA in the human brain as early as 7— but not 4—months of age [<xref ref-type="bibr" rid="c16">16</xref>], while the ability to respond specifically to the voice of their parents has been observed in fetuses in utero [<xref ref-type="bibr" rid="c17">17</xref>]. With the ongoing development of brain imaging and analysis techniques [<xref ref-type="bibr" rid="c18">18</xref>], it is realistic to expect successful, albeit noninvasive, fMRI results on task-related voice perception in utero in the near future. Along the evolutionary axis, evidence for TVA or, more generally, conspecific vocalization-sensitive brain areas has emerged primarily in dogs [<xref ref-type="bibr" rid="c19">19</xref>] and monkeys [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>] (<italic>Macaca mulatta</italic>), raising the question of whether such specialized brain areas are species-specific [<xref ref-type="bibr" rid="c22">22</xref>] and to what extent human and nonhuman primates share neural mechanisms that enable them to preferentially process conspecific vocalizations [<xref ref-type="bibr" rid="c23">23</xref>]. However, less attention has been paid to paradigms in which animal vocalizations are presented to humans, and to the best of our knowledge no study to date has reported selective human TVA activations for processing such auditory material, namely the vocalizations of other animals. Human processing of animal vocalizations has been studied with both monkey and cat material, but no <italic>specific</italic> cross-species activations have been observed within the TVA with respect to either species [<xref ref-type="bibr" rid="c24">24</xref>]. Other studies have focused more specifically on phylogenetic distance and have included nonhuman ape (chimpanzee, <italic>Pan troglodytes</italic>) and ‘Old World’ monkey (rhesus macaque, <italic>Macaca mulatta</italic>) vocalizations as stimuli. Such studies failed to identify species-specific brain activations—despite correctly discriminating chimpanzee affective vocalizations [<xref ref-type="bibr" rid="c25">25</xref>]—and observed ambivalent results for below [<xref ref-type="bibr" rid="c25">25</xref>] vs. above [<xref ref-type="bibr" rid="c26">26</xref>] chance discrimination of macaque affective vocalizations by human participants. A recent exception is a study in which functionally homologous anterior TVA activity was observed in both humans and macaques: this region was indeed specific to macaque calls in the macaque’s anterior TVA, and specific to human voices in the anterior TVA of humans, but no macaque-specific activity was observed in the human TVA [<xref ref-type="bibr" rid="c27">27</xref>]. This sparse literature motivated the present study, which aims to investigate cross-species TVA activations in humans when asked to categorize vocalizations from phylogenetically—and acoustically-close and -distant—species while undergoing fMRI scanning. The importance of acoustic differences between species and more specifically acoustic distance, particularly through fundamental frequency variations [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>] was indeed of great interest. Acoustic distance—calculated using Mahalanobis distance with 16 acoustic parameters extracted from the stimuli—was in fact a determining parameter in assessing affective cues recognition in nonhuman primate calls by human participants [<xref ref-type="bibr" rid="c30">30</xref>]. In this study, affiliative chimpanzee—but not bonobo—calls were acoustically the closest to positive human voice stimuli, suggesting a distinct evolution of bonobo calls [<xref ref-type="bibr" rid="c30">30</xref>]. Bonobo vocalizations are of particular interest because this species is thought to have undergone evolutionary changes in their communication, in part due to a neoteny process involving acoustic modifications, and although they are as phylogenetically close to humans as chimpanzees—with an estimated separation with the <italic>Homo</italic> lineage only 6-8 million years ago [<xref ref-type="bibr" rid="c31">31</xref>]. Previous research has shown that bonobos have a shorter larynx—a valid predictor of a species’ mean fundamental frequency [<xref ref-type="bibr" rid="c32">32</xref>]—compared to chimpanzees, resulting in a higher fundamental frequency in their calls [<xref ref-type="bibr" rid="c28">28</xref>]. Such a difference has been demonstrated in juvenile bonobo calls compared to chimpanzee and human baby calls [<xref ref-type="bibr" rid="c33">33</xref>], arguing for a greater acoustic distance between bonobo calls and human or chimpanzee vocalizations. For these reasons, we included vocalizations from both Pan species (chimpanzees, <italic>Pan troglodytes</italic>; bonobos, <italic>Pan paniscus</italic>), as well as a phylogenetically more distant species (<italic>Cercopithecidae</italic>: rhesus monkeys), with an estimated separation with the <italic>Homo</italic> lineage dating back to 25 million years ago. Indeed, any claim of human ‘uniqueness’ for TVA recruitment remains on hold and should be tested in light of these closely related species. Using the same stimuli, we previously investigated the specific frontal mechanisms involved in the categorization of nonhuman primate vocalizations independently of a selection of low-level acoustic parameters [<xref ref-type="bibr" rid="c34">34</xref>], but the possibility that acoustic differences would affect, at the auditory level, the ability of human participants to recognize nonhuman primate calls should be thoroughly examined, as we did in the present study. As suggested by research mentioned above, monkey vocalizations are overall less likely to be identified compared to ape vocalizations due to both phylogenetic and acoustic differences. Therefore, our mechanistic hypothesis of the difficulty for humans to recognize bonobo calls is that frequencies of the human tonotopic map in the auditory cortex—adapted and adjusted to the frequencies of the human voice during evolution—would not be tailored to process the frequencies generated by bonobo calls. It would also be the case for macaque calls, while frequencies of chimpanzee calls—being closer to the range of human voice fundamental frequency [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c30">30</xref>]—would be better represented in the human auditory cortex and therefore more easily processed and better identified by humans.</p>
<p>According to the literature mentioned so far and to the mechanistic hypothesis underlying the processing of chimpanzee as opposed to bonobo and/or macaque calls by human participants, we therefore predicted: (i) more acoustic proximity between human and chimpanzee vocalizations, whereas more distance would separate those of bonobos and macaques from the human voice; (ii) a recruitment of temporal brain areas—within the TVA—for the processing of vocalizations from the Pan taxon (chimpanzee, bonobo) but not <italic>Cercopithecidae</italic> (rhesus monkey) vocalizations, taking into account acoustic features of interest through a discriminant analysis of the parameters that best underlie our stimuli.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>Our hypotheses involve a systematic and thorough control of phylogeny through the inclusion of specific primate species as well as the selection of specific acoustic features. We programmed a task in which the vocalizations of each species were presented randomly and for which the participants (N=23) had to specify to which species each stimulus corresponded. We therefore included equal numbers of trials (N=72) with human, chimpanzee, bonobo and macaque vocalizations—N=18 each—as well as trial-level acoustic features of the vocalizations, using three distinct statistical models with specific covariates. These models are sorted from the least to the most sophisticated modeling to uncover the role(s) of acoustic features on TVA activity potentially specific to each/some species (see the Methods; Model 1: mean of vocalization fundamental frequency and energy; Model 2: multi-dimensional Mahalanobis acoustic distance between the human voice and the calls of each nonhuman primate species [<xref ref-type="bibr" rid="c35">35</xref>]; Model 3: between-species most discriminant acoustic features of our stimuli, extracted using a general discriminant analysis [<xref ref-type="bibr" rid="c30">30</xref>]). Acoustical analyses involved in Model 2 allowed us to validate our first hypothesis according to which chimpanzees are acoustically the closest to humans, followed by the calls of bonobos and macaques (<xref rid="fig1" ref-type="fig">Fig.1<bold>B</bold></xref>)— the main effect of Species on the acoustic distance was significant, F(3,88)=15.84, <italic>p</italic>&lt;.001, as well as all comparisons (see <xref rid="fig1" ref-type="fig">Fig.1<bold>B</bold></xref> and Table S2). In this study, we did not intend to focus on behavioral data since these have already been published with these stimuli in dedicated studies [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c34">34</xref>]. Instead, we were interested in the neural processing associated with the exposition of human participants to primate vocalizations (<xref rid="fig1" ref-type="fig">Fig.1<bold>A</bold></xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1:</label>
<caption><title>Timecourse of the species categorization task with stimuli example and acoustic distance data.</title>
<p>(<bold>A</bold>) Detail of the timecourse of four trials of the species categorization task in non-representative order, including waveform and spectrogram graphs for one example stimulus of each species. (<bold>B</bold>) Scatter plot and histogram of the acoustic Mahalanobis distance data of each stimulus for each species including mean (numbers represent exact mean value) and violin plots of the standard error of the mean in addition to distribution fit. ITI: inter trial interval; Hum: human; Chimp: chimpanzee; Bon: bonobo; Mac: macaque.</p></caption>
<graphic xlink:href="677258v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2a">
<title>Neuroimaging data within the sample-specific temporal voice areas</title>
<p>We aimed at uncovering functional changes relative to species categorization and processing within sample-specific (N=23) TVA, as delineated in our hypotheses. As described above, we used three distinct statistical models including trial-level parametric modulators (Model 1-3). We were particularly interested in human brain activity while processing vocalizations of our closest relatives—both acoustically and phylogenetically—namely the chimpanzee but also the bonobo. The present study did not aim at uncovering wholebrain results underlying the processing of each species’ vocalizations but rather focused on human voice-sensitive areas, namely the TVA, although corrected statistics (voxelwise <italic>p</italic>&lt;.05 False Discovery Rate) presented in this section were computed with a wholebrain voxelwise approach for higher data reproducibility and generalizability, and not using region-of-interest (ROI) analyses. ROI analyses would most probably have artificially amplified the number of voxels in the TVA in this study. Clusters outside the bounds of the sample-specific TVA are therefore visible but in a desaturated hue to better highlight TVA activations. These clusters are even more visible in supplementary figures with the same contrasts as those presented in this section—with the addition of the [human,chimpanzee &gt; bonobo,macaque] contrast—but with an outline of the TVA from an independent, larger sample of participants excluding the 23 participants of this study (N=98; Fig.S2-4). No difference in terms of a potential attentional bias towards any species of the stimuli used in this study was found (Independent sample of N=28, see Methods and Fig.S1 for detailed information on this aspect).</p>
</sec>
<sec id="s2b">
<title>Model 1: Effects of species processing with vocalization mean fundamental frequency and mean energy as covariates of no-interest at the trial level</title>
<p>In this first ‘simple’ model, we first wanted to remove from brain activations the part of variance correlating with basic low-level acoustics—as reported in the literature [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c33">33</xref>], namely mean voice fundamental frequency and energy. A total of four contrasts were overlaid in the figure to test our second hypothesis, according to which phylogenetic—and especially acoustic, see Model 2—proximity would trigger enhanced activity in the TVA, just as the human voice does. Brain activity specific to chimpanzee vocalizations ([chimpanzee &gt; human, bonobo, macaque]) led to enhanced activity in a cluster of the left anterior STG (aSTG<sup>1</sup>, k=91 voxels, <xref rid="fig2" ref-type="fig">Fig.2<bold>AD</bold></xref>) located within the TVA (<xref rid="fig2" ref-type="fig">Fig.2<bold>AC</bold></xref>). A homologous cluster of the right anterior STG was found as well in this contrast (aSTG<sup>2</sup>, <xref rid="fig2" ref-type="fig">Fig.2<bold>B</bold></xref>). A similar result was observed when directly contrasting chimpanzee to human vocalizations ([chimpanzee &gt; human]; <xref rid="fig2" ref-type="fig">Fig.2<bold>EFH</bold></xref>) as well as chimpanzee to nonhuman primate calls ([chimpanzee &gt; bonobo, macaque]) in three other clusters of the aSTG, located again within the TVA (<xref rid="fig2" ref-type="fig">Fig.2<bold>ABC</bold></xref>, <xref rid="tbl1" ref-type="table">Table 1</xref>). Enhanced activity for human relative to chimpanzee vocalizations ([human &gt; chimpanzee]) was observed in large parts of the anterior, mid and posterior superior and middle temporal cortex (<xref rid="fig2" ref-type="fig">Fig.2<bold>EFG</bold></xref>, <xref rid="tbl1" ref-type="table">Table 1</xref>). No voxels reached significance at the wholebrain level for the [bonobo &gt; human, chimpanzee, macaque], [bonobo &gt; chimpanzee, macaque], [bonobo &gt; human], [bonobo &gt; chimpanzee], [bonobo &gt; macaque], [macaque &gt; human, chimpanzee, bonobo], [macaque &gt; chimpanzee, bonobo], [macaque &gt; human], [macaque &gt; chimpanzee], [macaque &gt; bonobo] contrasts. This analysis therefore revealed that the human anterior TVA are sensitive to cross-species primate vocalizations—specifically to chimpanzee but not bonobo or macaque calls using this regression model.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2:</label>
<caption><title>Wholebrain results when contrasting the processing of chimpanzee to other species’ vocalizations with mean fundamental frequency and energy as trial-level covariates of no-interest (model 1).</title>
<p>(<bold>ABC</bold>) Enhanced brain activity on a sagittal view with activity specific to chimpanzee vocalizations (dark blue to green) as well as between chimpanzee calls vs bonobo and macaque calls (chimpanzee &gt; bonobo and macaque: brown to red with light yellow outline). (<bold>D</bold>) Percentage of signal change for each individual and relevant species according to the contrast in the left anterior superior temporal gyrus (aSTG<sup>1</sup>). Box plots represent mean value (black line) and the standard error of the mean with distribution fit. (<bold>EFG</bold>) Direct comparison between human and chimpanzee vocalizations (human &gt; chimpanzee: dark red to yellow; chimpanzee &gt; human: dark green to yellow) on a sagittal render. (<bold>H</bold>) Percentage of signal change in the anterior superior temporal gyrus (aSTG<sup>2</sup>) when contrasting chimpanzee to human vocalizations for each individual and relevant species according to the contrast with box plots representing mean value (black line) and the standard error of the mean with distribution fit. Brain activations are independent of low-level acoustic parameters for all species (mean fundamental frequency ‘F0’ and mean energy of vocalizations). Data corrected for multiple comparisons using wholebrain voxelwise false discovery rate (FDR) at a threshold of <italic>p</italic>&lt;.05. Percentage of signal change extracted at cluster peak including 9 surrounding voxels, selecting among these the ones explaining at least 85% of the variance using singular value decomposition. Circles represent individual values, boxplot represents the mean and its standard error, and half-violin plots show data distribution. Hum: human; Chimp: chimpanzee; Bon: bonobo; Mac: macaque. TVA: sample-specific (N=23) temporal voice areas. ‘a’ prefix: anterior; ‘m’ prefix: mid; ‘p’ prefix: posterior; STG: superior temporal gyrus; STS: superior temporal sulcus; L: left hemisphere; R: right hemisphere.</p></caption>
<graphic xlink:href="677258v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Activations, cluster size and coordinates for each contrast of interest of model 1 (mean of vocalization fundamental frequency and energy as trial-level covariates of no-interest) in the sample-specific temporal voice areas, wholebrain voxelwise <italic>p</italic>&lt;.05 FDR corrected, k&gt;10.</p></caption>
<graphic xlink:href="677258v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2c">
<title>Model 2: Effects of species processing with vocalization acoustic distance from human voice, per species, as covariate of no-interest at the trial level</title>
<p>In this second model, we wanted to remove from brain activations the part of variance correlating with the acoustic distance between each species and the human voice (see Methods for the detailed index of acoustic distance calculated with the human voice as reference). TVA brain activity specific to primate calls was triggered again for chimpanzee vocalizations ([chimpanzee &gt; human, bonobo, macaque]) in a cluster of the left anterior STG within the TVA (<xref rid="fig3" ref-type="fig">Fig.3<bold>ACD</bold></xref>). A similar result was observed when directly contrasting chimpanzee to human vocalizations ([chimpanzee &gt; human]; <xref rid="fig3" ref-type="fig">Fig.3<bold>EH</bold></xref>, <xref rid="tbl2" ref-type="table">Table 2</xref>). Enhanced activity for human relative to chimpanzee vocalizations ([human &gt; chimpanzee]) was again observed in large parts of the anterior, mid and posterior superior and middle temporal cortex (<xref rid="fig3" ref-type="fig">Fig.3<bold>EFG</bold></xref>, <xref rid="tbl2" ref-type="table">Table 2</xref>). Chimpanzee compared to other nonhuman primate calls ([chimpanzee &gt; bonobo, macaque]) led to enhanced activity in the bilateral aSTG (aSTG<sup>7</sup> and aSTG<sup>9</sup>, <xref rid="fig3" ref-type="fig">Fig.3<bold>ABC</bold></xref>). Using this second modelling of the MRI data, no voxels reached significance at the wholebrain level for the [bonobo &gt; human, chimpanzee, macaque], [bonobo &gt; chimpanzee, macaque], [bonobo &gt; chimpanzee], [bonobo &gt; macaque], [macaque &gt; human, chimpanzee, bonobo], [macaque &gt; chimpanzee, bonobo], [macaque &gt; bonobo] contrasts. Contrasting [bonobo &gt; human] and [macaque &gt; human] yielded to enhanced activity outside the TVA while the [macaque &gt; chimpanzee] comparison activated a very small cluster of within-TVA left planum temporale (see Fig.S5 for these contrasts). In this second model, again, only the calls of chimpanzees triggered specific activity in the anterior TVA.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3:</label>
<caption><title>Wholebrain results when contrasting the processing of chimpanzee to other species’ vocalizations with Mahalanobis acoustic distance as trial-level covariate of no-interest (model 2).</title>
<p>(<bold>ABC</bold>) Enhanced brain activity on a sagittal view with activity specific to chimpanzee vocalizations (chimp &gt; hum,bon,mac; dark blue to green) as well as between chimpanzee calls vs bonobo and macaque calls (chimpanzee &gt; bonobo and macaque: brown to red with light yellow outline). (<bold>D</bold>) Percentage of signal change for each individual and relevant species according to the contrast in the left anterior superior temporal gyrus (aSTG<sup>6</sup>). Box plots represent mean value (black line) and the standard error of the mean with distribution fit. (<bold>EFG</bold>) Direct comparison between human and chimpanzee vocalizations (human &gt; chimpanzee: dark red to yellow; chimpanzee &gt; human: dark green to yellow) on a sagittal render. (<bold>H</bold>) Percentage of signal change in the anterior superior temporal gyrus (aSTG<sup>8</sup>) when contrasting chimpanzee to human vocalizations for each individual and relevant species according to the contrast with box plots representing mean value (black line) and the standard error of the mean with distribution fit. Brain activations are independent from the acoustic distance of each stimulus for all species. Data corrected for multiple comparisons using wholebrain voxelwise false discovery rate (FDR) at a threshold of <italic>p</italic>&lt;.05. Percentage of signal change extracted at cluster peak including 9 surrounding voxels, selecting among these the ones explaining at least 85% of the variance using singular value decomposition. Circles represent individual values, boxplot represents the mean and its standard error, and half-violin plots show data distribution. Hum: human; Chimp: chimpanzee; Bon: bonobo; Mac: macaque. TVA: sample-specific (N=23) temporal voice areas. ‘a’ prefix: anterior; ‘m’ prefix: mid; ‘p’ prefix: posterior; STG: superior temporal gyrus; STS: superior temporal sulcus; L: left hemisphere; R: right hemisphere.</p></caption>
<graphic xlink:href="677258v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Activations, cluster size and coordinates for each contrast of interest of model 2 (inter-species vocalization acoustic distance as trial-level covariate of no-interest) in the sample-specific temporal voice areas, wholebrain voxelwise <italic>p</italic>&lt;.05 FDR corrected, k&gt;10.</p></caption>
<graphic xlink:href="677258v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2d">
<title>Model 3: Effects of species processing with vocalization most discriminant acoustic parameters (N=6) as covariates of no-interest at the trial level</title>
<p>In this last model, we wanted to elaborate more on the discriminant factors that characterize the low-level acoustic parameters of our set of stimuli. This approach is complementary to the inclusion of acoustic distance in model 2 and extends and refines these results. To do so, we used as trial-level covariates of no-interest the acoustic parameters explaining the most variance ([r &gt; 0.7] and [r &lt; -0.7]) in factors 1-3 of a discriminant analysis of these stimuli [<xref ref-type="bibr" rid="c30">30</xref>]— see the methods section for details on this analysis. These parameters therefore include, in this specific order: vocalization loudness, intensity, change in spectrum, bandwidth contour of the second formant (F2), power of the fundamental frequency (F0) and finally the difference in intensity contour. Having these acoustic features as covariates, we ran the same contrasts as in models 1 &amp; 2. As in previous modeling of the imaging data, TVA activity was triggered by chimpanzee vocalizations ([chimpanzee &gt; human, bonobo, macaque]) in yet other, larger bilateral clusters of the aSTG within the TVA (aSTG<sup>10</sup> and aSTG<sup>11</sup>, <xref rid="fig4" ref-type="fig">Fig.4<bold>ABCD</bold></xref>), closely resembling activations of model 2. A left-lateralized similar cluster was observed when directly contrasting chimpanzee to human vocalizations ([chimpanzee &gt; human]; aSTG<sup>12</sup>, <xref rid="fig4" ref-type="fig">Fig.4<bold>EH</bold></xref>, <xref rid="tbl3" ref-type="table">Table 3</xref>). Enhanced activity for human relative to chimpanzee vocalizations ([human &gt; chimpanzee]) was similarly represented as in models 1 &amp; 2 (anterior, mid and posterior superior and middle temporal cortex; <xref rid="fig4" ref-type="fig">Fig.4<bold>EFG</bold></xref>, <xref rid="tbl3" ref-type="table">Table 3</xref>). Chimpanzee compared to other nonhuman primate calls in this model ([chimpanzee &gt; bonobo, macaque]) led to the largest clusters observed in the aSTG—all models considered, still within the sample-specific TVA. Indeed, we observed a large left-lateralized cluster of the aSTG extending to the mid STG (aSTG<sup>13</sup>, <xref rid="fig4" ref-type="fig">Fig.4<bold>AC</bold></xref>) as well as a right-lateralized cluster (aSTG<sup>14</sup>, <xref rid="fig4" ref-type="fig">Fig.4<bold>BC</bold></xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4:</label>
<caption><title>Wholebrain results when contrasting the processing of chimpanzee to other species’ vocalizations with vocalization loudness, intensity, change in spectrum, F2 bandwidth contour, F0 power and intensity contour difference as trial-level covariates of no-interest (model 3).</title>
<p>(<bold>ABC</bold>) Enhanced brain activity on a sagittal view with activity specific to chimpanzee vocalizations (dark blue to green) as well as between chimpanzee calls vs bonobo and macaque calls (chimpanzee &gt; bonobo and macaque: brown to red with light yellow outline). (<bold>D</bold>) Percentage of signal change for each individual and relevant species according to the contrast in the left anterior superior temporal gyrus (aSTG<sup>10</sup>). Box plots represent mean value (black line) and the standard error of the mean with distribution fit. (<bold>EFG</bold>) Direct comparison between human and chimpanzee vocalizations (human &gt; chimpanzee: dark red to yellow; chimpanzee &gt; human: dark green to yellow) on a sagittal render. (<bold>H</bold>) Percentage of signal change in the anterior superior temporal gyrus (aSTG<sup>12</sup>) when contrasting chimpanzee to human vocalizations and when contrasting chimpanzee to bonobo and macaque calls (aSTG<sup>13</sup>) for each individual and relevant species according to the contrast with box plots representing mean value (black line) and the standard error of the mean with distribution fit. Brain activations are independent of the most discriminant low-level acoustic parameters of the stimuli set [<xref ref-type="bibr" rid="c30">30</xref>]. Data corrected for multiple comparisons using wholebrain voxelwise false discovery rate (FDR) at a threshold of <italic>p</italic>&lt;.05. Percentage of signal change extracted at cluster peak including 9 surrounding voxels, selecting among these the ones explaining at least 85% of the variance using singular value decomposition. Circles represent individual values, boxplot represents the mean and its standard error, and half-violin plots show data distribution. Hum: human; Chimp: chimpanzee; Bon: bonobo; Mac: macaque. TVA: sample-specific (N=23) temporal voice areas. ‘a’ prefix: anterior; ‘m’ prefix: mid; ‘p’ prefix: posterior; STG: superior temporal gyrus; STS: superior temporal sulcus; L: left hemisphere; R: right hemisphere.</p></caption>
<graphic xlink:href="677258v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>Activations, cluster size and coordinates for each contrast of interest of model 3 (vocalization loudness, intensity, change in spectrum, F2 bandwidth contour, F0 power and intensity contour difference as trial-level covariate of no-interest) in the sample-specific temporal voice areas, wholebrain voxelwise <italic>p</italic>&lt;.05 FDR corrected, k&gt;10.</p></caption>
<graphic xlink:href="677258v2_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="677258v2_tbl3a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>In this last modelling of the fMRI data, no voxels reached significance either at the wholebrain level or within the TVA for the [bonobo &gt; human, chimpanzee, macaque] or [bonobo &gt; chimpanzee, macaque], [bonobo &gt; chimpanzee], [bonobo &gt; macaque] contrasts. We however found activity specific to the processing of macaque calls only in the left TVA, more specifically in a small cluster of the left mid STS ([macaque &gt; human, chimpanzee, bonobo]) and in a small portion of the planum temporale adjacent to the primary auditory cortex for the [macaque &gt; chimpanzee, bonobo] contrast (see Fig.S6). We also observed significant activations but outside the TVA for the [bonobo &gt; human] and the [macaque &gt; chimpanzee] contrast. Within-TVA activations were observed when contrasting macaque to human vocalizations in the left mid STS, as well as in the in the planum temporale, left mid STS and right mid STG when contrasting macaque to bonobo calls. See Fig.S7 for these results. Using this third model, we again observed chimpanzee-specific activity in the anterior TVA as well as mid STS activity specific to macaque calls, within the TVA.</p>
</sec>
<sec id="s2e">
<title>A synthesis of the sensitivity of the human TVA to nonhuman primate calls</title>
<p>In the previous sections, we described three different models used to analyze our fMRI data. These models—from the simplest to the more sophisticated one—highlighted enhanced activity within sample-specific bilateral anterior TVA of our participants specifically when processing chimpanzee vocalizations—but also when processing macaque calls in model 3, in the bilateral mid STG, STS and planum temporale. When processing chimpanzee calls, TVA activity was especially enhanced in the aSTG but also in the anterior STS. We therefore regrouped these fourteen chimpanzee-specific aSTG clusters in <xref rid="fig5" ref-type="fig">Fig.5</xref>—most of them overlap greatly but we still named them individually according to each contrast and analysis for exhaustivity—overlaid with sample-specific TVA (<xref rid="fig5" ref-type="fig">Fig.5<bold>CD</bold></xref>) and with the more general TVA from an independent sample of ninety-eight participants (<xref rid="fig5" ref-type="fig">Fig.5<bold>AB</bold></xref>). Zooming closely, the area of maximal overlap between these regions (the orange surface) is located within the more general as well as within the sample-specific TVA. Interestingly, left-lateralized more medial clusters of aSTG were outside the outline of the sample-specific but not of the general TVA (<xref rid="fig5" ref-type="fig">Fig.5<bold>AC</bold></xref>), while this was not the case for right-lateralized aSTG activations. Comparing the areas recruited when processing chimpanzee to bonobo and macaques calls, this contrast— especially in model 3, yielded to distinct clusters of aSTG. This result is visible when looking at the three ‘rich blue’ outlines in every panel of <xref rid="fig5" ref-type="fig">Fig.5</xref>. The results synthesized here highlight the important role of acoustic parameters and emphasize the role of the most discriminant acoustic features on TVA activity relating to nonhuman primate vocalizations, especially those of chimpanzees and macaques.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 5:</label>
<caption><title>Synthesis of mid and anterior TVA clusters of activity recruited specifically by the processing of chimpanzee and macaque vocalizations (Models 1,2,3).</title>
<p>aSTG and aSTS clusters recruited for the processing of chimpanzee calls as opposed to: human voices (green); bonobo, macaque calls (blue) and human voice; bonobo and macaque calls (turquoise) in the general TVA (<bold>AB</bold>, N=98) as well as in the sample-specific TVA (<bold>CD</bold>, N=23). Macaque results are only significant for Model 3 (purple: Macaque vs all other species; lilac: Macaque vs other nonhuman primates). Clusters are represented across all statistical models (Model 1: dotted line; Model 2: dashed line; Model 3: solid line). Model 1: mean of fundamental frequency and energy (covariates of no-interest, N=2); Model 2: acoustic distance (covariate of no-interest, N=1); Model 3: acoustic parameters that characterize low-level acoustics of our stimuli following a discriminant analysis (covariates of no-interest, N=6). Data are all corrected for multiple comparison using wholebrain voxelwise false discovery rate (FDR) at a threshold of <italic>p</italic>&lt;.05. Hum: human; Chimp: chimpanzee; Bon: bonobo; Mac: macaque. TVA: temporal voice areas. ‘a’ prefix: anterior; STG: superior temporal</p></caption>
<graphic xlink:href="677258v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>These results are given even more weight by more fined-tuned comparisons of voice versus non-voice material in the voice-localizer task, namely by splitting the non-vocal blocks as a function of the auditory sounds they contain. In this more specific outline of TVA subregions, we observed that most chimpanzee- and macaque-specific STG and STS regions were still within the bounds of TVA, especially in the most relevant case when the outline represented a comparison of human voice signals to animal or nature sounds (<xref rid="fig6" ref-type="fig">Fig.6<bold>A-D</bold></xref>), while the outlines for human voice versus music or noise excluded most parts of the clusters of activation of these nonhuman primate species’ calls in the TVA (<xref rid="fig6" ref-type="fig">Fig.6<bold>E-H</bold></xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 6:</label>
<caption><title>Clusters recruited specifically by the processing of chimpanzee and macaque vocalizations (Model 3) in subregions of the TVA, as a function of non-vocal material type.</title>
<p>Enhanced brain activity on a sagittal views with activity specific to macaque vocalizations (red to yellow), specific to chimpanzee vocalizations (dark blue to green) as well as between chimpanzee calls vs bonobo and macaque calls (chimpanzee &gt; bonobo and macaque: brown to red with light yellow outline). Brain activations are independent of the most discriminant low-level acoustic parameters of the stimuli set [<xref ref-type="bibr" rid="c30">30</xref>]. Data corrected for multiple comparisons using wholebrain voxelwise false discovery rate (FDR) at a threshold of <italic>p</italic>&lt;.05. Black outline represents: voice compared to non-vocal stimuli of animal sounds (<bold>A,B</bold>), nature sounds (<bold>C,D</bold>), music (<bold>E,F</bold>), artificial noise (<bold>G,H</bold>). Hum: human; Chimp: chimpanzee; Bon: bonobo; Mac: macaque. TVA: sample-specific (N=23; white outline) temporal voice areas. STG: superior temporal gyrus; STS: superior temporal sulcus; ‘a’ prefix: anterior; ‘m’ prefix: mid; L: left hemisphere; R: right hemisphere.</p></caption>
<graphic xlink:href="677258v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The present study provides evidence of the sensitivity of the human TVA to cross-species vocalizations, especially to chimpanzee calls but also to macaque vocalizations, as illustrated by specific enhanced activity in the bilateral mid and anterior STG and STS—within sample-specific TVA. These results were obtained through statistical modeling of the MRI data that included either simple acoustics or the use of Mahalanobis acoustic distance between species and the most discriminant acoustic features specific to our stimuli as covariates. These two latter analyses converged and yielded to greatly overlapping results, especially in the anterior TVA. Therefore, our results suggest that vocalizations from another ape species recruits subregions of human temporal cortex that process species-specific voices in humans—namely the bilateral, sample-specific TVA. This evidence speaks in favor of cross-species primate vocalization processing in the anterior and mid TVA of humans—for chimpanzee and macaque calls, respectively. While our acoustic data confirmed the hypothesized hierarchy of acoustic distance as a function of phylogenetic distance between our species, we still observed mid STG and STS activity for macaque versus bonobo calls and a small cluster in the left mid STS specific to macaque calls in model 3—an unexpected result since macaques are the most distant species from humans both phylogenetically and acoustically in our study. Therefore, while we initially hypothesized that primate calls would exclusively recruit human TVA as a function of a combination of phylogenetic and acoustic proximity, our data also point toward the greater importance of the most discriminant acoustic features rather than acoustic distance alone. We discuss these aspects below in more detail and interpret their general meaning and subsequent scientific implications, in addition to highlighting the limitations of our study.</p>
<p>Often specifically associated with the processing of conspecific vocalizations (e.g., in humans [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c27">27</xref>], macaques [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c36">36</xref>], and dogs [<xref ref-type="bibr" rid="c19">19</xref>]), the present study challenges the common view of the TVA as ‘species-specific’ and illustrates that human voices, chimpanzee and macaque calls can enhance activity in the TVA. We think that the distinct locations of these TVA subregions recruited for processing the vocalizations of these primate species matters. In fact, there might be a possible association between anterior TVA—specific to processing chimpanzee calls—and the higher recognition performance of chimpanzee calls compared to those of bonobos or macaques in human participants [<xref ref-type="bibr" rid="c34">34</xref>]. Anterior TVA activity specific to the processing of chimpanzee calls occurred when these were compared to both human and nonhuman primate species, solely to other nonhuman primate vocalizations, or directly to the human voice. However, homologous specific results were not observed for bonobo and they were more scarce—especially between models—for macaque vocalizations: we found macaque-specific activity in a small area of the planum temporale and in a small cluster of the left mid STS, congruent for instance with locations observed in the general processing of animal sounds, especially in the planum temporale [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. On the other hand, within-TVA anterior STG activity was also observed when chimpanzee vocalizations were directly compared to the human voice. We think this result highlights the cross-species specificity of this anterior subregion of the TVA for processing species phylogenetically close to humans and especially with human-like acoustics, namely the calls of chimpanzees in the case of our study. Because of their vocal proximity, the perception of human voices and chimpanzee calls in socio-affective contexts could involve a common ‘social’ core of the brain, which increases activity in brain regions such as the anterior TVA, as reported previously in studies pertaining to social contextual information processing in the anterior STG [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>]. Differences at the level of processing complexity between the two types of vocalizations could also explain this observation, while we demonstrated that saliency or attention-related effects do not exist between our species stimuli. Indeed, previous studies have shown the role of the anterior STG and the anterior STS in the conceptual representation of social context by the human voice [<xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c42">42</xref>]. Therefore, our data might suggest that the anterior part of the superior temporal cortex could be recruited to process the social context of human and chimpanzee vocal stimuli. However, this processing would be more automated for the perception of the human voice than for chimpanzee calls because of our high exposure and expertise as humans to these vocal signals, but these hypotheses should be addressed scientifically in studies dedicated to this topic. Our results are also complementary to and coherent with a ‘voice patch’ system in the brain of primates, as put forward by Belin and colleagues [<xref ref-type="bibr" rid="c43">43</xref>], and according to which distinct ‘patches’ or subregions of the temporal lobe—especially its anterior portion, would be interconnected and would allow for the processing of voice information. Such system would be present in many primate species such as humans, macaques and marmosets, with most recent evidence suggesting a population of neurons in the anterior STG of the macaque brain selective to human voice [<xref ref-type="bibr" rid="c44">44</xref>], as also anticipated in another study on macaques and also in the anterior STG [<xref ref-type="bibr" rid="c45">45</xref>]. These fascinating and converging results mirror our present data—with ‘chimpanzee-selective’ responses in the anterior STG/TVA of our human participants—and strongly emphasize the need for pursuing a comparative approach in order to clarify the cross-species neural bases underlying the processing of human and nonhuman primate vocal signals. As we mentioned previously, these interpretations are, considering our results, free of any potential attentional bias towards one species over the others, since no effect was observed on that matter in a control, behavioral study involving an independent sample of twenty-eight participants in a species-specific exogenous cueing attentional paradigm— Methods and Fig.S1.</p>
<p>Importantly, our data also emphasize the influence of acoustic features and especially acoustic proximity between human and chimpanzee vocalizations: we show that activity in the anterior STG and more generally in the anterior TVA partly depends on phylogenetic and more importantly on key acoustic features and acoustic proximity. Consistent with previous studies [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>], we did not expect TVA activity for macaque calls processing because they are both phylogenetically and acoustically more distant from humans than the other species in this study—although, as above mentioned, we found a very small cluster in the left primary auditory cortex and mid STS for model 3. It is interesting to note that in model 2, with acoustic distance as trial-level covariate, we observed TVA activity only for chimpanzee but not macaque calls, giving further weight to the importance of acoustic distance in this context. Also, if only phylogenetic proximity mattered, bonobo calls should also elicit activity in the TVA because they are as phylogenetically close to humans as chimpanzees. But this viewpoint is rather reductive and our results show that this is not entirely correct, and that activity in the TVA crucially depends on the acoustic properties of the perceived vocalizations since we cannot infer phylogeny from vocalizations. This interpretation is strongly supported by the inclusion of acoustic Mahalanobis distance for each species compared to the human voice as a trial-level covariate of no-interest. Using such modelling, differential neuroimaging results between chimpanzee and bonobo vocalizations were explained by both acoustic and phylogenetic proximity in the TVA. These results are consistent with the recent proposal—and recent findings [<xref ref-type="bibr" rid="c30">30</xref>]—that there are substantial differences between chimpanzee and bonobo vocalizations. These encompass fundamental frequency range and mean due to larynx length [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c32">32</xref>]—despite the evolutionary relatedness to chimpanzees [<xref ref-type="bibr" rid="c28">28</xref>]. Therefore, the interaction between phylogeny and acoustic distance or proximity would explain the anterior TVA expansion for processing specifically chimpanzee but not bonobo vocalizations. This argument however falls short to explain the recruitment of the TVA by macaque calls in model 3.</p>
<p>Overall, it seems reasonable to hypothesize that TVA activity is not <italic>per se</italic> human-specific [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c41">41</xref>] but that TVA are instead sensitive to vocalizations from other primate species, provided that these vocalizations have sufficient acoustic proximity to human vocal signals—which would in itself be related to anatomical and/or behavioral changes throughout phylogenetic evolution. This integrative view is again consistent with the concept of a ‘voice patch’ system in the primate brain [<xref ref-type="bibr" rid="c43">43</xref>]. We therefore propose that the mid and anterior TVA, unlike the rest of the TVA, would be heterospecific—sensitive to vocalization acoustics triggered by evolution. This proposition also implies a validation of our mechanistic hypothesis according to which the mean fundamental frequency of chimpanzee but not bonobo calls—the former being much closer to the mean fundamental frequency of the human voice [<xref ref-type="bibr" rid="c28">28</xref>], would allow for a better identification and recognition of chimpanzee calls by humans. This advantage would rely on neurons of the human auditory cortex—both the primary and more secondary regions—being specialized in the processing of low to mid fundamental frequencies such as those of the human voice and chimpanzee calls. In our third analysis model, we looked further into this aspect and included several acoustic properties of our stimuli as a function of the four species in our stimuli. A discriminant analysis [<xref ref-type="bibr" rid="c30">30</xref>] allowed us to select specific acoustic features that best discriminate between our species stimuli. Namely, we took the six parameters explaining the most the differences between our stimuli, including vocalization loudness, intensity— similar to our ‘energy’ covariate of model 1, in addition to change in spectrum, F2 bandwidth contour, F0 power and intensity contour difference. Using these more sophisticated acoustic features as covariates of no-interest, we still obtained brain imaging results very similar to those of model 1 and even closer to model 2—with acoustic distance as covariate, yet with some subtle differences in anterior STG cluster size and location. The peaks were indeed located more ventral and were larger—as compared to results of model 1 &amp; 2, especially for the processing of chimpanzee-specific and macaque-specific vocalizations compared to all primate species and to nonhuman primates alone. These results suggest that the inclusion of spectrum change to intensity- and frequency-related acoustical parameters of the vocal signals slightly shifted and enlarged activation locations in the anterior STG. This result is again congruent with the proposed existence of ‘voice patches’ in the temporal lobe of primate species [<xref ref-type="bibr" rid="c43">43</xref>], with the interconnectivity of these patches highly depending on very fine-grained acoustic aspects of primate vocal signals. This step motivated the inclusion of these parameters as covariates of no-interest in neuroimaging model 3, to retain brain activations marginally independent of such acoustics. The congruence between these data should be explored in more detail in the future by the combination of computational bioacoustics and functional neuroimaging, due to the high relevance and sensitivity of combining these techniques to investigate primate social communication [<xref ref-type="bibr" rid="c46">46</xref>].</p>
<p>A final but maybe more secondary interpretation arising from our results regarding bonobo calls also supports the evolutionary divergence of this peculiar species. According to the self-domestication hypothesis, bonobos would have evolved differently than chimpanzees due to selection against aggression [<xref ref-type="bibr" rid="c47">47</xref>]. Interestingly, differentiation in the evolutionary path of bonobos has influenced both their behavior [<xref ref-type="bibr" rid="c31">31</xref>] and morphology, leading to differences at the level of call production [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c33">33</xref>]. Considering these documented acoustic differences and putting them in perspective with our neuroimaging data, the calls of our last common ancestor with the other Pan species 8 million years ago [<xref ref-type="bibr" rid="c48">48</xref>], may have been closer to those uttered by modern chimpanzees than to those of bonobos. Our data indeed show that modern human brains remain more sensitive to the acoustic characteristics of the calls of the former compared to the latter, arguing for more conserved calls between modern chimpanzees and humans. This aspect is also in line with significant differences between, for instance, the fundamental frequency of human baby cries or babbling (∼250-600Hz) compared to that of bonobos (∼1000-3500Hz) [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>], while they correspond more closely to the fundamental frequency of chimpanzee calls (∼500-1000Hz) [<xref ref-type="bibr" rid="c28">28</xref>]. In our study, bonobo calls definitely are so much different than those uttered by the species of our other stimuli, that they presumably fall outside of the phylogeny and acoustic proximity factors that we outlined so far. This would also put into perspective the recruitment of mid TVAs for macaque calls.</p>
<p>In a sense, we therefore validate our first hypothesis regarding the existence of acoustic distance between each primate species used in our study. We also partially validate our second hypothesis, albeit not completely. In fact, macaque compared to bonobo or other primate calls in model 3 revealed mid TVA activations, and we think that these activations may depend specifically on the importance of the most discriminant acoustic features. Several TVA subregions or ‘patches’ underlying cross-species primate vocalization processing might therefore exist, and our data highlight at least one of them in the mid and anterior portion of the TVA. We will now discuss in further detail task-related limitations that might account for the partial divergence between our results and hypotheses.</p>
<p>Even though we tried to control for critical acoustic features, species categories and their related evolutionary distance, several limitations should be mentioned. These limitations are both theoretical and methodological. First, we cannot rule out the fact that including more primate species in our set of stimuli would not have influenced the results. In fact, even though our species categories were specifically chosen for this task, the inclusion of vocalizations from other great apes—such as gorillas or orangutans—would have broadened the scope of our results. Related to this aspect, we can also mention that tackling primate phylogeny, which spans over millions of years, with only four species restricts the possible inference based on our results. Second, we observed improved sensitivity of our data by the use of more sophisticated acoustic modeling, namely the inclusion of both between-species acoustic distance and of the most discriminant acoustic features in the functional imaging data. However, we did not include as stimuli—or in a control task—the synthesized acoustic parameters of interest, for instance by using species-specific F0 contour or its spectral content in other neutral, comparable auditory stimuli. We cannot therefore <italic>completely</italic> rule out that such task would not trigger brain activations that overlap with our results—although such data would not be mutually exclusive with our data and interpretation. Future work should therefore address with the greatest level of detail the specific question of acoustics in primate vocalization processing, in addition to adding more—as well as synthesized—stimuli from other great ape species. The origin of these acoustic differences should also be investigated, since we can assume that these differences originated at least partially from evolutionary processes as well as survival and adaptation mechanisms. Finally, individual differences in the processing and preference of one species over another or over all the others cannot be ruled out, even though we provide evidence that attentional effects toward the vocalizations of a specific species did likely not exist in our data. Therefore, individual differences should be assessed in more detail in the future, with the inclusion of participant-level covariates such as questionnaire scores assessing the familiarity with primate vocalizations or the hedonic value of these vocalizations for each individual. Among the more general limitations of nonhuman primate neuroscience lies the fact that more inclusive and large-scale collaborations would be needed. Such collaborations and framework would lead to a better study and understanding of primate neuroscience, and previous initiatives have recently been put forward in this direction [<xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c52">52</xref>].</p>
<p>Taken together, our data suggest that phylogeny-driven specific acoustic features appear to be necessary to trigger cross-species activity in the human temporal voice areas—especially in subregions of the TVA in which increased activity underlies voice signals compared to animal and nature sounds. We provide evidence for specific anterior and mid TVA subregions that underlie the processing of the calls of one of our closest relatives, namely chimpanzees but also of macaques, respectively. In line with recently reported literature, we contend that the human TVA are also involved in the processing of heterospecific primate vocalizations, provided they exhibit sufficient phylogenetic and especially spectro-temporal acoustic proximity to the human voice; as such, we predict that other similarities will be uncovered in the processing of human and nonhuman primate communicative signals. Finally, our results support a critical evolutionary continuity between the structure of human and chimpanzee vocalizations, possibly reflecting one of their common ancestors, as opposed to bonobo vocalizations that underwent more recent and critical changes within the last 1-2 million years. In contrast, the chimpanzee vocal system may be closer to the one of the common ancestors of humans and chimpanzees, as shown by the conserved activation in the human modern brain.</p>
</sec>
<sec id="s4">
<title>Material and Methods</title>
<sec id="s4a">
<title>Species categorization task</title>
<sec id="s4a1">
<title>Participants</title>
<p>Twenty-five right-handed, healthy, either native or highly proficient French-speaking participants took part in the study. One participant was excluded because he had no correct response at all and may have fallen asleep, while another participant was excluded due to incomplete scanning and technical issues at the MRI scanner, leaving us with twenty-three participants (10 female, 13 male, mean age 24.65 years, SD 3.66). With this sample size and our study design, we achieved a power of 75.12% for a between-means comparison with Effect size dz=0.5 and alpha=0.05 as calculated in G*Power version 3.1.9.7 [<xref ref-type="bibr" rid="c53">53</xref>]. All participants were naive to the experimental design and study, had normal or corrected-to-normal vision, normal hearing and no history of psychiatric or neurologic incidents. Participants gave written informed consent for their participation in accordance with ethical and data security guidelines of the University of Geneva. The study was approved by the Ethics Cantonal Commission for Research of the Canton of Geneva, Switzerland (CCER) and was conducted according to the Declaration of Helsinki.</p>
</sec>
<sec id="s4a2">
<title>Stimuli</title>
<p>Seventy-two vocalizations of four primate species (human, chimpanzee, bonobo, and rhesus macaque) were used in this study (see <xref rid="fig1" ref-type="fig">Fig.1<bold>A</bold></xref>). The eighteen selected chimpanzee, bonobo and rhesus macaque vocalizations contained single calls or call sequences produced by 6 to 8 different individuals in agonistic (threat, distress) or affiliative (‘positive’) social contexts. These were randomly selected—in a between-participants fashion—among our full database of primate stimuli, containing specifically: 15 chimpanzee individuals (recorded in the wild in the Budongo forest, Uganda), 10 bonobo individuals (recorded in the wild in the Salonga national park, Democratic Republic of Congo, DRC) and 16 macaque individuals (recorded in the wild from semi-free monkeys on Cayo Santiago, Puerto Rico). We then selected eighteen human voices obtained from a nonverbal validated stimuli set of Belin and collaborators [<xref ref-type="bibr" rid="c54">54</xref>], which were expressed by two male and two female adults expressing positive or negative social interactions. All vocal stimuli were standardized to 750 milliseconds using PRAAT (<ext-link ext-link-type="uri" xlink:href="https://www.praat.org">www.praat.org</ext-link>) but were not normalized in any way in order to preserve the naturalness of the sounds [<xref ref-type="bibr" rid="c55">55</xref>] and to allow for low-level acoustic parameters to be used in neuroimaging data modelling.</p>
</sec>
<sec id="s4a3">
<title>Experimental procedure and paradigm</title>
<p>Laying comfortably in a 3T scanner, participants listened to a total of seventy-two stimuli pseudo-randomized and played binaurally using MRI compatible earphones at 70 dB SPL (Model ‘S14’, Sensimetrics Corporation, Gloucester, MA, USA). At the beginning of the experiment, participants were instructed to identify the species that expressed the vocalizations using a keyboard. For instance, the instructions could be “Human – press 1, Chimpanzee – press 2, Bonobo – press 3 or Macaque – press 4”. The pressed keys were pseudo-randomly assigned across participants (response box: fORP, Cortech Solutions, Inc., Wilmington, NC, USA). In a 3-5 second interval (jittering of 400 ms) after each stimulus, participants were asked to categorize the species. If the participant did not respond during this interval, the next stimulus followed automatically. See <xref rid="fig1" ref-type="fig">Fig.1<bold>A</bold></xref> for a detailed illustration of the paradigm.</p>
</sec>
</sec>
<sec id="s4b">
<title>Temporal voice areas localizer task</title>
<sec id="s4b1">
<title>Participants</title>
<p>Two independent samples of participants performed the task while undergoing fMRI scanning: the sample of this study (10 female, 13 male, mean age 24.65 years, SD 3.66), leading to the delineation of sample-specific TVA; an independent sample of ninety-eight right-handed, healthy, either native or highly proficient French-speaking participants (52 female, 46 male, mean age 24.66 years, SD 4.97) leading to the delineation of more general and representative TVA. All participants were naive to the experimental design and study, had normal or corrected-to-normal vision, normal hearing and no history of psychiatric or neurologic incidents. Participants gave written informed consent for their participation in accordance with ethical and data security guidelines of the University of Geneva. The study was approved by the Ethics Cantonal Commission for Research of the Canton of Geneva, Switzerland (CCER) and was conducted according to the current regulations in Switzerland.</p>
</sec>
<sec id="s4b2">
<title>Stimuli and paradigm</title>
<p>Auditory stimuli consisted of sounds from a variety of sources [<xref ref-type="bibr" rid="c2">2</xref>]. Vocal stimuli were obtained from 47 speakers: 7 babies, 12 adults, 23 children and 5 older adults. Stimuli included 20 blocks of vocal sounds and 20 blocks of non-vocal sounds. Vocal stimuli within a block could be either speech 33%: words, non-words, foreign language or non-speech 67%: laughs, sighs, various onomatopoeia. Non-vocal stimuli consisted of natural sounds 14%: wind, streams, animals, 29%: cries, gallops, the human environment, 37%: cars, telephones, airplanes or musical instruments, 20%: bells, harp, instrumental orchestra. The paradigm, design and stimuli were obtained through the Voice Neurocognition Laboratory website (<ext-link ext-link-type="uri" xlink:href="http://vnl.psy.gla.ac.uk/resources.php">http://vnl.psy.gla.ac.uk/resources.php</ext-link>). Stimuli were presented through earphones (Model ‘S14’, Sensimetrics Corporation, Gloucester, MA, USA) at an intensity that was kept constant throughout the experiment 70 dB sound-pressure level. Participants were instructed to actively listen to the sounds. The silent inter-block interval was 8 s long.</p>
</sec>
</sec>
<sec id="s4c">
<title>Exogenous attention task using species as cues</title>
<p>This task was performed to control for potential attentional biases or specific salience of one species compared to the others. This task was therefore a control task and the results are reported in Fig.S1. The task was designed according to work on attention orienting following vocal material presentation [<xref ref-type="bibr" rid="c56">56</xref>]. More specifically, a cue is first presented and is quickly followed by the presentation of a neutral target to detect. The detection of this target can reliably be delayed or accelerated depending on the nature of the cue. In this control study, the cue corresponded to the stimuli of the main species categorization task followed by a sine wave tone (a ‘bip’) that had to be detected as fast as possible (the target). Any specific attentional bias of a species would therefore trigger reaction times differences for target detection, while no differences would invalidate any attentional effect or increased salience linked to a specific species. We did not observe any difference between cue species in this task (χ<sup>2</sup>(2)=3.33, <italic>p</italic>=0.34, Fig.S1).</p>
<sec id="s4c1">
<title>Participants</title>
<p>Twenty-eight participants (independent of the samples presented so far) took part in this behavioral study (15 female, 13 male, mean age 22.63 years, SD 5.00). With this sample size and our study design, we achieved a power of 82.48% for a between-means comparison with Effect size dz=0.5 and alpha=0.05 as calculated in G*Power version 3.1.9.7 [<xref ref-type="bibr" rid="c53">53</xref>]. All participants were naive to the experimental design and study, had normal or corrected-to-normal vision, normal hearing and no history of psychiatric or neurologic incidents. Participants gave written informed consent for their participation in accordance with ethical and data security guidelines of the University of Geneva. The study was approved by the Ethics committee of the University of Geneva (Switzerland), Department of Psychology and Educational Sciences, and was conducted according to the Declaration of Helsinki.</p>
</sec>
<sec id="s4c2">
<title>Stimuli and paradigm</title>
<p>The cues of the task were the species stimuli (human voice, chimpanzee, bonobo and macaque calls) used in the species categorization task above. The target ‘bip’ was a sine wave tone created using Matlab (Matlab 2020a, The Mathworks, Inc., Natick, MA, USA) with a wave frequency of 600Hz, a fade-in and fade-out of 10 ms each and a total duration of 100 ms. The auditory material was presented through headphones (Sennheiser HD-25 II, Sennheiser electronic SE &amp; Co. KG, Germany) at a constant sound-pressure level of 70dB. The procedure unfolded as follows, on a computer with a light grey screen background: for each trial (N=172), a cue (human voice, chimpanzee, bonobo or macaque call) was presented for 750 ms while a black fixation cross was presented at the center of the screen. Following a jittered blank screen of duration 100 to 250 ms (in steps of 50 ms), the target bip was presented for 100 ms. Right at the end of the presented of the bip, the fixation cross turned white, indicating that the response screen had started and that a response was expected as fast and accurately as possible as instructed, by using the ‘space’ key of the keyboard. A varying inter-trial interval of 1 s to 2.5 s was used (in steps of 500 ms). Among the total of 172 trials, there were 24 trials per species (12 in agonistic and 12 in affiliative social contexts) for a total of 96 stimuli (4 species * 24 trials=96) that were each presented twice (N=96*2=172).</p>
</sec>
</sec>
<sec id="s4d">
<title>Behavioral data analysis</title>
<sec id="s4d1">
<title>Species categorization task</title>
<sec id="s4d1a">
<title>Accuracy</title>
<p>Behavioral data were exclusively used to exclude participants who had below chance level categorization of human voices. Therefore, data from twenty-three participants mentioned in the <italic>Species Categorization Task - Participants</italic> section above were analyzed using R studio software (R Studio team [<xref ref-type="bibr" rid="c57">57</xref>] Inc., Boston, MA, url: <ext-link ext-link-type="uri" xlink:href="http://www.rstudio.com/">http://www.rstudio.com/</ext-link>). These data can be found in a published article focused on decisional aspects in the frontal cortex—region-of-interest analysis and computational modelling of the probability of correct species categorization—using the same species stimuli as in this study [<xref ref-type="bibr" rid="c34">34</xref>]. Since behavioral data are not part of the questions of interest of this paper addressing neural correlates of the species-specific processing of vocalizations within the temporal voice areas in human participants, and since they are published elsewhere [<xref ref-type="bibr" rid="c34">34</xref>], these are not presented.</p>
</sec>
</sec>
<sec id="s4d2">
<title>Exogenous attention task</title>
<sec id="s4d2a">
<title>Reaction times</title>
<p>This study did not contain any good or bad response, and the dependent variable of interest was the reactions times to detect the target ‘bip’. In order to remove extreme values, we discarded for each participant the values below the 5<sup>th</sup> percentile and above the 95<sup>th</sup> percentile. In average, the number of trials per participants therefore went down from 172 to 165 (∼4.1% of trials removed). Data were then analyzed using R studio software (R Studio team [<xref ref-type="bibr" rid="c57">57</xref>] Inc., Boston, MA, url: <ext-link ext-link-type="uri" xlink:href="http://www.rstudio.com/">http://www.rstudio.com/</ext-link>) using linear mixed effects modeling of the lme4 package [<xref ref-type="bibr" rid="c58">58</xref>]. The formula was the following:
<disp-formula id="disp-eqn-1">
<graphic xlink:href="677258v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
in which <italic>RT</italic> is the reaction times (dependent variable), <italic>Species</italic> is the four species of the stimuli (human, chimpanzee, bonobo, macaque) interacting with the <italic>Context</italic> of production (affiliative, agonistic) (fixed effects). The random effects were: the random slope of the identity of each stimulus (<italic>StimID</italic>) as a function of participants (<italic>ParticipantID</italic>), participant gender (<italic>Gender</italic>) and age (<italic>Age</italic>). This model explained 57.54% of the variance in the data (R2c).</p>
<p>There was no significant effect of Species (χ<sup>2</sup>(3)=3.33, <italic>p</italic>=.34), a significant effect of Context (χ<sup>2</sup>(2)=11.39, <italic>p</italic>&lt;.01) and no interaction between Species and Context (χ<sup>2</sup>(6)=3.06, p=.80). The effect of Context was explained by slower reaction times for affiliative than agonistic vocalizations, independent of Species (χ<sup>2</sup>(1)=6.083.33, <italic>p</italic>&lt;.05). Descriptive statistics per Species for the reaction times were the following: Human, mean=207.28, SD=118.43; Chimpanzee, mean=211.31, SD=127.07; Bonobo, mean=209.45, SD=151.85; Macaque, mean=214.05, SD=117.84. Descriptive statistics per Context for the reaction times were: Affiliative, mean=215.03, SD=115.41; Agonistic, mean=208.16, SD=135.39. See illustration in Fig.S1.</p>
</sec>
</sec>
</sec>
<sec id="s4e">
<title>Acoustic analysis of the vocalizations</title>
<sec id="s4e1">
<title>Mahalanobis acoustic distance (Neuroimaging model 2)</title>
<p>To quantify the impact of acoustic similarities in human recognition of affective vocalizations of other primates, we extracted 88 acoustic parameters from all vocalizations using the extended Geneva Acoustic parameters set defined as the optimal acoustic indicators related to voice analysis (GeMAPS [<xref ref-type="bibr" rid="c59">59</xref>]). This open-source set of acoustical parameters was selected based on: i) their potential to index affective physiological changes in voice production, ii) their proven value in former studies as well as their automatic extractability, and iii) their theoretical significance. GeMAPS relies on an automatic extraction system, which therefore automatically extracts an acoustic parameter set from an audio file—in an unsupervised, minimalistic manner. Then, to assess the acoustic distance between vocalizations of all species, we ran a General Discriminant Analysis model (GDA). More precisely, we used the 88 acoustical parameters in a GDA in order to discriminate our stimuli based on the different species (human, chimpanzee, bonobo, and rhesus macaque). Among these 88 acoustical parameters, we excluded those that were strongly correlating—i.e., with correlation scores r&gt;.90—to avoid redundancy and minimize multicollinearity. Following this selection process and the GDA, we eventually retained 16 acoustic parameters (Table S1).</p>
<p>We subsequently computed multidimensional Mahalanobis distances to classify the 72 stimuli on these selected acoustical features. A Mahalanobis distance is a generalized pattern analysis comparing the distance of each vocalization from the centroids of the different species vocalizations. This analysis allowed us to obtain an acoustical distance matrix used to test how the acoustical distances were differentially related to the different species (see <xref rid="fig1" ref-type="fig">Fig.1<bold>B</bold></xref>) and we used it as a covariate of no-interest in neuroimaging model 2. Using a one-way ANOVA with the Distance as the dependent variables and the Species as independent variable, the main effect of Species was significant (F(3,88)=15.84, <italic>p</italic>&lt;.001). All between-species differences were significant (.01&lt;<italic>p</italic>&lt;.001; see <xref rid="fig1" ref-type="fig">Fig.1</xref> and Table S2).</p>
<p>These data are the topic of a publication about the impact of acoustic parameters on the recognition of the affective cues of primate vocalizations by human participants [<xref ref-type="bibr" rid="c30">30</xref>]. All details are described in this article for the present stimuli.</p>
</sec>
<sec id="s4e2">
<title>Most discriminant low-level acoustic parameters (Neuroimaging model 3)</title>
<p>Following the GDA on the 88 acoustic parameters of the species stimuli presented above, we decided to use as covariates of no-interest the most discriminant low-level acoustic features of our stimuli to maximize brain activations that are independent of these features. We therefore included the most significant acoustic features ([r&gt;.70] and [r&lt;-.70]) of the first three factors of the GDA, that explained 27.14%, 21.63% and 18.99% of the variance, respectively [<xref ref-type="bibr" rid="c30">30</xref>]. Such selection left us with the following acoustic features: Factor 1, (1) vocalization loudness (r=0.92), (2) intensity (r=0.87), (3) change in spectrum (r=0.72); Factor 2, (4) bandwidth contour of the second formant (F2; r=0.79); Factor 3, (5) power of the fundamental frequency (F0; r=0.80) and finally (6) the difference in intensity contour (r=-0.71). The acoustic parameters were used as covariates of no-interest (N=6) in that specific order—namely, from the highest to lowest factor saturation—in neuroimaging model 3.</p>
<p>Again, all details of this analysis are described in detail in a dedicated article for the present stimuli [<xref ref-type="bibr" rid="c30">30</xref>] and the values are reported in Table S1.</p>
</sec>
</sec>
<sec id="s4f">
<title>Imaging data acquisition</title>
<sec id="s4f1">
<title>Species categorization task</title>
<p>Structural and functional brain imaging data were acquired by using a 3T scanner Siemens Trio, Erlangen, Germany with a 32-channel coil. A 3D GR\IR magnetization-prepared rapid acquisition gradient echo sequence was used to acquire high-resolution (0.35 x 0.35 x 0.7 mm<sup>3</sup>) T1-weighted structural images (TR = 2400 ms, TE = 2.29 ms). Functional images were acquired by using fast fMRI, with a multislice echo planar imaging sequence with 79 transversal slices in descending order, slice thickness 3 mm, TR = 650 ms, TE = 30 ms, field of view = 205 x 205 mm2, 64 x 64 matrix, flip angle = 50 degrees, bandwidth 1562 Hz/Px. In total for this task, 636 functional volumes of 79 slices were acquired for each participant for a total of 50244 slices per participant. For our whole sample of twenty-three participants, 14628 volumes were acquired for a grand total of 1’155’612 slices.</p>
</sec>
<sec id="s4f2">
<title>Temporal voice areas localizer task</title>
<p>Structural and functional brain imaging data were acquired by using a 3T scanner Siemens Trio, Erlangen, Germany with a 32-channel coil. A magnetization-prepared rapid acquisition gradient echo sequence was used to acquire high-resolution (1 x 1 x 1 mm<sup>3</sup>) T1-weighted structural images TR = 1,900 ms, TE = 2.27 ms, TI = 900 ms. Functional images were acquired by using a multislice echo planar imaging sequence with 36 transversal slices in descending order, slice thickness 3.2 mm, TR = 2,100 ms, TE = 30 ms, field of view = 205 x 205 mm2, 64 x 64 matrix, flip angle = 90°, bandwidth 1562 Hz/Px. In total for this task, 230 functional volumes of 36 slices were acquired for each participant for a total of 8280 slices per participant. For our sample of ninety-eight participants, 22’540 volumes were acquired for a grand total of 811’440 slices. For the sample-specific data (N=23), 5290 volumes were acquired for a grand total of 190’440 slices.</p>
</sec>
</sec>
<sec id="s4g">
<title>Wholebrain data analysis</title>
<sec id="s4g1">
<title>Species categorization task analysis within the temporal voice areas</title>
<p>Functional images were analyzed with Statistical Parametric Mapping software (SPM12, Wellcome Trust Centre for Neuroimaging, London, UK). Preprocessing steps included realignment to the first volume of the time series, slice timing, normalization into the Montreal Neurological Institute [<xref ref-type="bibr" rid="c60">60</xref>] (MNI) space using the DARTEL toolbox [<xref ref-type="bibr" rid="c61">61</xref>] and spatial smoothing with an isotropic Gaussian filter of 8 mm full width at half maximum. To remove low-frequency components, we used a high-pass filter with a cutoff frequency of 1/128Hz. Three general linear models were used to compute first-level statistics, in which each event was modeled by using a boxcar function and was convolved with the hemodynamic response function, time-locked to the onset of each stimulus. In model 1, separate regressors were created for all trials of each species (Species factor: human, chimpanzee, bonobo, macaque vocalizations) and two covariates of no-interest each (mean fundamental frequency and mean energy of each species) for a total of 12 regressors. Finally, six motion parameters were included as regressors of no interest to account for movement in the data and our design matrix therefore included a total of 18 columns plus the constant term. The species regressors were used to compute simple contrasts for each participant, leading to separate main effects of human, chimpanzee, bonobo, and macaque vocalizations. Covariates were set to zero in order to model them as no-interest regressors. In model 2, separate regressors were created for all trials of each species (Species factor: human, chimpanzee, bonobo, macaque vocalizations) and one covariate of no-interest for each species (acoustic distance for each species relative to human voice stimuli) for a total of 8 regressors. Finally, six motion parameters were included as regressors of no interest to account for movement in the data and our design matrix therefore included a total of 14 columns plus the constant term. The species regressors were used to compute simple contrasts for each participant, leading to separate main effects of human, chimpanzee, bonobo, and macaque vocalizations excluding acoustic distance (the covariate was set to zero in order to model it as ‘of no-interest’). In model 3, separate regressors were created for all trials of each species (Species factor: human, chimpanzee, bonobo, macaque vocalizations) and six covariates of no-interest each (vocalization loudness, intensity, change in spectrum, bandwidth contour of the second formant (F2), power of the fundamental frequency (F0) and finally the difference in intensity contour) for a total of 28 regressors. Finally, six motion parameters were included as regressors of no-interest to account for movement in the data and our design matrix therefore included a total of 34 columns plus the constant term. The species regressors were used to compute simple contrasts for each participant, leading to separate main effects of human, chimpanzee, bonobo, and macaque vocalizations. Covariates were set to zero in order to model them as no-interest regressors.</p>
<p>For each first-level model, each of their respective four simple contrasts were then taken to two flexible factorial second-level analyses. For all of these second-level analyses there were two factors: Participants factor (independence set to yes, variance set to unequal) and the Species factor (independence set to no, variance set to unequal). For these analyses and to be consistent, we only included participants who were above chance level (25%) in the species categorization task (N=23). Brain region labelling was defined using xjView toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.alivelearn.net/xjview">http://www.alivelearn.net/xjview</ext-link>) implementing the Automated anatomical labelling (‘aal3’) atlas [<xref ref-type="bibr" rid="c62">62</xref>]. All neuroimaging activations were thresholded in SPM12 by using a voxelwise false discovery rate (FDR) correction at <italic>p</italic>&lt;.05 and an arbitrary cluster extent of k&gt;10 voxels to remove very small clusters of activity. The TVA highlighted in the figures are therefore only visual outlines of these regions, but no region-of-interest (ROI) analysis was performed here in order to maximize data representativeness—which is impacted negatively by ROI analysis.</p>
</sec>
<sec id="s4g2">
<title>Temporal voice areas localizer task</title>
<p>Functional images were analyzed with Statistical Parametric Mapping software (SPM12, Wellcome Trust Centre for Neuroimaging, London, UK). Preprocessing steps included realignment to the first volume of the time series, slice timing, normalization into the Montreal Neurological Institute [<xref ref-type="bibr" rid="c60">60</xref>] (MNI) space using the DARTEL toolbox [<xref ref-type="bibr" rid="c61">61</xref>] and spatial smoothing with an isotropic Gaussian filter of 8 mm full width at half maximum. To remove low-frequency components, we used a high-pass filter with a cutoff frequency of 1/128Hz. A general linear model was used to compute first-level statistics, in which each block was modeled by using a block function and was convolved with the hemodynamic response function, time-locked to the onset of each block. Separate regressors were created for each condition (vocal and non-vocal; condition factor). Finally, six motion parameters were included as regressors of no interest to account for movement in the data. The condition regressors were used to compute simple contrasts for each participant, leading to a main effect of vocal and non-vocal at the first-level of analysis: [1 0] for vocal, [0 1] for non-vocal. These simple contrasts were then taken to a flexible factorial second-level analysis in which there were two factors: Participants factor (independence set to yes, variance set to unequal) and the Condition factor (independence set to no, variance set to unequal). An identical analysis architecture was used to delineate more specific subregions of the TVA according to the type of non-vocal material, with categories: animal sounds, music, nature sounds, artificial noise sounds. Timing onsets of these newly created “events”—including their duration—within each non-vocal block of the task were determined and each main effect contrast computed at the first-level was then taken to a second-level flexible factorial analysis with settings identical to the above. All neuroimaging activations were thresholded in SPM12 by using a voxelwise false discovery rate (FDR) correction at <italic>p</italic>&lt;.05 and an arbitrary cluster extent of k&gt;10 voxels to remove very small clusters of activity. Activation outline for vocal &gt; non-vocal—the contrast revealing the TVA—was precisely delineated for the N=98 and the N=23 samples and overlaid on brain displays of the species categorization task (see Fig.S8). TVA subregions according to the auditory material (specific non-vocal condition blocks material) are reported in Fig.S9.</p>
</sec>
</sec>
</sec>

</body>
<back>
<sec id="s7" sec-type="data-availability">
<title>Data availability statement</title>
<p>All data, stimuli and codes used in this article will be made available in the FAIR-compliant open repository YARETA (URL: <ext-link ext-link-type="uri" xlink:href="https://yareta.unige.ch/specific-folder-here-upon-acceptance">https://yareta.unige.ch/specific-folder-here-upon-acceptance</ext-link>).</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank the Swiss National Science foundation (SNSF) for supporting this interdisciplinary project (grant CR13I1_162720 / 1 to DG-TG), the National Centre of Competence in Research (NCCR) (51NF40-104897 to DG) hosted by the Swiss Center for Affective Sciences, as well as the Fondation Ernst et Lucie Schmidheiny supporting CD. TG was additionally supported by a grant of the SNSF during the final editing of this article (grant PCEFP1_186832). We also thank Katie Slocombe and Zanna Clay for providing the nonhuman primates auditory stimuli and Daphne Bavelier for her advice on the design of the species categorization task. We would like also to acknowledge the staff of the Brain and Behavior Laboratory at the University of Geneva where all data were acquired.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author contributions</title>
<p>LC designed the task, acquired part of the data, analyzed the data, designed the figures, wrote and edited the manuscript. CD designed the task, acquired the data, analyzed the data, wrote and edited parts of the manuscript. TG provided theoretical background and edited the manuscript. DG helped design the task, the analyses and edited the manuscript.</p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary figures and tables</label>
<media xlink:href="supplements/677258_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ogawa</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Brain magnetic resonance imaging with contrast dependent on blood oxygenation</article-title>. <source>proceedings of the National Academy of Sciences</source>, <year>1990</year>. <volume>87</volume>(<issue>24</issue>): p. <fpage>9868</fpage>–<lpage>9872</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Voice-selective areas in human auditory cortex</article-title>. <source>Nature</source>, <year>2000</year>. <volume>403</volume>(<issue>6767</issue>): p. <fpage>309</fpage>–<lpage>312</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aglieri</surname>, <given-names>V.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Functional connectivity within the voice perception network and its behavioural relevance</article-title>. <source>NeuroImage</source>, <year>2018</year>. <volume>183</volume>: p. <fpage>356</fpage>–<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegstein</surname>, <given-names>K.V.</given-names></string-name> and <string-name><given-names>A.-L.</given-names> <surname>Giraud</surname></string-name></person-group>, <article-title>Distinct functional substrates along the right superior temporal sulcus for the processing of voices</article-title>. <source>Neuroimage</source>, <year>2004</year>. <volume>22</volume>(<issue>2</issue>): p. <fpage>948</fpage>–<lpage>955</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pernet</surname>, <given-names>C.R.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices</article-title>. <source>Neuroimage</source>, <year>2015</year>. <volume>119</volume>: p. <fpage>164</fpage>–<lpage>174</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frühholz</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Neural oscillations in human auditory cortex revealed by fast fMRI during auditory perception</article-title>. <source>NeuroImage</source>, <year>2020</year>. <volume>207</volume>: p. <fpage>116401</fpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latinus</surname>, <given-names>M.</given-names></string-name> and <string-name><given-names>P.</given-names> <surname>Belin</surname></string-name></person-group>, <article-title>Human voice perception</article-title>. <source>Current Biology</source>, <year>2011</year>. <volume>21</volume>(<issue>4</issue>): p. <fpage>R143</fpage>–<lpage>R145</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zäske</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>B.A.S.</given-names> <surname>Hasan</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Belin</surname></string-name></person-group>, <article-title>It doesn’t matter what you say: FMRI correlates of voice learning and recognition independent of speech content</article-title>. <source>cortex</source>, <year>2017</year>. <volume>94</volume>: p. <fpage>100</fpage>–<lpage>112</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roswandowitz</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Obligatory and facultative brain regions for voice-identity recognition</article-title>. <source>Brain</source>, <year>2017</year>. <volume>141</volume>(<issue>1</issue>): p. <fpage>234</fpage>–<lpage>247</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lahnakoski</surname>, <given-names>J.M.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Naturalistic FMRI mapping reveals superior temporal sulcus as the hub for the distributed brain network for social perception</article-title>. <source>Frontiers in human neuroscience</source>, <year>2012</year>. <volume>6</volume>: p. <fpage>233</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ethofer</surname>, <given-names>T.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Emotional voice areas: anatomic location, functional properties, and structural connections revealed by combined fMRI/DTI</article-title>. <source>Cerebral cortex</source>, <year>2012</year>. <volume>22</volume>(<issue>1</issue>): p. <fpage>191</fpage>–<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Witteman</surname>, <given-names>J.</given-names></string-name>, <string-name><given-names>V.J.</given-names> <surname>Van Heuven</surname></string-name>, and <string-name><given-names>N.O.</given-names> <surname>Schiller</surname></string-name></person-group>, <article-title>Hearing feelings: a quantitative meta-analysis on the neuroimaging literature of emotional prosody perception</article-title>. <source>Neuropsychologia</source>, <year>2012</year>. <volume>50</volume>(<issue>12</issue>): p. <fpage>2752</fpage>–<lpage>2763</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latinus</surname>, <given-names>M.</given-names></string-name>, <string-name><given-names>F.</given-names> <surname>Crabbe</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Belin</surname></string-name></person-group>, <article-title>Learning-induced changes in the cerebral processing of voice identity</article-title>. <source>Cerebral Cortex</source>, <year>2011</year>. <volume>21</volume>(<issue>12</issue>): p. <fpage>2820</fpage>–<lpage>2828</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latinus</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Norm-based coding of voice identity in human auditory cortex</article-title>. <source>Current Biology</source>, <year>2013</year>. <volume>23</volume>(<issue>12</issue>): p. <fpage>1075</fpage>–<lpage>1080</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Cerebral processing of voice gender studied using a continuous carryover fMRI design</article-title>. <source>Cerebral Cortex</source>, <year>2013</year>. <volume>23</volume>(<issue>4</issue>): p. <fpage>958</fpage>–<lpage>966</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grossmann</surname>, <given-names>T.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>The developmental origins of voice processing in the human brain</article-title>. <source>Neuron</source>, <year>2010</year>. <volume>65</volume>(<issue>6</issue>): p. <fpage>852</fpage>–<lpage>858</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kisilevsky</surname>, <given-names>B.S.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Effects of experience on fetal voice recognition</article-title>. <source>Psychological science</source>, <year>2003</year>. <volume>14</volume>(<issue>3</issue>): p. <fpage>220</fpage>–<lpage>224</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hüppi</surname>, <given-names>P.S</given-names></string-name></person-group>., <article-title>Cortical development in the fetus and the newborn: advanced MR techniques</article-title>. <source>Topics in Magnetic Resonance Imaging</source>, <year>2011</year>. <volume>22</volume>(<issue>1</issue>): p. <fpage>33</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andics</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Voice-sensitive regions in the dog and human brain are revealed by comparative fMRI</article-title>. <source>Current Biology</source>, <year>2014</year>. <volume>24</volume>(<issue>5</issue>): p. <fpage>574</fpage>–<lpage>578</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrodin</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Voice cells in the primate temporal lobe</article-title>. <source>Current Biology</source>, <year>2011</year>. <volume>21</volume>(<issue>16</issue>): p. <fpage>1408</fpage>–<lpage>1415</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petkov</surname>, <given-names>C.I.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>A voice region in the monkey brain</article-title>. <source>Nature neuroscience</source>, <year>2008</year>. <volume>11</volume>(<issue>3</issue>): p. <fpage>367</fpage>–<lpage>374</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fecteau</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Is voice processing species-specific in human auditory cortex? An fMRI study</article-title>. <source>Neuroimage</source>, <year>2004</year>. <volume>23</volume>(<issue>3</issue>): p. <fpage>840</fpage>–<lpage>848</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>P</given-names></string-name></person-group>., <article-title>Voice processing in human and non-human primates</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <year>2006</year>. <volume>361</volume>(<issue>1476</issue>): p. <fpage>2091</fpage>–<lpage>2107</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Human cerebral response to animal affective vocalizations</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>, <year>2008</year>. <volume>275</volume>(<issue>1634</issue>): p. <fpage>473</fpage>–<lpage>481</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fritz</surname>, <given-names>T.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Human behavioural discrimination of human, chimpanzee and macaque affective vocalisations is reflected by the neural response in the superior temporal sulcus</article-title>. <source>Neuropsychologia</source>, <year>2018</year>. <volume>111</volume>: p. <fpage>145</fpage>–<lpage>150</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Linnankoski</surname>, <given-names>I.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Recognition of emotions in macaque vocalizations by children and adults</article-title>. <source>Language &amp; Communication</source>, <year>1994</year>. <volume>14</volume>(<issue>2</issue>): p. <fpage>183</fpage>–<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bodin</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Functionally homologous representation of vocalizations in the auditory cortex of humans and macaques</article-title>. <source>Current Biology</source>, <year>2021</year>. <volume>31</volume>(<issue>21</issue>): p. <fpage>4839</fpage>–<lpage>4844. e4</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grawunder</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Higher fundamental frequency in bonobos is explained by larynx morphology</article-title>. <source>Current Biology</source>, <year>2018</year>. <volume>28</volume>(<issue>20</issue>): p. <fpage>R1188</fpage>–<lpage>R1189</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Slocombe</surname>, <given-names>K.E.</given-names></string-name> and <string-name><given-names>K.</given-names> <surname>Zuberbühler</surname></string-name></person-group>, <article-title>Chimpanzees modify recruitment screams as a function of audience composition</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <year>2007</year>. <volume>104</volume>(<issue>43</issue>): p. <fpage>17228</fpage>–<lpage>17233</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Debracque</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Humans recognize affective cues in primate vocalizations: acoustic and phylogenetic perspectives</article-title>. <source>Scientific reports</source>, <year>2023</year>. <volume>13</volume>(<issue>1</issue>): p. <fpage>10900</fpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gruber</surname>, <given-names>T.</given-names></string-name> and <string-name><given-names>Z.</given-names> <surname>Clay</surname></string-name></person-group>, <article-title>A comparison between bonobos and chimpanzees: A review and update</article-title>. <source>Evolutionary Anthropology: Issues, News, and Reviews</source>, <year>2016</year>. <volume>25</volume>(<issue>5</issue>): p. <fpage>239</fpage>–<lpage>252</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Titze</surname>, <given-names>I.</given-names></string-name>, <string-name><given-names>T.</given-names> <surname>Riede</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Mau</surname></string-name></person-group>, <article-title>Predicting achievable fundamental frequency ranges in vocalization across species</article-title>. <source>PLoS computational biology</source>, <year>2016</year>. <volume>12</volume>(<issue>6</issue>): p. <fpage>e1004907</fpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname>, <given-names>T.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Adult human perception of distress in the cries of bonobo, chimpanzee, and human infants</article-title>. <source>Biological Journal of the Linnean Society</source>, <year>2017</year>. <volume>120</volume>(<issue>4</issue>): p. <fpage>919</fpage>–<lpage>930</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ceravolo</surname>, <given-names>L.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Frontal mechanisms underlying primate calls recognition by humans</article-title>. <source>Cerebral Cortex Communications</source>, <year>2023</year>. <volume>4</volume>(<issue>4</issue>): p. tgad019.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Maesschalck</surname>, <given-names>R.</given-names></string-name>, <string-name><given-names>D.</given-names> <surname>Jouan-Rimbaud</surname></string-name>, and <string-name><given-names>D.L.</given-names> <surname>Massart</surname></string-name></person-group>, <article-title>The mahalanobis distance</article-title>. <source>Chemometrics and intelligent laboratory systems</source>, <year>2000</year>. <volume>50</volume>(<issue>1</issue>): p. <fpage>1</fpage>–<lpage>18</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrodin</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Natural asynchronies in audiovisual communication signals regulate neuronal multisensory interactions in voice-sensitive cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <year>2015</year>. <volume>112</volume>(<issue>1</issue>): p. <fpage>273</fpage>–<lpage>278</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engel</surname>, <given-names>L.R.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Different categories of living and non-living sound-sources activate distinct cortical networks</article-title>. <source>Neuroimage</source>, <year>2009</year>. <volume>47</volume>(<issue>4</issue>): p. <fpage>1778</fpage>–<lpage>1791</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewis</surname>, <given-names>J.W.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Distinct cortical pathways for processing tool versus animal sounds</article-title>. <source>Journal of neuroscience</source>, <year>2005</year>. <volume>25</volume>(<issue>21</issue>): p. <fpage>5148</fpage>–<lpage>5158</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simmons</surname>, <given-names>W.K.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>The selectivity and functional connectivity of the anterior temporal lobes</article-title>. <source>Cerebral Cortex</source>, <year>2010</year>. <volume>20</volume>(<issue>4</issue>): p. <fpage>813</fpage>–<lpage>825</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zahn</surname>, <given-names>R.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Social concepts are represented in the superior anterior temporal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <year>2007</year>. <volume>104</volume>(<issue>15</issue>): p. <fpage>6430</fpage>–<lpage>6435</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bestelmeyer</surname>, <given-names>P.E.</given-names></string-name>, <string-name><given-names>P.</given-names> <surname>Belin</surname></string-name>, and <string-name><given-names>M.-H.</given-names> <surname>Grosbras</surname></string-name></person-group>, <article-title>Right temporal TMS impairs voice detection</article-title>. <source>Current Biology</source>, <year>2011</year>. <volume>21</volume>(<issue>20</issue>): p. <fpage>R838</fpage>–<lpage>R839</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mellem</surname>, <given-names>M.S.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Sentence processing in anterior superior temporal cortex shows a social-emotional bias</article-title>. <source>Neuropsychologia</source>, <year>2016</year>. <volume>89</volume>: p. <fpage>217</fpage>–<lpage>224</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>P.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Bodin</surname></string-name>, and <string-name><given-names>V.</given-names> <surname>Aglieri</surname></string-name></person-group>, <article-title>A “voice patch” system in the primate brain for processing vocal information?</article-title> <source>Hearing research</source>, <year>2018</year>. <volume>366</volume>: p. <fpage>65</fpage>–<lpage>74</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giamundo</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>A population of neurons selective for human voice in the monkey brain</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <year>2024</year>. <volume>121</volume>(<issue>25</issue>): p. <fpage>e2405588121</fpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrodin</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Auditory and visual modulation of temporal lobe neurons in voice-sensitive and association cortices</article-title>. <source>Journal of Neuroscience</source>, <year>2014</year>. <volume>34</volume>(<issue>7</issue>): p. <fpage>2524</fpage>–<lpage>2537</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cauzinille</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Applying machine learning to primate bioacoustics: Review and perspectives</article-title>. <source>American Journal of Primatology</source>, <year>2024</year>. <volume>86</volume>(<issue>10</issue>): p. <fpage>e23666</fpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hare</surname>, <given-names>B.</given-names></string-name>, <string-name><given-names>V.</given-names> <surname>Wobber</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Wrangham</surname></string-name></person-group>, <article-title>The self-domestication hypothesis: evolution of bonobo psychology is due to selection against aggression</article-title>. <source>Animal Behaviour</source>, <year>2012</year>. <volume>83</volume>(<issue>3</issue>): p. <fpage>573</fpage>–<lpage>585</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perelman</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>A molecular phylogeny of living primates</article-title>. <source>PLoS Genet</source>, <year>2011</year>. <volume>7</volume>(<issue>3</issue>): p. <fpage>e1001342</fpage>.</mixed-citation></ref>
    <ref id="c49"><label>49.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Dewi</surname>, <given-names>S.P.</given-names></string-name>, <string-name><given-names>A.L.</given-names> <surname>Prasasti</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Irawan</surname></string-name></person-group>. <article-title>The study of baby crying analysis using MFCC and LFCC in different classification methods</article-title> <year>2019</year> <conf-name>IEEE International Conference on Signals and Systems (ICSigSys)</conf-name>.</mixed-citation></ref>
    <ref id="c50"><label>50.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Jagtap</surname>, <given-names>S.S.</given-names></string-name>, <string-name><given-names>P.K.</given-names> <surname>Kadbe</surname></string-name>, and <string-name><given-names>P.N.</given-names> <surname>Arotale</surname></string-name></person-group>. <article-title>System propose for Be acquainted with newborn cry emotion using linear frequency cepstral coefficient</article-title> <year>2016</year> <conf-name>International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)</conf-name>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartig</surname>, <given-names>R.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>A framework and resource for global collaboration in non-human primate neuroscience</article-title>. <source>Current research in neurobiology</source>, <year>2023</year>. <volume>4</volume>: p. <fpage>100079</fpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Milham</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Toward next-generation primate neuroscience: a collaboration-based strategic plan for integrative neuroimaging</article-title>. <source>Neuron</source>, <year>2022</year>. <volume>110</volume>(<issue>1</issue>): p. <fpage>16</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kang</surname>, <given-names>H</given-names></string-name></person-group>., <article-title>Sample size determination and power analysis using the G* Power software</article-title>. <source>Journal of educational evaluation for health professions</source>, <year>2021</year>. <volume>18</volume>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>P.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Fillion-Bilodeau</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Gosselin</surname></string-name></person-group>, <article-title>The Montreal Affective Voices: a validated set of nonverbal affect bursts for research on auditory affective processing</article-title>. <source>Behavior research methods</source>, <year>2008</year>. <volume>40</volume>(<issue>2</issue>): p. <fpage>531</fpage>–<lpage>539</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferdenzi</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Voice attractiveness: Influence of stimulus duration and type</article-title>. <source>Behavior research methods</source>, <year>2013</year>. <volume>45</volume>(<issue>2</issue>): p. <fpage>405</fpage>–<lpage>413</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ceravolo</surname>, <given-names>L.</given-names></string-name>, <string-name><given-names>S.</given-names> <surname>Frühholz</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Grandjean</surname></string-name></person-group>, <article-title>Modulation of auditory spatial attention by angry prosody: an fMRI auditory dot-probe study</article-title>. <source>Frontiers in neuroscience</source>, <year>2016</year>. <volume>10</volume>: p. <fpage>216</fpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>Team, R</collab></person-group>., <chapter-title>RStudio: integrated development for R</chapter-title>. <publisher-name>RStudio, Inc., Boston, MA</publisher-name> URL <ext-link ext-link-type="uri" xlink:href="http://www.rstudio.com">http://www.rstudio.com</ext-link>, <year>2015</year>. <volume>42</volume>: p. <fpage>14</fpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Bates</surname>, <given-names>D.</given-names></string-name></person-group>, <article-title>Fitting linear mixed-effects models using lme4</article-title>. <source>arXiv</source> preprint arXiv:<pub-id pub-id-type="arxiv">1406.5823</pub-id>, <year>2014</year>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eyben</surname>, <given-names>F.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing</article-title>. <source>IEEE transactions on affective computing</source>, <year>2015</year>. <volume>7</volume>(<issue>2</issue>): p. <fpage>190</fpage>–<lpage>202</lpage>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collins</surname>, <given-names>D.L.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Automatic 3D intersubject registration of MR volumetric data in standardized Talairach space</article-title>. <source>Journal of computer assisted tomography</source>, <year>1994</year>. <volume>18</volume>(<issue>2</issue>): p. <fpage>192</fpage>–<lpage>205</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ashburner</surname>, <given-names>J</given-names></string-name></person-group>., <article-title>A fast diffeomorphic image registration algorithm</article-title>. <source>Neuroimage</source>, <year>2007</year>. <volume>38</volume>(<issue>1</issue>): p. <fpage>95</fpage>–<lpage>113</lpage>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rolls</surname>, <given-names>E.T.</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Automated anatomical labelling atlas 3</article-title>. <source>Neuroimage</source>, <year>2020</year>. <volume>206</volume>: p. <fpage>116189</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108795.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study shows that regions of the human auditory cortex that respond strongly to voices are also sensitive to vocalizations from closely related primate species. The study is methodologically <bold>solid</bold>, though additional analyses - particularly those isolating the acoustic features that differentiate chimpanzee from bonobo calls - would further strengthen the conclusions. With additional analyses and discussions, the work has the potential to offer key insights into the evolutionary continuity of voice processing and would be of interest to researchers studying auditory processing and evolutionary neuroscience in general.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108795.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigates how human temporal voice areas (TVA) respond to vocalizations from nonhuman primates. Using functional MRI during a species-categorization task, the authors compare neural responses to calls from humans, chimpanzees, bonobos, and macaques while modeling both acoustic and phylogenetic factors. They find that bilateral anterior TVA regions respond more strongly to chimpanzee than to other nonhuman primate vocalizations, suggesting that these regions are sensitive not only to human voices but also to acoustically and evolutionarily related sounds.</p>
<p>The work provides important comparative evidence for continuity in primate vocal communication and offers a strong empirical foundation for modeling how specific acoustic features drive TVA activity.</p>
<p>Strengths:</p>
<p>­(1) Comparative scope: The inclusion of four primate species, including both great apes and monkeys, provides a rare and valuable cross-species perspective on voice processing.</p>
<p>­(2) Methodological rigor: Acoustic and phylogenetic distances are carefully quantified and incorporated into the analyses.</p>
<p>­(4) Neuroscientific significance: The finding of TVA sensitivity to chimpanzee calls supports the view that human voice-selective regions are evolutionarily tuned to certain acoustic features shared across primates.</p>
<p>­(4) Clear presentation: The study is well organized, the stimuli well controlled, and the imaging analyses transparent and replicable.</p>
<p>­(5) Theoretical contribution: The results advance understanding of the neural bases of voice perception and the evolutionary roots of voice sensitivity in the human brain.</p>
<p>Weaknesses:</p>
<p>­(1) Acoustic-phylogenetic confound: The design does not fully disentangle acoustic similarity from phylogenetic proximity, as species co-vary along both dimensions. A promising way to address this would be to include an additional model focusing on the acoustic features that specifically differentiate bonobo from chimpanzee calls, which share equal phylogenetic distance to humans.</p>
<p>­(2) Selectivity vs. sensitivity: Without non-vocal control sounds, the study cannot determine whether TVA responses reflect true selectivity for primate vocalizations or general auditory sensitivity.</p>
<p>
­</p>
<p>
(3) Task demands: The use of an active categorization task may engage additional cognitive processes beyond auditory perception; a passive listening condition would help clarify the contribution of attention and task performance.</p>
<p>­(4) Figures and presentation: Some results are partially redundant; keeping only the most representative model figure in the main text and moving others to the Supplementary Material would improve clarity.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108795.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigated how the human brain responds to vocalizations from multiple primate species, including humans, chimpanzees, bonobos, and rhesus macaques. The central finding - that subregions of the temporal voice areas (TVA), particularly in the bilateral anterior superior temporal gyrus, show enhanced responses to chimpanzee vocalizations - suggests a potential neural sensitivity to calls from phylogenetically close nonhuman primates.</p>
<p>Strengths:</p>
<p>The authors employed three analytical models to consistently demonstrate activation in the anterior superior temporal gyrus that is specific to chimpanzee calls. The methodology was logical and robust, and the results supporting these findings appear solid.</p>
<p>Weakness:</p>
<p>The interpretation of the findings in this paper regarding the evolutionary continuity of voice processing lacks sufficient evidence. A simple explanation is that the observed effects can be attributed to the similarity in low-level acoustic features, rather than effects specific to phylogenetically close species. The authors only tested vocalizations from three non-human primate species, other than humans. In this case, the species specificity of the effect does not fully represent the specificity of evolutionary relatedness.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108795.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Ceravolo et al. employed functional magnetic resonance imaging (fMRI) to examine how the temporal voice areas (TVA) in the human brain respond to vocalizations from different nonhuman primate species. Their findings reveal that the human TVA is not only responsible for human vocalizations but also exhibits sensitivity to the vocalizations of other primates, particularly chimpanzee vocalizations sharing acoustic similarities with human voices, which offers compelling evidence for cross-species vocal processing in the human auditory system. Overall, the study presents intellectually stimulating hypotheses and demonstrates methodological originality. However, the current findings are not yet solid enough to fully support the proposed claims, and the presentation could be enhanced for clarity and impact.</p>
<p>Strengths:</p>
<p>The study presents intellectually stimulating hypotheses and demonstrates methodological originality.</p>
<p>Weaknesses:</p>
<p>(1) The analysis of the fMRI data does not account for the participants' behavioral performance, specifically their reaction times (RTs) during the species categorization task.</p>
<p>(2) The figure organization/presentation requires significant revision to avoid confusion and redundancy.</p>
</body>
</sub-article>
</article>