<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">87719</article-id>
<article-id pub-id-type="doi">10.7554/eLife.87719</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87719.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A computationally informed comparison between the strategies of humans and rodents in visual object recognition</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Schnell</surname>
<given-names>Anna Elisabeth</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Leemans</surname>
<given-names>Maarten</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7038-9638</contrib-id>
<name>
<surname>Vinken</surname>
<given-names>Kasper</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Op de Beeck</surname>
<given-names>Hans</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Brain and Cognition &amp; Leuven Brain Institute</institution>, KU Leuven, Leuven, <country>Belgium</country></aff>
<aff id="a2"><label>2</label><institution>Department of Neurobiology, Harvard Medical School</institution>, Boston, Massachusetts, <country>United States of America</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Corresponding author, E-mail: <email>annaelisabeth.schnell@kuleuven.be</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-06-27">
<day>27</day>
<month>06</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP87719</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-04-17">
<day>17</day>
<month>04</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-03-15">
<day>15</day>
<month>03</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.15.532720"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Schnell et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Schnell et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-87719-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Many species are able to recognize objects, but it has been proven difficult to pinpoint and compare how different species solve this task. Recent research suggested to combine computational and animal modelling in order to obtain a more systematic understanding of task complexity and compare strategies between species. In the present study, we created a large multidimensional stimulus set and designed a visual categorization task partially based upon modelling with a convolutional deep neural network (cDNN). Experiments included rats (N = 11; 1115 daily sessions in total for all rats together) and humans (N = 50). Each species was able to master the task and generalize to a variety of new images. Nevertheless, rats and humans showed very little convergence in terms of which object pairs were associated with high and low performance, suggesting the use of different strategies. There was an interaction between species and whether stimulus pairs favoured early or late processing in a cDNN. A direct comparison with cDNN representations revealed that rat performance was best captured by late convolutional layers while human performance related more to the higher-up fully connected layers. These findings highlight the additional value of using a computational approach for the design of object recognition tasks. Overall, this computationally informed investigation of object recognition behaviour reveals a strong discrepancy in strategies between rodent and human vision.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://osf.io/9eqyz/">https://osf.io/9eqyz/</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Humans have (almost) no difficulty in invariant object recognition, the ability to recognize the same objects from different viewpoints or in different scenes (<xref ref-type="bibr" rid="c7">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="c36">Zoccolan, 2015</xref>). This ability is supported by the ventral visual stream, the so-called <italic>what stream</italic> (<xref ref-type="bibr" rid="c16">Logothetis &amp; Sheinberg, 1996</xref>). A question that is repeatedly addressed in vision studies is whether and how we can model this stream by means of animal models or computational models to further examine and quantify the representations along the ventral visual stream. Computationally, researchers have recently modelled this stream by using convolutional deep neural networks (cDNNs), as for example done by <xref ref-type="bibr" rid="c2">Avberšek and colleagues (2021)</xref>, <xref ref-type="bibr" rid="c4">Cadieu and colleagues (2014)</xref>, <xref ref-type="bibr" rid="c9">Duyck and colleagues (2021)</xref>, <xref ref-type="bibr" rid="c10">Güçlü &amp; Gerven (2015)</xref>, <xref ref-type="bibr" rid="c11">Kalfas and colleagues (2018)</xref>, <xref ref-type="bibr" rid="c12">Kar and colleagues (2019)</xref>, <xref ref-type="bibr" rid="c15">Kubilius and colleagues (2016)</xref>, <xref ref-type="bibr" rid="c24">Pospisil and colleagues (2018)</xref> and Vinken &amp; Op de Beeck (2021). Lately, the animal model of choice for vision studies has become the rodent model, motivated by the applicability of molecular and genetic tools rather than by the visual capabilities of rodents. Past studies have examined behavioural (<xref ref-type="bibr" rid="c1">Alemi-Neissi et al., 2013</xref>; <xref ref-type="bibr" rid="c6">De Keyser et al., 2015</xref>; <xref ref-type="bibr" rid="c8">Djurdjevic et al., 2018</xref>; <xref ref-type="bibr" rid="c25">Schnell et al., 2019</xref>; <xref ref-type="bibr" rid="c28">Tafazoli et al., 2012</xref>; Vermaercke &amp; Op de Beeck, 2012; <xref ref-type="bibr" rid="c34">Vinken et al., 2014</xref>; <xref ref-type="bibr" rid="c36">Zoccolan, 2015</xref>) (for a review see (<xref ref-type="bibr" rid="c36">Zoccolan, 2015</xref>)) as well as neural (<xref ref-type="bibr" rid="c17">Matteucci et al., 2019</xref>; <xref ref-type="bibr" rid="c29">Tafazoli et al., 2017</xref>; <xref ref-type="bibr" rid="c30">Vermaercke et al., 2014</xref>; <xref ref-type="bibr" rid="c33">Vinken et al., 2016</xref>) data of rodents (rats and mice) performing in visual pattern recognition tasks. The behavioural findings showed that rats are capable of learning complex visual discrimination tasks. Here we plan to integrate computational and animal modelling approaches, by using data about information processing in artificial neural networks when designing the animal experiments.</p>
<p>One aspect that almost all rodent studies have in common is that the exact task and stimuli are chosen based on what we know from human and monkey studies. Earlier research showed that the intuition of researchers about the complexity of visual tasks can be misleading (Vinken &amp; Op de Beeck, 2021). Through computational cDNN modelling of the tasks from previous studies, they showed that behavioural strategies that seem complex at first hand might be best modelled through relatively early levels of processing in cDNNs. They recommended that future studies could obtain more direct information about the complexity of visual tasks and behavioural strategies by incorporating neural network models in the design phase of the experiment. One way of implementing this is to train rodents in a challenging and multidimensional visual task and use cDNNs to select stimulus examples targeting strategies with different levels of complexity.</p>
<p>In the present study, we implemented this approach and created a large stimulus set that can be used for a variety of visual experiments. We decided to create the stimuli in a way that they are adaptable to different types of tasks, such as a “simple” categorization task or non-linear tasks (e.g. <xref ref-type="bibr" rid="c3">Bossens &amp; Op de Beeck, 2016</xref>). We then took a subset of these stimuli and performed a visual categorization experiment in rats (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for the design). The task itself was defined in a stimulus space with two dimensions, here referred to as concavity and alignment <bold>Error! Reference source not found.</bold>, and further complicated by transforming the stimuli along several dimensions that preserve the identity of the object. Once we trained the animals in a base stimulus pair, we used the identity-preserving transformations to test for generalization. After a number of transformation phases, we selected a final stimulus set by choosing a combination of transformations based on the outcomes of a trained cDNN. Using the neural network as a (basic) model for the different stages of ventral visual stream processing, we chose stimulus pairs that require either higher or lower levels of processing and thus allow us to maximally differentiate between the task strategies used by the animals. As a final part of the current study, we performed an online human experiment with the same stimuli and design as the experiment for the rats, providing us with a rich three-way comparison of rat behavioural data with human behavioural data and with cDNN data.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1</label>
<caption>
<title>The design of the animal study, including the stimuli.</title>
<p>Animals started with a standardized shaping procedure, followed by three training protocols, as indicated by the dashed outline. In these protocols, animals received real reward, i.e. reward for touching the target. They received correction trials for incorrect answers. After the three training protocols, the animals went through a number of testing protocols. The order of the first six protocols (*) and the last two testing protocols (**) was counterbalanced between the animals. During testing protocols, animals received one third old trials, and two third new trials. In the new trials, they received random reward in 80% of the trials whereas in the old trials, they received real reward and correction trials if necessary.</p>
</caption>
<graphic xlink:href="532720v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<p>In this study, we trained and tested 11 rats and 45 humans on a complex two-dimensional categorization task (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for the design of the rat study, and Supplemental Figure 6 for the design of the human study). Rats and humans were first trained in a base pair. Next we tested their ability to generalize across several image transformations. In the last two protocols of the design, we used a computational approach to select stimuli that require different visual strategies.</p>
<sec id="s2a">
<label>2.1</label>
<title>Animal study</title>
<sec id="s2a1">
<title>Training</title>
<p>We first checked the variation in performance across phases and stimulus pairs during training. In the first <italic>Training Phase</italic>, animals were trained in the base stimulus pair (maximally different target and distractor in the 4×4 stimulus grid). This training was successful for all twelve animals and lasted on average for 8.62 sessions (SD = 1.61). Animals were trained until they reached 80% performance for two consecutive sessions.</p>
<p>Once the animals were successfully trained, we examined whether they use both dimensions (concavity and alignment) by presenting them with two additional stimuli pairs where the target and distractor differ in only one dimension (see <xref ref-type="fig" rid="fig1">Figure 1</xref>, <italic>Dimension learning</italic>). Performance on the old pair was similar to training performance (85.83%). The animals performed well with the stimuli that differ only along the concavity dimension (78.79%), although it was significantly lower than the performance on the base pair (paired t-test on rat performance, t(11) = 3.77, p = 0.003). Performance dropped to 67.83% for the alignment-only pair, yet also significantly higher than chance level (one-sample t-test, p &lt; .0001). Overall, the <italic>Dimension learning</italic> protocol provides evidence that the animals have picked up each of the two dimensions. This finding already excludes trivial explanations in terms of simple visual dimensions. For example, while concavity is correlated with horizontal size (distractor wider) and with overall brightness (distractor brighter, thus the opposite relevance as in the shaping phase), these simple dimensions cannot explain above-chance performance on the alignment dimension.</p>
<p>The third training protocol consisted of a number of small transformations, as visualized in <xref ref-type="fig" rid="fig1">Figure 1</xref> (<italic>Transformations</italic>). The performance of the animals was not affected by these small transformations, with an average performance of 83.05% (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). The pairwise percentage matrix in <xref ref-type="fig" rid="fig2">Figure 2</xref> shows that the distractor with the Size transformation (most right column in the matrix) affected the rat performance the most.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption>
<title>Results of the Dimension learning and Transformations training protocols.</title>
<p>Each cell of the matrix indicates the average performance per stimulus pair, pooled over all animals. The columns represent the distractors, whereas the rows separate the targets. The colour bar indicates the performance correct.</p>
</caption>
<graphic xlink:href="532720v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The variation across targets and distractors can be due to a variety of factors. This can include simple dimensions such as brightness. In the base pair, the distractor is brighter than the target. While this is the opposite from the shaping task of detecting a shape versus a black screen, visual inspection of <xref ref-type="fig" rid="fig2">Figure 2</xref> suggests that the animals perform poorer on trials in which the distractor display is not so much brighter (e.g., when it is small). To quantify this effect of brightness, we calculated the correlation between the performances in the matrix and the difference in pixel values (and thus brightness) of the stimulus pairs. This resulted in a (Pearson) correlation of −0.59 (p &lt; 0.01), suggesting that there is indeed an effect of brightness. Yet, brightness is at best a partial explanation because all percentages in the matrix are above chance, with the lowest percentage in the matrix being 68.83%, even though in some pairs the difference in pixel values is abolished or even opposite from the base pair.</p>
<p>Overall the findings from the training phase and the above-chance performance on a variety of dimensions and transformations suggest that the rats have mastered a pattern classification task with a level of complexity that might be competitive with other tasks in the rodent literature.</p>
</sec>
<sec id="s2a2">
<title>Testing across transformations</title>
<p>The six protocols that test generalization to various transformations with new, untrained images are associated with performances lower than 80% (binomial test, see Supplemental Table 5 (lower table) for detailed table with results), but significantly higher than chance level (see Supplemental Table 5 (lower table)). The pairwise percentage matrices of the animals in <xref ref-type="fig" rid="fig3">Figure 3</xref> provide a more detailed view of what is happening in every test. The distractor has a higher impact on performance than the target in some tests. Supplemental Table 6 shows the marginal means and standard deviation for each target and distractor for these two test protocols. From these means it is clear that there is a higher variation in the performance between distractors in <italic>Rotation X</italic> (52%-65%) and <italic>Rotation Z</italic> (56%-73%) than between targets (55%-60% resp. 60%-66%). The same happens in the size test protocol.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3</label>
<caption>
<title>Pairwise percentage matrices of all nine testing protocols for the rat data.</title>
<p>The colour bar indicates the percentage correct of the pooled responses of all animals together. The more red a cell is, the higher the average performance. Chance or below chance values are indicated in the highest intensity of blue.</p>
</caption>
<graphic xlink:href="532720v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>After these first six test protocols, the animals were presented with a schedule where all three rotations are combined (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). On the new stimuli, the animals performed 58.56%, which is rather low, but still significantly different from chance level (binomial test on pooled performance of all animals: p &lt; 0.0001; 95% CI [0.57;:0.60]).</p>
</sec>
<sec id="s2a3">
<title>Testing computational levels of complexity</title>
<p>For the final two test protocols, we used a cDNN to find image pairs that would contrast strategies based upon a different stage in visual processing, with either early layers having lower performance than high layers (<italic>Zero vs. high</italic>), or early layers having better performance than high layers (<italic>High vs. zero</italic>). Rat performance was particularly low for <italic>Zero vs. high</italic> (56.47%), yet still significantly different from chance level (binomial test on pooled performance of all animals; p &lt; 0.0001; 95% CI [0.55;0.58]). In contrast, rats were able to solve the <italic>High vs. zero</italic> pairs not only better than chance (average: 64.84%; binomial test on pooled performance of all animals; p &lt; 0.0001; 95% CI [0.63;0.66]), but also significantly better than <italic>Zero vs. high</italic> (paired t-test on rat performance, t(10) = −4.49, p = 0.0012). This suggests that rats align with lower levels of processing when we purposely select image pairs that are optimized to contrast different levels of the visual processing hierarchy.</p>
<p>Next we checked how much individual cDNN layers can predict the variation in behavioural performance across image pairs when we take all test protocols together. We calculated the correlation of the generalization across image pairs between the cDNN classifier (summarized in <xref ref-type="fig" rid="fig7">Figure 7</xref>) and the rat performance of all nine test protocols. This correlation includes a total of 287 image pairs. We did this by concatenating all performances of the animals into one array and all Classification Scores of the network into another array, and calculating the correlation between these two arrays to retrieve a correlation for each network layer. The results are displayed in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Overall, we see quite low correlations, but several convolutional layers nevertheless show a significant positive correlation (permutation test) with the behavioural pattern of performance at the image pair level.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4</label>
<caption>
<title>Correlation of the Classification Score for single target/distractor pairs between single cDNN layers and the rat performance, for all nine test protocols together.</title>
<p>The black and grey horizontal lines on the x-axis indicated the layer blocks (block 1 consisting of conv1, norm1, pool1; block 2 consisting of conv2, norm2, pool2; block 3-4 corresponding to conv3-4 (respectively); block 5 consisting of conv5, pool5; block 6-7-8 corresponding to fc6-7-8, respectively. The vertical grey dashed line indicates the division between convolutional and fully connected layer blocks. The horizontal dashed line indicates a correlation of 0. The different markers indicate different sorts of layers: circle for convolutional layers, triangle for normalization layers, point for pool layers, and squares for fully connected layers. The asterisks indicate significant correlations according to a permutation test (* &lt; 0.05, ** &lt; 0.01 and *** &lt; 0.001).</p>
</caption>
<graphic xlink:href="532720v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Even though some of the correlations are significant, they are low. This could indicate that no cDNN layer is able to capture what rats do. Alternatively, it could be caused by a very low reliability of the behavioural data. To test the reliability of the variations in behavioural performance between stimulus pairs in all nine test protocols, we calculated the split-half reliability, as previously done in (<xref ref-type="bibr" rid="c26">Schnell et al., 2023</xref>), resulting in a correlation of 0.40. By applying the Spearman-Brown correction, we obtain a full-set reliability correlation of 0.58. This correlation is much higher than the correlations with individual cDNN layers.</p>
<p>It is possible that rat performance would be based upon multiple levels of processing, in which case we would need a combination of layers in order to explain the variation in performance across stimulus pairs. Given the low correlation between neighbouring layers (Supplemental Table 7), a multiple linear regression was calculated with the Classification Scores of the 13 layers as 13 regressors, and the rat performances as response vector. The results of this regression indicate a significant effect of the Classification Scores (F(287,273) = 2.22, p = 0.00907, R<sup>2</sup> = 0.10). Further investigating the 13 predictors showed that the later convolutional layers 8, 9 and 10 of the network were significant predictors in the regression model (see Supplemental Table 8 for results of the regression model). The R<sup>2</sup> = 10 of the full model would correspond to a correlation of around 0.32. This is better than the correlation of single layers, but still clearly smaller than the reliability of the rat data of 0.58. In conclusion, the cDNN model provides a partial explanation of how the performance of rats varies across image pairs.</p>
</sec>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Human study</title>
<p>A final part of this study was to include an online human study that follows the same design as the animal part. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows the average performance of humans (dark blue) versus rats (light blue) for all nine test protocols, as well as their performance on the old stimuli that were added in (or during) the testing protocols as quality control. Overall, humans performed better on all tests protocols than rats, with an average performance over all tests of 94.34% (humans) and 62.29% (rats). There was already a difference in terms of training performance (humans: 92.86% vs. rats: 77.84%), but the difference on the test protocols is larger. We subtracted the training performance of humans or rats from the testing performance of humans or rats, respectively, and even with this normalization for training performance there is still a significantly higher test performance in humans compared to rats (t(16) = −6.47, p &lt; 0.0001). Thus, not surprisingly, the degree of invariance in this object classification task is higher for humans compared to rat.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5</label>
<caption>
<title>Average performance of humans versus rats.</title>
<p>On the x-axis, the nine test protocols in addition to the performance on all old stimuli are presented in the following order: rotation x (RotX), rotation y (RotY), rotation z (RotZ), size, position (Pos), light location (LL), Combination rotation (CR), Zero vs. high (ZVH), High vs. zero (HVZ) and All Old. The dashed horizontal line indicates chance level. The error bars indicate standard error over humans/rats.</p>
</caption>
<graphic xlink:href="532720v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The variation in performance across test protocols and across image pairs can give an indication of the strategies that each species follows. Overall, humans and rats show a mild correspondence in terms of which image pairs are more difficult, with a human-rat correlation of 0.18 across all image pairs of the nine test protocols (p &lt; 0.001 with permutation test). Albeit significant, this correlation is clearly lower than the maximum value that could be obtained given the reliability of the data. The split-half reliability of the human data was 0.46, corresponding to a full-set reliability of 0.63. We reported above that full-set reliability is 0.58 for the rat data, resulting in a combined reliability of 0.60 (calculated as described in <xref ref-type="bibr" rid="c21">Op de Beeck et al., 2008</xref>). Thus, after taking data reliability into account there remains a pronounced discrepancy between rats and humans in terms of how performance varies across image pairs.</p>
<p>The main question of the present study is how this discrepancy relates to computational informed strategies. If we take a closer look specifically at the two cDNN-informed test protocols (<italic>Zero vs. high</italic> and <italic>High vs. zero</italic>), we see an opposite behaviour between animals and humans. Humans performed significantly better in the <italic>Zero vs. high</italic> protocol, i.e. where we used stimuli where the earlier layers of the network perform worse than the higher layers, than in the <italic>High vs. zero</italic> protocol (paired t-test: t(44) = 2.85, p = 0.0067). Rats, however, show the opposite (see above for statistics). There even is a significant interaction between species and test protocol (unpaired t-test: t(54) = 2.50, p = 0.016s). This suggests a different strategy between animals and humans: rats use strategies that are captured in the lower layers of the network, and thus correspond more to low level visual processing. Humans, however, tend to rely more on strategies captured by the higher layers of the network, and thus we are looking at more high-level visual processing.</p>
<p>As a next step, we calculated the correlation between the generalization across image pairs between the cDNN classifier and the human performance of all nine test protocols in an identical manner as for the rat performance (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The results are displayed in <xref ref-type="fig" rid="fig6">Figure 6</xref>. Overall, we see quite high correlations, especially in the higher layers. This pattern across layers is very different from the pattern in rats where the highest layers showed no correlations, which again suggests that, despite successful generalization, rats rely on decisively lower-level strategies than humans in the same categorization task.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6</label>
<caption>
<title>Correlation of the Classification Score for single target/distractor pairs between single cDNN layers and the human performance, for all nine test protocols together.</title>
<p>The naming convention on the x axis corresponds to the layers of the network, identical as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The black and grey horizontal lines on the x-axis indicated the layer blocks (block 1 consisting of conv1, norm1, pool1; block 2 consisting of conv2, norm2, pool2; block 3-4 corresponding to conv3-4 (respectively); block 5 consisting of conv5, pool5; block 6-7-8 corresponding to fc6-7-8, respectively. The vertical grey dashed line indicates the division between convolutional and fully connected layer blocks. The horizontal dashed line indicates a correlation of 0. The different markers indicate different sorts of layers: circle for convolutional layers, triangle for normalization layers, point for pool layers, and squares for fully connected layers. The asterisks indicate significant correlations according to a permutation test (* &lt; 0.05, ** &lt; 0.01 and *** &lt; 0.001).</p>
</caption>
<graphic xlink:href="532720v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>A multiple linear regression was calculated in an identical manner as we did with the rat performance. The results of this regression indicate a significant effect of the Classification Scores (F(287,273) = 6.8, p &lt; 0.0001, R<sup>2</sup> = 0.25). Further investigating the 13 predictors showed that in particular the fully connected layers 11, 12 and 13 of the network were strong predictors in the regression model (see Supplemental Table 9 for results of the regression model).</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>In the current study, we trained and tested rats and humans in a categorization task using two-dimensional stimuli, with the two dimensions being concavity and alignment. We tested generalization across a range of viewing conditions. For the last two testing protocols, we used a computational approach to select the stimuli in terms of specifically dissociating low and high stages of processing. Rats were able to learn both dimensions (concavity and alignment) and showed a preference for concavity. Their performance on the testing protocols revealed a wide variety in percentage correct: for some test protocols they performed just above chance level, e.g. <italic>Zero vs. high</italic>, whereas for others they could easily reach about 70% correct (<italic>Position</italic>). Humans, on the other hand, performed better overall, with performances of 80% or higher on the testing protocols. Addressing the question of the complexity of the underlying strategies, rats performed best on the test protocol designed to specifically target lower levels of processing whereas humans performed best on the high-level processing protocol. Likewise, direct comparisons with artificial neural network layers showed that the variation of rat performance across images was best explained by late convolutional layers, whereas human performance was most associated with representations in fully connected layers.</p>
<p>All animals started by being trained in three training protocols. The first <italic>Training</italic> protocol only included one image pair, the base pair, containing the most different target and distractor without any further transformations. Learning of the individual dimensions of concavity and alignment was investigated through the <italic>Dimension learning</italic> protocol. The results from this <italic>Dimension learning</italic> protocol indicate that our rats have more difficulties learning the alignment dimension as opposed to the concavity dimension. One possible explanation for the superior performance on the concavity dimension could be that the animals were partially solving the task such that the brighter stimulus, i.e. the convex base shape, is the distractor and that their strategy is to pick the stimulus with the lowest brightness. This was confirmed by analyses on the third training protocol (<italic>Transformations</italic>) that included small transformations along various dimensions. Nevertheless, the rats still performed above chance level for trials in which the brightness differences were reversed, indicating that other dimensions are involved and overrule a contribution from brightness. Similar findings have been obtained in human behaviour and neuroscience. For example, despite the clear category selectivity in regions such as the fusiform face area, the selectivity in these regions is also modulated very strongly by various low-level dimensions (<xref ref-type="bibr" rid="c35">Yue et al., 2011</xref>). With regard to the size and position transformations it is important to keep in mind that the animals were freely moving in the touchscreen chambers, and so even for the original base pair was already undergoing changes in retinal size and retinal position. What we manipulate, is rather the size and position relative to the rest of the set-up (e.g., relative to screen position and size).</p>
<p>After these three training protocols, the animals were tested for generalization in a variety of testing protocols, each testing a separate transformation on the stimuli. The first six test protocols included rotation along all the three axes, size, position and light location, following by a test protocol in which we combined the rotation along the three axes. Overall, we found that the performance of the animals on these test protocols is affected by these transformations, but still significantly above chance in each protocol. Studies in the literature would often stop here, or proceed by systematically testing even larger transformations. Stimulus choices are based upon intuitions of what strategy animals might be using, and upon theories of how visual perception works. However, in some cases, a further computational modelling of the task and stimuli finds that what intuitively seems like a task of a particular complexity might not be so complex after all. The first tests of invariant object recognition seemed impressive, but were found to be easily solved with earlier layers of processing (<xref ref-type="bibr" rid="c19">Minini &amp; Jeffery, 2006</xref>; Vinken &amp; Op de Beeck, 2021). This was recently also highlighted by relatively simple pixel-based analyses (<xref ref-type="bibr" rid="c13">Kell et al., 2020</xref>). As another example, Vinken &amp; Op de Beeck (2021) have used a computational approach to further investigate the levels of information processing in rodents by comparing three hallmark studies that provided evidence for higher order visual processing in rodents (<xref ref-type="bibr" rid="c8">Djurdjevic et al., 2018</xref>; <xref ref-type="bibr" rid="c34">Vinken et al., 2014</xref>; <xref ref-type="bibr" rid="c37">Zoccolan et al., 2009</xref>) with cDNNs. They found that for all three studies, the low and mid-level layers captured the rat performances best, providing thus evidence against the previously concluded high level visual processing in rodents.</p>
<p>For these reasons, we decided to directly test image pairs through computational modelling with cDNNs and select pairs that are particularly suited of dissociating different levels of processing. Stimuli were chosen by a cDNN from a very large set of possible stimuli and combinations, such that the higher layers and the lower layers of the network make distinct errors on classifying the stimuli (<italic>Zero vs. high</italic> and <italic>High vs. zero</italic> protocol), and thus are diagnostic of the level of underlying visual strategies. The stimuli of the <italic>Zero vs. high</italic> protocol included stimuli where the higher layers of the network performed better than the lower layers, and thus they address higher level visual processing. The opposite can be said for the <italic>High vs. zero</italic> protocol, which includes stimuli that specifically target lower level visual processing, given that the lower layers of the network perform best on these stimuli. After presenting these stimuli to the animals, we found that our rats performed best in the <italic>High vs. zero</italic> protocol, suggesting that they focus on low level visual cues to solve this categorization task. We found the opposite cDNN pattern for humans, indicating that they use high level visual processing. These findings provide more direct information about the level of processing that underlies the behavioural strategies compared to overall performance or to effects of image manipulations. This is a new promising way to design experiments in a way that is computationally informed rather than based on researcher intuitions or qualitative predictions.</p>
<p>Partially thanks to these computationally inspired tests, our total dataset finds a marked dissociation between how humans and rats solve this object recognition task. Our analyses show this most convincingly by correlating the variation in performance across image trials with the predictions of cDNN layers. There were significant correlations with multiple layers in both species. In humans, the most pronounced correlations were present for the highest, fully connected layers, while in rats correlations were limited to low and middle convolutional layers. This is the most direct evidence available in the literature that rats resolve object recognition tasks through a very different and computationally simpler strategy compared to humans. The cDNN approach does not inform us how we can verbalize this simpler strategy, but based upon earlier work (<xref ref-type="bibr" rid="c26">Schnell and colleagues, 2023</xref>); Vermaercke &amp; Op de Beeck, 2012) we would hypothesize that rats rely upon visual contrast features (e.g., this area is darker/lighter than that other area). Such contrast features are also used by humans and monkeys, e.g. for face detection (<xref ref-type="bibr" rid="c20">Ohayon et al., 2012</xref>; <xref ref-type="bibr" rid="c27">Sinha, 2002</xref>), but in addition humans have access to more complex strategies that e.g. refer to complex shape features such as aspect ratio and symmetry (<xref ref-type="bibr" rid="c3">Bossens &amp; Op de Beeck, 2016</xref>).</p>
<p>For future studies, it will be highly valuable to use this computational informed strategy on a wider battery of behavioural tasks, as well as a wider range of species such as tree shrews and marmosets (<xref ref-type="bibr" rid="c5">Callahan &amp; Petry, 2000</xref>; <xref ref-type="bibr" rid="c13">Kell et al., 2020</xref>, <xref ref-type="bibr" rid="c14">2021</xref>; <xref ref-type="bibr" rid="c18">Meyer et al., 2022</xref>; <xref ref-type="bibr" rid="c23">Petry et al., 2012</xref>; <xref ref-type="bibr" rid="c22">Petry &amp; Bickford, 2019</xref>). One step further, we can use the information from computational modelling together with behaviour and how it differs among stimuli to further select stimuli for neurophysiological investigations of neuronal response properties along the visual information processing hierarchy, in this way following experimental designs that are optimized for highlighting the primary differences between processing stages and between species.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Animal study</title>
<sec id="s4a1">
<label>4.1.1</label>
<title>Animals</title>
<p>A total of twelve male outbred Long Evans rats (Janvier Labs, Le Genest-Saint-Isle, France) started this behavioural study. Out of these twelve animals, two were tested extensively in a first pilot study, and were included in the remainder of the study as well. All animals were 11 weeks old at the start of shaping and were housed in groups of four per cage. Each cage was enriched with a plastic toy (Bio-Serv, Flemington, NJ), paper cage enrichment and wooden blocks. Near the end of the experiment, one animal had to be excluded because of health issues. During training and testing, the animals were food restricted to maintain a body weight between 85% and 90% of their underprived body weight. They received water ad libitum. All experiments and procedures involving living animals were approved by the Ethical Committee of the University of Leuven and were in accordance with the European Commission Directive of September 22, 2010 (2010/63/EU).</p>
</sec>
<sec id="s4a2">
<label>4.1.2</label>
<title>Setup</title>
<p>The setup is identical to the one used by <xref ref-type="bibr" rid="c25">Schnell and colleagues (2019)</xref> and <xref ref-type="bibr" rid="c26">Schnell and colleagues (2023)</xref>. A short description will follow here. The animals were trained and tested in four automated touch-screen rat-testing chambers (Campden Instruments, Ltd., Leicester, UK) with ABET II controller software (v2.18, WhiskerServer v4.5.0). The animals performed one session per day and each session lasted for 100 trials or 60 minutes, whichever came first. A reward tray in which sugar pellets (45-mg sucrose pellets, TestDiet, St. Louis, MO) could be delivered was installed on one side of the chamber. On the other side of the chamber, an infrared touchscreen monitor was installed. This monitor was covered with a black Perspex mask containing two square response windows (10.0 x 10.0 cm). A shelf (5.4cm wide) was installed onto this black mask (16.5cm above the floor) to force the animals to attend to the stimuli and to view the stimuli within their central visual fields. Close proximity to the screen was enough to elicit a response because the screens are infrared.</p>
</sec>
<sec id="s4a3">
<label>4.1.3</label>
<title>Stimuli</title>
<p>Stimuli were created using the Python scripting implementation of the 3D modelling software Blender 3D (version 2.93.3). In general, the stimuli were objects that consisted of a body (base) with three spheres attached to it. A first step was to alter two dimensions of the object, namely the concavity of the base and the alignment of the three spheres. The base was made either concave or convex by increasing (convex) or decreasing (concave) the base parameter. The alignment of the spheres was altered by changing the placement of the left and the right spheres. These spheres could either be horizontally aligned or misaligned. In the misaligned case, the spheres were placed diagonally from upper left to lower right. Supplemental Figure 1a shows two example stimuli, the ones that later were selected as the so-called “base pair”. Next, additional exemplars were created by uniformly tiling the two-dimensional stimulus space between these two example stimuli. We decided to create eleven levels of the concavity dimension and four levels of alignment. This already yields 44 stimuli (see Supplemental Figure 2). We chose these levels of concavity and alignment based on the pixel dissimilarity of the stimuli (see Supplemental Figure 3). The final goal was to construct a 4×4 stimulus grid by selecting a subset of the 4×11 stimulus grid. We chose a large number of concavity levels, as this ensures flexibility in the calibration of the two dimensions relative to each other.</p>
<p>We added identity-preserving transformations to the stimuli, such as rotation among the x-axis, y-axis and z-axis in six different angles (0° to 180° in steps of 30°), as well as changing the light location (left, under, up, right, front) and finally the size and position. The latter two transformations were implemented using Python (3.7.3). Excluding the size and position transformation, these transformations resulted in a total set of 75460 stimuli (4 (alignment) * 11 (concavity) * 7 (x-axis rotation) * 7 (y-axis rotation) * 7 (z-axis rotation) * 5 (light location) = 75460 stimuli). Supplemental Figure 3 shows examples of these transformations.</p>
</sec>
<sec id="s4a4">
<label>4.1.4</label>
<title>Protocols</title>
<p>Once the pilot was finished (see supplementary for details), we set up the experiment and chose our stimuli. We started by reducing the 4×11 stimulus grid to a 4×4 stimulus grid (see Supplemental Figure 1b). All stimuli on the diagonal can be seen as ambiguous stimuli (four stimuli in total), as they can be identified as a target as well as a distractor. The six stimuli above this diagonal create the target part of the grid, and the six stimuli below this diagonal resemble the distractor sub-grid.</p>
<p>The different phases of the experiment are shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. In the main Training phase, we trained the animals in the maximally different stimuli that are placed at the very ends of the corners (Supplemental Figure 1a). We refer to this as the base pair. After this Training phase, the experiment consisted of two further training protocols. In the Dimension learning training phase, we pushed the animals to learn both dimensions (concavity and alignment) by presenting them two additional stimuli pairs from Supplemental Figure 1b in which the target and distractor differ in only one dimension. A third training protocol (<italic>Transformations</italic>) consisted of stimuli with some small transformations, such as 30° rotation along the x-axis, 30° rotation along the y-axis, 30° rotation along the z-axis, light location below, and size reduction of 80%, resulting in a total of 25 possible stimulus pairs (every combination of target-distractor with the 5 transformed stimuli). During these two training protocols, one third of the trials were so-called “old trials” with the base pair. Correction trials were given if an animal answered incorrectly, i.e. the same trial was repeated until the animal answered correctly. These correction trials were excluded from the analyses. In all trials, rats received a reward for touching the correct screen, i.e. the screen with the target.</p>
<p>After these three training protocols, the testing part of the experiment included nine test protocols. The crucial defining difference between these test protocols and the prior training protocols is that rats received a reward randomly in 80% of the trials with new stimulus pairs, and no correction trials were given for an incorrect response. This random reward is important to keep the animals motivated during the testing protocols and to measure real generalization, and not training behaviour. We have used a similar approach in the past, where we rewarded the animals in every testing trial (<xref ref-type="bibr" rid="c25">Schnell et al., 2019</xref>; <xref ref-type="bibr" rid="c34">Vinken et al., 2014</xref>). One third of the trials in all test protocols consisted of old trials with the base pair, and here, the animals received reward for touching the target and correction trials were shown if necessary. Regularly, we inserted a <italic>Dimension learning</italic> session in between two test sessions to maintain the performance high enough on training stimuli, especially for the animals in which we saw a drop in performance on the base pair. We excluded any test sessions where the performance on the base pair stimuli dropped to below 65%.</p>
<p>The first six test protocols included one protocol for each transformation, i.e. <italic>Rotation X, Rotation Y, Rotation Z, Light Location, Size</italic> and <italic>Position</italic>. The order in which these first six test protocols were given to the animals was counterbalanced between the animals. The stimuli that were used in these six test protocols can be seen in <xref ref-type="fig" rid="fig1">Figure 1</xref> and every combination of target-distractor per test protocol was presented to the animals. For the rotation protocols, we used rotation degrees in steps of 30°, ranging from 30° to 180°. This resulted in 36 possible stimulus pairs for each of the three rotation protocols. In the <italic>Light Location</italic> protocol, we used stimuli where the light location was set at four different positions (below, left, right and up), resulting in 16 possible stimulus pairs for this protocol. In the <italic>Size</italic> protocol, we selected targets and distractors that were 80% and 60% reduced in size compared to the original, training pair. This protocol included 4 possible stimulus pairs. And finally, in the <italic>Position</italic> protocol, we changed the position of the 80% reduced in size stimuli and placed the objects in the lower left corner, lower right corner, centre, upper left corner and upper right corner. We have a total of 25 possible stimulus pairs for this protocol.</p>
<p>After these six test protocols, we presented the animals with six targets and six distractors where all three rotations were combined (<italic>Combination rotation</italic>), i.e. x-, y- and z-axis were rotated with the same degree (ranging from 30° to 180°, in steps of 30°). This resulted in a total of 36 new stimulus pairs. Again, no correction trials were included after the trials where rotated stimuli were shown and animals received random reward in 80% of the trials. One third of the trials consisted of the stimulus pair from the first <italic>Training</italic> phase (i.e. the base pair), and here, correction trials were given after an incorrect response and real reward was given to the animals.</p>
<p>In a final set of two test protocols, we created a cDNN-informed stimulus set. The details of the computational modelling are explained in the next section. The first protocol (<italic>Zero vs. high</italic>) included stimuli in which the lower layers of the network performed around chance level (i.e. target-distractor difference in Classification Scores (difference in signed distance to hyperplane) of about 0), whereas the higher layers scored high (see section 4.2). The second protocol (<italic>High vs. zero</italic>) included stimuli where the network did the opposite. That is, the earlier layers performed well whereas the higher layers performed around chance level. The order of the two test protocols was counterbalanced between the animals. Each of these test protocols included 7 targets and 7 distractors, giving a total of 49 new stimulus pairs.</p>
<p>Animals stayed in each session for 60 minutes or until they reached 100 training trials or 120 testing trials. We used an intertrial interval (ITI) of 20s and a time-out of 5s during training sessions. From another pilot study in the lab, we noticed we could decrease the ITI and time-out without affecting the rats’ performance. Therefore, we decided to use an ITI of 15s and time-out of 3s during testing, and to increase the number of trials during a testing session to 120 trials.</p>
<p>Each protocol was run for multiple sessions per animal. Given that we were interested in how performance would vary across stimulus pairs, we completed more sessions for the protocols that included more stimulus pairs.</p>
<p>Supplemental Table 1 indicates the average number of trials per test protocol for all rats together.</p>
<p>One animal was not placed in the <italic>Transformations</italic> phase as it was the slowest animal during training. However, its performance on the test protocols did not significantly differ from the other animals. We tested this by calculating the correlation of the variation of performance across stimulus pairs for each rat with the pooled responses of all other rats. The average correlation for each of the other animals with the pooled response was 0.24 (±0.09), and the correlation of this slowest animal with the others was very similar, 0.23.</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Computational modelling</title>
<p>One important goal of this study was to create a cDNN-informed stimulus set to present to the animals. To do so, we followed the steps of <xref ref-type="bibr" rid="c26">Schnell and colleagues (2023)</xref> and Vinken &amp; Op de Beeck (2021) to train a cDNN on the same stimuli on which our animals were trained. The steps of training the network are identical to <xref ref-type="bibr" rid="c26">Schnell and colleagues (2023)</xref> and a short description will follow here. We used the standard AlexNet cDNN architecture that was pre-trained on ImageNet to classify images into 1000 object categories (MATLAB 2021b Deep Learning Toolbox). Following Vinken &amp; Op de Beeck (2021), we applied principal component analysis to calculate the activations in every layer, to standardize the values across inputs and to reduce the dimensionality. We then trained a linear support vector machine classifier by using the MATLAB function fitclinear, with limited-memory BFGS solver and default regularization. We performed this with the standardized DNN layer activations in the principal component space as inputs, before ReLU, to our 24 training stimuli (see <xref ref-type="fig" rid="fig1">Figure 1</xref>), i.e. all stimuli of the <italic>Training, Dimension</italic> learning and <italic>Transformations</italic> protocols. The layers of AlexNet were divided into 13 sublayers, similar as in <xref ref-type="bibr" rid="c26">Schnell and colleagues (2023)</xref> and Vinken &amp; Op de Beeck (2021).</p>
<p><xref ref-type="fig" rid="fig7">Figure 7</xref> shows the performance of the network after training the network on our training stimuli for all test protocols. We added noise to the inputs of the network such that the average training performance, averaged over 100 iterations, lies around 75%. By adding noise in this way, the performance on the training pairs matches overall with rat performance on those pairs, otherwise the performance of the network would be at 100% on the training pairs and this would complicate comparisons with the animal data (see also Vinken &amp; Op de Beeck, 2021). Note that the results for the <italic>Size</italic> test are unreliable given the low number of stimulus pairs in that test. The performance of the network on the tests (green line in <xref ref-type="fig" rid="fig7">Figure 7</xref>) differs among the tests and across layers, but typically the network had no problems to achieve a training performance of about 85% in all test protocols in at least some layers. The change in performance across layers is variable across test protocols.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7</label>
<caption>
<title>The performance of the cDNN after training on our training stimuli, with noise added to its input.</title>
<p>The naming convention on the x axis corresponds to the layers of the network, identical as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The performance (y-axis) illustrates that each layer is challenged by at least part of the test protocols. The purple line indicates the training performance and the green line indicates the test performance of the neural network. The x axis on each subplot indicates the block of the layer: layers 1-13 correspond to convolutional layer 1, normalization layer 1, pool layer 1, convolutional layer 2, normalization layer 2, pool layer 2, convolutional layer 3, convolutional layer 4, convolutional layer 5, pool layer 5, fully connected layer 6, fully connected layer 7 and fully connected layer 8, respectively. The black and grey horizontal lines on the x-axis indicated the layer blocks (block 1 consisting of conv1, norm1, pool1; block 2 consisting of conv2, norm2, pool2; block 3-4 corresponding to conv3-4 (respectively); block 5 consisting of conv5, pool5; block 6-7-8 corresponding to fc6-7-8, respectively. The vertical grey dashed line indicates the division between convolutional and fully connected layer blocks. The horizontal dashed line indicates chance level. The shaded error bounds correspond to 95% confidence intervals calculated using Jackknife standard error estimates, as done previously in (<italic>Vinken &amp; Op de Beeck, 2021</italic>). The different markers indicate different sorts of layers: circle for convolutional layers, triangle for normalization layers, point for pool layers, and squares for fully connected layers.</p>
</caption>
<graphic xlink:href="532720v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To examine the performance of the model for specific image pairs during training and testing in more detail than possible with a binary categorization decision, we calculate the distance to the classifier’s hyperplane (decision boundary) of the targets and distractors. We do this by computing the difference in signed distance to the hyperplane between target and distractor (target – distractor). This is referred to as the Classification Score. For each stimulus pair in the test protocols we computed this Classification Score and we have such a score per layer.</p>
<p>We used this Classification Score to select image pairs for a cDNN-informed stimulus set. To do so, we randomly chose one target and one distractor from a subset of the pool of all 4×4 stimuli, including all possible transformations on these stimuli. This resulted in a stimulus pool of 10.290 stimuli (5145 targets, 5145 distractors) to randomly choose two from, and 5145*5145 (26 471 025) possible resulting pairs of two stimuli. Once one random target and one random distractor was chosen, the DNN was tested in a similar manner as we did for the six test protocols. We performed a total of 10000 iterations of randomly choosing a target and distractor pair. For each iteration, we calculated the average Classification Score of layers 1-3 and of layers 11-13 as we wanted to compare those two levels of processing (earlier layers vs higher layers). After these 10000 iterations, we finetuned and filtered the results according to the profile of performance across earlier and higher layers (see Supplemental Table 2). This finetuning started by calculating the distribution and standard deviation for two profiles of interest, i.e. (i) where early layers show an average Classification Score close to zero but higher layers show high Classification Scores (Zero vs High), and (ii) where early layers show high Classification Scores but higher layers show close to zero Classification Scores (High vs Zero). The performance was expressed relative to the distribution of values across all pairs, summarized by de standard deviation of the average target-distractor difference in Classification Scores of the early layers and the higher layers. We found a total of 48 stimulus pairs for these two criteria, and we ended up choosing 14 pairs, 7 of each criterion, that we used for the final part of the animal and human study (see lower two rows in <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p>
<p>Afterwards we also calculated the binary target-distractor cDNN decision performance for the image pairs in the Zero vs High and High vs Zero tests, which is shown in <xref ref-type="fig" rid="fig7">Figure 7</xref> (bottom row). The image pairs in the Zero vs High protocol are more difficult than the other protocols, in particular for the first half of the cDNN layers. In contrast, the <italic>High vs Zero</italic> protocol is the only protocol associated with chance performance in the last three layers. These analyses confirm that the cDNN-based image pair selection resulted in protocols that are very different from protocols that zoom in on intuitively chosen transformations and their combinations.</p>
<p>Comparing the rat performances to the Classification Scores of the network was done by calculating the correlation across image pairs between these model scores and the rat performances averaged across animals. We concatenated the performance of the animals on all nine test protocols, as well as the distance to hyperplane of the network on all nine test protocols. Correlating these two arrays resulted in the correlations as visualized in <xref ref-type="fig" rid="fig4">Figure 4</xref>. To test whether these correlations are significant, we performed a permutation test. We permutated these arrays 1000 times, resulting in a normal distribution of permutated data per layer. We then calculated, per layer, how many of the permutated values are higher than or equal to the correlation that is presented in <xref ref-type="fig" rid="fig4">Figure 4</xref>, and divided this by the number of permutations.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Human study</title>
<sec id="s4c1">
<label>4.3.1</label>
<title>Participants</title>
<p>Data was collected from 50 participants (average age 33.24 ± 12.23; 34 females) who participated in return for a gift voucher of 10 euro. All participants had normal or corrected-to-normal vision. The experiment was approved by the ethical commission of KU Leuven (G-2020-1902-R3) and each participant digitally signed an informed consent before the start of the experiment.</p>
</sec>
<sec id="s4c2">
<label>4.3.2</label>
<title>Setup</title>
<p>For the human part of this study, we developed an online experiment using PsychoPy3 (v2020.1.3, Python version 3.8.10) and placed it on the online platform Pavlovia. All participants received the link and their individual participant number by e-mail with which they could participate in the experiment on their own computer. It took 30-45 minutes to complete the online study.</p>
</sec>
<sec id="s4c3">
<label>4.3.3</label>
<title>Stimuli and protocols</title>
<p>We used the same stimuli as in the animal study. The human experiment underwent the same phases as depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>, albeit with small changes. We dropped the 1/3<sup>rd</sup> old trials in the test protocols and included two additional <italic>Dimension Learning</italic> protocols in between the first counterbalanced tests as quality check (see Supplemental Figure 6). Supplemental Table 3 provides an overview of the number of trials during the human experiment for each phase.</p>
<p>Similar as in <xref ref-type="bibr" rid="c3">Bossens &amp; Op de Beeck (2016)</xref>, we presented the targets and distractors briefly to the left and right side of a white fixation cross on a grey background. Each stimulus was presented for three frames, followed by a mask (a noise image with 1/f frequency spectrum for three frames). We used this fast and eccentric stimulus presentation with a mask to resemble the stimulus perception more closely to that of rats. Vermaercke &amp; Op de Beeck (2012) have found that human visual acuity in these fast and eccentric presentations is not significantly better than the reported visual acuity of rats. By using this approach we avoid that differences in strategies between humans and rats would be explained by such a difference in acuity. Participants could then answer using the f and ‘j’ keys to indicate which position they thought was the correct position. If they thought the target was on the left side of the fixation cross, they had to press ‘f’, and ‘j’ if they thought the target was on the right side. Participants received feedback during the shaping and the three training phases. This happened by colouring the fixation cross green if they answered correctly, and red if they answered incorrectly. Each trial was followed by an intertrial interval (ITI) of 0.5s. During the <italic>Shaping</italic> and <italic>Training</italic> phase, we kept a running average of the past 20 (<italic>Shaping</italic>) and 40 (<italic>Training</italic>) trials and participants continued to the next phase when they reached a performance of 80% or higher on the last 20 or 40 trials, similar as in <xref ref-type="bibr" rid="c3">Bossens &amp; Op de Beeck (2016)</xref>. There was no time limit for the participants for providing a response. The order of the first six test protocols (<italic>Rotation X, Rotation Y, Rotation Z, Size, Light Location</italic> and <italic>Position</italic>) was counterbalanced between the participants based on the participant number, as well as the order of the last two test protocols (<italic>Zero vs. high</italic> and <italic>High vs. zero</italic>), similar as the approach in the rat study. Supplemental Table 1 indicates the average number of trials per test protocol for all human participants together.</p>
<p>In terms of instructions, we explained to participants that they would see two figures appearing at the same time very quickly next to a fixation cross, and they would have to make a decision of which figure is the correct one. We mentioned that during training, the fixation cross would turn green if they answered correctly, and red if they answered incorrectly. Participants were informed that during testing, they would not get feedback (changing colour of the fixation cross) anymore and that they would have to use the knowledge they gained throughout training to make their decision in the testing.</p>
</sec>
</sec>
</sec>
<sec id="d1e952" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1030">
<label>SupplementalInformation</label>
<media xlink:href="supplements/532720_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<sec id="d1e967">
<title>Data availability</title>
<p>The data has been made publicly available via the Open Science Framework and can be accessed at <ext-link ext-link-type="uri" xlink:href="https://osf.io/9eqyz/">https://osf.io/9eqyz/</ext-link>.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Alemi-Neissi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rosselli</surname>, <given-names>F. B.</given-names></string-name>, &amp; <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> (<year>2013</year>). <article-title>Multifeatural Shape Processing in Rats Engaged in Invariant Visual Object Recognition</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>14</issue>), <fpage>5939</fpage>–<lpage>5956</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3629-12.2013</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Avberšek</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Zeman</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Op de Beeck</surname>, <given-names>H.</given-names></string-name> (<year>2021</year>). <article-title>Training for object recognition with increasing spatial frequency: A comparison of deep learning with human vision</article-title>. <source>Journal of Vision</source>, <volume>21</volume>(<issue>10</issue>), <fpage>14</fpage>. <pub-id pub-id-type="doi">10.1167/jov.21.10.14</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Bossens</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Op de Beeck</surname>, <given-names>H. P.</given-names></string-name> (<year>2016</year>). <article-title>Linear and Non-Linear Visual Feature Learning in Rat and Humans</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, <volume>10</volume>. <pub-id pub-id-type="doi">10.3389/fnbeh.2016.00235</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Cadieu</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name>, <string-name><surname>Pinto</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ardila</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Solomon</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Majaj</surname>, <given-names>N. J.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> (<year>2014</year>). <article-title>Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition</article-title>. <source>PLOS Computational Biology</source>, <volume>10</volume>(<issue>12</issue>), <fpage>e1003963</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Callahan</surname>, <given-names>T. L.</given-names></string-name>, &amp; <string-name><surname>Petry</surname>, <given-names>H. M.</given-names></string-name> (<year>2000</year>). <article-title>Psychophysical measurement of temporal modulation sensitivity in the tree shrew (Tupaia belangeri)</article-title>. <source>Vision Research</source>, <volume>40</volume>(<issue>4</issue>), <fpage>455</fpage>–<lpage>458</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(99)00194-7</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>De Keyser</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bossens</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kubilius</surname>, <given-names>J.</given-names></string-name>, &amp; Op de <string-name><surname>Beeck</surname>, <given-names>H. P.</given-names></string-name> (<year>2015</year>). <article-title>Cue-invariant shape recognition in rats as tested with second-order contours</article-title>. <source>Journal of Vision</source>, <volume>15</volume>(<issue>15</issue>), <fpage>14</fpage>. <pub-id pub-id-type="doi">10.1167/15.15.14</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Rust</surname>, <given-names>N. C.</given-names></string-name> (<year>2012</year>). <article-title>How Does the Brain Solve Visual Object Recognition?</article-title> <source>Neuron</source>, <volume>73</volume>(<issue>3</issue>), <fpage>415</fpage>–<lpage>434</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Djurdjevic</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Ansuini</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bertolini</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> (<year>2018</year>). <article-title>Accuracy of Rats in Discriminating Visual Objects Is Explained by the Complexity of Their Perceptual Strategy</article-title>. <source>Current Biology</source>, <volume>28</volume>(<issue>7</issue>), <fpage>1005</fpage>–<lpage>1015</lpage>.<page-range>e5</page-range>. <pub-id pub-id-type="doi">10.1016/j.cub.2018.02.037</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Duyck</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Martens</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>C.-Y.</given-names></string-name>, &amp; Op de <string-name><surname>Beeck</surname>, <given-names>H.</given-names></string-name> (<year>2021</year>). <article-title>How Visual Expertise Changes Representational Geometry: A Behavioral and Neural Perspective</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>33</volume>(<issue>12</issue>), <fpage>2461</fpage>–<lpage>2476</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01778</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Güçlü</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>Gerven</surname>, <given-names>M. A. J. van.</given-names></string-name> (<year>2015</year>). <article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>27</issue>), <fpage>10005</fpage>–<lpage>10014</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Kalfas</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Vinken</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Vogels</surname>, <given-names>R.</given-names></string-name> (<year>2018</year>). <article-title>Representations of regular and irregular shapes by deep Convolutional Neural Networks, monkey inferotemporal neurons and human judgments</article-title>. <source>PLOS Computational Biology</source>, <volume>14</volume>(<issue>10</issue>), <fpage>e1006557</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006557</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Kar</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kubilius</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Issa</surname>, <given-names>E. B.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> (<year>2019</year>). <article-title>Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>6</issue>), Article 6. <pub-id pub-id-type="doi">10.1038/s41593-019-0392-5</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="other"><string-name><surname>Kell</surname>, <given-names>A. J. E.</given-names></string-name>, <string-name><surname>Bokor</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Jeon</surname>, <given-names>Y.-N.</given-names></string-name>, <string-name><surname>Toosi</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Issa</surname>, <given-names>E. B.</given-names></string-name> (<year>2020</year>). <source>Conserved core visual object recognition across simian primates: Marmoset image-by-image behavior mirrors that of humans and macaques</source>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="web"><string-name><surname>Kell</surname>, <given-names>A. J. E.</given-names></string-name>, <string-name><surname>Bokor</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Jeon</surname>, <given-names>Y.-N.</given-names></string-name>, <string-name><surname>Toosi</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Issa</surname>, <given-names>E. B.</given-names></string-name> (<year>2021</year>). <article-title>Brain organization, not size alone, as key to high-level vision: Evidence from marmoset monkeys</article-title> (p. 2020.10.19.345561). <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2020.10.19.345561</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Kubilius</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bracci</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Op de Beeck</surname>, <given-names>H.</given-names></string-name> (<year>2016</year>). <article-title>Deep Neural Networks as a Computational Model for Human Shape Sensitivity</article-title>. <source>PLOS Computational Biology</source>, <volume>12</volume>(<issue>4</issue>), <fpage>e1004896</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004896</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="other"><string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name>, &amp; <string-name><surname>Sheinberg</surname>, <given-names>D. L.</given-names></string-name> (<year>1996</year>). <source>Visual Object Recognition</source>. <fpage>45</fpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="web"><string-name><surname>Matteucci</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Marotti</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Riggi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rosselli</surname>, <given-names>F. B.</given-names></string-name>, &amp; <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> (<year>2019</year>). <article-title>Nonlinear Processing of Shape Information in Rat Lateral Extrastriate Cortex | Journal of Neuroscience</article-title>. <source>Journal of Neuroscience</source>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1938-18.2018</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="book"><string-name><surname>Meyer</surname>, <given-names>E. E.</given-names></string-name>, <string-name><surname>Ong</surname>, <given-names>W. S.</given-names></string-name>, <string-name><surname>Balboa</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Arcaro</surname>, <given-names>M. J.</given-names></string-name> (<year>2022</year>). <source>Assessing tree shrew high-level visual behavior using conventional and natural paradigms</source> [Poster]. <publisher-name>Society for Neuroscience</publisher-name>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Minini</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Jeffery</surname>, <given-names>K. J.</given-names></string-name> (<year>2006</year>). <article-title>Do rats use shape to solve “shape discriminations”?</article-title> <source>Learning &amp; Memory</source>, <volume>13</volume>(<issue>3</issue>), <fpage>287</fpage>–<lpage>297</lpage>. <pub-id pub-id-type="doi">10.1101/lm.84406</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Ohayon</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name>, &amp; <string-name><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name> (<year>2012</year>). <article-title>What Makes a Cell Face Selective? The Importance of Contrast</article-title>. <source>Neuron</source>, <volume>74</volume>(<issue>3</issue>), <fpage>567</fpage>–<lpage>581</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.024</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Op de Beeck</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Torfs</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Wagemans</surname>, <given-names>J.</given-names></string-name> (<year>2008</year>). <article-title>Perceived Shape Similarity among Unfamiliar Objects and the Organization of the Human Object Vision Pathway</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>(<issue>40</issue>), <fpage>10111</fpage>–<lpage>10123</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2511-08.2008</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Petry</surname>, <given-names>H. M.</given-names></string-name>, &amp; <string-name><surname>Bickford</surname>, <given-names>M. E.</given-names></string-name> (<year>2019</year>). <article-title>The Second Visual System of The Tree Shrew</article-title>. <source>Journal of Comparative Neurology</source>, <volume>527</volume>(<issue>3</issue>), <fpage>679</fpage>–<lpage>693</lpage>. <pub-id pub-id-type="doi">10.1002/cne.24413</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Petry</surname>, <given-names>H. M.</given-names></string-name>, <string-name><surname>Clark</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Day-Brown</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bolin</surname>, <given-names>R. T.</given-names></string-name>, &amp; <string-name><surname>Bickford</surname>, <given-names>M.</given-names></string-name> (<year>2012</year>). <article-title>Behavioral measurement of RDK velocity discrimination thresholds in the tree shrew</article-title>. <source>Journal of Vision</source>, <volume>12</volume>(<issue>9</issue>), <fpage>1223</fpage>. <pub-id pub-id-type="doi">10.1167/12.9.1223</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Pospisil</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Pasupathy</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bair</surname>, <given-names>W.</given-names></string-name> (<year>2018</year>). <article-title>“Artiphysiology” reveals V4-like shape tuning in a deep network trained for image classification</article-title>. <source>ELife</source>, <volume>7</volume>, <fpage>e38242</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.38242</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Schnell</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Bergh</surname>, <given-names>G. V. den</given-names></string-name>, <string-name><surname>Vermaercke</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Gijbels</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bossens</surname>, <given-names>C.</given-names></string-name>, &amp; Op de <string-name><surname>Beeck</surname>, <given-names>H.</given-names></string-name> (<year>2019</year>). <article-title>Face categorization and behavioral templates in rats</article-title>. <source>Journal of Vision</source>, <volume>19</volume>(<issue>14</issue>), <fpage>9</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1167/19.14.9</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Schnell</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Vinken</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>de Beeck</surname>, <given-names>H. O.</given-names></string-name> (<year>2023</year>). <article-title>The importance of contrast features in rat vision</article-title>. <source>Scientific Reports</source>, <volume>13</volume>(<issue>1</issue>), Article <fpage>1</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-023-27533-3</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="book"><string-name><surname>Sinha</surname>, <given-names>P.</given-names></string-name> (<year>2002</year>). <chapter-title>Qualitative Representations for Recognition</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>H. H.</given-names> <surname>Bülthoff</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Wallraven</surname></string-name>, <string-name><given-names>S.-W.</given-names> <surname>Lee</surname></string-name>, &amp; <string-name><given-names>T. A.</given-names> <surname>Poggio</surname></string-name></person-group> (Eds.), <source>Biologically Motivated Computer Vision</source> (pp. <fpage>249</fpage>–<lpage>262</lpage>). <publisher-name>Springer</publisher-name>. <pub-id pub-id-type="doi">10.1007/3-540-36181-2_25</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Tafazoli</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Filippo</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> (<year>2012</year>). <article-title>Transformation-Tolerant Object Recognition in Rats Revealed by Visual Priming</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>(<issue>1</issue>), <fpage>21</fpage>–<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3932-11.2012</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Tafazoli</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Safaai</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>De Franceschi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Rosselli</surname>, <given-names>F. B.</given-names></string-name>, <string-name><surname>Vanzella</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Riggi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Buffolo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> (<year>2017</year>). <article-title>Emergence of transformation-tolerant representations of visual objects in rat lateral extrastriate cortex</article-title>. <source>ELife</source>, <volume>6</volume>, <fpage>e22794</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.22794</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Vermaercke</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Gerich</surname>, <given-names>F. J.</given-names></string-name>, <string-name><surname>Ytebrouck</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Arckens</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Op de Beeck</surname>, <given-names>H. P.</given-names></string-name>, &amp; <string-name><surname>Van den Bergh</surname>, <given-names>G.</given-names></string-name> (<year>2014</year>). <article-title>Functional specialization in rat occipital and temporal visual cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>112</volume>(<issue>8</issue>), <fpage>1963</fpage>–<lpage>1983</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00737.2013</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Vermaercke</surname>, <given-names>B.</given-names></string-name>, &amp; Op de <string-name><surname>Beeck</surname>, <given-names>H. P.</given-names></string-name> (<year>2012</year>). <article-title>A Multivariate Approach Reveals the Behavioral Templates Underlying Visual Discrimination in Rats</article-title>. <source>Current Biology</source>, <volume>22</volume>(<issue>1</issue>), <fpage>50</fpage>–<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2011.11.041</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Vinken</surname>, <given-names>K.</given-names></string-name>, &amp; Op de <string-name><surname>Beeck</surname>, <given-names>H.</given-names></string-name> (<year>2021</year>). <article-title>Using deep neural networks to evaluate object vision tasks in rats</article-title>. <source>PLOS Computational Biology</source>, <volume>17</volume>(<issue>3</issue>), <fpage>e1008714</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008714</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Vinken</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Van den Bergh</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Vermaercke</surname>, <given-names>B.</given-names></string-name>, &amp; Op de <string-name><surname>Beeck</surname>, <given-names>H. P.</given-names></string-name> (<year>2016</year>). <article-title>Neural Representations of Natural and Scrambled Movies Progressively Change from Rat Striate to Temporal Cortex</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>7</issue>), <fpage>3310</fpage>–<lpage>3322</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhw111</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Vinken</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Vermaercke</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Op de Beeck</surname>, <given-names>H.</given-names></string-name> (<year>2014</year>). <article-title>Visual Categorization of Natural Movies by Rats</article-title>. <source>Journal of Neuroscience</source>, <volume>34</volume>(<issue>32</issue>), <fpage>10645</fpage>–<lpage>10658</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3663-13.2014</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Yue</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Cassidy</surname>, <given-names>B. S.</given-names></string-name>, <string-name><surname>Devaney</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Holt</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Tootell</surname>, <given-names>R. B. H.</given-names></string-name> (<year>2011</year>). <article-title>Lower-Level Stimulus Features Strongly Influence Responses in the Fusiform Face Area</article-title>. <source>Cerebral Cortex</source>, <volume>21</volume>(<issue>1</issue>), <fpage>35</fpage>–<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhq050</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name> (<year>2015</year>). <article-title>Invariant visual object recognition and shape processing in rats</article-title>. <source>Behavioural Brain Research</source>, <volume>285</volume>, <fpage>10</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1016/j.bbr.2014.12.053</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Oertelt</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Cox</surname>, <given-names>D. D.</given-names></string-name> (<year>2009</year>). <article-title>A rodent model for the study of invariant visual object recognition</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>21</issue>), <fpage>8748</fpage>–<lpage>8753</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0811583106</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87719.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study reports vastly different object recognition performance in rats and humans using comparable and rigorous behavioural testing, showing that rats performance is driven by low-level visual features whereas human performance is driven by more complex features. The evidence in support of these claims is intriguing but <bold>incomplete</bold>, and additional analyses are needed to elucidate the visual features that drive these differences and investigate inter-individual variability. The authors also need to acknowledge and discuss other factors such as visual acuity and training paradigm (extensive training on only two objects) that could have led to these different patterns of performance.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87719.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this study, the authors have compared object recognition in humans and rats. To this end, they trained rats to touch a target object shown along with a distractor object. The rats were initially trained on a base pair, and then tested on sets of variant pairs where the target or distractor could be transformed through size, position, 3d rotation or lighting variations. In addition, the authors then used a cDNN to find image pairs that would elicit different performance from early vs late layers, and tested rats and humans on these pairs (zero vs high and high vs zero protocols). A similar protocol was used for humans as well to get their performance on the base pair and test pairs. Finally the authors find the correlation between cDNN performance on each layer and rat or human performance across all test protocols. The main finding is that rats show the best match to earlier cDNN layers compared to humans. Based on this the authors conclude that humans and rats show contrasting performance on object recognition.</p>
<p>General comments</p>
<p>
Whether rats and humans have similar object representations is an interesting and fundamental question, and I commend the authors for their extensive matched experiments on rats and humans. However, the conclusions must be tempered by the fact that the authors are testing a limited set of object variations derived from just two objects. There are also potentially substantial differences in the tasks given to rats and humans, if I understand the methods and procedures correctly. My concerns are detailed below.</p>
<p>My main concern is that the authors find very low agreement between rats and humans on comparable tasks, but it would be important if they can identify qualitative differences in the performance. For instance, can they say that rats are using low-level visual cues compared to humans. They could compare several low-level visual models (see below) and report how human and rat accuracy compares to each of these models. Since the visual representations of cDNNs are unknown, such a comparison would be useful.</p>
<p>The authors should also discuss the potential reason for the human-rat differences too, and importantly discuss whether these differences are coming from the rather unusual approach of training used in rats (i.e. to identify one item among a single pair of images), or perhaps due to the visual differences in the stimuli used (what were the image sizes used in rats and humans?). Can they address whether rats trained on more generic visual tasks (e.g. same-different, or category matching tasks) would show similar performance as humans?</p>
<p>I also found that a lot of essential information is not conveyed clearly in the manuscript. Perhaps it is there in earlier studies but it is very tedious for a reader to go back to some other studies to understand this one. For instance, the exact number of image pairs used for training and testing for rats and humans was either missing or hard to find out. The task used on rats was also extremely difficult to understand. An image of the experimental setup or a timeline graphic showing the entire trial with screenshots would have helped greatly.</p>
<p>The authors state that the rats received random reward on 80% of the trials, but is that on 80% of the correctly responded trials or on 80% of trials regardless of the correctness of the response? If these are free choice experiments, then the task demands are quite different. This needs to be clarified. Similarly, the authors mention that 1/3 of the trials in a given test block contained the old base pair - are these included in the accuracy calculations?</p>
<p>It would be good for the authors to articulate more clearly the nature of the differences between humans and rats. For instance, rat behaviour was found to be correlated with low-level image properties like brightness, whereas presumably, human behaviour is not. It would be useful if the authors can compare rat behaviour against several possible alternative models, including the dCNN layers in Figure 4. These models could include other rats (giving a reliability estimate), luminance based models, contrast based models, models based on V1 simple cells, etc - these models would elucidate further the nature of the rat performance. A similar analysis could be done for human performance.</p>
<p>The authors were injecting noise with stimuli to cDNN to match its accuracy to rat. However, that noise potentially can interacted with the signal in cDNN and further influence the results. That could generate hidden confound in the results. Can they acknowledge/discuss this possibility?</p>
<p>The authors claimed that discrimination task in rats was more dependent on concavity than component arrangement (figure 1 left panel). But that could be just an artifact due to sampling more values of concavity than component arrangement. In that case these two attributes are not comparable at all. Could the authors address this issue in some way.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87719.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Schnell et al. performed two extensive behavioral experiments concerning the processing of objects in rats and humans. To this aim, they designed a set of objects parametrically varying along alignment and concavity and then they used activations from a pretrained deep convolutional neural network to select stimuli that would require one of two different discrimination strategies, i.e. relying on either low- or high-level processing exclusively. The results show that rodents rely more on low-level processing than humans.</p>
<p>Strengths:</p>
<p>1. The results are challenging and call for a different interpretation of previous evidence. Indeed, this work shows that common assumptions about task complexity and visual processing are probably biased by our personal intuitions and are not equivalent in rodents, which instead tend to rely more on low-level properties.</p>
<p>
2. This is an innovative (and assumption-free) approach that will prove useful to many visual neuroscientists. Personally, I second the authors' excitement about the proposed approach, and its potential to overcome the limits of experimenters' creativity and intuitions. In general, the claims seem well supported and the effects sufficiently clear.</p>
<p>
3. This work provides an insightful link between rodent and human literature on object processing. Given the increasing number of studies on visual perception involving rodents, these kinds of comparisons are becoming crucial.</p>
<p>
4. The paper raises several novel questions that will prompt more research in this direction.</p>
<p>Weaknesses:</p>
<p>1. There are a few inconsistencies in the number of subjects reported. Sometimes 45 humans are mentioned and sometimes 50. Probably they are just typos, but it's unclear.</p>
<p>
2. A few aspects mentioned in the introduction and results are only defined in the Methods thus making the manuscript a bit hard to follow (e.g. the alignment dimension), htus I had to jump often from the main text to the methods to get a sense of their meaning.</p>
<p>
3. The choices related to the stimulus design and the network used to categorize them are not fully described and would benefit from some further clarification/justification. The choice of alignment and concavity as baseline properties of the stimuli is not properly discussed. Also, from the low-correlations I got the feeling that AlexNet is just not a good model of rat visual processing. Which indeed can be interpreted as further evidence of what the authors are trying to demonstrate, but it might also be an isolated case.</p>
<p>
4. Many important aspects of the task are not fully described in the Methods (e.g. size of the stimuli, reaction times and basic statistics on the responses).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87719.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Schnell and colleagues trained rats on a visual categorization task. They found that rats could discriminate objects across various image transformations. Rat performance correlated best with late convolutional layers of an artificial neural network. In contrast, human performance showed the strongest correlation with higher, fully connected layers, indicating that rats employed simpler strategies to accomplish this task as compared to humans. This is a methodologically rigorous study. The authors tested a substantial number of rats across a large variety of stimuli. One notable strength is the use of neural networks to generate stimuli with varying levels of complexity. This approach shows significant potential as a principled model for conducting studies on object recognition and other related visual behavioral phenomena. The data strongly support the conclusion that rats and humans rely on different visual features for discrimination tasks. Overall, this is a valuable study that provides novel, important insights into the visual capabilities of rats. However, some aspects of the study need further clarification. The study does not provide clear insights into the visual features that enable rats to perform these discriminations. The relationship between neural network layers and specific aspects of visual behavior is not well-defined, representing a key limitation of the current work. Further, the current analyses do not adequately address the consistency of visual behaviors across different rats or whether different rats rely on the same visual features to accomplish the task. Lastly, rodent performance was substantially lower compared to humans and generally worse than neural network classification. The factors contributing to this disparity are unclear.</p>
</body>
</sub-article>
</article>