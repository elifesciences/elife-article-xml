<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107423</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107423</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107423.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Context-Free Model of Savings in Motor Learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4883-4376</contrib-id>
<name>
<surname>Shahbazi</surname>
<given-names>Mahdiyar</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0796-5457</contrib-id>
<name>
<surname>Codol</surname>
<given-names>Olivier</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5179-3181</contrib-id>
<name>
<surname>Michaels</surname>
<given-names>Jonathan A</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1368-032X</contrib-id>
<name>
<surname>Gribble</surname>
<given-names>Paul L</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>pgribble@uwo.ca</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Dept. Psychology, Western University</institution></institution-wrap>, <city>London</city>, <country country="CA">Canada</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05c22rx21</institution-id><institution>Mila–Québec Artificial Intelligence Institute</institution></institution-wrap>, <city>Montréal</city>, <country country="CA">Canada</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0161xgx34</institution-id><institution>Dept. Neuroscience, Université de Montréal</institution></institution-wrap>, <city>Montréal</city>, <country country="CA">Canada</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Dept. Physiology &amp; Pharmacology, Schulich School of Medicine &amp; Dentistry</institution></institution-wrap>, <city>London</city>, <country country="CA">Canada</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05fq50484</institution-id><institution>School of Kinesiology and Health Science, Faculty of Health, York University</institution></institution-wrap>, <city>Toronto</city>, <country country="CA">Canada</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ponte Costa</surname>
<given-names>Rui</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>co-senior authors</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-08-20">
<day>20</day>
<month>08</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107423</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-05-20">
<day>20</day>
<month>05</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-04-23">
<day>23</day>
<month>04</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.03.26.645562"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Shahbazi et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Shahbazi et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107423-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Learning to adapt voluntary movements to an external perturbation, whether mechanical or visual, is faster during a second encounter than during the first. The mechanisms underlying this phenomenon, known as savings, remain unclear. Recent studies propose that the high dimensionality of neural control enables the retention of learning traces that may facilitate savings. To test this idea we used MotorNet, a framework for training recurrent neural networks (RNNs) to control biomechanical models of the human upper limb. RNNs were trained to perform reaching movements with a velocity-dependent force field (FF) and without (NF) in the sequence NF1 (baseline), FF1 (adaptation), NF2 (washout), and FF2 (re-adaptation). RNNs showed behavioural signatures of savings in the absence of any explicit contextual input signalling the presence or absence of the FF. Savings was more robust in RNNs with larger numbers of units. We identified a component of RNN activity associated with savings—a shift in preparatory activity that persisted even after washout. Displacing this preparatory activity in the direction of the shift enhanced savings, whereas perturbations in the opposite direction reduced or eliminated savings. These findings suggest a potential neural basis for motor memory retention underlying savings that is reliant on the high dimensionality of neural space, and is independent of cognitive or strategic learning.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>artificial recurrent neural network</kwd>
<kwd>motor learning</kwd>
<kwd>savings</kwd>
<kwd>arm movement</kwd>
<kwd>MotorNet</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h531d29</institution-id>
<institution>Natural Sciences and Engineering Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>RGPIN/05458-2018</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>corrections made to Grants section and to Author contributions section</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>In studies of motor learning, <italic>savings</italic> commonly refers to a phenomenon in which learning is superior after previous exposure to a motor task. Savings has been demonstrated in the context of voluntary reaching movements for adaptation to novel visuomotor perturbations and for learning to counter novel mechanical environments such as velocity-dependent curl force fields (FF) [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c7">7</xref>]. In a typical experiment, participants are initially exposed to a novel perturbation environment and they practice reaching to targets until an asymptotic level of performance is reached, for example recovery of approximately straight-line hand paths. Following this initial learning the perturbation is removed, and participants practice again until behavioural performance in this “washout” phase returns to pre-learning baseline levels. After washout participants are re-exposed to the perturbing environment. Savings is observed as a faster learning rate when re-exposed to the perturbing environment compared to initial learning, and sometimes also as a superior initial level of performance compared to when the perturbing environment was first encountered [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>].</p>
<p>There is some debate about the mechanisms that may be responsible for savings [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c5">5</xref>]. Some propose that savings is produced by explicit, cognitive or strategic processes such as a conscious memory of the action selection strategy [<xref ref-type="bibr" rid="c2">2</xref>], a contextual signal associated with the perturbing environment [<xref ref-type="bibr" rid="c9">9</xref>], meta-learning of adaptation parameters such as learning rate [<xref ref-type="bibr" rid="c10">10</xref>–<xref ref-type="bibr" rid="c12">12</xref>], or reinforcement-based memories of successful execution [<xref ref-type="bibr" rid="c13">13</xref>]. Others have proposed that savings may arise from implicit learning processes that are not based on conscious, strategic mechanisms, including an increased sensitivity to previously experienced errors [<xref ref-type="bibr" rid="c8">8</xref>], use-dependent plasticity [<xref ref-type="bibr" rid="c14">14</xref>], or implicit updating of internal models that predict the sensory consequences of action [<xref ref-type="bibr" rid="c15">15</xref>].</p>
<p>Recent advances have been made in the ability to record from large numbers of neurons during motor learning tasks [<xref ref-type="bibr" rid="c16">16</xref>] and this has resulted in new approaches to understanding the relationship between high dimensional neural population activity and sensory, motor and task parameters during, and even prior to, voluntary movement [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c18">18</xref>]. These new ways of thinking have prompted us and others to consider how seemingly complex behavioural phenomena may emerge from the dynamics of populations of neurons [<xref ref-type="bibr" rid="c19">19</xref>]. Sun, O’Shea, and colleagues recorded neural activity in primary motor cortex of rhesus macaques during a FF reaching task and identified a neural subspace of network activity during the preparatory period prior to movement that shifted after learning [<xref ref-type="bibr" rid="c20">20</xref>]. This “uniform shift” persisted even after behavioural washout of FF learning. The authors proposed that this neural trace of prior learning could facilitate subsequent savings [<xref ref-type="bibr" rid="c20">20</xref>]. Losey and colleagues used a brain-computer interface learning paradigm to study how neural population activity in the primary motor cortex of monkeys supports motor learning of multiple tasks [<xref ref-type="bibr" rid="c21">21</xref>]. They identified a change in a subspace of neural population activity that supported behavioural performance of a new task without interfering with a previously learned task. They proposed that the high dimensionality of neural activity in primary motor cortex allows for the formation of memory traces that supports multiple behaviours without interference.</p>
<p>In the present paper we used artificial recurrent neural network (RNN) models of upper limb motor control to test the idea that high dimensional neural control facilitates the encoding of new motor memories without interfering with previously stored motor control policies, and that these neural traces of previous learning underlie subsequent savings, without the need for additional contextual signals. We used MotorNet [<xref ref-type="bibr" rid="c22">22</xref>] to train RNNs to control a mathematical model of the upper limb neuromuscular system [<xref ref-type="bibr" rid="c23">23</xref>] in the context of a simulated FF reaching task [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>]. Even without any explicit contextual cue signalling the presence of absence of FFs, RNNs showed behavioural signatures of savings. In addition savings was more robust as the number of units in RNNs increased. Using approaches similar to those described in previous studies we identified a learning-related shift in neural activity in the preparatory period prior to movement onset [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. We established a causal relationship between this neural shift and savings by perturbing neural activity along the direction of this neural shift. When RNN hidden unit activity was shifted in the direction of the neural shift, savings was enhanced, whereas neural perturbations in the opposite direction reduced or abolished savings. Our findings support the hypothesis that a neural basis of motor memory retention underlies savings, one that could be independent of any cognitive or strategic learning components and that depends upon the high dimensionality of neural space.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<p>We trained 40 recurrent neural networks (RNNs) with 128 fully connected gated recurrent units (GRUs) to control a mathematical model of the human upper limb [<xref ref-type="bibr" rid="c22">22</xref>] (<xref rid="fig1" ref-type="fig">Figure 1a,b</xref>). Task inputs to the RNN are the Cartesian coordinates of the movement target (<italic>x, y</italic>) and a binary go signal (0 or 1) indicating when to initiate movement (<xref rid="fig1" ref-type="fig">Figure 1c</xref>). The RNN also receives time-delayed feedback signals corresponding to the length and velocity of each muscle, and the Cartesian coordinates of the endpoint of the limb. The output of the RNN is time-varying muscle stimulation commands to each of 6 upper limb muscles (<xref rid="fig1" ref-type="fig">Figure 1c</xref>). Muscle stimulation commands range between 0 and 1 and act on a musculoskeletal model of the upper limb which includes multi-joint limb dynamics and a hill-type muscle model [<xref ref-type="bibr" rid="c23">23</xref>].</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Recurrent neural network model inputs and outputs.</title>
<p>(<bold>a</bold>) RNNs receive a 17-dimensional input signal consisting of the location of the movement target in Cartesian coordinates, a “visual” feedback signal giving the arm’s endpoint position delivered with a 70 ms delay, a “proprioceptive” feedback signal consisting of the length and velocity of each of the 6 limb muscles delivered with a 20 ms delay, and a binary go cue. RNNs output 6 motor stimulation commands (between 0 and 1) to drive each muscle: SF (shoulder flexors), SE (shoulder extensors), EE (elbow extensors), EF (elbow flexors), BE (bi-articular extensors), and BF (bi-articular flexors). (<bold>b</bold>) The 17-dimensional input signal was mapped to the recurrent network using linear weights <italic>W</italic><sub>in</sub>. RNN output was transformed into motor commands by linear weights <italic>W</italic><sub>out</sub>. The vector <italic>h</italic><sub><italic>t</italic></sub> is the activity of hidden units at time <italic>t</italic>. (<bold>c</bold>) Task-related RNN inputs in a reach toward the rightmost target depicted in (a). For the purpose of illustration in this Figure, we translated the starting and target positions such that the start position is at the coordinates 0, 0. The simulation duration was 1 s, with 10 ms time steps. The goal (dashed lines) was set to the hand’s starting position before the go signal changed to 1, to the movement target position after that. (<bold>d</bold>) Sample endpoint trajectories after training RNNs on reaches to random target locations. Reaching trajectories are indicated in orange, and small gray dots show target positions. The large gray circle indicates the position of the centre-out reaches within the workspace. (<bold>e</bold>) Reaching trajectories and hidden unit activity over time at the end of training in the centre-out task. colours indicate each of 8 targets. The go-cue switches from 0 to 1 at time <italic>t</italic> = 0.</p></caption>
<graphic xlink:href="645562v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The networks were initially trained to produce point to point reaching movements between targets located in random locations throughout the limb’s workspace. No perturbing FF was applied during this initial “growing up” training phase. We refer to the absence of a perturbing FF as a “null field” (NF). RNN weights were updated using backpropagation through time, using the Adam optimizer [<xref ref-type="bibr" rid="c26">26</xref>] implemented in PyTorch [<xref ref-type="bibr" rid="c27">27</xref>]. The loss function for optimization was based on minimizing the difference between hand position and target position, and also included regularization terms that encouraged the network to produce smooth, human-like kinematics, phasic muscle commands, and stable hidden unit activity [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c29">29</xref>] (see <xref rid="fig1" ref-type="fig">Figure 1e</xref> and Methods for details).</p>
<p>After training, the networks produced reaching movements with human-like characteristics including smooth, relatively straight hand paths with bell-shaped velocity profiles, and phasic activity in agonist and antagonist muscles spanning the shoulder and elbow (<xref rid="fig1" ref-type="fig">Figure 1c</xref>).</p>
<p><xref rid="fig1" ref-type="fig">Figure 1d</xref> shows examples of reaching trajectories for reaches to targets located randomly across the limb’s workspace. Models produced human-like reaches both when tested on reaches with random starting points and targets (<xref rid="fig1" ref-type="fig">Figure 1d</xref>) and when tested on centre-out reaches toward 8 equidistant targets (<xref rid="fig1" ref-type="fig">Figure 1e</xref>). Consistent with electrophysiological recordings in monkeys, RNN hidden units showed activity patterns that were relatively stable over time, and distinct for different movement targets during the delay period prior to the go signal (time 0 in <xref rid="fig1" ref-type="fig">Figure 1e</xref>). RNN hidden unit activity during movement was similarly distinct for different movement directions, and showed oscillatory activity consistent with that seen in recordings from motor cortex in non-human primates [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>].</p>
<sec id="s2a">
<label>2.1</label>
<title>Force field adaptation</title>
<p>After the networks were trained to perform point to point reaches in a NF, we implemented a relatively standard experimental sequence common in studies of human motor learning. We trained networks on a centre-out reaching task either in the absence of perturbing forces (null field, NF) or in the presence of a velocity-dependent curl force field (FF) (see Methods). First, networks were exposed to a NF (NF1) to characterize baseline performance. Following this, networks were trained to produce reaches in a clockwise FF (FF1, 3200 batches of training). After initial FF learning networks were re-trained in a NF (NF2, 10000 batches). Following this “washout” phase networks were re-trained in the FF (FF2, 3200 batches) (see <xref rid="fig2" ref-type="fig">Figure 2a</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Networks learn to compensate for curl force fields without any contextual input.</title>
<p>(<bold>a</bold>) Lateral deviation averaged over 8 centre-out reaches for each batch. Black indicates null-field phases (NF1 and NF2), green indicates the first phase of the force field (FF1), and purple indicates the second phase of the force field (FF2). Positive values indicate deviation in the direction of the force field, which is clockwise relative to the line connecting the starting and target positions. (<bold>b</bold>) Simulated reaching trajectories at the beginning and end of each phase, grouped in different columns. (<bold>c</bold>) Motor commands during reaching toward the rightmost target for NF1 and FF1.</p></caption>
<graphic xlink:href="645562v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We characterized behavioural performance of the networks by measuring the maximum deviation of each hand trajectory from a straight line connecting initial and final target positions. During centre-out reaching in the initial NF baseline tests (NF1, <xref rid="fig2" ref-type="fig">Figure 2b</xref>) the network produced straight hand paths with very little lateral deviation.</p>
<p>We then trained the networks to compensate for the effects of a clockwise force field (FF1). The first time networks encountered the FF (batch 0), they exhibited large lateral deviations from a straight line trajectory (<xref rid="fig2" ref-type="fig">Figure 2a,b</xref>). Over the course of training the network’s hidden weights were modified so that the networks produced different muscle stimulation commands that compensated for the forces produced by the FF (<xref rid="fig2" ref-type="fig">Figure 2b</xref>). Only the hidden (recurrent) weights were modified after the “growing up” phase, and not the input/output weights. By the end of FF1, relatively straight hand paths were recovered, and lateral deviation was near that in the NF baseline tests (NF1).</p>
<p>Importantly, at no time during training did the networks receive any contextual signal related to the presence or absence of the FF. Adaptation occurred during FF training because as the simulated limb is perturbed by the FF, hand position deviates away from the target, and the loss function increases. Over training the values of the RNN hidden unit weights are changed to minimize the loss function, and in turn, recover straight hand paths.</p>
<p>As an additional control we trained networks after the growing up phase on an opposing force field (CCW) and then as above, exposed the networks to a NF washout phase, and then to a CW force field. In this case no savings was observed in the CW force field. This underscores that the savings effect observed in the main study was learning-specific— it was due to prior learning of the CW force field, and not a general effect of learning any novel dynamics.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Washout</title>
<p>After force field adaptation (FF1) we trained the networks in a washout phase (NF2) in which we removed the simulated FF that the networks trained on in FF1. As is the case in empirical studies of FF learning, the networks initially showed an after-effect in movement kinematics in the opposite direction of the force field (<xref rid="fig2" ref-type="fig">Figure 2a,b</xref>), indicating that the networks prepared muscle commands to compensate for the (now absent) FF. After training in NF2, networks recovered straight line hand paths that were very similar to the performance in the NF1 baseline phase (only 0.1 mm difference in lateral deviation).</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Re-adaptation</title>
<p>Following washout we re-trained networks in the same curl field (FF2) that they had been exposed to previously in the FF1 block. We observed that when networks initially encountered the FF in FF2, they exhibited smaller lateral deviations than those observed in the beginning of FF1, when they were first exposed to the simulated FF (<italic>t</italic> (39) = 10.940, <italic>p</italic> = 1.9<italic>e</italic> − 13) (<xref rid="fig3" ref-type="fig">Figure 3a</xref>). In addition, networks adapted to the force field in FF2 faster than they did in FF1, as measured by an increased learning rate based on an exponential fit to the learning curve (see Methods; <italic>t</italic> (39) = 9.284, <italic>p</italic> = 2.0<italic>e</italic> − 11). This pattern of improved performance in FF2 is seen in both human and monkey studies of motor learning and is typically referred to as savings.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>RNN models exhibit behavioural characteristics of savings.</title>
<p>(<bold>a</bold>) Averaged learning curves shown in <xref ref-type="fig" rid="fig3">Figure 3a</xref> are overlaid. Sub-panels indicate (left) the lateral deviation at batch 0 (before training in the corresponding phase) and (right) the learning rate <italic>r</italic> after fitting an exponential curve to the lateral deviations over training for each network. (<bold>b</bold>) The percentage of networks (40 total) with savings is plotted against the number of RNN hidden units. The dashed line indicates the percentage of networks with savings, defined as the learning rate in FF2 being larger than in FF1. The solid line shows percentage of networks showing savings defined by lateral deviation at batch 0 being smaller in FF2 than FF1. Error bars indicate the 95% confidence interval.</p></caption>
<graphic xlink:href="645562v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The difference in performance between FF1 and FF2 indicates that the network weights changed in such a way that facilitated improved initial performance and faster learning rate in FF2 as compared to FF1, while also not interfering with the performance in NF2. In other words, after training FF1, and washout in NF2, the networks retained enough information about the force field to improve re-learning in FF2, and this retained information was stored in such a way that it did not interfere with the ability of the networks to perform in NF2 just as they had done in NF1, before any FF learning. The pattern of savings observed here in our RNNs occurred despite the absence of any explicit contextual signal indicating the presence or absence of the simulated FF. When contextual signals are present, neural network models often create separate representations for two different tasks [<xref ref-type="bibr" rid="c32">32</xref>]. We hypothesize that in our study the high dimensionality of the RNNs allows them to develop representations that effectively serve the ongoing task while also preserving information about previously learned tasks.</p>
<p>We tested this hypothesis by repeating all simulations using RNNs with different numbers of hidden units. We found that as the number of hidden units increases, the likelihood of networks expressing savings also increases. For models with 256 hidden units, there was an 80% chance of expressing savings based on both the learning rate (if the learning rate is faster in FF2 than in FF1) and lateral deviation (if the initial lateral deviation in FF2 is smaller than when FF1 is first encountered). This probability could drop to nearly chance level if the number of hidden units is smaller than 32. This supports the idea that the ability to store previously learned information in RNNs in a way that doesn’t interfere with previous learning is based on the dimensionality of the network’s weight space.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Learning related changes in hidden unit activity</title>
<p>To characterize changes in hidden unit activity after learning we followed a similar approach to that described by Sun, O’Shea, and colleagues [<xref ref-type="bibr" rid="c20">20</xref>], in which they investigated how neural population activity in motor cortex changed after monkeys learned to adapt to FFs in an upper limb reaching task. The focus is on hidden unit activity during the preparatory phase, prior to the go signal, as this is the primary determinant of the feed-forward motor commands to muscles [<xref ref-type="bibr" rid="c31">31</xref>].</p>
<p>We examined changes in a subspace defined by the relationship between hidden unit activity at the end of the preparatory period, just prior to the go cue, and movement-related force at the initial acceleration phase of the movement (hereafter referred to as the TDR subspace, see Methods for further details). We identified the TDR subspace by linearly regressing the preparatory hidden unit activity (340 ms before the go-cue) onto the endpoint (hand) force early during execution (90 ms after the go-cue; see Methods). Projecting the preparatory hidden unit activity associated with all 8 centre-out reaches onto this subspace revealed a ring formation (<xref rid="fig4" ref-type="fig">Figure 4a</xref>). This circular pattern has also been observed in empirical studies of adaptation for motor cortical neurons [<xref ref-type="bibr" rid="c20">20</xref>]. This ring rotated in a counter-clockwise (CCW) following adaptation to a clockwise (CW) FF. This is consistent with the idea that that after training in the CWFF the preparatory activity of the RNN hidden units is tuned to facilitate the production of forces in the direction opposite to the upcoming FF during movement.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Changes in the preparatory activity of RNN hidden units following FF learning.</title>
<p><bold>a</bold>: Projection of the hidden preparatory activity (340 ms before the go-cue) of an example trained model performing 8 centre-out reaches on the force-predictive subspace acquired with targeted dimensionality reduction (TDR). Different reaching targets are indicated with different colours, and different adaptation phases are indicated with different shapes: circle for NF1, triangle for FF1, square for NF2, and star for FF2. <bold>b</bold>: A schematic illustration of the uniform shift. Each cross indicates the centre of the hidden preparatory activity for NF1 and FF1, and the arrow indicates the uniform shift. <bold>c</bold>: Projection of the hidden preparatory activity of all phases onto the uniform shift after orthogonalizing the uniform shift with respect to TDR. The data are scaled so that the projection of NF1 onto the uniform shift is zero and the projection of FF1 is one.</p></caption>
<graphic xlink:href="645562v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>After washout (NF2), this rotation reverted, leaving little or no residual part of the initial CCW rotation in the preparatory hidden unit activity (<xref rid="fig4" ref-type="fig">Figure 4a</xref>). The preparatory hidden unit activity again rotated in a CCW direction after adaptation to the CWFF in FF2.</p>
<p>We probed the learning-related changes in RNN hidden unit activity that occurred outside of the TDR subspace. To do this, we calculated the extent to which the centroid of the preparatory hidden unit activity for 8 centre-out targets shifted following FF learning. Following the procedure used in Sun, O’Shea et al. [<xref ref-type="bibr" rid="c20">20</xref>], to isolate learning-related changes in hidden unit activity common to all reach directions we calculated the difference between the mean preparatory activity (340 ms before go cue) of NF1 and FF1 and then orthogonalized this with respect to the TDR subspace. The result is referred to as a “uniform shift” (<xref rid="fig4" ref-type="fig">Figure 4b</xref>). After projecting the mean preparatory activity of all experimental phases onto this uniform shift and normalizing the projection values such that the NF1 projection is 0 and the FF1 projection is 1 (see Methods), we observed that at the end of the washout phase (NF2), the RNN hidden unit activity still showed a projection onto this direction that was significantly different than zero (t(39)=7.484, p=4.6e-9; <xref rid="fig4" ref-type="fig">Figure 4c</xref>). This indicates that the mean hidden unit activity during the preparatory period did not fully revert to pre-adaptation levels, despite full behavioural washout by the end of the washout phase (<xref rid="fig2" ref-type="fig">Figure 2a</xref>). This result is consistent with findings in monkey motor cortex [<xref ref-type="bibr" rid="c20">20</xref>], and the idea that this residual hidden unit activity captures information about the previously learned FF, and can be linked to subsequent savings.</p>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Perturbing preparatory hidden unit activity along the uniform shift</title>
<p>The fact that the preparatory activity of RNN hidden units did not fully revert to the NF1 levels in the uniform shift direction suggests that this residual activity might underlie savings [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. We tested this idea directly by perturbing hidden unit activity along the direction of the uniform shift, and we probed the effect of these perturbations on behavioural measures of savings.</p>
<p>We added a proportion of the uniform shift to the preparatory hidden unit activity of networks performing centre-out reaches and we measured the resulting changes in the lateral deviation of simulated hand trajectories. Importantly, the perturbations to preparatory hidden unit activity resulted in little or no changes in muscle activity prior to movement, and during movement itself no perturbations were delivered. We used models at batch 0 of the FF2 phase, when they had not yet been trained on FF adaptation a second time. We have already shown that in FF2, hand trajectory lateral deviation is smaller than in FF1, and we used this as a metric to characterize savings (<xref rid="fig3" ref-type="fig">Figure 3a</xref>). We examined how much the lateral deviation at batch 0 would change following perturbations to RNN hidden unit activity in the direction of the uniform shift. Details about how the perturbations to hidden unit activity were implemented may be found in Methods.</p>
<p>When we perturbed the preparatory hidden unit activity in the opposite direction of the uniform shift (negative magnitudes in <xref rid="fig5" ref-type="fig">Figure 5a</xref>), lateral deviation of hand trajectories increased. By increasing the magnitude of perturbation further, we could effectively abolish savings altogether. In contrast, if we perturbed the RNN hidden unit preparatory activity in the same direction as the uniform shift, lateral deviation of hand trajectories was reduced, thus enhancing savings. Example trajectories toward the right-most target are shown in <xref rid="fig5" ref-type="fig">Figure 5a</xref> for each uniform shift perturbation. These results support the idea that the hidden unit activity in the direction of the uniform shift that remains after washout represents a neural trace of the initial FF learning, one that supports subsequent savings when the models adapt to the FF a second time.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Uniform shift in RNN hidden unit activity is related to savings.</title>
<p><bold>A</bold>: Lateral deviation in FF2 (purple) when the hidden preparatory activity was perturbed in the positive (+) and negative (−) directions of the uniform shift with different magnitudes. Arrows point to the trajectories of an example model when the hidden unit activity was perturbed (red trajectories) or not (blue trajectories). The lateral deviation hand trajectories for FF1 are illustrated in green. <bold>B</bold>: Motor commands of the same model performing the same reach as shown in panel A when the hidden unit activity was perturbed with a magnitude of 2.0. The vertical dashed line indicates the time at which the perturbation was delivered. The lower sub-panel shows the difference from the unperturbed motor command. <bold>C</bold>: The activity of 128 RNN hidden units after being perturbed (dashed line). The lower sub-panel shows the difference between the unperturbed and perturbed RNN hidden unit activity.</p></caption>
<graphic xlink:href="645562v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The perturbations changed the activity of hidden units (<xref rid="fig5" ref-type="fig">Figure 5c</xref>). These changes were large early after the delivery of the perturbation, and then they reached a steady state. However, is it important to note that these changes in the preparatory hidden unit activity did not result in substantive changes in the motor commands (<xref rid="fig5" ref-type="fig">Figure 5b</xref>), which emphasizes that the uniform shift resides in the null space of motor output.</p>
<p>In summary, the activity of networks along the direction of the uniform shift did not revert fully to pre-adaptation levels after the washout phase, despite the behaviour (hand trajectories and muscle activity) being fully washed out, the same as pre-adaptation baseline performance. Perturbing RNN hidden unit activity along the direction of the uniform shift enhanced savings while perturbations which reduced this residual hidden unit activity reduced or eliminated savings. RNN models with higher dimensionality (more hidden units) were more likely to exhibit evidence of savings while smaller models were less likely to show savings.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>We have shown here that RNNs trained to control a computational model of the upper limb show behavioural signatures of savings when learning multiple FFs in succession. We also found that savings is enhanced when the dimensionality of the control network is higher. Following the approach described in a recent electrophysiological study of monkey motor cortex [<xref ref-type="bibr" rid="c20">20</xref>], we identified a subspace of RNN hidden unit activity during the preparatory period after target presentation but prior to movement that shifts after initial FF learning, and subsequently retreats after behavioural washout in a NF. Importantly, this “uniform shift” did not retreat all the way back to pre-FF baseline levels after washout (see <xref rid="fig4" ref-type="fig">Figure 4c</xref>). Despite this residual trace of FF learning after washout, in a NF the RNN produced reaches to targets that were the same as those produced prior to initial learning. Importantly, alternating network training on opposing fields (CW and CCW) did not produce savings.</p>
<p>We interpret this residual hidden unit activity as a neural trace of the initial learning that remains after washout, and this is consistent with the idea that this persistent signal contributes to savings [<xref ref-type="bibr" rid="c20">20</xref>]. We tested this hypothesis directly by perturbing RNN hidden unit activity during the preparatory period along the direction of the uniform shift—an approach that is possible in a computational model but is not presently feasible in a biological neural network. After NF washout when we re-exposed networks to a previously learned FF, perturbations that amplified the residual trace of learning (increasing activity along the uniform shift) resulted in increased savings. When we perturbed hidden unit activity in the other direction to reduce activity along the uniform shift, savings was reduced and in some cases abolished altogether. These results provide evidence of a causal link between the identified neural trace of learning that remains after washout and subsequent savings when RNNs are re-exposed to the previously learned FF.</p>
<p>Our results are compatible with the proposal by Sun, O’Shea, et al. [<xref ref-type="bibr" rid="c20">20</xref>] that the activity of neurons in primary motor cortex during the preparatory period prior to movement contains a component that tracks previously learned motor behaviour. They propose that these residual traces of prior learned behaviour are encoded in a way that separates the associated motor memories in neural state space and facilitates recall of the appropriate control policies. Losey et al. [<xref ref-type="bibr" rid="c21">21</xref>] describe a similar account in the context of a brain-machine interface in which new motor learning is encoded in a neighbouring region of neural state space such that it solves the motor control task but doesn’t interfere with prior learning. In our RNNs this was achieved through learning-related changes in the recurrent weights, such that after NF washout the component of the uniform shift that remained didn’t interfere with NF motor behaviour, but did produce savings when networks were re-exposed to the previously learned FF.</p>
<p>Our finding that higher-dimensional RNNs are more likely to produce savings supports the idea that encoding newly learned control policies so that they do not interfere with previously learned motor memories depends upon the availability of adequate dimensionality in neural state space. This implies that multiple motor memories can be encoded in neural subspaces so that they do not interfere with each other. A number of recent empirical studies support this idea. Kim et al. [<xref ref-type="bibr" rid="c33">33</xref>] recorded from anterior lateral motor cortex of mice over several months and tracked how neurons represented different learned motor tasks. They found that learning produced new neural representations that did not modify existing representations, and re-exposure to a previously learned motor task re-activated the previous neural activity patterns. In a recent paper Bernardi et al. [<xref ref-type="bibr" rid="c34">34</xref>] recorded neural activity in prefrontal cortex and hippocampus of monkeys during cognitive tasks, and found that multiple abstract task-related variables were encoded in neural state space using a geometry that allowed separability using linear classifiers. Neural recordings in monkey motor cortex show that this kind of task representation emerges prior to movement, in preparatory activity after a movement target is shown but before a go signal instructs the animal to begin a movement [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c30">30</xref>]. A similar time course emerged in our RNN simulations here (<xref rid="fig1" ref-type="fig">Figure 1e</xref>, <xref rid="fig4" ref-type="fig">Figure 4a</xref>).</p>
<p>In primates presumably high-level contextual cues can aid in indexing the appropriate previously learned control policy by activating populations of neurons in a neural direction appropriate for the previously learned task. Indeed a number of theoretical accounts exist that position contextual cues as a driver of motor memory encoding and selection [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>]. Similar indexing is likely occurring in accounts where the learning of FFs that would normally interfere is avoided by associating each with the planning [<xref ref-type="bibr" rid="c37">37</xref>] or imagination [<xref ref-type="bibr" rid="c38">38</xref>] of different follow-through movements. In our RNNs no such contextual signal was provided, and so the question arises, how are residual traces of previously learned FFs activated? One possibility is that the error signals encountered when our RNNs are re-exposed to a previously learned FF activate neurons within the previously learned subspace. This kind of scheme in which re-exposure to previously encountered errors produces savings is consistent with accounts in which a history of errors or corrections to errors plays a role in motor memory formation [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c8">8</xref>].</p>
<p>In a recent computational modelling study Dubreuil et al. [<xref ref-type="bibr" rid="c18">18</xref>] proposed that non-random neural population connectivity structures involving multiple subpopulations that play functionally distinct roles encode multiple tasks better than random connectivity structures. This seems compatible with the idea presented here and in other recent work that low-dimensional recurrent subspaces embedded within a high-dimensional neural control space are used to encode features of movements such as target directions [<xref ref-type="bibr" rid="c30">30</xref>], anticipated sensory consequences of perturbations [<xref ref-type="bibr" rid="c39">39</xref>], and motor skills [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. The idea that motor memories are encoded in a distributed sensorimotor network and that features of motor adaptation emerge as a result of the dynamical properties of recurrent neural circuits have also been discussed in other computational modelling studies using recurrent neural networks. Ajemian et al. [<xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c41">41</xref>] proposed a theoretical framework in which error signals prompt a reorganization of synaptic connectivity to encode motor memories within a high dimensional neural space.</p>
<p>The work presented here adds to the growing literature documenting how complex features of behaviour can arise due to the dynamics of recurrent neural circuits. Fuelner et al. [<xref ref-type="bibr" rid="c42">42</xref>] show that a number of features of motor adaptation emerge as a result of the dynamical properties of recurrent neural circuits in which sensory feedback modulates motor output. In a recent paper Smoulder et al. [<xref ref-type="bibr" rid="c19">19</xref>] show that neural activity in monkey motor cortex scales with reward magnitude, and that this reward signal interacts with movement preparation signals in such a way that high rewards disrupt movement preparation and result in poor motor performance compared to moderate rewards. They propose that these interactions constitute a neural basis of “choking under pressure”.</p>
<p>The phenomenon of savings in motor learning implies that motor memory associated with an initial bout of training leads to faster subsequent relearning. The nature of the memory that is stored and how it influences subsequent relearning has been a topic of some debate in the recent literature. One account of savings emphasizes the effect of explicit, strategic components of motor learning [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c44">44</xref>]. Another line of work focuses on the idea that faster relearning may also be driven by implicit learning processes that result from previously experienced errors [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c45">45</xref>].</p>
<p>The results of our work here with RNNs and the related electrophysiological studies of populations of motor cortical neurons of non-human primates [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>] point to a neural basis of savings. We showed here that by increasing the number of hidden units in our RNNs, and hence increasing the dimensionality of the control space, RNNs were more likely to produce behavioural signatures of savings (<xref rid="fig3" ref-type="fig">Figure 3b</xref>). The high dimensionality of neural space enables a new motor memory to be encoded in such a way that it doesn’t interfere with other previously learned information, while still facilitating savings when the network is re-exposed to the newly learned skill. This neural basis of savings can be characterized as implicit, since in our RNN simulations we did not provide the network with any contextual input that would signal the presence or absence of any given FF.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>RNN model</title>
<p>Recurrent neural networks (RNNs) were trained to control movements of a simulated two degree of freedom arm that included rotations about a shoulder joint and an elbow joint in a horizontal plane. The model includes six rigid-tendon Hill-type muscle actuators comprising mono-articular flexors and extensors spanning the shoulder and elbow, as well as a pair of bi-articular muscles producing flexion or extension forces about both shoulder and elbow joints [<xref ref-type="bibr" rid="c23">23</xref>]. RNN models are implemented in PyTorch [<xref ref-type="bibr" rid="c27">27</xref>] and receive a 17-dimensional input signal to a linear input layer, which is fully connected to a recurrent layer consisting of gated recurrent units (GRUs) [<xref ref-type="bibr" rid="c46">46</xref>]. The GRU layer is connected to a 6-dimensional linear output layer which provides stimulation commands over time to each of the 6 muscles in the arm model (see <xref rid="fig1" ref-type="fig">Figure 1</xref>). Simulations were carried out in Python 3.10 using our open-source MotorNet toolbox [<xref ref-type="bibr" rid="c22">22</xref>].</p>
<p>Input-hidden (<italic>W</italic><sub>in</sub>) and hidden-hidden recurrent (<italic>W</italic><sub>r</sub>) weights (see <xref rid="fig1" ref-type="fig">Figure 1</xref>) were initialized using Glorot initialization [<xref ref-type="bibr" rid="c47">47</xref>] and orthogonal initialization [<xref ref-type="bibr" rid="c48">48</xref>], respectively, with biases set to 0. The output layer used a sigmoid activation function. The hidden-output weights (<italic>W</italic><sub>out</sub>) were initialized with the Glorot initialization scheme, and its biases were set to −5.0. The sigmoid activation function ensured the controller’s output remained close to 0 at the start of training, promoting a stable initialization state. We set the initial hidden unit activity of the network (<bold>h</bold><sub>0</sub>) as a learnable parameter and initialized it to 0.</p>
<p>The RNN models received a 17-dimensional input vector consisting of task-related inputs along with time-delayed feedback representing visual and proprioceptive signals. The task-related input consisted of a 2-element vector of (<italic>x, y</italic>) Cartesian coordinates for the target position, and a binary go-cue signal that switched from 0 to 1 when the movement should be initiated. The visual feedback was a Cartesian coordinate of the arm’s endpoint (<italic>x, y</italic>), and the proprioceptive feedback was the lengths and velocities of all 6 muscles. The time step for simulations was set to 10 ms, the visual delay (Δ<sub><italic>v</italic></sub>) was 70 ms, and the proprioceptive delay (Δ<sub><italic>p</italic></sub>) was 20 ms. We also treated the go cue as a visual signal, meaning that at each time step the network received the 70 ms time delayed value. At each time step the RNN model transformed the above described 17-dimensional input into a 6-dimensional muscle stimulation command.</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Growing up training phase</title>
<p>During an initial “growing up” phase new initialized RNN models were trained to move the arm from random starting positions to random target positions, both drawn from a uniform distribution across the joint space of the arm model. Note that due to muscle lengths and joint geometry, only a subset of the workspace was reachable for the model (<xref rid="fig1" ref-type="fig">Figure 1d</xref>). In 50% of simulations, no go-cue was provided (a catch trial). This was done to ensure that the network avoided producing anticipatory muscle stimulation commands. In the other 50% of cases the time of the go-cue switch from 0 to 1 was drawn from a random uniform distribution between 100 ms and 300 ms after the start of each simulated trial.</p>
<p>The loss function for training was mainly comprised of position loss, the Euclidean distance between the arm endpoint position <bold>x</bold><sub><italic>t</italic></sub> and the desired position <inline-formula><inline-graphic xlink:href="645562v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The desired position was set to be equal to the starting position of the limb’s endpoint before the go cue, and after that the target position. We also included terms in the loss function that punished large and oscillatory hidden and muscle activity, and the jerk (the second derivative of acceleration) of the endpoint trajectory [<xref ref-type="bibr" rid="c49">49</xref>]. The full form of the loss function is shown in <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>:
<disp-formula id="eqn1">
<graphic xlink:href="645562v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the subscript <italic>t</italic> indicates time step, <italic>N</italic> is the total number of time steps in the simulation, <italic>T</italic> is the transpose operation, and ∥∥<sub>1</sub> is the <italic>L</italic><sub>1</sub> norm. <inline-formula><inline-graphic xlink:href="645562v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="645562v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> indicate position, jerk, muscle, and hidden loss, respectively. <bold>h</bold><sub><italic>t</italic></sub> is a <italic>n</italic>-element vector of hidden unit activity, and <inline-formula><inline-graphic xlink:href="645562v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is its derivative. <bold>f</bold><sub><italic>t</italic></sub> is a 6-element vector of muscle forces, and <inline-formula><inline-graphic xlink:href="645562v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is its derivative. Note that muscle forces are different from muscle stimulation commands (<xref rid="fig1" ref-type="fig">Figure 1A,C</xref>), which are inputs to the Ordinary Differential Equation that produces muscle forces [<xref ref-type="bibr" rid="c23">23</xref>].</p>
<p>The RNN models were initially trained on 20,000 batches with a batch size of 32 on simulations of 1 s (100 time steps). RNN weights were adjusted using the Adam optimization scheme [<xref ref-type="bibr" rid="c26">26</xref>] with a learning rate <italic>lr</italic> = 0.003.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Motor learning phases</title>
<p>Once the networks were trained to perform reaches to random targets in a null-field (NF), we fixed the input-to-hidden weights (<italic>W</italic><sub>in</sub>) and the hidden-to-output weights (<italic>W</italic><sub>out</sub>) and their biases. This allowed us to isolate subsequent learning-related changes in hidden unit activity resulting from our experimental manipulations to the recurrent connectivity of hidden units [<xref ref-type="bibr" rid="c42">42</xref>].</p>
<p>We trained networks on centre-out reaches from a start position to 8 equidistant targets around the circumference of a circle (see <xref rid="fig1" ref-type="fig">Figure 1</xref>). The start position corresponded to external joint angles of 60 degrees at the shoulder and 90 degrees at the elbow. We exposed models sequentially to FFs or NFs and in each phase we continued to adjust hidden recurrent weights to optimize the loss function described in <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>. During these experimental phases we used a stochastic gradient descent optimization scheme with learning rate parameter <italic>lr</italic> = 0.005 [<xref ref-type="bibr" rid="c50">50</xref>]. This ensures batch-local learning, and thus provides greater control and transparency over the course of learning. It also results in gradual learning over batches, better resembling learning curves from empirical studies of force field learning in humans and non-human primates.</p>
<p>In the NF1 experimental phase the models were trained on centre-out reaches only. We trained the models for 10,000 batches of size 32 (4 repetitions of each of the 8 targets). As in the growing-up phase, 50% of trials were catch-trials in which the go-cue did not change from 0 to 1. After NF1 training, we continued to train the models to perform centre-out reaches but we introduced a clockwise curl force field (CWFF) for all movements (FF1). The external force <italic>F</italic><sub><italic>x</italic></sub>, <italic>F</italic><sub><italic>y</italic></sub> applied at the arm’s endpoint that produced a CWFF is described by the following equation:
<disp-formula id="eqn2">
<graphic xlink:href="645562v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="645562v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>and <inline-formula><inline-graphic xlink:href="645562v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the velocity of the arm’s endpoint in Cartesian coordinates and <italic>b</italic> = 8 Ns/m is a scalar constant defining the strength of the FF. In the null field (NF), <italic>b</italic> = 0. We trained the models for 3,200 batches of size 32 in the FF1 experimental phase, with 50% catch-trials.</p>
<p>Following FF1, the models were trained again in a null field (NF2), using the same procedures as in NF1 (10,000 batches of size 32). After NF2, the models were again trained in the presence of a CWFF (FF2), exactly as in FF1.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Lateral deviation</title>
<p>We evaluated the behavioural performance of the models during NF1, FF1, NF2, and FF2 by calculating the maximum lateral deviation of the endpoint trajectory from straight lines connecting the starting and target positions. We will refer to this measure as “lateral deviation”, and it was considered positive if it was in the clockwise direction from the straight line, and negative otherwise. For each batch of training we calculated the mean lateral deviations across all 8 targets.</p>
<p>For each model, we characterized the learning rate during FF1 and FF2 by fitting an exponential function of the following form to the mean lateral deviation across training batches:
<disp-formula id="eqn3">
<graphic xlink:href="645562v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>ŷ</italic> is the modelled lateral deviation at batch number <italic>x</italic><sub><italic>n</italic></sub>, <italic>α</italic> is a scaling factor that determines the initial lateral deviation, and <italic>r</italic> is the rate at which lateral deviation decays over time (indicating the learning rate). Before fitting we smoothed learning rates over batches by window-averaging with a kernel size of 5 batches.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Targeted dimensionality reduction</title>
<p>Following the procedure described in [<xref ref-type="bibr" rid="c20">20</xref>] we identified a subspace of RNN hidden unit activity that predicts the arm’s endpoint initial forces based on the preparatory activity of hidden units (hidden unit activity before the go-cue). To do this we applied targeted dimensionality reduction (TDR) using model data at the end of the NF1 experimental phase. The subspace is defined as:
<disp-formula id="eqn4">
<graphic xlink:href="645562v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="645562v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the matrix of hidden unit activity of size (targets × units) 340 ms before the is the go-cue, <inline-formula><inline-graphic xlink:href="645562v2_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> matrix of endpoint forces of size (targets × 2) 90 ms after go-cue, ⊮ is the targets-element vector concatenated to the force matrix, and <italic>W</italic> is the matrix of size (3 × units). For the <inline-formula><inline-graphic xlink:href="645562v2_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> parameter 90 ms was chosen because it coincides with peak acceleration. For the <inline-formula><inline-graphic xlink:href="645562v2_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> parameter, 340 ms was chosen because it ensures that hidden unit activity is stabilized.</p>
<p>We then calculated the pseudo-inverse of <italic>W</italic>, resulting in a (units×3) matrix <italic>W</italic><sup>+</sup>. To find the force-predictive subspace, we took the first two columns of <italic>W</italic><sup>+</sup> (ignoring the intercept) and orthogonalized them using the Gram-Schmidt orthogonalization scheme, resulting in <italic>Ŵ</italic> <sup>+</sup>. We projected the preparatory hidden unit activity of all experimental phases <inline-formula><inline-graphic xlink:href="645562v2_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> onto <italic>Ŵ</italic> <sup>+</sup> after removing their global mean.</p>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Uniform shift</title>
<p>Following the experimental phases FF1 and FF2 we calculated the direction in which the RNN hidden unit activity during the preparatory period shifted, averaged across movement directions. We averaged the preparatory hidden unit activity over the 8 targets in FF1 and NF1, and then calculated the difference. Following the convention in [<xref ref-type="bibr" rid="c20">20</xref>] this shift is referred to as a “uniform shift” (us):
<disp-formula id="eqn5">
<graphic xlink:href="645562v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the bar indicates averaging over 8 movement directions. We orthogonalized the uniform shift with respect to <italic>Ŵ</italic> <sup>+</sup>. This allowed us to test for changes outside the force-predictive subspace.</p>
<p>We then projected the preparatory hidden unit activity (340 ms before the go-cue) of all experimental phases after removing the global mean. Next, we normalized the projection values for each model so that the projection of <bold>H</bold><sup>NF1</sup>-340 ms onto the uniform shift is zero, and the projection of <bold>H</bold><sup>FF1</sup>-340 ms onto the uniform shift is one.</p>
</sec>
<sec id="s4g">
<label>4.7</label>
<title>Perturbing hidden unit activity</title>
<p>To conduct a causal test of the idea that the non-zero uniform shift activity that remained following NF2 is related to savings, we perturbed the activity of hidden units at the end of the preparatory period by adding to each hidden unit a proportion (−2, −1, 0, 1, 2) of the projection of that unit onto the uniform shift. We conducted these perturbations separately for each movement direction. The perturbation was applied 340 ms prior to the go cue and the duration of the perturbation was one simulation time step.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>The authors wish to thank Mehrdad Kashefi for useful discussions about this study.</p>
</ack>
<sec id="d1e1396" sec-type="additional-information">
<title>Additional information</title>
<sec id="s8">
<title>Author contributions</title>
<p>M.S., O.C., J.A.M., and P.L.G. conception and design of research; M.S. performed simulations; M.S. and P.L.G. analyzed data; M.S., P.L.G., and J.A.M. interpreted results of experiments; M.S. prepared figures; M.S., P.L.G., and J.A.M. drafted manuscript; M.S., P.L.G., and J.A.M. edited and revised manuscript; M.S., O.C.,P.L.G., and J.A.M. approved final version of manuscript.</p>
</sec>
    <sec id="s5">
        <title>Grants</title>
        <p>This work was supported by the Natural Sciences and Engineering Research Council of Canada through a Discovery Grant RGPIN/05458-2018 to P.L.G., and a FRQNT Strategic Clusters Program grant to O.C. J.A.M. was supported by a Banting Postdoctoral Fellowship and a BrainsCAN Postdoctoral Fellowship, and by Canadian Institutes of Health Research grant PJT-175010 to Dr. Andrew Pruszynski.</p>
    </sec>
    <sec id="s6">
        <title>Code Availability</title>
        <p>Python code to reproduce the simulations and analyses described here is available on GitHub at the following repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/mshahbazi1997/MotorSavingModel.git">https://github.com/mshahbazi1997/MotorSavingModel.git</ext-link></p>
    </sec>
</sec>
    <sec id="nt1">
        <title>Note</title>
        <p>This reviewed preprint has been updated to correct level 1 headings, and the formatting in the affiliations.</p>
    </sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. M.</given-names> <surname>Haith</surname></string-name>, <string-name><given-names>D. M.</given-names> <surname>Huberdeau</surname></string-name>, and <string-name><given-names>J. W.</given-names> <surname>Krakauer</surname></string-name></person-group>. <article-title>The influence of movement preparation time on the expression of visuomotor learning and savings</article-title>. <source>J. Neurosci</source>., <volume>35</volume>(<issue>13</issue>):<fpage>5109</fpage>–<lpage>5117</lpage>, <year>2015</year>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.3869-14.2015</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. R.</given-names> <surname>Morehead</surname></string-name>, <string-name><given-names>S. E.</given-names> <surname>Qasim</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Crossley</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Ivry</surname></string-name></person-group>. <article-title>Savings upon re-aiming in visuomotor adaptation</article-title>. <source>J. Neurosci</source>., <volume>35</volume>(<issue>42</issue>):<fpage>14386</fpage>–<lpage>14396</lpage>, <year>2015</year>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.1046-15.2015</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.-A.</given-names> <surname>Leow</surname></string-name>, <string-name><given-names>A.</given-names> <surname>de Rugy</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Marinovic</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Riek</surname></string-name>, and <string-name><given-names>T. J.</given-names> <surname>Carroll</surname></string-name></person-group>. <article-title>Savings for visuomotor adaptation require prior history of error, not prior repetition of successful actions</article-title>. <source>J. Neurophysiol</source>., <volume>116</volume>(<issue>4</issue>):<fpage>1603</fpage>–<lpage>1614</lpage>, <year>2016</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.01055.2015</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. P.</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>E.</given-names> <surname>McKenna</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Colucci-Chang</surname></string-name>, <string-name><given-names>L. C. J.</given-names> <surname>Bray</surname></string-name>, <string-name><given-names>E. A.</given-names> <surname>Hosseini</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Alhussein</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Rezazad</surname></string-name>, and <string-name><given-names>W. M.</given-names> <surname>Joiner</surname></string-name></person-group>. <article-title>The 24-h savings of adaptation to novel movement dynamics initially reflects the recall of previous performance</article-title>. <source>J. Neurophysiol</source>., <volume>122</volume>(<issue>3</issue>):<fpage>933</fpage>–<lpage>946</lpage>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.00569.2018</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Yin</surname></string-name> and <string-name><given-names>K.</given-names> <surname>Wei</surname></string-name></person-group>. <article-title>Savings in sensorimotor adaptation without explicit strategy</article-title>. <source>J. Neurophysiol</source>., <year>2020</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.00524.2019</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. M.</given-names> <surname>Hadjiosif</surname></string-name>, <string-name><given-names>J. R.</given-names> <surname>Morehead</surname></string-name>, and <string-name><given-names>M. A.</given-names> <surname>Smith</surname></string-name></person-group>. <article-title>A double dissociation between savings and long-term memory in motor learning</article-title>. <source>PLoS Biol</source>., <volume>21</volume>(<issue>4</issue>):<fpage>e3001799</fpage>, <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.3001799</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. K.</given-names> <surname>Coltman</surname></string-name>, <string-name><given-names>J. G. A.</given-names> <surname>Cashaback</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Gribble</surname></string-name></person-group>. <article-title>Both fast and slow learning processes contribute to savings following sensorimotor adaptation</article-title>. <source>J. Neurophysiol</source>., <volume>121</volume>(<issue>4</issue>):<fpage>1575</fpage>–<lpage>1583</lpage>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.00794.2018</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. J.</given-names> <surname>Herzfeld</surname></string-name>, <string-name><given-names>P. A.</given-names> <surname>Vaswani</surname></string-name>, <string-name><given-names>M. K.</given-names> <surname>Marko</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Shadmehr</surname></string-name></person-group>. <article-title>A memory of errors in sensorimotor learning</article-title>. <source>Science</source>, <volume>345</volume>(<issue>6202</issue>):<fpage>1349</fpage>–<lpage>1353</lpage>, <year>2014</year>. doi: <pub-id pub-id-type="doi">10.1126/science.1253138</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. B.</given-names> <surname>Heald</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Lengyel</surname></string-name>, and <string-name><given-names>D. M.</given-names> <surname>Wolpert</surname></string-name></person-group>. <article-title>Contextual inference underlies the learning of sensorimotor repertoires</article-title>. <source>Nature</source>, <volume>600</volume>(<issue>7889</issue>):<fpage>489</fpage>–<lpage>493</lpage>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1038/s41586-021-04129-3</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Zarahn</surname></string-name>, <string-name><given-names>G. D.</given-names> <surname>Weston</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Liang</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Mazzoni</surname></string-name>, and <string-name><given-names>J. W.</given-names> <surname>Krakauer</surname></string-name></person-group>. <article-title>Explaining savings for visuomotor adaptation: linear time-invariant state-space models are not sufficient</article-title>. <source>J. Neurophysiol</source>., <volume>100</volume>(<issue>5</issue>):<fpage>2537</fpage>–<lpage>2548</lpage>, <year>2008</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.90529.2008</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. D.</given-names> <surname>McDougle</surname></string-name>, <string-name><given-names>K. M.</given-names> <surname>Bond</surname></string-name>, and <string-name><given-names>J. A.</given-names> <surname>Taylor</surname></string-name></person-group>. <article-title>Explicit and implicit processes constitute the fast and slow processes of sensorimotor learning</article-title>. <source>J. Neurosci</source>., <volume>35</volume>(<issue>26</issue>):<fpage>9568</fpage>–<lpage>9579</lpage>, <year>2015</year>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.5061-14.2015</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. T.</given-names> <surname>Albert</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Shadmehr</surname></string-name></person-group>. <article-title>Estimating properties of the fast and slow adaptive processes during sensorimotor adaptation</article-title>. <source>J. Neurophysiol</source>., <volume>119</volume>(<issue>4</issue>):<fpage>1367</fpage>–<lpage>1393</lpage>, <year>2018</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.00197.2017</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V. S.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Haith</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Mazzoni</surname></string-name>, and <string-name><given-names>J. W.</given-names> <surname>Krakauer</surname></string-name></person-group>. <article-title>Rethinking motor learning and savings in adaptation paradigms: model-free memory for successful actions combines with internal models</article-title>. <source>Neuron</source>, <volume>70</volume>(<issue>4</issue>):<fpage>787</fpage>–<lpage>801</lpage>, <year>2011</year>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.012</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Diedrichsen</surname></string-name>, <string-name><given-names>O.</given-names> <surname>White</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Newman</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Lally</surname></string-name></person-group>. <article-title>Use-dependent and error-based learning of motor behaviors</article-title>. <source>J. Neurosci</source>., <volume>30</volume>(<issue>15</issue>):<fpage>5159</fpage>–<lpage>5166</lpage>, <year>2010</year>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.5406-09.2010</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. M.</given-names> <surname>Wolpert</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Ghahramani</surname></string-name>, and <string-name><given-names>M. I.</given-names> <surname>Jordan</surname></string-name></person-group>. <article-title>An internal model for sensorimotor integration</article-title>. <source>Science</source>, <volume>269</volume>(<issue>5232</issue>):<fpage>1880</fpage>–<lpage>1882</lpage>, <year>1995</year>. doi: <pub-id pub-id-type="doi">10.1126/science.7569931</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>E. M.</given-names> <surname>Trautmann</surname></string-name>, <string-name><given-names>J. K.</given-names> <surname>Hesse</surname></string-name>, <string-name><given-names>G. M.</given-names> <surname>Stine</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Xia</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>D. J.</given-names> <surname>O’Shea</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Karsh</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Colonell</surname></string-name>, <string-name><given-names>F. F.</given-names> <surname>Lanfranchi</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Vyas</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Zimnik</surname></string-name>, <string-name><given-names>N. A.</given-names> <surname>Steinmann</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Wagenaar</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Andrei</surname></string-name>, <string-name><given-names>C. M.</given-names> <surname>Lopez</surname></string-name>, <string-name><given-names>J.</given-names> <surname>O’Callaghan</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Putzeys</surname></string-name>, <string-name><given-names>B. C.</given-names> <surname>Raducanu</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Welkenhuysen</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Churchland</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Moore</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Shadlen</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Shenoy</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Tsao</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Dutta</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Harris</surname></string-name></person-group>. <article-title>Large-scale high-density brain-wide neural recording in nonhuman primates</article-title>. <source>bioRxiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.02.01.526664</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Kobak</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Brendel</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Constantinidis</surname></string-name>, <string-name><given-names>C. E.</given-names> <surname>Feierstein</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kepecs</surname></string-name>, <string-name><given-names>Z. F.</given-names> <surname>Mainen</surname></string-name>, <string-name><given-names>X.-L.</given-names> <surname>Qi</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Romo</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Uchida</surname></string-name>, and <string-name><given-names>C. K.</given-names> <surname>Machens</surname></string-name></person-group>. <article-title>Demixed principal component analysis of neural population data</article-title>. <source>eLife</source>, <volume>5</volume>:<elocation-id>9424</elocation-id>, <year>2016</year>. doi: <pub-id pub-id-type="doi">10.7554/eLife.10989</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Dubreuil</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Valente</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Beiran</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Mastrogiuseppe</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Ostojic</surname></string-name></person-group>. <article-title>The role of population structure in computations through neural dynamics</article-title>. <source>Nat. Neurosci</source>., <volume>25</volume>(<issue>6</issue>):<fpage>783</fpage>–<lpage>794</lpage>, <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1038/s41593-022-01088-4</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. L.</given-names> <surname>Smoulder</surname></string-name>, <string-name><given-names>P. J.</given-names> <surname>Marino</surname></string-name>, <string-name><given-names>E. R.</given-names> <surname>Oby</surname></string-name>, <string-name><given-names>S. E.</given-names> <surname>Snyder</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Miyata</surname></string-name>, <string-name><given-names>N. P.</given-names> <surname>Pavlovsky</surname></string-name>, <string-name><given-names>W. E.</given-names> <surname>Bishop</surname></string-name>, <string-name><given-names>B. M.</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>S. M.</given-names> <surname>Chase</surname></string-name>, and <string-name><given-names>A. P.</given-names> <surname>Batista</surname></string-name></person-group>. <article-title>A neural basis of choking under pressure</article-title>. <source>Neuron</source>, <volume>0</volume>(<issue>0</issue>), <year>2024</year>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2024.08.012</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>X.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>D. J.</given-names> <surname>OShea</surname></string-name>, <string-name><given-names>M. D.</given-names> <surname>Golub</surname></string-name>, <string-name><given-names>E. M.</given-names> <surname>Trautmann</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Vyas</surname></string-name>, <string-name><given-names>S. I.</given-names> <surname>Ryu</surname></string-name>, and <string-name><given-names>K. V.</given-names> <surname>Shenoy</surname></string-name></person-group>. <article-title>Cortical preparatory activity indexes learned motor memories</article-title>. <source>Nature</source>, <volume>602</volume>(<issue>7896</issue>):<fpage>274</fpage>–<lpage>279</lpage>, <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1038/s41586-021-04329-x</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. M.</given-names> <surname>Losey</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Hennig</surname></string-name>, <string-name><given-names>E. R.</given-names> <surname>Oby</surname></string-name>, <string-name><given-names>M. D.</given-names> <surname>Golub</surname></string-name>, <string-name><given-names>P. T.</given-names> <surname>Sadtler</surname></string-name>, <string-name><given-names>K. M.</given-names> <surname>Quick</surname></string-name>, <string-name><given-names>S. I.</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>E. C.</given-names> <surname>Tyler-Kabara</surname></string-name>, <string-name><given-names>A. P.</given-names> <surname>Batista</surname></string-name>, <string-name><given-names>B. M.</given-names> <surname>Yu</surname></string-name>, and <string-name><given-names>S. M.</given-names> <surname>Chase</surname></string-name></person-group>. <article-title>Learning leaves a memory trace in motor cortex</article-title>. <source>Current Biology</source>, <volume>34</volume>(<issue>7</issue>):<fpage>1519</fpage>–<lpage>1531.e4,</lpage> <year>2024</year>. issn: <issn>0960-9822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2024.03.003</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O.</given-names> <surname>Codol</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Michaels</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Kashefi</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Pruszynski</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Gribble</surname></string-name></person-group>. <article-title>MotorNet: a python toolbox for controlling differentiable biomechanical effectors with artificial neural networks</article-title>. <source>eLife</source>, <year>2024</year>. doi: <pub-id pub-id-type="doi">10.7554/eLife.88591</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. A.</given-names> <surname>Kistemaker</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Wong</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Gribble</surname></string-name></person-group>. <article-title>The central nervous system does not minimize energy cost in arm movements</article-title>. <source>J. Neurophysiol</source>., <volume>104</volume>(<issue>6</issue>):<fpage>2985</fpage>–<lpage>2994</lpage>, <year>2010</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.00483.2010</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Shadmehr</surname></string-name> and <string-name><given-names>F. A.</given-names> <surname>Mussa-Ivaldi</surname></string-name></person-group>. <article-title>Adaptive representation of dynamics during learning of a motor task</article-title>. <source>J. Neurosci</source>., <volume>14</volume>(<issue>5</issue> Pt 2):<fpage>3208</fpage>–<lpage>3224</lpage>, <year>1994</year>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.14-05-03208.1994</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. A.</given-names> <surname>Conditt</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Gandolfo</surname></string-name>, and <string-name><given-names>F. A.</given-names> <surname>Mussa-Ivaldi</surname></string-name></person-group>. <article-title>The motor system does not learn the dynamics of the arm by rote memorization of past experience</article-title>. <source>J. Neurophysiol</source>., <volume>78</volume>(<issue>1</issue>):<fpage>554</fpage>–<lpage>560</lpage>, <year>1997</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.1997.78.1.554</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>D. P.</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Ba</surname></string-name></person-group>. <article-title>Adam: a method for stochastic optimization</article-title>. <source>arXiv</source> preprint arXiv:<pub-id pub-id-type="arxiv">1412.6980</pub-id>, <year>2014</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Paszke</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Gross</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Massa</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Lerer</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Bradbury</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Chanan</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Killeen</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Gimelshein</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Antiga</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Desmaison</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Köpf</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>DeVito</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Raison</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Tejani</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Chilamkurthy</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Steiner</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Fang</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Bai</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Chintala</surname></string-name></person-group>. <article-title>PyTorch: an imperative style, high-performance deep learning library</article-title>. <source>arXiv</source>, <year>2019</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. A.</given-names> <surname>Michaels</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Schaffelhofer</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Agudelo-Toro</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Scherberger</surname></string-name></person-group>. <article-title>A goal-driven modular neural network predicts parietofrontal neural dynamics during grasping</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>., <volume>117</volume>(<issue>50</issue>):<fpage>32124</fpage>–<lpage>32135</lpage>, <year>2020</year>. doi: <pub-id pub-id-type="doi">10.1073/pnas.2005087117</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Sussillo</surname></string-name>, <string-name><given-names>M. M.</given-names> <surname>Churchland</surname></string-name>, <string-name><given-names>M. T.</given-names> <surname>Kaufman</surname></string-name>, and <string-name><given-names>K. V.</given-names> <surname>Shenoy</surname></string-name></person-group>. <article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title>. <source>Nat. Neurosci</source>., <volume>18</volume>(<issue>7</issue>):<fpage>1025</fpage>–<lpage>1033</lpage>, <year>2015</year>. doi: <pub-id pub-id-type="doi">10.1038/nn.4042</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. M.</given-names> <surname>Churchland</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Cunningham</surname></string-name>, <string-name><given-names>M. T.</given-names> <surname>Kaufman</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Foster</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Nuyujukian</surname></string-name>, <string-name><given-names>S. I.</given-names> <surname>Ryu</surname></string-name>, and <string-name><given-names>K. V.</given-names> <surname>Shenoy</surname></string-name></person-group>. <article-title>Neural population dynamics during reaching</article-title>. <source>Nature</source>, <volume>487</volume>(<issue>7405</issue>):<fpage>51</fpage>–<lpage>56</lpage>, <year>2012</year>. doi: <pub-id pub-id-type="doi">10.1038/nature11129</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. M.</given-names> <surname>Churchland</surname></string-name> and <string-name><given-names>K. V.</given-names> <surname>Shenoy</surname></string-name></person-group>. <article-title>Preparatory activity and the expansive null-space</article-title>. <source>Nat. Rev. Neurosci</source>., <volume>25</volume>(<issue>4</issue>):<fpage>213</fpage>–<lpage>236</lpage>, <year>2024</year>. doi: <pub-id pub-id-type="doi">10.1038/s41583-024-00796-z</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>L.</given-names> <surname>Driscoll</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Shenoy</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Sussillo</surname></string-name></person-group>. <article-title>Flexible multitask computation in recurrent networks utilizes shared dynamical motifs</article-title>. <source>bioRxiv</source>, <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1038/s41593-024-01668-6</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.-H.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Daie</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Li</surname></string-name></person-group>. <article-title>A combinatorial neural code for long-term motor memory</article-title>. <source>Nature</source>, <volume>637</volume>(<issue>8046</issue>):<fpage>663</fpage>–<lpage>672</lpage>, <year>2025</year>. doi: <pub-id pub-id-type="doi">10.1038/s41586-024-08193-3</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Bernardi</surname></string-name>, <string-name><given-names>M. K.</given-names> <surname>Benna</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Rigotti</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Munuera</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Fusi</surname></string-name>, and <string-name><given-names>C. D.</given-names> <surname>Salzman</surname></string-name></person-group>. <article-title>The geometry of abstraction in the hippocampus and prefrontal cortex</article-title>. <source>Cell</source>, <volume>183</volume>(<issue>4</issue>):<fpage>954</fpage>–<lpage>967.e21,</lpage> <year>2020</year>. doi: <pub-id pub-id-type="doi">10.1016/j.cell.2020.09.031</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. M.</given-names> <surname>Wolpert</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Kawato</surname></string-name></person-group>. <article-title>Multiple paired forward and inverse models for motor control</article-title>. <source>Neural Netw</source>., <volume>11</volume>(<issue>7-8</issue>):<fpage>1317</fpage>–<lpage>1329</lpage>, <year>1998</year>. doi: <pub-id pub-id-type="doi">10.1016/s0893-6080(98)00066-5</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Haruno</surname></string-name>, <string-name><given-names>D. M.</given-names> <surname>Wolpert</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Kawato</surname></string-name></person-group>. <article-title>Mosaic model for sensorimotor learning and control</article-title>. <source>Neural Comput</source>., <volume>13</volume>(<issue>10</issue>):<fpage>2201</fpage>–<lpage>2220</lpage>, <year>2001</year>. doi: <pub-id pub-id-type="doi">10.1162/089976601750541778</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H. R.</given-names> <surname>Sheahan</surname></string-name>, <string-name><given-names>D. W.</given-names> <surname>Franklin</surname></string-name>, and <string-name><given-names>D. M.</given-names> <surname>Wolpert</surname></string-name></person-group>. <article-title>Motor planning, not execution, separates motor memories. en</article-title>. <source>Neuron</source>, <volume>92</volume>(<issue>4</issue>):<fpage>773</fpage>–<lpage>779</lpage>, <month>November</month> <year>2016</year>. issn: <issn>0896-6273</issn>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H. R.</given-names> <surname>Sheahan</surname></string-name>, <string-name><given-names>J. N.</given-names> <surname>Ingram</surname></string-name>, <string-name><given-names>G. M.</given-names> <surname>Žalalyte</surname></string-name>, and <string-name><given-names>D. M.</given-names> <surname>Wolpert</surname></string-name></person-group>. <article-title>Imagery of movements immediately following performance allows learning of motor skills that interfere. en</article-title>. <source>Scientific reports</source>, <volume>8</volume>(<issue>1</issue>):<fpage>14330</fpage>, <month>September</month> <year>2018</year>. issn: <issn>2045-2322</issn>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>J. A.</given-names> <surname>Michaels</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Kashefi</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Codol</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Weiler</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Kersten</surname></string-name>, <string-name><given-names>P. L.</given-names> <surname>Gribble</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Diedrichsen</surname></string-name>, and <string-name><given-names>J. A.</given-names> <surname>Pruszynski</surname></string-name></person-group>. <article-title>Sensory expectations shape neural population dynamics in motor circuits</article-title>. <source>bioRxiv</source>:2024.12.22.629295, <year>2024</year>. doi: <pub-id pub-id-type="doi">10.1101/2024.12.22.629295</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Ajemian</surname></string-name>, <string-name><given-names>A.</given-names> <surname>D’Ausilio</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Moorman</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Bizzi</surname></string-name></person-group>. <article-title>Why professional athletes need a prolonged period of warm-up and other peculiarities of human motor learning</article-title>. <source>J. Mot. Behav</source>., <volume>42</volume>(<issue>6</issue>):<fpage>381</fpage>–<lpage>388</lpage>, <year>2010</year>. doi: <pub-id pub-id-type="doi">10.1080/00222895.2010.528262</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Ajemian</surname></string-name>, <string-name><given-names>A.</given-names> <surname>D’Ausilio</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Moorman</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Bizzi</surname></string-name></person-group>. <article-title>A theory for how sensorimotor skills are learned and retained in noisy and nonstationary neural circuits</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>., <volume>110</volume>(<issue>52</issue>):<fpage>E5078</fpage>–<lpage>87</lpage>, <year>2013</year>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1320116110</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Feulner</surname></string-name>, <string-name><given-names>M. G.</given-names> <surname>Perich</surname></string-name>, <string-name><given-names>L. E.</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Clopath</surname></string-name>, and <string-name><given-names>J. A.</given-names> <surname>Gallego</surname></string-name></person-group>. <article-title>A neural implementation model of feedback-based motor learning</article-title>. <source>Nat. Commun</source>., <volume>16</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>14</lpage>, <year>2025</year>. doi: <pub-id pub-id-type="doi">10.1038/s41467-024-54738-5</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. M.</given-names> <surname>Huberdeau</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Haith</surname></string-name>, and <string-name><given-names>J. W.</given-names> <surname>Krakauer</surname></string-name></person-group>. <article-title>Formation of a long-term memory for visuomotor adaptation following only a few trials of practice</article-title>. <source>J. Neurophysiol</source>., <volume>114</volume>(<issue>2</issue>):<fpage>969</fpage>–<lpage>977</lpage>, <year>2015</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.00369.2015</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Avraham</surname></string-name>, <string-name><given-names>J. Ryan</given-names> <surname>Morehead</surname></string-name>, <string-name><given-names>H. E.</given-names> <surname>Kim</surname></string-name>, and <string-name><given-names>R. B.</given-names> <surname>Ivry</surname></string-name></person-group>. <article-title>Re-exposure to a sensorimotor perturbation produces opposite effects on explicit and implicit learning processes</article-title>. <source>bioRxiv</source>:2020.07.16.205609, <year>2020</year>. doi: <pub-id pub-id-type="doi">10.1101/2020.07.16.205609</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. K.</given-names> <surname>Coltman</surname></string-name>, <string-name><given-names>R. J.</given-names> <surname>van Beers</surname></string-name>, <string-name><given-names>W. P.</given-names> <surname>Medendorp</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Gribble</surname></string-name></person-group>. <article-title>Sensitivity to error during visuomotor adaptation is similarly modulated by abrupt, gradual, and random perturbation schedules</article-title>. <source>J. Neurophysiol</source>., <volume>126</volume>(<issue>3</issue>):<fpage>934</fpage>–<lpage>945</lpage>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1152/jn.00269.2021</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Cho</surname></string-name>, <string-name><given-names>B.</given-names> <surname>van Merrienboer</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Gulcehre</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Bahdanau</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Bougares</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Schwenk</surname></string-name>, and <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name></person-group>. <article-title>Learning phrase representations using RNN encoder-decoder for statistical machine translation</article-title>. <source>arXiv</source>, <year>2014</year>. doi: <pub-id pub-id-type="doi">10.3115/v1/D14-1179</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>X.</given-names> <surname>Glorot</surname></string-name> and <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name></person-group>. <article-title>Understanding the difficulty of training deep feedforward neural networks</article-title>. In <person-group person-group-type="editor"><string-name><given-names>Y. W.</given-names> <surname>Teh</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Titterington</surname></string-name>, editors</person-group>, <conf-name>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research</conf-name>, pages <fpage>249</fpage>–<lpage>256</lpage>, <publisher-name>Chia Laguna Resort</publisher-name>, <publisher-loc>Sardinia, Italy. Pmlr</publisher-loc>, <year>2010</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>W.</given-names> <surname>Hu</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Xiao</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Pennington</surname></string-name></person-group>. <article-title>Provable benefit of orthogonal initialization in optimizing deep linear networks</article-title>. <source>arXiv</source>, <year>2020</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2001.05992</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names> <surname>Flash</surname></string-name> and <string-name><given-names>N.</given-names> <surname>Hogan</surname></string-name></person-group>. <article-title>The coordination of arm movements: an experimentally confirmed mathematical model</article-title>. <source>J Neurosci</source>, <volume>5</volume>(<issue>7</issue>):<fpage>1688</fpage>–<lpage>1703</lpage>, <year>1985</year>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.05-07-01688.1985</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>I.</given-names> <surname>Sutskever</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Martens</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Dahl</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Hinton</surname></string-name></person-group>. <article-title>On the importance of initialization and momentum in deep learning</article-title>. In <person-group person-group-type="editor"><string-name><given-names>S.</given-names> <surname>Dasgupta</surname></string-name> and <string-name><given-names>D.</given-names> <surname>McAllester</surname></string-name>, editors</person-group>, <conf-name>Proceedings of the 30th International Conference on Machine Learning, volume 28 (3) of Proceedings of Machine Learning Research</conf-name>, pages <fpage>1139</fpage>–<lpage>1147</lpage>, <publisher-loc>Atlanta, Georgia, USA</publisher-loc>. <publisher-name>Pmlr</publisher-name>, <year>2013</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107423.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ponte Costa</surname>
<given-names>Rui</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents <bold>valuable</bold> computational findings on the neural basis of learning new motor memories and the savings using recurrent neural networks. The evidence supporting the claims of the authors is <bold>solid</bold>, but it would benefit from more controls and from considering the role of explicit strategies and other brain regions. This work will be of interest to computational and experimental neuroscientists working in motor learning.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107423.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Shahbazi et al used a recurrent neural network model trained to control a musculoskeletal model of the arm to investigate how neural populations accommodate activity patterns underpinning savings. The paper draws upon the recent finding of a &quot;uniform shift&quot; in preparatory activity in monkey motor cortex associated with savings, and leverages full access to a computational model to establish causality.</p>
<p>Strengths:</p>
<p>The paper is well written, and the figures are clearly presented. The key finding that the uniform shift first reported based on neural recordings by Sun et al. emerges in artificial neural networks performing a similar task is interesting and well-backed by their analyses. Manipulating this uniform shift to show that it drives behavioural savings is an important causal confirmation of the proposal by Sun et al.</p>
<p>Weaknesses / Comments:</p>
<p>As mentioned earlier, the core results are well backed by the analyses. Most of my comments relate to adding more controls and additional questions that could be explored with the model to strengthen the paper.</p>
<p>(1) Savings are quantified as more rapid relearning of the FF upon re-exposure (e.g., Figure 3). This finding is based on backpropagation through time, but would this hold when using a different optimiser, e.g., FORCE?</p>
<p>(2) The authors should include a &quot;null model&quot; showing that training on a different reaching task following NF, as opposed to FF2, won't show something akin to a uniform shift during preparation due to the adoption of TDR and having similar targets.</p>
<p>(3) The analyses of network activity during movement preparation (Figure 4) nicely replicate the key finding in Sun et al, but I think the authors could leverage the full access to their network and go further, e.g., by examining changes (or the lack of) during execution in FF2 with respect to FF (and perhaps in a future NF2 with respect to NF), including whether execution activity lives also lives in parallel hyperplanes, etc.</p>
<p>(4) Related to the above, while the results are interesting and the paper is well done, I kept wishing that the authors had done &quot;more&quot; with their model. This could be one or two final sections on &quot;predictions&quot; that would nicely complement their &quot;validation&quot; of the uniform shift, and that, in my opinion, would greatly increase the impact of the paper. In particular:</p>
<p>
a) What would be the effect of learning more &quot;tasks&quot;? For example, is there a limit on how many fields can be learned? (You show something related by manipulating network size, but this is slightly different.)</p>
<p>
b) Figure 5 is a nice causal demonstration that the uniform shift is related to savings. However, and related to comment #3, it'd be interesting to see more details about how the behaviour and the network activity changes as preparatory activity shifts along this axis, in particular regarding how moving the preparatory states affect the organisation and dynamics of upcoming execution activity -these are the kind of intuitions that modelling studies like this one can provide.</p>
<p>
c) The authors focus on a task design that spans baseline, FF, NF, FF2 to replicate the original study by Sun et al. However, it would be interesting if they generated predictions for neural changes to other types of tasks that have been studied behaviourally. These could include, for example: (i) modelling a visuomotor rotation or a mirror reversal task; (ii) having to adapt to a FF in the opposite direction; (iii) investigating the role of adding an explicit context and having the networks learn multiple FF; and (iv) trying to learn FF fields in opposite directions, perhaps restricted to specific targets. As the authors know, all these questions and more have been studied with similar behavioural paradigms, and it would be nice to see what neural predictions are generated by this model.</p>
<p>(5) On the Discussion: When extrapolating from neural network results to animals, the fact that your networks can learn implicitly doesn't mean that animals do learn implicitly. Indeed, I think the consensus view is that different perturbations may lead to the expression of different types of savings (e.g., FF vs VR, which seems to be more explicit). Besides, these different mechanisms may be primarily implemented by brain regions less directly tied to motor control (e.g., cerebellum, parietal cortex?), which are not directly implemented in the authors' model.</p>
<p>These aspects (limitations) should be discussed in the paper.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107423.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Shahbazi et al. trained recurrent neural networks (RNNs) to simulate human upper limb movement during adaptation to a force field perturbation. They demonstrated that throughout adaptation, the pattern of motor commands to the muscles of the simulated arm changed, allowing the perturbed movements to regain their typical, perturbation-free straight-line paths. After this initial learning block (FF1), the network encountered null-fields to wash out the adaptation, before re-experiencing the force in a second learning block (FF2). Upon re-exposure, the network learned faster than during initial learning, consistent with the savings observed in behavioral studies of adaptation. They also found that as the number of hidden units in the RNN increased, so did the probability of exhibiting savings. The authors concluded that these results propose a neural basis for savings that is independent of context and strategic processes.</p>
<p>Strengths:</p>
<p>The paper addresses an important and controversial topic in motor adaptation: the mechanism underlying motor memory. The RNN simulation reproduces behavioral hallmarks of adaptation, and it provides a useful illustration of the pattern of muscle activity underlying human-like movements under both normal and perturbing conditions. While the savings effect produced by the network, though significant, appears somewhat small, the simulation demonstrating an increase in savings with a greater number of hidden units is particularly intriguing.</p>
<p>Weaknesses:</p>
<p>(1) To be transparent, savings in motor adaptation have been a primary focus of my own research. Some core findings presented in this paper are at odds with the ideas I and others have previously put forward. While I don't want to impose my agenda on the authors of this paper, I do think the authors should address these issues.</p>
<p>a) The authors acknowledge the ongoing debate in the literature regarding the mechanisms underlying savings, particularly whether it stems from explicit or implicit learning processes. However, it remains unclear how the current work addresses this debate. There is already a considerable body of research, particularly in visuomotor adaptation, demonstrating that savings is predominantly driven by explicit strategies. For example, when people are asked to report their strategy, they recall a strategy that was useful during the first learning block (Morehead et al. 2015). Furthermore, savings are abolished under experimental manipulations designed to eliminate strategic contributions (e.g., Haith et al., 2015; Huberdeau et al., 2019; Avraham et al., 2021). The authors briefly state that their findings support the hypothesis that a neural basis of memory retention underlying savings can be independent of cognitive or strategic learning components, and that savings can be characterized as implicit. While these statements may be true, it is not clear how this work substantiates these claims.</p>
<p>
b) Our research has also demonstrated that if implicit adaptation is completely washed out after the initial learning block, it not only fails to exhibit savings but is actually attenuated relative to the first learning block (Avraham et al., 2021). This phenomenon of attenuation upon relearning can also be seen in other studies of visuomotor adaptation (e.g., Leow et al., 2020; Yin and Wei, 2020; Hamel et al., 2021; Hamel et al., 2022; Wang and Ivry, 2023; Hadjiosif et al., 2023). More recently, we have shown that this attenuation is due to anterograde interference arising from the experience with the washout block experience (Avraham and Ivry, 2025). We illustrated that the implicit system is highly susceptible to interference; it doesn't require exposure to salient opposite errors and can occur even following prolonged exposure to veridical feedback. The central thesis of this paper, namely that implicit savings can emerge through RNNs, is at odds with these empirical results. The authors should address this discrepancy.</p>
<p>(2) This brings me to the question about neural correlates: The results are linked to activity in the primary motor cortex. How does that align with the well-established role of the cerebellum in implicit motor adaptation? And with the studies showing that savings are due to explicit strategies, which are generally associated with prefrontal regions?</p>
<p>(3) The analysis on the complexity of the neural network (i.e., the number of hidden units) and its relationship to savings is very interesting. It makes sense to me that more complex networks would show more savings. I'm not sure I follow the author's explanation, but my understanding is that increased network complexity makes it more difficult to override the formed memory through interference (e.g., from the experience with NF2). Also, the results indicate that a network with 32 units led to a less-than-chance level of networks exhibiting savings (Figure 3b). What behavioral output does this configuration produce? Could this behavior manifest as attenuation upon relearning? Furthermore, if one were to examine an even smaller, simpler network (perhaps one more closely reflecting cerebellar circuits), would such a model predict attenuation rather than savings?</p>
<p>(4) The authors emphasize that their network did not receive any explicit contextual signals related to the presence or absence of the force field (FF), thus operating in a 'context-free' manner. From my understanding, some existing models of context's role in motor memories (e.g., Oh and Schweighofer, 2019; Heald et al., 2021) propose that memory-related changes can be observed even without explicit contextual information, as contextual changes can be inferred from sudden or significant environmental shifts (e.g., the introduction or removal of perturbations). Given this, could the observed savings in the current simulation be explained by some form of contextual retrieval, inferred by the network from the re-presentation of the perturbation in FF2?</p>
<p>(5) If there is residual hidden unit activity related to the FF at the end of the NF2 phase, how does the simulated movement revert back to baseline? Are there any differences in the movement trajectory, beyond just lateral deviation, between NF1 and NF2? The authors state that &quot;changes in the preparatory hidden unit activity did not result in substantive changes in the motor commands (Figure 5b), which emphasizes that the uniform shift resides in the null space of motor output.&quot; However, Figure 5b appears to show visible changes in hidden unit activity. Don't these changes reflect a pattern of muscle activity that is the basis for behavior? These changes are indeed small, but it seems that so is the effect size for savings (Figure 3a). Could this suggest that there is not, in fact, a complete washout of initial learning during NF2 within the network?</p>
</body>
</sub-article>
</article>