<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">91662</article-id>
<article-id pub-id-type="doi">10.7554/eLife.91662</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91662.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Pinpoint: trajectory planning for multi-probe electrophysiology and injections in an interactive web-based 3D environment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3748-6289</contrib-id>
<name>
<surname>Birman</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yang</surname>
<given-names>Kenneth J.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>West</surname>
<given-names>Steven J.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Karsh</surname>
<given-names>Bill</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Browning</surname>
<given-names>Yoni</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<collab>the International Brain Laboratory</collab>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Siegle</surname>
<given-names>Joshua H.</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7029-2908</contrib-id>
<name>
<surname>Steinmetz</surname>
<given-names>Nicholas A.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Biological Structure, University of Washington</institution>, Seattle, WA 98195, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Sainsbury Wellcome Centre</institution>, London W1T 4JG, <country>United Kingdom</country></aff>
<aff id="a3"><label>3</label><institution>HHMI Janelia Research Campus</institution>, Ashburn, Virginia 20147, <country>USA</country></aff>
<aff id="a4"><label>4</label><institution>Allen Institute for Neural Dynamics</institution>, Seattle, WA 98109, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gallego</surname>
<given-names>Juan Alvaro</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Imperial College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Wassum</surname>
<given-names>Kate M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of California, Los Angeles</institution>
</institution-wrap>
<city>Los Angeles</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Corresponding author: <email>dbirman@uw.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-10-16">
<day>16</day>
<month>10</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP91662</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-08-14">
<day>14</day>
<month>08</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-07-15">
<day>15</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.07.14.548952"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Birman et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Birman et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-91662-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Targeting deep brain structures during electrophysiology and injections requires intensive training and expertise. Even with experience, researchers often can’t be certain that a probe is placed precisely in a target location and this complexity scales with the number of simultaneous probes used in an experiment. Here, we present <italic>Pinpoint</italic>, open-source software that allows for interactive exploration of stereotaxic insertion plans. Once an insertion plan is created, Pinpoint allows users to save these online and share them with collaborators. 3D modeling tools allow users to explore their insertions alongside rig and implant hardware and ensure plans are physically possible. Probes in Pinpoint can be linked to electronic micro-manipulators allowing real-time visualization of current brain region targets alongside neural data. In addition, Pinpoint can control manipulators to automate and parallelize the insertion process. Compared to previously available software, Pinpoint’s easy access through web browsers, extensive features, and real-time experiment integration enable more efficient and reproducible recordings.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://virtualbrainlab.org/pinpoint/installation_and_use.html">https://virtualbrainlab.org/pinpoint/installation_and_use.html</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/VirtualBrainLab/Pinpoint">https://github.com/VirtualBrainLab/Pinpoint</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The availability of high-density low-cost probes, such as Neuropixels (<xref ref-type="bibr" rid="c10">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="c19">Steinmetz et al., 2021</xref>), has led to a step change in the scale (<xref ref-type="bibr" rid="c2">Allen et al., 2019</xref>; <xref ref-type="bibr" rid="c5">Durand et al., 2022</xref>; International Brain Laboratory et al., 2023; <xref ref-type="bibr" rid="c20">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="c21">Stringer et al., 2019</xref>) and ambition (<xref ref-type="bibr" rid="c1">Abbott et al., 2017</xref>; <xref ref-type="bibr" rid="c13">Koch et al., 2022</xref>) of modern neuroscience. This significant advance in experimental capability is possible in part due to the experimental support provided by advanced software interfaces which support data acquisition (<xref ref-type="bibr" rid="c11">Karsh, 2016</xref>; <xref ref-type="bibr" rid="c18">Siegle et al., 2017</xref>) and experimental planning (<xref ref-type="bibr" rid="c7">Fuglstad et al., 2023</xref>; <xref ref-type="bibr" rid="c16">Peters, 2022</xref>). Here, we introduce Pinpoint, software that enables high-quality large-scale electrophysiology research by rendering anatomical brain atlases, calculating the paths in 3D space for dozens of multi-shank probes, preventing collisions, and interfacing with external software and hardware, all in an intuitive and accessible package. Pinpoint lowers the barrier to experimental planning and disconnects the need for expert anatomical knowledge of the brain from the technical skill required to perform complex simultaneous multi-probe recordings. Pinpoint can be run in a web browser with no installation requirements and has a minimal learning curve, making it useful not only for experiment planning, but also for exploring mouse brain anatomy and for teaching new researchers about how <italic>in vivo</italic> experiments are performed.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>Pinpoint provides users with an interactive 3D scene in which electrophysiology trajectories can be explored within the anatomical context of the mouse brain (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). At the center of the 3D scene we render transparent 3D meshes for the major brain structures in the mouse Common Coordinate Framework (CCF) (<xref ref-type="bibr" rid="c22">Wang et al., 2020</xref>). 3D models of Neuropixels probes (<xref ref-type="bibr" rid="c10">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="c19">Steinmetz et al., 2021</xref>) or injection needles can be added to the scene and moved through intuitive click+drag or keyboard interaction. Pinpoint provides visualizations of the brain areas that each probe is passing through via a “channel map” (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>) and an “in-plane slice” (top right, <xref rid="fig1" ref-type="fig">Fig. 1b</xref>). As output, Pinpoint provides the stereotaxic coordinates needed to perform an insertion (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Overview of the Pinpoint interface. (a) Vertical panels show the channel map for two Neuropixels 1.0 probes. Channels are colored by the mouse Common Coordinate Framework region they are inserted in. Area acronyms are shown on the right. (b) The 3D scene shows a transparent view of the mouse brain and models of the Neuropixels probes. On the top right of the user interface (UI), an orientation widget (red, yellow, and blue crosshairs) helps users track the orientation and snap the view to axial, coronal, or sagittal views. The right UI panels include an in-plane slice of the reference atlas, individual area search, stereotaxic coordinates, and at the bottom right a “Share” button which creates a permanent URL to re-load the same scene in another browser. (c) Searching for regions highlights them in the 3D scene as opaque 3D models (highlighted here are parts of visual cortex, hippocampus, and thalamus). (d) Examples of the probes available in Pinpoint. (e) The stereotaxic coordinates relative to a reference coordinate (Bregma, by default) for performing the insertion. The angles (yaw, pitch, and roll) are used to set up the manipulator prior to an experiment. (f) A craniotomy placement tool, combined with a model of the mouse skull, helps users plan surgeries. (g) Example of 3D models that can be placed in the scene, here a “skull cap” used for an implant surgery is shown over the mouse skull, followed by additional examples of experimental hardware including headbars and an imaging lens. (h) Pinpoint detects collisions, to ensure that multi-probe insertion plans are4possible. Colliding models are highlighted in red.</p></caption>
<graphic xlink:href="548952v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2a">
<title>Planning an insertion trajectory in 3D space</title>
<p>In addition to this brief overview of planning, we also include a video tutorial (<xref rid="fig2" ref-type="fig">Fig. 2</xref>) and a comprehensive written tutorial in the Methods. Additional <ext-link ext-link-type="uri" xlink:href="https://virtualbrainlab.org/pinpoint/installation_and_use.html">documentation</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://virtualbrainlab.org/pinpoint/tutorial.html">in-depth tutorial videos</ext-link> about individual features can be found online.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Still frame from the <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=htTz5tUpCwo">Plan an insertion in Pinpoint</ext-link> video, a six-minute demonstration of key features.</p></caption>
<graphic xlink:href="548952v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>A central challenge in performing experiments with large-scale electrophysiology is identifying the 3D trajectory required to target the brain region or regions of interest, while constrained by feasible craniotomy locations and experimental apparatus. Visualizing and computing such a trajectory from a 2D paper atlas (<xref ref-type="bibr" rid="c15">Paxinos &amp; Franklin, 2019</xref>) is difficult and time-consuming.</p>
<p>To plan an insertion, users need to see both the probe, the areas that they are targeting, and constraints imposed by their setup, while having the ability to optimize probe insertion paths to reach each area of interest. In the default view, Pinpoint displays transparent 3D models of the mouse brain areas defined in the CCF reference atlas. Users can search for areas of interest, highlight these (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>), and “snap” probe tips to the center of a particular region. Once a probe is in position, keyboard presses or mouse click+drag controls are used to optimize the trajectory.</p>
<p>To perform the actual implant of a probe, users target the stereotaxic entry coordinate and angles provided by Pinpoint. Pinpoint provides users with the entry coordinate on the brain surface and the depth of the insertion (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>) in stereotaxic space. By default, Pinpoint defines the entry coordinate of an insertion relative to Bregma (AP=+5400, ML=+5700, DV=+332 <italic>µ</italic>m in CCF space). Probe angles are defined as yaw (angle around the up axis), pitch (angle off the horizontal plane), and roll (rotation around the probe’s axis).</p>
<p>Because the Allen Common Coordinate Framework (CCF) reference atlas was defined using perfused brains, some users may find that their targeting is improved by using a deformation of this space which better matches the live mouse brain (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). In Pinpoint, users can choose to plan their trajectories in the Allen Common Coordinate Framework (CCF, <xref rid="fig3" ref-type="fig">Fig. 3b</xref>) space or in a deformation of the CCF (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>). The transformed spaces were defined from anatomical MRI images that were aligned to the CCF and in theory provide a better estimate of the live mouse brain, depending on the age and strain. Pinpoint includes two MRI transforms: the Dorr2008 transform (<xref ref-type="bibr" rid="c4">Dorr et al., 2008</xref>) taken from the average of 40 in-skull MRIs after death and the Qiu2018 transform (<xref ref-type="bibr" rid="c17">Qiu et al., 2018</xref>) taken from the average of 12 live mice. The transforms can be further tailored to individual mice by isometric scaling of the transformed space according to the ratio between an individual mouse’s measured Bregma-Lambda distance relative to the average in the reference atlases, 4150 <italic>µ</italic>m. Additional transforms, such as the Paxinos atlas (<xref ref-type="bibr" rid="c15">Paxinos &amp; Franklin, 2019</xref>) will be available in future releases.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Examples of mouse brain reference atlases included in Pinpoint. (a) Pinpoint supports both histological and anatomically accurate reference atlases. The default axes of the Unity world space represent a standard cartesian coordinate system. To define the space of a reference atlas, such as the Allen Common Coordinate Framework, a Coordinate Space is created, which redefines the zero point to be the top-left-front corner of the reference volume. Because some volumes were not defined using the brains of live animals, we further support Coordinate Transforms, which are affine transforms of an atlas and may optionally redefine the zero coordinate to a different position, such as Bregma, as shown here. (b) Sagittal and coronal slices are shown for the Allen CCF Coordinate Space and (c) for the <xref ref-type="bibr" rid="c17">Qiu et al. (2018)</xref> Coordinate Transform, demonstrating how the transform is tilted upwards, stretched along the AP axis and compressed along the DV and ML axes.</p></caption>
<graphic xlink:href="548952v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When planning complex insertions, users may want to ensure that the 3D geometry of their recording plan will be compatible within the constraints of their craniotomies and experimental hardware. To plan probes relative to craniotomies, Pinpoint can display a skull model and has an interface for adding circular holes of varying radius (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>). The 3D scene in Pinpoint can be further modified by adding 3D models for experimental hardware, so long as the position is known relative to Bregma. For example, users can display custom headbars and imaging lenses (<xref rid="fig1" ref-type="fig">Fig. 1g</xref>). Pinpoint automatically detects collisions between probes and between probes and hardware and warns the user if a set of insertions will cause problems (<xref rid="fig1" ref-type="fig">Fig. 1h</xref>). Adding experimental hardware is not limited to just 3D models, advanced users can use the Unity editor to modify the entire Pinpoint scene to accommodate alternative coordinate systems and rig geometries.</p>
</sec>
<sec id="s2b">
<title>Interfacing with data acquisition software and hardware</title>
<p>As multi-probe recordings become increasingly complex, there is a need for anatomical targeting information to be available live during recordings. We developed two sets of features in Pinpoint to support this: a hardware application programming interface (API) that allows Pinpoint to communicate with micro-manipulators and a software API that broadcasts the current per-channel annotation data to data acquisition software such as the Open Ephys GUI or SpikeGLX.</p>
<p>We expect users to take advantage of these features in three ways (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). First, users can send the planned anatomical targets for an insertion to their data acquisition software as a reference to compare their electro-physiology features against (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). Although this planned anatomical information is not updated as a probe moves, it provides a reference frame that the electrophysiology can be compared against to improve the accuracy of targeting. This can be especially helpful when researchers are targeting regions for the first time, or when performing complex insertions.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Integration with external hardware and software. Pinpoint exposes two application programming inter-faces (APIs) that allow for annotation and position data to be streamed to other applications and for manipulator positions to be echoed in the Pinpoint scene. We present three use cases for these external tools. (a) For researchers who want to improve their targeting accuracy during insertion, Pinpoint can send anatomical information about the final predicted target location of a probe to data acquisition software, such as the Open Ephys GUI or SpikeGLX. Users can compare these per-channel annotations against the electrophysiological signatures they observe on their probe to optimize their targeting. An example of the API channel annotation format is shown in the center box. (b) Pinpoint can also be linked to hardware micro-manipulators from Sensapex and New Scale, showing a live estimate of the current position of the probe during an experiment. This data can then be optionally forwarded to data acquisition software to display a real-time estimate of the probe’s position alongside the electrophysiology. (c) Finally, for users who want to maximize the efficiency of multi-probe recordings and minimize the potential for user error, Pinpoint offers a Copilot mode in which the insertion process is run by the software with minimal user intervention. (d,e) Examples of the Open Ephys and SpikeGLX graphical interfaces displaying probe annotation information.</p></caption>
<graphic xlink:href="548952v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>During a live experiment, researchers often want to know where they are positioning their probe during an insertion. This information helps trainees learn about the brain more efficiently and helps experts avoid missing their target regions. The second way that users can take advantage of the API is by linking a hardware manipulator to Pinpoint and then streaming the probe’s live position to to Open Ephys (<xref rid="fig4" ref-type="fig">Fig. 4d</xref>) or SpikeGLX (<xref rid="fig4" ref-type="fig">Fig. 4e</xref>) to communicate this information, so that those programs can show the anatomical locations of live electrophysiology data during an experiment. To develop this capability we built a Python package “Electrophysiology Manipulator Link” (Ephys Link) which acts as an intermediate server between Pinpoint and different hardware manipulator platforms. Pinpoint, and other programs, use Ephys Link to read position information from linked manipulators. During an experiment, users place their probe at a reference coordinate (Bregma, by default) and link the live probe to the virtual probe in Pinpoint. Once linked, all movements on the manipulator are reflected in Pinpoint, allowing researchers to see an estimate of their live position in the brain as they insert their probe.</p>
<p>Finally, the hardware API also supports sending commands back to the micro-manipulators, allowing us to control probes both using keyboard commands and to automate the probe insertion process. Pinpoint includes an automation sequence we call “Copilot” that simplifies and parallelizes the process of inserting multiple probes at the same time. Tracking probe positions, logging data about probe movements, and performing the actual movement of probes to their entry coordinates, as well as inserting and removing probes from the brain are all automated, allowing users to focus on other aspects of conducting their experiment. The Copilot sequence does require two manual steps, first when users reference their probes to Bregma, and second when they insert the probe tip into the brain.</p>
</sec>
<sec id="s2c">
<title>Sharing insertion plans</title>
<p>To plan ongoing complex experiments with many recordings split across multiple days, Pinpoint allows users to save and share their insertion plans. Users can create an account in the Pinpoint system, allowing them to save and load sets of insertions. Users can also at any time create a permanent URL link to a set of insertions, allowing them to share these with collaborators over the internet.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In the past, researchers planned their insertions using 2D slices of the mouse brain, requiring careful calculation to plan complex trajectories across multiple slices. Pinpoint improves on this substantially by allowing users to explore potential probe trajectories through an intuitive 3D interface without any installation requirements. Pinpoint’s hardware and software integration allow users to see their probe position live and to parallelize the insertion process, yielding huge time savings for researchers as they scale up from one or two probes to dozens of simultaneous insertions.</p>
<p>Among Pinpoint’s strengths are its easy access via web browsers, intuitive user interface, and powerful APIs. These advances remain limited by the individual variability of the brain across mice. While Pinpoint can account for isometric scaling that is correlated with skull size through the Bregma-Lambda scaling feature, we are not able to account for nonlinear differences across brains, or uncorrelated variability. Future development could include additional brain atlases, for example developmental mouse atlases (<xref ref-type="bibr" rid="c24">Young et al., 2021</xref>) or the Waxholm rat atlas (<xref ref-type="bibr" rid="c14">Papp et al., 2014</xref>), and the capability to load individual anatomical MRI data which is critical for targeting in non-human primates.</p>
<p>The future of neuroscience requires that researchers be capable of recording from thousands of neurons, simulta-neously, and across many different model systems. Pinpoint is well-positioned to support this future.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Application and code</title>
<p>Pinpoint is available as a <ext-link ext-link-type="uri" xlink:href="https://data.virtualbrainlab.org/Pinpoint/">website</ext-link> or as a standalone desktop application. Desktop releases are distributed through <ext-link ext-link-type="uri" xlink:href="https://store.steampowered.com/app/2434260/Pinpoint/">Steam</ext-link>. Documentation and tutorials can be found on our <ext-link ext-link-type="uri" xlink:href="https://virtualbrainlab.org/pinpoint/installation_and_use.html">main website</ext-link>. The code is open-source and can be found on <ext-link ext-link-type="uri" xlink:href="https://github.com/VirtualBrainLab/Pinpoint">GitHub</ext-link>.</p>
<p>Pinpoint is developed using the Unity Real-Time Development Platform (Unity). Unity is a cross-platform game engine that supports the development of interactive 2D and 3D experiences. We use a number of specific Unity packages to support features in Pinpoint. The base editor (v2021.3.10f1) enables the core features including the 3D scene, use of materials and shaders, and point-and-click and keyboard interaction. The base editor also enables us to distribute Pinpoint for WebGL and the desktop platforms Windows, Linux, and MacOS. The user interface was developed using the Unity UI package. Large assets are bundled and loaded asynchronously using Addressables. Pinpoint also depends on several Unity Asset Store assets: Best HTTP/2, Unisave, Clean Flat UI, and UX Flat Icons.</p>
<p>Some of the functionality developed for Pinpoint is bundled in separate repositories, allowing other users to re-use these specific features in their own projects. The Electrophysiology Manipulator Link server is distributed in the <ext-link ext-link-type="uri" xlink:href="https://github.com/VirtualBrainLab/ephys-link">ephys-link repository</ext-link> and releases can be found on <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/ephys-link/">PyPI</ext-link>. To support the CCF and other atlases, we developed the <ext-link ext-link-type="uri" xlink:href="https://github.com/VirtualBrainLab/BrainAtlas">BrainAtlas for Unity</ext-link> package, which is a wrapper around the BrainGlobe brain-atlas Python package (<xref ref-type="bibr" rid="c3">Claudi et al., 2020</xref>).</p>
</sec>
<sec id="s4b">
<title>Tutorial</title>
<p>The following sections describe the step-by-step process necessary to plan a probe insertion in Pinpoint.</p>
<sec id="s4b1">
<title>Camera controls</title>
<p>Before planning an insertion, it is helpful to master the camera control system. Left click and hold the mouse down in the 3D scene to pitch (Y axis) and roll the brain (X axis). You can also hold space bar while dragging the mouse to yaw the brain. In the top right corner of the scene there is a small axis widget that helps you track the brain’s orientation. The yellow axis is DV (dorsal-ventral), the red axis is AP (anterior-posterior), and the blue axis is ML (medial-lateral, or left-right). Double clicking any of these axes on the widget will snap the brain to the corresponding axial, coronal, or sagittal view, respectively. The scroll wheel on the mouse can be used to zoom in and out. Right click and hold the mouse down to pan the camera and double right-click to reset this.</p>
</sec>
<sec id="s4b2">
<title>Select a probe</title>
<p>To add a probe to the scene, select the “add new probe” button and then select your Neuropixels, UCLA, or pipette probe style. The Pinpoint scene supports an unlimited number of probes, but only 16 shanks can be shown at the same time on the user interface. If you add more shanks to the scene, the “Show only active probe panels” setting will be enabled, hiding the channel map data for probes that are not actively selected.</p>
</sec>
<sec id="s4b3">
<title>Setting up targets</title>
<p>Target regions can be highlighted in the 3D scene by searching for them in the search bar (“Search for area…”, right side of UI) and then clicking on the area in the hierarchy. Clicking on a region again removes it from the 3D scene and all highlighted areas can be removed using the red “Clear” button. When there is an active probe in the 3D scene the “Snap” button (downward pointing arrow) can be used to move the probe tip to the center of a particular brain region. Areas can also be highlighted by clicking them in the in-plane slice view (top right of UI).</p>
</sec>
<sec id="s4b4">
<title>Keyboard and mouse controls</title>
<p>Once a probe is aligned to a target region, the trajectory will need to be optimized. Users can enter changes in the angles and position of the probe through the stereotaxic coordinate panel (right side of UI), or by using the keyboard or mouse. To use the coordinate panel, type the new AP, ML, DV, Depth, Yaw, Pitch, or Roll value into the corresponding box and press enter to apply the change. Note that Pinpoint always reports back the <italic>entry coordinate</italic> and depth of an insertion. This means that if you enter a target coordinate for the tip of an insertion (with a depth of zero) the values you enter will be replaced in the coordinate panel with the entry coordinate and depth when you apply the change.</p>
<p>To use the keyboard, press the key corresponding to the axis and direction: anterior-posterior, W and S; left-right, A and D; dorsal-ventral, E and Q. Keyboard clicks can also rotate probes: yaw, <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> ; pitch, <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and ; <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> roll, <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. To move the probe along its depth axis press <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> or <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>. Each of these keyboard presses moves the probe tip by 10 <italic>µ</italic>m along that axis. Holding <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline9.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> increases the step size to 100 <italic>µ</italic>m m while holding <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline10.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> reduces it to 1 <italic>µ</italic>m.</p>
<p>To use the mouse click and drag controls, press and hold the left mouse button on the 3D model of the probe. While holding the mouse button down, press any of the axis keys (e.g. <inline-formula><alternatives><inline-graphic xlink:href="548952v1_inline11.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for anterior-posterior). Then, drag the mouse along the axis to move the probe in a continuous motion. Note that the dragging is along the axis, so from some camera views certain axes are not available, for example when looking at the axial plane (from above) it is not possible to drag the probe along the dorsal-ventral axis.</p>
</sec>
<sec id="s4b5">
<title>Channel map</title>
<p>On the left side of the UI, the channel maps for the active probe will be visible. By default, the channel map uses the bank zero setting for Neuropixels probes or all channels for UCLA probes and pipettes, this can be changed in the Probe settings panel. Channels are colored by the region they are inside of within the CCF hierarchy and the acronyms for each region are shown centered alongside the channels. A thin colored line next to the channel map helps to clearly show the boundaries between regions. Note that the channel map is deformed to match the available UI space and does not correspond to the real-world scaling of the probe.</p>
</sec>
</sec>
<sec id="s4c">
<title>Surgery coordinates</title>
<p>Pinpoint computes the entry coordinate for surgeries by finding the coordinate in the reference atlas where the brain surface and the trajectory intersect. Each probe insertion is defined by a tip coordinate (AP, ML, DV) and angles (Yaw, Pitch, Roll). To find the entry coordinate, we step down the probe insertion vector until we are inside the brain. The last coordinate <italic>outside</italic> the brain is considered to be the entry. Raw coordinates are reported in their transformed space, so that they best match the live mouse brain. By default, Pinpoint uses the 25 <italic>µ</italic>m CCF atlas, so entry coordinates will necessarily differ by a small amount from the dura surface but these differences are smaller than the expected variability introduced by experimenters (International Brain Laboratory et al., 2022).</p>
<p>By default, Pinpoint reports the entry coordinate relative to Bregma. This setting can be changed in the Atlas settings, using CCF conventions for the coordinate. For example, to place probes in raw CCF coordinates you would set the reference coordinate to (0,0,0).</p>
<p>To perform a recording, users must level the mouse skull along the Bregma-Lambda and Left-Right axes and set up their probe according to the specified angles. Users then zero their stereotax with the probe tip at the reference coordinate (Bregma, by default) and move the probe to the entry coordinate. If necessary, the probe should be lowered along the DV axis to reach the brain surface. Once at the entry coordinate on the brain surface, users should zero the depth axis and lock all other axes. The target brain area is then reached by driving to the specified depth value. Note that to achieve accurate targeting two references are set: first at Bregma, and then independently at the dura surface for the depth axis.</p>
</sec>
<sec id="s4d">
<title>Atlases and transformations</title>
<p>We define anatomical atlases in Pinpoint using a CoordinateSpace and a CoordinateTransform. The CoordinateS-pace defines the transformation of coordinates in Unity “World” space into the reference atlas. By default in Unity the Z axis points forward, the X axis to the right, and the Y axis up. The CoordinateSpace for the Allen CCF rotates a vector such that it is redefined according to the anterior-poster, left-right or medial-lateral, and ventral-dorsal axes instead of the world axes. According to the specifications, the CCF is defined such that the (0,0,0) coordinate is at the front, top, left coordinate. By collecting these rotations and offsets into a CoordinateSpace class we make it easy to extend Pinpoint in the future to other anatomical atlases.</p>
<p>Because the CCF was defined from perfused brains it is deformed from the live mouse brain. A CoordinateTrans-form attempts to reverse these deformations and make it possible to examine anatomical data in the space of individual mouse brains. Each CoordinateTransform defines the transformation of a point from a CoordinateSpace into a deformed space defined by anatomical data from a particular mouse or strain, or an average of several mice.</p>
<p>To define each CoordinateTransform we registered the mouse CCF (<xref ref-type="bibr" rid="c22">Wang et al., 2020</xref>) atlas into an anatomical MRI space. Pinpoint implements two CoordinateTransforms: the Dorr2008 atlas defined from 40 in-skull anatomical MRIs taken from p84 C57BL/6j mice after death (<xref ref-type="bibr" rid="c4">Dorr et al., 2008</xref>) and the Qiu2018 atlas defined from 12 p65 C57BL/6j mice that were alive at the time of anatomical scanning (<xref ref-type="bibr" rid="c17">Qiu et al., 2018</xref>). Pinpoint also implements a rotation in each CoordinateTransform to level the Bregma-Lambda surface, pitching the CCF atlas up by 5 degrees (International Brain Laboratory et al., 2022).</p>
<p>Transforms were computed using BrainRegister (<xref ref-type="bibr" rid="c23">West, 2021</xref>), a registration pipeline based on elastix (<xref ref-type="bibr" rid="c12">Klein et al., 2010</xref>) with optimised parameters for mouse brain registration.</p>
<sec id="s4d1">
<title>Bregma and lambda coordinates</title>
<p>The Bregma and Lambda coordinates in CCF space were determined by finding these points on the average MRI model from <xref ref-type="bibr" rid="c17">Qiu et al. (2018)</xref> after alignment to the CCF reference atlas.</p>
</sec>
</sec>
<sec id="s4e">
<title>Sharing and Accounts</title>
<p>To save and load experimental plans we developed a feature to share insertion plans through permanent online links and an accounts system that allows users to store insertions in a private database. The share button creates a Base64-encoded query string consisting of the JSON-formatted insertion data for all probes visible in the scene, as well as limited information about the scene view itself.</p>
<p>For the accounts system we integrated Unisave, a back-end database which stores information about insertions in an account secured by a user’s email and a password. Passwords are encrypted according to industry standard practices and are not recoverable. Each account in Unisave is a collection of experiments and each experiment is a collection of insertions and their metadata (e.g. name, probe color).</p>
</sec>
<sec id="s4f">
<title>Application Programming Interface (API)</title>
<p>Pinpoint exposes an API for sharing per-channel information about each active probe in the scene. The API generates an array of data strings using the format: “[probe0 data, probe 1 data, …]” with each probe’s data formatted using the syntax: “probe-id:channel 0 data;channel 1 data”, and finally with the channel data formatted as “channel#,ccf-atlas-id,ccf-atlas-acronym,hex-color”. The string can be manually exported from the API tab in the settings. Toggling the API features for Open Ephys or SpikeGLX forwards compressed versions of the data string designed for their respective APIs. Each program can then display anatomical information alongisde the electrophysiology data.</p>
</sec>
<sec id="s4g">
<title>Electrophysiology Manipulator Link (Ephys Link)</title>
<p>To enable position echoing and automated control of micro-manipulators, we developed an intermediate Python package <italic>ephys-link</italic> which handles communication between Pinpoint and various hardware manipulators. Ephys Link is a Python-based WebSocket server split into three layers that connects client applications to manipulator platforms using a standardized set of WebSocket events. The main layer is an asynchronous HTTP server that is responsible for serving the WebSocket connection, error checking, serial connections to peripherals, and managing the user interface. The server calls a manipulator platform layer which defines platform-specific implementations for the declared WebSocket events, manages connected manipulator instances, and handles API calls that affect all instances simultaneously. Each manipulator platform can optionally define a manipulator class that holds details specific to <italic>in vivo</italic> manipulator instances such as its ID, position, axis transform, and movement queue. Creating a client application that uses Ephys Link to communicate with manipulator platforms requires making calls to the defined WebSocket events and handling the returned information.</p>
<sec id="s4g1">
<title>Position echoing</title>
<p>Knowing the position of a probe live during an experiment allows researchers to make substantial changes to their insertion plan while still being confident about reaching their intended brain region target. To support this, <italic>ephys-link</italic> detects and communicates with manipulators that are connected to the same computer and exposes a WebSocket connection for communicating with clients, such as Pinpoint. Once connected, users need to match their probes in the Pinpoint scene to the live manipulators and then set a reference position. To match the probes the yaw, pitch, and roll angles must be set to be identical and the mouse’s skull must be level along the Bregma-Lambda and Left-Right axes. To set the reference, the live probe needs to be moved to the reference coordinate (Bregma, by default) so that Pinpoint can record this position and subtract it from further movements. After linking and referencing, further movements of the micro-manipulator are echoed in the scene allowing users to visualize their probe position live relative to the mouse brain.</p>
</sec>
<sec id="s4g2">
<title>Copilot</title>
<p>To maximize efficiency and reproducibility of experiments, it’s critical to use an insertion process that is repeatable and which minimizes errors. When connected to the <italic>ephys-link</italic> package, Pinpoint can act as a Copilot during the insertion process to achieve perfect repetition of pre-specified insertion plans at safe insertion speeds. To use Copilot in Pinpoint we developed a standard sequence for performing multi-probe insertions: first, users set up the Pinpoint scene to have the 3D models of all probes. Second, they link these probes to the hardware manipulators and choose the target insertions. Third, they zero all of the probes at Bregma individually. Finally, they allow the insertion sequence to run. The sequence moves probes first to their insertion coordinate, allows the user to enter the brain, and finally drives the probes to the appropriate depth at a speed which minimizes damage during the insertion (<xref ref-type="bibr" rid="c6">Fiáth et al., 2019</xref>) and includes time for settling. Note that brain entry step is manual: we don’t support automated entry because probes often do not enter smoothly and require user input to avoid damage to probes. After a recording, probes are automatically retracted in parallel, freeing up researchers to focus on other tasks.</p>
</sec>
</sec>
<sec id="s4h">
<title>Other settings</title>
<p>In addition to the features and settings already described, Pinpoint also features a number of other optional settings that make planning insertions more convenient. Users can choose to change the coordinate system to be aligned to the axis of their probe, automatically converting from AP/ML coordinates to their probe’s Forward/Side axes. Pinpoint defines a probe’s angles to be at (0,0,0) when the probe tip is pointing anterior with the electrode surface pointing up, but a setting allows for alternate conventions such as those used by the IBL (where 0,0,0 is a probe pointing ventral with the electrode surface pointing left). Area settings allow users to switch between using area acronyms or full names and to drop white matter tracts that are unlikely to be targeted during an experiment. Atlas settings allow users to switch transforms and to turn on coronal and sagittal slices which move with the probe tip, helping to better align probes within the brain. A coverage atlas (saved as a flattened numpy byte array using np.to_bytes(), where 0 = transparent and 1/2/… indicate increasing coverage) can also be loaded from the Atlas settings from any URL and overlaid onto the reference atlas. Graphics settings allow users to change the background color, make inactive probes transparent, hide the entry coordinate on the brain surface, and hide brain regions that are not being targeted.</p>
</sec>
<sec id="s4i">
<title>Contributing</title>
<p>Pinpoint is an open-source project and is open to reports about bugs as well as contributions from the neuroscience community. Bugs, comments, and concerns can be reported through the <ext-link ext-link-type="uri" xlink:href="https://github.com/VirtualBrainLab/Pinpoint/issues">issues page</ext-link>. Instructions for developing and contributing to Pinpoint can be found on our <ext-link ext-link-type="uri" xlink:href="https://virtualbrainlab.org/pinpoint/development.html">development page</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<sec id="s5">
<title>Funding</title>
<p>We acknowledge the generous support of the Washington Research Foundation Postdoctoral Fellowship to DB, the Research Scholarship from the Mary Gates Endowment for Students to KY, the Wellcome Trust (216324),</p>
<p>Simons Foundation, and NIH Grant U19NS123716 to the IBL.</p>
</sec><sec id="s6">
<title>Acknowledgements</title>
<p>We thank Kai Nylund for help designing an early prototype, the Steinmetz Lab and International Brain Laboratory for feedback on the user experience and interface, and Sensapex and New Scale Technologies for their assistance with SDK access to their hardware platforms.</p>
</sec></ack>
<sec id="s7">
<title>Contributions</title>
<p>DB -conceptualization, software, supervision, funding acquisition, visualization, methodology, project administration, writing -original draft &amp; review and editing, KY -software, visualization, SJ -formal analysis, methodology, BK -software, JS -software, writing -review and editing, YB -methodology, software, IBL -funding acquisition, writing -review and editing, NS -conceptualization, supervision, funding acquisition, writing -review and editing.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>Angelaki</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fiete</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name>, <etal>et al.</etal> (<year>2017</year>). <article-title>An international laboratory for systems and computational neuroscience</article-title>. <source>Neuron</source>, <volume>96</volume> (<issue>6</issue>), <fpage>1213</fpage>–<lpage>1218</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Allen</surname>, <given-names>W. E.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>M. Z.</given-names></string-name>, <string-name><surname>Pichamoorthy</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Tien</surname>, <given-names>R. H.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name> (<year>2019</year>). <article-title>Thirst regulates motivated behavior through modulation of brainwide neural population dynamics</article-title>. <source>Science</source>, <volume>364</volume> (<issue>6437</issue>), <fpage>eaav3932</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Claudi</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Petrucco</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tyson</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Branco</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Margrie</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Portugues</surname>, <given-names>R.</given-names></string-name> (<year>2020</year>). <article-title>Brainglobe atlas api: A common interface for neuroanatomical atlases</article-title>. <source>Journal of Open Source Software</source>, <volume>5</volume> (<issue>54</issue>), <fpage>2668</fpage>–<lpage>2668</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Dorr</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lerch</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Spring</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kabani</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Henkelman</surname>, <given-names>R. M.</given-names></string-name> (<year>2008</year>). <article-title>High resolution three-dimensional brain atlas using an average magnetic resonance image of 40 adult c57bl/6j mice</article-title>. <source>Neuroimage</source>, <volume>42</volume> (<issue>1</issue>), <fpage>60</fpage>–<lpage>69</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="other"><string-name><surname>Durand</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Heller</surname>, <given-names>G. R.</given-names></string-name>, <string-name><surname>Ramirez</surname>, <given-names>T. K.</given-names></string-name>, <string-name><surname>Luviano</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Williford</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>D. T.</given-names></string-name>, <string-name><surname>Cahoon</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Farrell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Groblewski</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal> (<year>2022</year>). <article-title>Acute head-fixed recordings in awake mice with multiple neuropixels probes</article-title>. <source>Nature Protocols</source>, <fpage>1</fpage>–<lpage>34</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Fiáth</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Márton</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Mátyás</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Pinke</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Márton</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Tóth</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Ulbert</surname>, <given-names>I.</given-names></string-name> (<year>2019</year>). <article-title>Slow insertion of silicon probes improves the quality of acute neuronal recordings</article-title>. <source>Scientific Reports</source>, <volume>9</volume> (<issue>1</issue>), <fpage>111</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Fuglstad</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Saldanha</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Paglia</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Whitlock</surname>, <given-names>J. R.</given-names></string-name> (<year>2023</year>). <article-title>Histological e-data registration in rodent brain spaces</article-title>. <source>Elife</source>, <volume>12</volume>, <fpage>e83496</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="other"><collab>International Brain Laboratory, T</collab>., <etal>et al.</etal> (<year>2023</year>). <article-title>A brain-wide map of neural activity during complex behaviour</article-title>. <source>bioRxiv</source>, <fpage>2023</fpage>–<lpage>07</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="other"><collab>International Brain Laboratory, T</collab>., <string-name><surname>Banga</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Benson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bonacchi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bruijns</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Campbell</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chapuis</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Davatolhagh</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>H. D.</given-names></string-name>, <etal>et al.</etal> (<year>2022</year>). <article-title>Reproducibility of in-vivo electrophysiological measurements in mice</article-title>. <source>bioRxiv</source>, <fpage>2022</fpage>–<lpage>05</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Jun</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Denman</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Bauza</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Barbarits</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Anastassiou</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Andrei</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>AydIn</surname>, <given-names>Ç.</given-names></string-name>, <etal>et al.</etal> (<year>2017</year>). <article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title>. <source>Nature</source>, <volume>551</volume> (<issue>7679</issue>), <fpage>232</fpage>–<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="web"><string-name><surname>Karsh</surname>, <given-names>B.</given-names></string-name> (<year>2016</year>). <ext-link ext-link-type="uri" xlink:href="http://billkarsh.github.io/SpikeGLX/">http://billkarsh.github.io/SpikeGLX/</ext-link></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Klein</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Staring</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Murphy</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Viergever</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Pluim</surname>, <given-names>J. P. W.</given-names></string-name> (<year>2010</year>). <article-title>Elastix: A toolbox for intensity based medical image registration</article-title>. <source>IEEE Transactions on Medical Imaging</source>, <volume>29</volume> (<issue>1</issue>), <fpage>196</fpage>–<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2009.2035616</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bernard</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Basso</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Fairhall</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Groblewski</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Lecoq</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Mainen</surname>, <given-names>Z. F.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name>, <etal>et al.</etal> (<year>2022</year>). <article-title>Next-generation brain observatories</article-title>. <source>Neuron</source>, <volume>110</volume> (<issue>22</issue>), <fpage>3661</fpage>–<lpage>3666</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Papp</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Leergaard</surname>, <given-names>T. B.</given-names></string-name>, <string-name><surname>Calabrese</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>G. A.</given-names></string-name>, &amp; <string-name><surname>Bjaalie</surname>, <given-names>J. G.</given-names></string-name> (<year>2014</year>). <article-title>Waxholm space atlas of the sprague dawley rat brain</article-title>. <source>Neuroimage</source>, <volume>97</volume>, <fpage>374</fpage>–<lpage>386</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="book"><string-name><surname>Paxinos</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Franklin</surname>, <given-names>K. B.</given-names></string-name> (<year>2019</year>). <source>Paxinos and franklin’s the mouse brain in stereotaxic coordinates</source>. <publisher-name>Academic press</publisher-name>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="book"><string-name><surname>Peters</surname>, <given-names>A.</given-names></string-name> (<year>2022</year>). <source>petersaj/neuropixels_trajectory_explorer: First release</source> (Version v1.0.0). <publisher-name>Zenodo</publisher-name>. <pub-id pub-id-type="doi">10.5281/zenodo.7043460</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Qiu</surname>, <given-names>L. R.</given-names></string-name>, <string-name><surname>Fernandes</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Szulc-Lerch</surname>, <given-names>K. U.</given-names></string-name>, <string-name><surname>Dazai</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Nieman</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Turnbull</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Foster</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Palmert</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Lerch</surname>, <given-names>J. P.</given-names></string-name> (<year>2018</year>). <article-title>Mouse mri shows brain areas relatively larger in males emerge before those larger in females</article-title>. <source>Nature communications</source>, <volume>9</volume> (<issue>1</issue>), <fpage>2615</fpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>López</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>Y. A.</given-names></string-name>, <string-name><surname>Abramov</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ohayon</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Voigts</surname>, <given-names>J.</given-names></string-name> (<year>2017</year>). <article-title>Open ephys: An opensource, plugin-based platform for multichannel electrophysiology</article-title>. <source>Journal of neural engineering</source>, <volume>14</volume> (<issue>4</issue>), <fpage>045003</fpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Aydin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lebedeva</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Okun</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bauza</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Beau</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bhagat</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Böhm</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Broux</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal> (<year>2021</year>). <article-title>Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings</article-title>. <source>Science</source>, <volume>372</volume> (<issue>6539</issue>), <fpage>eabf4588</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Zatka-Haas</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> (<year>2019</year>). <article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title>. <source>Nature</source>, <volume>576</volume> (<issue>7786</issue>), <fpage>266</fpage>–<lpage>273</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Reddy</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> (<year>2019</year>). <article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title>. <source>Science</source>, <volume>364</volume> (<issue>6437</issue>), <fpage>eaav7893</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>S.-L.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Royall</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lesnar</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Graddis</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Naeemi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Facer</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ho</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal> (<year>2020</year>). <article-title>The allen mouse brain common coordinate framework: A 3d reference atlas</article-title>. <source>Cell</source>, <volume>181</volume> (<issue>4</issue>), <fpage>936</fpage>–<lpage>953</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="web"><string-name><surname>West</surname>, <given-names>S. J.</given-names></string-name> (<year>2021</year>). <source>Brainregister</source> (Version 0.9.0). <ext-link ext-link-type="uri" xlink:href="https://github.com/stevenjwest/brainregister">https://github.com/stevenjwest/brainregister</ext-link></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Young</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Fazel Darbandi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schwartz</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bonzell</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Yuruk</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Nojima</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gole</surname>, <given-names>L. C.</given-names></string-name>, <string-name><surname>Rubenstein</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Sanders</surname>, <given-names>S. J.</given-names></string-name> (<year>2021</year>). <article-title>Constructing and optimizing 3d atlases from 2d data with application to the developing mouse brain</article-title>. <source>Elife</source>, <volume>10</volume>, <fpage>e61408</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91662.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gallego</surname>
<given-names>Juan Alvaro</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Imperial College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>Birman et al. present a <bold>valuable</bold> software interface, Pinpoint, for planning anatomically precise insertions of rigid instruments (e.g., electrodes, injection needles/pipettes, fibre optic implants) into the mouse brain. The authors provide <bold>compelling</bold> evidence of the potential of this software since, it: (1) incorporates the geometrical constraints of the rig and instruments; (2) interfaces with popular manipulator systems and data acquisition software; (3) runs on any browser; and (4) allows for easy collaboration among users. Despite these exciting features, quantification of the gains in experimental efficiency and accuracy derived from Pinpoint is needed.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91662.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
Previously, researchers targeting certain brain areas in mice have relied on manual reconstruction of 3D trajectories based on published atlases of 2D sections in standardized anatomical planes. Over a decade ago, Leica's AngleTwo software provided an early proprietary software interface to rodent atlases based on 2D graphics. However, the more recent advent of open-source 3D gaming engines and CAD software (here the authors used Unity) and the adoption of a common 3D atlas framework (the Common Coordinate Framework, or CCF, from the Allen Institute) by the neuroscience community have enabled more advanced targeting based on 3D anatomy, as primate researchers and human clinicians have done previously with MRI data using bespoke and commercial software solutions. The Neuropixels Trajectory Explorer (<ext-link ext-link-type="uri" xlink:href="https://github.com/petersaj/neuropixels_trajectory_explorer">https://github.com/petersaj/neuropixels_trajectory_explorer</ext-link>, by Andy Peters) pioneered a software interface to the 3D mouse atlas for electrode insertions, and here Birman et al. have built on the aforementioned previous efforts to provide the most comprehensive trajectory planning software in mice to date, which they call Pinpoint. The most critical improvement lies in the ability to model the experimental rig and instruments in the same 3D environment as the atlas, since previously researchers needed to iteratively guess and check whether instruments physically fit with each other and the other constraints imposed by the rig. Other key features include coordinate transforms to map the CCF to more accurate in vivo anatomical data, as well as an API and hardware interface to commonly used micromanipulators.</p>
<p>Strengths:</p>
<p>
The feature set in Pinpoint makes it the best available software for planning instrument trajectories given geometrical constraints. Additionally, the documentation and open-source nature of the software should allow many extensions and improvements in the future, and as the authors note, it can also be used as a powerful teaching tool. Especially as researchers continue to push the boundaries of concurrent electrodes and optical fibers or other instruments within a single brain, this software will be of great use for neuroscience.</p>
<p>Weaknesses:</p>
<p>
Although Pinpoint enables instrument insertion planning with geometrical constraints for the first time and has many other novel features, it remains to be quantified how useful it is in terms of time/efficiency gains and accuracy of planned trajectories. For instance, although using a coordinate transform to MRI anatomical data is more accurate than the CCF alone in principle, users will need to verify how much this improved planning ability translates to time saved and/or improved trajectories as reconstructed from histology of dyed electrode tracks. The utility of the hardware interface for automating experiments versus the risk of damaging instruments with such an approach also remains to be quantified. Researchers using experimental subjects other than adult mice will have to wait for future integration of their atlases of choice, although the open-source nature of the project invites others to try adding this and other desired features themselves.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91662.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Pinpoint by Birman et al. serves not only as a probe trajectory planning tool but also offers a far richer suite of functionalities. It provides a simple and intuitive environment that users can learn within minutes and start planning trajectories for multiple probes based on the Allen mouse brain atlas. Pinpoint further includes two MRI-based transformations to better map the Allen atlas to live brains. It features a coefficient to adjust for different Bregma-Lambda distances and includes a mouse skull model to provide a better approximation of the craniotomy coordinates, rather than the coordinate of the point of insertion on the brain. It also offers tools to link the application to manipulator controllers to visualise the position of probes in the brain in real-time. Remarkably, most of these features are available right from the web browser, without the need to install anything or any coding knowledge.</p>
<p>The authors developed an open-source and well-documented software. Although I did not test it myself, it can communicate with the most common recording softwares (Open Ephys, SpikeGLX) and manipulators (New Scale, Sensapex) in the field. The current level of support by the developers on GitHub is reassuring, and I hope this continues as Pinpoint matures into a more stable and robust version.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91662.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
Birman and colleagues have introduced an invaluable tool designed specifically for electrophysiologists, simplifying the precise planning of trajectories for placing high-density probes within designated locations. Pinpoint offers users an interactive 3D environment within which they can explore electrophysiological trajectories within the anatomical context of the mouse brain. Within this environment, users can visualize the probe, target regions, and the constraints imposed by their experimental setup. Advanced users also have the flexibility to customize the entire Pinpoint scene to align with alternative coordinate systems and rig geometries. In cases involving multiple-probe recordings, Pinpoint shows 3D paths while issuing warnings about potential collisions. Additionally, Pinpoint can account for the individual variability in brain size among mice.</p>
<p>Strengths:</p>
<p>
Pinpoint provides real-time visualization of current brain region targets alongside neural data. Anatomical targeting information is accessible live during recordings. This is made possible through two sets of features: hardware that allows Pinpoint to communicate with micro-manipulators and software that broadcasts the current location of each recording channel to data acquisition software. Researchers can monitor the precise positioning of their probe during insertion and observe the anatomical locations of live electrophysiology data throughout an experiment, enabling them to make corrections if necessary.</p>
<p>Weaknesses:</p>
<p>
1. Pinpoint's novelty lies in its ability to be linked to data acquisition programs and electronic micro-manipulators. However, a similar program, Neuropixels Trajectory Explorer, was released before Pinpoint with comparable features. Please refer to <ext-link ext-link-type="uri" xlink:href="https://github.com/petersaj/neuropixels_trajectory_explorer">https://github.com/petersaj/neuropixels_trajectory_explorer</ext-link>. It would be beneficial to clarify the distinctions between these two applications and discuss on the necessity and advantages of creating Pinpoint.</p>
<p>2. Currently, in Pinpoint, users can only select one area of the mouse brain for probe placement and then use the controller to adjust the probe´s position if they wish to target multiple brain areas. This can complicate planning when inserting multiple probes. It would be advantageous to have the option to choose the specific areas the probes are to traverse, with Pinpoint automatically suggesting the most optimal trajectories while avoiding potential collisions. While this may require additional development, a comment on this possibility would be appreciated.</p>
</body>
</sub-article>
</article>