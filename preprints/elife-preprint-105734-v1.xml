<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105734</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105734</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105734.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Evidence for systematic - yet task- and motor-contingent - rhythmicity of auditory perceptual judgements</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0573-6378</contrib-id>
<name>
<surname>Fabio</surname>
<given-names>Cécile</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7362-5704</contrib-id>
<name>
<surname>Kayser</surname>
<given-names>Christoph</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>Christoph.kayser@uni-bielefeld.de</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02hpadn98</institution-id><institution>Department for Cognitive Neuroscience, Faculty of Biology, Bielefeld University</institution></institution-wrap>, <city>Bielefeld</city>, <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-05-07">
<day>07</day>
<month>05</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105734</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-01-09">
<day>09</day>
<month>01</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-12-17">
<day>17</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.12.17.628892"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Fabio &amp; Kayser</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Fabio &amp; Kayser</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105734-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Numerous studies advocate for a rhythmic mode of perception; however, the evidence in the context of auditory perception remains inconsistent. We propose that the divergent conclusions drawn from previous work stem from conceptual and methodological issues. These include ambiguous assumptions regarding the origin of perceptual rhythmicity, variations in listening tasks and attentional demands, differing analytical approaches, and the reliance on fixed participant samples for statistical testing. To systematically address these points, we conducted a series of experiments in which human participants performed auditory tasks involving monaural target sounds presented against binaural white noise backgrounds, while also recording eye movements. These experiments varied in whether stimuli were presented randomly or required motor initialization by the participant, the necessity of memory across trials and the manipulation of attentional demands across modalities. Our findings challenge the notion of universal rhythmicity in hearing, but support the existence of paradigm- and ear-specific fluctuations in perceptual sensitivity and response bias that emerge at multiple frequencies. Notably, the rhythmicity for sounds in the left and right ears appears to be largely independent among participants, and the strength of rhythmicity in behavioural data is linked to oculomotor activity and attentional requirements of the task. Overall, these results resolve conflicting conclusions drawn in previous work and provide specific avenues for further studies into the rhythmicity of auditory perception.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>hearing</kwd>
<kwd>listening</kwd>
<kwd>rhythmicity</kwd>
<kwd>perception</kwd>
<kwd>perceptual cycles</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Many studies over the past decades promote the concept of a ‘rhythmic mode’ of perception [<xref ref-type="bibr" rid="c1">1</xref>-<xref ref-type="bibr" rid="c3">3</xref>]. Following this, perception is an active process, shaped not only by external stimuli but also by intrinsic states of the sensory pathways and their interactions with the motor system [<xref ref-type="bibr" rid="c4">4</xref>-<xref ref-type="bibr" rid="c8">8</xref>]. This led to the idea of ‘perceptual cycles’, whereby stimuli occurring during specific moments in time are more likely to be perceived, or are perceived more accurately, compared to those during other moments [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>]. Supporting this concept in the context of hearing, psychoacoustic studies suggest that our ability to detect or discriminate sounds can vary periodically relative to a reference time point, such as the onset of an experimental trial or another sound [<xref ref-type="bibr" rid="c11">11</xref>-<xref ref-type="bibr" rid="c15">15</xref>]. For example, one study showed that pitch judgments for tones presented on a background noise vary at the scale of 6-8 Hz depending on their timing relative to this background [<xref ref-type="bibr" rid="c12">12</xref>]. Neuroimaging studies further support this concept by linking large-scale rhythmic activity to perceptual judgements [<xref ref-type="bibr" rid="c16">16</xref>-<xref ref-type="bibr" rid="c21">21</xref>], and in vivo recordings confirm the prominence of rhythmic changes in neural excitability and information transmission directly in the auditory pathways [<xref ref-type="bibr" rid="c22">22</xref>-<xref ref-type="bibr" rid="c25">25</xref>].</p>
<p>However, it has been difficult to obtain coherent evidence for perceptual cycles in hearing based on purely behavioural data. While some studies reported positive results, others failed to find such or reported only weak evidence for rhythmicity [<xref ref-type="bibr" rid="c11">11</xref>-<xref ref-type="bibr" rid="c15">15</xref>]. This mixed evidence may result from multiple aspects of the previous work as discussed in the following. To address this conundrum, we probed for rhythmicity in auditory perceptual judgments systematically across different tasks and analysis approaches. We here focus on the question of whether perceptual judgements exhibit rhythmicity in the absence of explicitly temporally entraining sounds, which is related by different from the question of whether rhythmicity persists following the presentation of sounds with explicit temporal structure [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c26">26</xref>].</p>
<p>Previous studies for example presented brief sounds presented during unstructured noise and showed that participants’ judgements vary with the delay between the onset of the noise and the target. Yet, the reported rhythmicity varies in time scale and significance [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>]. One reason may be the assumptions about where along the neural pathways’ rhythmicity emerges [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c29">29</xref>-<xref ref-type="bibr" rid="c31">31</xref>] (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). Some studies employed binaural targets and hence assumed that rhythmicity is either present at the same frequency in monaural pathways or arises after binaural convergence [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>]. In contrast, other studies directly tested monaural targets and reported effects at different frequencies for each ear [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>]. Yet both the robustness of these results and the precise origin of putative rhythmicity in listening remain unclear. In order to probe the ear-specificity of rhythmicity, we presented targets monaurally and analysed the data from individual ears, but also when collapsing across ears.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
    <caption><title>Experimental design and rationale.</title>
    <p><bold>A.</bold> Rhythmicity in behavioural data could have several origins. One the one extreme are rhythmic processes in monaural auditory processes that operate at different frequencies for sounds in each ear (left scheme). As a result, one would observe signatures of rhythmicity in behavioural data for monaural targets at different time scales, and a yet different pattern when presenting binaural stimuli (orange). On the other extreme are high-level processes possibility more tied to cognition than sensation, that operate in the same manner on any sound and which gives to the same rhythmicity in behavioural data following monaural or binaural stimuli (right scheme). <bold>B.</bold> Genera design of the four experiments. In each experiment independent white noise was presented to each ear over a period of 1.8 s. The task-relevant sounds were presented monaurally and at random time points between 0.3 and 1.5 s following the noise onset. Experiments 1,2,4 featured a tone discrimination task with a decision criterion fixed across trials (tones categorized as ‘low’ or ‘high’). Experiment 3 featured a within trial discrimination of two subsequent tones. Experiment 1 and 2 different in that sound presentation was automatically paced or required manual initialization by the participant. Experiment 1 and 4 differed in that the latter also required participants to perform a dual-task on in the visual fixation This was intended to divert attention across sensory modalities.</p></caption>
<graphic xlink:href="628892v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>A second reason may be the use of different listening tasks, that range from target detection to target discrimination or the continuous sampling of acoustic signals over time. To account for this, we compared two auditory tasks that differ in their requirements to contrast stimuli solely within or between trials. A third reason may result from differences in statistical sensitivity, which can be attributed to different analysis methods used to quantify rhythmicity and variations in the sample size in previous work [<xref ref-type="bibr" rid="c32">32</xref>-<xref ref-type="bibr" rid="c35">35</xref>]. To address this, we compared different approaches to quantify rhythmicity in the behavioural data. We ensured that these are comparable ‘in principle’, by calibrating their statistical specificity on simulated data. And in addition to testing the significance of effects within the specific participant sample, we probed whether putative effects generalize across participant samples using bootstrap simulations [<xref ref-type="bibr" rid="c36">36</xref>], thereby avoiding the use of a single threshold applied to one specific participant sample to determine the presence or absence of effects.</p>
<p>A fourth reason may relate to differential requirements for attention or motor-related processes in previous studies. It has been proposed that a rhythmic mode of listening is specifically engaged when attention is divided across the senses, while a strong focus on hearing may shift perception towards a continuous mode [<xref ref-type="bibr" rid="c37">37</xref>-<xref ref-type="bibr" rid="c39">39</xref>]. Based on this we contrast the same listening task when performed in isolation or in combination with a dual task diverting attention to the visual modality. Changes in attention may also directly reflect as changes in oculomotor behaviour and it is known that the oculomotor system is intricately connected with the auditory pathways [<xref ref-type="bibr" rid="c39">39</xref>-<xref ref-type="bibr" rid="c47">47</xref>]. More generally, it has been suggested that motor activity provides a scaffold for temporally organising perception, and hence it is conceivable that differences in motor commands contribute to shaping rhythmicity in behavioural data [<xref ref-type="bibr" rid="c6">6</xref>-<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c49">49</xref>]. We tested this by contrasting paradigms requiring, or devoid of, the explicit initialization of target presentation and by including eye-tracking data about oculomotor activity and pupil size in the analysis.</p>
<p>Motivated by previous work, we capitalised on a monaural pitch discrimination task and probed how different metrics of participants’ judgements (sensitivity, bias and reaction time) vary as a function of the delay between the onset of a background noise and the target tone. We implemented four variations of this paradigm, each testing more than 25 participants (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). One experiment tested the original paradigm by Ho et al. [<xref ref-type="bibr" rid="c12">12</xref>], and featured automatically paced trials asking participants to categorise the pitch of a target tone as ‘low’ or ‘high’ (Experiment 1). A variation of this experiment provided participants with explicit knowledge about the timing of individual trials, by requiring them to manually initialise stimulus presentation for each trial (Experiment 2). In a third experiment we implemented a within-trial pitch discrimination task (Experiment 3), which does not require the implicit comparison of a target stimulus to an absolute reference that is maintained across trials (as is the case in experiment 1). Given that a between-trial discrimination task requires memory of a stimulus boundary across trials, it is conceivable that the presence or absence of such memory and related top-down processes may affect rhythmicity in behaviour. And finally, we combined the auditory task with a dual-task diverting attention to a visual fixation point, in which we also collected eye tracking data (Experiment 4).</p>
<p>In the following we present a number of experiments, different analytical approaches for quantifying rhythmicity and probing for rhythmicity in different behavioural metrics. Given that the individual statistical outcomes (i.e. significances) have to be taken with care, we base our interpretations on the prevalence of effects among random variations in the participant sample both within and between experiments.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Participants and sample size</title>
<p>We collected data from four experiments in which adult volunteers participated after providing informed consent. All participants had self-reported normal vision and hearing and none indicated a history of neurological disorders. Data collection was anonymous and it is possible that some individuals participated in more than one of the experiments. Participants were compensated for their time and the procedures were approved by the ethics committee of Bielefeld University. We set the a priori sample size for each experiment to n=25. Due to parallel data collection the actual sample sizes for experiments 1-3 were slightly higher. For experiment 4 we collected more data, in part due to technical problems with also collecting eye tracking data (see below).</p>
</sec>
<sec id="s2b">
<title>General procedures</title>
<p>The experiments were performed in a darkened and sound-proof booth (E: Box; Desone, Germany). Participants sat in front of a computer monitor (27” monitor; ASUS PG279Q, about 1m from participant’s head) on which visual stimuli were presented. Acoustic stimuli were presented over head phones (Sennheiser DH200Pro). Stimulus presentation was controlled using the Psychophysics Toolbox (Version 3.0.14) using MATLAB (Version R2017a; The MathWorks, Inc., Natick, MA). Participants responded using a computer keyboard. The loudness of the acoustic stimuli was calibrated using a sound level metre (Model 2250 Bruel &amp; Kjær, Denmark).</p>
</sec>
<sec id="s2c">
<title>Experimental paradigms</title>
<p>The experimental paradigms were modelled based on previous studies and involved the discrimination of monaurally presented target sounds presented among binaural white noise backgrounds (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). Targets were presented at different delays relative to the onset of the background in order to probe the influence of this delay on behavioural performance. The individual experiments differed in that the sensory information necessary to perform the task had either to be maintained between trials (discrimination of tones as ‘high’ or ‘low’; experiments 1,2,4) or that the discrimination was based on the relative difference of two stimuli presented within a given trial (experiment 3). We also varied whether stimuli were presented at an automatic pace or whether stimulus presentation was initialised by the participant (experiment 1 vs experiment 2), and we compared the same experiment when participants focused solely on the auditory task or performed a dual task (experiment 1 vs experiment 4). Participants were instructed to respond as ‘fast and accurately’ as possible.</p>
<p>Experiment 1 was based on the study by Ho et al. [<xref ref-type="bibr" rid="c12">12</xref>] and required the categorization of a target tone as either ‘high’ or ‘low’. Target tones lasted 40 ms (with 5 ms cosine-ramps) and had frequencies of 2048 Hz and 1024 Hz. Tones were presented in one ear, at an intensity that was adjusted for each participant and ear to achieve a comparable performance (percent correct responses) between ears and tone frequencies. Target tones were presented at random delays between 300 ms and 1500 ms from the onset of the background noise (sampled uniformly). Background noises were generated on each trial independently for each ear and had an r.m.s level of 65 dB SPL. The target intensity was determined for each participant prior to the experiment using one 3-down 1-up staircase per ear and frequency (with these 4 staircases presented in an interleaved manner). Thresholds were obtained from the last five reversals. During the actual experiment tone intensities were updated to maintain performance between 69% and 80% correct responses (determined over the preceding 30 trials) by adjusting the threshold by an amount of 4% of the current value. Each participant performed 8 blocks of 190 trials in one session, resulting in 380 trials per frequency and ear. Each of these 380 trials presented the target at a different (uniformly sampled) delay from background onset. Inter-trial intervals lasted between 1100 ms and 1900 ms (uniform) and the start of each trial was indicated by the appearance of a central fixation dot, with the background noise starting 300 ms to 500 ms (uniform) after the appearance of the dot. The presentation of the background noise was stopped once participants responded, or latest maximally 1800 ms. For this experiment we collected data from 27 participants. The response keys (left and right arrow keys) for high and low responses were counterbalanced across participants.</p>
<p>Experiment 2 was similar to experiment 1 except that participants initialised the sound presentation for each trial by pressing a button on the keyboard. Trials started with the presentation of a fixation dot, subsequently to which participants had to press any button to continue the trial. The presentation of the background noise started immediately after registering this button press. For this experiment we collected data from 26 participants.</p>
<p>Experiment 3 consisted of a within-trial pitch discrimination task modelled based on previous work [<xref ref-type="bibr" rid="c50">50</xref>]. Background noises were as in experiment 1 but target stimuli consisted of two 30 ms tones separated by 40 ms and presented at 60 dB SPL(I). Participants’ task was to determine which tone had higher pitch (‘first’ or ‘second’). Task difficulty was set for each ear and participant by adjusting the frequency difference of the tones (keeping the reference fixed at 1024 Hz). This frequency difference was determined prior to the main experiment using two 3-down 1-up staircase for each ear; it was maintained during the experiment by updating this difference to maintain performance between 69% and 80% correct responses (determined over the last 30 trials) by adjusting the difference by an amount of 4% of the current value. Inter-trial intervals, fixation periods and the sampling of delays between background onset and target tones (here defined as delay to the start of the first target tone) were as in experiment 1. Other than in experiment 1 response keys were fixed for each participant (left arrow corresponding to 1<sup>st</sup> tone being higher, right arrow to 2<sup>nd</sup> tone). Participants performed 8 blocks of 190 trials and we collected data from 26 participants.</p>
<p>Experiment 4 was based on experiment 1 but was designed to probe the role of divided attention. For half the blocks the design was identical to experiment 1, but the other half required participants to perform the auditory task and a dual-task on a central fixation dot. During half the trials the intensity of this dot changed at a random time and participants had to report whether they perceived this change. This judgement was made subsequent to the auditory judgement. The intensity change was implemented by either increasing or decreasing the RGB values at a random interval between 100 ms from to background onset to the target (initial values [200, 80, 80], with an in- or decrement of 50). The responses to this dual task were made using an orthogonal axis to that used for the auditory task (using the up and down arrows for ‘yes’ and ‘no’ responses, counterbalanced across participants). Each participant performed 16 blocks comprising 130 trials, split over two sessions taking place on different days. In each session 4 blocks involved the dual task, with counterbalanced order across the two sessions. This resulted in 260 trials for each tone frequency, ear and task design (dual task, no dual task), with each trial probing a different delay between background onset and target. In addition to probing the role of the dual task we also recorded eye movements in experiment 4. Because of technical difficulties in obtaining stable eye movement recordings in some participants and sessions (e.g. resulting from reflections on lenses etc.) we collected data from a total of 36 participants.</p>
<p>Eye tracking data were recorded from the left eye using an EyeLink 1000 plus eye-tracking system (SR Research) with sampling rate at 500 Hz (for 18 participants) or 2000 Hz (17 participants). Eye-tracking calibration was performed at the beginning of each block using a 9-point grid. The parameters for saccade detection in the EyeLink system (“cognitive” setting) were a velocity threshold of 30°/s and an acceleration threshold of 8000°/s. Given that we here used the eye tracking to characterise fixation stability and pupil size we combined data obtained with both sampling rates.</p>
</sec>
<sec id="s2d">
<title>Data preparation</title>
<p>Outliers in the behavioural data were determined as trials with very short (&lt; 150 ms) or long reaction times (&gt; 2.5 sec). We also removed (very rare) trials on which participants pressed a button not assigned to the task. Effectively we retained 1424±17 (mean±s.e.m.) for experiment 1, 1435±11 for experiment 2, 1490±8 for experiment 3 and 1865±23 for experiment 4. For subsequent analysis we transformed reaction times using the square-root transform.</p>
<p>For the analysis of the eye tracking data we proceeded as follows. We determined blinks and periods containing noisy data (usually arising when participants’ eye was directed outside an ±14° window on either horizontal or vertical axis). We then epoched the data in an interval of -0.4 s to +1.5 s around background onset and retained only trials not involving any of these artefacts at any time point in this epoch. We then retained only those participants with at least 600 trials with good behavioural and eye tracking data. This was the case for 30 participants (with 1216±67 trials on average). The eye tracking data for these are shown in <xref rid="fig7" ref-type="fig">Figure 7</xref>. To link the eye data to the rhythmicity in behaviour (see below) we split the trials by either the pupil size or the stability of fixation during each trial. Pupil data were z-scored within each participant over all available epochs. We then calculated for each trial the pupil size as the average size in the interval between the onset of the background noise to the target. Fixation stability was determined as the arithmetic mean of the standard deviations of the eye position along the horizontal and vertical axes in this time interval.</p>
</sec>
<sec id="s2e">
<title>Overall analysis strategy</title>
<p>The main focus of this study was to probe for signatures of rhythmicity in the behavioural data as a function of the delay between the time of target presentation relative to the onset of the background noise. This was probed using the data of individual participants, once by combining trials regardless of the ear on which the target was presented and separately for trials in each ear. This main question was probed using the data from experiments 1-3, while the data from experiment 4 were used to probe the impact of dual task and the eye properties (see ‘Analysis of an influence of dual task and eye metrics’).</p>
<p>Given that previous studies differ in how they obtained statistical evidence for rhythmicity in behavioural data and in the metric that was analysed (e.g. response accuracy, reaction time, or sensitivity and bias derived from signal detection theory) we aimed for a comprehensive approach that covered multiple metrics and analyses approaches. In particular, we implemented one analysis line in which the delay was binned into a number of equally-spaced bins. Such binning by the variable of interest allows the calculation of behavioural metrics that are only defined across multiple trials (e.g. sensitivity and bias obtained using signal detection theory). However, binning data by the variable of interest can also distort and lead to spurious effects [<xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c52">52</xref>]. To avoid such pitfalls and to exploit the full potential of single trial data we implemented a separate approach that focused on response accuracy and reaction times of individual trials.</p>
<p>To quantify the strength of putative rhythmicity in the data we relied on two statistical approaches. In one we calculated the frequency spectrum of the (delay-binned) data and compared this spectrum to a surrogate distribution. In a second approach we characterised the data using linear models to separate rhythmic components from non-rhythmic ones.</p>
<p>Given that it is difficult to a priori arbitrate between the different analysis approaches, and given the debate in the literature, we opted for a comprehensive study using each of these. This allowed us to characterise the data using metrics from signal detection theory by combining data within delay-bins but to also probe for rhythmicity in the single trial data. Before explaining the approaches in detail, we summarise the approaches briefly:
<list list-type="order">
<list-item><p>Based on delay-binned data we probed rhythmicity in sensitivity, bias and reaction times using the respective frequency spectra, contrasting the actual data with surrogate data obtained using auto-regressive models (termed Spectral approach in the following).</p></list-item>
<list-item><p>Based on delay-binned data we probed rhythmicity in sensitivity, bias and reaction times using linear models separating rhythmic from non-rhythmic predictors. We contrasted the vector strength of the rhythmic predictor with surrogate data obtained using a shuffling procedure (termed Binned approach in the following)</p></list-item>
<list-item><p>Based on the single trial data we probed rhythmicity in response accuracy and reaction times using linear models separating rhythmic from non-rhythmic predictors. We contrasted the vector strength of the rhythmic predictor with surrogate data obtained using a shuffling procedure (termed Trial-based approach).</p></list-item>
</list>
Each approach probes for rhythmicity under the assumption that for a given experiment and analysis the sample of participants exhibits rhythmicity at the same frequency. However, they do not assume that each participant exhibits rhythmicity at the same phase.</p>
<p>For each approach we tested ‘whether there is rhythmicity at any of the tested frequencies. We opted for this approach as previous studies have reported rhythmicity at frequencies ranging from the delta to the alpha band and hence no unique and frequency-specific hypothesis can easily be derived from the previous literature. Given that this involves multiple tests across frequencies, and given that each of the approaches may have a different statistical sensitivity and false positive rate, we relied on simulations to calibrate the false positive rate between approaches (see ‘Simulations’). This also allowed us to determine their sensitivity on simulated data. To draw any inference from these analyses, we investigated both the probability of observing significant effects in the specific participant samples recruited, corresponding to a classical frequentist approach testing for effects on a fixed sample of participants (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). However, each collected participant sample is only of many potential samples that could be derived from the entire population [<xref ref-type="bibr" rid="c36">36</xref>]. Hence, we also explored the within-sample variability of the associated effects using bootstrapping to derive the prevalence of significant effects across variations in the participants (<xref rid="fig5" ref-type="fig">Fig. 5</xref>; see ‘Bootstrapping to determine the within-sample variability’).</p>
</sec>
<sec id="s2f">
<title>Spectral analysis of time binned data</title>
<p>As a first step we binned the data by the effective delay between background onset and target. We relied on equally-spaced bins of 60 ms duration, resulting in 20 bins covering delays from 0 ms to 1200 ms. Based on the trials in each bin we computed sensitivity and bias using signal detection theory and the average (square-root transformed) reaction times. We then computed the spectra of each metric after linearly detrending the data and zero-padding by 30 points on either size. Based on these parameters, frequency spectra were computed at effective frequencies between 1.05Hz and 8.1 Hz at steps of about 0.2 Hz (resulting in similar frequencies as used for the other approaches below).</p>
<p>We then compared the group-averaged spectra to a distribution of surrogate spectra following suggestions in the literature [<xref ref-type="bibr" rid="c34">34</xref>]. These were obtained for each participant and metric by generating a distribution of 10’000 spectra based on auto-regressive processes of order 1. The parameters estimate for the respective model and simulations were implemented using the ARfit toolbox in Matlab (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/174-arfit">https://www.mathworks.com/matlabcentral/fileexchange/174-arfit</ext-link>). During AR-model prediction the first 2000 samples were each ignored. The surrogate spectra were averaged over participants resulting in a distribution of surrogate data based on which the p-value of the actual group spectrum was computed for each frequency. These first-level p-values were then thresholded at an appropriate level to achieve a desired false positive rate across analysis approaches (see ‘Simulations’).</p>
</sec>
<sec id="s2g">
<title>Linear model-based analysis of time binned data</title>
<p>We computed sensitivity, bias and reaction time for the delay-binned data as described above. We then modelled the data using linear models comprising both rhythmic and non-rhythmic predictors as implemented previously [<xref ref-type="bibr" rid="c28">28</xref>]. Effectively, we described the data using the following terms in a linear model assuming normally distributed data and an identical link function: an offset, a linear influence of delay, a u/v shaped influence reflecting changes in behavioural data tied to the duration of the overall period of target uncertainty (here using a frequency of 0.5Hz), and a rhythmic predictor consisting of the sine and cosine component at a specific frequency. For each participant we then derived the vector strength of the rhythmic predictor, defined as the arithmetic mean of the squared betas for the sine and cosine components. This vector strength reflects the overall prominence of the rhythmic predictor.</p>
<p>For statistical testing we contrasted the group-average vector strength to a surrogate distribution obtained under the null hypothesis of no relation between behavioural data and delay. This was implemented for each participant by shuffling this assignment 10’000 times and calculating the respective model. The resulting vector strengths were averaged across participants resulting in a distribution of surrogate values based on which the p-value of the vector strength was computed. Separate models were fit for rhythmic predictors at frequencies between 1.2Hz and 8 Hz, with steps of 0.1Hz between 1.2 and 4Hz and steps of 0.2 Hz above. Again, these first-level p-values were thresholded at an appropriate level to achieve a desired false positive rate.</p>
<p>We note that the spectral- and the model-based approach are related but also distinct. Computing the frequency spectrum effectively describes the data as a superposition of distinct but simultaneously present rhythmic components, while the model-based approach tests one rhythmic predictor at a time. The spectral approach provides a more accurate depiction of the data if rhythmicity exists at several frequencies, as each predictor is considered simultaneously. However, due to spectral blurring the statistical power at individual frequencies may also be diluted.</p>
</sec>
<sec id="s2h">
<title>Linear model-based on single trial data</title>
<p>We computed similar linear models as described above, but applied these to the single trial accuracy and the reaction times as metrics. For reaction times we relied on a linear model assuming normally distributed data and a linear link function, for accuracy we relied on a binomial model and a logistic link function. As for the binned data, we compared the actual group-averaged vector strength at individual frequencies to surrogate data. As the single trial data effectively allow a higher temporal resolution compared to the binned data, we here tested frequencies between 1.2Hz and 12 Hz (with steps of 0.8 Hz between 8 and 12 Hz).</p>
</sec>
<sec id="s2i">
<title>Visualisation of statistical results</title>
<p>To visualise the evidence for a rhythmic effect for the specific participant sample we show the associated (log-transformed) first level p-values (<xref rid="fig4" ref-type="fig">Figure 4</xref>). Although this measure of statistical significance does not reflect a measure of the underlying effect size (e.g. spectral power), it allows presenting the results on a scale that can be directly compared between analysis approaches, metrics, frequencies and analyses focusing on individual ears or the combined data. Each approach has a different statistical sensitivity, and the underlying effect sizes (e.g. spectral power) vary with frequency for both the actual data and null distribution. As a result, the effect size reaching statistical significance varies with frequency, metrics and analyses. By showing p-values we overcome this variability and present the data on a scale where the cut-offs for significance are the same across all dimensions (c.f. lines in <xref rid="fig4" ref-type="fig">Fig. 4</xref>).</p>
</sec>
<sec id="s2j">
<title>Calibrating analysis approaches on simulated data</title>
<p>Each approach may differ in the effective sensitivity and specificity of detecting rhythmicity [<xref ref-type="bibr" rid="c32">32</xref>-<xref ref-type="bibr" rid="c34">34</xref>]. We hence based the interpretation not on the first-level p-values obtained by the individual comparisons of actual vs. surrogate data. Rather we simulated data with genuine rhythmicity and data without and calculated sensitivity and specificity of each approach at different signal to noise ratios (SNR’s). We then selected those first-level p-values that produce a false positive rate (specificity) of 0.05 when detecting a rhythmic effect at any frequencies in the simulated data (i.e. correcting for multiple tests along the frequency axis).</p>
<p>Practically, we simulated data for a sample of 25 participants, with 700 trials each. This corresponds to a sample size and trial count similar to the actual data, matching the analysis of individual ears. We simulated normally distributed data that was generated using a linear model (similar to that used during data analysis): we generated data using a superposition of an offset, a linear slope, a u/v shaped term and both sine and cosine predictors for the rhythmic component. For each parameter setting (see below) we simulated 1’000 samples of participants (i.e. virtual experiments). In each simulation we drew the single trial betas for the model generating the data from Gaussian distributions with predefined means and standard deviations independently across participants and simulations. For simulations with a rhythmic effect the parameters were as follows: offset (1, 0.1; mean, SD of the Gaussian distribution used to draw the single participant values), linear slope (0.1, 0.1), u/V term (0.1, 0.1), sine at 4Hz (0.2, 0.1), cosine at 4Hz (0.2, 0.1). For simulations without rhythmic effect the sine and cosine terms were zeroed. The simulated data were analysed in the same manner as the actual data, resulting in 1’000 samples of first-level p-values for each analysis approach.</p>
<p>We implemented simulations using different signal to noise ratios (SNR). These were achieved by adding Gaussian noise to the simulated data, with mean zero but varying SDs (taking values from 2 to 8 for simulations with a rhythmic effect; and values of 2,4 and 6 for simulations without). We then calculated the fraction of simulations in which a significant effect was detected at any frequency using different first-level p-values as cut-offs. For runs with a simulated rhythmic effect this yields the statistical sensitivity and for runs without such the statistical specificity. For each approach we selected those thresholds for the first-level significance yielding an approximate false positive rate of 0.05 (and separately also for 0.01) across all 3’000 runs without effect. These cut-offs are shown together with the actual data in <xref rid="fig4" ref-type="fig">Figure 4</xref>. <xref rid="fig2" ref-type="fig">Figure 2</xref> shows the resulting true and false positive rates on simulated data when using a cut-off of p∼0.05.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
    <caption><title>Calibration of analysis approaches on simulated data.</title>
    <p>We simulated data with and without rhythmicity to calibrate the statistical specificity (false positive rate) of each approach. The approaches are based on the spectra derived from delay-binned data (‘Spectra’), the vector strength of a rhythmic component in linear models applied to delay-binned data (‘Binned’) and the vector strength of a rhythmic component in linear models applied to single trial data (‘Trials’). For each we determined the respective first-level threshold (i.e. method specific p-value) that results in a false-positive rate of about 0.05 when probing for rhythmicity at any frequency. <bold>A.</bold> Sensitivity of each approach in detecting a rhythmic effect for data generated with a rhythmic effect and different signal to noise ratios (SNR). <bold>B.</bold> Specificity, shown here as false positive rate in detecting a rhythmic effect in data generated without such an effect (calibrated to about 0.05). <bold>C.</bold> Illustration of the (log-transformed) first-level p-values for simulated data with an effect at 4Hz and different SNRs, showing the frequency specificity of each approach in detecting an effect.</p></caption>
<graphic xlink:href="628892v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2k">
<title>Bootstrapping to determine the within-sample variability</title>
<p>In addition to presenting the significance of effects for the collected participant sample, we explored the prevalence of effects among random variations in this sample [<xref ref-type="bibr" rid="c36">36</xref>]. We used bootstrapping to repeatedly draw participant samples from the entire sample (e.g. for experiment 1 we sampled 26 participants at random with replacement from the pool of 26 participants collected). We then determined the percentage of simulations that yield a significant effect at each frequency. These estimates of effect-prevalence are shown in <xref rid="fig5" ref-type="fig">Figure 5</xref> and provide an estimate of the reproducibility of effects across random variations in the participant sample. This approach to investigating the prevalence of a specific (significant) effect in the participant sample has the advantage that it avoids potentially biased conclusions drawn from one single sample of participants and provides a quantification of how consistent an effect is within a population of participants.</p>
</sec>
<sec id="s2l">
<title>Analysis of an influence of dual task and eye metrics</title>
<p>We used the data of experiment 4 to probe the influence of a dual-task on signatures of rhythmicity. For this we contrasted the data from blocks with the dual task to those without dual task. We also used the data from experiment 4 to probe whether signatures of rhythmicity are related to pupil size as a measure of task-engagement [<xref ref-type="bibr" rid="c53">53</xref>, <xref ref-type="bibr" rid="c54">54</xref>] or the overall mobility of eyes during the trial.</p>
<p>In contrast to the analyses of the data from experiments 1-3, in which we probed the existence of signatures of rhythmicity against a suitable null distribution, we here followed a different logic. The analyses for experiment 4 are based on contrasting two halves of the data, effectively comparing two equivalent signatures for rhythmicity within participants. To derive such a signature of rhythmicity we relied on the linear model-based analysis of the delay-binned data. For a given set of trials, we derived the group-averaged vector strength of the rhythmic predictor as a function of frequency. These group-level vector strengths were then contrasted between conditions (e.g. trials with or without dual task) using paired t-tests, the p-values of which were corrected for multiple tests across frequencies using the Benjamini &amp; Hochberg procedure (<xref rid="fig7" ref-type="fig">Figure 7</xref>). As for the analysis of experiments 1-3, we looked at both the significance for the specific available participant sample, and tested for the generalization of effects among participant samples using bootstrapping. For the latter we again repeated the analysis using randomly sampled participants, and counted the number of participant samples yielding a significant effect at a specific frequency.</p>
<p>For the data split by dual task we contrasted the vector strength obtained using the trials from the respective blocks (n=37 participants). For the effect of pupil size and eye mobility, we implemented median splits on the respective variables within each participant. Given that the available number of trials with good eye tracking data varied across participants, and given that this data split further reduces the effective number of trials per condition of interest, we implemented this analysis only for those n=24 participants that had 400 trials per split (average number of trials per split 674±29).</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Illustration of the data for experiments 1-3</title>
<p>The experiments involved auditory discrimination tasks performed on monaural target sounds presented at different delays relative to the onset of a binaural white noise background. <xref rid="fig3" ref-type="fig">Figure 3</xref> illustrates the main aspects of this data. Panel A shows the overall sensitivity and bias for targets presented to each ear and reaction times for individual ears and targets. Panel B shows the behavioural metrics as a function of the delay using binned data. This illustrates temporal structure in behaviour, such as decreasing sensitivity or reaction times for targets presented late in the trial. This is in line with previous studies reporting temporal structure in behavioural data at multiple time scales, which is often observed in perceptual decision-making paradigms and may reflect individual strategies for analysing the sensory environment, leakage in decision processes, or the urgency to respond [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c56">56</xref>]. Note that this visualisation of the data implicitly assumes that all participants exhibit rhythmicity (if they do) at the same frequency and phase. However, the assumption about the same phase may not be warranted and is not a requirement in the following statistical analyses.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
    <caption><title>Illustration of the data for experiments 1-3.</title>
    <p><bold>A.</bold> Sensitivity (d-prime), bias and reaction times (rt) regardless of target delay. These metrics are shown separately for each ear (R, L) and reaction times (in seconds) are shown separately for each stimulus condition (f1, f2; corresponding to the two target frequencies in experiments 1&amp;2 or the two orders of pitch in experiment 3). <bold>B.</bold> The same metrics for trials with targets presented at specific delays within the 1.2 s range of target uncertainty. Grey dots and lines denote the individual participant data, thick dots and error-bars denote the group average and standard deviation.</p></caption>
<graphic xlink:href="628892v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3b">
<title>Statistical evidence for rhythmicity in experiments 1-3</title>
<p>We report evidence for rhythmicity in the behavioural data in two ways: first as significance of effects in the specific sample of the collected data and then as the prevalence of effects when generalizing over variations of participants drawn from this sample.</p>
<p><xref rid="fig4" ref-type="fig">Figure 4</xref> shows the p-values for a rhythmic effect (vs. a suitable null distribution) for each behavioural metric and the three approaches. We determined for each experiment and metric whether there was a significant effect in any of the analysis approaches (at p&lt;0.05, corrected for multiple tests across frequencies; indicated by the grey lines). For experiment 1 this included an effect for d-prime around 2 Hz for the right ear, for experiment 2 an effect for bias around 6.5 Hz for the left ear, and for experiment 3 an effect in d-prime around 3 Hz for the combined-ear data. These effects are highlighted across approaches (shading in <xref rid="fig4" ref-type="fig">Fig. 4</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
    <caption><title>Significance of rhythmicity in the behavioural data.</title>
    <p>The individual panels show the group-level (first-level) p-values for each approach (panels <bold>A-C</bold>) and experiment together with the statistical cut-off used to determine significance (thick grey line; corresponding to p&lt;0.05 corrected for multiple tests across frequencies, calibrating false-positive rate across analyses). For comparison the dashed grey line also shows the cut-off at p&lt;0.01. The coloured shadings indicate significant effects to facilitate their comparison across panels. The precise frequencies with significant effects were: Exp1: Binned/d-prime: 1.8-2.0 Hz; Exp 2: Binned/bias 6.4-6.6Hz; Exp3: Binned/d-prime 2.4-2.8 Hz.</p></caption>
<graphic xlink:href="628892v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To probe whether and which effects generalise across random variations in the participant sample we used bootstrapping (<xref rid="fig5" ref-type="fig">Figure 5</xref>). This revealed that the above-mentioned effects prevail for at least 50% of the simulated experiments, corroborating their robustness within the participant sample. However, the prevalence data also provide critical insights beyond those obtained from the significance in <xref rid="fig4" ref-type="fig">Figure 4</xref> and suggest that conclusions drawn from this significance-testing approach may be misleading.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
    <caption><title>Prevalence of significant effects across random variations in the participant sample.</title>
    <p>Using bootstrapping we determined the percentage of participant samples that yield a significant effect when repeating the analysis. For each simulated participant sample, we determined significant effects at p&lt;0.05 (as in <xref rid="fig4" ref-type="fig">Fig. 4</xref>) and then counted the number of simulations with effects at each frequency. A value of above e.g. 50% (dashed line) indicates that when randomly sampling from within the collected participants more than 50% of such simulated experiments yield a significant effect. The coloured shadings highlight the same effects as shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>.</p></caption>
<graphic xlink:href="628892v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For experiment 1 sensitivity is modulated in the right ear at around 2 Hz and the left ear around 6.5 Hz with comparable prevalence in multiple approaches (Spectra: 45% vs. 38%; Binned: 35% vs. 55%). Furthermore, the prevalence of an effect for bias is also considerable (e.g. Spectra 43%). Collectively, this suggests that in fact both, perceptual sensitivity and bias, may be modulated and in both ears, but at different frequencies. For experiment 2, the effect in bias for the left ear around 6.5 Hz does not seem to be accompanied by a corresponding effect in the right or the combined-ear data at the same frequency (prevalence below 10%). However, there is considerable prevalence of an effect in sensitivity for the right ear around 2Hz (Spectra 38%), suggesting that also in this experiment both ears may exhibit rhythmicity, again possibly at distinct frequencies. For experiment 3, all three approaches reveal the prevalence of rhythmicity in sensitivity around 2-3 Hz for the combined-ear data (Spectra 44%, Binned 78% and Trials 49%) and to a smaller degree also in the left ear (Spectra 40%, Binned 34%) without direct counterpart for the right ear at the same frequency. Finally, especially the spectral approach also suggests a considerable prevalence of effects in reaction times. Hence, based on this analysis of the data we conclude that multiple behavioural metrics reveal a largely comparable prevalence of rhythmicity across experiments and ears, and that the precise frequencies and prevalence vary between approaches and experiments.</p>
</sec>
<sec id="s3c">
<title>Evidence for the ear-specificity of rhythmicity</title>
<p>To determine whether rhythmicity indeed prevails at distinct frequencies for the two ears consistently across experiments, we averaged the prevalence data across experiments 1-3 (<xref rid="fig6" ref-type="fig">Fig. 6</xref>). We restricted this analysis to sensitivity and bias and the two approaches showing the most prevalent effects above (Spectra, Binned).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
    <caption><title>Averaged prevalence across experiments 1-3.</title>
    <p>We focus on the two approaches and metrics yielding the strongest effects in the data for individual experiments. To allow better visibility of the differences between individual ears, the combined-ear data are shown dashed.</p></caption>
<graphic xlink:href="628892v1_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>This summary corroborates that the two ears indeed tend to exhibit the highest prevalence of effects at complementary frequencies. This is seen both for sensitivity and bias and in both approaches. In particular, peaks for sensitivity in the right ear prevail around 2 Hz and 4 Hz, and for the left ear around 3 Hz and 6 Hz. A similar picture emerges for bias, though these results differ more between analysis approaches. Given that the prevalence data reflect the likelihood of observing a significant effect for a given random sample of participants, it is possible that for a specific sample one may observe only one of these spectral peaks. This may be one reason for the diversity of frequencies and effects reported in the literature, given that most studies focused on the effects that exist in a specific and unique participant sample.</p>
<p>The data also leave the possibility that the rhythmic effects in the left and right ears emerge in distinct groups of participants and may not be strictly linked. To probe this, we correlated the occurrence of significant effects in the left and right ears across the 5000 bootstrapped samples. Specifically, we coded the presence of significant effects in the right ear around 1.5 – 2Hz and of effects in the left ear around 5.5 – 6 Hz for the data of experiment 1 as binary variables. The correlations of these co-occurrences of effects in the two ears were minimal (Spectra: r=-0.01, p=0.46, Binned: r=0.018, p=0.20, n=5000), suggesting that any rhythmicity for left and right ears may not emerge simultaneously but in part prevails in separate groups of participants.</p>
</sec>
<sec id="s3d">
<title>Role of arousal and eye mobility</title>
<p>Experiment 4 was designed to test additional properties of putative rhythmicity in behavioural data. First, the experiment contrasted blocks featuring a pure auditory task with blocks including the diversion of attention by a dual visual task. Second, we measured eye movements to probe whether the presence of rhythmicity is linked to arousal (indexed using pupil dilation) or to the overall mobility of the oculomotor system (indexed using fixation stability).</p>
<p>Participants performed the dual task well. Their sensitivity to the intensity changes of the fixation dot was high (d-prime, 3.96±0.12, mean±s.e.m., n= 36) and they exhibited no obvious bias in that judgement (bias 0.08±0.04). Reaction times (with dual task 1.16+0.01 vs. without 1.04+0.01, t=11.3, p&lt;10-10) and participants bias towards one judgement differed significantly between blocks with and without the dual task (0.07+0.05 vs. -0.03+0.04, t=3.0, p=0.008), while sensitivity to the tone frequencies did not (d-prime, 1.52+0.06 vs. 1.47+0.06, t=1.2, p=0.21). To test for an effect of dual task on the strength of rhythm city in behaviour we contrasted these between blocks with and without dual task. <xref rid="fig7" ref-type="fig">Figure 7A</xref> shows the result for the combined ear data: the left panel shows the difference in vector strength between blocks with and without dual task for the specific participant sample, the right panels show the prevalence of significant effects for individual ears and the combined-ear data derived using bootstrapping. The direct statistical comparison between conditions revealed no significant difference (at p&lt;0.05 for the combined-ear data; paired t-tests, corrected for multiple tests across frequencies) and the prevalence of significant effects in variations of the participant sample was low for the unilateral data. However, for the sensitivity in the combined ear data the prevalence of significant effects was nearly 40% around 4 Hz and 8 Hz, suggesting that at these frequencies a subgroup of participants tends to exhibit more rhythmicity when performing the dual task.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
    <caption><title>Influence of dual task and eye metrics on rhythmicity in experiment 4.</title>
    <p>Evidence for rhythmicity was obtained as the group-average vector strength in the model-based analysis of delay-binned data. This evidence was compared between conditions, with the graphs showing the difference (mean, s.e.m.). <bold>A.</bold> Comparison of blocks with minus dual task those without (n=36). <bold>B.</bold> Comparison of trials with large pupil diameter minus trials with small diameter (n=24). <bold>C.</bold> Comparison of trials with less fixation stability (more eye mobility) minus trials with more stability (less eye mobility, n=24). The left panels show the group-level difference (mean, s.e.m.) across the collected participant sample for the combined-ear data. Dots indicate significant effects (paired t-tests, corrected for multiple tests across frequencies at p&lt;0.05; for d-prime in panel C these are 3.4-3.5 Hz). The right panels show the prevalence of effects across random variations in the participant sample for both the combined-ear and individual data.</p></caption>
<graphic xlink:href="628892v1_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The eye tracking data are illustrated in <xref rid="fig8" ref-type="fig">Figure 8</xref>. When splitting trials by trial-wise pupil dilation we also found no significant difference between trials with larger or smaller pupil size prior to the target in the specific participant sample (<xref rid="fig7" ref-type="fig">Figure 7B</xref>; left panels). In support of no relation between pupil dilation and rhythmicity we also found that the prevalence of significant effects was very low (right panels). However, when splitting the data by the trial-wise fixation stability we found that trials with less fixation stability had significantly stronger rhythmic effect sizes for sensitivity around 3.5 Hz than trials with more fixation stability (at p&lt;0.05; <xref rid="fig7" ref-type="fig">Fig. 7C</xref>). Furthermore, the prevalence of significant effects in the combined-ear data among participant samples was high (64% at 3.5 Hz). This suggests that rhythmicity in behaviour seems to be more expressed when overall eye mobility is larger and underscores an influence of (oculo-)motor behaviour.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
    <caption><title>Illustration of eye tracking data for experiment 4.</title>
    <p><bold>A.</bold> Trial averaged eye position for horizontal (X) and vertical (Y) dimensions. <bold>B.</bold> Standard deviation of eye position for each dimension. <bold>C.</bold> Trial averaged pupil size<bold>. D.</bold> Average number of saccades per trial and time bin. Thick black lines indicate the participant average (n=30 with good eye tracking data), dashed lines individual data. Note that the figure only shows the data for those trials in which the target appeared with a delay of more than 0.6 s (hence later than 0.9s in the trial) in order to avoid contamination with response periods on this display. This corresponds to half the trials in the experiment. Time 0 s corresponds the onset of the background noise.</p></caption>
<graphic xlink:href="628892v1_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>The presence of rhythmicity in auditory perceptual judgements remains controversial. We here probed for such rhythmicity in experiments devoid of explicitly entraining sounds. Across data from four experiments and using different analyses approaches we provide evidence for rhythmicity in listening behaviour. These effects tend to prevail at different frequencies for each ear and the precise nature of effects differs between experiments, hence corroborating the ear- and paradigm-specificity of the putative rhythmicity of auditory perception.</p>
<sec id="s4a">
<title>Assumptions about the specific origin of rhythmicity</title>
<p>One central but often implicit assumption in this line of work concerns the origin of rhythmicity along the neural pathways. Prominent in vivo recordings have revealed rhythmic activity in the auditory cortex, and hence presumably in areas featuring representations of inputs from both ears [<xref ref-type="bibr" rid="c22">22</xref>-<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c57">57</xref>]. However, rhythmic activity may also be present in the auditory thalamus, and can even be seen in cochlear recordings [<xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. At the same time rhythmic activity is also prominent in amodal brain regions involved in decision making, such as the prefrontal cortex [<xref ref-type="bibr" rid="c58">58</xref>, <xref ref-type="bibr" rid="c59">59</xref>]. Any rhythmicity seen in behavioural data could hence originate from neural processes at one a single processing stage, or from multiple stages, and could originate from neural processes sensitive only to monaural signals or after binaural integration (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>).</p>
<p>Importantly, different origins have different implications for how rhythmic effects should manifest in the data. Separate origins in monaural neural representations could in principle result in effects at different frequencies for each ear (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>, left panel). If this was the case, one may predict that if the data were combined across both ears (i.e. collapsing trials with targets in left and right ears) the evidence for rhythmicity may diminish because incommensurable frequencies would cancel. Furthermore, one may not find rhythmicity when using binaural targets, as they would tap into mechanisms operating at distinct time scales that may cancel during subsequent processing stages. Alternatively, if rhythmicity originates at a single stage after binaural integration, one could expect effects at the same frequency when testing individual ears, when presenting stimuli binaurally or when combining the data across ears (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>, right panel). Hence depending on the origin or rhythmicity, the choice of monaural or binaural stimuli may dictate which or whether effects can actually be observed.</p>
</sec>
<sec id="s4b">
<title>Rhythmic effects emerge at multiple frequencies and independently in each ear</title>
<p>With this in mind we probed monaural targets and performed the analysis both on the data from individual ears and for both ears combined. Overall, we find evidence that rhythmicity in auditory tasks exists. Importantly, the data suggest that rhythmic effects emerge at distinct frequencies for each ear. In experiments 1 and 2 the evidence for individual ears was not accompanied by concomitant evidence in the combined-ear data, hence suggesting that these effects arise from ear-specific processes operating at different frequencies. Such representations may either relate to monaural auditory representations or representations strongly modulated by spatial attention and their precise origin needs to be investigated in future work. The ear-specificity of the underlying processes may in part explain the diverging conclusions drawn from studies relying on binaural targets and suggests that rhythmic modes of hearing are better investigated using monaural sounds.</p>
<p>A central question for future work will also be to better understand whether the effects reflected at a specific frequency and ear are related to each other, or whether they reflect independent phenomena. It is possible that specific spectral peaks emerge only in a sub-sample of participants and our analysis directly suggests that for a given experiment the effects in the left and right ears are not correlated among samples of participants. Hence, while rhythmicity seems to exist at different frequencies in the left and right ears, it is possible that these effects prevail, at least partly, in distinct parts of the population.</p>
<p>The present data support conclusions from a previous study using the same experimental paradigm. That study reported effects at different frequencies for perceptual sensitivity and bias, and at slightly different frequencies for each ear [<xref ref-type="bibr" rid="c12">12</xref>]. However, in this previous study the frequencies for both ears fell in a comparable frequency range (6-8 Hz), while in the present data they differ to a larger degree (varying from 2 to about 6Hz). One reason for this discrepancy may be that the study by Ho et al. only reported effects for frequencies above 4 Hz and hence may have missed rhythmicity at slower time scales based on the more narrow frequency range of interest.</p>
<p>Neural processes at the just-mentioned different time scales have been associated with distinct functions in hearing. Auditory delta band activity has been implied in acoustic filtering of attended information and the task-relevant engagement of auditory networks [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c57">57</xref>, <xref ref-type="bibr" rid="c60">60</xref>] and was speculated to reflect one prominent rhythmic mode of listening [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c31">31</xref>]. In contrast, alpha band activity has been linked to task engagement and spatial attention [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c61">61</xref>, <xref ref-type="bibr" rid="c62">62</xref>]. Given that the prevalence of rhythmicity across experiments exhibits multiple peaks, one may conclude that multiple processes shape the rhythmicity of hearing, with their respective prevalence depending on the precise task and participant.</p>
<p>In experiment 3 we found an effect in sensitivity when combining data across ears and a weaker prevalence for an effect in the left ear only. This could speak in favour of an origin in binaural regions, with the single-ear data perhaps not being sufficiently powerful to reach statistical significance. Using the same paradigm as in experiment 3, we have previously shown that the power of EEG-derived oscillatory activity at a time scale of 2-4Hz modulates the strength by which auditory regions encode the respective evidence about the stimulus sequence [<xref ref-type="bibr" rid="c50">50</xref>]. Such rhythmicity in neural processes supposedly arising from auditory regions may directly relate to the rhythmicity in the perceptual sensitivity observed here. The difference in results between experiments 1 and 3 further underscores the influence of task demands on the apparent rhythmicity in behaviour.</p>
</sec>
<sec id="s4c">
<title>Role of attention and oculomotor activity</title>
<p>The notion of a rhythmic listening mode has been introduced under the framework of active sensing, whereby perception engages motor routines like eye movements or sniffing to collect sensory information [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c63">63</xref>]. Behavioural studies have shown that making active movements can facilitate listening outcomes, and suggest that the sampling capacity of the auditory system may derive from the motor system [<xref ref-type="bibr" rid="c6">6</xref>-<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c49">49</xref>]. In fact, corollary signals about oculomotor behaviour are introduced early along the auditory pathway [<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c64">64</xref>] and modulate neural responses in the auditory midbrain [<xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c44">44</xref>] and cortex [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c47">47</xref>].</p>
<p>Motivated by this we tested whether the prominence of a rhythmic mode differs when the same task is performed based on automatically paced trials or requires the manual initialization of each stimulus by the participant (experiment 1 vs 2). The diverging prevalence of effects in these experiments, which differ in time scale and relevant metrics, corroborate an influence of stimulus-initialization on rhythmicity. We did not collect data on the handedness of the participants and future studies would need to replicate this finding and determine whether the lateralization of effects bears any relation to the manual action when initializing the trial. Using eye tracking data, we also directly asked whether the amount of amount of oculomotor activity prior to the target relates to the strength of rhythmicity. We found that more oculomotor activity is associated with greater rhythmicity of perceptual sensitivity around 3-4 Hz. This further supports the notion that hearing is an active process tied to motor behaviour.</p>
<p>Previous studies have speculated that the auditory system may either operate in a continuous or a rhythmic mode, depending on the current requirements for the task [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c57">57</xref>]. The former is supposedly engaged when high levels of attention are paid to hearing, while the latter becomes engaged when attention is divided across multiple stimuli or modalities. In particular the time scale of delta band activity has been implied in this duality of listening modes [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c31">31</xref>]. We simulated the latter using a dual task requiring participants to pay attention to acoustic targets and the fixation dot. While we did not observe significant differences between blocks for the specific participant sample, the bootstrap simulations point to a considerable prevalence of differences around 4Hz, with dual-task requirements leading to stronger rhythmicity in perceptual sensitivity. Hence, the present data to lend some support that attention-related task-demands do shape a rhythmic listening mode.</p>
</sec>
<sec id="s4d">
<title>Analytical approaches to study rhythmicity</title>
<p>Previous studies have used different analytical approaches to test for rhythmicity and have debated the advantages and disadvantages of these. The individual approaches differ in their implicit assumptions, such as how a null distribution under the assumption of no rhythmic effects is derived [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c32">32</xref>-<xref ref-type="bibr" rid="c34">34</xref>]. They may also differ in their statistical sensitivity and specificity, although for specific studies these remain often unknown. We here implemented multiple approaches, relying on different ways to calculate a measure of effect-size for rhythmicity and the respective null distribution. We calibrated these on simulated data, with the aim of reporting effects that are robust to the specific choice of approach. However, the results on the actual data exhibit considerable heterogeneity. And while for the simulated data the single-trial approach was most sensitive, this the lowest prevalence of effects for the actual data. One potential explanation is that the nature of the simulated data differs from that of the actual data, making the former a suboptimal benchmark for the latter. However, given the absence of reliable and reproducible data on the rhythmicity in behavioural data, establishing a proper benchmark remains difficult. Given the (dis-) advantages of specific methods, the present results do not lend themselves for clear conclusions on the suitability of the three approaches.</p>
<p>Importantly, results obtained from a single approach may be misleading, in particular if the inference is drawn from a single statistical threshold applied to one specific participant sample. Testing for the prevalence of effects in random variations of the participant sample did in part alleviate some of the discrepancies between approaches in the present data. In fact, given the emergence of multiple spectral peaks in the prevalence data, the results suggest that inference drawn from a single participant sample fails to provide the full diversity of effects present in a population. Hence, it is conceivable that some of the discrepancies in the previous literature are related to statistical noise resulting from small sample sizes and the peculiarities of specific participant samples or analysis approaches.</p>
</sec>
</sec>
<sec id="s5">
<title>Conclusion</title>
<p>The present data speak against the presence of a mechanism that results in mandatory rhythmicity that governs auditory perception per se. Rather the present data support the existence of paradigm-specific effects that pertain to sensitivity or response biases independently and which may emerge at different frequencies for each ear. In line with other recent work we speculate that multiple processes, including temporal entrainment, neural adaptation, temporally predictive processes and motor-related processes interact to shape auditory perception [<xref ref-type="bibr" rid="c65">65</xref>]. Given that specific paradigms tap into a unique combination of perceptual and motor requirements this may explain the diversity of previous results on the rhythmicity of hearing.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Lena Hehemann, Sepideh Mirzaei and Stella Thiele for their help with collecting the data. This study was supported by the German Research Foundation (DFG, KA 2661/6-1)</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keitel</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ruzzoli</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dugué</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Busch</surname>, <given-names>N.A.</given-names></string-name>, and <string-name><surname>Benwell</surname>, <given-names>C.S.Y</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Rhythms in cognition: The evidence revisited</article-title>. <source>Eur J Neurosci</source> <volume>55</volume>, <fpage>2991</fpage>-<lpage>3009</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>VanRullen</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Perceptual Cycles</article-title>. <source>Trends Cogn Sci</source> <volume>20</volume>, <fpage>723</fpage>-<lpage>735</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>VanRullen</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Koch</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Is perception discrete or continuous?</article-title> <source>Trends Cogn Sci</source> <volume>7</volume>, <fpage>207</fpage>-<lpage>213</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schroeder</surname>, <given-names>C.E.</given-names></string-name>, <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kajikawa</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Partan</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Puce</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Neuronal oscillations and visual amplification of speech</article-title>. <source>Trends Cogn Sci</source> <volume>12</volume>, <fpage>106</fpage>-<lpage>113</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benedetto</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Morrone</surname>, <given-names>M.C.</given-names></string-name>, and <string-name><surname>Tomassini</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2020</year>). <article-title>The Common Rhythm of Action and Perception</article-title>. <source>J Cogn Neurosci</source> <volume>32</volume>, <fpage>187</fpage>-<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morillon</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Hackett</surname>, <given-names>T.A.</given-names></string-name>, <string-name><surname>Kajikawa</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Schroeder</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Predictive motor control of sensory dynamics in auditory active sensing</article-title>. <source>Curr Opin Neurobiol</source> <volume>31</volume>, <fpage>230</fpage>-<lpage>238</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lima</surname>, <given-names>C.F.</given-names></string-name>, <string-name><surname>Krishnan</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Scott</surname>, <given-names>S.K</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Roles of Supplementary Motor Areas in Auditory Processing and Auditory Imagery</article-title>. <source>Trends Neurosci</source> <volume>39</volume>, <fpage>527</fpage>-<lpage>542</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Kock</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gladhill</surname>, <given-names>K.A.</given-names></string-name>, <string-name><surname>Ali</surname>, <given-names>M.N.</given-names></string-name>, <string-name><surname>Joiner</surname>, <given-names>W.M.</given-names></string-name>, and <string-name><surname>Wiener</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2021</year>). <article-title>How Movements Shape the Perception of Time</article-title>. <source>Trends in Cognitive Sciences</source>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latour</surname>, <given-names>P.L</given-names></string-name></person-group>. (<year>1967</year>). <article-title>Evidence of internal clocks in the human operator</article-title>. <source>Acta Psychol (Amst)</source> <volume>27</volume>, <fpage>341</fpage>-<lpage>348</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pöppel</surname>, <given-names>E</given-names></string-name></person-group>. (<year>1970</year>). <article-title>Excitability cycles in central intermittency</article-title>. <source>Psychologische Forschung</source> <volume>34</volume>, <fpage>1</fpage>-<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ho</surname>, <given-names>H.T.</given-names></string-name>, <string-name><surname>Burr</surname>, <given-names>D.C.</given-names></string-name>, <string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Morrone</surname>, <given-names>M.C</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Auditory Perceptual History Is Propagated through Alpha Oscillations</article-title>. <source>Curr Biol</source> <volume>29</volume>, <fpage>4208</fpage>-<lpage>4217.</lpage> </mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ho</surname>, <given-names>H.T.</given-names></string-name>, <string-name><surname>Leung</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Burr</surname>, <given-names>D.C.</given-names></string-name>, <string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Morrone</surname>, <given-names>M.C</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Auditory Sensitivity and Decision Criteria Oscillate at Different Frequencies Separately for the Two Ears</article-title>. <source>Curr Biol</source> <volume>27</volume>, <fpage>3643</fpage>-<lpage>3649.</lpage> </mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Farahbod</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Saberi</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2015</year>). <article-title>The Rhythm of Perception: Entrainment to Acoustic Rhythms Induces Subsequent Perceptual Oscillation</article-title>. <source>Psychol Sci</source> <volume>26</volume>, <fpage>1006</fpage>-<lpage>1013</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Farahbod</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Saberi</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Hickok</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2020</year>). <article-title>The rhythm of attention: Perceptual modulation via rhythmic entrainment is lowpass and attention mediated</article-title>. <source>Atten Percept Psychophys</source>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zoefel</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>VanRullen</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Oscillatory Mechanisms of Stimulus Processing and Selection in the Visual and Auditory Systems: State-of-the-Art, Speculations and Suggestions</article-title>. <source>Front Neurosci</source> <volume>11</volume>, <fpage>296</fpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ng</surname>, <given-names>B.S.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Kayser</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2012</year>). <article-title>A precluding but not ensuring role of entrained low-frequency oscillations for auditory perception</article-title>. <source>J Neurosci</source> <volume>32</volume>, <fpage>12268</fpage>-<lpage>12276</lpage>.</mixed-citation></ref>
    <ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strauss</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Henry</surname>, <given-names>M.J.</given-names></string-name>, and <string-name><surname>Scharinger</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Alpha phase determines successful lexical decision in noise</article-title>. <source>J Neurosci</source> <volume>35</volume>, <fpage>3256</fpage>-<lpage>3262</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henry</surname>, <given-names>M.J.</given-names></string-name>, <string-name><surname>Herrmann</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Obleser</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Neural Microstates Govern Perception of Auditory Input without Rhythmic Structure</article-title>. <source>J Neurosci</source> <volume>36</volume>, <fpage>860</fpage>-<lpage>871</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wöstmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Waschke</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Obleser</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Prestimulus neural alpha power predicts confidence in discriminating identical auditory stimuli</article-title>. <source>Eur J Neurosci</source> <volume>49</volume>, <fpage>94</fpage>-<lpage>105</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>ten Oever</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Sack</surname>, <given-names>A.T</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Oscillatory phase shapes syllable perception</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>112</volume>, <fpage>15833</fpage>-<lpage>15837</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Florin</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Vuvan</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Peretz</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Baillet</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Pre-target neural oscillations predict variability in the detection of small pitch changes</article-title>. <source>PLoS One</source> <volume>12</volume>, <fpage>e0177836</fpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Shah</surname>, <given-names>A.S.</given-names></string-name>, <string-name><surname>Knuth</surname>, <given-names>K.H.</given-names></string-name>, <string-name><surname>Ulbert</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Karmos</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Schroeder</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>2005</year>). <article-title>An oscillatory hierarchy controlling neuronal excitability and stimulus processing in the auditory cortex</article-title>. <source>J Neurophysiol</source> <volume>94</volume>, <fpage>1904</fpage>-<lpage>1911</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Musacchia</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>O’Connel</surname>, <given-names>M.N.</given-names></string-name>, <string-name><surname>Falchier</surname>, <given-names>A.Y.</given-names></string-name>, <string-name><surname>Javitt</surname>, <given-names>D.C.</given-names></string-name>, and <string-name><surname>Schroeder</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>2013</year>). <article-title>The spectrotemporal filter mechanism of auditory selective attention</article-title>. <source>Neuron</source> <volume>77</volume>, <fpage>750</fpage>-<lpage>761</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Safaai</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sakata</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Panzeri</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Rhythmic auditory cortex activity at multiple timescales shapes stimulus-response gain and background firing</article-title>. <source>J Neurosci</source> <volume>35</volume>, <fpage>7750</fpage>-<lpage>7762</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Clause</surname>, <given-names>A.R.</given-names></string-name>, <string-name><surname>Barth-Maron</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Polley</surname>, <given-names>D.B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>A Corticothalamic Circuit for Dynamic Switching between Feature Detection and Discrimination</article-title>. <source>Neuron</source> <volume>95</volume>, <fpage>180</fpage>-<lpage>194.</lpage> </mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Obleser</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Kayser</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Neural Entrainment and Attentional Selection in the Listening Brain</article-title>. <source>Trends Cogn Sci</source> <volume>23</volume>, <fpage>913</fpage>-<lpage>926</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Michalareas</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Poeppel</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2021</year>). <article-title>The impact of phase entrainment on auditory detection is highly variable: Revisiting a key finding</article-title>. <source>Eur J Neurosci</source>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kayser</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Evidence for the Rhythmic Perceptual Sampling of Auditory Scenes</article-title>. <source>Front Hum Neurosci</source> <volume>13</volume>, <fpage>249</fpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ilhan</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>VanRullen</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2012</year>). <article-title>No counterpart of visual perceptual echoes in the auditory system</article-title>. <source>PLoS ONE</source> <volume>7</volume>, <fpage>e49287</fpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zoefel</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>VanRullen</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2015</year>). <article-title>The Role of High-Level Processes for Oscillatory Phase Entrainment to Speech Sound</article-title>. <source>Front Hum Neurosci</source> <volume>9</volume>, <fpage>651</fpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haegens</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Zion Golumbic</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Rhythmic facilitation of sensory processing: A critical review</article-title>. <source>Neurosci Biobehav Rev</source> <volume>86</volume>, <fpage>150</fpage>-<lpage>165</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zoefel</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Davis</surname>, <given-names>M.H.</given-names></string-name>, <string-name><surname>Valente</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Riecke</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>How to test for phasic modulation of neural and behavioural responses</article-title>. <source>Neuroimage</source> <volume>202</volume>, <fpage>116175</fpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tosato</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rohenkohl</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Dowdall</surname>, <given-names>J.R.</given-names></string-name>, and <string-name><surname>Fries</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Quantifying rhythmicity in perceptual reports</article-title>. <source>Neuroimage</source> <volume>262</volume>, <fpage>119561</fpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brookshire</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Putative rhythms in attentional switching can be explained by aperiodic temporal structure</article-title>. <source>Nature human behaviour</source> <volume>6</volume>, <fpage>1280</fpage>-<lpage>1291</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lundqvist</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Wutz</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2022</year>). <article-title>New methods for oscillation analyses push new theories of discrete cognition</article-title>. <source>Psychophysiology</source> <volume>59</volume>, <fpage>e13827</fpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kulesa</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Krzywinski</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Blainey</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Altman</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Sampling distributions and the bootstrap</article-title>. <source>Nat Methods</source> <volume>12</volume>, <fpage>477</fpage>-<lpage>478</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schroeder</surname>, <given-names>C.E.</given-names></string-name>, and <string-name><surname>Lakatos</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Low-frequency neuronal oscillations as instruments of sensory selection</article-title>. <source>Trends Neurosci</source> <volume>32</volume>, <fpage>9</fpage>-<lpage>18</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schroeder</surname>, <given-names>C.E.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>D.A.</given-names></string-name>, <string-name><surname>Radman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Scharfman</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Lakatos</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Dynamics of Active Sensing and perceptual selection</article-title>. <source>Curr Opin Neurobiol</source> <volume>20</volume>, <fpage>172</fpage>-<lpage>176</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Connell</surname>, <given-names>M.N.</given-names></string-name>, <string-name><surname>Barczak</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>McGinnis</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Mackin</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Mowery</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>C.E.</given-names></string-name>, and <string-name><surname>Lakatos</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2020</year>). <article-title>The Role of Motor and Environmental Visual Rhythms in Structuring Auditory Cortical Excitability</article-title>. <source>iScience</source> <volume>23</volume>, <fpage>101374</fpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gehmacher</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Reisinger</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hartmann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Keintzel</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rösch</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schwarz</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Direct Cochlear Recordings in Humans Show a Theta Rhythmic Modulation of Auditory Nerve Activity by Selective Attention</article-title>. <source>J Neurosci</source> <volume>42</volume>, <fpage>1343</fpage>-<lpage>1351</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Köhler</surname>, <given-names>M.H.A.</given-names></string-name>, and <string-name><surname>Weisz</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Cochlear Theta Activity Oscillates in Phase Opposition during Interaural Attention</article-title>. <source>J Cogn Neurosci</source> <volume>35</volume>, <fpage>588</fpage>-<lpage>602</lpage>.</mixed-citation></ref>
    <ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gruters</surname>, <given-names>K.G.</given-names></string-name>, <string-name><surname>Murphy</surname>, <given-names>D.L.K.</given-names></string-name>, <string-name><surname>Jenson</surname>, <given-names>C.D.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>D.W.</given-names></string-name>, <string-name><surname>Shera</surname>, <given-names>C.A.</given-names></string-name>, and <string-name><surname>Groh</surname>, <given-names>J.M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>The eardrums move when the eyes move: A multisensory effect on the mechanics of hearing</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>115</volume>, <fpage>E1309</fpage>-<lpage>E1318</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bulkin</surname>, <given-names>D.A.</given-names></string-name>, and <string-name><surname>Groh</surname>, <given-names>J.M</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Distribution of eye position information in the monkey inferior colliculus</article-title>. <source>J Neurophysiol</source> <volume>107</volume>, <fpage>785</fpage>-<lpage>795</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Populin</surname>, <given-names>L.C.</given-names></string-name>, <string-name><surname>Tollin</surname>, <given-names>D.J.</given-names></string-name>, and <string-name><surname>Yin</surname>, <given-names>T.C</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Effect of eye position on saccades and neuronal responses to acoustic stimuli in the superior colliculus of the behaving cat</article-title>. <source>J Neurophysiol</source> <volume>92</volume>, <fpage>2151</fpage>-<lpage>2167</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Werner-Reiss</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Kelly</surname>, <given-names>K.A.</given-names></string-name>, <string-name><surname>Trause</surname>, <given-names>A.S.</given-names></string-name>, <string-name><surname>Underhill</surname>, <given-names>A.M.</given-names></string-name>, and <string-name><surname>Groh</surname>, <given-names>J.M.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Eye position affects activity in primary auditory cortex of primates</article-title>. <source>Curr Biol</source> <volume>13</volume>, <fpage>554</fpage>-<lpage>562</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname>, <given-names>D.M.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Mooney</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2014</year>). <article-title>A synaptic and circuit basis for corollary discharge in the auditory cortex</article-title>. <source>Nature</source>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leszczynski</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nentwich</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Russ</surname>, <given-names>B.E.</given-names></string-name>, <string-name><surname>Parra</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Schroeder</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Saccadic modulation of neural excitability in auditory areas of the neocortex</article-title>. <source>Curr Biol</source> <volume>33</volume>, <fpage>1185</fpage>-<lpage>1195.</lpage> </mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morillon</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>C.E.</given-names></string-name>, and <string-name><surname>Wyart</surname>, <given-names>V</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Motor contributions to the temporal precision of auditory attention</article-title>. <source>Nat Commun</source> <volume>5</volume>, <fpage>5255</fpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zalta</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Petkoski</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Morillon</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Natural rhythms of periodic temporal attention</article-title>. <source>Nat Commun</source> <volume>11</volume>, <fpage>1051</fpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kayser</surname>, <given-names>S.J.</given-names></string-name>, <string-name><surname>McNair</surname>, <given-names>S.W.</given-names></string-name>, and <string-name><surname>Kayser</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Prestimulus influences on auditory perception from sensory representations and decision processes</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>113</volume>, <fpage>4842</fpage>-<lpage>4847</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dowse</surname>, <given-names>H.B.</given-names></string-name>, and <string-name><surname>Ringo</surname>, <given-names>J.M</given-names></string-name></person-group>. (<year>1989</year>). <article-title>The search for hidden periodicities in biological time series revisited</article-title>. <source>Journal of Theoretical Biology</source> <volume>139</volume>, <fpage>487</fpage>-<lpage>515</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dowse</surname>, <given-names>H.B.</given-names></string-name>, and <string-name><surname>Ringo</surname>, <given-names>J.M</given-names></string-name></person-group>. (<year>1994</year>). <article-title>Summing locomotor activity data into “bins”: How to avoid artifact in spectral analysis</article-title>. <source>Biological Rhythm Research</source> <volume>25</volume>, <fpage>2</fpage>-<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joshi</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Gold</surname>, <given-names>J.I</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Pupil Size as a Window on Neural Substrates of Cognition</article-title>. <source>Trends Cogn Sci</source> <volume>24</volume>, <fpage>466</fpage>-<lpage>480</lpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zekveld</surname>, <given-names>A.A.</given-names></string-name>, <string-name><surname>Koelewijn</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Kramer</surname>, <given-names>S.E</given-names></string-name></person-group>. (<year>2018</year>). <article-title>The Pupil Dilation Response to Auditory Stimuli: Current State of Knowledge</article-title>. <source>Trends in hearing</source> <volume>22</volume>, <elocation-id>e2331216518777174</elocation-id>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Okazawa</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sha</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Purcell</surname>, <given-names>B.A.</given-names></string-name>, and <string-name><surname>Kiani</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Psychophysical reverse correlation reflects both sensory and decision-making processes</article-title>. <source>Nat Commun</source> <volume>9</volume>, <fpage>3479</fpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waskom</surname>, <given-names>M.L.</given-names></string-name>, <string-name><surname>Kiani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Okazawa</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sha</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Purcell</surname>, <given-names>B.A.</given-names></string-name>, and <string-name><surname>Kiani</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Decision Making through Integration of Sensory Evidence at Prolonged Timescales</article-title>. <source>Curr Biol</source> <volume>28</volume>, <fpage>3850</fpage>-<lpage>3856.</lpage> </mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Barczak</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Neymotin</surname>, <given-names>S.A.</given-names></string-name>, <string-name><surname>McGinnis</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ross</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Javitt</surname>, <given-names>D.C.</given-names></string-name>, and <string-name><surname>O’Connell</surname>, <given-names>M.N</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Global dynamics of selective attention and its lapses in primary auditory cortex</article-title>. <source>Nat Neurosci</source> <volume>19</volume>, <fpage>1707</fpage>-<lpage>1717</lpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Samaha</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Iemi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Haegens</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Busch</surname>, <given-names>N.A</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Spontaneous Brain Oscillations and Perceptual Decision-Making</article-title>. <source>Trends Cogn Sci</source> <volume>24</volume>, <fpage>639</fpage>-<lpage>653</lpage>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Diverse Phase Relations among Neuronal Rhythms and Their Potential Function</article-title>. <source>Trends Neurosci</source> <volume>39</volume>, <fpage>86</fpage>-<lpage>99</lpage>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Connell</surname>, <given-names>M.N.</given-names></string-name>, <string-name><surname>Barczak</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>C.E.</given-names></string-name>, and <string-name><surname>Lakatos</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Layer specific sharpening of frequency tuning by selective attention in primary auditory cortex</article-title>. <source>J Neurosci</source> <volume>34</volume>, <fpage>16496</fpage>-<lpage>16508</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henry</surname>, <given-names>M.J.</given-names></string-name>, and <string-name><surname>Obleser</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Frequency modulation entrains slow neural oscillations and optimizes human listening behavior</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>, <fpage>20095</fpage>-<lpage>20100</lpage>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henry</surname>, <given-names>M.J.</given-names></string-name>, and <string-name><surname>Herrmann</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Low-frequency neural oscillations support dynamic attending in temporal context</article-title>. <source>Timing &amp; Time Perception</source> <volume>2</volume>, <fpage>62086</fpage>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hatsopoulos</surname>, <given-names>N.G.</given-names></string-name>, and <string-name><surname>Suminski</surname>, <given-names>A.J</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Sensing with the motor cortex</article-title>. <source>Neuron</source> <volume>72</volume>, <fpage>477</fpage>-<lpage>487</lpage>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmehl</surname>, <given-names>M.N.</given-names></string-name>, and <string-name><surname>Groh</surname>, <given-names>J.M</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Visual Signals in the Mammalian Auditory System</article-title>. <source>Annual review of vision science</source>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>L’Hermite</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Zoefel</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Rhythmic Entrainment Echoes in Auditory Perception</article-title>. <source>J Neurosci</source> <volume>43</volume>, <fpage>6667</fpage>-<lpage>6678</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105734.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This high-N, multi-task study offers a comprehensive examination of rhythmicity in behavioral performance during listening. It presents a <bold>valuable</bold> set of findings that reveal task- and ear-specific effects, challenging the notion of a universal rhythmicity in auditory perception. While the evidence is <bold>solid</bold>, the study would benefit from a stronger conceptual framework to contextualize and explain the observed patterns. Nonetheless, the work is likely to be of significant interest to behavioral and cognitive scientists focused on perception and neural oscillations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105734.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents results from four independent experiments, each of which tests for rhythmicity in auditory perception. The authors report rhythmic fluctuations in discrimination performance at frequencies between 2 and 6 Hz. The exact frequency depends on the ear and experimental paradigm, although some frequencies seem to be more common than others.</p>
<p>Strengths:</p>
<p>The first sentence in the abstract describes the state of the art perfectly: &quot;Numerous studies advocate for a rhythmic mode of perception; however, the evidence in the context of auditory perception remains inconsistent&quot;. This is precisely why the data from the present study is so valuable. This is probably the study with the highest sample size (total of &gt; 100 in 4 experiments) in the field. The analysis is very thorough and transparent, due to the comparison of several statistical approaches and simulations of their sensitivity. Each of the experiments differs from the others in a clearly defined experimental parameter, and the authors test how this impacts auditory rhythmicity, measured in pitch discrimination performance (accuracy, sensitivity, bias) of a target presented at various delays after noise onset.</p>
<p>Weaknesses:</p>
<p>(1) The authors find that the frequency of auditory perception changes between experiments. I think they could exploit differences between experiments better to interpret and understand the obtained results. These differences are very well described in the Introduction, but don't seem to be used for the interpretation of results. For instance, what does it mean if perceptual frequency changes from between- to within-trial pitch discrimination? Why did the authors choose this experimental manipulation? Based on differences between experiments, is there any systematic pattern in the results that allows conclusions about the roles of different frequencies? I think the Discussion would benefit from an extension to cover this aspect.</p>
<p>(2) The Results give the impression of clear-cut differences in relevant frequencies between experiments (e.g., 2 Hz in Experiment 1, 6 Hz in Exp 2, etc), but they might not be so different. For instance, a 6 Hz effect is also visible in Experiment 1, but it just does not reach conventional significance. The average across the three experiments is therefore very useful, and also seems to suggest that differences between experiments are not very pronounced (otherwise the average would not produce clear peaks in the spectrum). I suggest making this point clearer in the text.</p>
<p>(3) I struggle to understand the hypothesis that rhythmic sampling differs between ears. In most everyday scenarios, the same sounds arrive at both ears, and the time difference between the two is too small to play a role for the frequencies tested. If both ears operate at different frequencies, the effects of the rhythm on overall perception would then often cancel out. But if this is the case, why would the two ears have different rhythms to begin with? This could be described in more detail.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105734.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The current study aims to shed light on why previous work on perceptual rhythmicity has led to inconsistent results. They propose that the differences may stem from conceptual and methodological issues. In a series of experiments, the current study reports perceptual rhythmicity in different frequency bands that differ between different ear stimulations and behavioral measures. The study suggests challenges regarding the idea of universal perceptual rhythmicity in hearing.</p>
<p>Strengths:</p>
<p>The study aims to address differences observed in previous studies about perceptual rhythmicity. This is important and timely because the existing literature provides quite inconsistent findings. Several experiments were conducted to assess perceptual rhythmicity in hearing from different angles. The authors use sophisticated approaches to address the research questions.</p>
<p>Weaknesses:</p>
<p>(1) Conceptional concerns:</p>
<p>The authors place their research in the context of a rhythmic mode of perception. They also discuss continuous vs rhythmic mode processing. Their study further follows a design that seems to be based on paradigms that assume a recent phase in neural oscillations that subsequently influence perception (e.g., Fiebelkorn et al.; Landau &amp; Fries). In my view, these are different facets in the neural oscillation research space that require a bit more nuanced separation. Continuous mode processing is associated with vigilance tasks (work by Schroeder and Lakatos; reduction of low frequency oscillations and sustained gamma activity), whereas the authors of this study seem to link it to hearing tasks specifically (e.g., line 694). Rhythmic mode processing is associated with rhythmic stimulation by which neural oscillations entrain and influence perception (also, Schroeder and Lakatos; greater low-frequency fluctuations and more rhythmic gamma activity). The current study mirrors the continuous rather than the rhythmic mode (i.e., there was no rhythmic stimulation), but even the former seems not fully fitting, because trials are 1.8 s short and do not really reflect a vigilance task. Finally, previous paradigms on phase-resetting reflect more closely the design of the current study (i.e., different times of a target stimulus relative to the reset of an oscillation). This is the work by Fiebelkorn et al., Landau &amp; Fries, and others, which do not seem to be cited here, which I find surprising. Moreover, the authors would want to discuss the role of the background noise in resetting the phase of an oscillation, and the role of the fixation cross also possibly resetting the phase of an oscillation. Regardless, the conceptional mixture of all these facets makes interpretations really challenging. The phase-reset nature of the paradigm is not (or not well) explained, and the discussion mixes the different concepts and approaches. I recommend that the authors frame their work more clearly in the context of these different concepts (affecting large portions of the manuscript).</p>
<p>(2) Methodological concerns:</p>
<p>The authors use a relatively unorthodox approach to statistical testing. I understand that they try to capture and characterize the sensitivity of the different analysis approaches to rhythmic behavioral effects. However, it is a bit unclear what meaningful effects are in the study. For example, the bootstrapping approach that identifies the percentage of significant variations of sample selections is rather descriptive (Figures 5-7). The authors seem to suggest that 50% of the samples are meaningful (given the dashed line in the figure), even though this is rarely reached in any of the analyses. Perhaps &gt;80% of samples should show a significant effect to be meaningful (at least to my subjective mind). To me, the low percentage rather suggests that there is not too much meaningful rhythmicity present. I suggest that the authors also present more traditional, perhaps multi-level, analyses: Calculation of spectra, binning, or single-trial analysis for each participant and condition, and the respective calculation of the surrogate data analysis, and then comparison of the surrogate data to the original data on the second (participant) level using t-tests. I also thought the statistical approach undertaken here could have been a bit more clearly/didactically described as well.</p>
<p>The authors used an adaptive procedure during the experimental blocks such that the stimulus intensity was adjusted throughout. In practice, this can be a disadvantage relative to keeping the intensity constant throughout, because, on average, correct trials will be associated with a higher intensity than incorrect trials, potentially making observations of perceptual rhythmicity more challenging. The authors would want to discuss this potential issue. Intensity adjustments could perhaps contribute to the observed rhythmicity effects. Perhaps the rhythmicity of the stimulus intensity could be analyzed as well. In any case, the adaptive procedure may add variance to the data.</p>
<p>Additional methodological concerns relate to Figure 8. Figures 8A and C seem to indicate that a baseline correction for a very short time window was calculated (I could not find anything about this in the methods section). The data seem very variable and artificially constrained in the baseline time window. It was unclear what the reader might take from Figure 8.</p>
<p>Motivation and discussion of eye-movement/pupillometry and motor activity: The dual task paradigm of Experiment 4 and the reasons for assessing eye metrics in the current study could have been better motivated. The experiment somehow does not fit in very well. There is recent evidence that eye movements decrease during effortful tasks (e.g., Contadini-Wright et al. 2023 J Neurosci; Herrmann &amp; Ryan 2024 J Cog Neurosci), which appears to contradict the results presented in the current study. Moreover, by appealing to active sensing frameworks, the authors suggest that active movements can facilitate listening outcomes (line 677; they should provide a reference for this claim), but it is unclear how this would relate to eye movements. Certainly, a person may move their head closer to a sound source in the presence of competing sound to increase the signal-to-noise ratio, but this is not really the active movements that are measured here. A more detailed discussion may be important. The authors further frame the difference between Experiments 1 and 2 as being related to participants' motor activity. However, there are other factors that could explain differences between experiments. Self-paced trials give participants the opportunity to rest more (inter-trial durations were likely longer in Experiment 2), perhaps affecting attentional engagement. I think a more nuanced discussion may be warranted.</p>
<p>Discussion:</p>
<p>The main data in Figure 3 showed little rhythmicity. The authors seem to glance over this fact by simply stating that the same phase is not necessary for their statistical analysis. Previous work, however, showed rhythmicity in the across-participant average (e.g., Fiebelkorn's and similar work). Moreover, one would expect that some of the effects in the low-frequency band (e.g., 2-4 Hz) are somewhat similar across participants. Conduction delays in the auditory system are much smaller than the 0.25-0.5 s associated with 2-4 Hz. The authors would want to discuss why different participants would express so vastly different phases that the across-participant average does not show any rhythmicity, and what this would mean neurophysiologically.</p>
<p>An additional point that may require more nuanced discussion is related to the rhythmicity of response bias versus sensitivity. The authors could discuss what the rhythmicity of these different measures in different frequency bands means, with respect to underlying neural oscillations.</p>
<p>Figures:</p>
<p>Much of the text in the figures seems really small. Perhaps the authors would want to ensure it is readable even for those with low vision abilities. Moreover, Figure 1A is not as intuitive as it could be and may perhaps be made clearer. I also suggest the authors discuss a bit more the potential monoaural vs binaural issues, because the perceptual rhythmicity is much slower than any conduction delays in the auditory system that could lead to interference.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105734.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The finding of rhythmic activity in the brain has, for a long time, engendered the theory of rhythmic modes of perception, that humans might oscillate between improved and worse perception depending on states of our internal systems. However, experiments looking for such modes have resulted in conflicting findings, particularly in those where the stimulus itself is not rhythmic. This paper seeks to take a comprehensive look at the effect and various experimental parameters which might generate these competing findings: in particular, the presentation of the stimulus to one ear or the other, the relevance of motor involvement, attentional demands, and memory: each of which are revealed to effect the consistency of this rhythmicity.</p>
<p>The need the paper attempts to resolve is a critical one for the field. However, as presented, I remain unconvinced that the data would not be better interpreted as showing no consistent rhythmic mode effect. It lacks a conceptual framework to understand why effects might be consistent in each ear but at different frequencies and only for some tasks with slight variants, some affecting sensitivity and some affecting bias.</p>
<p>Strengths:</p>
<p>The paper is strong in its experimental protocol and its comprehensive analysis, which seeks to compare effects across several analysis types and slight experiment changes to investigate which parameters could affect the presence or absence of an effect of rhythmicity. The prescribed nature of its hypotheses and its manner of setting out to test them is very clear, which allows for a straightforward assessment of its results</p>
<p>Weaknesses:</p>
<p>There is a weakness throughout the paper in terms of establishing a conceptual framework both for the source of &quot;rhythmic modes&quot; and for the interpretation of the results. Before understanding the data on this matter, it would be useful to discuss why one would posit such a theory to begin with. From a perceptual side, rhythmic modes of processing in the absence of rhythmic stimuli would not appear to provide any benefit to processing. From a biological or homeostatic argument, it's unclear why we would expect such fluctuations to occur in such a narrow-band way when neither the stimulus nor the neurobiological circuits require it.</p>
<p>Secondly, for the analysis to detect a &quot;rhythmic mode&quot;, it must assume that the phase of fluctuations across an experiment (i.e., whether fluctuations are in an up-state or down-state at onset) is constant at stimulus onset, whereas most oscillations do not have such a total phase-reset as a result of input. Therefore, some theoretical positing of what kind of mechanism could generate this fluctuation is critical toward understanding whether the analysis is well-suited to the studied mechanism.</p>
<p>Thirdly, an interpretation of why we should expect left and right ears to have distinct frequency ranges of fluctuations is required. There are a large number of statistical tests in this paper, and it's not clear how multiple comparisons are controlled for, apart from experiment 4 (which specifies B&amp;H false discovery rate). As such, one critical method to identify whether the results are not the result of noise or sample-specific biases is the plausibility of the finding. On its face, maintaining distinct frequencies of perception in each ear does not fit an obvious conceptual framework.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105734.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Fabio</surname>
<given-names>Cécile</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0573-6378</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Kayser</surname>
<given-names>Christoph</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7362-5704</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We are grateful to the reviewers for their extensive and constructive feedback. In large the three reviewers noted the following main points:</p>
<disp-quote content-type="editor-comment">
<p>(1) The overall evidence for any rhythmicity in this data is not ‘very strong’.</p>
</disp-quote>
<p>We do agree and will tone down the conclusions accordingly. However, as one of the reviewers noted, a qualitative interpretation of the specific statistical results remains somewhat vague and speculative by necessity.</p>
<disp-quote content-type="editor-comment">
<p>(2) The differences between the results for the individual experiments are generally small. Yet, the same reviewer also asks for speculations as to how differences between experiments can be interpreted.</p>
</disp-quote>
<p>We will consider these, but also note that a clear demonstration of the robustness of specific effects requires the replication of individual experiments in a separate experiment.</p>
<disp-quote content-type="editor-comment">
<p>(3) A clear-cut interpretation of the current experimental design in the context of continuous listening and true vigilance tasks remains difficult. This makes the interpretation and generalization of the results difficult.</p>
</disp-quote>
<p>We do agree in principle, but also note that task designs very widely in previous work, which may be one reason for why there is no clear consensus on the existence or absence of a rhythmic mode of listening. We will consider specific suggestions for future work to be included in the revision.</p>
<disp-quote content-type="editor-comment">
<p>(4) The adjustment of task difficulty in the present task design may pose a challenge. Reviewers also suggest analyzing potential rhythmicity in this task difficulty parameter.</p>
</disp-quote>
<p>We will consider this for the revision.</p>
<disp-quote content-type="editor-comment">
<p>(5) A more clear-cut interpretation of what potential differences in the rhythmicity of sensitivity and bias would mean should be included.</p>
</disp-quote>
<p>We will provide this in the revision.</p>
<disp-quote content-type="editor-comment">
<p>(6) The study should provide a stronger conceptual framework both for the source of &quot;rhythmic modes&quot; and why one may expect differences between ears.</p>
</disp-quote>
<p>In large this has been put forward by many previous studies testing and reporting rhythmicity in auditory tasks.  Rhythmicity is pervasive in neural activity, but whether and how this relates to behavioral data remains less clear. These points will be clarified in a revision.</p>
<disp-quote content-type="editor-comment">
<p>(7) Parallels to work in the visual domain by Fiebelkorn, Landau &amp; Fries should be included.</p>
</disp-quote>
<p>We will discuss similarities and differences between studies on perceptual rhythmicity in the visual and auditory domains.</p>
</body>
</sub-article>
</article>