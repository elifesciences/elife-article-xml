<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108186</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108186</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108186.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Biochemistry and Chemical Biology</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>Squidly: Enzyme Catalytic Residue Prediction Harnessing a Biology-Informed Contrastive Learning Framework</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-9574-3468</contrib-id>
<name>
<surname>Rieger</surname>
<given-names>William JF</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3548-268X</contrib-id>
<name>
<surname>Boden</surname>
<given-names>Mikael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4027-364X</contrib-id>
<name>
<surname>Arnold</surname>
<given-names>Frances</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1331-8192</contrib-id>
<name>
<surname>Mora</surname>
<given-names>Ariane</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>amora@aithyra.at</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rqy9422</institution-id><institution>School of Chemistry and Molecular Biology, University of Queensland</institution></institution-wrap>, <city>Brisbane</city>, <country country="AU">Australia</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Division of Chemistry and Chemical Engineering, California Institute of Technology</institution></institution-wrap>, <city>Pasadena</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00dgdnb65</institution-id><institution>AITHYRA GmbH, Research Institute for Biomedical Artificial Intelligence of the Austrian Academy of Sciences</institution></institution-wrap>, <city>Vienna</city>, <country country="AT">Austria</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Graña</surname>
<given-names>Martin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Institut Pasteur de Montevideo</institution>
</institution-wrap>
<city>Montevideo</city>
<country country="UY">Uruguay</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Cui</surname>
<given-names>Qiang</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Boston University</institution>
</institution-wrap>
<city>Boston</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-10-02">
<day>02</day>
<month>10</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2026-01-02">
<day>02</day>
<month>01</month>
<year>2026</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108186</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-06-28">
<day>28</day>
<month>06</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-06-20">
<day>20</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.06.13.659624"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-10-02">
<day>02</day>
<month>10</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.108186.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.108186.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.108186.1.sa1">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.108186.1.sa0">Reviewer #2 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Rieger et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Rieger et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108186-v2.pdf"/>
<abstract><p>Enzymes present a sustainable alternative to traditional chemical industries, drug synthesis, and bioremediation applications. Because catalytic residues are the key amino acids that drive enzyme function, their accurate prediction facilitates enzyme function prediction. Sequence similarity-based approaches such as BLAST are fast but require previously annotated homologs. Machine learning approaches aim to overcome this limitation; however, current gold-standard machine learning (ML)-based methods require high-quality 3D structures limiting their application to large datasets. To address these challenges, we developed Squidly, a sequence-only tool that leverages contrastive representation learning with a biology-informed, rationally designed pairing scheme to distinguish catalytic from non-catalytic residues using per-token Protein Language Model embeddings. Squidly surpasses state-of-the-art ML annotation methods in catalytic residue prediction while remaining sufficiently fast to enable wide-scale screening of databases. We ensemble Squidly with BLAST to provide an efficient tool that annotates catalytic residues with high precision and recall for both in- and out-of-distribution sequences.</p>
</abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution>Schmidt Science Fellows</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Mora</surname>
<given-names>Ariane</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-2">
<funding-source>
<institution-wrap>
<institution>Australian Government Research Training Program</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Rieger</surname>
<given-names>William JF</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>As part of the eLife review process, we have made extensive revisions to the manuscript. Further details, including the reviewer comments and our responses are available at eLife.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Enzymes are efficient, non-toxic and sustainable alternatives to traditional chemical catalysts in applications such as bio-remediation, pharmaceutical and biofuel production<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Catalytic residues refer to the amino acids in the protein sequence directly involved in the mechanism of action. Catalytic residues are involved both directly and indirectly in catalysis; for example, they can stabilize transition states <sup><xref ref-type="bibr" rid="c2">2</xref></sup>, act as direct electron donors, or interact with secondary residues, water molecules, or cofactors that are then directly involved in the catalytic mechanism. As catalytic activity depends on specific structural conformations and sequence context, identifying catalytic residues remains a challenge.</p>
<p>Catalytic residue annotations can be used to infer protein function prediction and enable insights into mechanisms of action and evolutionary relationships <sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c5">5</xref></sup>. Annotations are used to facilitate enzyme engineering methods such as directed evolution and rational design <sup><xref ref-type="bibr" rid="c6">6</xref></sup>, and can help construct the theoretical catalytic sites known as theozymes that are used to condition <italic>de novo</italic> enzyme design <sup><xref ref-type="bibr" rid="c7">7</xref></sup>. Experimental methods to determine catalytic residues are time-consuming, labour-intensive, and costly, as they often require experimentally determined enzyme-substrate structures. To expedite annotation, computational approaches have been developed to predict catalytic residues from protein sequences or predicted structures <sup><xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>, using annotations in databases such as UniProt <sup><xref ref-type="bibr" rid="c12">12</xref></sup> and M-CSA, a manually curated enzyme mechanism database <sup><xref ref-type="bibr" rid="c13">13</xref></sup>. Existing computational methods can be broadly grouped into three categories: 1) similarity-based methods and sequence alignment, 2) structural methods, or 3) statistical and machine learning approaches.</p>
<p>Similarity-based approaches are effective when homologous sequences have been previously annotated. Structure-based approaches use a variety of geometric search algorithms to find potential binding pockets and then map sequence conservation scores to these regions to predict catalytic residues with higher specificity <sup><xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c16">16</xref></sup>. Geometric methods improve the accuracy of approaches reliant on homology, while the limitation of requiring homologous sequences remains. Data-driven approaches such as statistical models or machine learning algorithms offer a more flexible framework <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup> but are restricted by the quality and diversity of datasets available for training. The current machine learning (ML) state-of-the-art (SOTA) tools apply multimodal deep learning strategies and often emphasise the value of the structural modality. Shen et al. <sup><xref ref-type="bibr" rid="c10">10</xref></sup> combined an adaptive edge-gated graph attention neural network (AEGAN) with graphs constructed using the 3D structure and sequence-derived positional-specific scoring matrices. SCREEN leverages protein sequence and structure by combining graphical convolutional neural networks and contrastive learning on Protein Language Models (PLMs) <sup><xref ref-type="bibr" rid="c9">9</xref></sup>. EasIFA, developed by Wang et al. <sup><xref ref-type="bibr" rid="c11">11</xref></sup>, predicts both catalytic residues and binding sites by incorporating graph attention representations of chemical reactions with PLM information and structure representations. Despite the reported improvements on sequence-based methods by multi-modal approaches, current benchmarks inherently obscure the reliance on structural prediction as they often contain sequences with known structures, which are more likely to be predicted accurately by tools such as AlphaFold <sup><xref ref-type="bibr" rid="c18">18</xref></sup>. This bias may limit the evaluation of generalisability to unseen data, i.e. enzymes without experimentally derived structures. Furthermore, computing structures remains computationally intensive.</p>
<p>In comparison to structural models PLMs learn the distribution of amino acids across protein sequences. PLMs embed biologically meaningful representations that capture complex dependencies between residues within a protein sequence <sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup>. These models are trained in an unsupervised manner on large numbers (billions) of diverse sequences <sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>. Remote homology search and alignment algorithms based on PLM embeddings have been reported to outperform sequence similarity-based methods at low sequence identities <sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>. In enzyme function prediction, models highlight the utility of PLM embeddings in combination with contrastive learning, to achieve SOTA ML performance in the prediction of enzyme function (by proxy of enzyme classification numbers) and associated reactions catalysed <sup><xref ref-type="bibr" rid="c26">26</xref>–<xref ref-type="bibr" rid="c28">28</xref></sup>; however, similarity-based approaches (e.g. BLAST or reaction distance) perform comparably <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. We hypothesised that using PLM embeddings to predict catalytic residues would complement sequence similarity approaches in low homology settings.</p>
<p>Contrastive learning is a discriminative modelling approach that facilitates learning by distinguishing between paired observations in data that are “similar” or “different”, thus enabling information to be extracted from dense PLM embeddings <sup><xref ref-type="bibr" rid="c29">29</xref></sup>. Data engineering, such as choosing a dataset to enhance specific relationships <sup><xref ref-type="bibr" rid="c30">30</xref></sup>, enables domain expertise to inform pair schemes for training. For example, it is common to explicitly incorporate and maximise “hard negatives” (samples that are dissimilar but close in the latent space) to refine the model’s ability to discriminate subtle differences in the feature space <sup><xref ref-type="bibr" rid="c31">31</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup>.</p>
<p>Although pair schemes can be generated automatically in contrastive learning frameworks <sup><xref ref-type="bibr" rid="c30">30</xref></sup>, we posited that the physical principles of enzyme functions could be used to maximise hard negatives and create an accurate catalytic residue prediction algorithm, as in ref <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. Enzyme function can be grouped into different classes using the domain standard Enzyme Class (EC) ontology, which classifies enzymes into four levels based on the reactions they catalyse. The first level represents the general type of reaction (e.g., oxidoreductases), while subsequent levels add specificity, capturing information about substrates, cofactors, and products <sup><xref ref-type="bibr" rid="c34">34</xref></sup>. This hierarchical structure provides as basis for relationships between enzyme functions and mechanisms that can be used to create biologically meaningful pairs.</p>
<p>To overcome the compute limitations with structural methods and existing benchmarks, we developed Squidly and a new benchmark for low sequence identity catalytic residue predictions. We combined a contrastive learning framework on PLM per-token embeddings with a rationally designed hierarchical pair scheme to create a sequence-based catalytic residue predictor that is accurate, scalable, and able to generalise in the absence of sequence similarity. Squidly exceeds the performance of both BLAST and SOTA ML tools in existing benchmarks achieving a F1 of greater than 0.85 across enzyme families, in addition to generalising to low sequence identity enzymes, with an F1 of 0.64 on sequences with less than 30% identity. Squidly is over 100x faster than folding a structure, enabling it to be used to annotate existing databases, in metagenomics pipelines, and to filter <italic>de novo</italic> designed proteins. Finally, owing to the added interpretability of sequence similarity-based approaches we provide Squidly as an ensemble with BLAST, and suggest using Squidly where the sequence identity is less than 30%. We envision Squidly will be broadly applicable in the enzyme engineering field and will complement existing sequence homology and structure-based methods.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>AEGAN training and test datasets</title>
    <p>To ensure comparability with the SOTA tools for catalytic residue prediction, the same training, test, and benchmark datasets (Uni14230, Uni3750) were utilised as described in prior work <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. These were originally sourced from UniProt and M-CSA databases. Uni14230 and Uni3750 datasets also include computational and manually curated predictions from SwissProt. To reduce redundancy, the data was filtered to retain only sequences with less than 60% sequence identity, resulting in 8,784 sequences in the training set and 1,955 sequences in the test set. Additionally, Squidly was evaluated against six previously reported benchmark datasets <sup><xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c38">38</xref></sup>. The EF family, superfamily, fold, and HA superfamily datasets were specifically designed to include one representative sequence from each enzyme family, fold, or superfamily present in existing databases, and are thus not designed to predict “novel” active sites <sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>. The NN and PC datasets were both constructed to be structurally and functionally heterogeneous based on SCOP and CATH classifications <sup><xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup>. These datasets were generated prior to 2007 and are therefore limited in their catalytic diversity compared to the current databases. Both AEGAN and SCREEN were originally benchmarked against distinct subsets of these datasets, however, we found that they contained sequences with high identity to their respective training data, as detailed in <xref ref-type="table" rid="tbls2">Table S2</xref>. Squidly’s reported performance with these datasets is based on the Uni14230 training set to ensure a fair comparison with AEGAN and given the lower overall sequence similarity between the training and test sets. An ablation of Squidly’s pair scheme was performed with single independent models using the Uni3175 test set. All other benchmarks report Squidly’s ensemble performance.</p>
</sec>
<sec id="s2b">
<title>New benchmark: CataloDB</title>
<p>To address the shortcomings of existing benchmark datasets for the evaluation of catalytic residue predictions, we introduce a benchmarking dataset named CataloDB. CataloDB collects the annotation data available for enzymes in the SwissProt database. The total set of enzymes includes experimentally determined annotations in addition to reviewed sequences with known structures, see Appendix A.1 for more details. Sequences with greater than 90% sequence similarity, based on shared overlap, were removed using mmSeqs2 <sup><xref ref-type="bibr" rid="c39">39</xref></sup>. Structural and sequence-based clustering was performed on the filtered dataset to create a test set of 232 sequences with less than 30% sequence and structural identity to the training set (the remaining 5357 sequences). Structural identity was calculated using FoldSeek <sup><xref ref-type="bibr" rid="c40">40</xref></sup>. Sequences that were in any of the six commonly used benchmarks were omitted from the training data to ensure compatibility between CataloDB and existing benchmarks. Additional detailed descriptions of the data are available in the Appendix A.2 and see <xref rid="fig3" ref-type="fig">Figures 3F</xref> and <xref rid="fig3" ref-type="fig">3G</xref> for distribution of amino acids and EC classes in the benchmark dataset.</p>
<p>To enable direct comparison with the SOTA tool SCREEN, we retrained a lightweight version of the model using the CataloDB benchmark training data. This version of SCREEN, provided by the original authors, omits costly evolutionary features which are cumbersome for larger datasets. The authors report that these features have minimal impact on performance <sup><xref ref-type="bibr" rid="c9">9</xref></sup>. Minor changes were made to the source code to improve model training stability. Notably, we allowed optimal stopping to occur after 5 epochs of training, rather than after 200 epochs in the original source code as we observed exploding gradients at later epochs when trained on CataloDB.</p>
</sec>
<sec id="s2c">
<title>ESM2 embeddings</title>
<p>Each sequence was encoded by either the 3 billion parameter or 15 billion ESM2 model <sup><xref ref-type="bibr" rid="c23">23</xref></sup>. Pre-trained model weights were used without fine-tuning and per-token embeddings were extracted from the final layer of the encoder. These embeddings contain a vector of length 2560 (3B) or 5120 (15B) for each amino acid in the sequence. No normalization was applied to the embeddings before downstream use.</p>
</sec>
<sec id="s2d">
<title>Contrastive model</title>
<p>The supervised contrastive model is a network that takes per-residue embeddings as input (2560 or 5120 length vectors) and is trained to output a new embedding (N=128) of the residue. The model contains an initial dropout layer connected to the inputs, with a dropout rate of 10%, and two subsequent hidden layers of 1280 and 640 size each. During training, batches of 16,000 pairs are passed through the model and the distance between the output representations of each pair is calculated. The following cosine embedding loss function implemented by PyTorch is used to calculate the loss based on the distances:
<disp-formula id="disp-eqn-1">
<graphic xlink:href="659624v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub> are input vectors and <italic>y</italic> is the label, with <italic>y</italic> = 1 indicating a positive pair and <italic>y</italic> = <italic>−</italic>1 indicating a negative pair. When <italic>y</italic> = 1, the loss function rewards the model for maximising the cosine similarity, cos(<italic>x</italic><sub>1</sub><italic>, x</italic><sub>2</sub>). The loss function minimises the cosine similarity when <italic>y</italic> = <italic>−</italic>1. Importantly, the pair selection scheme optimises for hard-negative pairs, and thus the margin is set equal to 0 when defining the loss function.</p>
</sec>
<sec id="s2e">
<title>Reaction informed pair-mining</title>
<p>Positive and negative pairs were created to enable contrastive comparisons where in the simplest form positives are pairs of exclusively catalytic or exclusively non-catalytic residues while negatives include a catalytic and a non-catalytic residue. As all possible pair combinations of amino acids become intractable, we are required to reduce this to make training feasible, hence we test three methods to subsample from this pool of possible pairs. Specifically, we used information about the EC number of the sequence and amino acid characters to refine the pair selection process. For further information, see Appendix A.2.</p>
<p>Three pair schemes were created to test for the effect of the pair scheme, herein referred to as “reaction- informed pair-mining”. See <xref rid="fig1" ref-type="fig">Figure 1.B</xref> for a visual description.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1:</label>
<caption><title>Overview of the model.</title><p>A. Depicting a sequence being translated into per-token embeddings via ESM2, each per-token embedding is then translated into a smaller representation space by the MLP trained with contrastive loss (CL). Each residue from the CL model is then predicted as catalytic or not catalytic via a bidirectional LSTM network. The CL and LSTM models are ensembled (5 models) to improve performance and capture variance between the models. B. Representation of the pair-mining schemes depict the negative and positive classes for each scheme. Scheme 1 is the most permissive and is amino acid and EC <italic>unaware</italic>. Schemes 2 and 3 are more restrictive, requiring both the amino acids and the EC classes to be shared or different in the pair-mining scheme. Scheme 3 included additional negative pairs to separate EC numbers in the latent space.</p></caption>
<graphic xlink:href="659624v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Scheme 1 samples pairs across all sequences and residues such that positive pairs are pairs of residues that are either both catalytic or both non-catalytic.</p>
<p>Scheme 2 samples positive and negative pairs much like in scheme 1, except that each pair of residues must be the same amino acid and must have come from a sequence which has the same EC number.</p>
<p>Scheme 3 performs the same pair sampling as in scheme 2; however, scheme 3 also includes additional negative pairs that are either exclusively catalytic residues or exclusively non-catalytic and come from different EC numbers or amino acids.</p>
<p>To combat the inherent bias in the data towards certain EC numbers, sequences were grouped by EC number at the 2<italic><sup>nd</sup></italic> level, and an upper limit of 600 catalytic residues and 600 non-catalytic residues were sampled from each EC number. An equal number of positive and negative pairs were generated for each EC number. A parameter called the sample limit was set to limit the total number of pairs made for each group. The sample limit chosen was 16,000, providing a total of 8-10 million pairs for schemes 2 and 3. Scheme 1 was then trained with an equal total number of pairs.</p>
</sec>
<sec id="s2f">
<title>LSTM classifier</title>
<p>All LSTM classification models were trained using the same parameters. The models are bidirectional LSTM networks designed for binary classification of catalytic versus non-catalytic residues in protein sequences. The model takes as input 128-dimensional per-residue embeddings (from the contrastive model) that are processed by a 2-layer LSTM (hidden size: 128) with a dropout rate of 0.2, followed by a linear layer mapping the output to a single logit score, see <xref rid="fig1" ref-type="fig">Figure 1.A</xref>. Class imbalance is addressed by weighting the misclassification of catalytic residues 100 times higher during loss computation. The models were trained using a 90:10 train-validation split, with a batch size of 400 sequences, a learning rate of 0.01, and early stopping of 5 epochs was used. These parameters were not optimised. See <xref rid="fig1" ref-type="fig">Figure 1.A</xref> for a depiction of the model architecture.</p>
</sec>
<sec id="s2g">
<title>Squidly ensemble</title>
    <p>For each training dataset, five models were ensembled by taking the per-residue output from the LSTM and then combining these to compute the mean score, along with the predictive entropy for each residue<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. Users can specify a threshold for classifying a residue as catalytic (between 0 to 1.0): if the mean score exceeds this threshold, the residue is designated as “catalytic”. Users can optionally use the variance across the predictions to further filter for increased precision. The default thresholds were determined using the Uni3175 test datasets, see <xref rid="figs3" ref-type="fig">Figure S3</xref>.</p>
</sec>
<sec id="s2h">
<title>BLAST similarity-based search</title>
<p>BLAST is a local alignment search tool able to identify homologous sequences from a reference database<sup><xref ref-type="bibr" rid="c42">42</xref></sup>. For our study, we utilised diamond BLAST to search the training set for similar sequences to each test set <sup><xref ref-type="bibr" rid="c43">43</xref></sup>. The ultra-sensitive flag was used to identify sequences with low sequence similarities. Upon retrieval of results, the sequence with the highest sequence similarity was retained. This sequence was then aligned against the query using the alignment tool ClustalOmega <sup><xref ref-type="bibr" rid="c44">44</xref></sup>. The gapped alignment was used to infer the catalytic residues of the query sequence.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>ESM2 per-token embeddings contain catalytic residue specific variation</title>
<p>To explore the information content contained within per-residue embeddings associated with catalytic residues, we performed principal component analysis (PCA) on equally sampled catalytic and non- catalytic residues from EC 3.1 and 2.7, <xref rid="figs1" ref-type="fig">Figure S1</xref>. For EC 3.1, the first two principal components explained 5.24% and 1.86% of the variance for all residues, respectively. Although the explained variance in PCs 1 and 2 is minimal, the separation in PCA space suggests that there may be some variation related to catalytic roles, <xref rid="figs1" ref-type="fig">Figure S1</xref>. The separability from a linear transformation suggests that ESM2 per-residue embeddings could be used to capture catalytic-specific signals.</p>
</sec>
<sec id="s3b">
<title>Reaction informed pair-mining improves upon contrastive learning and state of the art prediction performance</title>
<p>We hypothesised that contrastive learning would present an effective approach to leverage the rich feature space of ESM2 embeddings to distinguish between catalytic and non-catalytic roles. We implemented a biologically informed hierarchical contrastive learning pair selection framework to select pairs which are informative at the per-residue level.</p>
</sec>
<sec id="s3c">
<title>Ablation of pair selection as a key design choice in Uni3175 performance</title>
<p>To demonstrate the impact of pair-scheme design choices, we systematically evaluated the performance on the Uni3175 dataset of 1) a baseline approach using ESM2 embeddings, 2) the standard approach of randomly selecting pairs, versus 3) our proposed reaction-informed pair-mining schemes. Scheme 1, see <xref rid="fig1" ref-type="fig">Figure 1.B</xref>, is a “control” scheme and uses a standard contrastive loss function with random pair selection. We observed that scheme 1 performs poorly overall on the Uni3175 dataset, <xref rid="fig2" ref-type="fig">Figure 2.A</xref>. Across all datasets, Scheme 1 achieved relatively low F1 scores, with a mean F1 score of 0.49 and 0.43 for the 3B and 15B ESM2 models. Scheme 1 failed to surpass the LSTM classification model using unchanged ESM2 embeddings, which had a mean F1 score of 0.73, <xref rid="fig2" ref-type="fig">Figure 2.A</xref>. Schemes 2 and 3, both of which employ reaction-informed pair-mining, exhibited improved performance relative to both Scheme 1 and the ESM2 baseline. Scheme 2 achieved mean F1 scores of 0.80 and 0.82 using the 3B and 15B ESM2 models, respectively. Scheme 3 improves upon scheme 2 by contrasting catalytic and non-catalytic sites based on distinct EC and amino acid labels, <xref rid="fig1" ref-type="fig">Figure 1.B</xref>. This resulted in mean F1 scores of 0.79 and 0.84. The best performing model had an F1 score of 0.86 achieved using Scheme 3 and the 15B ESM2 model. Thus, scheme 3 is used for the ensemble model of Squidly and reported in all future benchmarks.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2:</label>
<caption><title>Performance of Squidly pair-mining schemes on the Uni3175 dataset.</title><p>A. We compare the Squidly schemes without the ensemble with AEGAN, the tool by Shen et al., the authors of this dataset <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. Reaction informed schemes (Schemes 2 and 3) achieved the highest F1 scores, slightly surpassing AEGAN when using the 15B ESM2 model. The 3B ESM2 model remains competitive at a much lower computation cost. In comparison to the rationally informed pair schemes, the Scheme 1 (random pairing, S1) and raw embedding LSTM models performed poorly. B. Sequence similarity based on sequence identity to the closest sequence in the training set between each family. C. F1, recall, and precision for the ensemble of Squidly (3B, scheme 3) and BLAST with different sequence identity cut-offs. The cut-offs indicate the point at which to transition from a Squidly to a BLAST prediction. At 100% only Squidly is used for predictions, and at 0%, BLAST is used, unless there are no similar sequences. For sequences with less than 30% sequence identity (dashed black line), Squidly predictions are used. The dashed horizontal line represents the total score for BLAST on the specific dataset, while the dots represent the combined BLAST and Squidly ensemble approach. Squidly reports higher scores for recall, while BLAST is more precise. The performance is best when used as an ensemble.</p></caption>
<graphic xlink:href="659624v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The results from Uni3175 benchmark show that Squidly improved marginally upon the performance of previous work and the current SOTA ML prediction model AEGAN by Shen et al. <sup><xref ref-type="bibr" rid="c10">10</xref></sup> Squidly uses only sequence through ESM2 embeddings for prediction, which, compared to the homology and structural modalities included in AEGAN, makes for a more efficient predictor without compromising performance.</p>
<p>We additionally benchmarked Squidly with the multiclass classification benchmark from EasIFA on a further 66 sequences with less than 40% sequence identity to either training set <sup><xref ref-type="bibr" rid="c11">11</xref></sup>, see Appendix 4 for more details. EasIFA performs best, with a recall of 79.05% and precision of 90.22%. Squidly had a recall of 78.10% and precision of 73.87%.</p>
<p>Overall, the results on the Uni3175 dataset indicate that the rationally informed pair-mining strategy is an effective method for discriminative tasks using per-token ESM2 embeddings. It also indicates that sequence-based models using per-token PLM embeddings can compete with models using structural modalities. See Methods for details on each scheme design.</p>
</sec>
<sec id="s3d">
<title>Squidly outperforms other ML-based methods yet works best in conjunction with sequence similarity methods</title>
    <p>Squidly was evaluated against six previously reported benchmark datasets, namely the EF fold, family, and superfamily benchmarks, as well as the HA superfamily, NN, and PC benchmarks, see <xref>Table S1</xref> and Appendix 4 for description of each dataset, see <xref rid="fig2" ref-type="fig">Figure 2.B</xref> for sequence identity between test and train sets. Squidly’s performance exceeded those of SOTA models AEGAN and SCREEN, <xref rid="tbl1" ref-type="table">Table 1</xref>. When compared to the baseline of BLAST, we find that BLAST outperforms all other ML models except Squidly, likely due to the homologous sequences in the test and train sets, see Appendix A3, and <xref ref-type="table" rid="tbls2">Table S2</xref> for details.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>F1 scores of tools on 6 common benchmark datasets.</title></caption>
<graphic xlink:href="659624v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We hypothesised that BLAST would work best in instances where a similar sequence existed while Squidly would excel in low sequence similarity situations. To test for the optimal conditions for integration we varied the rate at which either predictor was used, <xref rid="fig2" ref-type="fig">Figure 2.C</xref>. For sequences with low sequence identity (&lt;50%), using Squidly improves the F1 score. However, Squidly has a higher likelihood than BLAST to record false positives where similar sequences exist while BLAST is more precise. The cutoff of 50% sequence identity was used to compare the performance of the ensemble method to existing tools. On all datasets using this cutoff an ensemble of BLAST and Squidly performs SOTA, <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
</sec>
<sec id="s3e">
<title>Squidly is scalable and fast</title>
<p>Squidly’s sequence-based approach reduces the overall computational cost of prediction to enable the screening of large databases. SOTA ML models rely on accurate structures as input, and thus structural prediction must be done prior to inference when screening databases such as metagenomic samples. Squidly’s smaller 3B model can be run locally and can predict roughly 10 sequences a second in our tests using an NVIDIA H100 GPU. In comparison to the current widely used structural generation tool Chai <sup><xref ref-type="bibr" rid="c45">45</xref></sup>, Squidly is approximately 200-fold faster (e.g. N=100, Squidly=44s, Chai=13263s). Note this under- represents the time for the structural tools to run, as they also require further prediction which also adds time, <xref rid="tbl2" ref-type="table">Table 2</xref>.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Resource usage measurements for Squidly models compared to structure prediction tool Chai-1 when predicting 100 sequences with an average length of 443 residues.</title></caption>
<graphic xlink:href="659624v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s3f">
<title>Squidly generalises to low identity sequences</title>
<p>To evaluate the capacity of Squidly to generalise to low identity sequences, we developed a benchmark dataset, CataloDB with low structural and sequence identity, see Methods and <xref rid="fig3" ref-type="fig">Figure 3.A, B</xref> for similarities between the test and train sets. This benchmark serves as an alternative test set that takes structural and sequence similarity into account, allowing for fair comparison between future structural and sequence-based tools and testing in low identity settings. Squidly 3B and 15B models showed similar performance on the benchmark with most sequences (N=148/232, N=138/232), recording at least one correct catalytic residue. For comparison, BLAST identifies only 68 sequences that have a catalytic residue recovered correctly. Both Squidly models performed similarly with F1 scores of (0.66, 0.69), precision of (0.86, 0.81), and recall of (0.52, 0.61) for the 15B and 3B models (respectively), while BLAST records an F1 of 0.37, recall of 0.25, and precision of 0.69, see <xref rid="fig3" ref-type="fig">Figure 3.C</xref>. We also retrain SCREEN, with performances recorded above BLAST however still below Squidly, <xref rid="fig3" ref-type="fig">Figure 3.C</xref>, see Methods for details.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3:</label>
<caption><p>A. BLAST sequence similarity to the training set for low identity CataloDB test set sequences. B. Structural similarity for the final test set, filtered on both structure and sequence. C. F1, recall, and precision for Squidly (scheme 3), SCREEN and BLAST on CataloDB. D. The number of CataloDB catalytic residues predicted (true positives) by Squidly (scheme 3) for the 6 available EC classes relative to the decreasing total number of true catalytic residues. E. The Precision, F1 and Recall score of Squidly predictions for the 6 available EC classes. F. The distribution of the identity of the catalytic residues is not even and represents the uneven distribution of the catalytic mechanism within the benchmark dataset. G. Almost 50% of the data points are in the well characterised class of EC 3, which are hydrolases. Neither EC 7 nor EC 6 is represented, which are translocases and ligases, respectively. H. Performance of the Squidly 15B ensemble model on the CataloDB benchmark under varying prediction and variance thresholds. Note, default 0.5 prediction cutoffs were used to evaluate Squidly’s performance in C.</p></caption>
<graphic xlink:href="659624v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Reflecting the bias of public data, CataloDB shows an over-representation of certain EC numbers and amino acids. To examine the effect of this bias, we stratified Squidly’s performance across the EC classes and observed a measurable drop for less well-represented sequences. Despite this, F1 scores remained relatively stable for two of the underrepresented classes, ECs 5 and 6, see <xref rid="fig3" ref-type="fig">Figure 3.E</xref>. However, we note that EC 6 was represented by only three sequences in the test data. Across all ECs, the models displayed a consistent trend of higher precision than recall, see <xref rid="fig3" ref-type="fig">Figure 3.D</xref>. Overall, these results suggest that Squidly can generalise to low identity settings and be used in predictions where traditional homology transfer is not possible. However, caution should be taken when applied to poorly represented enzyme classes such as EC 6 and 7.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>Rationally constraining pair generation in contrastive learning can improve model performance on biological data. Using publicly available EC number annotations and amino acid sequences, we categorised pairs of catalytic residues into distinct groups enriched with hard negatives to enable sequence- only prediction of catalytic residues. Squidly’s pair selection procedure produces distinct samples of the training data for training each model. The lightweight architecture of the per-token contrastive learning model, coupled with these independent data subsets, make Squidly well-suited to ensemble learning, as the diversity between model training data increases ensemble robustness and reduces correlated errors across models<sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup>. This approach led to substantial improvements compared to a standard LSTM classifier and typical pair-mining strategies. Squidly significantly outperformed other ML-based sequence methods and showed marginal improvement to SOTA ML-based catalytic residue prediction models that use structural data while running in a fraction of the time. Furthermore, Squidly performs comparably to BLAST when homologous sequences exist, yet also shows good performance in low-identity settings, where BLAST is unable to identify any similar sequences. Given the complementarity of BLAST and Squidly, we provide Squidly as an ensemble with sequence similarity, leveraging the interpretability of sequence similarity-based methods where homologous sequences exist, and Squidly, in out-of-distribution settings. The focus of this work was catalytic site prediction; however, of similar utility are combined binding and catalytic prediction models, such as EasIFA <sup><xref ref-type="bibr" rid="c11">11</xref></sup>, and therefore future work should additionally consider binding sites.</p>
<p>A limitation of this study is the inherent bias in publicly available data towards well studied enzyme classes. The bias likely confounds the perceived performance on less studied mechanisms, and in these instances the results should be considered with caution. For example, when ensembling models, we observed that the optimal classification threshold varied across EC numbers and ESM model variants, indicating sensitivity to the underlying distribution. This suggests that out-of-distribution datasets explicitly designed to reflect the intended application domain are needed for fair threshold calibration. A further challenge is the lack of a consolidated benchmark for catalytic residue prediction, making direct and fair comparison between tools difficult. EasIFA showed superior performance on the overlapping test set (n=66), indicating when structures are available, EasIFA is a good tool for catalytic residue prediction. However, we note that the multiclass classification objective and benchmarks used to evaluate EasIFA <sup><xref ref-type="bibr" rid="c11">11</xref></sup>, differences in sequence similarity to the test set, and small test set size limit these conclusions. Existing benchmark datasets do not capture the diversity of enzyme catalysis. Although we have provided a new benchmark that improves and expands upon previous datasets by using structural identity filtering, the core issues remain. CataloDB and current benchmarks are small and are likely enriched for model organisms with well-studied enzyme mechanisms. While low sequence and structural similarity samples are held out from training, they may not accurately represent the generalisability of these methods in out- of-distribution settings. Furthermore, the datasets often consist of sequences with known structures in the PDB, many of which were likely part of the AlphaFold training set or structurally similar clusters represented in the set. Tools also evaluate their performance using experimental, rather than predicted structures <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. Consequently, SOTA tools that rely on AlphaFold predicted structures tend to perform better in these benchmarks but are likely to underperform in practical applications involving truly unique and uncharacterised sequences. This issue also applies, albeit to a lesser extent, to the method proposed in this study, which utilises ESM2 representations. While ESM2’s training set is more diverse and comprehensive, it may not generalise to distant out-of-distribution sequences compared to the well-studied sequences used in the benchmarks. Although Squidly facilitates annotation in cases where similarity methods fail, its usability in practice should be considered provisional, as generalisability to less-studied enzyme lineages and catalytic mechanisms remain uncertain and may not extend beyond the well- represented mechanisms captured in current benchmarks.</p>
<p>Beyond these benchmark constraints, Squidly and related methods are constrained by the experimental annotations that underpin the training data. Currently there exists a limited set of experimentally validated annotations <sup><xref ref-type="bibr" rid="c13">13</xref></sup>. Datasets such as Uni14230 and CataloDB used for training in this study include SwissProt sequences with reviewed computational annotations to increase the size of the training data. However, this does not improve the diversity of catalytic mechanisms and enzyme sequences, as these methods reflect the inherent biases in databases towards well represented EC classes. Squidly, having learnt from this biased data, may indeed replicate this bias during inference and is therefore less likely to provide insights into novel or understudied catalytic mechanisms. Despite the limited data for catalytic residue prediction, we show that an ensemble approach of machine learning and sequence-similarity-based methods performs SOTA across multiple benchmark sets. By leveraging contrastive learning, we find that ML sequence-based predictions can generalize to low sequence/structural similarity settings. Finally, the methods reported in this paper that leverage rational data engineering for ensemble contrastive learning have broad implications for learning across biological data. Biological sequences are generated through evolutionary processes occurring over billions of years, resulting in hierarchically structured data that has significant variance between evolutionarily distant sequences. Moreover, mechanisms in biochemistry are often similar for related chemical reactions. By leveraging ontologies such as EC numbers, pairs can be selected rationally to improve training on proteins that are not only evolutionarily related but also share broader biochemical functions. A wide range of alternative ontologies exist that can capture different structures within biological data, suggesting that this approach is likely to be applicable across other biological classification tasks, particularly in studies utilising contrastive frameworks for per-token in- sequence classification <sup><xref ref-type="bibr" rid="c47">47</xref></sup>. Future studies could explore alternative pair combinations beyond EC numbers, as well as additional ontologies to better understand the utility of such data structures.</p>
</sec>


</body>
<back>
<sec id="s13" sec-type="supplementary">
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. S1:</label>
    <caption><title>Principal component analysis of per-residue embeddings from EC numbers 3.1 and 2.7.</title>
        <p>In the upper row, red colours indicate a catalytic residue, and blue, non-catalytic. In the lower row, residues are coloured by amino acid class which represent their structural and chemical properties. The x and y axes represent the first and second principal components in each subplot. A clear separation of catalytic and non-catalytic residues is seen in the principal components which explain the most variance in our data.</p></caption>
<graphic xlink:href="659624v2_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2:</label>
<caption><p>A. Recall, precision, and false positive rate (FPR) for AEGAN, EasIFA, and Squidly on the filtered EasIFA test subset. EasIFA shows balanced performance, Squidly is competitive given its sequence-only inputs, and AEGAN displays unusually low precision that may reflect a benchmarking artefact. B. Most of the test set are sequences from EC classes 2 and 3, with no representation for classes 4, 6 and 7. C. The differences in false positives, and true positives predicted by Squidly’s best model and EasIFA. Although the models agree with 100% of true positive predictions made by both tools, Squidly has a higher tendency for false positives, with many of the false positives being true binding sites.</p></caption>
<graphic xlink:href="659624v2_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3:</label>
    <caption><title>The default ensemble threshold for prediction and variance cutoffs of 0.6 and 0.225 (outlined in red) were selected to balance the precision and recall of the Squidly ensemble model on the Uni3175 benchmark under varying mean prediction and variance thresholds.</title>
        <p>Shown are F1 scores, precision, and recall for the 3B and 15B models. A. F1 of the 3B model, peaking at 0.70. B. F1 of the 15B model, peaking at 0.69. C. Precision of the 3B model shows that as the cutoff increases. D. Precision of the 15B model. E. Recall of the 3B model. F. Recall of the 15B model.</p></caption>
<graphic xlink:href="659624v2_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4:</label>
    <caption><title>Performance of the Squidly ensemble model on the CataloDB benchmark under varying mean prediction and variance thresholds.</title>
        <p>Shown are F1 scores, precision, and recall for the 3B and 15B models. A. F1 of the 3B model, peaking at 0.70. B. F1 of the 15B model, peaking at 0.69. C. Precision of the 3B model shows that as the cutoff increases. D. Precision of the 15B model. E. Recall of the 3B model. F. Recall of the 15B model. The default ensemble threshold for prediction and variance cutoffs of 0.6 and 0.225 (outlined in red) were selected using the Uni3175 benchmark.</p></caption>
<graphic xlink:href="659624v2_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5:</label>
    <caption><title>The maximum F1 score achievable with the Squidly ensemble model for each EC number in the CataloDB benchmark under varying mean prediction and variance thresholds.</title>
        <p>The default threshold of 0.6 and 0.225 determined using the uni3175 benchmark is relatively stable for each EC number. However, the limited data for EC 5 and 6 (N= 8, N=3, respectively) is such that users are recommended to further test the decision thresholds for their intended application.</p></caption>
<graphic xlink:href="659624v2_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5:</label>
    <caption><title>Amino acid distribution of Catalytic Residues in Dataset 1.</title>
        <p>Dataset 1 contains 3,873 catalytic residues, with 15 possible amino acids. The database is skewed towards certain amino acids which are commonly involved in catalysis.</p></caption>
<graphic xlink:href="659624v2_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6:</label>
    <caption><title>Dataset 1 EC Distribution.</title>
        <p>Dataset 1 is made up of 2,210 sequences from Swissprot. The sequences have experimental validation for the catalytic residue annotations. Validation sets are derived exclusively from this distribution. A clear bias exists for hydrolases (EC 3.X).</p></caption>
<graphic xlink:href="659624v2_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Figure S7:</label>
    <caption><title>Amino acid distribution of Catalytic Residues in Dataset 2.</title>
        <p>Catalytic residue distribution. Dataset 2 contains 10,564 catalytic residues, with 18 possible amino acids. Amino acids M, V and F are additions not seen in dataset 1, although they exist in very few numbers.</p></caption>
<graphic xlink:href="659624v2_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Figure S8:</label>
    <caption><title>Dataset 3 EC Distribution.</title>
        <p>Dataset 3 is made up of 48,625 sequences. The sequences are taken from all the available proteins with catalytic site annotations in Swissprot. A very similar distribution is seen between datasets 3 and 2, with a notable increase in EC 2 again.</p></caption>
<graphic xlink:href="659624v2_figs9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Figure S9:</label>
    <caption><title>Amino acid distribution of Catalytic Residues in Dataset 3.</title>
        <p>Dataset 3 contains 78,966 catalytic residues, with 19 possible catalytic amino acids. Isoleucine is an additional amino acid not seen in dataset 1 or 2. The distribution seen here is very similar to that of dataset 1 and 2.</p></caption>
<graphic xlink:href="659624v2_figs10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs11" position="float" orientation="portrait" fig-type="figure">
<label>Figure S10:</label>
    <caption><title>Dataset 3 EC Distribution.</title>
        <p>Dataset 3 is made up of 48,625 sequences. The sequences are taken from all the available proteins with catalytic site annotations in Swissprot. A very similar distribution is seen between datasets 3 and 2, with a notable increase in EC 2 again.</p></caption>
<graphic xlink:href="659624v2_figs11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Brief description of methods compared to Squidly in common benchmarks.</title></caption>
<graphic xlink:href="659624v2_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table S2.</label>
<caption><title>Benchmark datasets in prior work.</title></caption>
<graphic xlink:href="659624v2_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>All data associated with this manuscript is available at Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.15541320">https://doi.org/10.5281/zenodo.15541320</ext-link>) . Additionally, source code is available and documented on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/WRiegs/Squidly">https://github.com/WRiegs/Squidly</ext-link>). Data and code specifically used in the revised manuscript are also available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/WRiegs/Squidly/tree/main/revision">https://github.com/WRiegs/Squidly/tree/main/revision</ext-link>).</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>A.M. is supported by the Schmidt Science Fellows, in partnership with the Rhodes Trust. W.R. is supported by the Australian Government Research Training Program (RTP) Scholarship. The authors thank Charlie Trimble for his generous contribution to the project.</p>
</ack>
    <sec id="additional-info" sec-type="additional-information">
        <title>Additional information</title>
        <sec id="s7">
            <title>Author contributions</title>
            <p>W.R. and A.M. conceived the experiment(s), W.R. designed and implemented the model, W.R. and A.M. ran the experiment(s), analysed the results, and wrote the manuscript. M.B. and F.H.A reviewed the manuscript and provided input to the research.</p>
        </sec>
    </sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>(1)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reisenbauer</surname>, <given-names>J. C.</given-names></string-name>; <string-name><surname>Sicinski</surname>, <given-names>K. M.</given-names></string-name>; <string-name><surname>Arnold</surname>, <given-names>F. H</given-names></string-name></person-group>. <article-title>Catalyzing the Future: Recent Advances in Chemical Synthesis Using Enzymes</article-title>. <source>Current Opinion in Chemical Biology</source> <year>2024</year>, <volume>83</volume>, <fpage>102536</fpage>. <pub-id pub-id-type="doi">10.1016/j.cbpa.2024.102536</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>(2)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ribeiro</surname>, <given-names>A. J. M.</given-names></string-name>; <string-name><surname>Tyzack</surname>, <given-names>J. D.</given-names></string-name>; <string-name><surname>Borkakoti</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Holliday</surname>, <given-names>G. L.</given-names></string-name>; <string-name><surname>Thornton</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>A Global Analysis of Function and Conservation of Catalytic Residues in Enzymes</article-title>. <source>Journal of Biological Chemistry</source> <year>2020</year>, <volume>295</volume> (<issue>2</issue>), <fpage>314</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1074/jbc.REV119.006289</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>(3)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Precord</surname>, <given-names>T. W.</given-names></string-name>; <string-name><surname>Ramesh</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Dommaraju</surname>, <given-names>S. R.</given-names></string-name>; <string-name><surname>Harris</surname>, <given-names>L. A.</given-names></string-name>; <string-name><surname>Kille</surname>, <given-names>B. L.</given-names></string-name>; <string-name><surname>Mitchell</surname>, <given-names>D. A</given-names></string-name></person-group>. <article-title>Catalytic Site Proximity Profiling for Functional Unification of Sequence-Diverse Radical S- Adenosylmethionine Enzymes</article-title>. <source>ACS Bio &amp; Med Chem Au</source> <year>2023</year>, <volume>3</volume> (<issue>3</issue>), <fpage>240</fpage>–<lpage>251</lpage>. <pub-id pub-id-type="doi">10.1021/acsbiomedchemau.2c00085</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>(4)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martínez-Martínez</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Coscolín</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Santiago</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Chow</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Stogios</surname>, <given-names>P. J.</given-names></string-name>; <string-name><surname>Bargiela</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Gertler</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Navarro-Fernández</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Bollinger</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Thies</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Méndez-García</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Popovic</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Brown</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Chernikova</surname>, <given-names>T. N.</given-names></string-name>; <string-name><surname>García-Moyano</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Bjerga</surname>, <given-names>G. E. K.</given-names></string-name>; <string-name><surname>Pérez-García</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Hai</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Del Pozo</surname>, <given-names>M. V.</given-names></string-name>; <string-name><surname>Stokke</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Steen</surname>, <given-names>I. H.</given-names></string-name>; <string-name><surname>Cui</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Xu</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Nocek</surname>, <given-names>B. P.</given-names></string-name>; <string-name><surname>Alcaide</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Distaso</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Mesa</surname>, <given-names>V.</given-names></string-name>; <string-name><surname>Peláez</surname>, <given-names>A. I.</given-names></string-name>; <string-name><surname>Sánchez</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Buchholz</surname>, <given-names>P. C. F.</given-names></string-name>; <string-name><surname>Pleiss</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Fernández-Guerra</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Glöckner</surname>, <given-names>F. O.</given-names></string-name>; <string-name><surname>Golyshina</surname>, <given-names>O. V.</given-names></string-name>; <string-name><surname>Yakimov</surname>, <given-names>M. M.</given-names></string-name>; <string-name><surname>Savchenko</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Jaeger</surname>, <given-names>K.-E.</given-names></string-name>; <string-name><surname>Yakunin</surname>, <given-names>A. F.</given-names></string-name>; <string-name><surname>Streit</surname>, <given-names>W. R.</given-names></string-name>; <string-name><surname>Golyshin</surname>, <given-names>P. N.</given-names></string-name>; <string-name><surname>Guallar</surname>, <given-names>V.</given-names></string-name>; <string-name><surname>Ferrer</surname>, <given-names>M</given-names></string-name></person-group>.<article-title>; The INMARE Consortium. Determinants and Prediction of Esterase Substrate Promiscuity Patterns</article-title>. <source>ACS Chem. Biol</source>. <year>2018</year>, <volume>13</volume> (<issue>1</issue>), <fpage>225</fpage>–<lpage>234</lpage>. <pub-id pub-id-type="doi">10.1021/acschembio.7b00996</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>(5)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Yin</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Lee</surname>, <given-names>J. S.</given-names></string-name>; <string-name><surname>Parasuram</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Somarowthu</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Ondrechen</surname>, <given-names>M. J</given-names></string-name></person-group>. <article-title>Protein Function Annotation with Structurally Aligned Local Sites of Activity (SALSAs)</article-title>. <source>BMC Bioinformatics</source> <year>2013</year>, <volume>14</volume> (<issue>3</issue>), <fpage>S13</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-14-S3-S13</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>(6)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Ma</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Dalby</surname>, <given-names>P. A</given-names></string-name></person-group>. <article-title>Hot Spots-Making Directed Evolution Easier</article-title>. <source>Biotechnology Advances</source> <year>2022</year>, <volume>56</volume>, <fpage>107926</fpage>. <pub-id pub-id-type="doi">10.1016/j.biotechadv.2022.107926</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>(7)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ahern</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Yim</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Tischer</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Salike</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Woodbury</surname>, <given-names>S. M.</given-names></string-name>; <string-name><surname>Kim</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Kalvet</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Kipnis</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Coventry</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Altae-Tran</surname>, <given-names>H. R.</given-names></string-name>; <string-name><surname>Bauer</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Barzilay</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Jaakkola</surname>, <given-names>T. S.</given-names></string-name>; <string-name><surname>Krishna</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Baker</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Atom Level Enzyme Active Site Scaffolding Using RFdiffusion2</article-title> <source>bioRxiv</source>. <year>2025</year></mixed-citation></ref>
<ref id="c8"><label>(8)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Chen</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Shen</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Ruan</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Kurgan</surname>, <given-names>L</given-names></string-name></person-group>. <article-title>Accurate Sequence-Based Prediction of Catalytic Residues</article-title>. <source>Bioinformatics</source> <year>2008</year>, <volume>24</volume> (<issue>20</issue>), <fpage>2329</fpage>–<lpage>2338</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btn433</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>(9)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pan</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Bi</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Webb</surname>, <given-names>G. I.</given-names></string-name>; <string-name><surname>Gasser</surname>, <given-names>R. B.</given-names></string-name>; <string-name><surname>Kurgan</surname>, <given-names>L.</given-names></string-name>; <string-name><surname>Song</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>SCREEN: A Graph-Based Contrastive Learning Tool to Infer Catalytic Residues and Assess Enzyme Mutations</article-title>. <source>Genomics, Proteomics &amp; Bioinformatics</source> <year>2024</year>, <fpage>qzae094</fpage>. <pub-id pub-id-type="doi">10.1093/gpbjnl/qzae094</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>(10)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Zhang</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Long</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Chen</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Cui</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Chen</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Tan</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>A Highly Sensitive Model Based on Graph Neural Networks for Enzyme Key Catalytic Residue Prediction</article-title>. <source>Journal of Chemical Information and Modeling</source> <year>2023</year>, <volume>63</volume> (<issue>14</issue>), <fpage>4277</fpage>–<lpage>4290</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.3c00273</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>(11)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Yin</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Jiang</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Zhao</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Wu</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Zhang</surname>, <given-names>O.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Deng</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Liu</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Luo</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Han</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Hou</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Yao</surname>, <given-names>X.</given-names></string-name>; <string-name><surname>Hsieh</surname>, <given-names>C.-Y</given-names></string-name></person-group>. <article-title>Multi-Modal Deep Learning Enables Efficient and Accurate Annotation of Enzymatic Active Sites</article-title>. <source>Nat Commun</source> <year>2024</year>, <volume>15</volume> (<issue>1</issue>), <fpage>7348</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-024-51511-6</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>(12)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Consortium</surname>, <given-names>T. U</given-names></string-name></person-group>. <article-title>UniProt: The Universal Protein Knowledgebase in 2023</article-title>. <source>Nucleic Acids Research</source> <year>2022</year>, <volume>51</volume> (<issue>D1</issue>), <fpage>D523</fpage>–<lpage>D531</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkac1052</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>(13)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ribeiro</surname>, <given-names>A. J. M.</given-names></string-name>; <string-name><surname>Holliday</surname>, <given-names>G. L.</given-names></string-name>; <string-name><surname>Furnham</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Tyzack</surname>, <given-names>J. D.</given-names></string-name>; <string-name><surname>Ferris</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Thornton</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>Mechanism and Catalytic Site Atlas (M-CSA): A Database of Enzyme Reaction Mechanisms and Active Sites</article-title>. <source>Nucleic Acids Research</source> <year>2018</year>, <volume>46</volume> (<issue>D1</issue>), <fpage>D618</fpage>–<lpage>D623</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkx1012</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>(14)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glaser</surname>, <given-names>F.</given-names></string-name>; <string-name><surname>Pupko</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Paz</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Bell</surname>, <given-names>R. E.</given-names></string-name>; <string-name><surname>Bechor-Shental</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Martz</surname>, <given-names>E.</given-names></string-name>; <string-name><surname>Ben-Tal</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>ConSurf: Identification of Functional Regions in Proteins by Surface-Mapping of Phylogenetic Information</article-title>. <source>Bioinformatics</source> <year>2003</year>, <volume>19</volume> (<issue>1</issue>), <fpage>163</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/19.1.163</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>(15)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mayrose</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Graur</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Ben-Tal</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Pupko</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Comparison of Site-Specific Rate-Inference Methods for Protein Sequences: Empirical Bayesian Methods Are Superior</article-title>. <source>Molecular Biology and Evolution</source> <year>2004</year>, <volume>21</volume> (<issue>9</issue>), <fpage>1781</fpage>–<lpage>1791</lpage>. <pub-id pub-id-type="doi">10.1093/molbev/msh194</pub-id>.</mixed-citation></ref>
    <ref id="c16"><label>(16)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>del Sol</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Pazos</surname>, <given-names>F.</given-names></string-name>; <string-name><surname>Valencia</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Automatic Methods for Predicting Functionally Important Residues</article-title>. <source>Journal of Molecular Biology</source> <year>2003</year>, <volume>326</volume> (<issue>4</issue>), <fpage>1289</fpage>–<lpage>1302</lpage>. <pub-id pub-id-type="doi">10.1016/S0022-2836(02)01451-1</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>(17)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>F.</given-names></string-name>; <string-name><surname>Takemoto</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Haffari</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Akutsu</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Chou</surname>, <given-names>K.-C.</given-names></string-name>; <string-name><surname>Webb</surname>, <given-names>G. I</given-names></string-name></person-group>. <article-title>PREvaIL, an Integrative Approach for Inferring Catalytic Residues Using Sequence, Structural, and Network Features in a Machine-Learning Framework</article-title>. <source>Journal of Theoretical Biology</source> <year>2018</year>, <volume>443</volume>, <fpage>125</fpage>–<lpage>137</lpage>. <pub-id pub-id-type="doi">10.1016/j.jtbi.2018.01.023</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>(18)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jumper</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Evans</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Green</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Figurnov</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name>; <string-name><surname>Tunyasuvunakool</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Bates</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Žídek</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Potapenko</surname>, <given-names>A.</given-names></string-name> <etal>et al</etal></person-group>. <article-title>Highly Accurate Protein Structure Prediction with AlphaFold</article-title>. <source>nature</source> <year>2021</year>, <volume>596</volume> (<issue>7873</issue>), <fpage>583</fpage>–<lpage>589</lpage>.</mixed-citation></ref>
<ref id="c19"><label>(19)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tule</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Foley</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Boden</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Do Protein Language Models Learn Phylogeny?</article-title> <source>bioRxiv</source> <year>2024</year>. <pub-id pub-id-type="doi">10.1101/2024.09.23.614642</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>(20)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Meier</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Ovchinnikov</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Rives</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Transformer Protein Language Models Are Unsupervised Structure Learners</article-title>. <source>bioRxiv</source> <year>2020</year> <pub-id pub-id-type="doi">10.1101/2020.12.15.422761</pub-id>.</mixed-citation></ref>
    <ref id="c21"><label>(21)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Vig</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Madani</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Varshney</surname>, <given-names>L. R.</given-names></string-name>; <string-name><surname>Xiong</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Socher</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Rajani</surname>, <given-names>N. F.</given-names></string-name></person-group> <article-title>BERTology Meets Biology: Interpreting Attention in Protein Language Models</article-title>, <year>2021</year>. <source>arxiv</source> <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.15222">https://arxiv.org/abs/2006.15222</ext-link>.</mixed-citation></ref>
<ref id="c22"><label>(22)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Heinzinger</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Weissenow</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Sanchez</surname>, <given-names>J. G.</given-names></string-name>; <string-name><surname>Henkel</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Mirdita</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Steinegger</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Rost</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>Bilingual Language Model for Protein Sequence and Structure</article-title>. <source>bioRxiv</source> <year>2024</year>, 2023.07.23.550085. <pub-id pub-id-type="doi">10.1101/2023.07.23.550085</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>(23)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Akin</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Rao</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Hie</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Zhu</surname>, <given-names>Z.</given-names></string-name>; <string-name><surname>Lu</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Smetanin</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>Kabeli</surname>, <given-names>O.</given-names></string-name>; <string-name><surname>Shmueli</surname>, <given-names>Y</given-names></string-name>.; <string-name><surname>dos Santos Costa</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Fazel-Zarandi</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Sercu</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Candido</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Rives</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Evolutionary-Scale Prediction of Atomic-Level Protein Structure with a Language Model</article-title>. <source>Science</source> <year>2023</year>, <volume>379</volume> (<issue>6637</issue>), <fpage>1123</fpage>–<lpage>1130</lpage>. <pub-id pub-id-type="doi">10.1126/science.ade2574</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>(24)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kulikova</surname>, <given-names>A. V.</given-names></string-name>; <string-name><surname>Parker</surname>, <given-names>J. K.</given-names></string-name>; <string-name><surname>Davies</surname>, <given-names>B. W.</given-names></string-name>; <string-name><surname>Wilke</surname>, <given-names>C. O</given-names></string-name></person-group>. <article-title>Semantic Search Using Protein Large Language Models Detects Class II Microcins in Bacterial Genomes</article-title>. <source>bioRxiv</source> <year>2023</year>, 2023.11.15.567263. <pub-id pub-id-type="doi">10.1101/2023.11.15.567263</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>(25)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McWhite</surname>, <given-names>C. D.</given-names></string-name>; <string-name><surname>Armour-Garb</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Singh</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Leveraging Protein Language Models for Accurate Multiple Sequence Alignments</article-title>. <source>Genome Research</source> <year>2023</year>, <volume>33</volume> (<issue>7</issue>), <fpage>1145</fpage>–<lpage>1153</lpage>. <pub-id pub-id-type="doi">10.1101/gr.277675.123</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>(26)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Mikhael</surname>, <given-names>P. G.</given-names></string-name>; <string-name><surname>Chinn</surname>, <given-names>I.</given-names></string-name>; <string-name><surname>Barzilay</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>CLIPZyme: Reaction-Conditioned Virtual Screening of Enzymes</article-title>. <source>arXiv</source>, <year>2024</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2402.06748</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>(27)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>T.</given-names></string-name>; <string-name><surname>Cui</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>J. C.</given-names></string-name>; <string-name><surname>Luo</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Jiang</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Zhao</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>Enzyme Function Prediction Using Contrastive Learning</article-title>. <source>Science</source> <year>2023</year>, <volume>379</volume> (<issue>6639</issue>), <fpage>1358</fpage>–<lpage>1363</lpage>. <pub-id pub-id-type="doi">10.1126/science.adf2465</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>(28)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Mora</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Liu</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Wittmann</surname>, <given-names>B. J.</given-names></string-name>; <string-name><surname>Anandkumar</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Arnold</surname>, <given-names>F. H.</given-names></string-name>; <string-name><surname>Yue</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>CARE: A Benchmark Suite for the Classification and Retrieval of Enzymes</article-title>. <source>arXiv</source> <year>2025</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2406.15669</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>(29)</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chopra</surname>, <given-names>S.</given-names></string-name>; <string-name><surname>Hadsell</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>Learning a Similarity Metric Discriminatively, with Application to Face Verification</article-title>. In <source>2005 IEEE computer society conference on computer vision and pattern recognition (CVPR’05)</source>; <conf-name>IEEE</conf-name>, <year>2005</year>; Vol. <volume>1</volume>, pp <fpage>539</fpage>–<lpage>546</lpage>.</mixed-citation></ref>
<ref id="c30"><label>(30)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le-Khac</surname>, <given-names>P. H.</given-names></string-name>; <string-name><surname>Healy</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Smeaton</surname>, <given-names>A. F</given-names></string-name></person-group>. <article-title>Contrastive Representation Learning: A Framework and Review</article-title>. <source>IEEE Access</source> <year>2020</year>, <volume>8</volume>, <fpage>193907</fpage>–<lpage>193934</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3031549</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>(31)</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kalantidis</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Sariyildiz</surname>, <given-names>M. B.</given-names></string-name>; <string-name><surname>Pion</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Weinzaepfel</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Larlus</surname>, <given-names>D.</given-names></string-name></person-group><article-title>Hard Negative Mixing for Contrastive Learning</article-title>. In <conf-name>Advances in Neural Information Processing Systems</conf-name> <year>2020</year>; Vol. <volume>33</volume>, pp <fpage>21798</fpage>–<lpage>21809</lpage>.</mixed-citation></ref>
<ref id="c32"><label>(32)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khosla</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Teterwak</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Sarna</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Tian</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Isola</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Maschinot</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Liu</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Krishnan</surname>, <given-names>D.</given-names></string-name> . In ; <collab>Curran Associates, Inc</collab></person-group><article-title>Supervised Contrastive Learning</article-title>. In <source>Advances in Neural Information Processing Systems</source>; ., <year>2020</year>; Vol. <volume>33</volume>, pp <fpage>18661</fpage>–<lpage>18673</lpage>.</mixed-citation></ref>
<ref id="c33"><label>(33)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lin</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Qin</surname>, <given-names>G.</given-names></string-name>; <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Yang</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Zhou</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>An Effective Deployment of Contrastive Learning in Multi-Label Text Classification</article-title>. <source>arXiv</source> <year>2022</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2212.00552</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>(34)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonald</surname>, <given-names>A. G.</given-names></string-name>; <string-name><surname>Tipton</surname>, <given-names>K. F</given-names></string-name></person-group>. <article-title>Enzyme Nomenclature and Classification: The State of the Art</article-title>. <source>The FEBS Journal</source> <year>2023</year>, <volume>290</volume> (<issue>9</issue>), <fpage>2214</fpage>–<lpage>2231</lpage>. <pub-id pub-id-type="doi">10.1111/febs.16274</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>(35)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Youn</surname>, <given-names>E.</given-names></string-name>; <string-name><surname>Peters</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Radivojac</surname>, <given-names>P.</given-names></string-name>; <string-name><surname>Mooney</surname>, <given-names>S. D</given-names></string-name></person-group>. <article-title>Evaluation of Features for Catalytic Residue Prediction in Novel Folds</article-title>. <source>Protein Science</source> <year>2007</year>, <volume>16</volume> (<issue>2</issue>), <fpage>216</fpage>–<lpage>226</lpage>. <pub-id pub-id-type="doi">10.1110/ps.062523907</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>(36)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chea</surname>, <given-names>E.</given-names></string-name>; <string-name><surname>Livesay</surname>, <given-names>D. R</given-names></string-name></person-group>. <article-title>How Accurate and Statistically Robust Are Catalytic Site Predictions Based on Closeness Centrality?</article-title> <source>BMC Bioinformatics</source> <year>2007</year>, <volume>8</volume> (<issue>1</issue>), <fpage>153</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-8-153</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>(37)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bartlett</surname>, <given-names>G. J.</given-names></string-name>; <string-name><surname>Porter</surname>, <given-names>C. T.</given-names></string-name>; <string-name><surname>Borkakoti</surname>, <given-names>N.</given-names></string-name>; <string-name><surname>Thornton</surname>, <given-names>J. M</given-names></string-name></person-group>. <article-title>Analysis of Catalytic Residues in Enzyme Active Sites</article-title>. <source>Journal of Molecular Biology</source> <year>2002</year>, <volume>324</volume> (<issue>1</issue>), <fpage>105</fpage>–<lpage>121</lpage>. <pub-id pub-id-type="doi">10.1016/S0022-2836(02)01036-7</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>(38)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petrova</surname>, <given-names>N. V.</given-names></string-name>; <string-name><surname>Wu</surname>, <given-names>C. H</given-names></string-name></person-group>. <article-title>Prediction of Catalytic Residues Using Support Vector Machine with Selected Protein Sequence and Structural Properties</article-title>. <source>BMC Bioinformatics</source> <year>2006</year>, <volume>7</volume> (<issue>1</issue>), <fpage>312</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-7-312</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>(39)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinegger</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Söding</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>MMseqs2 Enables Sensitive Protein Sequence Searching for the Analysis of Massive Data Sets</article-title>. <source>Nature Biotechnology</source> <year>2017</year>, <volume>35</volume> (<issue>11</issue>), <fpage>1026</fpage>–<lpage>1028</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.3988</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>(40)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Kempen</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Kim</surname>, <given-names>S. S.</given-names></string-name>; <string-name><surname>Tumescheit</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Mirdita</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Gilchrist</surname>, <given-names>C. L. M.</given-names></string-name>; <string-name><surname>Söding</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Steinegger</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Fast and Accurate Protein Structure Search with Foldseek</article-title>. <source>Nat Biotechnol</source> <year>2024</year>, <volume>42</volume> (<issue>2</issue>), <fpage>243</fpage>–<lpage>246</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-023-01773-0</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>(41)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lakshminarayanan</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Blundell</surname>, <given-names>C.</given-names></string-name> . In ; <collab>Curran Associates, Inc</collab></person-group><article-title>Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles</article-title>. In <source>Advances in Neural Information Processing Systems</source>; ., <year>2017</year>; Vol. <volume>30</volume>.</mixed-citation></ref>
<ref id="c42"><label>(42)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altschul</surname>, <given-names>S. F.</given-names></string-name>; <string-name><surname>Gish</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Miller</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Myers</surname>, <given-names>E. W.</given-names></string-name>; <string-name><surname>Lipman</surname>, <given-names>D. J</given-names></string-name></person-group>. <article-title>Basic Local Alignment Search Tool</article-title>. <source>J Mol Biol</source> <year>1990</year>, <volume>215</volume> (<issue>3</issue>), <fpage>403</fpage>–<lpage>410</lpage>. <pub-id pub-id-type="doi">10.1016/S0022-2836(05)80360-2</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>(43)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buchfink</surname>, <given-names>B.</given-names></string-name>; <string-name><surname>Xie</surname>, <given-names>C.</given-names></string-name>; <string-name><surname>Huson</surname>, <given-names>D. H</given-names></string-name></person-group>. <article-title>Fast and Sensitive Protein Alignment Using DIAMOND</article-title>. <source>Nat Methods</source> <year>2015</year>, <volume>12</volume> (<issue>1</issue>), <fpage>59</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3176</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>(44)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sievers</surname>, <given-names>F.</given-names></string-name>; <string-name><surname>Wilm</surname>, <given-names>A.</given-names></string-name>; <string-name><surname>Dineen</surname>, <given-names>D.</given-names></string-name>; <string-name><surname>Gibson</surname>, <given-names>T. J.</given-names></string-name>; <string-name><surname>Karplus</surname>, <given-names>K.</given-names></string-name>; <string-name><surname>Li</surname>, <given-names>W.</given-names></string-name>; <string-name><surname>Lopez</surname>, <given-names>R.</given-names></string-name>; <string-name><surname>McWilliam</surname>, <given-names>H.</given-names></string-name>; <string-name><surname>Remmert</surname>, <given-names>M.</given-names></string-name>; <string-name><surname>Söding</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Thompson</surname>, <given-names>J. D.</given-names></string-name>; <string-name><surname>Higgins</surname>, <given-names>D. G. Fast</given-names></string-name></person-group>, <article-title>Scalable Generation of High- quality Protein Multiple Sequence Alignments Using Clustal Omega</article-title>. <source>Molecular Systems Biology</source> <year>2011</year>, <volume>7</volume> (<issue>1</issue>), <fpage>539</fpage>. <pub-id pub-id-type="doi">10.1038/msb.2011.75</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>(45)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><collab>Chai Discovery</collab></person-group>. <article-title>Chai-1: Decoding the Molecular Interactions of Life</article-title>. <source>bioRxiv</source> <year>2024</year>. <pub-id pub-id-type="doi">10.1101/2024.10.10.615955</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>(46)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hansen</surname>, <given-names>L. K.</given-names></string-name>; <string-name><surname>Salamon</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Neural Network Ensembles</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <year>1990</year>, <volume>12</volume> (<issue>10</issue>), <fpage>993</fpage>–<lpage>1001</lpage>. <pub-id pub-id-type="doi">10.1109/34.58871</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>(47)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>; <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>; <string-name><surname>Tian</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>Protein-Small Molecule Binding Site Prediction Based on a Pre-Trained Protein Language Model with Contrastive Learning</article-title>. <source>Journal of Cheminformatics</source> <year>2024</year>, <volume>16</volume> (<issue>1</issue>), <fpage>125</fpage>. <pub-id pub-id-type="doi">10.1186/s13321-024-00920-2</pub-id>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
    <title>Appendix 1</title>
<sec id="s8">
<label>A.1</label>
<title>ESM2 per-token embeddings contain catalytic residue specific variation</title>
<p>To illustrate the information content of per-residue embeddings for catalytic residue differentiation, we performed principal component analysis (PCA) on equally sampled catalytic and non-catalytic residues from EC 3.1 and 2.7, <xref rid="figs1" ref-type="fig">Figure S1</xref>. For EC 3.1, the first two principal components explained 5.24% and 1.86% of the variance for all residues, respectively. Although the explained variance in PCs 1 and 2 is minimal, the separation in PCA space suggests that the largest two signatures of linear variation could be related to catalytic roles, catalytic (red-coloured) and non-catalytic (blue-coloured) residues, <xref rid="figs1" ref-type="fig">Figure S1</xref>. This exploratory analysis motivated us to test this hypothesis and proceed with per-residue embeddings for our contrastive learning model.</p>
</sec>
</app>
    <app id="app2">
        <title>Appendix 2</title>
<sec id="s9">
<label>A.2</label>
<title>Rationale for developing CataloDB</title>
<p>To enable comparisons to existing work, we utilized the AEGAN training and test datasets as described above. However, the limitations identified motivated us to create a new benchmark, CataloDB, for future evaluations. The pre-existing benchmarks were published prior to 2007 and were developed by selecting proteins with active site annotations representing unique protein families, superfamilies, or folds. Their construction lacked standardized metrics for ensuring appropriate similarity separation between training and test data. The continued use of these historical benchmarks likely persists to maintain backward compatibility in comparisons with earlier methods. However, these six benchmarks predominantly represent well-studied folds and families, making it challenging to create properly separated training and test sets.</p>
<p>Our results demonstrate that BLAST already performs comparably or better when predicting catalytic residues in sequences with greater than 0.35 identity. Therefore, we believe machine learning methods should aim to develop tools that generalize effectively to low-identity sequences. CataloDB addresses this challenge, and the concerns by containing test sequences with strictly less than 0.3 sequence and structure identity to the training set. This benchmark is more extensive than previous collections while still allowing for a larger training set with a higher redundancy threshold (90%), enabling machine learning methods to better learn from the underlying data distribution.</p>
<sec id="s9a">
<title>Evaluation of Swissprot data for use in CataloDB</title>
<p>To overcome the limited availability of experimentally validated catalytic residue annotations, annotations from sequences which have been reviewed by Swissprot’s expert review panel have been included in training and test data in recent machine learning tools <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. However, the assumption that these annotations are valid is not a given. Swissprot provides limited information to clarify how their review process for catalytic mechanisms is conducted <sup><xref ref-type="bibr" rid="c12">12</xref></sup>. These annotations may have been generated manually or by computational predictions and likely contain the same bias present in the original experimentally validated set. Here we present initial analysis done, using an earlier version of Squidly, to determine whether reviewed sequences improved performance, and thus, the quality of training data.</p>
<p>Three datasets were curated to evaluate the impact of reviewed sequences on training catalytic residue prediction models. Dataset 1 contained 2,209 sequences with only experimentally validated catalytic residue annotations. Dataset 2 included 6,437 sequences, including experimentally validated annotations and sequences with catalytic residues supported by known structures. Dataset 3 included 99,926 sequences comprising all catalytic residue annotations in the SwissProt database. Redundancy reduction removed sequences with over 90% sequence identity. After filtering, Dataset 1 was reduced to 2,030 sequences, Dataset 2 to 5,921 sequences, and Dataset 3 had the highest proportion of redundant sequences with only 57,698 sequences left after filtering. See <xref rid="tbl2" ref-type="table">Table 2</xref> for details.</p>
    <p>Contrary to the expectation that increasing data size would improve model performance, we found that increasing the available training data by including reviewed sequences from SwissProt does not improve the overall bias or variance of the available experimental data. Figures <xref ref-type="fig" rid="figs7">S6</xref>, <xref ref-type="fig" rid="figs9">S8</xref> and <xref ref-type="fig" rid="figs11">S10</xref> show the EC distributions (EC numbers up to the second tier) across the datasets. All three datasets are biased toward hydrolases (EC 3), followed by classes 2, 1, 4, 5, 6, and 7. The relative abundance of EC class 2 increases between dataset 1, dataset 2, and dataset 3, while other classes remain relatively constant. This trend suggests that the additional sequences in the reviewed datasets likely originate from similarity-based prediction methods that incorporate experimental data, thus maintaining the relative abundance in EC representation despite increasing dataset size.</p>
    <p>Figures <xref ref-type="fig" rid="figs4">S4</xref>, <xref ref-type="fig" rid="figs7">S6</xref> and <xref ref-type="fig" rid="figs9">S8</xref> compare the distributions of amino acids acting as catalytic residues across the datasets. The distributions are dominated by histidine, aspartic acid, glutamic acid, cysteine, and serine, in line with the dominant classes being hydrolases. Dataset 2 and dataset 3 show a relative increase in aspartic acid abundance compared to dataset 1. Despite this, and the much larger size of datasets 2 and 3, their amino acid distributions closely resemble dataset 1, indicating that reviewed datasets contribute little diversity in rare catalytic residues, which may correspond to mechanisms that are less studied. Overall, these findings suggest that training the models using the additional data will not greatly improve the performance of the models when generalising to sequences with low identity in the experimental ground truth set. Based on our analyses we use dataset 2 for CataloDB.</p>
</sec>
</sec>
</app>
    <app id="app3">
        <title>Appendix 3</title>
<sec id="s10">
<label>A.3</label>
<title>Reaction-informed pair schemes</title>
<p>By performing contrastive learning with pairs, we move from having too few samples to having too many samples. The large pool of amino acid pairs in our dataset varies in how informative each pair is for the model. Therefore, we must find the most effective ways to down sample and select the most informative pairs for our model to learn from. Particularly, we want to maximise the proportion of hard negative pairs in the training data.</p>
<p>The Uni14320 training dataset contains 8,735 non-redundant sequences from Dataset 1 (experimentally validated) with an average sequence length of 407 residues. Given the total pool of 3,555,145 residues, a total number of 6.32 × 10<sup>12</sup> unique pairwise comparisons can be made, as determined by the binomial coefficient <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="659624v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. We developed pair schemes 2 and 3 to capture the most informative pairs from this available data. Maximising hard negatives improves the model’s ability to discriminate catalytic and non- catalytic residues in a more generalisable way. If two pairs of residue embeddings are inherently very distinct from one another, the model will be unlikely to learn much from them. This is not just because our loss function will penalise the model less during training, but because the relationships extracted from the feature space that are used to separate these two inputs might not necessarily be related to our primary objective.</p>
<p>Reaction informed pair-mining assumes that there must be key biological contexts which create harder negative pairs for the model to differentiate. First, we make the basic assumption that the model will be unable to correctly discriminate between similar amino acids pairs. It is likely that a contrastive model will find pairs individual amino acids, such as histidine, harder to contrast, because the unique structure and properties of each amino acid determine the catalytic or non-catalytic roles in which these amino acids play within enzymes. Therefore, we decided to implicitly include the amino acid character of each catalytic site into the pair sampling scheme.</p>
<p>Another assumption we made was inspired by the use of contrastive learning in another catalytic site prediction tool developed by Tong Pan et al. <sup><xref ref-type="bibr" rid="c9">9</xref></sup>. Their contrastive model creates EC specific representations of sequences for further use in a convolutional graph neural network classification model. Their contrastive model, however, did not try to use these contexts to better differentiate between catalytic and non-catalytic sites in their model, but as a support to ensure the information about the sequence EC is propagated through their multimodal system. EC numbers separate enzymes based on the reactions they perform. Not only that, but lower-level EC numbers group enzymes by the types of substrate bonds they cleave, or mechanisms by which they catalyse reactions. Inspired by this idea, we leveraged EC numbers to sample hard negative pairs. We specifically chose to represent sequences with only the first two levels of their EC-numbers so that there is sufficient variety in the data that represents each label.</p>
</sec>
    </app>
    <app id="app4">
        <title>Appendix 4</title>
<sec id="s11">
<label>A.4</label>
<title>Evaluation of Existing Benchmarks</title>
<p>The six standard benchmarks traditionally used for catalytic residue prediction exhibit methodological inconsistencies in the literature. In our comparative analysis, we discovered that state-of-the-art methods benchmarked in this study – specifically SCREEN and AEGAN utilized different independent subsets of these benchmarks, complicating direct performance comparisons. According to the SCREEN authors, data preparation involved clustering the EF family and M-CSA training data to less than 0.4 sequence identity, selecting unique cluster representatives for training and validation. The authors reported that they “excluded enzymes from these five test datasets from our training/validation dataset.”</p>
<p>Our analysis identified that up to 85.4% of the test sequences had greater than 0.9 identity to the training set in the already filtered subsets provided by the authors. Notably, the high sequence similarity appears reduced in the EF superfamily and EF fold datasets, likely resulting from the authors’ use of the closely related EF family dataset during training set curation. The test results published by SCREEN align with our observations, showing a marked increase in F1 score when comparing the HA, NN, and PC datasets with the EF datasets, corresponding to their differing levels of data leakage.</p>
<p>Additionally, the curation process employed in SCREEN for the training data appears unnecessarily stringent, with the average closest training sequence similarity measured at only 0.21. This sparse training data likely impacts machine learning models attempting to fit such data distributions effectively. We hypothesize this factor may explain why AEGAN reportedly performed poorly when retrained using this data in the SCREEN authors’ experiments.</p>
</sec>
    </app>
    <app id="app5">
        <title>Appendix 5</title>
<sec id="s12">
<label>A.5</label>
<title>Benchmarking Squidly against EasIFA multiclass classification</title>
<p>EasIFA provides both a multiclass classification model and a binary classifier that labels residues as active sites. The binary labels include catalytic, binding, and “other” residues from UniProt, whereas Squidly, AEGAN and SCREEN are trained specifically to identify catalytic residues. This definitional difference makes direct comparisons challenging, owing to the difference in training dataset between the models.</p>
    <p>Here we re-evaluated the performance of EasIFA, AEGAN, and Squidly using the pretrained EasIFA multiclass classifier and its benchmark dataset. To minimize overlap with training data, we retained only sequences that had &lt;40% sequence identity to the Squidly and AEGAN training data (Uni14230) and to EasIFA’s training data, and that contained at least one annotated catalytic residue. After filtering, 66 sequences remained, these include no representative sequences from EC classes 4, 6 and 7, See <xref ref-type="fig" rid="figs2">Figure S2B</xref>. The subset had an average identity of 0.082 to the Squidly/AEGAN training sets and 0.238 to EasIFA’s training set.</p>
<p>In the EasIFA manuscript, recall was calculated as zero for sequences without catalytic residues, which artificially reduced the reported averages. As such we recalculated recall per-sequence after removing sequences with no catalytic residues.</p>
<p>The benchmark yielded the following outcomes. EasIFA achieved a recall of 79.42 and a precision of 79.55, while the best-performing Squidly model reached a recall of 77.40 with a precision of 65.76. AEGAN recorded a high recall of 91.78 but an unusually low precision of 7.85, which may indicate a benchmarking artefact rather than an inherent property of the model. Across the nine independently initialized Squidly models tested, performance showed considerable variance, with recall averaging 58.74% (standard deviation 11.06%) and precision averaging 54.00% (standard deviation 6.82%). EasIFA’s benchmark did not provide variance estimates, suggesting that the best-performing models were also reported there.</p>
    <p>Squidly and AEGAN tend to agree with each other, having on average the same true positive and false positive predictions 96.20% of the time (TP=100.00%, FP=92.41%). Many of the false positives predicted by Squidly were binding sites, See <xref ref-type="fig" rid="figs2">Figure 2C</xref>. This tendency is perhaps because of the overlap between binding and catalytic residue annotations sometimes found in UniProt. Overall, EasIFA shows strong performance in this benchmark. However, due to the dataset limitations, differences in training data and label definition, and EasIFA’s higher sequence identity to the test set, these results should be interpreted cautiously. This benchmark shows that EasIFA may have improved precision compared to Squidly and AEGAN. Additionally, the EasIFA model can predict binding sites alongside Catalytic Residues, which is advantageous when studying cofactor-dependent enzymes using EasIFA’s method.</p>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108186.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Graña</surname>
<given-names>Martin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Institut Pasteur de Montevideo</institution>
</institution-wrap>
<city>Montevideo</city>
<country>Uruguay</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> contribution to enzyme annotation offers a deep learning framework for catalytic site prediction. Integrating biochemical knowledge with large language models, the authors demonstrate how to extract meaningful information from sequence alone. They introduce Squidly, a freely available new ML modeling framework, that outperforms existing tools on standard benchmarks, including the CataloDB dataset. The evidence is <bold>convincing</bold>, with an extensively and carefully addressed narrative upon revision.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108186.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this well-written and timely manuscript, Rieger et al. introduce Squidly, a new deep learning framework for catalytic residue prediction. The novelty of the work lies in the aspect of integrating per-residue embeddings from large protein language models (ESM2) with a biology-informed contrastive learning scheme that leverages enzyme class information to rationally mine hard positive/negative pairs. Importantly, the method avoids reliance on the use of predicted 3D structures, enabling scalability, speed, and broad applicability. The authors show that Squidly outperforms existing ML-based tools and even BLAST in certain settings, while an ensemble with BLAST achieves state-of-the-art performance across multiple benchmarks. Additionally, the introduction of the CataloDB benchmark, designed to test generalization at low sequence and structural identity, represents another important contribution of this work.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108186.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors aim to develop Squidly, a sequence-only catalytic residue prediction method. By combining protein language model (ESM2) embedding with a biologically inspired contrastive learning pairing strategy, they achieve efficient and scalable predictions without relying on three-dimensional structure. Overall, the authors largely achieved their stated objectives, and the results generally support their conclusions. This research has the potential to advance the fields of enzyme functional annotation and protein design, particularly in the context of screening large-scale sequence databases and unstructured data. However, the data and methods are still limited by the biases of current public databases, so the interpretation of predictions requires specific biological context and experimental validation.</p>
<p>Strengths:</p>
<p>The strengths of this work include the innovative methodological incorporation of EC classification information for &quot;reaction-informed&quot; sample pairing, thereby enhancing the discriminative power of contrastive learning. Results demonstrate that Squidly outperforms existing machine learning methods on multiple benchmarks and is significantly faster than structure prediction tools, demonstrating its practicality.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108186.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Rieger</surname>
<given-names>William JF</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-9574-3468</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Boden</surname>
<given-names>Mikael</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3548-268X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Arnold</surname>
<given-names>Frances</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4027-364X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mora</surname>
<given-names>Ariane</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1331-8192</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1:</bold></p>
<p>In this well-written and timely manuscript, Rieger et al. introduce Squidly, a new deep learning framework for catalytic residue prediction. The novelty of the work lies in the aspect of integrating per-residue embeddings from large protein language models (ESM2) with a biology-informed contrastive learning scheme that leverages enzyme class information to rationally mine hard positive/negative pairs. Importantly, the method avoids reliance on the use of predicted 3D structures, enabling scalability, speed, and broad applicability. The authors show that Squidly outperforms existing ML-based tools and even BLAST in certain settings, while an ensemble with BLAST achieves state-of-the-art performance across multiple benchmarks. Additionally, the introduction of the CataloDB benchmark, designed to test generalization at low sequence and structural identity, represents another important contribution of this work.</p>
</disp-quote>
<p>We thank the reviewer for their constructive and encouraging assessment of the manuscript. We appreciate the recognition of Squidly’s biology-informed contrastive learning framework with ESM2 embeddings, its scalability through the avoidance of predicted 3D structures, and the contribution of the CataloDB benchmark. We are pleased that the reviewer finds these aspects to be of value, and their comments will help us in further clarifying the strengths and scope of the work.</p>
<disp-quote content-type="editor-comment">
<p>The manuscript acknowledges biases in EC class representation, particularly the enrichment for hydrolases. While CataloDB addresses some of these issues, the strong imbalance across enzyme classes may still limit conclusions about generalization. Could the authors provide per-class performance metrics, especially for underrepresented EC classes?</p>
</disp-quote>
<p>We thank the reviewer for raising this point. We agree that per-class performance metrics provide important insight into generalizability across underrepresented EC classes. In response, we have updated Figure 3 to include two additional panels: (i) per-EC F1, precision and recall scores, and (ii) a relative display of true positives against the total number of predictable catalytic residues. These additions allow the class imbalance to be more directly interpretable. We have also revised the text between lines 316-321 to better contextualize our generalizability claims in light of these results.</p>
<disp-quote content-type="editor-comment">
<p>An ablation analysis would be valuable to demonstrate how specific design choices in the algorithm contribute to capturing catalytic residue patterns in enzymes.</p>
</disp-quote>
<p>We agree an ablation analysis is beneficial to show the benefits of a specific approach. We consider the main design choice in Squidly to be how we select the training pairs, hence we chose a standard design choice for the contrastive learning model. We tested the effect of different pair schemes on performance and report the results in Figure 2A and lines 244258. These results are a targeted ablation in which we evaluate Squidly against AEGAN using the AEGAN training and test datasets, while systematically varying the ESM2 model size and pair-mining scheme. As a baseline, we included the LSTM trained directly on ESM2 embeddings and random pair selection.  We showed that indeed the choice of pairs has a large impact on performance, which is significantly improved when compared to naïve pairing. This comparison suggests that performance gains are attributable to reactioninformed pair-mining strategies. We recognize that the way these results were originally presented made this ablation less clear. We have revised the wording in the Results section (lines 244-247) and updated the caption to Figure 2A to emphasize the purpose of this section of the paper.</p>
<disp-quote content-type="editor-comment">
<p>The statement that users can optionally use uncertainty to filter predictions is promising but underdeveloped. How should predictive entropy values be interpreted in practice? Is there an empirical threshold that separates high- from low-confidence predictions? A demonstration of how uncertainty filtering shifts the trade-off between false positives and false negatives would clarify the practical utility of this feature.</p>
</disp-quote>
<p>Thank you for the suggestion. Your comment prompted us to consider what is the best way to represent the uncertainty and, additionally, what is the best metric to return to users and how to visualize the results. Based on this, we included several new figures (Figure 3H and Supplementary Figures S3-5). We used these figures to select the cutoffs (mean prediction of 0.6, and variance &lt; 0.225) which were then set as the defaults in Squidly, and used in all subsequent analyses. The effect of these cutoffs is most evident in the tradeoff of precision and recall. Hence users may opt to select their own filters based on the mean prediction and variance across the predictions, and these cutoffs can be passed as command line parameters to Squidly. The choice to use a consistent default cutoff selected using the Uni3175 benchmark has slightly improved the reported performance for the benchmarks seen in table 1, and figure 3C. However, our interpretation remains the same.</p>
<disp-quote content-type="editor-comment">
<p>The excerpt highlights computational efficiency, reporting substantial runtime improvements (e.g., 108 s vs. 5757 s). However, the comparison lacks details on dataset size, hardware/software environment, and reproducibility conditions. Without these details, the speedup claim is difficult to evaluate. Furthermore, it remains unclear whether the reported efficiency gains come at the expense of predictive performance</p>
</disp-quote>
<p>Thank you for pointing out this limitation in how we presented the runtime results. We have rerun the tests and updated the table. An additional comment is added underneath, which details the hardware/software environment used to run both tools, as well as that the Squidly model is the ensemble version. As per the relationship between efficiency gains and predictive performance, both 3B and 15B models are benchmarked side by side across the paper.</p>
<p>Compared to the tools we were able to comprehensively benchmark, it does not come at a cost. However, we note that the increased benefits in runtime assume that a structure must be folded, which is not the case for enzymes already present in the PDB. If that is the case, then it is likely already annotated and, in those cases, we recommend using BLAST which is superior in terms of run time than either Squidly or a structure-based tool and highly accurate for homologous or annotated sequences.</p>
<disp-quote content-type="editor-comment">
<p>Given the well-known biases in public enzyme databases, the dataset is likely enriched for model organisms (e.g., E. coli, yeast, human enzymes) and underrepresents enzymes from archaea, extremophiles, and diverse microbial taxa. Would this limit conclusions about Squidly's generalizability to less-studied lineages?</p>
</disp-quote>
<p>The enrichment for model organisms in public enzyme databases may indeed affect both ESM2 and Squidly when applied to underrepresented lineages such as archaea, extremophiles, and diverse microbial taxa. We agree that this limitation is significant and have adjusted and expanded the previous discussion of benchmarking limitations accordingly (lines 358, 369). We thank the reviewer for highlighting this issue, which has helped us to improve the transparency and balance of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2:</bold></p>
<p>The authors aim to develop Squidly, a sequence-only catalytic residue prediction method. By combining protein language model (ESM2) embedding with a biologically inspired contrastive learning pairing strategy, they achieve efficient and scalable predictions without relying on three-dimensional structure. Overall, the authors largely achieved their stated objectives, and the results generally support their conclusions. This research has the potential to advance the fields of enzyme functional annotation and protein design, particularly in the context of screening large-scale sequence databases and unstructured data. However, the data and methods are still limited by the biases of current public databases, so the interpretation of predictions requires specific biological context and experimental validation.</p>
<p>Strengths:</p>
<p>The strengths of this work include the innovative methodological incorporation of EC classification information for &quot;reaction-informed&quot; sample pairing, thereby enhancing the discriminative power of contrastive learning. Results demonstrate that Squidly outperforms existing machine learning methods on multiple benchmarks and is significantly faster than structure prediction tools, demonstrating its practicality.</p>
<p>Weaknesses:</p>
<p>Disadvantages include the lack of a systematic evaluation of the impact of each strategy on model performance. Furthermore, some analyses, such as PCA visualization, exhibit low explained variance, which undermines the strength of the conclusions.</p>
</disp-quote>
<p>We thank the reviewer for their comments and feedback.</p>
<disp-quote content-type="editor-comment">
<p>The authors state that &quot;Notably, the multiclass classification objective and benchmarks used to evaluate EasIFA made it infeasible to compare performance for the binary catalytic residue prediction task.&quot; However, EasIFA has also released a model specifically for binary catalytic site classification. The authors should include EasIFA in their comparisons in order to provide a more comprehensive evaluation of Squidly's performance.</p>
</disp-quote>
<p>We thank the reviewer for raising this point. EasIFA’s binary classification task includes catalytic, binding, and “other” residues, which differs from Squidly’s strict catalytic residue prediction. This makes direct comparison non-trivial, which is why we originally had opted to not benchmark against EasIFA and instead highlight it in our discussion.</p>
<p>Given your comment, we did our best to include a benchmark that could give an indication of a comparison between the two tools. To do this, we filtered EasIFA’s multiclass classification test dataset for a non-overlapping subset with Squidly and AEGAN training data and &lt;40% sequence identity to all training sets. This left only 66 catalytic residue– containing sequences that we could use as a held-out test set from both tools. We note it is not directly equal as Squidly and AEGAN had lower average identity to this subset (8.2%) than EasIFA (23.8%), placing them at a relative disadvantage.</p>
<p>We also identified a potential limitation in EasIFA’s original recall calculation, where sequences lacking catalytic residues were assigned a recall of 0. We adapted this to instead consider only the sequences which do have catalytic residues, which increased recall across all models. With the updated evaluation, EasIFA continues to show strong performance, consistent with it being SOTA if structural inputs are available. Squidly remains competitive given it operates solely from sequence and has a lower sequence identity to this specific test set.</p>
<p>Due to the small and imbalanced benchmark size, differences in training data overlap, and differences in our analysis compared with the original EasIFA analysis, we present this comparison in a new section (A.4) of the supplementary information rather than in the main text. References to this section have been added in the manuscript at lines 265-268. Additionally, we do update the discussion and emphasize the potential benefits of using EasIFA at lines (353-356).</p>
<disp-quote content-type="editor-comment">
<p>The manuscript proposes three schemes for constructing positive and negative sample pairs to reduce dataset size and accelerate training, with Schemes 2 and 3 guided by reaction information (EC numbers) and residue identity. However, two issues remain:</p>
<p>(a) The authors do not systematically evaluate the impact of each scheme on model performance.</p>
<p>(b) In the benchmarking results, it is not explicitly stated which scheme was used for comparison with other models (e.g., Table 1, Figure 6, Figure 8). This lack of clarity makes it difficult to interpret the results and assess reproducibility.</p>
<p>(c) Regarding the negative samples in Scheme 3 in Figure 1, no sampling patterns are shown for residue pairs with the same amino acid, different EC numbers, and both being catalytic residues.</p>
</disp-quote>
<p>We thank the reviewer for these suggestions, which enabled us to improve the clarity and presentation of the manuscript. Please find our point by point response:</p>
<p>(a) We thank the reviewer for highlighting the lack of clarity in the way we have presented our evaluation in the section describing the Uni3175 benchmark. We aimed to systematically evaluate the impact of each scheme using the Uni3175 benchmark and refer to these results at lines 244-258, Additionally, we have adjusted the presentation of this section at lines 244-247 also in line with related comments from reviewer 1 in order to make the intention of this section and benchmark results to allow a comparison of each scheme to baseline models and AEGAN. These results led us to use Scheme 3 in both models for the other benchmarks in Figures 2 and 3. Please let us know if there is anything we can do to further improve the interpretability of Squidly’s performance.</p>
<p>(b) We thank the reviewer for highlighting this issue and improving the clarity of our manuscript. We agree that after the Uni3175 benchmark was used to evaluate the schemes, we did not clearly state in the other benchmarks that scheme 3 was chosen for both the 3B and 15B models. We have made changes in table 1 and the Figure legends of Figures 2 and 3 to state that scheme 3 was used. In addition, we integrated related results into panel figures (e.g. Figures 2 and 3 now show models trained and tested on consistent benchmark datasets) and standardized figure colors and legend formatting throughout. Furthermore, we suspect that the previous switch from using the individual vs ensembled Squidly models during the paper was not well indicated, and likely to confuse the reader. Therefore, we decided to consistently report the ensembled Squidly models for all benchmarks except in the ablation study (Figure 2A). In line with this, we altered the overview Figure 1A, so that it is clearer that the default and intended version of Squidly is the ensemble.</p>
<p>(c) We appreciate the reviewer pointing this out. You’re correct, we explicitly did not sample the negatives described by the reviewer in scheme 3 as our focus was on the hard negatives that relate most to the binary objective.  We do think this is a great idea and would be worth exploring further in future versions of Squidly, where we will be expanding the label space used for hard-negative sampling and including binding sites in our prediction. We have updated the discussion at lines 395-396 to highlight this potential direction.</p>
<disp-quote content-type="editor-comment">
<p>The PCA visualization (Figure 3) explains very little variance (~5% + 1.8%), but its use to illustrate the separability of embedding and catalytic residues may overinterpret the meaning of the low-dimensional projection. We question whether this figure is appropriate for inclusion in the main text and suggest that it be moved to the Supporting Information.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. We had discussed this as well, and in the end decided to include it in the main manuscript. We agree that the explained variance is low. However, when we first saw the PCA we were surprised that there was any separation at all. This then prompted us to investigate further, so we kept it in the manuscript to be true to the scientific story. However, we do agree that our interpretation could be interpreted as overly conclusive given the minimal variance explained by the top 2 PCs. Therefore, we agree with the assessment that the figure, alongside the accompanying results section, is more appropriately placed in the supplementary information. We moved this section (A.1) to the appendix to still explain the exploratory data analysis process that we used to tackle this problem, so that the general thought process behind Squidly is available for further reading.</p>
<disp-quote content-type="editor-comment">
<p>Minor Comments:</p>
<p>(1) Figure Quality and Legends a) In Figure 4, the legend is confusing: &quot;Schemes 2 and 3 (S1 and S2) ...&quot; appears inconsistent, and the reference to Scheme 3 (S3) is not clearly indicated.</p>
<p>(b) In Figure 6, the legend overlaps with the y-axis labels, reducing readability. The authors should revise the figures to improve clarity and ensure consistent notation.</p>
</disp-quote>
<p>The reviewer correctly notes inconsistencies in figure presentation. We have revised the legend of Figure 4 (now 2A) to ensure schemes are referred to consistently and Scheme 3 (S3) is clearly indicated. We also adjusted Figure 6 (now 2c) to remove the overlap between the legend and y-axis labels.</p>
<p>Conclusion</p>
<p>We thank the reviewers and editor again for their constructive input. We believe the revisions and clarifications substantially strengthened the manuscript and the resource</p>
</body>
</sub-article>
</article>