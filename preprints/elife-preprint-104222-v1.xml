<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">104222</article-id>
<article-id pub-id-type="doi">10.7554/eLife.104222</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104222.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Dynamic estimation of the attentional field from visual cortical activity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7926-6500</contrib-id>
<name>
<surname>Bloem</surname>
<given-names>Ilona M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2741-5532</contrib-id>
<name>
<surname>Bakst</surname>
<given-names>Leah</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6259-0809</contrib-id>
<name>
<surname>McGuire</surname>
<given-names>Joseph T</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6735-2508</contrib-id>
<name>
<surname>Ling</surname>
<given-names>Sam</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>samling@bu.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Department of Psychological &amp; Brain Sciences, Boston University</institution></institution-wrap>, <city>Boston</city>, <country>United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05csn2x06</institution-id><institution>Department of Computational Cognitive Neuroscience and Neuroimaging, Netherlands Institute for Neuroscience, Amsterdam, Netherlands &amp; Spinoza Centre for Neuroimaging</institution></institution-wrap>, <city>Amsterdam</city>, <country>Netherlands</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Alabama at Birmingham</institution>
</institution-wrap>
<city>Birmingham</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors provided equal contribution</p></fn>
<fn fn-type="coi-statement"><p>Competing Interest Statement: The authors have declared no competing interest.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-01-16">
<day>16</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP104222</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-10-24">
<day>24</day>
<month>10</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-10-08">
<day>08</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.05.611383"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Bloem et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Bloem et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-104222-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Navigating around the world, we must adaptively allocate attention to our surroundings based on anticipated future stimuli and events. This allocation of spatial attention boosts visuocortical representations at attended locations and locally enhances perception. Indeed, spatial attention has often been analogized to a “spotlight” shining on the item of relevance. Although the neural underpinnings of the locus of this attentional spotlight have been relatively well studied, less is known about the size of the spotlight: to what extent can the attentional field be broadened and narrowed in accordance with behavioral demands? In this study, we developed a paradigm for dynamically estimating the locus and spread of covert spatial attention, inferred from visuocortical activity using fMRI in humans. We measured BOLD activity in response to an annulus while participants (4 female, 4 male) used covert visual attention to determine whether more numbers or letters were present in a cued region of the annulus. Importantly, the width of the cued area was systematically varied, calling for different sizes of the attentional spotlight. The deployment of attention was associated with an increase in BOLD activity in corresponding retinotopic regions of visual areas V1—V3. By modeling the visuocortical attentional modulation, we could reliably recover the cued location, as well as a broadening of the attentional enhancement with wider attentional cues. This modeling approach offers a useful window into the dynamics of attention and spatial uncertainty.</p>
</abstract>
<abstract abstract-type="teaser">
<title>Significance Statement</title>
<p>This study explores whether spatial attention can dynamically adapt by shifting and broadening the attentional field. While previous research has focused on the modulation of neural responses at attended locations, less is known about how the size of the attentional field is represented within visual cortex. Using fMRI, we developed a novel paradigm to estimate the spatial tuning of the attentional field and demonstrate that we were able to recover both the location as well as the width of the attentional field. Our findings offer new insights into the neural mechanisms underlying the deployment of spatial attention, contributing to a deeper understanding of how spatial attention supports visual perception.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Figure 4 is modified; Figure 5 has been added; Materials and Methods are moved to after the Discussion section; statistics are updated</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>We bounce attention around all the time. Take, for instance, when we’re monitoring oncoming traffic while driving. It isn’t sufficient to attend to the single most likely source of traffic. Instead, attention adaptively broadens and narrows to cover the anticipated spatial distribution of relevant events. The need to spread attention across different swaths of the visual field is driven, to a large degree, by spatial uncertainty: statistical regularities give us a general sense as to where something useful might happen, and this evolves from moment to moment. We navigate this uncertainty by dynamically deploying spatial attention.</p>
<p>Covert spatial attention improves behavioral performance at attended locations at the cost of performance at unattended locations (<xref ref-type="bibr" rid="c46">Posner, 1980</xref>), leading to a common metaphor that spatial selective attention acts as a ‘spotlight’ or ‘zoom lens’ (<xref ref-type="bibr" rid="c53">Shaw and Shaw, 1977</xref>; <xref ref-type="bibr" rid="c46">Posner, 1980</xref>; <xref ref-type="bibr" rid="c16">Eriksen and St. James, 1986</xref>; <xref ref-type="bibr" rid="c10">Carrasco, 2011</xref>). This attentional ‘spotlight’ is characterized by a specific size and location and traverses the visual field based on behavioral demands (<xref ref-type="bibr" rid="c16">Eriksen and St. James, 1986</xref>; <xref ref-type="bibr" rid="c11">Castiello and Umiltà, 1990</xref>), selectively boosting information at the attended location within the visual system while suppressing information elsewhere. Animal studies have observed multiplicative increases in visuocortical neural responses at attended locations (<xref ref-type="bibr" rid="c33">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="c32">Maunsell, 2015</xref>) and human neuroimaging studies have found similar focal enhancements of population responses (<xref ref-type="bibr" rid="c28">Kastner <italic>et al</italic>., 1998</xref>; <xref ref-type="bibr" rid="c9">Brefczynski and DeYoe, 1999</xref>; <xref ref-type="bibr" rid="c35">McMains and Somers, 2004</xref>; <xref ref-type="bibr" rid="c13">Datta and DeYoe, 2009</xref>; <xref ref-type="bibr" rid="c56">Sprague and Serences, 2013</xref>; <xref ref-type="bibr" rid="c48">Puckett and DeYoe, 2015</xref>; <xref ref-type="bibr" rid="c51">Samaha, Sprague and Postle, 2016</xref>; <xref ref-type="bibr" rid="c54">Shioiri <italic>et al</italic>., 2016</xref>; <xref ref-type="bibr" rid="c8">Bloem and Ling, 2019</xref>).</p>
<p>While neural modulation at the locus of attention has been relatively well studied, less is known regarding the neural signatures of the size of the attentional field (<xref ref-type="bibr" rid="c61">Yeshurun, 2019</xref>). Spreading attention over a larger region of visual space can decrease behavioral performance, but only a handful of studies have interrogated associated effects within visual cortex (<xref ref-type="bibr" rid="c37">Müller <italic>et al</italic>., 2003</xref>; <xref ref-type="bibr" rid="c22">Herrmann <italic>et al</italic>., 2010</xref>; <xref ref-type="bibr" rid="c26">Itthipuripat <italic>et al</italic>., 2014</xref>; <xref ref-type="bibr" rid="c18">Feldmann-Wüstefeld and Awh, 2020</xref>). This is surprising, as the spatial distribution of the attentional field is a key feature in an influential theoretical model of attention (<xref ref-type="bibr" rid="c50">Reynolds and Heeger, 2009</xref>). The model assumes that the size of the attentional field can be adjusted based on task demands and that the interaction between attentional field size and stimulus-related factors can predict observed attentional gain effects.</p>
<p>While the studies that have experimentally manipulated the attentional field size found evidence congruent with this prominent theory (<xref ref-type="bibr" rid="c22">Herrmann <italic>et al</italic>., 2010</xref>; <xref ref-type="bibr" rid="c26">Itthipuripat <italic>et al</italic>., 2014</xref>; <xref ref-type="bibr" rid="c30">Kınıklıoğlu and Boyaci, 2022</xref>), few studies have directly investigated the spatial extent of the attentional window and its concomitant neural representation. One neuroimaging study revealed that the attentional field expanded in the face of greater task-related uncertainty (<xref ref-type="bibr" rid="c22">Herrmann <italic>et al</italic>., 2010</xref>), while other studies showed that the responsive area of visual cortex increased in size, coupled with a decrease of the overall population response (<xref ref-type="bibr" rid="c37">Müller <italic>et al</italic>., 2003</xref>; <xref ref-type="bibr" rid="c18">Feldmann-Wüstefeld and Awh, 2020</xref>). While these studies are consistent with the notion that the attentional field size can be detected in visual cortex, methods for dynamically recovering location and field size from moment to moment are lacking.</p>
<p>In this study, we developed a paradigm that allowed us to dynamically characterize the spatial tuning of spatial attention across the visual field. Using fMRI in humans, we examined whether attentional modulation of the BOLD response spanned a larger area of visual cortex when participants were cued to perform attend to a larger region of space. Behavioral performance confirmed that participants could successfully allocate their attention to different-sized swaths of the visual field. This deployment of attention was associated with a boost in cortical activity in the corresponding retinotopic areas of visual cortex. By modeling the location and spread of the visuocortical modulation, we dynamically recovered the cued location from the attentional activity with a high degree of fidelity, together with a broadening of the attentional enhancement for wider attentional cues.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Behavioral performance indicates effective deployment of covert spatial attention</title>
<p>We set out to investigate the spatial distribution of attentional modulation within visual cortex. To do so, we first ensured that participants (<italic>n</italic>=8) could successfully allocate covert spatial attention to cued portions of the visual field. During the experiment, participants’ task was to fixate the center of the screen and report whether there were more <italic>numbers</italic> or <italic>letters</italic> in a cued peripheral region (<xref rid="fig1" ref-type="fig">Figure 1a</xref>). The cued region varied in location and width: it could be centered on any of 20 polar angles and could span any of four widths (18°, 54°, 90°, and 162° of polar angle). Task performance indicated that participants used the cue effectively, as the proportion of correct responses was significantly above chance for all width conditions (<xref rid="fig1" ref-type="fig">Figure 1b</xref>; t-test, all p&lt;.001). We verified, with eye tracking, that participants performed the task using peripheral vision while maintaining central fixation. The upper bound of the 95% CI for each participant’s average gaze eccentricity ranged from 0.29° (degrees of visual angle) to 0.64° (mean = 0.48°; <xref rid="fig1" ref-type="fig">Figure 1c</xref>), suggesting that gaze did not exceed the cue annulus at fixation and that participants used covert spatial attention to perform the task.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p><bold>a.</bold> Task schematic. Participants’ were instructed to maintain central fixation and use covert spatial attention to determine whether there were more numbers or letters present within a cued region of a white noise annulus. On each trial, the red cue was displayed alone for 1.35 s and remained present throughout the trial. Twenty digits and letters were then presented for 0.5 s, equally spaced and overlaid on the annulus. Participants had 1.25 s to indicate via button press whether more digits or letters were present in the cued region. The cue remained stable for 5 trials (10 TRs, 15.5 s), had a width of 1, 3, 5, or 9 segments (18°, 54°, 90°, or 162°), and was centered on any of the 20 digit/letter slots. <bold>b.</bold> Behavioral task performance: Group mean accuracy for each cue width. Error bars are SEM; gray circles show individual participants. <bold>c.</bold> Group mean gaze eccentricity (in degrees of visual angle) for each cue width, conventions as in <bold>b</bold>.</p></caption>
<graphic xlink:href="611383v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b">
<title>Attentional modulation of BOLD responses broadens with cue width</title>
<p>We assessed the spatial distribution of attention by visualizing how the BOLD response was modulated by the location and width of the cue. To do so, we used each voxel’s population receptive field (pRF) to project BOLD responses for each attentional cue into the visual field. The resulting 2D visual field maps were averaged across trials for each cue width by rotating the maps, so the attentional cue aligned to 0° polar angle (right horizontal meridian). The reconstructed visual field maps revealed that increasing cue width led to a concomitant broadening of attentional modulation in cortex (<xref rid="fig2" ref-type="fig">Figure 2a</xref>). While this pattern was evident in all three early visual regions (V1–V3), the effect appeared to strengthen when ascending the visuocortical hierarchy.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p><bold>a.</bold> BOLD response projected into the visual field for each attentional cue width. Heatmaps represent the group mean BOLD activity using each voxel’s population receptive field (pRF) location within the visual field, shown separately for V1, V2, and V3. Maps were rotated to align all attentional cue locations to 0° polar angle (rightward). Concentric circles indicated by black dashed lines represent the location of the white noise annulus. <bold>b.</bold> Average spatial modulation profiles at the eccentricity of the annulus. The spatial profiles were recentered to 0° polar angle based on the cue location. Solid lines represent the group mean BOLD activity and shaded regions the SEM across participants.</p></caption>
<graphic xlink:href="611383v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Next, we computed the one-dimensional profile of attentional modulation at a fixed eccentricity. We were able to do this because we manipulated the location of the attentional field only as a function of polar angle, so all cues directed the attentional field to iso-eccentric locations. We selected voxels with pRFs that overlapped the white noise annulus and sorted them according to their polar angle preference.</p>
<p>For visualization purposes, the spatial response modulations were recentered to align all cues at 0° polar angle and averaged across trials for each cue width separately. Much like in the visual field reconstructions, there was a clear attentional enhancement centered on 0°, which broadened and decreased in amplitude with cue width – a pattern that was particularly evident in area V3 (<xref rid="fig2" ref-type="fig">Figure 2b</xref>).</p>
</sec>
<sec id="s2c">
<title>Dynamic model-based recovery of the attentional field</title>
<p>We next applied a modeling approach to estimate the location and width of attentional modulation, allowing us to further investigate the spread of attention in visual cortex. To do this, we averaged the spatial response profiles across TRs within each 10-TR block, in which the cue maintained a consistent location and width, yielding between 27 and 53 averaged spatial response profiles per participant for each width condition. We fit a generalized Gaussian function to each of these spatial profiles to estimate the location and width of attentional modulation per spatial profile (see <xref rid="fig3" ref-type="fig">Figure 3a</xref>). The width of attentional modulation was quantified in terms of the full width at half maximum (FWHM) of the best fitting model prediction (see <xref rid="fig3" ref-type="fig">Figure 3b</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p><bold>a.</bold> Modeling approach. The generalized Gaussian model is characterized by parameters for location (<italic>μ</italic>), scale (<italic>σ</italic>), and shape (<italic>β</italic>). <bold>b.</bold> Example model fits for two spatial profiles. Dots indicate BOLD response amplitudes for two attentional cues differing in position and width. Solid lines indicate the best fitting model estimate. To quantify the attentional field, we extracted the location and gain (dashed arrows), as well as the width (FWHM; solid arrows).</p></caption>
<graphic xlink:href="611383v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Can we dynamically recover the attentional field from activity within visual cortex? Model fits explained a substantial proportion of variance in the spatial profiles of BOLD activity (<bold>V1</bold>: for 18° cues, mean [standard deviation] of R<sup>2</sup> = 0.42 [0.03]; for 54° cues, 0.43 [0.03]; for 90° cues, 0.44 [0.03]; for 162° cues, 0.42 [0.03]; <bold>V2</bold>: for 18° cues, 0.51 [0.05]; for 54° cues, 0.54 [0.05]; for 90° cues, 0.54 [0.04]; for 162° cues, 0.55 [0.04]; <bold>V3</bold>: for 18° cues, 0.50 [0.03]; for 54° cues, 0.56 [0.04]; for 90° cues, 0.55 [0.03]; for 162° cues, 0.51 [0.02]). To interpret the estimated model parameters, we excluded the bottom 20% of fits based on a pooled R<sup>2</sup> across V1, V2, and V3, leaving roughly equal proportions of included blocks across cue width conditions (18°: mean [standard deviation] = 0.78 [0.04], 54°: 0.83 [0.05], 90°: 0.83 [0.04], 162°: 0.77 [0.07]).</p>
<p>To assess how well the model-estimated attentional field matched the cued location, we first calculated the angular error between the cue center and the model’s estimated location parameter. The angular error distribution across blocks, separated by width condition, is shown in <xref rid="fig4" ref-type="fig">Figure 4</xref> for one example participant to display block-to-block variation. The model reliably captured the location of the attentional field with low angular error. This result was consistent across participants. The group mean absolute angular error in V1 was 41.9° (SEM=2.86°), in V2 was 32.2° (2.31°), and in V3 was 24.7° (1.54°). Additionally, the magnitude of the absolute error did not vary linearly with the width of the cue in V1 or V2 (regression slopes tested against zero at the group level using a t-test; V1: <italic>t</italic>(7)=0.65, <italic>p</italic>=.537; V2: t(7)=1.24, <italic>p</italic>=.253; <xref rid="fig5" ref-type="fig">Figure 5</xref>). In V3, we observed a small but statistically significant increase in absolute error magnitude associated with greater cue widths (mean slope=1.4, <italic>t</italic>(7)=4.18, <italic>p=</italic>.004).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Attentional field parameter estimates for an example participant. The full parameter estimate distributions across blocks for location, width, and amplitude are shown for one example participant in V1, V2, and V3. Median parameter estimates are shown by the white points, with the box plot representing the 25th to 75th percentile, and whiskers extending to all non-outlier points. <bold>b.</bold> Group results for location, width, and amplitude estimates. Overall group mean and standard error are shown, separated by cue width and brain region.</p></caption>
<graphic xlink:href="611383v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Attentional field parameter estimates. Group results for location, width, and amplitude estimates. Overall group mean and standard error are shown in solid black, separated by cue width and brain region. Individual participant median estimates are shown in grey.</p></caption>
<graphic xlink:href="611383v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Next, we evaluated the width of the attentional field by visualizing the distribution of FWHM for the same example participant (<xref rid="fig4" ref-type="fig">Figure 4</xref>), and at the group level (<xref rid="fig5" ref-type="fig">Figure 5</xref>). Confirming the broadening of the attentional field observed in the visual field reconstruction maps, we found that the estimated FWHM increased with greater cue widths in V2 and V3 (V2 <italic>t</italic>(7)=5.63, <italic>p</italic>&lt;.001; V3 <italic>t</italic>(7)=6.49, <italic>p</italic>&lt;.001). The effect was not statistically significant in V1 (<italic>t</italic>(7)=1.68, <italic>p</italic>=.136).</p>
<p>Finally, we assessed the gain of the attentional modulation in the model (<xref rid="fig4" ref-type="fig">Figure 4</xref> and <xref rid="fig5" ref-type="fig">5</xref> for the example participant and group data, respectively). We observed no significant relationship between amplitude and cue width in V1 and V2 (V1 <italic>t</italic>(7)=-.54, <italic>p</italic>=.605; V2 <italic>t</italic>(7)=-2.19, <italic>p</italic>=.065), though we did find a significant effect in V3 (<italic>t</italic>(7)=-3.12, <italic>p</italic>=.017). We also found that the overall gain was greater in V2 and V3 compared to V1 (paired t-test, both <italic>p</italic>&lt;=.01).</p>
</sec>
<sec id="s2d">
<title>Temporal interval analysis</title>
<p>In the previous analyses, we leveraged the fact that the attentional cue remained constant for 5-trial blocks (spatial profiles were computed by averaging BOLD measurements across a block of 10 TRs). We next examined the degree to which we were able to recover the attentional field on a moment-by-moment (TR-by-TR) basis. To examine the consistency of the attentional field over a varying number of TRs with an identical cue, we systematically adjusted the number of TRs that contributed to the averaged spatial response profile. To maintain a constant number of observations across the temporal interval conditions, we randomly sampled a subset of TRs from each block. When we systematically varied the number of TRs included for each model fit (1, 2, 3, 5, or 10 TRs), we found a significant effect of cue width on recovered FWHM when averaging two or more TRs in V3 (all <italic>t</italic>(7)&gt;=2.38, all <italic>p&lt;</italic>=.049), and ten TRs in V2 (results as reported in prior section; <xref rid="fig6" ref-type="fig">Figure 6a</xref>). As described above, V1 did not reliably show a significant relationship between cue width and FWHM, even when averaging ten TRs. We found that increasing the number of TRs had a small but significant positive effect on FWHM estimates in V2 and V3 (V2, mean slope=2.7, <italic>t</italic>(7)=2.95, p=.021; V3, mean slope=1.16, <italic>t</italic>(7)=3.22, p=.015), although a significant effect was not observed in V1 (<italic>t</italic>(7)=1.82, p=.111).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Effect of number of TRs. Model fits were computed using BOLD data averaged across different temporal intervals (1, 2, 3, 5, or 10 TRs). Group means (with SEM) are plotted for FWHM, absolute angular error, amplitude estimates, and R<sup>2</sup>, separated by cue width, brain region, and the number of TRs used for each model fit.</p></caption>
<graphic xlink:href="611383v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The number of TRs significantly affected the absolute angular error associated with the estimated location of the attentional field (<xref rid="fig6" ref-type="fig">Figure 6b</xref>). Error magnitude decreased with TRs in all three visual regions (all t(7)&lt;=-4.48, all p&lt;=.003), suggesting that more data yielded more accurate estimates, though absolute angular error remained consistently below chance (90°) even when fitting the model to single-TR BOLD responses. Angular error remained stable across width conditions in V1 and V2 (V1, <italic>t</italic>(7)=-.55, p=.598; V2, <italic>t</italic>(7)=1.92, p=.098), though we found that larger cue width had a small but significant associated with larger errors in V3 (mean slope=.02, <italic>t</italic>(7)=3.28, p=.014).</p>
<p>The estimated gain of the attentional enhancement showed a dependence on number of TRs, with more TRs associated with lower gain estimates in V1 and V3 (V1, <italic>t</italic>(7)=-7.21, <italic>p</italic>&lt;.001; V3, <italic>t</italic>(7)=-9.97, <italic>p</italic>&lt;.001), though this was not clearly observed in V2 (<italic>t</italic>(7)=-1.60, <italic>p=</italic>.154). There was no evident dependence on cue width in V1 and V2 (V1 <italic>t</italic>(7)=-.19, <italic>p</italic>=.856; V2 <italic>t</italic>(7)=-2.34, <italic>p</italic>=.052), though we did observe a significant relationship in V3 (<italic>t</italic>(7)=-2.86, <italic>p</italic>=.024; <xref rid="fig6" ref-type="fig">Figure 6c</xref>).</p>
<p>Finally, the model’s goodness of fit improved with more data, with larger R<sup>2</sup> associated with greater numbers of TRs included in the average profiles (all <italic>t</italic>(7)&gt;=2.99, all <italic>p</italic>&lt;=0.020), though all R<sup>2</sup> were above 0.3 across all visual regions even for single-TR model fits. We did not observe a dependence of R<sup>2</sup> on cue width (all <italic>t</italic>(7)&lt;=1.26, all <italic>p</italic>&gt;=.249; <xref rid="fig6" ref-type="fig">Figure 6d</xref>).</p>
</sec>
<sec id="s2e">
<title>Width of the attentional field mimics perceptual modulation</title>
<p>While the attentional field broadened as expected when participants were cued to attend to a larger portion of the white noise annulus, the size of the estimated attentional modulation was greater than the true size of the cued region. The cue width varied between 18° and 162°, whereas the width estimate derived from spatial profiles of BOLD modulation varied between 103° and 179° (<xref rid="fig4" ref-type="fig">Figure 4b</xref>). We wondered what the underlying cause of this disparity might be. One possibility is that the BOLD-derived FWHM might tend to overestimate the retinotopic extent of the modulation. If this were the case, we would expect to obtain overestimates of FWHM when applying the same modeling approach to perceptual modulations as well. Alternatively, the true subjective attentional field might be consistently broader than cued, despite the presence of nearby distractors. If this were the case, modulation driven by perceptual differences should <italic>not</italic> result in the same large FWHM estimates.</p>
<p>To address this, we compared our estimates of the attentional field with equivalent estimates for spatial profiles induced by a perceptual manipulation. In this additional experiment, we varied the contrast intensity of sections of the white noise annulus. Participants were not asked to deploy spatial attention to the stimulus and were instead instructed to perform a color change detection task at fixation. The regions of increased noise contrast matched the attentional cue widths (18°, 54°, 90°, and 162°, plus an additional intermediate width of 126°), and were centered on one of the four cardinal locations (0°, 90°, 180°, 270° polar angle).</p>
<p>As expected, we observed a broadening of the spatial profile of BOLD modulation in all three visual areas as the region of increased contrast widened (<xref rid="fig7" ref-type="fig">Figure 7a</xref>). Using an identical modeling procedure, we estimated the spatial profile of the <italic>perceptual</italic> BOLD modulation. The group results for model estimates revealed that: 1) we were highly accurate in estimating the location of the contrast increment; 2) FWHM of the spatial profiles broadened across contrast widths, and 3) the amplitude remained stable across contrast widths (<xref rid="fig7" ref-type="fig">Figure 7b</xref>).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p><bold>a.</bold> Spatial profiles of perceptual modulation. Solid lines represent the group mean BOLD activity and shaded regions the SEM. <bold>b.</bold> Group level parameter estimates. Overall group mean and standard error are shown for the absolute angular error, FWHM, and amplitude, separated by contrast width and brain region. <bold>c.</bold> Comparison of FWHM estimates obtained from the attentional manipulation and the physical contrast manipulation. Dot color indicates brain region; each point represents the mean FWHM for a given width condition across participants.</p></caption>
<graphic xlink:href="611383v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Mirroring the results from the attentional manipulation, FWHM estimates systematically exceeded the nominal size of the perceptually modulated region of the visual field. Comparing the estimated FWHMs of the perceptual and attentional spatial profiles (<xref rid="fig7" ref-type="fig">Figure 7c</xref>) revealed that the estimated widths were highly comparable (Pearson correlation r=0.664 across width conditions and visual regions). This finding implies that the BOLD-derived generative Gaussian model may have had a general tendency to return upwardly biased width estimates, but that it recovered relative differences in a similar manner for attentional and perceptual forms of modulation.</p>
<p>For the perceptual contrast manipulation, the increase in the recovered FWHM with contrast width was observed in both V1 and V3 (<xref rid="fig7" ref-type="fig">Figure 7b</xref>; V1, <italic>t</italic>(4)=6.94, <italic>p</italic>=.002; V3 <italic>t</italic>(4)=11.34, <italic>p&lt;</italic>.001), though this effect was not clearly observed in V2 (<italic>t</italic>(4)=1.37, <italic>p</italic>=.242). The mean magnitude of angular error between the model-estimated location and the center of the contrast stimulus had no significant dependence on contrast width in any of the three brain regions (magnitude of all <italic>t</italic>(4)&lt;=.915, all <italic>p</italic>&gt;=.412). The estimated amplitude of modulation also did not show a relationship to contrast width in any of the visual areas (magnitude of all <italic>t</italic>(4)&lt;=1.71, all <italic>p</italic>&gt;=0.163).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We investigated the topographic spread of spatial attention in human visual cortex by characterizing the spatial profile of BOLD responses while participants attended to different portions of the visual field. Behavioral performance confirmed that participants used the fixation cue to dynamically allocate attention to different swaths of the visual field. Attention allocation was associated with a boost in the BOLD response in corresponding retinotopic areas of visual cortex. To characterize the topography of that boost, our approach involved selecting voxels with pRF preferred eccentricities that overlapped our white noise annulus, and organizing those voxels into one-dimensional profiles of attentional modulation as a function of preferred polar angle. This allowed us to model the location and spread of the attentional field and test how well it tracked the nominal location and width of the cue presented at fixation. Using a generalized Gaussian model, the cued location could be recovered with high fidelity. Furthermore, we observed a broadening of the estimated attentional field in areas V2 and V3 with the cue width, suggesting our method was capable of dynamically recovering the location and size of the attentional field from moment to moment.</p>
<p>This work builds on the concept of an attentional ‘spotlight’ or ‘zoom lens’ that has long been theorized to aid in spatial attention (<xref ref-type="bibr" rid="c53">Shaw and Shaw, 1977</xref>; <xref ref-type="bibr" rid="c46">Posner, 1980</xref>; <xref ref-type="bibr" rid="c16">Eriksen and St. James, 1986</xref>; <xref ref-type="bibr" rid="c10">Carrasco, 2011</xref>). By flexibly adjusting and shifting the focus of the spotlight, visual representations are selectivity enhanced within a specific region of the visual field. However, the empirical evidence demonstrating that attention can change its <italic>spread</italic> across the visual field by modulating brain responses is surprisingly lacking (<xref ref-type="bibr" rid="c61">Yeshurun, 2019</xref>). Our understanding of how the attentional window interacts with spatial representations is mainly based on behavioral reports (<xref ref-type="bibr" rid="c20">Gobell, Tseng and Sperling, 2004</xref>; <xref ref-type="bibr" rid="c43">Palmer and Moore, 2009</xref>; <xref ref-type="bibr" rid="c22">Herrmann <italic>et al</italic>., 2010</xref>; <xref ref-type="bibr" rid="c6">Beilen <italic>et al</italic>., 2011</xref>; <xref ref-type="bibr" rid="c57">Taylor <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="c24">Huang <italic>et al</italic>., 2017</xref>; <xref ref-type="bibr" rid="c30">Kınıklıoğlu and Boyaci, 2022</xref>), but see (<xref ref-type="bibr" rid="c23">Hopf <italic>et al</italic>., 2006</xref>; <xref ref-type="bibr" rid="c26">Itthipuripat <italic>et al</italic>., 2014</xref>; <xref ref-type="bibr" rid="c58">Tkacz-Domb and Yeshurun, 2018</xref>; <xref ref-type="bibr" rid="c18">Feldmann-Wüstefeld and Awh, 2020</xref>), despite it forming a crucial component in an influential theoretical model of attention (<xref ref-type="bibr" rid="c50">Reynolds and Heeger, 2009</xref>). This model proposes that the interaction between stimulus properties (such as its size and specific features) and the attentional field can explain a wide variety of attentional effects reported in behavioral and neurophysiological studies (<xref ref-type="bibr" rid="c22">Herrmann <italic>et al</italic>., 2010</xref>; <xref ref-type="bibr" rid="c26">Itthipuripat <italic>et al</italic>., 2014</xref>; <xref ref-type="bibr" rid="c8">Bloem and Ling, 2019</xref>; <xref ref-type="bibr" rid="c27">Jigo, Heeger and Carrasco, 2021</xref>). The present study sought to address this gap, with our results showing that the visuocortical attentional field broadened as we increased the cue width (<xref rid="fig5" ref-type="fig">Figure 5</xref>). This provides compelling evidence that the attention-related cortical response can, in fact, flexibly vary in its position and spatial distribution. In this study, we modeled the attentional field using a one-dimensional distribution. This approach aligned with our experimental design, as the attentional cue was manipulated only as a function of polar angle. However, we know that spatial processing varies substantially as a function of eccentricity. Spatial resolution is highest at the fovea and rapidly drops in the periphery (<xref ref-type="bibr" rid="c2">Anton-Erxleben and Carrasco, 2013</xref>). The spatial distribution of attention will presumably also vary with eccentricity and will likely take on different functional properties close to the fovea, where spatial resolution is high, compared to the far periphery where spatial resolution is low (<xref ref-type="bibr" rid="c25">Intriligator and Cavanagh, 2001</xref>; <xref ref-type="bibr" rid="c27">Jigo, Heeger and Carrasco, 2021</xref>). Future work can help provide a better understanding of the contribution of spatial attention by considering how the attentional field interacts with these well described spatial variations across the visual field. Measuring the full spatial distribution of the attentional field (across both eccentricity and polar angle) will shed light on how spatial attention guides perception by interacting with the non-uniformity of spatial representations.</p>
<p>The spread of the attentional field likely influences the degree to which spatial resolution at the attended location is transformed, leading to enhanced behavioral performance. In our experiment, we cued participant to varying swaths of an iso-eccentric annulus of white noise and participants had to discriminate whether more numbers or more letters were presented within the cued region. Spatial attention was vital for this task, as enhanced spatial perception allowed the participants to better discriminate all stimuli within the cued region (<xref ref-type="bibr" rid="c2">Anton-Erxleben and Carrasco, 2013</xref>). However, the estimated spatial spread of the attentional modulation (as indicated by the recovered FWHM) was consistently wider than the cued region itself. We therefore compared the spread of the attention field with the spatial profile of a <italic>perceptually</italic> induced width manipulation. Our model overestimated the retinotopic extent of the cued region in both the attentional and perceptual versions of the task (<xref rid="fig7" ref-type="fig">Figure 7c</xref>), suggesting that the BOLD-derived FWHM systematically overestimated the extent of modulation. Future work could unpack the degree to which the size of the attentional field influences the spatial resolution of visual cortical representations (<xref ref-type="bibr" rid="c31">Klein, Harvey and Dumoulin, 2014</xref>; <xref ref-type="bibr" rid="c60">Vo, Sprague and Serences, 2017</xref>; <xref ref-type="bibr" rid="c59">Tünçok, Carrasco and Winawer, 2024</xref>), and how this influences spatial perception.</p>
<p>Beyond addressing core questions related to the function of spatial attention, this method also lays groundwork for addressing questions about spatial predictive uncertainty and belief updating. Prior work on these topics has relied almost entirely on inferring participants’ predictions from their behavior, often requiring participants to report overt point predictions (<xref ref-type="bibr" rid="c39">Nassar <italic>et al</italic>., 2010</xref>; <xref ref-type="bibr" rid="c34">McGuire <italic>et al</italic>., 2014</xref>; <xref ref-type="bibr" rid="c12">D’Acremont and Bossaerts, 2016</xref>; <xref ref-type="bibr" rid="c41">Nassar, Bruckner and Frank, 2019</xref>), or inferring participants’ predictions from their sequences of decisions (<xref ref-type="bibr" rid="c14">Daw <italic>et al</italic>., 2006</xref>; <xref ref-type="bibr" rid="c5">Behrens <italic>et al</italic>., 2007</xref>; <xref ref-type="bibr" rid="c45">Payzan-LeNestour and Bossaerts, 2011</xref>; <xref ref-type="bibr" rid="c44">Payzan-LeNestour <italic>et al</italic>., 2013</xref>). These approaches have shed light on how we dynamically adapt our learning and belief updating processes over time in differently structured contexts. However, methods for recovering information about full predictive belief distributions have been limited, relying on indirect measurements such as eye movements (<xref ref-type="bibr" rid="c42">O’Reilly <italic>et al</italic>., 2013</xref>; <xref ref-type="bibr" rid="c3">Bakst and McGuire, 2021</xref>, <xref ref-type="bibr" rid="c4">2023</xref>), and physiological measures of uncertainty and surprise in EEG and pupillometry (<xref ref-type="bibr" rid="c47">Preuschoff, ’t Hart and Einhauser, 2011</xref>; <xref ref-type="bibr" rid="c40">Nassar <italic>et al</italic>., 2012</xref>; <xref ref-type="bibr" rid="c41">Nassar, Bruckner and Frank, 2019</xref>). The methods developed here offer a potential way to recover the location and width of a spatial predictive distribution via the attentional field in contexts in which it is unknown <italic>a priori</italic> and might be dependent on how a given participant has integrated previous sequential evidence. Future work could extend this method to more directly interrogate how predictive uncertainty is represented throughout the brain on a moment-by-moment basis.</p>
<p>In summary, we found evidence that people could dynamically adapt the spread of spatial attention, and that the retinotopic extent of attentional enhancement of the BOLD response reflected this dynamic adaptation. These findings address a gap in our understanding of spatial attentional control, supporting core theoretical models of attention. Our modeling approach also lays the groundwork to address further questions related to how the attentional field interacts with the non-uniformity of spatial representations and how uncertainty in spatial contexts is represented in the human brain.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Eight healthy adults (4 female, 4 male, mean age = 30) participated in the main attention experiment, five of whom also participated in a second experiment featuring a contrast manipulation. All participants had normal or corrected-to-normal vision. All procedures were approved by the Boston University Institutional Review Board, and informed consent was obtained from all participants.</p>
</sec>
<sec id="s4b">
<title>Apparatus and stimuli</title>
<p>Participants were presented with stimuli generated using PsychoPy (v1.85.1; Peirce, 2007) on a MacBook Pro. The visual stimuli were displayed on a rear-projection screen (subtending ∼20°×16° visual angle) using a VPixx Technologies PROPixx DLP LED projector (maximum luminance 306 cd/m<sup>2</sup>). Participants viewed the screen through a front surface mirror. Participants were placed comfortably in the scanner with padding to minimize head motion.</p>
</sec>
<sec id="s4c">
<title>Procedure</title>
<sec id="s4c1">
<title>Attentional width manipulation</title>
<p>Participants were instructed to fixate a central point (radius 0.08° visual angle) while dynamic pixelwise white noise (flickering at 10 Hz, 50% contrast) was presented in the periphery (annulus spanning 4.6° to 7.4° visual angle). The annulus was segmented into 20 bins (18° polar angle per bin) by white grid lines radiating from a white circle at the center of the screen (radius 0.25°), passing behind the annulus, and terminating at 8.5° eccentricity. In the middle of each bin, a number or letter (height: 2.1°) was superimposed on the white noise annulus (see <xref rid="fig1" ref-type="fig">Figure 1a</xref>). For a subset of the participants (3 out of 8) the screen distance inside the scanner was changed, therefore for those participants the letter size was 1.86° visual angle, and the white noise annulus spanned 4.1° to 6.5° visual angle. The set of possible letters included all lowercase letters of the Latin alphabet except a, b, e, g, i, o, and u. The set of numbers included 2, 3, 4, 5, 7, and 8.</p>
<p>Participants were cued to attend covertly to a contiguous subset of the bins and their task was to report, via button press, whether there were more <italic>numbers</italic> or <italic>letters</italic> present within the cued region. The cue was a bold red segment on the central white circle, which corresponded to 1, 3, 5, or 9 bins (18°, 54°, 90°, or 162° polar angle; see <xref rid="fig1" ref-type="fig">Figure 1a</xref>). The true proportion of letters versus numbers was controlled within each cue width condition. For cued regions of 1 bin, there was either a single number or letter in the bin. For cued regions of 3 bins, the ratio was always 2:1 (either two numbers and one letter or vice versa). For cued regions of 5 bins, the ratio was 3:2, and for cued regions of 9 bins, the ratio was 6:3. Cues could be centered on any of the 20 bins.</p>
<p>Participants completed 8 to 12 runs of the task (mean = 10.4), with each run lasting 341 s and containing 100 trials. Each cue remained constant for a block of five trials (lasting 15.5 s, 10 TRs), although the letters and numbers within the cued region changed on every trial. Thus, each participant saw 20 unique cues (combinations of cue location and width) per run. Each run began and ended with 15.5 s of the dynamic noise annulus.</p>
<p>During each trial, the cue and white noise annulus were presented alone for 1.35 s. The numbers and letters were then displayed for 0.5 s. Thereafter, the cue and white noise remained visible while the participant had 1.25 s to indicate whether there had been more digits or letters within the cued region, resulting in a total trial duration of 3.1 s (2 TRs). No accuracy feedback was provided during the main experiment. However, all participants completed three training runs with trial-by-trial feedback prior to the scan session. During training runs, the response window was shortened to 1 s and the remaining 0.25 s presented feedback in the form of a change in color of the fixation point (blue for correct responses and orange for incorrect responses).</p>
</sec>
<sec id="s4c2">
<title>Physical contrast manipulation</title>
<p>A subset of participants (n=5) also participated in an experiment that enhanced the physical contrast intensity of the dynamic visual noise in segments of the annulus. This additional experiment was carried out during the same scan session and allowed for benchmarking the detectability of stimulus-evoked modulation in visual cortex using our analyses. The stimuli and trial structure were similar to the attentional manipulation. The task differed in the following ways: (1) the contrast of the white noise annulus was increased to 100% for segments of the annulus corresponding to 1, 3, 5, 7 or 9 bins (18°, 54°, 90°, 126°, or 162° polar angle), with a Gaussian rolloff (σ = 15°) that spanned 25% of the furthest included bins and 25% of the adjacent excluded bins; (2) the enhanced segments were always centered on the cardinal directions (0°, 90°, 180°, and 270° polar angle); (3) the contrast increase remained constant for 15.5 seconds (10 TRs); (4) participants performed a color change detection task at fixation. Each unique combination of 4 locations and 5 widths of the contrast enhancement was shown once per run, with the order randomized. To estimate a baseline response, each run started and ended with 15.5 seconds without contrast modulation. Participants completed two runs total, each lasting 341 seconds (220 TRs). Throughout the <italic>physical contrast</italic> runs, participants were instructed to fixate on a central point (radius 0.08° visual angle) and to press a button when the fixation point switched color (alternating white and red). The fixation point remained a color for at least one second and then had a 10% probability of switching every 100 ms. No cue was presented associated with the regions of increased contrast. Additionally, no letters or numbers were superimposed on the white noise annulus.</p>
</sec>
<sec id="s4c3">
<title>Population receptive field mapping</title>
<p>Population receptive field (pRF) estimates were obtained for each participant in a separate scan session. We used the experimental procedure as described in the Human Connectome Project 7T Retinotopy dataset (<xref ref-type="bibr" rid="c7">Benson <italic>et al</italic>., 2018</xref>). Stimuli were composed of a pink noise background with colorful objects and faces at various spatial scales, displayed on a mean luminance gray background. Stimuli were updated at a rate of 15 Hz while participants performed a color change detection task at fixation. Participants viewed two types of mapping stimuli: (1) contracting/expanding rings and rotating wedges; (2) moving bar stimuli (<xref ref-type="bibr" rid="c15">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="c29">Kay <italic>et al</italic>., 2013</xref>). A total of 4-6 scans (300 TRs) were collected for each participant (2-3 scans per stimulus type).</p>
</sec>
</sec>
<sec id="s4d">
<title>MRI data acquisition</title>
<p>All MRI data were acquired at Boston University’s Cognitive Neuroimaging Center (Boston, Massachusetts) on a research-dedicated Siemens Prisma 3T scanner using a 64-channel head coil. A scanning session lasted 2 hours. All functional neuroimaging data were acquired using a simultaneous multislice (SMS) gradient echo echoplanar acquisition protocol (<xref ref-type="bibr" rid="c36">Moeller <italic>et al</italic>., 2010</xref>; <xref ref-type="bibr" rid="c52">Setsompop <italic>et al</italic>., 2012</xref>): 2 mm isotropic voxels; FoV = 212 x 212 mm; 72 axial slices; TR = 1.55 s; TE = 35.60 ms; flip angle = 72°; multiband acceleration factor 4. We computed distortion field maps by using a spin echo echoplanar protocol with opposite y-axis phase encoding directions (2 mm isotropic voxels; FOV = 212 x 212 mm; TR = 8850 ms; TE = 70.80 ms; flip angle = 90°). During a separate scan session, we acquired a whole-brain anatomical scan using a T1-weighted multi-echo MPRAGE 3d sequence (1 mm isotropic; FoV = 256 x 256 mm; 176 sagittal slices; TR = 2530 ms; TE = 1.69 ms; flip angle = 7°), and the pRF scans (occipital coverage only; right-left phase encoding; 2 mm isotropic voxels; FoV = 136 x 136 mm; 36 slices; TR = 1 s; TE = 35.4 ms; flip angle = 64°; multiband acceleration factor 3).</p>
</sec>
<sec id="s4e">
<title>MRI data analysis</title>
<sec id="s4e1">
<title>Structural data preprocessing</title>
<p>Whole brain T1-weighted anatomical data were analyzed using the standard ‘recon-all’ pipeline provided by Freesurfer software (Freesurfer version 5.3, (<xref ref-type="bibr" rid="c19">Fischl, 2012</xref>)), generating cortical surface models, whole-brain segmentation, and cortical parcellations.</p>
</sec>
<sec id="s4e2">
<title>Functional data preprocessing</title>
<p>All analyses were performed in the native space for each participant. First, EPI distortion correction was applied to all fMRI BOLD time-series data using a reverse phase-encode method (<xref ref-type="bibr" rid="c1">Andersson, Skare and Ashburner, 2003</xref>) implemented in FSL (<xref ref-type="bibr" rid="c55">Smith <italic>et al</italic>., 2004</xref>). All functional data were then preprocessed using FS-FAST (<xref ref-type="bibr" rid="c19">Fischl, 2012</xref>), including standard motion-correction procedures, Siemens slice timing correction, and boundary-based registration between anatomical and functional volumetric spaces (<xref ref-type="bibr" rid="c21">Greve and Fischl, 2009</xref>). To facilitate voxel-wise analysis, no volumetric smoothing was performed and across-run within-modality robust rigid registration was applied (<xref ref-type="bibr" rid="c49">Reuter, Rosas and Fischl, 2010</xref>), with the middle time-point of the first run serving as the target volume, and the middle time-point of each subsequent run used as a movable volume for alignment. Lastly, data were detrended (0.005 Hz high-pass filter) and converted to percent signal change for each voxel independently using custom code written in MATLAB (version 2020b).</p>
</sec>
<sec id="s4e3">
<title>Population receptive field mapping and voxel selection</title>
<p>The time series were analyzed using the analyzePRF toolbox in MATLAB, implementing a compressive spatial summation pRF model (<xref ref-type="bibr" rid="c29">Kay <italic>et al</italic>., 2013</xref>). The results of the pRF analysis were used to manually draw boundaries between early visual regions (V1, V2, and V3), which served as our regions of interest (ROIs).</p>
<p>Within each ROI, pRF modeling results were used to constrain voxel selection used in the main experiment. We excluded voxels with a preferred eccentricity outside the bounds of the pRF stimulus (&lt;0.7° and &gt;9.1°), with a pRF size smaller than 0.01°, or with poor spatial selectivity as indicated by the pRF model fit (R<sup>2</sup> &lt; 10%). Following our 2D visualizations (see below), we further constrained voxel selection by only including voxels whose pRF overlapped with the white noise annulus.</p>
</sec>
<sec id="s4e4">
<title>2D visualizations of attentional modulation</title>
<p>To visualize the topography of attentional modulation under different cue widths, we projected the average BOLD responses for a given block (10 TRs with a consistent cue location and width, shifted by 3 TRs [4.65 s] to compensate for the hemodynamic delay) into the visual field using each voxel’s pRF location. This method is similar to that described in (<xref ref-type="bibr" rid="c17">Favila, Kuhl and Winawer, 2022</xref>). First, we computed the Cartesian (x,y) coordinates from the pRF eccentricity and polar angle estimates for each voxel. Then, within a given ROI, we interpolated the BOLD responses over (x,y) space to produce a full-field representation. Each representation was then z-scored to allow for comparison across blocks, cue conditions, and participants. Finally, the representation was rotated so that the center of the cue was aligned to the right horizontal meridian (see <xref rid="fig2" ref-type="fig">Figure 2a</xref>).</p>
</sec>
<sec id="s4e5">
<title>1D spatial profile of attentional modulation</title>
<p>We also examined the spatial profile of attentional modulation as a function of polar angle. Voxels with pRFs overlapping the white noise annulus were grouped into 60 bins according to their pRF polar angle estimate (6° polar angle bin width). We computed a median BOLD response within each bin. To improve the signal-to-noise ratio, the resulting profile was smoothed with a moving average filter (width 18° polar angle; see <xref rid="fig2" ref-type="fig">Figure 2b</xref>).</p>
</sec>
<sec id="s4e6">
<title>Model fitting</title>
<p>We quantified the spatial profile of attentional modulation with a generalized Gaussian model (<xref ref-type="bibr" rid="c38">Nadarajah, 2005</xref>). The generalized Gaussian function (G) combines Gaussian and Laplace distributions:
<disp-formula id="eqn1">
<graphic xlink:href="611383v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The function has free parameters for location (<italic>μ</italic>), scale (<italic>σ</italic>), and shape (<italic>β</italic>). The shape parameter enables the tails of the distribution to become heavier than Gaussian (when <italic>β</italic> &lt; 2), or lighter than Gaussian (when <italic>β</italic> &gt; 2); as <italic>β</italic> → ∞, the model approaches a uniform distribution.</p>
<p>Next, <italic>G</italic> was normalized to range between 0 and 1, and vertically scaled and shifted by two additional free parameters for amplitude (<italic>a</italic>) and baseline offset (<italic>b</italic>):
<disp-formula id="eqn2">
<graphic xlink:href="611383v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We fit the five free parameters (<italic>μ</italic>, <italic>σ</italic>, <italic>β</italic>, <italic>a</italic>, <italic>b</italic>) using the MATLAB optimization tool <italic>fmincon,</italic> minimizing the squared error between the model prediction and the 1D profile described above. To avoid local minima, we first ran a grid search to find the initialization values with the lowest SSE (6 possible values for <italic>μ</italic>, equally spaced between 0 and 360°, crossed with 6 possible values for <italic>σ</italic>, equally spaced between 9° and 162° polar angle; <italic>β</italic> = 4; <italic>a</italic> = 1; <italic>b</italic> = 0). We imposed the following parameter bounds on the search: <italic>σ</italic>: [6°, 180° polar angle], <italic>β</italic>: [1.8, 50], and <italic>a</italic>: [0, 20]. <italic>μ</italic> was unbounded, but was wrapped to remain within [0°, 360°].</p>
<p>From the model fits we computed the following summary metrics: 1) angular error, defined as the polar-angle distance between the true and estimated location; 2) the full width at half-maximum (FWHM) of the best-fitting generalized Gaussian function, which served as our measure of the width of attentional modulation. The FWHM was controlled mainly by the scale parameter (<italic>σ</italic>) but also to a lesser degree by the shape parameter (<italic>β</italic>; see <xref rid="fig3" ref-type="fig">Figure 3a</xref>); 3) the gain modulation of the spatial profile (<italic>a</italic>); 4) the model’s goodness of fit quantified as the percentage of explained variance (<italic>R</italic><sup>2</sup>) in the spatial response profile:
<disp-formula id="eqn3">
<graphic xlink:href="611383v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
</sec>
<sec id="s4f">
<title>Statistical testing</title>
<p>To assess how the attentional cue width manipulation influenced the 1D spatial profile of BOLD modulation, we tested whether the computed summary metrics (absolute angular error, FWHM, and amplitude) varied as a function of cue width. Specifically, we performed a linear regression for each metric within each subject and tested whether the slopes differed from zero at the group level using a t-test. This was done independently for each ROI. When testing whether the number of TRs impacted our metrics, our linear regressions used both cue width and number of TRs as explanatory variables.</p>
</sec>
<sec id="s4g">
<title>Eye-position monitoring</title>
<p>Gaze data were collected for all participants using an MR-compatible SR Research EyeLink 1000+ eye tracker sampling at 1 kHz. Data from blink periods were excluded from analysis. Participants maintained fixation throughout the task, with average gaze eccentricity below 0.5° for all participants. Gaze eccentricity did not significantly vary by cued width (pairwise comparison of width conditions using a paired t-test, all <italic>p</italic> &gt;= 0.205 with Bonferroni correction for multiple comparisons) nor location (pairwise comparison, all <italic>p</italic> &gt;= 0.522 with Bonferroni correction for multiple comparisons).</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by National Science Foundation grants SMA-1809071, BCS-1625552, and BCS-1755757, National Institutes of Health grants F32-EY029134, R01-EY028163, and R01-MH126971, Office of Naval Research grant N00014-17-1-2304, and the Center for Systems Neuroscience Postdoctoral Fellowship at Boston University. The content of this paper does not necessarily represent the official views of the funding agencies.</p>
</ack>
<sec id="d1e1394" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author contributions</title>
<p>Conceptualization, Methodology &amp; Writing – Review &amp; Editing I.M.B., L.B., J.T.M., S.L.; Investigation, Analysis, Visualization, &amp; Writing – Original Draft, I.M.B. &amp; L.B.; Resources &amp; Supervision, J.T.M &amp; S.L.; Funding Acquisition L.B., J.T.M., S.L.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersson</surname>, <given-names>J.L.R.</given-names></string-name>, <string-name><surname>Skare</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Ashburner</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2003</year>) ‘<article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title>’, <source>NeuroImage</source>, <volume>20</volume>(<issue>2</issue>), pp. <fpage>870</fpage>–<lpage>888</lpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00336-7</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anton-Erxleben</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Carrasco</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2013</year>) ‘<article-title>Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence</article-title>’, <source>Nature Reviews Neuroscience</source>, <volume>14</volume>(<issue>3</issue>), pp. <fpage>188</fpage>–<lpage>200</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3443</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bakst</surname>, <given-names>L.</given-names></string-name> and <string-name><surname>McGuire</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2021</year>) ‘<article-title>Eye movements reflect adaptive predictions and predictive precision</article-title>’, <source>Journal of Experimental Psychology: General</source>, <volume>150</volume>(<issue>5</issue>), pp. <fpage>915</fpage>–<lpage>929</lpage>. <pub-id pub-id-type="doi">10.1037/xge0000977</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bakst</surname>, <given-names>L.</given-names></string-name> and <string-name><surname>McGuire</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2023</year>) ‘<article-title>Experience-driven recalibration of learning from surprising events</article-title>’, <source>Cognition</source>, <volume>232</volume>, p. <fpage>105343</fpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2022.105343</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Behrens</surname>, <given-names>T.E.J.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2007</year>) ‘<article-title>Learning the value of information in an uncertain world</article-title>’, <source>Nature Neuroscience</source>, <volume>10</volume>(<issue>9</issue>), pp. <fpage>1214</fpage>–<lpage>1221</lpage>. <pub-id pub-id-type="doi">10.1038/nn1954</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beilen</surname>, <given-names>M. van</given-names></string-name> <etal>et al.</etal></person-group> (<year>2011</year>) ‘<article-title>Attentional Window Set by Expected Relevance of Environmental Signals</article-title>’, <source>PLOS One</source>, <volume>6</volume>(<issue>6</issue>), p. <fpage>e21262</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0021262</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benson</surname>, <given-names>N.C.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2018</year>) ‘<article-title>The Human Connectome Project 7 Tesla retinotopy dataset: Description and population receptive field analysis</article-title>’, <source>Journal of Vision</source>, <volume>18</volume>(<issue>13</issue>), p. <fpage>23</fpage>. <pub-id pub-id-type="doi">10.1167/18.13.23</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bloem</surname>, <given-names>I.M.</given-names></string-name> and <string-name><surname>Ling</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2019</year>) ‘<article-title>Normalization governs attentional modulation within human visual cortex</article-title>’, <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), p. <fpage>5660</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-13597-1</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brefczynski</surname>, <given-names>J.A.</given-names></string-name> and <string-name><surname>DeYoe</surname>, <given-names>E.A</given-names></string-name></person-group>. (<year>1999</year>) ‘<article-title>A physiological correlate of the “spotlight” of visual attention</article-title>’, <source>Nature Neuroscience</source>, <volume>2</volume>(<issue>4</issue>), pp. <fpage>370</fpage>–<lpage>374</lpage>. <pub-id pub-id-type="doi">10.1038/7280</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carrasco</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2011</year>) ‘<article-title>Visual attention: The past 25 years</article-title>’, <source>Vision Research</source>, <volume>51</volume>(<issue>13</issue>), pp. <fpage>1484</fpage>–<lpage>1525</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2011.04.012</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Castiello</surname>, <given-names>U.</given-names></string-name> and <string-name><surname>Umiltà</surname>, <given-names>C</given-names></string-name></person-group>. (<year>1990</year>) ‘<article-title>Size of the attentional focus and efficiency of processing</article-title>’, <source>Acta Psychologica</source>, <volume>73</volume>(<issue>3</issue>), pp. <fpage>195</fpage>–<lpage>209</lpage>. <pub-id pub-id-type="doi">10.1016/0001-6918(90)90022-8</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>D’Acremont</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Bossaerts</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2016</year>) ‘<article-title>Neural Mechanisms Behind Identification of Leptokurtic Noise and Adaptive Behavioral Response</article-title>’, <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>4</issue>), pp. <fpage>1818</fpage>–<lpage>1830</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhw013</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Datta</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>DeYoe</surname>, <given-names>E.A</given-names></string-name></person-group>. (<year>2009</year>) ‘<article-title>I know where you are secretly attending! The topography of human visual attention revealed with fMRI</article-title>’, <source>Vision Research</source>, <volume>49</volume>(<issue>10</issue>), pp. <fpage>1037</fpage>–<lpage>1044</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2009.01.014</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daw</surname>, <given-names>N.D.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2006</year>) ‘<article-title>Cortical substrates for exploratory decisions in humans</article-title>’, <source>Nature</source>, <volume>441</volume>(<issue>7095</issue>), pp. <fpage>876</fpage>–<lpage>879</lpage>. <pub-id pub-id-type="doi">10.1038/nature04766</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dumoulin</surname>, <given-names>S.O.</given-names></string-name> and <string-name><surname>Wandell</surname>, <given-names>B.A</given-names></string-name></person-group>. (<year>2008</year>) ‘<article-title>Population receptive field estimates in human visual cortex</article-title>’, <source>NeuroImage</source>, <volume>39</volume>(<issue>2</issue>), pp. <fpage>647</fpage>–<lpage>660</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eriksen</surname>, <given-names>C.W.</given-names></string-name> and <string-name><surname>St. James</surname>, <given-names>J.D</given-names></string-name></person-group>. (<year>1986</year>) ‘<article-title>Visual attention within and around the field of focal attention: A zoom lens model</article-title>’, <source>Perception &amp; Psychophysics</source>, <volume>40</volume>(<issue>4</issue>), pp. <fpage>225</fpage>–<lpage>240</lpage>. <pub-id pub-id-type="doi">10.3758/BF03211502</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Favila</surname>, <given-names>S.E.</given-names></string-name>, <string-name><surname>Kuhl</surname>, <given-names>B.A.</given-names></string-name> and <string-name><surname>Winawer</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2022</year>) ‘<article-title>Perception and memory have distinct spatial tuning properties in human visual cortex</article-title>’, <source>Nature Communications</source>, <volume>13</volume>(<issue>1</issue>), p. <fpage>5864</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-022-33161-8</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feldmann-Wüstefeld</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Awh</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2020</year>) ‘<article-title>Alpha-band Activity Tracks the Zoom Lens of Attention</article-title>’, <source>Journal of Cognitive Neuroscience</source>, <volume>32</volume>(<issue>2</issue>), pp. <fpage>272</fpage>–<lpage>282</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01484</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2012</year>) ‘<article-title>FreeSurfer</article-title>’, <source>NeuroImage</source>, <volume>62</volume>(<issue>2</issue>), pp. <fpage>774</fpage>–<lpage>781</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gobell</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Tseng</surname>, <given-names>C.</given-names></string-name> and <string-name><surname>Sperling</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2004</year>) ‘<article-title>The spatial distribution of visual attention</article-title>’, <source>Vision Research</source>, <volume>44</volume>(<issue>12</issue>), pp. <fpage>1273</fpage>–<lpage>1296</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2004.01.012</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greve</surname>, <given-names>D.N.</given-names></string-name> and <string-name><surname>Fischl</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2009</year>) ‘<article-title>Accurate and robust brain image alignment using boundary-based registration</article-title>’, <source>NeuroImage</source>, <volume>48</volume>(<issue>1</issue>), pp. <fpage>63</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrmann</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2010</year>) ‘<article-title>When size matters: attention affects performance by contrast or response gain</article-title>’, <source>Nature Neuroscience</source>, <volume>13</volume>(<issue>12</issue>), pp. <fpage>1554</fpage>–<lpage>1559</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2669</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hopf</surname>, <given-names>J.-M.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2006</year>) ‘<article-title>The Neural Site of Attention Matches the Spatial Scale of Perception</article-title>’, <source>Journal of Neuroscience</source>, <volume>26</volume>(<issue>13</issue>), pp. <fpage>3532</fpage>–<lpage>3540</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4510-05.2006</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2017</year>) ‘<article-title>The time course of attention modulation elicited by spatial uncertainty</article-title>’, <source>Vision Research</source>, <volume>138</volume>, pp. <fpage>50</fpage>–<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2017.06.008</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Intriligator</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Cavanagh</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2001</year>) ‘<article-title>The Spatial Resolution of Visual Attention</article-title>’, <source>Cognitive Psychology</source>, <volume>43</volume>(<issue>3</issue>), pp. <fpage>171</fpage>–<lpage>216</lpage>. <pub-id pub-id-type="doi">10.1006/cogp.2001.0755</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Itthipuripat</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2014</year>) ‘<article-title>Changing the Spatial Scope of Attention Alters Patterns of Neural Gain in Human Cortex</article-title>’, <source>Journal of Neuroscience</source>, <volume>34</volume>(<issue>1</issue>), pp. <fpage>112</fpage>–<lpage>123</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3943-13.2014</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jigo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D.J.</given-names></string-name> and <string-name><surname>Carrasco</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2021</year>) ‘<article-title>An image-computable model of how endogenous and exogenous attention differentially alter visual perception</article-title>’, <source>Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>33</issue>), p. <fpage>e2106436118</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2106436118</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> (<year>1998</year>) ‘<article-title>Mechanisms of Directed Attention in the Human Extrastriate Cortex as Revealed by Functional MRI</article-title>’, <source>Science</source>, <volume>282</volume>(<issue>5386</issue>), pp. <fpage>108</fpage>–<lpage>111</lpage>. <pub-id pub-id-type="doi">10.1126/science.282.5386.108</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kay</surname>, <given-names>K.N.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2013</year>) ‘<article-title>Compressive spatial summation in human visual cortex</article-title>’, <source>Journal of Neurophysiology</source>, <volume>110</volume>(<issue>2</issue>), pp. <fpage>481</fpage>–<lpage>494</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00105.2013</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kınıklıoğlu</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Boyaci</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2022</year>) ‘<article-title>Increasing the spatial extent of attention strengthens surround suppression</article-title>’, <source>Vision Research</source>, <volume>199</volume>, p. <fpage>108074</fpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2022.108074</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname>, <given-names>B.P.</given-names></string-name>, <string-name><surname>Harvey</surname>, <given-names>B.M.</given-names></string-name> and <string-name><surname>Dumoulin</surname>, <given-names>S.O</given-names></string-name></person-group>. (<year>2014</year>) ‘<article-title>Attraction of Position Preference by Spatial Attention throughout Human Visual Cortex</article-title>’, <source>Neuron</source>, <volume>84</volume>(<issue>1</issue>), pp. <fpage>227</fpage>–<lpage>237</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.047</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maunsell</surname>, <given-names>J.H.R</given-names></string-name></person-group>. (<year>2015</year>) ‘<article-title>Neuronal Mechanisms of Visual Attention</article-title>’, <source>Annual Review of Vision Science</source>, <volume>1</volume>(Volume 1, 2015), pp. 373–391. <pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035431</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAdams</surname>, <given-names>C.J.</given-names></string-name> and <string-name><surname>Maunsell</surname>, <given-names>J.H</given-names></string-name></person-group>. (<year>1999</year>) ‘<article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4</article-title>’, <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>, <volume>19</volume>(<issue>1</issue>), pp. <fpage>431</fpage>–<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-01-00431.1999</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGuire</surname>, <given-names>J.T.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2014</year>) ‘<article-title>Functionally Dissociable Influences on Learning Rate in a Dynamic Environment</article-title>’, <source>Neuron</source>, <volume>84</volume>(<issue>4</issue>), pp. <fpage>870</fpage>–<lpage>881</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.013</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McMains</surname>, <given-names>S.A.</given-names></string-name> and <string-name><surname>Somers</surname>, <given-names>D.C</given-names></string-name></person-group>. (<year>2004</year>) ‘<article-title>Multiple Spotlights of Attentional Selection in Human Visual Cortex</article-title>’, <source>Neuron</source>, <volume>42</volume>(<issue>4</issue>), pp. <fpage>677</fpage>–<lpage>686</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(04)00263-6</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2010</year>) ‘<article-title>Multiband multislice GE-EPI at 7 tesla, with 16-fold acceleration using partial parallel imaging with application to high spatial and temporal whole-brain fMRI</article-title>’, <source>Magnetic Resonance in Medicine</source>, <volume>63</volume>(<issue>5</issue>), pp. <fpage>1144</fpage>–<lpage>1153</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.22361</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Müller</surname>, <given-names>N.G.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2003</year>) ‘<article-title>A Physiological Correlate of the “Zoom Lens” of Visual Attention</article-title>’, <source>Journal of Neuroscience</source>, <volume>23</volume>(<issue>9</issue>), pp. <fpage>3561</fpage>–<lpage>3565</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-09-03561.2003</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nadarajah</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2005</year>) ‘<article-title>A generalized normal distribution</article-title>’, <source>Journal of Applied Statistics</source>, <volume>32</volume>(<issue>7</issue>), pp. <fpage>685</fpage>–<lpage>694</lpage>. <pub-id pub-id-type="doi">10.1080/02664760500079464</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nassar</surname>, <given-names>M.R.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2010</year>) ‘<article-title>An Approximately Bayesian Delta-Rule Model Explains the Dynamics of Belief Updating in a Changing Environment</article-title>’, <source>Journal of Neuroscience</source>, <volume>30</volume>(<issue>37</issue>), pp. <fpage>12366</fpage>–<lpage>12378</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nassar</surname>, <given-names>M.R.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2012</year>) ‘<article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title>’, <source>Nature Neuroscience</source>, <volume>15</volume>(<issue>7</issue>), pp. <fpage>1040</fpage>–<lpage>1046</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3130</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nassar</surname>, <given-names>M.R.</given-names></string-name>, <string-name><surname>Bruckner</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Frank</surname>, <given-names>M.J.</given-names></string-name></person-group><year>2019</year>) ‘<article-title>Statistical context dictates the relationship between feedback-related EEG signals and learning</article-title>’, <source>eLife</source>., <volume>8</volume>, p. <elocation-id>e46975</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.46975</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Reilly</surname>, <given-names>J.X.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2013</year>) ‘<article-title>Dissociable effects of surprise and model update in parietal and anterior cingulate cortex</article-title>’, <source>Proceedings of the National Academy of Sciences</source>, <volume>110</volume>(<issue>38</issue>), pp. <fpage>E3660</fpage>–<lpage>E3669</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1305373110</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Palmer</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Moore</surname>, <given-names>C.M</given-names></string-name></person-group>. (<year>2009</year>) ‘<article-title>Using a filtering task to measure the spatial extent of selective attention</article-title>’, <source>Vision Research</source>, <volume>49</volume>(<issue>10</issue>), pp. <fpage>1045</fpage>–<lpage>1064</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2008.02.022</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Payzan-LeNestour</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2013</year>) ‘<article-title>The Neural Representation of Unexpected Uncertainty during Value-Based Decision Making</article-title>’, <source>Neuron</source>, <volume>79</volume>(<issue>1</issue>), pp. <fpage>191</fpage>–<lpage>201</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.037</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Payzan-LeNestour</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Bossaerts</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2011</year>) ‘<article-title>Risk, Unexpected Uncertainty, and Estimation Uncertainty: Bayesian Learning in Unstable Settings</article-title>’, <source>PLOS Computational Biology</source>, <volume>7</volume>(<issue>1</issue>), p. <fpage>e1001048</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1001048</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Posner</surname>, <given-names>M.I</given-names></string-name></person-group>. (<year>1980</year>) ‘<article-title>Orienting of Attention</article-title>’, <source>Quarterly Journal of Experimental Psychology</source>, <volume>32</volume>(<issue>1</issue>), pp. <fpage>3</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1080/00335558008248231</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Preuschoff</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>’t Hart</surname>, <given-names>B.M.</given-names></string-name> and <string-name><surname>Einhauser</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2011</year>) <article-title>‘Pupil Dilation Signals Surprise: Evidence for Noradrenaline’s Role in Decision Making’</article-title>, <source>Frontiers in Neuroscience</source>, <volume>5</volume>. <pub-id pub-id-type="doi">10.3389/fnins.2011.00115</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Puckett</surname>, <given-names>A.M.</given-names></string-name> and <string-name><surname>DeYoe</surname>, <given-names>E.A</given-names></string-name></person-group>. (<year>2015</year>) ‘<article-title>The Attentional Field Revealed by Single-Voxel Modeling of fMRI Time Courses</article-title>’, <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>12</issue>), pp. <fpage>5030</fpage>–<lpage>5042</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3754-14.2015</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reuter</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rosas</surname>, <given-names>H.D.</given-names></string-name> and <string-name><surname>Fischl</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2010</year>) ‘<article-title>Highly accurate inverse consistent registration: A robust approach</article-title>’, <source>NeuroImage</source>, <volume>53</volume>(<issue>4</issue>), pp. <fpage>1181</fpage>–<lpage>1196</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.020</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reynolds</surname>, <given-names>J.H.</given-names></string-name> and <string-name><surname>Heeger</surname>, <given-names>D.J</given-names></string-name></person-group>. (<year>2009</year>) ‘<article-title>The Normalization Model of Attention</article-title>’, <source>Neuron</source>, <volume>61</volume>(<issue>2</issue>), pp. <fpage>168</fpage>–<lpage>185</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Samaha</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sprague</surname>, <given-names>T.C.</given-names></string-name> and <string-name><surname>Postle</surname>, <given-names>B.R</given-names></string-name></person-group>. (<year>2016</year>) ‘<article-title>Decoding and Reconstructing the Focus of Spatial Attention from the Topography of Alpha-band Oscillations</article-title>’, <source>Journal of cognitive neuroscience</source>, <volume>28</volume>(<issue>8</issue>), pp. <fpage>1090</fpage>–<lpage>1097</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00955</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Setsompop</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2012</year>) ‘<article-title>Blipped-controlled aliasing in parallel imaging for simultaneous multislice echo planar imaging with reduced g-factor penalty</article-title>’, <source>Magnetic Resonance in Medicine</source>, <volume>67</volume>(<issue>5</issue>), pp. <fpage>1210</fpage>–<lpage>1224</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.23097</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shaw</surname>, <given-names>M.L.</given-names></string-name> and <string-name><surname>Shaw</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1977</year>) ‘<article-title>Optimal allocation of cognitive resources to spatial locations</article-title>’, <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>3</volume>(<issue>2</issue>), pp. <fpage>201</fpage>–<lpage>211</lpage>. <pub-id pub-id-type="doi">10.1037/0096-1523.3.2.201</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shioiri</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2016</year>) ‘<article-title>Visual attention spreads broadly but selects information locally</article-title>’, <source>Scientific Reports</source>, <volume>6</volume>(<issue>1</issue>), p. <fpage>35513</fpage>. <pub-id pub-id-type="doi">10.1038/srep35513</pub-id>.</mixed-citation></ref>
<ref id="c55"><label>55)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S.M.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2004</year>) ‘<article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title>’, <source>NeuroImage</source>, <volume>23</volume>, pp. <fpage>S208</fpage>–<lpage>S219</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>56)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sprague</surname>, <given-names>T.C.</given-names></string-name> and <string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2013</year>) ‘<article-title>Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices</article-title>’, <source>Nature Neuroscience</source>, <volume>16</volume>(<issue>12</issue>), pp. <fpage>1879</fpage>–<lpage>1887</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3574</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>57)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taylor</surname>, <given-names>J.E.T.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2015</year>) <article-title>‘Attentional cartography: mapping the distribution of attention across time and space’, <italic>Attention, Perception</italic></article-title>, <source>&amp; Psychophysics</source>, <volume>77</volume>(<issue>7</issue>), pp. <fpage>2240</fpage>–<lpage>2246</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-015-0943-0</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>58)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tkacz-Domb</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Yeshurun</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2018</year>) ‘<article-title>The size of the attentional window when measured by the pupillary response to light</article-title>’, <source>Scientific Reports</source>, <volume>8</volume>(<issue>1</issue>), p. <fpage>11878</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-30343-7</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>59)</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tünçok</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Carrasco</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Winawer</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2024</year>) ‘<article-title>Spatial attention alters visual cortical representation during target anticipation</article-title>’. <source>bioRxiv</source>, p. <fpage>2024.03.02.583127</fpage>. <pub-id pub-id-type="doi">10.1101/2024.03.02.583127</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>60)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vo</surname>, <given-names>V.A.</given-names></string-name>, <string-name><surname>Sprague</surname>, <given-names>T.C.</given-names></string-name> and <string-name><surname>Serences</surname>, <given-names>J.T</given-names></string-name></person-group>. (<year>2017</year>) ‘<article-title>Spatial Tuning Shifts Increase the Discriminability and Fidelity of Population Codes in Visual Cortex</article-title>’, <source>Journal of Neuroscience</source>, <volume>37</volume>(<issue>12</issue>), pp. <fpage>3386</fpage>–<lpage>3401</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3484-16.2017</pub-id>.</mixed-citation></ref>
<ref id="c61"><label>61)</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yeshurun</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2019</year>) ‘<article-title>The spatial distribution of attention</article-title>’, <source>Current Opinion in Psychology</source>, <volume>29</volume>, pp. <fpage>76</fpage>–<lpage>81</lpage>. <pub-id pub-id-type="doi">10.1016/j.copsyc.2018.12.008</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104222.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Alabama at Birmingham</institution>
</institution-wrap>
<city>Birmingham</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study addresses a gap in our understanding of how the size of the attentional field is represented within the visual cortex. The evidence supporting the role of visual cortical activity is <bold>solid</bold>, based on a novel modeling analysis of fMRI data. The results will be of interest to psychologists and cognitive neuroscientists.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104222.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors conducted an fMRI study to investigate the neural effects of sustaining attention to areas of different sizes. Participants were instructed to attend to alphanumeric characters arranged in a circular array. The size of attention field was manipulated in four levels, ranging from small (18 deg) to large (162 deg). They used a model-based method to visualize attentional modulation in early visual cortex V1 to V3, and found spatially congruent modulations of the BOLD response, i.e., as the attended area increased in size, the neural modulation also increased in size in the visual cortex. They suggest that this result is a neural manifestation of the zoom-lens model of attention and that the model-based method can effectively reconstruct the neural modulation in the cortical space.</p>
<p>The study is well-designed with sophisticated and comprehensive data analysis. The results are robust and show strong support for a well-known model of spatial attention, the zoom-lens model. Overall, I find the results interesting and useful for the field of visual attention research. I have questions about some aspects of the results and analysis as well as the bigger picture.</p>
<p>(1) It appears that the modulation in V1 is weaker than V2 and V3 (Fig 2). In particular, the width modulation in V1 is not statistically significant (Fig 5). This result seems a bit unexpected. Given the known RF properties of neurons in these areas, in particular, smaller RF in V1, one might expect more spatially sensitive modulation in V1 than V2/V3. Some explanations and discussions would be helpful. Relatedly, one would also naturally wonder if this method can be applied to other extrastriate visual areas such as V4 and what the results look like.</p>
<p>(2) I'm a bit confused about the angular error result. Fig 4 shows that the mean angular error is close to zero, but Fig 5 reports these values to be about 30-40 deg. Why the big discrepancy? Is it due to the latter reporting absolute errors? It seems reporting the overall bias is more useful than absolute value.</p>
<p>(3) A significant effect is reported for amplitude in V3 (line 78), but the graph in Fig 5 shows hardly any difference. Please confirm the finding and also explain the directionality of the effect if there is indeed one.</p>
<p>(4) The purpose of the temporal interval analysis is rather unclear. I assume it has to do with how much data is needed to recover the cortical modulation and hence how dynamic a signal the method can capture. While the results make sense (i.e., more data is better), there is no obvious conclusion and/or interpretation of its meaning.</p>
<p>(5) I think it would be useful for the authors to make a more explicit connection to previous studies in this literature. In particular, two studies seem particularly relevant. First, how do the present results relate to those in Muller et al (2003, reference 37), which also found a zoom-lens type of neural effects. Second, how does the present method compare with spatial encoding model in Sprague &amp; Serences (2013, reference 56), which also reconstructs the neural modulation of spatial attention. More discussions of these studies will help put the current study in the larger context.</p>
<p>(6) Fig 4b, referenced on line 123, does not exist.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104222.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The study in question utilizes functional magnetic resonance imaging (fMRI) to dynamically estimate the locus and extent of covert spatial attention from visuocortical activity. The authors aim to address an important gap in our understanding of how the size of the attentional field is represented within the visual cortex. They present a novel paradigm that allows for the estimation of the spatial tuning of the attentional field and demonstrate the ability to reliably recover both the location and width of the attentional field based on BOLD responses.</p>
<p>Strengths:</p>
<p>(1) Innovative Paradigm: The development of a new approach to estimate the spatial tuning of the attentional field is a significant strength of this study. It provides a fresh perspective on how spatial attention modulates visual perception.</p>
<p>
(2) Refined fMRI Analysis: The use of fMRI to track the spatial tuning of the attentional field across different visual regions is methodologically rigorous and provides valuable insights into the neural mechanisms underlying attentional modulation.</p>
<p>
(3) Clear Presentation: The manuscript is well-organized, and the results are presented clearly, which aids in the reader's comprehension of the complex data and analyses involved.</p>
<p>Weaknesses:</p>
<p>(1) Lack of Neutral Cue Condition: The study does not include a neutral cue condition where the cue width spans 360{degree sign}, which could serve as a valuable baseline for assessing the BOLD response enhancements and diminishments in both attended and non-attended areas.</p>
<p>
(2) Clarity on Task Difficulty Ratios: The explicit reasoning for the chosen letter-to-number ratios for various cue widths is not detailed. Ensuring clarity on these ratios is crucial, as it affects the task difficulty and the comparability of behavioral performance across different cue widths. It is essential that observed differences in behavior and BOLD signals are attributable solely to changes in cue width and not confounded by variations in task difficulty.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104222.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this report, the authors tested how manipulating the contiguous set of stimuli on the screen that should be used to guide behavior - that is, the scope of visual spatial attention - impacts the magnitude and profile of well-established attentional enhancements in visual retinotopic cortex. During fMRI scanning, participants attended to a cued section of the screen for blocks of trials and performed a letter vs digit discrimination task at each attended location (and judged whether the majority of characters were letters/digits). Importantly, the visual stimulus was identical across attention conditions, so any observed response modulations are due to top-down task demands rather than visual input. The authors employ population receptive field (pRF) models, which are used to sort voxel activation with respect to the location and scope of spatial attention and fit a Gaussian-like function to the profile of attentional enhancement from each region and condition. The authors find that attending to a broader region of space expands the profile of attentional enhancement across the cortex (with a larger effect in higher visual areas), but does not strongly impact the magnitude of this enhancement, such that each attended stimulus is enhanced to a similar degree. Interestingly, these modulations, overall, mimic changes in response properties caused by changes to the stimulus itself (increase in contrast matching the attended location in the primary experiment). The finding that attentional enhancement primarily broadens, but does not substantially weaken in most regions, is an important addition to our understanding of the impact of distributed attention on neural responses, and will provide meaningful constraints to neural models of attentional enhancement.</p>
<p>Strengths:</p>
<p>- Well-designed manipulations (changing location and scope of spatial attention), and careful retinotopic/pRF mapping, allow for a robust assay of the spatial profile of attentional enhancement, which has not been carefully measured in previous studies</p>
<p>
- Results are overall clear, especially concerning width of the spatial region of attentional enhancement, and lack of clear and consistent evidence for reduction in the amplitude of enhancement profile</p>
<p>
- Model-fitting to characterize spatial scope of enhancement improves interpretability of findings</p>
<p>Weaknesses:</p>
<p>- Task difficulty seems to vary as a function of spatial scope of attention, with varying ratios of letters/digits across spatial scope conditions, which may complicate interpretations of neural modulation results</p>
<p>
- Some aspects of analysis/data sorting are unclear (e.g., how are voxels selected for analyses?)</p>
<p>
- While the focus of this report is on modulations of visual cortex responses due to attention, the lack of inclusion of results from other retinotopic areas (e.g. V3AB, hV4, IPS regions like IPS0/1) is a weakness</p>
<p>
- Additional analyses comparing model fits across amounts of data analyzed suggest the model fitting procedure is biased, with some parameters (e.g., FWHM, error, gain) scaling with noise.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104222.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bloem</surname>
<given-names>Ilona M</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7926-6500</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bakst</surname>
<given-names>Leah</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2741-5532</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>McGuire</surname>
<given-names>Joseph T</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6259-0809</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Ling</surname>
<given-names>Sam</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6735-2508</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the three reviewers for their insightful feedback. We look forward to addressing the raised concerns in a revised version of the manuscript. There were a few common themes among the reviews that we will briefly touch upon now, and we will provide more details in the revised manuscript.</p>
<p>First, the reviewers asked for the reasoning behind the task ratios we implemented for the different attentional width conditions. The different ratios were selected to be as similar as possible given the size and spacing of our stimuli (aside from the narrowest cue width of one bin, the ratios for the others were 0.67, 0.60, and 0.67). As Figure 1b shows, task accuracy showed small and non-monotonic changes across the three larger cue widths, dissociable from the monotonic pattern seen for the model-estimated width of the attentional field. Furthermore, prior work has indicated that there is a relationship between task difficulty and the overall magnitude of the BOLD response, however we don’t suspect that this will influence the width of the modulation. How task difficulty influences the BOLD response is an important topic, and we hope that future work will investigate this relationship more directly.</p>
<p>Second, reviewers expressed interest in the distribution of spatial attention in higher visual areas. In our study we focus only on early visual regions (V1-V3). This was primarily driven by pragmatic considerations, in that we only have retinotopic estimates for our participants in these early visual areas. Our modeling approach is dependent on having access to the population receptive field estimates for all voxels, and while the main experiment was scanned using whole brain coverage, retinotopy was measured in a separate session using a field of view only covering the occipital cortex.</p>
<p>Lastly, we appreciate the opportunity to clarify the purpose of the temporal interval analysis. The reviewer is correct in assuming we set out to test how much data is needed to recover the cortical modulation and how dynamic a signal the method can capture. This analysis does show that more data provided more reliable estimates. The more important finding, however, is that the model was still able to recover the location and width of the attentional cue at shorter timescales of as few as two TRs. This has implications for the potential applicability of our approach to paradigms that involve more dynamic adaptation of the attentional field.</p>
</body>
</sub-article>
</article>