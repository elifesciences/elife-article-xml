<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95761</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95761</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95761.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Statistical learning of uniform- and mixed-length artificial languages: Different computational mechanisms support different task demands</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Luo</surname>
<given-names>Meili</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="author-notes" rid="FN1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cao</surname>
<given-names>Ran</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="fn" rid="FN1">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Wang</surname>
<given-names>Felix Hao</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<email>haowang1@sas.upenn.edu</email>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/036trcv74</institution-id><institution>School of Psychology, Nanjing Normal University</institution></institution-wrap>, <city>Nanjing</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3395-7234</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="FN1"><label>*</label><p>These authors contributed equally to this work.</p></fn>
<fn id="FN2"><p>The reported experiments were not preregistered.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-03-27">
<day>27</day>
<month>03</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-11-26">
<day>26</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95761</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2024-01-29">
<day>29</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-01">
<day>01</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.31219/osf.io/tb26v"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-03-27">
<day>27</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95761.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.95761.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95761.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95761.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Luo et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Luo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95761-v2.pdf"/>
<self-uri xlink:href="tb26v.pdf" content-type="docx" xlink:role="full-text"/>
<abstract>
<p>Statistical learning is a powerful mechanism that can support a variety of learning tasks. Many theories have assumed a single mechanism for statistical learning across different tasks, where a unitary mechanism is supposed to explain results from various studies, even across different modalities. In this study, we studied auditory statistical learning by comparing two different experimental paradigms, target detection and word segmentation, and examined if different mechanisms are required to explain results from the two paradigms. Previous work using the word segmentation paradigm suggested that learning is better with sequences containing uniform-length words than with sequences containing mixed-length words. If the same mechanism supports the target detection task, the same results are predicted. However, while learning was successful in both Experiments 1 and 2 with the target detection paradigm, the effect was larger in the mixed condition than in the uniform condition. We further replicated the uniform condition advantage in the word segmentation paradigm in Experiment 3. Thus, we hypothesized that the target detection paradigm required a different mechanism from those in word segmentation. To understand these mechanisms, we proposed both theoretical analyses and a computational model to simulate results from the target detection paradigm. We found that a prediction mechanism, rather than clustering, could explain the data from target detection. Crucially, this mechanism can produce facilitation effects without performing segmentation. We discuss both the theoretical and empirical reasons why the target detection and word segmentation paradigm might engage different processes, and how these findings contribute to our understanding of statistical word segmentation.</p>
</abstract>
<kwd-group>
<title>Keywords</title>
<kwd>Statistical segmentation</kwd>
<kwd>Transitional probability</kwd>
<kwd>Prediction</kwd>
<kwd>Rapid learning</kwd>
</kwd-group>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
    <institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id>
<institution>MOST | National Natural Science Foundation of China (NSFC)</institution>
</institution-wrap>
</funding-source>
<award-id>32500953</award-id>
<principal-award-recipient>
<name>
<surname>Wang</surname>
<given-names>Felix Hao</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-2">
<funding-source>
<institution-wrap>
<institution>National Science Foundation of Jiangsu Province of China</institution>
</institution-wrap>
</funding-source>
<award-id>BK20240588</award-id>
<principal-award-recipient>
<name>
<surname>Wang</surname>
<given-names>Felix Hao</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<p>For novice language learners, one of the first tasks is to understand the structure of the continuous speech streams they hear by segmenting the speech into words. In the literature, two types of information that can be used for segmentation are discussed. Prosodic information, such as stress, though never perfectly correlates with word boundaries in natural languages, can often provide useful information to word boundaries and has been shown to be used for word segmentation (<xref ref-type="bibr" rid="c20">Johnson &amp; Jusczyk, 2001</xref>; <xref ref-type="bibr" rid="c24">Jusczyk, 1999</xref>; <xref ref-type="bibr" rid="c25">Jusczyk &amp; Aslin, 1995</xref>; <xref ref-type="bibr" rid="c24">Jusczyk et al., 1999</xref>). Another type of information is the distributional information of the syllables in a sequence, which was shown to be used in word segmentation as well (e.g., <xref ref-type="bibr" rid="c32">Saffran et al., 1996</xref>; <xref ref-type="bibr" rid="c1">Aslin et al., 1998</xref>). The theory is that learners would track the co-occurrence information between syllables and use this co-occurrence information to compute transitional probability, which can be a cue to word boundaries. The seminal work on statistical learning (<xref ref-type="bibr" rid="c32">Saffran et al., 1996</xref>) demonstrated that young infants can segment word forms in a rapid syllable stream in two minutes, where the syllables in the stream formed statistical patterns to word boundaries. In this influential study, learning only required exposure to a syllable stream, consisting of four trisyllabic words occurring 45 times each, where prosodic cues such as stress and co-articulation to word boundaries were not present.</p>
<p>Following this initial work showing powerful learning, there is now a large literature on how the underlying computational mechanism can be best described, as well as the constraints for word segmentation to be successful. To understand the underlying computational mechanism, different computational models have been proposed (e.g., <xref ref-type="bibr" rid="c14">Frank et al., 2010</xref>; <xref ref-type="bibr" rid="c15">Giroux &amp; Rey, 2009</xref>; <xref ref-type="bibr" rid="c30">Perruchet &amp; Vinter, 1998</xref>; <xref ref-type="bibr" rid="c34">Swingley, 2005</xref>). For example, different models implement ideas on boundary finding (e.g., <xref ref-type="bibr" rid="c34">Swingley, 2005</xref>) vs. chunking (e.g., the PARSER model from <xref ref-type="bibr" rid="c30">Perruchet &amp; Vinter, 1998</xref>). Through computational modeling, concrete predictions of different theoretical approaches can be generated, which offer testable hypotheses about these different mechanisms that researchers were able to test further, using experimental methods (<xref ref-type="bibr" rid="c10">Endress &amp; Mehler, 2009</xref>). In addition to computational models, leveraging the learning constraints also helps understand the computational mechanism. Elsewhere in the language acquisition literature, for example, learning constraints are an important piece in understanding why the nature of the learning problem requires a representation that’s structure-dependent when studying the acquisition of syntax. In this instance, knowing when a set of learning theories succeeds and fails allows us to understand the intricacies of the learning mechanism. For word segmentation, one prominent constraint is that, even though infants and adults alike have shown success segmenting syllable sequences consisting of words that were uniform in length (i.e., all words were either disyllabic; <xref ref-type="bibr" rid="c11">Graf Estes et al., 2007</xref>; or trisyllabic, <xref ref-type="bibr" rid="c1">Aslin et al., 1998</xref>), both infants and adults have shown difficulty with syllable sequences consisting of words of mixed length (<xref ref-type="bibr" rid="c23">Johnson &amp; Tyler, 2010</xref>; <xref ref-type="bibr" rid="c21">Johnson &amp; Jusczyk, 2003a</xref>; <xref ref-type="bibr" rid="c22">2003b</xref>; <xref ref-type="bibr" rid="c16">Hoch et al., 2013</xref>). For example, <xref ref-type="bibr" rid="c23">Johnson and Tyler (2010)</xref> showed that if the sequence is constructed by concatenating two trisyllabic and two disyllabic words, infants were unable to segment from such a sequence, even though the infants in the same study had no trouble segmenting a sequence with its four words being all trisyllabic. Similarly, <xref ref-type="bibr" rid="c16">Hoch et al. (2013)</xref> showed that adults learned much worse with a mixed-length language than with a uniform-length language.</p>
<p>Another way of understanding the mechanisms for segmentation is by studying how fast learning takes place. Fast learning has always been a feature of statistical word segmentation, with the initial work showing that infants can segment words with only 2 to 3 minutes of exposure (<xref ref-type="bibr" rid="c32">Saffran et al., 1996</xref>; <xref ref-type="bibr" rid="c1">Aslin et al., 1998</xref>). It is also an important theoretical question, as the relationship between the amount of exposure and learning can be leveraged to understand the mechanism. A recent study showed that learners can succeed at segmentation when they are exposed to a stream where word forms occurred only two times under the word segmentation paradigm (<xref ref-type="bibr" rid="c37">Wang et al., 2023</xref>). This finding suggested that learners can rapidly extract word forms and remember them, and learning did not require a slow accumulation process. However, though segmentation was successful under these minimal conditions, the effect size of learning was small. Testing the same set of syllable sequences but with each word occurring four times, <xref ref-type="bibr" rid="c37">Wang et al. (2023)</xref> found that the effect size of learning was significantly larger in the latter condition, suggesting that, even though word forms may be extracted rapidly, repetition of the word forms in the sequence may be required for robust recognition when they were later queried in the test phase. There are other experiment paradigms in statistical learning, and the target detection paradigm was better suited to understand how fast learning occurs (<xref ref-type="bibr" rid="c3">Batterink, 2017</xref>). In this target detection paradigm, participants were asked to listen to syllable streams and press a key to detect a particular syllable in the stream. In each trial, twelve syllables were randomly grouped into four trisyllabic words, which were used to create a syllable sequence with all four words occurring 4 times. <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> found that, after <italic>one</italic> exposure to a trisyllabic word (e.g., tugola), learners were able to react faster to the second (or third) syllable of that trisyllabic word (<italic>go</italic> or <italic>la</italic>) than to the first syllable (<italic>tu</italic>). This effect was called the facilitation effect, with the idea being that learners who know a trisyllabic word can use the first syllable to predict the second and third syllables. This thus demonstrates that learners have sensitivity to the statistical structure of the stream after one exposure, under the target detection paradigm.</p>
<p>Understanding this effect is of great interest because it would inform the theories of statistical word segmentation and identify the computational models that can describe the effect best. <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> discussed that a chunking model, such as the one described in PARSER (<xref ref-type="bibr" rid="c30">Perruchet &amp; Vinter, 1998</xref>), is more consistent with the results than the use of conditional probabilities. With a chunking model such as PARSER, exposure to the syllable sequence would result in random chunks, which are stored in memory. After a single exposure to a word form in continuous input in PARSER (say, ABC with different letters representing different syllables) from the syllable sequence, the random process may sometimes produce a chunk that contains or partially contains the word form (such as ABC, or AB), and this can be used for facilitation. Notably, PARSER was originally used to explain performance with word segmentation paradigms, and if it were the case that PARSER can explain performance with target detection paradigms, it would suggest that the same mechanism would explain performance from two different paradigms.</p>
<p>In our view, however, there are reasons why PARSER is not well fit to explain the results from the target detection paradigms that have yet to be discussed. Of importance, forming chunks in a random manner only serves to mislead the learner whenever the chunks misalign with statistical regularities. Rather, it’s possible that the facilitation effect can occur without segmentation, word extraction, or chunking, per se. Storing bigrams and tracking transitional probabilities (TPs) from continuous input may suffice. By storing all bigrams in the input per standard statistical learning theory, learners would have access to TPs for different syllable transitions, and would be faster to detect targets with high TPs than those with low TPs. Under such a view, fast learning can be explained as long as learners can use small amounts of exposure to obtain bigram counts and TP values.</p>
<p>Notably, this hypothesized mechanism to explain performance in target detection tasks differs from mechanisms hypothesized to explain word segmentation, an offline task. While both tasks have been hypothesized to make use of tracking TPs, word segmentation involves additional constraints, as we mentioned above. The different performance of segmenting syllable sequences consisting of words that were uniform in length and syllable sequences consisting of words of mixed length suggests that additional processes, such as rhythm perception, are required to explain performance in word segmentation tasks (<xref ref-type="bibr" rid="c23">Johnson &amp; Tyler, 2010</xref>; Wang et al., under review). Thus, while learners could potentially learn from the same syllable sequence in both a word segmentation task and a target detection task, the different task demands may tap into different learning mechanisms.</p>
<p>The goal of the current study is threefold. First, we aim to replicate the findings of <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> and provide additional empirical evidence for the fast learning that was observed in that study. This replication would also provide an effect size estimate for when learners perform target detection in syllable sequences consisting of words that were uniform in length in Experiment 1. Secondly, we extend the target detection task to scenarios when learners perform target detection in syllable sequences consisting of words that were mixed in length in Experiment 2. Thirdly, we replicate the finding in word segmentation, showing a marked difference between segmenting a uniform and a mixed word-length sequence. Between these three experiments, we probe the mechanisms in the online target detection task and its relationship to the offline word segmentation task. This was done by leveraging what we know of typical word segmentation tasks, where segmentation tends to be successful when the input sequence contains uniform-length words, but fails when the words are mixed in length. If the target detection task shares the same mechanism with word segmentation, we would expect that the facilitation effect is stronger in sequences with uniform-length words compared to sequences with mixed-length words. However, if the mechanism for the online target detection task is different from segmentation, it may not require the learner to segment the continuous input, and word length uniformity does not matter. In this case, target detection with sequences containing mixed-length words would also be similarly successful as with sequences containing uniform-length words, and the mechanism involved would be the process of storing bigrams and calculating TPs. To provide further computational evidence for the proposed mechanisms, we conducted three simulations of these experiments. These simulations used either the TP tracking approach (Simulation 1) or a clustering approach (Simulations 2 and 3) to examine the learning effects in the experiments. These simulations provide information on how well these approaches using different information can approximate human performance. Together, the experiments and simulations provide insight into the mechanisms involved in the target detection task and its relationship to word segmentation.</p>
<sec id="s1">
<title>Experiment 1</title>
<p>In Experiment 1, we replicate <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> using the same material and the same uniform-length word design from the study. We conducted the replication two times, an exact replication and a conceptual replication. In addition to strengthening the robustness of the empirical finding, these different designs allowed a comparison of a nuisance variable, namely, whether the sequence initial (the first and the second) or the sequence final (the 47th and the 48th) syllables were included in the detection task. This manipulation was included to inform us of how specific the learning condition needs to be for the effect to occur.</p>
<sec id="s1-1">
<title>Methods</title>
<sec id="s1-1-1">
<title>Participants</title>
<p>The number of participants for the replication was determined based on a power analysis of the data from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, with some over-sampling. The main effect of interest was the interaction for RTs between the first and second presentation, where the second and third syllables were predictable during the second presentation but unpredictable during the first presentation. Based on the data from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, this difference was −13.6ms (the standard error was 4.91). In a one-sided test, this produced a post-hoc power of 0.85 with 19 subjects, which means that the original study was well-powered. As long as we have 19 subjects in any condition in our replication, it would also ensure the power of the replication study here.</p>
<p>We ran the study until the end of the semester, and by the time we stopped collecting data, in the exact-replication condition, we included data from twenty-one adult participants from both the University of Nevada, Las Vegas, and the University of Southern California. In the conceptual-replication condition, we included forty-eight participants from the same two institutions. IRB approval was obtained at each institution separately prior to conducting the experiment.</p>
</sec>
<sec id="s1-1-2">
<title>Stimuli</title>
<p>The stimuli were the same set from Batterink, who provided open materials online (retrieved from <ext-link ext-link-type="uri" xlink:href="https://osf.io/z69fs/">https://osf.io/z69fs/</ext-link>). Syllable sequences are constructed by concatenating syllables from two syllable inventories (from a male and a female speaker), each consisting of 24 unique syllables at a rate of 300 ms per syllable.</p>
</sec>
<sec id="s1-1-3">
<title>Design and procedure</title>
<p>The study closely followed the design of <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. To reiterate the design briefly here, each participant completed 144 iterations of the target detection task. In each iteration, 12 syllables were randomly chosen from a syllable inventory (male or female), which were used to create four trisyllabic words, exhausting all 12 syllables (i.e., one syllable occurred only in one word). Next, a syllable sequence was created by repeating the four words four times in a pseudo-random fashion, with the constraint that a word does not immediately follow itself. This meant that each syllable sequence was 48 (4*4*3) syllables long. The 144 iterations of the task included the use of 72 male- and female-voice syllable sequences, where either male or female first is counterbalanced between subjects. The experiment was self-paced and took about an hour to complete.</p>
<sec id="s1-1-3-1">
<title>Instructions</title>
<p>The experiment began with a short instruction phase. The following instruction was given, and the experimenter read the instructions aloud to the participants, allowing participants to ask questions at any point of the instruction phase.</p>
<p>“In this study, you will be presented with a rapid succession of syllables, and your job is to detect a particular syllable in a given sequence. In each trial, a target syllable will be presented (for example, ku), both visually on the screen and aurally in the headphones. After this, you will hear the syllable sequence (for example, bakufoka...) in the headphones, and your job is to press Space every time you detect the target syllable.</p>
<p>The key to this task is that you need to press the Space as soon as you detect the target syllable. As it would become clear to you in a moment, the syllables go by very quickly, and your job is to detect all of the target syllables as quickly and as accurately as you possibly can.</p>
<p>If you have understood the instructions, you may press Space to move to the next screen. If you have any questions regarding the task, please ask the experimenter now.”</p>
</sec>
<sec id="s1-1-3-2">
<title>Syllable detection phase</title>
<p>After the instruction phase, the syllable detection phase began. First, the participant was given the opportunity to practice for two trials, while the experimenter was present. After the practice period, the experimenter made sure that the participant was doing the task correctly and left the room.</p>
<p>Each trial in the syllable detection task began with the screen displaying “Get ready now. Press Space to start.” After the participant pressed the Space bar, they saw the target syllable displayed on the screen (e.g., “target syllable: vu”). After 1.5 seconds of silence, the participant heard the syllable from the headphones (e.g., the syllable vu), which lasted 0.3 seconds, and another 3.2 seconds of silence followed the target syllable. At this point (5 seconds after the start of the trial), the syllable stream began to play. The syllable stream lasted 14.4 seconds, during which the subjects were free to press Space to indicate that they detected the target syllable. At the end of the trial, the participant was informed as such, and the next trial began (“That is the end of this trial. The next trial will begin now.”). The study ended after all 144 trials were done. An illustration is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>An illustration of the target detection task, for both Experiment 1 and 2.</title>
<p>Two sets of sample vocabulary, targets, and syllable sequences are shown. The arrows indicate where the targets are in the syllable sequence.</p></caption>
<graphic xlink:href="tb26vv2_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>There were two conditions in Experiment 1, though the difference between the two was minimal. In the exact-replication condition, syllables were not detection targets if they were the first two or the last two in the syllable stream, the same as in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. In the conceptual-replication condition, this constraint did not apply. There seemed, prima facie, no reason to exclude the detection of a syllable when it was among the first two syllables or the last two syllables of the sequence, and the conceptual-replication condition was conducted to test this effect. The conceptual-replication condition thus served to test whether this design difference would make a difference in terms of the facilitation effect. Our null hypothesis here was whether the target syllable occurring in these arbitrary locations would not interfere with whether the learner could remember the sequence and use it for prediction.</p>
</sec>
</sec>
</sec>
<sec id="s1-2">
<title>Predictions</title>
<p>We reiterate the predictions for the replication study here. The prediction is that the second syllable in a trisyllabic word is detected faster than the first syllable after one (or more) exposures, and similarly for the third syllable compared to the first syllable, because while the first syllable is unpredictable, the second and the third syllables become predictable if the participant is able to remember the trisyllabic word given one exposure.</p>
</sec>
<sec id="s1-3">
<title>Results and Discussion</title>
<p>Prior to conducting the analysis, we dropped the trials that involved the first two/last two positions to make sure that the analysis examined the same type of data for the conceptual-replication condition. This meant that all the analyses below were based on reaction time data when the syllable to be detected was in the stream position 3-46. The rest of the analysis plan closely followed the analysis described in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. Before the analysis, we combined the counterbalancing conditions (female voice/male voice first).</p>
<p>For the main analysis, the first step we took was to convert the raw reaction time data into RT data for the target syllables. This calculation included two parts: whether a target syllable was detected, and what the RT was for that syllable. A target syllable was treated as detected if there was a key press within 1200ms after the onset of the syllable. Given this criterion, participants in the exact-replication condition detected 87.7% of the syllables on average, and participants in the conceptual-replication condition detected 87.2% of the syllables on average. Thus, the detection rates of syllables in both conditions were comparable to the one reported in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, which is 87.4%. All subsequent analyses are conducted on these data (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Reaction time (RT) data with syllable position (first, second, or third syllable in the word) on the x-axis, and word presentation (first, second, third, or fourth occurrence of the word in the stream) as different lines in the Figure.</title>
<p>The left panels show the raw data means, and the right panels show the regression model fit. The top panels showed the data from the conceptual-replication condition, and the bottom panel showed the data from the exact-replication condition. The style of the plot is similar to the one in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> for ease of comparison. Error bars represent ±1 SEM.</p></caption>
<graphic xlink:href="tb26vv2_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Next, the crucial prediction from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> was examined, i.e., that after just one exposure, there is an effect of “word form extraction” where there is an interaction between syllable position and presentation order such that syllable position 2 and 3 as opposed to 1 should have a smaller reaction time in later presentations (2, 3, 4) as opposed to the first presentation. This pattern was found in both of the conditions, which showed up in the right panels (predicted values from the regression model) in <xref ref-type="fig" rid="fig1">Figure 1</xref>. For visual inspection, one easy way is to observe the slopes of the lines connecting the data points for syllable positions 1 through 3, as this slope is negative if syllables 2 and 3 are reacted to faster than syllable 1. The prediction is thus that the slope for presentation 1 is not negative, but the slopes for presentations 2, 3, and 4 would be. Looking at <xref ref-type="fig" rid="fig1">Figure 1</xref>, we saw that the line for presentations 2 to 4 had negative slopes, whereas the slope for presentation 1 was not negative. To examine this effect statistically, we ran a linear mixed effects model in which RT is the dependent variable for each condition. The independent variable included fixed effects of word presentation (1-4, categorical; the choice of the variables being categorical vs. continuous was made in <xref ref-type="bibr" rid="c3">Batterink, 2017</xref>), syllable position (1-3, continuous), overall stream position (3rd through 46th syllable in the syllable sequence, continuous), and the interaction between word presentation and syllable position. Note that the overall stream position was found to be a significant predictor in addition to the rest of the variables in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, so it was included here. Random effects included participant as a random intercept and stream position as a random slope. For each condition, we first report the statistical significance of the omnibus interaction between word presentation and syllable position, and then report the pairwise comparisons between different pairs of presentation times.</p>
<p>Two more aspects of the data were examined following the analysis from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, which informs on the direction of the effect. If the effect was due to a slowdown of the unpredictable syllables for later presentations (i.e., presentations 2, 3, and 4) compared to the first presentation, this would predict the RTs for syllable position 1 to be smaller during presentation 1 compared to later presentations. This would also predict the RTs for syllable positions 2 and 3 to be the same between the presentations. On the other hand, if the effect was due to facilitation to react to the predictable syllables in later presentations, this would predict the RTs for syllable positions 2 and 3 to be smaller in later presentations compared to the first presentation, but the RTs for syllable position 1 to be similar for different presentations.</p>
<p>For the exact-replication condition, the omnibus interaction between word presentation and syllable position was significant (χ<sup>2</sup>(3) = 14.91, p = 0.002); also of note, stream position was not significant (β=0.0002, z=0.84, p=0.400); this might have been a result of a relatively small number of subjects in this condition. Next, pairwise comparisons between presentation 1 and later presentations were conducted; if the interaction coefficient is negative, it means the prediction was confirmed. The interaction between presentations 1 and 2 was negative and significant (β=−0.012, z=−2.95, p=0.003), and so was the interaction between presentations 1 and 4 (β=−0.015, z=−3.49, p&lt; 0.001). Only the interaction between presentations 1 and 3 did not reach significance (β=−0.005, z=−1.07, p= 0.287). Thus, all of the effects were numerically in the right direction, and most of the predictions were confirmed in this condition.</p>
<p>For the conceptual-replication condition, the omnibus interaction between word presentation and syllable position was significant (χ<sup>2</sup>(3) = 16.66, p = 0.001). The stream position was also significant (β=0.001, z=5.65, p&lt;0.001), successfully replicating this effect from <xref ref-type="bibr" rid="c3">Battarink (2017)</xref>, where syllables occurring later in the syllable stream are detected more slowly than syllables occurring earlier in the syllable stream. The interaction between presentation 1 and presentation 2 was negative and significant (β=−0.007, z=−2.17, p=0.030), so was the interaction between presentation 1 and 3 (β=−0.010, z=−3.03, p=0.002) and between presentation 1 and 4 (β=− 0.014, z=−3.94, p&lt;0.001). All of the effects were confirmed in the conceptual-replication condition. In sum, all of the results from the two samples showed that participants were able to react faster to the later syllables of a word compared to the first syllable following a single exposure.</p>
<p>Lastly, we examined the direction of the facilitation effect. This analysis mainly regards whether RTs for syllable position 1 in later presentations are faster or slower than in the first presentation. To conduct this analysis, we combined the data from the conceptual-replication condition and the exact-replication condition to achieve better statistical power. In the first mixed effects regression, the fixed effects included presentation and stream position, and the random effects included a by-subject intercept and a random slope of stream position. Results showed that later presentations took significantly longer to respond to compared to the first presentation (χ<sup>2</sup>(3) = 10.70, p=0.014), where the effect grew larger with each presentation (second presentation: β=0.011, z=1.82, p=0.069; third presentation: β=0.019, z=2.40, p=0.016; fourth presentation: β=0.034, z=3.23, p=0.001). Thus, while the predictable syllables (from positions 2 and 3 in the later presentations) were responded to faster, unpredictable syllables were also responded to more slowly, starting from the third presentation. This finding was different from the results from <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, who did not find the effect of presentation on syllable position 1 to be significant, possibly due to a power issue (if we ran the same analysis with data from either the exact or the conceptual condition, this effect was not significant either, which indicates the same power issue; we thank our reviewer for making this suggestion for running this analysis by combining data from the two conditions for better statistical power)..</p>
<p>In sum, both the exact-replication condition and the conceptual-replication condition were successful in replicating all of the aspects of <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. The exclusion of the detection of a syllable when it is among the first two syllables or the last two syllables of the sequence did not make a difference in generating the facilitation effect.</p>
</sec>
</sec>
<sec id="s2">
<title>Experiment 2</title>
<p>As we noted above, part of testing a powerful learning mechanism involves testing conditions when the learning mechanism is known to fail in specific conditions. To this end, we conducted Experiment 2, which differed from Experiment 1 in one crucial aspect. That is, we changed the lengths of the words that made up the continuous syllable sequences in Experiment 2. Rather than having them be all three syllables long, which is the case in Experiment 1, the four words making up sequences in Experiment 2 included 2 disyllabic and 2 trisyllabic words. In the word segmentation literature, using mixed-length designs leads to no segmentation (<xref ref-type="bibr" rid="c23">Johnson &amp; Tyler, 2010</xref>) or significantly weaker segmentation than with uniform sequences (<xref ref-type="bibr" rid="c16">Hoch et al., 2013</xref>; also see Experiment 3). Experiment 2 allows us to examine whether the target detection paradigm employs the same mechanism as word segmentation, which would predict that there would be a weaker facilitation effect in Experiment 2 compared to Experiment 1.</p>
<sec id="s2-1">
<title>Methods</title>
<sec id="s2-1-1">
<title>Participants</title>
<p>Twenty-one undergraduate students were recruited from Psychology Department subject pools at both the University of Nevada, Las Vegas, and the University of Southern California.</p>
</sec>
<sec id="s2-1-2">
<title>Stimuli</title>
<p>The stimuli were identical to the stimuli in Experiment 1.</p>
</sec>
<sec id="s2-1-3">
<title>Design and procedure</title>
<p>All aspects of the experiment were the same as Experiment 1, except for the sequences used for target detection. In Experiment 2, we generated the sequences by concatenating two disyllabic, and two trisyllabic words. In each sequence, the four words occurred 4 times, which is the same as in Experiment 1. This meant that each sequence was 40 syllables long. Target syllables could have been in any position for words of any length. All the rest of the dimensions are the same as the conceptual-replication condition from Experiment 1.</p>
</sec>
</sec>
<sec id="s2-2">
<title>Results and Discussion</title>
<p>Under the criterion that a syllable is detected if there is a key press within 1200ms after the onset of the syllable, participants, on average, detected 88.9% of the syllables. Before the analysis was run, we only kept data for stream positions 3-38, where the data for the first and last two positions in the stream were dropped.</p>
<p>The main analysis for Experiment 2 involved a linear mixed effects model that involved all factors of interest. In this regression (which we call regression 1), the RT was the dependent variable, and the independent variable included fixed effects of word presentation (1-4, categorical), position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous), and word length (disyllabic/trisyllabic) and their three-way interaction. One additional fixed effect was included, which was overall stream position (3rd through 46th syllable in the syllable sequence, continuous). Random effects included participant as a random intercept and stream position as a random slope. The main analysis showed that the three-way interaction was not significant (χ<sup>2</sup>(3) = 6.19, p = 0.103), suggesting that word length did not significantly interact with the main interaction of interest (between presentation and syllable position). Thus, we dropped the threeway interaction, and the new regression included word length as a main effect, the two-way interaction between presentation and syllable position, and a fixed effect of overall stream position as well as the same random effects as above (which we call regression 2). In this regression, the omnibus interaction between word presentation and syllable position was significant (χ<sup>2</sup>(3) =49.77, p&lt;0.001), suggesting that the interaction between presentation and syllable position was significant for both disyllabic and trisyllabic words. This was in addition to a significant fixed effect of word length (β=0.018, z=6.19, p&lt;0.001). This result thus shows that there is an overall effect of word length in addition to the interaction between presentation and syllable position, so next, we turn to pairwise comparisons in different word length conditions.</p>
<p>With data from both disyllabic and trisyllabic words, we constructed a regression model, each looking at the interactions between syllable position and presentation pairs. For trisyllabic words, the interaction between presentation 1 and presentation 2 was negative but not significant (β= −0.007, z=−1.09, p=0.277). The interaction between presentations 1 and 3 was negative and significant (β=−0.014, z=−2.07, p=0.039). The interaction between presentations 1 and 4 was negative and significant (β=−0.043, z=−6.21, p&lt;0.001). For disyllabic words, the regression containing both participant as a random intercept and stream position as a random slope did not converge. Thus, the regression for disyllabic words only included a by-subject random intercept as the random effect. The interaction between presentation 1 and presentation 2 was negative and significant (β= −0.024, z=−2.20, p=0.028). The interaction between presentation 1 and 3 was negative and marginally significant (β=−0.020, z=−1.85, p=0.065). The interaction between presentations 1 and 4 was negative and significant (β=−0.027, z=−2.16, p=0.031). Together, these analyses showed that there was a robust effect for trisyllabic and disyllabic words alike. A plot of the data is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Reaction time (RT) data with syllable position (first, second, or third syllable in the word) on the x-axis, and word presentation (first, second, third, or fourth occurrence of the word in the stream) as different lines in the Figure.</title>
<p>The left panels show the raw data means, and the right panels show the regression model fit. The top panels showed the data for the disyllabic words, and the bottom panel showed the data for the trisyllabic words. Error bars represent ±1 SEM.</p></caption>
<graphic xlink:href="tb26vv2_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Notably, we wondered if there is evidence that word length made no difference for the interaction between presentation and syllable position, and the frequentist approach (regression 1) only yielded a non-significant p-value, which cannot be considered as evidence for the null. To this end, we conducted a Bayesian analysis, using the approach outlined in Harms and Lakens (2018). The Bayes factor for the interaction can be obtained by computing Bayesian Information Criteria (BIC) for the null model and the alternative model (which are regression 2 and regression 1, respectively). The Bayes Factor is then given by e<sup>(BICalt - BICnull)/2</sup>. The Bayes Factor was found to be e<sup>25.65</sup> which is more than 10<sup>11</sup>, providing strong evidence that word length made no difference for the interaction between presentation and syllable position.</p>
<p>Lastly, we want to answer the question of whether the facilitation effect is larger in the uniform condition in Experiment 1 than in the mixed condition in Experiment 2, which would be the prediction if the current target detection task engages the same mechanism as the word segmentation paradigm. Notably, there are some differences in terms of the structure of data in the uniform and mixed conditions. First, the mixed condition (Experiment 2) involved both disyllabic and trisyllabic words, whereas the uniform condition (Experiment 1) only had trisyllabic words. For the analysis below, we put the experiment number in the fixed effect (with potential interactions with other variables). Secondly, the length of syllable streams was shorter in Experiment 2 compared to in Experiment 1, because half of the words were disyllabic in Experiment 2. This meant that streams were 48 syllables long in the uniform condition, but only 40 syllables long in the mixed condition. Since stream position has consistently been a significant predictor of reaction times, this is likely to affect the effects as well. Putting these two variables as main effects allowed us to observe the interaction of interest while controlling for these variables.</p>
<p>The prediction for the difference between the mixed and uniform conditions in the present target detection tasks, if they act similarly to word segmentation tasks, is that the effect is smaller in the mixed condition than in the uniform condition. For this analysis, we compared the data from Experiment 2 to the exact-replication condition in Experiment 1, which had a similar number of subjects (though using data from the conceptual-replication condition yielded the same results; see Appendix). To examine this effect, we set up the following mixed effects regression with a threeway interaction. The RT was the dependent variable, and the independent variable included fixed effects of experiment (Experiment 1/2, which correspond with uniform/mixed conditions, coded categorically), word presentation (1-4, coded categorically), position (1-3 for trisyllabic words and 1-2 for disyllabic words, coded continuously), and the interaction between the three. Fixed effect further included overall stream position (3rd through 46th syllable in the syllable sequence in the uniform condition, 3rd through 38th syllable in the mixed condition, both continuous) and word length (disyllabic/trisyllabic, categorical). Random effects included participant as a random intercept and stream position as a random slope. The omnibus three-way interaction was significant (χ<sup>2</sup>(3) =15.79, p=0.001), suggesting that the ways syllable position and presentation interact in the two experiments are different. To understand this three-way interaction, we looked at the three-way interaction between syllable position, condition, and pairs of presentations (i.e., 1 and 2, 1 and 3, and 1 and 4). We found that the three-way interaction for presentations 1 and 2 (β=−0.003, z=−0.52, p=0.600) was negative and not significant, became positive and not significant for presentations 1 and 3 (β=0.011, z=1.65, p=0.099), and became positive and significant for presentations 1 and 4 (β=0.021, z=2.99, p=0.003). In other words, the coefficients grow as a function of presentation in this three-way interaction. Looking at a plot of model fit (<xref ref-type="fig" rid="fig4">Figure 4</xref>), this pattern becomes clear: while the slopes (from syllable position 1 to 3) for presentation 1 were flat for both conditions, the negative slope for presentation 4 for the mixed condition was the largest in absolute value (from 570ms to 494ms) for all slopes, more than in presentation 4 for the uniform condition (from 579ms to 546ms). This was the three-way interaction we saw. We could understand this result as the mixed condition having a larger effect than the uniform condition, but as we explore in the simulation below, this statistical difference is consistent with a scenario where the facilitation effect is the same in both conditions.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Regression model fit from the three-way interaction between condition (mixed/uniform, categorical), word presentation (1-4, categorical), and position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous).</title>
<p>Figure 4A showed results from the uniform condition from Experiment 1, and Figure 4B showed results from the mixed condition from Experiment 2. Error bars represent ±1 SEM.</p></caption>
<graphic xlink:href="tb26vv2_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>These results differ from our a priori hypothesis that there is less learning in the mixed condition: the mixed condition did not generate a smaller effect than the uniform condition. This finding suggests that the mechanism behind the target detection task examined in this paper was different than the mechanisms involved in word segmentation. As the computational model below would suggest, the larger interaction effect in the mixed condition compared to the uniform condition is consistent with a facilitation effect that is compared in two conditions. Importantly, however, these effects are not expected if there is less learning in the mixed condition compared to the uniform condition, which has been the finding from segmentation tasks.</p>
<p>Before we report our computational model for these target detection results, we report an additional experiment testing word segmentation with both uniform and mixed length sequences. Even though we have cited evidence in the literature that participants are worse at segmenting mixed-length sequences than uniform-length sequences, this result is worth replicating in the same paper.</p>
</sec>
</sec>
<sec id="s3">
<title>Experiment 3</title>
<p>Experiment 3 was designed to provide evidence that word lengths make a difference for a different experiment paradigm, i.e., word segmentation. In the experiment presented below, we conducted a replication study of <xref ref-type="bibr" rid="c23">Johnson and Tyler (2010)</xref> with adult participants, asking whether adults rely on rhythm for segmentation tasks, as we propose infants do. Notably, if the conclusion is that syllable sequences with uniform and mixed word lengths are similarly learnable for adults, it demonstrates that infants alone have difficulty segmenting sequences with mixed word lengths given the comparison to <xref ref-type="bibr" rid="c23">Johnson and Tyler (2010)</xref>. Conversely, if adults also have difficulty learning sequences with mixed word lengths but no problem with uniform word lengths, this provides evidence that infants and adults alike have difficulty segmenting continuous sequences without rhythm, and with transitional probability alone, and this would be in contrast to findings from target detection paradigms in Experiments 1 and 2.</p>
<sec id="s3-1">
<title>Methods</title>
<sec id="s3-1-1">
<title>Subjects</title>
<p>A total of sixty undergraduate students at the University of Pennsylvania were recruited from the Psychology Department subject pool. Thirty subjects each were run in the uniform word length (UWL) condition and the mixed word length (MWL) condition.</p>
</sec>
<sec id="s3-1-2">
<title>Stimuli</title>
<p>We used the mbrola software package (Dutoit et al., 1996) to synthesize the speech stimuli in Experiment 1, in order to be consistent with the quality of the speech in <xref ref-type="bibr" rid="c23">Johnson and Tyler (2010)</xref>. For our English-speaking adults, we used the us1 diphone set. The syllables used were (in SAMPA notation that mbrola uses): [p A], [b I], [t I], [b u], [g @U], [l A], [t u], [d A], [d @U], [p I]. Consonants were 49ms long and vowels were 173ms long, so that all syllables were 222ms long. The fundamental frequency was 220Hz.</p>
</sec>
<sec id="s3-1-3">
<title>Design and procedure</title>
<p>Both the UWL and the MWL conditions in Experiment 1 had two phases: a learning phase followed by a testing phase. In the learning phase, participants were told to listen to a syllable sequence passively through the headphones while the screen was blank. They were asked to pay attention to the syllable sequence as they were to be tested on it later.</p>
<p>There were two conditions: a uniform word length (UWL) condition and a mixed word length (MWL) condition. In the UWL condition, four disyllabic words were randomly concatenated, two of which occurred with high frequency (90 times) and the other two occurred with low frequency (45 times). In the MWL condition, the two high frequency words were trisyllabic (each occurring 90 times) and the two low frequency words were disyllabic (each occurring 45 times). These two conditions were designed such that the part-words in both the UWL and the MWL conditions are the same and have the same bigram frequency (45 times). Each subject received a different random concatenation of the syllables during training (similar to <xref ref-type="bibr" rid="c23">Johnson &amp; Tyler, 2010</xref>), and the test items were also individually generated, given the randomization in the training sequence.</p>
<p>Immediately after the learning phase, we displayed instructions for the test phase on the screen. The instructions indicated that participants would hear a number of sound sequences and make judgments about the sequences. There was a total of 4 test items, two words and two partwords. The presentation sequence of test trials was randomized for each participant. Participants initiated each test trial. For each test item, a disyllabic sequence (either a word or a part-word) was played 3 times, with a 1-second pause between each sequence. After the presentation of the test item, participants were asked to rate the familiarity of the item: “Do you think that you heard this sequence in the previous section?” Their responses were marked on a scale containing five response items: “Definitely”, “Maybe”, “Not Sure”, “Maybe Not”, and “Definitely Not”. Once the participants made their choice, the screen went blank and the trial ended. After a one-second intertrial interval, the next trial began.</p>
</sec>
</sec>
<sec id="s3-2">
<title>Results and Discussion</title>
<p>We coded the scale of “Definitely”, “Maybe”, “Not Sure”, “Maybe Not”, and “Definitely Not” into the numeric values 4, 3, 2, 1, and 0, respectively (<xref ref-type="fig" rid="fig5">Figure 5</xref>). To compare these ratings statistically, we ran mixed effects linear regressions on the data. First, we ran an analysis on rating for both conditions separately, with item type (word vs. part-word) as the fixed effect and bysubject random intercepts. In the MWL condition, item type was not significant, indicating that there was no learning (β=−0.333, z=−1.65, p=0.100). We calculated a Bayes Factor for this condition with the same procedure as in Experiment 2, and found that the Bayes Factor was 2.87. According to <xref ref-type="bibr" rid="c19">Jeffreys (1998)</xref>, a Bayes Factor between 1.6 and 3.3 is considered to be substantial evidence for the null. In the UWL condition, item type was significant, indicating learning (β=−0.933, z=−4.52, p&lt;0.001). Second, we ran an analysis on data from both conditions, using the interaction and main effects of both conditions (UWL vs. MWL) and item type (word vs. part word). The interaction was significant (β=−0.600, z=−2.07, p=0.038), suggesting that learning was different between the two conditions.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Means for words and part-words, and difference scores for each subject in Experiment 1, collapsed over counterbalancing conditions.</title>
<p>In <xref ref-type="fig" rid="fig5">Figures 5A and 5C</xref>, each circle represents the mean rating of a subject for all words and part-words in the uniform and mixed word length conditions, with a solid line indicating the mean value for each item type. In Figure 5B and 5D, each circle represents the difference between mean ratings (words - part-words) for each subject in the uniform and mixed word length conditions, with the solid line showing the mean difference, and shadows showing 95% confidence intervals around the mean. The dotted line at 0 represents chance.</p></caption>
<graphic xlink:href="tb26vv2_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>A strong interpretation of this result is that adults, like 5.5-month-old and 8-month-old infants in <xref ref-type="bibr" rid="c23">Johnson and Tyler (2010)</xref>, are able to learn from a uniform-length word sequence but not from a mixed-length word sequence. A weaker interpretation is that adults are able to learn from a uniform-length word sequence, and learn it much better than the mixed-length word sequence. However, it is clear that both interpretations are consistent with our prediction that a crucial factor for infants and adults to perform word segmentation in these artificial language studies is word length uniformity (and as such, mechanisms other than tracking transitional probabilities may have explanatory power).</p>
</sec>
</sec>
<sec id="s4">
<title>Simulation 1</title>
<p>In Simulation 1, we hypothesized a specific computational process that can potentially explain human performance in the target detection paradigm in Experiments 1 and 2. The purpose of Simulation 1 is to provide a simple computational process that instantiates the assumption of what we believe contributes to the facilitation effect in target detection. In this model, we directly model RTs in this simulation, with the simple idea that syllables are either predictable or unpredictable in the input stream. RTs for predictable syllables are generated with one pattern, and RTs for unpredictable syllables are generated with another pattern.</p>
<p>This model is to process syllable sequences online and to generate an RT for each syllable that is processed. At the beginning of processing a syllable sequence, the model assumes the learner to detect the target with a baseline amount of time, RT0, which is a constant. From this point on, the model stores each bigram it encounters. Based on the bigrams that are stored at any point, the next syllable is either predictable or unpredictable. The core assumptions are that 1) predictable syllables get a facilitation effect when they are reacted to, and 2) unpredictable syllables do not. As such, we propose a simple recursive relation between the RT of a syllable occurring for the n<sup>th</sup> time and the n+1<sup>th</sup> time, which is:
<disp-formula id="FD1">
<alternatives>
<mml:math id="M1" display="block"><mml:mtext>RT</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>RT</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>n</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>stream</mml:mtext><mml:mo>_</mml:mo><mml:mtext>pos  </mml:mtext><mml:mo>*</mml:mo><mml:mtext> stream</mml:mtext><mml:mo>_</mml:mo><mml:mtext>inc            </mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>RT</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>n</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>stream</mml:mtext><mml:mo>_</mml:mo><mml:mtext>pos  </mml:mtext><mml:mo>*</mml:mo><mml:mtext> stream</mml:mtext><mml:mo>_</mml:mo><mml:mtext>inc</mml:mtext><mml:mo>+</mml:mo><mml:mtext>occ</mml:mtext><mml:mo>_</mml:mo><mml:mtext>inc</mml:mtext><mml:mo>*</mml:mo><mml:mtext>occurence   </mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math>
<graphic xlink:href="tb26vv2_eqn1.tif" mime-subtype="tif" mimetype="image"/>
</alternatives>
</disp-formula>
</p>
<p>and</p>
<p>RT(1) = RT0 + stream_pos * stream_inc, where the n in RT(n) represents the RT for the n<sup>th</sup> presentation of the target syllable, stream_pos is the position (3-46) in the stream, and occurrence is the number of occurrences that the syllable has occurred so far in the stream.</p>
<p>This process applies to the rest of the syllables in the sequence until the end of the syllable stream. At this point, each syllable in the sequence will have a corresponding RT. To simulate the process of a participant reacting to a single target syllable, we will output the RTs corresponding to a random target syllable, such that the only data left for a syllable sequence are 4 RT values for the 4 occurrences of the target syllable.</p>
<p>Here is a more in-depth discussion of the assumptions behind this simple model. First, if a syllable occurs for the first time, we expect the learner to detect the target with a baseline amount of time. Theoretically, we take this to mean that it would take a certain amount of time to recognize and react to a syllable for the first time. Secondly, the next time the target syllable occurs, the amount of time it takes to react to the target syllable depends on whether this syllable is predictable or not. If it is unpredictable, the amount of time it takes to react is the same amount of time as the last time it was reacted to. If the target syllable is predictable, the amount of time it takes to react is different from the last time, by a constant (occ_inc) times the number of times this syllable has occurred so far. Theoretically, if it takes a certain amount of time to react to the target syllable the last time, this time, the reaction to the target syllable is facilitated by its predictability, where the amount is proportional to the number of times this target syllable has occurred so far. The assumption that the facilitation amount is proportional to the number of times the target syllable has already occurred is based on the empirical finding that the more the target syllable is detected, the faster the RT is. The constant (occ_inc) represents the amount of facilitation effect due to predictability. In addition to the predictability factors, one more (positive) number needs to be added to each RT, which is a stream-position effect: the later the syllable is in the stream, the slower the RT is. This is also based on empirical findings from the task. For a discussion of the specifics of setting these parameters, see Appendix.</p>
<p>There are three parameters in our set of equations. The first, the baseline RT (RT0), does not factor into the pattern of data results later, as all RTs share this component equally. We set this RT0 to be the constant from the regression coefficient from previous regressions. The second constant is the stream_inc, the increment amount for the stream position. Again, it is common to all RTs. We set it as a small, positive number, which represents the general trend that RTs are larger the later the target is in the stream. The third constant is occ_inc, the increment for the number of targets that have already occurred. We know this number to be negative (i.e., more occurrences would mean smaller RTs). We took a small, negative number from the corresponding regression coefficient. Notably, though we took the estimates from the regressions, this by no means would mean that the resulting RT distribution would resemble the RT distributions from humans. The point of this simulation is to consider the properties of the model when we only consider very few factors (predictability/structure of the syllable sequence) and see if RT distributions based on these factors can be similar to the RT distributions from human data.</p>
<p>To implement this model computationally, we went through a few steps. First, we constructed the syllable sequences in the same way as we did in the experiments. Note that, during this step, there is randomness in constructing the syllable sequences, as different words can be concatenated in different orders while maintaining the constraints for the order (i.e., no word can follow itself). Next, we implement the target detection section of the task, randomly picking a target syllable in the syllable stream. We generated RTs for all syllables based on the formula described above, though, for the data from this simulation, only the RTs associated with the targets were saved in the data. To do this, in an online fashion, the model stores the bigrams that it has encountered so far, and calculates the RT of the next syllable based on the bigrams from the collection of bigrams that are remembered, and the RT of the syllable from the last occurrence. Simply put, the RTs for the unpredictable syllables only include the baseline RT plus positive change as a function of the stream position. The RTs for the predictable syllables are a function of how predictable they are, on top of initial conditions. Again, note that no “word extraction” is required: the model only requires exposure to the input and stores the bigrams it encounters. There are bigrams that are predictable and unpredictable, and there is no need to make inferences over where the word boundaries are in the input sequence for the model to operate.</p>
<p>Given this model, we conducted two simulations: a uniform condition simulation and a mixed condition simulation. These two simulations mirrored the structure of Experiments 1 and 2 above, in terms of how the syllable sequences were set up. In each simulation, we generated data for the same number of subjects (19, from <xref ref-type="bibr" rid="c3">Batterink 2017</xref>) and the same number of trials (144). For each trial, we generated the RT values according to the formula described above. Notably, the same parameters are used in both conditions. The simulations thus represent learners with the same learning characteristics: by using the same set of parameters going into the two conditions, we are assuming these learners behave the same for the two conditions.</p>
<p>Running the model generates simulated data for each condition. With simulated data, we ran the same set of regressions as we did in the experiments. First, for each simulation, we looked at the (fixed) effect of syllable position (1-3), presentation (1-4), and their interaction, in addition to stream position (1-48). Next, we conducted a three-way interaction for syllable position (1-3), presentation (1-4), and condition (uniform/mixed). All these regressions included by-subject random intercepts and a random slope of stream position, the same as the regressions we ran for experiments.</p>
<p>The results for the model mirrored the qualitative pattern of data from human experiments. First, we found that the slope for the first presentation in the fitted model across three syllable presentations is the same, flat slope as we observed in the human data, for both the uniform and mixed conditions. Second, we found that there was a three-way interaction, the same way as the human results: The slope for the fourth presentation of the mixed condition is larger than in the uniform condition, given the same slopes for the first presentations in both conditions (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Regression model fit from the three-way interaction between condition (mixed/uniform, categorical), word presentation (1-4, categorical), and position (1-3 for trisyllabic words and 1-2 for disyllabic words, continuous) for the simulated data.</title>
<p>Figure 6A showed results from the simulation for the uniform condition, and Figure 6B showed results from the simulation for the mixed condition.</p></caption>
<graphic xlink:href="tb26vv2_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>The fact that such a simple model can capture the same patterns from the human results is remarkable. The simplicity is based on the number of assumptions that went into the model, which are simply that predictable targets get shorter RTs, the amount of which is based on the number of times this particular target has occurred so far. This means that no other assumptions are required for the facilitation effect to occur. If one compares this model to other models for segmentation (e.g., the ones listed in <xref ref-type="bibr" rid="c4">Bernard et al., 2020</xref>), this model would have the fewest number of assumptions built in. More importantly, when we set the same parameter for the uniform and the mixed condition in the simulations, that is, setting the amount of change to be the same for predictable items in two conditions, we find that the same difference as we found in the human experiments, which is that the mixed condition showed a larger effect than the uniform condition. This may provide an explanation for our behavioral result, namely, that the larger effect in the mixed condition does not suggest that people reacted more quickly in the mixed condition, but is a reflection of mean lengths of the words in the syllable sequence - that is, the effect may be a result of total stream length difference between the two conditions (for a more thorough exploration of this effect, see the additional simulations in the Appendix). Importantly, for the current discussion on the origin of the difference, the same amount of facilitation effect from previous occurrences in our computational model provides a good fit for the human data.</p>
</sec>
<sec id="s5">
<title>Simulation 2</title>
<p>An alternative computational model to explain the facilitation effect in the target detection task is PARSER (<xref ref-type="bibr" rid="c30">Perruchet &amp; Vinter, 1998</xref>), which was proposed as a potential explanation for the facilitation effect in <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>. PARSER processes continuous syllable sequences in two iterative steps. In the first step, PARSER randomly picks a number from 1 through n (typically 3), and clusters this random number of syllables as a chunk. This step creates chunks and stores them in memory with certain weights associated with each one (termed Perceptual Shaper). In the second step, PARSER either strengthens the weight or decreases the weight of items in the Perceptual Shaper: If the incoming chunk matches an existing chunk, the weight of the existing chunk (and its components) is increased. However, if the incoming chunk is completely new, it is added to the Perceptual Shaper, but at the same time, the weights of all the previous chunks are decreased. The updating of the weights occurs in time steps, and the two steps occur during each time step. With this iterative process, PARSER can successfully segment a syllable sequence into its component words, because these words (and their components) are more likely to repeatedly occur, much more likely than part-words.</p>
<p>So, can PARSER explain the facilitation effect, given that the facilitation effect occurs after a single exposure in both <xref ref-type="bibr" rid="c3">Batterink (2017)</xref> and Experiment 1 in the present study? To answer this question empirically, we created the following simulations using the U-Learn program (<xref ref-type="bibr" rid="c31">Perruchet et al., 2014</xref>), a program developed by the authors of the original PARSER paper. Whereas the PARSER model was created to model word segmentation of continuous syllable sequences, there is a lack of a linking assumption between the weights of different chunks and data from target detection experiments. We made the following linking assumption: after asking PARSER to output weights for words vs. part-words, if the words and part-words are differentially weighted, this is equivalent to the facilitation effect in target detection (i.e., we take the learning effect from PARSER to indicate learning). As the weights of syllable sequences are the main output of the PARSER model, we felt that this was the most reasonable approach to adopt.</p>
<p>One other feature of the U-Learn program is that it reports its weight changes in 10 timesteps (which corresponds to 1/10 of the learning sequence, however long the learning sequence is). We made use of this feature to examine the relationship between the amount of exposure and learning. That is, if we use 4 different words each of which occurs 10 times (modifying the existing “ready-to-use configurations”), 1/10 of the syllable sequence is 4 words long (which should contain a word form once, on average) and 2/10 of the syllable sequence is 8 words long (which should contain the same word form twice, on average). The idea is that, if the weights for words became significantly more than the weights for part-words after 1 or 2 time points in this simulation, it would be evidence that there is learning after 1 or 2 exposures to a word form. We created the sequence and the test items by modifying the ready-to-se configurations (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>The set-up for simulation one, where four trisyllabic words occur 10 times each.</title></caption>
<graphic xlink:href="tb26vv2_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Using this setup, we ran the simulation 50 times to represent running 50 subjects on this task. The results are shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>. We find that PARSER is successful after finishing running the 40-word sequence most of the time in simulation. The crucial question for the current simulation is whether there is any learning after 1/10<sup>th</sup> or 2/10<sup>th</sup> of the sequence. Observing the weight changes in the learning curve, we see that there is no learning (<xref ref-type="fig" rid="fig8">Figure 8</xref>) during this period. The model also produced the weights for a potential statistical comparison, but in this instance, all weights of words and partwords for 1/10<sup>th</sup> and 2/10<sup>th</sup> of the sequences were 0s (and thus a statistical test is not needed). Thus, PARSER failed to segment/show a facilitation effect after either one or even two exposures. In fact, it can be seen from <xref ref-type="fig" rid="fig8">Figure 8</xref> that out of 50 runs of the model in each condition, only 1 run produced a non-zero weight for words during 3/10<sup>th</sup> of the sequence.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>The results for Simulation 2, where the thin lines show the weight changes of words and part-words over the course of learning in individual runs, the thick lines show the mean weight of words and part-words across runs over the course of learning.</title>
<p>On the x-axis, each number represents the model having processed 1/10 of the sequence (i.e., 3 means the model has processed 3/10 of the sequence so far). The y-axis represents weights, which are in arbitrary units (as described in PARSER documentation).</p></caption>
<graphic xlink:href="tb26vv2_fig8.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Simulation 2 thus shows that PARSER cannot learn after a small amount of exposure, and thus cannot explain the facilitation in the target detection tasks. In both scenarios, humans are better learners than PARSER as they were able to learn from sequences much faster than the algorithm using clustering as the mechanism for learning.</p>
</sec>
<sec id="s6">
<title>Simulation 3</title>
<p>In Simulation 3, we used PARSER to perform a simulation of Experiment 3. In Experiment 3, there was a uniform-length condition and a mixed-length condition, so Simulation 3 can potentially inform us of whether clustering-based approaches can explain human performance in the word segmentation task. To set this up, we modified the existing <xref ref-type="bibr" rid="c1">Aslin et al. 1998</xref> simulation in PARSER. Specifically, the uniform-length condition was the same as the existing <xref ref-type="bibr" rid="c1">Aslin et al. 1998</xref> simulation. The mixed-length condition was modified in the way shown in <xref ref-type="fig" rid="fig9">Figure 9</xref>. Specifically, the two low frequency words were changed to disyllabic, while maintaining the other two high frequency words to be trisyllabic. The test items were changed to disyllabic words, as was done in other experiments.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><title>The set-up for the mixed condition in Simulation 2.</title></caption>
<graphic xlink:href="tb26vv2_fig9.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Next, we ran the simulations for the two conditions 50 times each. This corresponds to running 50 subjects in each condition. The results are shown in <xref ref-type="fig" rid="fig10">Figure 10</xref>. From these simulations, we see that PARSER is similarly successful in both conditions. With the weights from the simulations, we performed two statistical comparisons to answer two questions. The first question was whether the weight difference between words and part-words is different between conditions. For this analysis, we gathered the final weights (the weights after all the sequences have been learned, which correspond to the 10<sup>th</sup> segment in the figures) from each type of item in the two conditions. With the two factors (word type and condition), we performed a two-way anova, which showed that there was no significant interaction (F(1) = 1.78, p=0.184). This suggests that, following the exposure to the entire sequence in each condition (which simulates human learning performance in segmentation), there was no difference in terms of PARSER performance in the two conditions. The second question was whether there was any point during the time course of the exposure to the sequence where learning performance diverged. For this analysis, we gathered the weights in each of the 10 segments and performed the same two-way anova, asking if there was a significant interaction between word type and condition. Even without multiple comparison correction, which lowers the alpha values to a value smaller than 0.05, none of the 10 segments had a significant interaction, as all the p-values were larger than 0.14. Thus, we find that there was no point at which there was a significant interaction between word type and condition.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><title>The results for Simulation 3, where Figure 10A shows the weight changes of words and part-words in the uniform condition, and Figure 10B shows the weight changes of words and part-words in the mixed condition.</title>
<p>On the x-axis, each number represents the model having processed 1/10 of the sequence (i.e., 3 means the model has processed 3/10 of the sequence so far). The y-axis represents weights, which are in arbitrary units (as described in PARSER documentation).</p></caption>
<graphic xlink:href="tb26vv2_fig10.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Given these results from Simulation 3, we see that PARSER is capable of learning in both mixed and uniform conditions. We also see that the rate of learning as well as final learning outcomes, are qualitatively similar. When compared with human performance in the word segmentation task given a relatively large amount of exposure, PARSER represents a more powerful learner than humans, as PARSER has no trouble with segmenting a mixed-length sequence.</p>
<p>Taken together, we find that results from PARSER and human performance differ in important ways. When learning from a small amount of exposure, which is the case in a target detection task, PARSER cannot learn, but humans can. When learning after a large amount of exposure, which is the case in word segmentation tasks, PARSER learns uniform and mixed word length sequences equally well, which also differ from human performance. Thus, we find that PARSER is not as sensitive to statistical regularities as humans during the initial encounters with statistical regularities, and is too powerfully equipped to learn complicated sequences when humans would have trouble.</p>
</sec>
<sec id="s7" sec-type="discussion">
<title>General Discussion</title>
<p>This paper investigated the mechanisms involved in statistical word segmentation and target detection, reporting three experiments and three simulations. In Experiment 1, we reported a replication of <xref ref-type="bibr" rid="c3">Batterink (2017)</xref>, including both a conceptual replication and an exact replication. The facilitation effect was successfully replicated in both cases, where the reaction time was shorter for predictable syllables (syllable positions 2 and 3 in a triplet) compared to unpredictable syllables (syllable position 1) in later presentations (2, 3, 4) as opposed to the first presentation. In Experiment 2, we changed the structure of the syllable sequences in the study, where instead of using words of uniform length (which was the case for Experiment 1), we used sequences with mixed-length words. Such a change was shown to generate a smaller amount of learning in the word segmentation literature using the word segmentation paradigm. Contrary to the prediction based on the segmentation literature that uniform-length sequences are learned better than mixed-length sequences, we found that the effect in the mixed condition (Experiment 2) was larger in the uniform condition (Experiment 1). In Experiment 3, the finding that adults are better at segmenting a sequence containing uniform-length words than a sequence containing mixed-length words using the word segmentation task was replicated.</p>
<p>To explain the computational mechanisms for our experimental results, we conducted three simulations. In Simulation 1, we computationally modeled the facilitation effect in the target detection task. The assumption built into the model involved changes to the RT based on predictability, where we assumed a recursive relation between RTs for consecutive items, and predictive and unpredictable targets had different relations to the RT of the previous item. The simulation produced data patterns similar to data from Experiments 1 and 2, and to the uniformlength conditions and mixed-length conditions alike. Simulation 1 thus provided evidence that the same computational processes and parameters generated similar effects for both the uniform and mixed conditions. In fact, with the same facilitation parameter in both conditions, we found a larger effect in the mixed condition, and this is consistent with our data. In Simulation 2, we explored the viability of using clustering-based approaches (specifically, the computational model PARSER) as it was assumed to account for target detection performance in the previous literature (<xref ref-type="bibr" rid="c3">Batterink, 2017</xref>). Simulation 2 showed that, while PARSER can eventually produce different weights for words vs. part-words, which were akin to the facilitation effect, this effect did not appear unless the model encountered more than one or two exposures. Compared to human performance, this computational model required more exposure to learn statistical regularities in the input. In Simulation 3, we used PARSER again to simulate Experiment 3, which was a word segmentation task. We found that PARSER was able to learn from uniform-length and mixed-length sequences in a similar fashion, which differed from how humans learned the two types of sequences. Together, these results suggest that a simple prediction-based mechanism can explain the results from the target detection task, and clustering-based approaches such as PARSER cannot, contrary to previous claims.</p>
<p>The experimental data and the simulations provide evidence for our hypothesis that statistical word segmentation and target detection tasks employ different mechanisms. Notably, a null hypothesis would be that a single mechanism would explain performance in the two tasks. Following the discussion above, if this singular mechanism were the tracking of transitional probabilities, it could explain performance in target detection tasks, but could not explain performance differences in learning sequences with uniform-length and mixed-length words in word segmentation tasks. If this single mechanism were rhythm perception, it could explain performance in statistical word segmentation tasks, but could not explain performance in target detection tasks. If this single mechanism were clustering, it could neither explain the better performance in performance differences in learning sequences with uniform-length and mixed-length words in word segmentation tasks, nor the rapid learning in target detection tasks, as we showed in our simulations. As such, different mechanisms need to be posited to explain performance in different tasks, even though the tasks share the feature of requiring learners to learn from statistical regularities.</p>
<sec id="s7-1">
<title>Different mechanisms in the target detection and word segmentation tasks</title>
<p>Having shown that statistical word segmentation and target detection tasks employ different mechanisms, we will discuss potential reasons why different tasks require different mechanisms, which include different computations involved in the two tasks as well as the representations required to support the computations. To summarize, we believe that there are potentially shared computational processes for the two tasks, but the learning effects require different computations at different locations in the syllable sequence, and the representations supporting the two tasks are also different.</p>
<p>While both tasks potentially require learners to store bigrams and larger n-grams during learning, the two tasks require learners to use this information differently. In the target detection task, learners need to store the bigrams and compute transitional probabilities for syllable transitions, and this knowledge of syllable transitions with different TPs is all that is required. Whereas the syllable transitions with low TPs are responded to with a baseline level of RTs in the target detection task, the main effect of interest, i.e., the facilitation effect, occurs at word internal locations. This differs from the word segmentation task, where learners need to make decisions about where the word boundaries are. As an example, we consider the following sequence: GHIABCDEF<underline>ABC</underline>GHI, where the word “ABC” is preceded and followed by different words. In the target detection task, if the target is syllable B, it is theoretically possible to predict B from the preceding syllable A after one occurrence of AB, and experimental evidence supports this theoretical analysis. This differs from word segmentation, where learners need to decide where the word boundaries are. For each word, two word boundaries are required. Given the example sequence in this paragraph, after the learner processed the ABCGHI section, they would understand that A can be preceded by I or F, C is followed by D or G, both of which are required for segmentation. As such, multiple exposures (at least two) are required to make segmentation possible. Whereas a single exposure could allow predictions between syllables in target detection to occur.</p>
<p>(There are cases exceptional to these discussions. If the learner can segment through subtraction, it would only require a single exposure to the novel word. In this instance, the linguistic materials surrounding it need to be known. For example, consider the English sentence “I bought a dax yesterday”, where one knows all the other words in the sentence. In this case, the novel word can be segmented distributionally through subtraction (Lignos &amp; Yang, 2010). But most discussions on word segmentation regard scenarios where most if not all word forms are unknown, so the example in this footnote counts more as an exception.)</p>
<p>The two tasks also require different representations. In target detection, the detection speed for reacting to the target is only influenced by the knowledge of the specific transitions in the sequence (i.e., whether the current syllable is predictable or unpredictable given the previous one), as we showed in our model. In this sense, no segmentation is required; remembering bigrams, as we demonstrated in our model, would suffice for this task. It is possible that the only representations required for target detection are the bigrams. This differs from the word segmentation tasks in several aspects. First, the segmentation task requires the learner not only to segment the sequence, but also to remember the segmented subsequences in memory. Representing only where the word boundaries are would not be enough. Secondly, in word segmentation, learners use all possible information that they can use, including prosodic information (e.g., <xref ref-type="bibr" rid="c24">Jusczyk et al., 1999</xref>). As such, it requires learners to represent other information, such as prosodic information. Specifically in sequences used for statistical word segmentation tasks, while prosodic information such as stress is typically not present, sequence with uniformlength words produces a rhythm percept that learners can use for segmentation. This representation of rhythm is required to explain the different levels of learning performance for uniform- and mixed-length sequences. In contrast, learners performing target detection in these different types of sequences had similar performance, suggesting that rhythm did not play a role in target detection.</p>
<p>In sum, the two tasks may both require the learner to use co-occurrence information from the sequence, they require the learner to process different information to accomplish and thus require different task-demands and mechanisms.</p>
</sec>
<sec id="s7-2">
<title>Time courses for the facilitation and other similar effects</title>
<p>Through empirical work and a computational model, we provided evidence that the facilitation effect happened only after one exposure in the target detection task, and we mentioned that two exposures may suffice for the word segmentation task (<xref ref-type="bibr" rid="c37">Wang et al., 2023</xref>). While these results nicely demonstrate the rapidity with which statistical learning occurs, there is a fuller picture of the relationship between learning and the amount of exposure when we consider studies that provide more exposure to learners. For example, even though learning was successful within the word segmentation paradigm with only two exposures, four exposures produced significantly more robust learning (<xref ref-type="bibr" rid="c37">Wang et al., 2023</xref>). At the same time, it’s not the case that more exposure equals more learning. For example, Bulgarelli and Weiss (2016) conducted a study looking at the time course of learning. Participants were presented with multiple 67-second syllable sequences (which contained hundreds of syllables) and tested between the presentation of each syllable sequence. Learning plateaued after a single block of learning, where the effect size of learning never changed following the first block or after several blocks of learning (similar results have been reported in <xref ref-type="bibr" rid="c12">Finn &amp; Hudson Kam, 2008</xref>; <xref ref-type="bibr" rid="c29">Newport and Aslin, 2004</xref>). In sum, the relationship between exposure and learning is complicated, requiring an examination of the cognitive mechanisms involved in segmentation as a function of time and complexity of the learning materials (e.g., sequences with uniform- vs. mixed-length words), a topic for future work.</p>
<p>There is another line of studies that provides insight into the relationship between exposure and online learning, similar to the target detection task. In serial reaction time (SRT) tasks, participants make a key press for every stimulus, unlike the target detection task, which requires key presses only for a single target. These studies have also been used to examine the learning of statistical dependencies (e.g., <xref ref-type="bibr" rid="c17">Howard &amp; Howard, 1997</xref>; <xref ref-type="bibr" rid="c18">Hunt &amp; Aslin, 2001</xref>; <xref ref-type="bibr" rid="c36">Wang &amp; Kaiser, 2022</xref>), though the learning effects are typically observed in blocks, rather than for the first few observations. For example, in <xref ref-type="bibr" rid="c18">Hunt and Aslin (2001)</xref>, participants completed 70-word sessions, and completed 8 sessions a day for 6 consecutive days. While the question of how many sessions are required to produce a reliable effect was not explored directly in that study, the data showed participants took multiple sessions to show a learning effect in many experiments. The slow emergence of the learning effect may have to do with the fact that making a key press for every stimulus requires the learner to pay constant attention to the upcoming stimulus in order to take an action (making a key press). In contrast, in target detection tasks, there is no action required for most of the stimuli, so that the participants may plan their action while processing the stimuli. It is of interest to understand the specific time course of when the learning effect takes place in SRT tasks in future studies.</p>
</sec>
<sec id="s7-3">
<title>Conclusions</title>
<p>In summary, the current study found that the facilitation effect from the target detection task is empirically robust and can be shown with sequences with uniform-length words or mixed-length words alike. The speed for a facilitation effect following a predictable sequence to appear is indeed at its theoretical limit of just one prior encounter, both when the syllable sequence contains mixed-length words and only uniform-length words. Through computational modeling, we provided a possible mechanism to explain this facilitation effect from the target detection task and ruled out a computational model that was previously theorized to account for the learning process. Importantly, these results provide evidence for our claim that the mechanisms involved in the target detection task are different from those in the word segmentation task. Future exploration is needed to understand the relationship between the amount of exposure and learning in statistical word segmentation, as well as a characterization of the memory mechanisms that are involved during the segmentation process.</p>
</sec>
</sec>
</body>
<back>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>The data reported in this paper is available, at <ext-link ext-link-type="uri" xlink:href="https://osf.io/63y2g/">https://osf.io/63y2g/</ext-link>.</p>
</sec>
    <sec id="additional-files" sec-type="supplementary-material">
        <title>Additional files</title>
        <supplementary-material id="supp1">
            <label>Supplementary file 1</label>
            <media xlink:href="tb26vv2_file01.docx"/>
        </supplementary-material>
    </sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, <string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Computation of conditional probability statistics by 8-month-old infants</article-title>. <source>Psychological science</source>, <volume>9</volume>(<issue>4</issue>), <fpage>321</fpage>–<lpage>324</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barakat</surname>, <given-names>B. K.</given-names></string-name>, <string-name><surname>Seitz</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2013</year>). <article-title>The effect of statistical learning on internal stimulus representations: Predictable items are enhanced even when not predicted</article-title>. <source>Cognition</source>, <volume>129</volume>(<issue>2</issue>), <fpage>205</fpage>–<lpage>211</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Batterink</surname>, <given-names>L. J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Rapid statistical learning supporting word extraction from continuous speech</article-title>. <source>Psychological Science</source>, <volume>28</volume>(<issue>7</issue>), <fpage>921</fpage>–<lpage>928</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernard</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thiolliere</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Saksida</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Loukatou</surname>, <given-names>G. R.</given-names></string-name>, <string-name><surname>Larsen</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M.</given-names></string-name>, … &amp; <string-name><surname>Cristia</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>WordSeg: Standardizing unsupervised word form segmentation from text</article-title>. <source>Behavior research methods</source>, <volume>52</volume>, <fpage>264</fpage>–<lpage>278</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bertels</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Boursain</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Destrebecqz</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Gaillard</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Visual statistical learning in children and young adults: how implicit?</article-title> <source>Frontiers in Psychology</source>, <volume>5</volume>, <fpage>1541</fpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bertels</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Demoulin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Franco</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Destrebecqz</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Side effects of being blue: influence of sad mood on visual statistical learning</article-title>. <source>PloS one</source>, <volume>8</volume>(<issue>3</issue>), <elocation-id>e59832</elocation-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bertels</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Franco</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Destrebecqz</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2012</year>). <article-title>How implicit is visual statistical learning?</article-title> <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>38</volume>(<issue>5</issue>), <fpage>1425</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conway</surname>, <given-names>C. M.</given-names></string-name></person-group> (<year>2020</year>). <article-title>How does the brain learn environmental structure? Ten core principles for understanding the neurocognitive mechanisms of statistical learning</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>112</volume>, <fpage>279</fpage>–<lpage>299</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davachi</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>DuBrow</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>How the hippocampus preserves order: the role of prediction and context</article-title>. <source>Trends in cognitive sciences</source>, <volume>19</volume>(<issue>2</issue>), <fpage>92</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Endress</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Mehler</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>The surprising power of statistical learning: When fragment knowledge leads to false memories of unheard words</article-title>. <source>Journal of Memory and Language</source>, <volume>60</volume>(<issue>3</issue>), <fpage>351</fpage>–<lpage>367</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Estes</surname>, <given-names>K. G.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Alibali</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Can infants map meaning to newly segmented words? Statistical segmentation and word learning</article-title>. <source>Psychological science</source>, <volume>18</volume>(<issue>3</issue>), <fpage>254</fpage>–<lpage>260</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finn</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Hudson Kam</surname>, <given-names>C. L.</given-names></string-name></person-group> (<year>2008</year>). <article-title>The curse of knowledge: First language knowledge impairs adult learners’ use of novel statistics for word segmentation</article-title>. <source>Cognition</source>, <volume>108</volume>(<issue>2</issue>), <fpage>477</fpage>–<lpage>499</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franco</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Eberlen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Destrebecqz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cleeremans</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bertels</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Rapid serial auditory presentation</article-title>. <source>Experimental psychology</source>, <volume>62</volume>(<issue>5</issue>), <fpage>346</fpage>–<lpage>351</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frank</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Goldwater</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name>, &amp; <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Modeling human performance in statistical word segmentation</article-title>. <source>Cognition</source>, <volume>117</volume>(<issue>2</issue>), <fpage>107</fpage>–<lpage>125</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giroux</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Rey</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Lexical and sublexical units in speech perception</article-title>. <source>Cognitive Science</source>, <volume>33</volume>(<issue>2</issue>), <fpage>260</fpage>–<lpage>272</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoch</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tyler</surname>, <given-names>M. D.</given-names></string-name>, &amp; <string-name><surname>Tillmann</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Regularity of unit length boosts statistical learning in verbal and nonverbal artificial languages</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>20</volume>, <fpage>142</fpage>–<lpage>147</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13423-012-0309-8</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Howard</surname>, <given-names>J. H.</given-names>, <suffix>Jr.</suffix></string-name>, &amp; <string-name><surname>Howard</surname>, <given-names>D. V.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Age differences in implicit learning of higher order dependencies in serial patterns</article-title>. <source>Psychology and Aging</source>, <volume>12</volume>, <fpage>634</fpage>–<lpage>656</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0882-7974.12.4.634</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hunt</surname>, <given-names>R. H.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Statistical learning in a serial reaction time task: access to separable statistical cues by individual learners</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>130</volume>(<issue>4</issue>), <fpage>658</fpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Jeffreys</surname>, <given-names>H.</given-names></string-name></person-group> (<year>1998</year>). <source>The theory of probability</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Word segmentation by 8-month-olds: When speech cues count more than statistics</article-title>. <source>Journal of memory and language</source>, <volume>44</volume>(<issue>4</issue>), <fpage>548</fpage>–<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>2003a</year>). <article-title>Exploring possible effects of language-specific knowledge on infants’ segmentation of an artificial language</article-title>. <source>Jusczyk Lab Final Report</source>, <fpage>141</fpage>–<lpage>148</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>2003b</year>). <article-title>Exploring statistical learning by 8-month-olds: The role of complexity and variation</article-title>. <source>Jusczyk Lab Final Report</source>, <fpage>141</fpage>–<lpage>148</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Tyler</surname>, <given-names>M. D.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Testing the limits of statistical learning for word segmentation</article-title>. <source>Developmental Science</source>, <volume>13</volume>(<issue>2</issue>), <fpage>339</fpage>–<lpage>345</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name></person-group> (<year>1999</year>). <article-title>How infants begin to extract words from speech</article-title>. <source>Trends in cognitive sciences</source>, <volume>3</volume>(<issue>9</issue>), <fpage>323</fpage>–<lpage>328</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Infants’ detection of the sound patterns of words in fluent speech</article-title>. <source>Cognitive psychology</source>, <volume>29</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name>, <string-name><surname>Houston</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Newsome</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>The beginnings of word segmentation in English-learning infants</article-title>. <source>Cognitive psychology</source>, <volume>39</volume>(<issue>3-4</issue>), <fpage>159</fpage>–<lpage>207</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jusczyk</surname>, <given-names>P. W.</given-names></string-name>, <string-name><surname>Houston</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Newsome</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>The beginnings of word segmentation in English-learning infants</article-title>. <source>Cognitive psychology</source>, <volume>39</volume>(<issue>3-4</issue>), <fpage>159</fpage>–<lpage>207</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Seitz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Feenstra</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Testing assumptions of statistical learning: is it long-term and implicit?</article-title> <source>Neuroscience letters</source>, <volume>461</volume>(<issue>2</issue>), <fpage>145</fpage>–<lpage>149</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name>, &amp; <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Learning at a distance I. Statistical learning of non-adjacent dependencies</article-title>. <source>Cognitive psychology</source>, <volume>48</volume>(<issue>2</issue>), <fpage>127</fpage>–<lpage>162</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perruchet</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Vinter</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1998</year>). <article-title>PARSER: A model for word segmentation</article-title>. <source>Journal of Memory and Language</source>, <volume>39</volume>(<issue>2</issue>), <fpage>246</fpage>–<lpage>263</lpage>.</mixed-citation></ref>
    <ref id="c31"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Perruchet</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Robinet</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Lemaire</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2014</year>). <article-title>U-Learn: Finding optimal coding units from unsegmented sequential databases</article-title>. (Unpublished manuscript)</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, &amp; <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Statistical learning by 8-month-old infants</article-title>. <source>Science</source>, <volume>274</volume>(<issue>5294</issue>), <fpage>1926</fpage>–<lpage>1928</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Expectation in perceptual decision making: neural and computational mechanisms</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>15</volume>(<issue>11</issue>), <fpage>745</fpage>–<lpage>756</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Swingley</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Statistical clustering and the contents of the infant vocabulary</article-title>. <source>Cognitive Psychology</source>, <volume>50</volume>(<issue>1</issue>), <fpage>86</fpage>–<lpage>132</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turk-Browne</surname>, <given-names>N. B.</given-names></string-name>, <string-name><surname>Scholl</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M. K.</given-names></string-name>, &amp; <string-name><surname>Chun</surname>, <given-names>M. M.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Implicit perceptual anticipation triggered by statistical learning</article-title>. <source>Journal of Neuroscience</source>, <volume>30</volume>, <fpage>11177</fpage>–<lpage>11187</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>F. H.</given-names></string-name>, &amp; <string-name><surname>Kaiser</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Linguistic Priming and Learning Adjacent and Nonadjacent Dependencies in Serial Reaction Time Tasks</article-title>. <source>Language Learning</source>, <volume>72</volume>(<issue>3</issue>), <fpage>695</fpage>–<lpage>727</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Statistical word segmentation succeeds given the minimal amount of exposure</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
    <ref id="c38"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>F. H.</given-names></string-name>, <string-name><surname>Trueswell.</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zevin</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Mintz</surname>, <given-names>T. H.</given-names></string-name></person-group>. <article-title>Repetition induced rhythm as an alternative account to statistical word segmentation: A model and meta-analysis</article-title>. (under review) <year>no date</year></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95761.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3395-7234</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>The authors present <bold>valuable</bold> empirical and modelling evidence that statistical learning in speech perception may contain sub-processes. While the evidence for statistical learning effects is <bold>solid</bold>, the link between the pattern of effects (both empirical and simulated) and the theoretical concepts of the sub-processes (e.g., segmentation, anticipation) could be further developed. This work is of broad interest to researchers working on, or with, statistical learning, and to any researcher interested in the challenges of how data and models adjudicate between competing theoretical constructs.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95761.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents three experiments. Experiments 1 and 3 use a target detection paradigm to investigate the speed of statistical learning. The first experiment is a replication of Batterink, 2017, in which participants are presented with streams of uniform-length, trisyllabic nonsense words and asked to detect a target syllable. The results replicate previous findings, showing that learning (in the form of response time facilitation to later-occurring syllables within a nonsense word) occurs after a single exposure to a word. In the second experiment, participants are presented with streams of variable length nonsense words (two trisyllabic words and two disyllabic words), and perform the same task. A similar facilitation effect was observed as in Experiment 1. In Experiment 3 (newly added in the Revised manuscript), an adult version of the study by Johnson and Tyler is included. Participants were exposed to streams of words of either uniform length (all disyllabic) or mixed length (two disyllabic, two trisyllabic) and then asked to perform a familiarity judgment on a 1-5 scale on two words from the stream and two part-words. Performance was better in the uniform length condition.</p>
<p>The authors interpret these findings as evidence that target detection requires mechanisms different from segmentation. They present results of a computational model to simulate results from the target detection task, and find that a bigram model can produce facilitation effects similar to the ones observed by human participants in Experiments 1 and 2 (though this model was not directly applied to test whether human-like effects were also produced to account for the data in Experiment 3). PARSER was also tested and produced differing results from those observed by humans across all three experiments. The authors conclude that the mechanisms involved in the target detection task are different from those involved in the word segmentation task.</p>
<p>Strengths:</p>
<p>The paper presents multiple experiments that provide internal replication of a key experimental finding, in which response times are facilitated after a single exposure to an embedded pseudoword. Both experimental data and results from a computational model are presented, providing converging approaches for understanding and interpreting the main results. The data are analyzed very thoroughly using mixed effects models with multiple explanatory factors. The addition of Experiment 3 provides direct evidence that the profile of performance for familiarity ratings and target detection differ as a function of word length variability.</p>
<p>Weaknesses:</p>
<p>(1) The concept of segmentation is still not quite clear. The authors seem to treat the testing procedure of Experiment 3 as synonymous with segmentation. But the ability to more strongly endorse words from the stream versus part-words as familiar does not necessarily mean that they have been successfully &quot;segmented&quot;, as I elaborated on in my earlier review. In my view, it would be clearer to refer to segmentation as the mechanism or conceptual construct of segmenting continuous speech into discrete words. This ability to accurately segment component words could support familiarity judgments but is not necessary for above-chance familiarity or recognition judgments, which could be supported by more general memory signals. In other words, segmentation as an underlying ability is sufficient but not necessary for above-chance performance on familiarity-driven measures such as the one used in experiment 3.</p>
<p>(2) The addition of experiment 3 is an added strength of the revised paper and provides more direct evidence of dissociations as a function of word length on the two tasks (target detection and familiarity ratings), compared to the prior strategy of just relying on previous work for this claim. However, it is not clear why the authors chose not to use the same stimuli as used in experiment 1 and 2, which would have allowed for more direct comparisons to be made. It should also be specified whether test items in the UWL and MWL were matched for overall frequency during exposure. Currently, the text does not specify whether test words in the UWL condition were taken from the high frequency or low frequency group; if they were taken from the high frequency group this would of course be a confound when comparing to the MWL condition. Finally, the definition of part-words should also be clarified,</p>
<p>(3) The framing and argument for a prediction/anticipation mechanism was dropped in the Revised manuscript, but there are still a few instances where this framing and interpretation remain.
E.g. Abstract - &quot;we found that a prediction mechanism, rather than clustering, could explain the data from target detection.&quot;
Discussion page 43 &quot;Together, these results suggest that a simple prediction-based mechanism can explain the results from the target detection task, and clustering-based approaches such as PARSER cannot, contrary to previous claims.&quot;</p>
<p>Minor
(4) It was a bit unclear as to why a conceptual replication of Batterink 2017 was conducted, given that the target syllables at the beginning and end of the streams were immediately dropped from further analysis. Why include syllable targets within these positions in the design if they are not analyzed?</p>
<p>(5) Figures 3 and 4 are plotted on different scales, which makes it difficult to visually compare the effects between word length conditions.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95761.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The valuable study investigates how statistical learning may facilitate a target detection task and whether the facilitation effect is related to statistical learning of word boundaries. Solid evidence is provided that target detection and word segmentation rely on different statistical learning mechanisms.</p>
<p>Strengths:</p>
<p>The study is well designed, using the contrast between the learning of words of uniform length and words of variable length to dissociate general statistical learning effects and effects related to word segmentation.</p>
<p>Weaknesses:</p>
<p>The study relies on the contrast between word length effects on target detection and word learning. However, the study only tested the target detection condition and did not attempt to replicate the word segmentation effect. It is true that the word segmentation effect has been replicated before but it is still worth reviewing the effect size of previous studies.</p>
<p>The paper seems to distinguish prediction, anticipation, and statistical learning, but it is not entirely clear what each terms refers to.</p>
<p>Comments on revisions:</p>
<p>The authors did not address my concerns...they only replied to reviewer 1.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95761.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Luo</surname>
<given-names>Meili</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cao</surname>
<given-names>Ran</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Felix Hao</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>This paper presents two experiments, both of which use a target detection paradigm to investigate the speed of statistical learning. The first experiment is a replication of Batterink, 2017, in which participants are presented with streams of uniform-length, trisyllabic nonsense words and asked to detect a target syllable. The results replicate previous findings, showing that learning (in the form of response time facilitation to later-occurring syllables within a nonsense word) occurs after a single exposure to a word. In the second experiment, participants are presented with streams of variable-length nonsense words (two trisyllabic words and two disyllabic words) and perform the same task. A similar facilitation effect was observed as in Experiment 1. The authors interpret these findings as evidence that target detection requires mechanisms different from segmentation. They present results of a computational model to simulate results from the target detection task and find that an &quot;anticipation mechanism&quot; can produce facilitation effects, without performing segmentation. The authors conclude that the mechanisms involved in the target detection task are different from those involved in the word segmentation task.</p>
<p>Strengths:</p>
<p>The paper presents multiple experiments that provide internal replication of a key experimental finding, in which response times are facilitated after a single exposure to an embedded pseudoword. Both experimental data and results from a computational model are presented, providing converging approaches for understanding and interpreting the main results. The data are analyzed very thoroughly using mixed effects models with multiple explanatory factors.</p>
<p>Weaknesses:</p>
<p>In my view, the main weaknesses of this study relate to the theoretical interpretation of the results.</p>
<p>(1) The key conclusion from these findings is that the facilitation effect observed in the target detection paradigm is driven by a different mechanism (or mechanisms) than those involved in word segmentation. The argument here I think is somewhat unclear and weak, for several reasons:</p>
<p>First, there appears to be some blurring in what exactly is meant by the term &quot;segmentation&quot; with some confusion between segmentation as a concept and segmentation as a paradigm.</p>
<p>Conceptually, segmentation refers to the segmenting of continuous speech into words. However, this conceptual understanding of segmentation (as a theoretical mechanism) is not necessarily what is directly measured by &quot;traditional&quot; studies of statistical learning, which typically (at least in adults) involve exposure to a continuous speech stream followed by a forced-choice recognition task of words versus recombined foil items (part-words or nonwords). To take the example provided by the authors, a participant presented with the sequence GHIABCDEFABCGHI may endorse ABC as being more familiar than BCG, because ABC is presented more frequently together and the learned association between A and B is stronger than between C and G. However, endorsement of ABC over BCG does not necessarily mean that the participant has &quot;segmented&quot; ABC from the speech stream, just as faster reaction times in responding to syllable C versus A do not necessarily indicate successful segmentation. As the authors argue on page 7, &quot;an encounter to a sequence in which two elements co-occur (say, AB) would theoretically allow the learner to use the predictive relationship during a subsequent encounter (that A predicts B).&quot; By the same logic, encoding the relationship between A and B could also allow for the above-chance endorsement of items that contain AB over items containing a weaker relationship.</p>
<p>Both recognition performance and facilitation through target detection reflect different outcomes of statistical learning. While they may reflect different aspects of the learning process and/or dissociable forms of memory, they may best be viewed as measures of statistical learning, rather than mechanisms in and of themselves.</p>
</disp-quote>
<p>Thanks for this nuanced discussion, and this is an important point that R2 also raised. We agree that segmentation can refer to both an experimental paradigm and a mechanism that accounts for learning in the experimental paradigm. In the experimental paradigm, participants are asked to identify which words they believe to be (whole) words from the continuous syllable stream. In the target-detection experimental paradigm, participants are not asked to identify words from continuous streams, and instead, they respond to the occurrences of a certain syllable. It’s possible that learners employ one mechanism in these two tasks, or that they employ separate mechanisms. It’s also the case that, if all we have is positive evidence for both experimental paradigms, i.e., learners can succeed in segmentation tasks as well as in target detection tasks with different types of sequences, we would have no way of talking about different mechanisms, as you correctly suggested that evidence for segmenting AB and processing B faster following A, is not evidence for different mechanisms.</p>
<p>However, that is not the case. When the syllable sequences contain same-length subsequences (i.e., words), learning is indeed successful in both segmentation and target detection tasks. However, in studies such as Hoch et al. (2013), findings suggest that words from mixed-length sequences are harder to segment than words from uniform-length sequences. This finding exists in adult work (e.g., Hoch et al. 2013) as well as infant work (Johnson &amp; Tyler, 2010), and replicated here in the newly included Experiment 3, which stands in contrast to the positive findings of the facilitation effect with mixed-length sequences in the target detection paradigm (one of our main findings in the paper). Thus, it seems to be difficult to explain, if the learning mechanisms were to be the same, why humans can succeed in mixed-length sequences in target detection (as shown in Experiment 2) but fail in uniform-length sequences (as shown in Hoch et al. and Experiment 3).</p>
<p>In our paper, we have clarified these points describe the separate mechanisms in more detail, in both the Introduction and General Discussion sections.</p>
<disp-quote content-type="editor-comment">
<p>(2) The key manipulation between experiments 1 and 2 is the length of the words in the syllable sequences, with words either constant in length (experiment 1) or mixed in length (experiment 2). The authors show that similar facilitation levels are observed across this manipulation in the current experiments. By contrast, they argue that previous findings have found that performance is impaired for mixed-length conditions compared to fixed-length conditions. Thus, a central aspect of the theoretical interpretation of the results rests on prior evidence suggesting that statistical learning is impaired in mixed-length conditions. However, it is not clear how strong this prior evidence is. There is only one published paper cited by the authors - the paper by Hoch and colleagues - that supports this conclusion in adults (other mentioned studies are all in infants, which use very different measures of learning). Other papers not cited by the authors do suggest that statistical learning can occur to stimuli of mixed lengths (Thiessen et al., 2005, using infant-directed speech; Frank et al., 2010 in adults). I think this theoretical argument would be much stronger if the dissociation between recognition and facilitation through RTs as a function of word length variability was demonstrated within the same experiment and ideally within the same group of participants.</p>
</disp-quote>
<p>To summarize the evidence of learning uniform-length and mixed-length sequences (which we discussed in the Introduction section), “even though infants and adults alike have shown success segmenting syllable sequences consisting of words that were uniform in length (i.e., all words were either disyllabic; Graf Estes et al., 2007; or trisyllabic, Aslin et al., 1998), both infants and adults have shown difficulty with syllable sequences consisting of words of mixed length (Johnson &amp; Tyler, 2010; Johnson &amp; Jusczyk, 2003a; 2003b; Hoch et al., 2013).” The newly added Experiment 3 also provided evidence for the difference in uniform-length and mixed-length sequences. Notably, we do not agree with the idea that infant work should be disregarded as evidence just because infants were tested with habituation methods; not only were the original findings (Saffran et al. 1996) based on infant work, so were many other studies on statistical learning.</p>
<p>There are other segmentation studies in the literature that have used mixed-length sequences, which are worth discussing. In short, these studies differ from the Saffran et al. (1996) studies in many important ways, and in our view, these differences explain why the learning was successful. Of interest, Thiessen et al. (2005) that you mentioned was based on infant work with infant methods, and demonstrated the very point we argued for: In their study, infants failed to learn when mixed-length sequences were pronounced as adult-directed speech, and succeeded in learning given infant-directed speech, which contained prosodic cues that were much more pronounced. The fact that infants failed to segment mixed-length sequences without certain prosodic cues is consistent with our claim that mixed-length sequences are difficult to segment in a segmentation paradigm. Another such study is Frank et al. (2010), where continuous sequences were presented in “sentences”. Different numbers of words were concatenated into sentences where a 500ms break was present between each sentence in the training sequence. One sentence contained only one word, or two words, and in the longest sentence, there were 24 words. The results showed that participants are sensitive to the effect of sentence boundaries, which coincide with word boundaries. In the extreme, the one-word-per-sentence condition simply presents learners with segmented word forms. In the 24-word-per-sentence condition, there are nevertheless sentence boundaries that are word boundaries, and knowing these word boundaries alone should allow learners to perform above chance in the test phase. Thus, in our view, this demonstrates that learners can use sentence boundaries to infer word boundaries, which is an interesting finding in its own right, but this does not show that a continuous syllable sequence with mixed word lengths is learnable without additional information. In summary, to our knowledge, syllable sequences containing mixed word lengths are better learned when additional cues to word boundaries are present, and there is strong evidence that syllable sequences containing uniform-word lengths are learned better than mixed-length ones.</p>
<p>Frank, M. C., Goldwater, S., Griffiths, T. L., &amp; Tenenbaum, J. B. (2010). Modeling human performance in statistical word segmentation. Cognition, 117(2), 107-125.</p>
<p>To address your proposal of running more experiments to provide stronger evidence for our theory, we were planning to run another study to have the same group of participants do both the segmentation and target detection paradigm as suggested, but we were unable to do so as we encountered difficulties to run English-speaking participants. Instead, we have included an experiment (now Experiment 3), showing the difference between the learning of uniform-length and mixed-length sequences with the segmentation paradigm that we have never published previously. This experiment provides further evidence for adults’ difficulties in segmenting mixed-length sequences.</p>
<disp-quote content-type="editor-comment">
<p>(3) The authors argue for an &quot;anticipation&quot; mechanism in explaining the facilitation effect observed in the experiments. The term anticipation would generally be understood to imply some kind of active prediction process, related to generating the representation of an upcoming stimulus prior to its occurrence. However, the computational model proposed by the authors (page 24) does not encode anything related to anticipation per se. While it demonstrates facilitation based on prior occurrences of a stimulus, that facilitation does not necessarily depend on active anticipation of the stimulus. It is not clear that it is necessary to invoke the concept of anticipation to explain the results, or indeed that there is any evidence in the current study for anticipation, as opposed to just general facilitation due to associative learning.</p>
</disp-quote>
<p>Thanks for raising this point. Indeed, the anticipation effect we reported is indistinguishable from the facilitation effect that we reported in the reported experiments. We have dropped this framing.</p>
<disp-quote content-type="editor-comment">
<p>In addition, related to the model, given that only bigrams are stored in the model, could the authors clarify how the model is able to account for the additional facilitation at the 3rd position of a trigram compared to the 2nd position?</p>
</disp-quote>
<p>Thanks for the question. We believe it is an empirical question whether there is an additional facilitation at the 3rd position of a trigram compared to the 2nd position. To investigate this issue, we conducted the following analysis with data from Experiment 1. First, we combined the data from two conditions (exact/conceptual) from Experiment 1 so as to have better statistical power. Next, we ran a mixed effect regression with data from syllable positions 2 and 3 only (i.e., data from syllable position 1 were not included). The fixed effect included the two-way interaction between syllable position and presentation, as well as stream position, and the random effect was a by-subject random intercept and stream position as the random slope. This interaction was significant (χ<sup>2</sup>(3) =11.73, p=0.008), suggesting that there is additional facilitation to the 3rd position compared to the 2nd position.</p>
<p>For the model, here is an explanation of why the model assumes an additional facilitation to the 3rd position. In our model, we proposed a simple recursive relation between the RT of a syllable occurring for the nth time and the n+1<sup>th</sup> time, which is:</p>
<disp-formula id="sa3equ1">
<graphic mime-subtype="jpg" xlink:href="elife-95761-sa3-equ1.jpg" mimetype="image"/>
</disp-formula>
<p>and</p>
<p>RT(1) = RT0 + stream_pos * stream_inc, where the n in RT(n) represents the RT for the n<sup>th</sup> presentation of the target syllable, stream_pos is the position (3-46) in the stream, and occurrence is the number of occurrences that the syllable has occurred so far in the stream.</p>
<p>What this means is that the model basically provides an RT value for every syllable in the stream. Thus, for a target at syllable position 1, there is a RT value as an unpredictable target, and for targets at syllable position 2, there is a facilitation effect. For targets at syllable position 3, it is facilitated the same amount. As such, there is an additional facilitation effect for syllable position 3 because effects of predication are recursive.</p>
<disp-quote content-type="editor-comment">
<p>(4) In the discussion of transitional probabilities (page 31), the authors suggest that &quot;a single exposure does provide information about the transitions within the single exposure, and the probability of B given A can indeed be calculated from a single occurrence of AB.&quot; Although this may be technically true in that a calculation for a single exposure is possible from this formula, it is not consistent with the conceptual framework for calculating transitional probabilities, as first introduced by Saffran and colleagues. For example, Saffran et al. (1996, Science) describe that &quot;over a corpus of speech there are measurable statistical regularities that distinguish recurring sound sequences that comprise words from the more accidental sound sequences that occur across word boundaries. Within a language, the transitional probability from one sound to the next will generally be highest when the two sounds follow one another within a word, whereas transitional probabilities spanning a word boundary will be relatively low.&quot; This makes it clear that the computation of transitional probabilities (i.e., Y | X) is conceptualized to reflect the frequency of XY / frequency of X, over a given language inventory, not just a single pair. Phrased another way, a single exposure to pair AB would not provide a reliable estimate of the raw frequencies with which A and AB occur across a given sample of language.</p>
</disp-quote>
<p>Thanks for the discussion. We understand your argument, but we respectively disagree that computing transitional probabilities must be conducted under a certain theoretical framework. In our humble opinion, computing transitional probabilities is a mathematical operation, and as such, it is possible to do so with the least amount of data possible that enables the mathematical operation, which concretely is a single exposure during learning. While it is true that a single exposure may not provide a reliable estimate of frequencies or probabilities, it does provide information with which the learner can make decisions.</p>
<p>This is particularly true for topics under discussion regarding the minimal amount of exposure that can enable learning. It is important to distinguish the following two questions: whether learners can learn from a short exposure period (from a single exposure, in fact) and how long of an exposure period does the learner require for it to be considered to produce a reliable estimate of frequencies. Incidentally, given the fact that learners can learn from a single exposure based on Batterink (2017) and the current study, it does not appear that learners require a long exposure period to learn about transitional probabilities.</p>
<disp-quote content-type="editor-comment">
<p>(5) In experiment 2, the authors argue that there is robust facilitation for trisyllabic and disyllabic words alike. I am not sure about the strength of the evidence for this claim, as it appears that there are some conflicting results relevant to this conclusion. Notably, in the regression model for disyllabic words, the omnibus interaction between word presentation and syllable position did not reach significance (p= 0.089). At face value, this result indicates that there was no significant facilitation for disyllabic words. The additional pairwise comparisons are thus not justified given the lack of omnibus interaction. The finding that there is no significant interaction between word presentation, word position, and word length is taken to support the idea that there is no difference between the two types of words, but could also be due to a lack of power, especially given the p-value (p = 0.010).</p>
</disp-quote>
<p>Thanks for the comment. Firstly, we believe there is a typo in your comment, where in the last sentence, we believe you were referring to the p-value of 0.103 (source: “The interaction was not significant (χ2(3) = 6.19, p= 0.103”). Yes, a null result with a frequentist approach cannot support a null claim, but Bayesian analyses could potentially provide evidence for the null.</p>
<p>To this end, we conducted a Bayes factor analysis using the approach outlined in Harms and Lakens (2018), which generates a Bayes factor by computing a Bayesian information criterion for a null model and an alternative model. The alternative model contained a three-way interaction of word length, word presentation, and word position, whereas the null model contained a two-way interaction between word presentation and word position as well as a main effect of word length. Thus, the two models only differ in terms of whether there is a three-way interaction. The Bayes factor is then computed as exp[(BICalt − BICnull)/2]. This analysis showed that there is strong evidence for the null, where the Bayes Factor was found to be exp(25.65) which is more than 1011. Thus, there is no power issue here, and there is strong evidence for the null claim that word length did not interact with other factors in Experiment 2.</p>
<p>There is another issue that you mentioned, of whether we should conduct pairwise comparisons if the omnibus interaction did not reach significance. This would be true given the original analysis plan, but we believe that a revised analysis plan makes more sense. In the revised analysis plan for Experiment 2, we start with the three-way interaction (as just described in the last paragraph). The three-way interaction was not significant, and after dropping the third interaction terms, the two-way interaction and the main effect of word length are both significant, and we use this as the overall model. Testing the significance of the omnibus interaction between presentation and syllable position, we found that this was significant (χ<sup>2</sup>(3) =49.77, p&lt;0.001). This represents that, in one model, that the interaction between presentation and syllable position using data from both disyllabic and trisyllabic words. This was in addition to a significant fixed effect of word length (β=0.018, z=6.19, p&lt;0.001). This should motivate the rest of the planned analysis, which regards pairwise comparisons in different word length conditions.</p>
<disp-quote content-type="editor-comment">
<p>(6) The results plotted in Figure 2 seem to suggest that RTs to the first syllable of a trisyllabic item slow down with additional word presentations, while RTs to the final position speed up. If anything, in this figure, the magnitude of the effect seems to be greater for 1st syllable positions (e.g., the RT difference between presentation 1 and 4 for syllable position 1 seems to be numerically larger than for syllable position 3, Figure 2D). Thus, it was quite surprising to see in the results (p. 16) that RTs for syllable position 1 were not significantly different for presentation 1 vs. the later presentations (but that they were significant for positions 2 and 3 given the same comparison). Is this possibly a power issue? Would there be a significant slowdown to 1st syllables if results from both the exact replication and conceptual replication conditions were combined in the same analysis?</p>
</disp-quote>
<p>Thanks for the suggestion and your careful visual inspection of the data. After combining the data, the slowdown to 1st syllables is indeed significant. We have reported this in the results of Experiment 1 (with an acknowledgement to this review):</p>
<p>Results showed that later presentations took significantly longer to respond to compared to the first presentation (χ<sup>2</sup>(3) = 10.70, p=0.014), where the effect grew larger with each presentation (second presentation: β=0.011, z=1.82, p=0.069; third presentation: β=0.019, z=2.40, p=0.016; fourth presentation: β=0.034, z=3.23, p=0.001).</p>
<disp-quote content-type="editor-comment">
<p>(7) It is difficult to evaluate the description of the PARSER simulation on page 36. Perhaps this simulation should be introduced earlier in the methods and results rather than in the discussion only.</p>
</disp-quote>
<p>Thanks for the suggestions. We have added two separate simulations in the paper, which should describe the PARSER simulations sufficiently, as well as provide further information on the correspondence between the simulations and the experiments. Thanks again for the great review! We believe our paper has improved significantly as a result.</p>
</body>
</sub-article>
</article>