<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">104101</article-id>
<article-id pub-id-type="doi">10.7554/eLife.104101</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104101.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Learning of state representation in recurrent network: the power of random feedback and biological constraints</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Tsurumi</surname>
<given-names>Takayuki</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6306-6600</contrib-id>
<name>
<surname>Kato</surname>
<given-names>Ayaka</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8044-9195</contrib-id>
<name>
<surname>Kumar</surname>
<given-names>Arvind</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2192-4248</contrib-id>
<name>
<surname>Morita</surname>
<given-names>Kenji</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a5">5</xref>
<email>morita@p.u-tokyo.ac.jp</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057zh3y96</institution-id><institution>Physical and Health Education, Graduate School of Education, The University of Tokyo</institution></institution-wrap>, <city>Tokyo</city>, <country>Japan</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qg15b79</institution-id><institution>Theoretical Sciences Visiting Program, Okinawa Institute of Science and Technology</institution></institution-wrap>, <city>Tancha</city>, <country>Japan</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Department of Psychiatry, Icahn School of Medicine at Mount Sinai</institution></institution-wrap>, <city>New York</city>, <country>USA</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026vcq606</institution-id><institution>Division of Computational Science and Technology, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology</institution></institution-wrap>, <city>Stockholm</city>, <country>Sweden</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/057zh3y96</institution-id><institution>International Research Center for Neurointelligence (WPI-IRCN), The University of Tokyo</institution></institution-wrap>, <city>Tokyo</city>, <country>Japan</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2025-01-14">
<day>14</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP104101</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-11-05">
<day>05</day>
<month>11</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-10-04">
<day>04</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.08.22.609100"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Tsurumi et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Tsurumi et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-104101-v1.pdf"/>
<abstract>
<title>Abstract</title><p>How external/internal ‘state’ is represented in the brain is crucial, since appropriate representation enables goal-directed behavior. Recent studies suggest that state representation and state value can be simultaneously learnt through reinforcement learning (RL) using reward-prediction-error in recurrent-neural-network (RNN) and its downstream weights. However, how such learning can be neurally implemented remains unclear because training of RNN through the ‘backpropagation’ method requires downstream weights, which are biologically unavailable at the upstream RNN. Here we show that training of RNN using random feedback instead of the downstream weights still works because of the ‘feedback alignment’, which was originally demonstrated for supervised learning. We further show that if the downstream weights and the random feedback are biologically constrained to be non-negative, learning still occurs without feedback alignment because the non-negative constraint ensures loose alignment. These results suggest neural mechanisms for RL of state representation/value and the power of random feedback and biological constraints.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have revised and enriched the Discussion.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Multiple lines of studies have suggested that Temporal-Difference-Reinforcement-Leaning (TDRL) is implemented in the cortico-basal ganglia-dopamine(DA) circuits in the way that DA represents TD reward-prediction-error (RPE) <sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c7">7</xref></sup> and DA-dependent plasticity of cortico-striatal synapses represents TD-RPE-dependent update of state/action values <sup><xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c10">10</xref></sup>. Traditionally, TDRL in the cortico-basal ganglia-DA circuits was considered to serve only for relatively simple behavior. However, subsequent studies suggested that more sophisticated, apparently goal-directed/model-based behavior can also be achieved by TDRL if states are appropriately represented <sup><xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup> and that DA signals indeed reflect model-based predictions <sup><xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref></sup>. Conversely, state representation-related issues could potentially cause behavioral or mental-health problems <sup><xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c20">20</xref></sup>. Early modeling studies treated state representations appropriate to the situation/task as given (’handcrafted’ by the authors), but representation itself should be learnt in the brain <sup><xref ref-type="bibr" rid="c21">21</xref>–<xref ref-type="bibr" rid="c26">26</xref></sup>. Recently it was shown that appropriate state representation can be learnt through RL in a recurrent neural network (RNN) by minimization of squared value-error without explicit teacher/target <sup><xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup>, while state value can be simultaneously learnt in the downstream of RNN.</p>
<p>However, whether such a learning method, named the value-RNN <sup><xref ref-type="bibr" rid="c2">2</xref></sup>, can be implemented in the brain remains unclear, because there are problems in terms of biological plausibility. A major problem, among others, is that the update rule proposed in the previous work for the connections onto the ’neurons’ in the RNN <sup><xref ref-type="bibr" rid="c2">2</xref></sup>, derived from the gradient-descent error-’backpropagation’ (hereafter referred to as backprop) method <sup><xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref></sup>, involves the weights of the connections from these RNN units onto the downstream value-encoding unit. Given that the state-representing RNN and the value-encoding unit are implemented by the intra-cortical circuit and the striatal neurons, respectively, as generally suggested <sup><xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref></sup>, this means that the update (plasticity) rule for intra-cortical connections involves the downstream cortico-striatal synaptic strengths, which would not be able to be accessed from the cortex. Indeed, this is an example of the long-standing difficulty in biological implementation of backprop <sup><xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref></sup>, in which update of upstream connections requires biologically unavailable downstream connection strengths.</p>
<p>Recently, a potential solution for this difficulty has been proposed <sup><xref ref-type="bibr" rid="c33">33</xref></sup> (see also <sup><xref ref-type="bibr" rid="c34">34</xref>–<xref ref-type="bibr" rid="c41">41</xref></sup> for other potential solutions). Specifically, in supervised learning of feed-forward network, it was shown that when the downstream connection strengths used for updating upstream connections in backprop were replaced with fixed random strengths, comparable learning performance was still achieved <sup><xref ref-type="bibr" rid="c33">33</xref></sup>. This was suggested to be because the information of the introduced fixed random strengths transferred, through learning, to the upstream connections and then to the downstream feed-forward connections so that these feed-forward connections became aligned to the random feedback strengths, and thus in turn, the random feedback can play the same role as the one played by the downstream connection strengths in backprop. This mechanism was named the ’feedback alignment’ <sup><xref ref-type="bibr" rid="c33">33</xref></sup>, and was subsequently shown to work also in supervised learning of RNN <sup><xref ref-type="bibr" rid="c42">42</xref></sup> and proposed to be neurally implemented <sup><xref ref-type="bibr" rid="c43">43</xref></sup> (in a different way from the present study as we discuss in the Discussion).</p>
<p>The value-RNN <sup><xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup>, the above-introduced simultaneous RL of state values and state representation through minimization of squared value-error, differs from supervised learning considered in these previous feedback-alignment studies in two ways: i) it is TD learning, i.e., it approximates the true error by the TD-RPE because the true error, or true state value, is unknown, and ii) it uses a scalar error (TD-RPE) rather than a vector error. Therefore it was nontrivial whether the feedback alignment mechanism could work also for the value-RNN. In the present work, we first examined this, demonstrating that it does work and providing a mechanistic insight into how it works.</p>
<p>After that, we further addressed other biological-plausibility problems. Specifically, we imposed biological constraints that the downstream (cortico-striatal) weights and the fixed random feedback, as well as the activities of neurons in the RNN, were all non-negative. Moreover, we also remedied the non-monotonic dependence of the update of RNN connection-strength on post-synaptic neural activity. We then found, unexpectedly, that the non-negative constraint appeared to aid, rather than degrade, the learning by ensuring that the downstream weights and the fixed random feedback are loosely aligned even without operation of the feedback alignment mechanism. These results suggest how learning of state representation and value can be neurally implemented, more specifically, through synaptic plasticity depending on DA, which represents TD-RPE, in the cortex and the striatum.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Consideration of the value-RNN with fixed random feedback</title>
<p>We considered an implementation of the value-RNN in the cortico-basal ganglia circuits (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). A cortical region/population is supposed to represent information of sensory observation (<bold><italic>o</italic></bold>) and send it to another cortical region/population, which has rich recurrent connections and therefore can be approximated by an RNN. Activities of neurons in the RNN (<bold><italic>x</italic></bold>) are supposed to learn to represent states, through updates of the strengths of recurrent connections <bold>A</bold> and feed-forward connections <bold>B</bold>. The activity of a population of striatal neurons that receive inputs from the RNN is supposed to learn to represent the state values (<italic>v</italic>), by learning the weights of cortico-striatal connections (from the RNN to the striatal neurons) (<bold><italic>w</italic></bold>) indicating the value weights. DA neurons in the ventral tegmental area (VTA) receive (direct and indirect) inputs from the striatum and other structure conveying information of obtained reward (<italic>r</italic>), and thereby the activity of the DA neurons, as well as released DA, represents TD-RPE (<italic>δ</italic>). TD-RPE-representing DA is released in the striatum and also in the cortical RNN through mesocorticolimbic projections, and used for modifying the strengths of cortical recurrent and feed-forward connections (<bold>A</bold> and <bold>B</bold>) and cortico-striatal connections (<bold><italic>w</italic></bold>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1</label>
<caption><p>Implementation of the value-RNN in the cortico-basal ganglia-DA circuits.</p></caption>
<graphic xlink:href="609100v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In the original value-RNN <sup><xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup>, update rule for the connections onto the RNN (<bold>A</bold> and <bold>B</bold>) requires the (gradually changing) value weights (<bold><italic>w</italic></bold>), but this is biologically implausible because the corticostriatal synaptic strengths are not available in the cortex as discussed above. Therefore, we considered a modified value-RNN by replacing the cortico-striatal weights used in the updates of intra-cortical connections with fixed random strengths (<bold><italic>c</italic></bold>). Besides, the original value-RNN adopted a learning rule called the backpropagation through time (BPTT) <sup><xref ref-type="bibr" rid="c44">44</xref></sup>, in which the error in the output needs to be incrementally accumulated in the temporally backward order, but such an acausality is also biologically implausible, as previously pointed out <sup><xref ref-type="bibr" rid="c42">42</xref></sup>. Therefore, we instead used an online learning rule, which considers only the influence of the recurrent weights at the previous time step (see the Methods for details and equations).</p>
</sec>
<sec id="s2b">
<title>Simulation of a Pavlovian cue-reward association task with variable inter-trial intervals</title>
<p>We compared the learning of the modified value-RNN with fixed random feedback (referred to as VRNNrf) and the value-RNN with backprop (referred to as VRNNbp), both of which adopted the online learning rule rather than the BPTT, and also untrained RNN. The number of RNN units was set to 7 for all the cases. Traditional TD-RL agents with punctate state representation (called the complete serial compound, CSC <sup><xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c45">45</xref></sup>) were also compared. We simulated a Pavlovian cue-reward association task, in which a cue was followed by a reward three time-steps later, and inter-trial interval (i.e., reward to next cue) was randomly chosen from 4, 5, 6, or 7 time-steps (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). In this task, states can be defined by relative timings from the cue, and we estimated the true state values through simulations according to the definition of state value, i.e., expected cumulative discounted future rewards <sup><xref ref-type="bibr" rid="c46">46</xref></sup> (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, black line). Expected TD-RPE calculated from these estimated true values (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, red line) was almost 0 at any states, as expected. Agent having punctate/CSC state representation and state values without continuation between trials (i.e., the value of the last state in a trial was not updated by TD-RPE upon entering the next trial) developed positive values between cue and reward, and abrupt TD-RPE upon cue (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). Agent having punctate/CSC state representation and continuously updated state values across trials developed positive values also for states in the inter-trial interval (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>). VRNNbp developed state values between cue and reward, and to some extent in the inter-trial interval, and showed abrupt TD-RPE upon cue and smaller TD-RPE upon reward (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>). This indicates that this agent largely learned the task structure, confirming the previously proposed effectiveness of value-RNN in this different task.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption><p>Simulation of a Pavlovian cue-reward association task. (<bold>A</bold>) Simulated task with variable inter-trial intervals. (<bold>B</bold>) Black line: Estimated true values of states, defined by relative timings from the cue, through simulations according to the definition of state value, i.e., expected cumulative discounted future rewards. Red line: TD-RPEs calculated from the estimated true state values. (<bold>C-G</bold>) State values (black lines) and TD-RPEs (red lines) at 1000-th trial, averaged across 100 simulations (error-bars indicating ± SEM across simulations), in different types of agent: (C) TD-RL agent having punctate/CSC state representation and state values without continuation between trials (i.e., the value of the last state in a trial was not updated by TD-RPE upon entering the next trial); (D) TD-RL agent having punctate/CSC state representation and continuously updated state values across trials; (E) Value-RNN with backprop (VRNNbp). The number of RNN units was 7 (same applied to (F,G); (F) Value-RNN with fixed random feedback (VRNNrf); (G) Agent with untrained RNN. (<bold>H</bold>) State values at 1000-th trial in individual simulations of VRNNbp (top), VRNNrf (middle), and untrained RNN (bottom). (<bold>I</bold>) Histograms of the value of the pre-reward state (i.e., the state one-time step before the reward state) at 1000-th trial in individual simulations of the three models. The vertical black dashed lines indicate the true value of the pre-reward state (estimated through simulations). (<bold>J</bold>) Learning performance of VRNNbp (red line), VRNNrf (blue line), and the untrained RNN (gray line) when the number of RNN units was varied from 5 to 40 (horizontal axis). Leaning performance was measured by the sum of squares of differences between the state values developed at 1000-th trial by each of these three types of agent and the estimated true state values between cue and reward (vertical axis), averaged over 100 simulations (error-bars indicating ± SEM across simulations).</p></caption>
<graphic xlink:href="609100v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>VRNNrf, having fixed random feedback instead of backprop-based feedback, developed state values that were largely similar to, although smaller (on average across simulations) than, those developed by VRNNbp (<xref rid="fig2" ref-type="fig">Fig. 2F</xref> black line). VRNNrf generated abrupt TD-RPEs upon cue and reward, again similarly to VRNNbp although the relative size of reward-response was (on average) larger (<xref rid="fig2" ref-type="fig">Fig. 2F</xref> red line). As a comparison, agent with untrained RNN developed (on average) even smaller state values and larger relative size of TD-RPE upon reward (<xref rid="fig2" ref-type="fig">Fig. 2G</xref>). These results indicate that value-</p>
<p>RNN could be trained by fixed random feedback at least to a certain extent, although somewhat less effectively (as maybe expected) than by backprop-based feedback. <xref rid="fig2" ref-type="fig">Figure 2H</xref> shows state values developed in individual simulations of VRNNbp (top), VRNNrf (middle), and untrained RNN (bottom), and <xref rid="fig2" ref-type="fig">Figure 2I</xref> shows the histograms of the value of the pre-reward state (i.e., one time-step before the state where reward was obtained) developed in individual simulations of these three models. These figures indicate that VRNNrf did not tend to develop moderately smaller state values than VRNNbp in each simulation. Rather, state values developed in VRNNrf were largely comparable to those developed in VRNNbp once they were successfully learned but the success rate was smaller than VRNNbp while still larger than the untrained RNN.</p>
<p>So far, we examined the cases where the number of RNN units was 7. We compared the learning performance of VRNNbp, VRNNrf, and untrained RNN when the number of RNN units was varied from 5 to 40. Leaning performance was measured by the sum of squares of differences between the state values developed by each of these three types of agents and the estimated true state values (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>) between cue and reward. As shown in <xref rid="fig2" ref-type="fig">Fig. 2J</xref>, on average across simulations, VRNNbp generally achieved the highest performance, but VRNNrf also exhibited largely comparable performance and it always outperformed the untrained RNN. As the number of RNN units increased from 5 to 15, all these three agents improved their performance, while additional increase of RNN units to 20 or 25 resulted in smaller changes. Further increase of RNN units caused decrease in the mean performance in all the three agents, and when the number of RNN units was increased to 45, there were occasions where learning appeared to diverge. We will discuss these in the Discussion.</p>
</sec>
<sec id="s2c">
<title>Occurrence of feedback alignment and an intuitive understanding of its mechanism</title>
<p>We questioned if feedback alignment underlay the learnability of VRNNrf. Returning to the case with 7 RNN units, we examined whether the value weight vector <bold><italic>w</italic></bold> became aligned to the random feedback vector <bold><italic>c</italic></bold> in VRNNrf, by looking at the changes in the angle between these two vectors across trials. As shown in <xref rid="fig3" ref-type="fig">Fig. 3A</xref>, this angle, averaged across simulations, decreased over trials, indicating that the value weight <bold><italic>w</italic></bold> indeed tended to become aligned to the random feedback <bold><italic>c</italic></bold>. We then examined whether better alignment of <bold><italic>w</italic></bold> to <bold><italic>c</italic></bold> related to better development of state value by looking at the relation between the angle between <bold><italic>w</italic></bold> and <bold><italic>c</italic></bold> and the value of the pre-reward state at 1000-th trial. As shown in <xref rid="fig3" ref-type="fig">Fig. 3B</xref>, there was a negative correlation such that the smaller the angle was (i.e., more aligned), the larger the state value tended to be (<italic>r</italic> = −0.288, <italic>p</italic> = 0.00362), in line with our expectation. These results indicate that the mechanism of feedback alignment, previously shown to work for supervised learning, also worked for TD learning of value weights and recurrent/feed-forward connections.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3</label>
<caption><p>Occurrence of feedback alignment and an intuitive understanding of its mechanism. (<bold>A</bold>) Over-trial changes in the angle between the value-weight vector <italic>w</italic> and the fixed random feedback vector <italic>c</italic> in the simulations of VRNNrf (7 RNN units). The solid line and the dashed lines indicate the mean and ± SD across 100 simulations, respectively. (<bold>B</bold>) The relation between the angle between <italic>w</italic> and <italic>c</italic> (horizontal axis) and the value of the pre-reward state (vertical axis) at 1000-th trial. The dots indicate the results of individual simulations, and the line indicates the regression line. (<bold>C</bold>) Angle between the hypothetical change in <italic>x</italic>(<italic>t</italic>) = <italic>f</italic>(A<italic>x</italic>(<italic>t</italic>−1),B<italic>o</italic>(<italic>t</italic>−1)) in case A and B were replaced with their updated ones, multiplied with the sign of TD-RPE (sign(<italic>δ</italic>(<italic>t</italic>))), and the fixed random feedback vector <italic>c</italic> across time-steps. The black thick line and the gray lines indicate the mean and ± SD across 100 simulations, respectively (same applied to (D)). (<bold>D</bold>) Multiplication of TD-RPEs in successive trials at individual states (top: cue, 4th from the top: reward). Positive or negative value indicates that TD-RPEs in successive trials have the same or different signs, respectively. (<bold>E</bold>) <italic>Left</italic>: RNN trajectories mapped onto the primary and secondary principal components (horizontal and vertical axes, respectively) in three successive trials (red, blue, and green lines (heavily overlapped)) at different phases in an example simulation (10th-12th, 300th-302th, 600th-602th, and 900th-902th trials from top to bottom). The crosses and circles indicate the cue and reward states, respectively. <italic>Right</italic>: State values (black lines) and TD-RPEs (red lines) at 11th, 301th, 601th, and 901th trial.</p></caption>
<graphic xlink:href="609100v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>How did the feedback alignment mechanistically occur? We made an attempt to obtain an intuitive understanding. Assume that positive TD-RPE (<italic>δ</italic>(<italic>t</italic>) &gt; 0) is generated at a state, <italic>S</italic> (= <bold><italic>x</italic></bold>(<italic>t</italic>)), in a task trial. Because of the update rule for <bold><italic>w</italic></bold> (<bold><italic>w</italic></bold> ← <bold><italic>w</italic></bold> + <italic>aδ</italic>(<italic>t</italic>)<bold><italic>x</italic></bold>(<italic>t</italic>)), <bold><italic>w</italic></bold> is updated in the direction of <bold><italic>x</italic></bold>(<italic>t</italic>). Next, what is the effect of updates of recurrent/feed-forward connections (<bold>A</bold> and <bold>B</bold>) on <bold><italic>x</italic></bold>? For simplicity, here we consider the case where observation is null (<bold><italic>o</italic></bold> = <bold>0</bold>) and so <bold><italic>x</italic></bold>(<italic>t</italic>) = <bold><italic>f</italic></bold>(<bold>A<italic>x</italic></bold>(<italic>t</italic>−1)) holds (but similar argument can be done in the case where observation is not null). If <bold>A</bold> is replaced with its updated one, it can be calculated that <italic>i</italic>-th element of <bold>A<italic>x</italic></bold>(<italic>t</italic>−1) will hypothetically change by <italic>c<sub>i</sub></italic> × (a positive value) (technical note: the value is <italic>aδ</italic>(<italic>t</italic>){Σ<italic><sub>j</sub>x<sub>j</sub></italic>(<italic>t</italic>−1)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>}(0.5 + <italic>x<sub>i</sub></italic>(<italic>t</italic>))(0.5 − <italic>x<sub>i</sub></italic>(<italic>t</italic>)) which is positive unless <italic>x</italic>(<italic>t</italic>−1) = <bold>0</bold>), and therefore the vector <bold>A<italic>x</italic></bold>(<italic>t</italic>−1) as a whole will hypothetically change by a vector that is in a relatively close angle with <bold><italic>c</italic></bold> (in a sense that, for example, [<italic>c</italic><sub>1</sub> <italic>c</italic><sub>2</sub> <italic>c</italic><sub>3</sub>]<sup>T</sup> and [0.5<italic>c</italic><sub>1</sub> 1.2<italic>c</italic><sub>2</sub> 0.8<italic>c</italic><sub>3</sub>]<sup>T</sup> are in a relatively close angle in the same quadrant). Then, because <italic>f</italic> is a monotonically increasing sigmoidal function, <bold><italic>x</italic></bold>(<italic>t</italic>) = <bold><italic>f</italic></bold>(<bold>A<italic>x</italic></bold>(<italic>t</italic>−1)) will also hypothetically change by a vector that is in a relatively close angle with <bold><italic>c</italic></bold>. This was indeed the case in our simulations as shown in <xref rid="fig3" ref-type="fig">Fig. 3C</xref>.</p>
<p>In this way, at state <italic>S</italic> where TD-RPE is positive, <bold><italic>w</italic></bold> is updated in the direction of <bold><italic>x</italic></bold>(<italic>t</italic>), and <bold><italic>x</italic></bold>(<italic>t</italic>) will hypothetically change by a vector that is in a relatively close angle with <bold><italic>c</italic></bold> if <bold>A</bold> is replaced with its updated one. Then, if the update of <bold><italic>w</italic></bold> and the hypothetical change in <bold><italic>x</italic></bold>(<italic>t</italic>) due to the update of <bold>A</bold> could be integrated, <bold><italic>w</italic></bold> would become aligned to <bold><italic>c</italic></bold> (if TD-RPE is instead negative, <bold><italic>w</italic></bold> is updated in the opposite direction of <bold><italic>x</italic></bold>(<italic>t</italic>), and <bold><italic>x</italic></bold>(<italic>t</italic>) will hypothetically change by a vector that is in a relatively close angle with −<bold><italic>c</italic></bold>, and so the same story holds in the end).</p>
<p>There is, however, a caveat regarding how the update of <bold><italic>w</italic></bold> and the hypothetical change in <bold><italic>x</italic></bold>(<italic>t</italic>) can be integrated. Although technical, here we briefly describe it, and a possible solution. The updates of <bold><italic>w</italic></bold> and <bold>A</bold> use TD-RPE, which is calculated based on <italic>v</italic>(<italic>t</italic>) = <bold><italic>w</italic></bold><sup>T</sup> <bold><italic>x</italic></bold>(<italic>t</italic>) and <italic>v</italic>(<italic>t</italic>+1) = <bold><italic>w</italic></bold><sup>T</sup> <bold><italic>x</italic></bold>(<italic>t</italic>+1), and so <bold><italic>x</italic></bold>(<italic>t</italic>) and <bold><italic>x</italic></bold>(<italic>t</italic>+1) should already be determined beforehand. Therefore, the hypothetical change in <bold><italic>x</italic></bold>(<italic>t</italic>) due to the update of <bold>A</bold>, described in the above, does not actually occur (this was why we mentioned ‘hypothetical’) and thus cannot be integrated with the update of <bold><italic>w</italic></bold>. Nevertheless, integration could still occur across successive trials, at least to a certain extent. Specifically, although TD-RPEs at <italic>S</italic> in successive trials would generally differ from each other, they would still tend to have the same sign, as was indeed the case in our simulations (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). Also, although the trajectories of RNN activity (<bold><italic>x</italic></bold>) in successive trials would differ, we could expect a certain level of similarity because the RNN is entrained by observation-representing inputs, again as was indeed the case in our example simulation (<xref rid="fig3" ref-type="fig">Fig. 3E</xref>). Then, the hypothetical change in <bold><italic>x</italic></bold>(<italic>t</italic>) due to the update of <bold>A</bold>, considered above, could become a reality in the next trial, to a certain extent, and could thus be integrated into the update of <bold><italic>w</italic></bold>, explaining the occurrence of feedback alignment.</p>
</sec>
<sec id="s2d">
<title>Simulation of tasks with probabilistic structures of reward timing/existence</title>
<p>We also simulated two tasks (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>) that were qualitatively similar to (though simpler than) the two tasks examined in previous experiments <sup><xref ref-type="bibr" rid="c1">1</xref></sup> and modeled by the original value-RNN with backprop<sup><xref ref-type="bibr" rid="c2">2</xref></sup>. In our task 1, a cue was always followed by a reward either two or four time-steps later with equal probabilities. Task 2 was the same as task 1 except that reward was omitted with 40% probability. In task 1, if reward was not given at the early timing (i.e., two-steps later than cue), agent could predict that reward should be given at the late timing (i.e., four-steps later than cue), and thus TD-RPE upon reward at the late timing is expected to be smaller than TD-RPE upon reward at the early timing (if agent perfectly learned the task structure, TD-RPE upon reward at the late timing should be 0). By contrast, in task 2, if reward was not given at the early timing, it might indicate that reward was given at the late timing but might instead indicate that reward was omitted in that trial, and thus TD-RPE upon reward at the late timing is expected to exist and can even be larger than TD-RPE upon reward at the early timing.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4</label>
<caption><p>Simulation of two tasks having probabilistic structures, which were qualitatively similar to the two tasks examined in experiments <sup><xref ref-type="bibr" rid="c1">1</xref></sup> and modeled by value-RNN <sup><xref ref-type="bibr" rid="c2">2</xref></sup>. (<bold>A</bold>) Simulated two tasks, in which reward was given at the early or the late timing with equal probabilities in all the trials (task 1) or 60% of trials (task 2). (<bold>B</bold>) (<bold>a</bold>) <italic>Top</italic>: Trial types. Two trial types (with early reward and with late reward) in task 1 and three trial types (with early reward, with late reward, and without reward) in task 2. <italic>Bottom</italic>: Value (expected discounted cumulative future rewards) of each timing in each trial type. (<bold>b</bold>) Agent’s probabilistic belief about the current trial type, in the case where agent was in fact in the trial with early reward (top row), the trial with late reward (second row), or the trial without reward (third row in task 2). (<bold>c</bold>) <italic>Top</italic>: States defined by considering the probabilistic beliefs at each timing from cue. <italic>Bottom</italic>: State values (expected discounted cumulative future rewards, estimated through simulations), which should theoretically match an integration (multiplication) of the values of each trial type (shown in (a)-bottom) with the probabilistic beliefs (shown in (b)). (<bold>C</bold>) Expected TD-RPE calculated from the estimated true values of the states for task 1 (left) and task 2 (right). Red lines: case where reward was given at the early timing, blue lines: case where reward was given at the late timing. (<bold>D-H</bold>) TD-RPEs at the latest trial within 1000 trials in which reward was given at the early timing (red lines) or the late timing (blue lines), averaged across 100 simulations (error-bars indicating ± SEM across simulations), in the different types of agent: (D,E) TD-RL agent having punctate/CSC state representation and state values without (D) or with (E) continuation between trials; (F) VRNNbp. The number of RNN units was 12 (same applied to (G,H); (G) VRNNrf; (H) Untrained RNN.</p></caption>
<graphic xlink:href="609100v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In these tasks, states can be defined in the following way. There were two types of trials, with early or late reward, in task 1, and additionally one more type of trial, without reward, in task 2 (<xref ref-type="fig" rid="fig4">Fig. 4Ba</xref>, top). For each timing in each of these trial types, its value, i.e., expected discounted cumulative future rewards, can be estimated through simulations (<xref ref-type="fig" rid="fig4">Fig. 4Ba</xref>, bottom). Agent could not know the current trial type until receiving reward at the early timing or the late timing or receiving no reward at both timings. Until these timings, agent could have probabilistic belief about the current trial type, e.g., 50% in the trial with early reward and 50% in the trial with late reward (in task 1) or 30% in the trial with early reward, 30% in the trial with late reward, and 40% in the trial without reward (in task 2) (<xref ref-type="fig" rid="fig4">Fig. 4Bb</xref>). States can be defined by incorporating these probabilistic beliefs at each timing (<xref ref-type="fig" rid="fig4">Fig. 4Bc</xref>, top), and state values (<xref ref-type="fig" rid="fig4">Fig. 4Bc</xref>, bottom: expected discounted cumulative future rewards, estimated through simulations) should theoretically match an integration (multiplication) of the values of each trial type (<xref ref-type="fig" rid="fig4">Fig. 4Ba</xref>, bottom) with the probabilistic beliefs (<xref ref-type="fig" rid="fig4">Fig. 4Bb</xref>). Expected TD-RPE calculated from these estimated state values (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>) exhibited features that matched the conjecture mentioned above: in task 1, TD-RPE upon reception of late reward, which was actually 0, was smaller than TD-RPE upon reception of early reward, whereas in task 2, TD-RPE upon reception of late reward was larger than TD-RPE upon reception of early reward.</p>
<p>The previous experimental work <sup><xref ref-type="bibr" rid="c1">1</xref></sup> has shown that VTA DA neurons exhibited similar activity patterns to the abovementioned TD-RPE patterns, and the theoretical work <sup><xref ref-type="bibr" rid="c2">2</xref></sup> has shown that the original value-RNN with backprop could reproduce such TD-RPE patterns. We examined what TD-RPE patterns were generated in the agents with punctate/CSC representation, VRNNbp, VRNNrf, and untrained RNN (12 RNN units in all cases) in our simulated two tasks. VRNNbp developed similar TD-RPE patterns (smaller TD-RPE upon late than early timing in task 1 but opposite pattern in task 2) (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>), qualitatively reproducing the result of the previous work <sup><xref ref-type="bibr" rid="c2">2</xref></sup>. Crucially, VRNNrf also developed similar TD-RPE patterns (<xref rid="fig4" ref-type="fig">Fig. 4G</xref>), indicating that this agent with random feedback could also learn the distinct structures of the two tasks. By contrast, agents with punctate/CSC state representation without or with continuous value update across trials (<xref rid="fig4" ref-type="fig">Fig. 4D,E</xref>), as well as agent with untrained RNN (<xref rid="fig4" ref-type="fig">Fig. 4H</xref>), could not develop such patterns well.</p>
</sec>
<sec id="s2e">
<title>Value-RNN with further biological constraints</title>
<p>So far, the activities of neurons in the RNN (<bold><italic>x</italic></bold>) were initialized to pseudo standard normal random numbers, and thereafter took numbers in the range between −0.5 and 0.5 that was the range of the sigmoidal input-output function. The value weights (<bold><italic>w</italic></bold>) could also take both positive and negative values since no constraint was imposed. The fixed random feedback in VRNNrf (<bold><italic>c</italic></bold>) was generated by pseudo standard normal random numbers, and so could also be positive or negative. Negativity of the neurons’ activities and the value weights could potentially be regarded as inhibitory or smaller-than-baseline quantities. However, because neuronal firing rate is non-negative and cortico-striatal projections are excitatory, it would be biologically more plausible to assume that the activities of neurons in the RNN and the value weights are non-negative. As for the fixed random feedback, if it is negative, the update rule becomes anti-Hebbian under positive TD-RPE, and so assuming non-negativity would be plausible since Hebbian property has been suggested for rapid plasticity of cortical synapses <sup><xref ref-type="bibr" rid="c47">47</xref></sup>. There was another issue in the update rule for recurrent and feed-forward connections, derived from the gradient descent. Specifically, the dependence on the post-synaptic activity was non-monotonic, maximized at the middle of the range of activity. It would be more biologically plausible to assume monotonic dependence.</p>
<p>In order to address these issues, we considered revised models. We first considered a revised VRNNbp, referred to as revVRNNbp, in which the RNN activities and the value weights were constrained to be non-negative, while the non-monotonic dependence of the update rule on the post-synaptic activity remained unchanged (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). We then considered a revised VRNNrf, referred to as bioVRNNrf, in which the fixed random feedback, as well as the RNN activities and the value weights, were constrained to be non-negative, and also the update rule was modified so that the dependence on the post-synaptic activity became monotonic (with saturation) (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5</label>
<caption><p>Modified value-RNN models with further biological constraints. (<bold>A</bold>) RevVRNNbp: VRNNbp (value-RNN with backprop) was modified so that the activities of neurons in the RNN (<italic>x</italic>) and the value weights (<italic>w</italic>) became non-negative. (<bold>B</bold>) BioVRNNrf: VRNNrf (value-RNN with fixed random feedback) was modified so that <italic>x</italic> and <italic>w</italic>, as well as the fixed random feedback (<italic>c</italic>), became non-negative and also the dependence of the update rules for recurrent/feed-forward connections (<bold>A</bold> and <bold>B</bold>) on post-synaptic activity became monotonic + saturation.</p></caption>
<graphic xlink:href="609100v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We examined how these revised models, in comparison with untrained RNN that also had non-negative constraint for <bold><italic>x</italic></bold> and <bold><italic>w</italic></bold>, performed in the Pavlovian cue-reward association task examined above (the number of neurons/trials were set to 12/1500). RevVRNNbp well developed state values toward reward (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>). BioVRNNrf also developed state values to a largely comparable extent (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>). By contrast, untrained RNN could not develop such a pattern of state values (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>). This, however, could be because initially set recurrent/feed-forward connections were far from those learned in the value-RNNs. Therefore, as a more strict control, we conducted simulations of untrained RNN with non-negative <bold><italic>x</italic></bold> and <bold><italic>w</italic></bold>, where in each simulation the recurrent/feed-forward connections were set to be those shuffled from the learnt connections in a simulation of bioVRNNrf. Untrained RNN with this setting performed somewhat better than the original untrained RNN case (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>), but still worse than revVRNNbp and bioVRNNrf. We varied the number of neurons in the RNN, and compared the performance (sum of squared errors from the true state values) of revVRNNbp and bioVRNNrf, in comparison with untrained RNN (both naive one and the one with shuffled learnt connections from bioVRNNrf). As shown in <xref rid="fig6" ref-type="fig">Fig. 6E</xref>, regardless of the number of neurons, the performance of bioVRNNrf was largely comparable to that of revVRNNbp, and better than the performance of untrained RNN of both kinds. <xref rid="fig6" ref-type="fig">Figure 6F</xref> shows the mean of the elements of the recurrent and feed-forward connections at 1500-th trial in the different models. As shown in the figure, these connections (initialized to pseudo standard normal random numbers) were learnt to become negative on average, in revVRNNbp and more prominently in bioVRNNrf. This learnt negative-dominance (inhibition-dominance) could possibly be related, e.g., through prevention of excessive activity, to the good performance of bioVRNNrf and also the better performance of the untrained RNN with connections shuffled from bioVRNNrf than that of the naive untrained RNN.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6</label>
<caption><p>Performances of the modified value-RNN models in the cue-reward association task, in comparison with untrained RNN that also had the non-negative constraint. <bold>(A-D)</bold> State values (black lines) and TD-RPEs (red lines) at 1500-th trial in revVRNNbp (A), bioVRNNrf (B), untrained RNN with <bold><italic>x</italic></bold> and <bold><italic>w</italic></bold> constrained to be non-negative (C), and untrained RNN with non-negative <bold><italic>x</italic></bold> and <bold><italic>w</italic></bold> and having connections shuffled from those learnt in bioVRNNrf (D). The number of RNN units was 12 in all the cases. Error-bars indicate mean ± SEM across 100 simulations (same applied to (E,F)). The right histograms show the across-simulation distribution of the value of the pre-reward state in each model. The vertical black dashed lines in the histograms indicate the true value of the pre-reward state (estimated through simulations). <bold>(E)</bold> Learning performance of revVRNNbp (red line), bioVRNNrf (blue line), untrained RNN (gray solid line: partly out of view), and untrained RNN with connections shuffled from those learnt in bioVRNNrf (gray dotted line) when the number of RNN units was varied from 5 to 40 (horizontal axis). Leaning performance was measured by the sum of squares of differences between the state values developed at 1500-th trial by each of these four types of agent and the estimated true state values between cue and reward (vertical axis). <bold>(F)</bold> Mean of the elements of the recurrent and feed-forward connections (at 1500-th trial) of revVRNNbp (red line), bioVRNNrf (blue line), and untrained RNN (gray solid line).</p></caption>
<graphic xlink:href="609100v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We examined how the angle between the value weights (<bold><italic>w</italic></bold>) and the random feedback (<bold><italic>c</italic></bold>) changed across trials in bioVRNNrf. As shown in <xref rid="fig7" ref-type="fig">Fig. 7A</xref>, the angle was on average smaller than the chance-level angle (90°) from the beginning, while there was no further alignment over trials. This could be understood as follows. Because both the value weights (<bold><italic>w</italic></bold>) and the random feedback (<bold><italic>c</italic></bold>) were now constrained to be non-negative, these two vectors were ensured to be in a relatively close angle (i.e., in the same quadrant) from the beginning. By virtue of this loose alignment, the random feedback could act similarly to backprop-derived proper feedback, even without further alignment. We examined if the angle between the value weights (<bold><italic>w</italic></bold>) and the random feedback (<bold><italic>c</italic></bold>) at 1500-th trial was associated with the developed value of pre-reward state across simulations, but found no association (<italic>r</italic> = 0.0117, <italic>p</italic> = 0.908) (<xref rid="fig7" ref-type="fig">Fig. 7B</xref>). We then examined if the <bold><italic>w</italic></bold>-<bold><italic>c</italic></bold> angle at earlier trials (2nd - 500-th trials) was associated with the developed values at 500-th trial, with the number of simulations increased to 1000 so that small correlation could be detected. We found that the <bold><italic>w</italic></bold>-<bold><italic>c</italic></bold> angle at initial trials (2nd - around 10-th trials) was negatively correlated with the developed values of the reward state and preceding states at 500-th trial (<xref rid="fig7" ref-type="fig">Fig. 7C</xref>). As for the reward state, negative correlation at around 100-th - 300-th trial was also observed. These results suggest that better alignment of <bold><italic>w</italic></bold> and <bold><italic>c</italic></bold> at initial and early timings was associated with better development of state values, in line with the conjecture that loose alignment of <bold><italic>w</italic></bold> and <bold><italic>c</italic></bold> coming from the non-negative constraint supported learning. It should be noted, however, that there were cases where positive (although small) correlation was observed. Its exact reason is not sure, but it could be related to the fact that largeness of developed values or the speed of value development does not necessarily mean good learning.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7</label>
<caption><p>Loose alignment of the value weights (<bold><italic>w</italic></bold>) and the random feedback (<bold><italic>c</italic></bold>) in bioVRNNrf (with 12 RNN units), and its relation to the developed state values. <bold>(A)</bold> Over-trial changes in the angle between the value weights <bold><italic>w</italic></bold> and the fixed random feedback <bold><italic>c</italic></bold>. The solid line and the dashed lines indicate the mean and ± SD across 100 simulations, respectively. <bold>(B)</bold> Relation between the <bold><italic>w</italic></bold>-<bold><italic>c</italic></bold> angle (horizontal axis) and the value of the pre-reward state (vertical axis) at 1500-th trial. The dots indicate the results of individual simulations. <bold>(C)</bold> Correlation between the <bold><italic>w</italic></bold>-<bold><italic>c</italic></bold> angle at <italic>k</italic>-th trial (horizontal axis) and the value of the cue, post-cue, pre-reward, or reward state (top-bottom panels) at 500-th trial across 1000 simulations. The solid lines indicate the correlation coefficient, and the short vertical bars at the top of each panel indicates the cases in which <italic>p</italic>-value was less than 0.05.</p></caption>
<graphic xlink:href="609100v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We further examined how the revised value-RNN models performed in the two tasks with probabilistic structures examined above. Since the revised value-RNN models with 12 neurons appeared not able to produce the different patterns of TD-RPEs in the two tasks (TD-RPE at early reward &gt; TD-RPE at late reward in task 1 and opposite pattern in task 2), we increased the number of neurons to 20. Then, both revVRNNbp and bioVRNNrf produced such TD-RPE patterns (<xref rid="fig8" ref-type="fig">Fig. 8A,B</xref>) whereas untrained RNN of both kinds (naive, and with connections shuffled from bioVRNNrf) could not (<xref rid="fig8" ref-type="fig">Fig. 8C,D</xref>). This indicates that the value-RNN with random feedback and further biological constraints could learn the differential characteristics of the tasks.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8</label>
<caption><p>Performances of the modified value-RNN models in the two tasks having probabilistic structures, in comparison with untrained RNN having the non-negative constraint. TD-RPEs at the latest trial within 2000 trials in which reward was given at the early timing (red lines) or the late timing (blue lines) in task 1 (left) and task 2 (right), averaged across 100 simulations (error-bars indicating ± SEM across simulations), are shown for the four types of agent: (A) revVRNNbp; (B) bioVRNNrf; (C) untrained RNN with non-negative <bold><italic>x</italic></bold> and <bold><italic>w</italic></bold>; (D) untrained RNN with non-negative <bold><italic>x</italic></bold> and <bold><italic>w</italic></bold> and having connections shuffled from those learnt in bioVRNNrf. The number of RNN units was 20 for all the cases.</p></caption>
<graphic xlink:href="609100v2_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We have shown that state representation and value can be learned in the RNN and its downstream by using random feedback instead of backprop-derived biologically unavailable downstream weights. In the model without non-negative constraint, the feedback alignment, previously shown for supervised learning, occurred, and we have presented an intuitive understanding of its mechanism. In the model with non-negative constraint, loose alignment occurred from the beginning because of the constraint, and it appeared to support learning. Below we discuss implementation of the value-RNN with random feedback, pointing to a crucial role of DA outside of striatum, and also heterogeneity of DA signals. We further discuss limitations, relations to other proposals and suggestions, and future perspectives.</p>
<sec id="s3a">
<title>Implementation of the value-RNN with random feedback, featuring a role of DA outside of striatum</title>
<p>DA neurons in the midbrain project to not only the striatum but also the cortex, including the prefrontal cortex <sup><xref ref-type="bibr" rid="c48">48</xref></sup> and the hippocampus <sup><xref ref-type="bibr" rid="c49">49</xref></sup>. Previous studies demonstrated a crucial role of prefrontal DA in working memory <sup><xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c51">51</xref></sup>, presumably through the effects on synaptic/ionic conductance <sup><xref ref-type="bibr" rid="c52">52</xref>, <xref ref-type="bibr" rid="c53">53</xref></sup>. Roles of prefrontal DA in behavioral flexibility or decision making have also been suggested <sup><xref ref-type="bibr" rid="c54">54</xref></sup>. Moreover, a role of hippocampal DA in modulation of aversive memory formation has been demonstrated <sup><xref ref-type="bibr" rid="c55">55</xref></sup>. However, although i) there has been increasing evidence that DA represents TD-RPE <sup><xref ref-type="bibr" rid="c56">56</xref></sup>, ii) human fMRI experiments found TD-RPE correlates in cortical regions <sup><xref ref-type="bibr" rid="c57">57</xref></sup>, iii) DAergic modulation or initiation of plasticity in the prefrontal cortex <sup><xref ref-type="bibr" rid="c58">58</xref></sup> or the hippocampus <sup><xref ref-type="bibr" rid="c59">59</xref></sup> have been demonstrated, and iv) lesion or inactivation of prefrontal or hippocampal regions were found to disrupt DA’s encoding of RPE reflecting appropriate state representation <sup><xref ref-type="bibr" rid="c60">60</xref>–<xref ref-type="bibr" rid="c62">62</xref></sup>, what computational role in RL is played by TD-RPE-representing DA in the cortex remains to be clarified. This is behind compared to the case of striatum, where it has been widely considered that DAergic modulation of cortico-striatal synaptic weights implements TD-RPE-based update of state/action values <sup><xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c63">63</xref></sup>.</p>
<p>The value-RNN with fixed random feedback and biological constraints considered in the present work suggests a possibility that TD-RPE-representing DA modulates plasticity of RNN in the cortex so that state representation can be learnt. Different from the original value-RNN with backprop <sup><xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup>, update of intra-cortical connections does not require downstream cortico-striatal weights but requires only non-negative fixed random feedback specific to each post-synaptic neuron. The non-negativity was assumed so that the update rule became Hebbian under positive TD-RPE, since Hebbian plasticity has been suggested for rapid plasticity of cortical synapses <sup><xref ref-type="bibr" rid="c47">47</xref></sup>. The fixed randomness would naturally be achieved by intrinsic heterogeneity of neurons. The successful learning performance of our model thus indicates that DA-dependent modulation of Hebbian plasticity of cortical excitatory connections serves for learning of state representation that captures task structure.</p>
<p>VTA DA neurons project also to regions other than the striatum and cortex, including the basolateral amygdala (BLA) <sup><xref ref-type="bibr" rid="c64">64</xref></sup>, and DA was suggested to regulate plasticity also in the BLA <sup><xref ref-type="bibr" rid="c65">65</xref></sup>. Recent work <sup><xref ref-type="bibr" rid="c66">66</xref></sup> demonstrated that VTA→BLA DA entailed properties of TD-RPE, although increased rather than decreased upon aversive event, and was not itself reinforcing but necessary and sufficient for the formation of environmental model. BLA has recurrent connections <sup><xref ref-type="bibr" rid="c67">67</xref></sup>, projects to the striatum <sup><xref ref-type="bibr" rid="c68">68</xref>, <xref ref-type="bibr" rid="c69">69</xref></sup>, and engages in abstract context representation together with the prefrontal cortex <sup><xref ref-type="bibr" rid="c70">70</xref></sup>. Thus, given that environmental relationships needed for goal-directed behavior could be embedded in state representation <sup><xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup>, it seems possible that mechanism partly akin to the learning of state representation, but not value, in the RNN of our model takes place in the BLA. It remains open, however, whether and how such sophisticated representation can be learned. It might require multidimensional error <sup><xref ref-type="bibr" rid="c71">71</xref></sup> beyond TD-RPE, and/or multi-compartment unit <sup><xref ref-type="bibr" rid="c26">26</xref></sup>, both of which we will further discuss below.</p>
</sec>
<sec id="s3b">
<title>DA’s encoding of TD-RPE and other variables</title>
<p>There have been many results suggesting heterogeneity of DA signals. Recent work <sup><xref ref-type="bibr" rid="c72">72</xref></sup> suggested that there (co)exist different origins: (i) heterogeneity of learning target (reward or other), (ii) heterogeneity of state features, and (iii) others, such as ramping patterns. (i) is typically observed in DA neurons projecting to different regions, which can represent prediction errors of things other than reward. In contrast, (ii) is applied to DA neurons projecting to a same region, in which even though individual DA neurons show heterogeneous responses, the resulting merged DA signal still represents a scalar error such as TD-RPE.</p>
<p>Referring to a result <sup><xref ref-type="bibr" rid="c73">73</xref></sup> of type (i) and the fact that DA neurons receive inputs from the cerebellum <sup><xref ref-type="bibr" rid="c74">74</xref>, <xref ref-type="bibr" rid="c75">75</xref></sup> that supposedly implements supervised learning <sup><xref ref-type="bibr" rid="c76">76</xref></sup>, a recent modeling work <sup><xref ref-type="bibr" rid="c43">43</xref></sup> proposed that DA neurons convey vector-valued error signals, which are used for supervised learning of actions in continuous space. This previous work showed that learning occurred without adjustment of DA projection strengths because the feedback alignment mechanism worked. In contrast, in the present work, we assumed scalar TD-RPE, which can be consistent with type (ii) heterogeneity of DA signals. We have shown that the feedback alignment mechanism works also for RL, and moreover, learning could also occur by virtue of loose alignment coming from the biological constraints even without the operation of feedback alignment. Notably, the previous model <sup><xref ref-type="bibr" rid="c43">43</xref></sup> and our model can coexist, given that different DA neuronal populations may encode vector-valued error and TD-RPE, or even same single DA neuron might represent both errors depending on the context, reflecting which inputs are active.</p>
</sec>
<sec id="s3c">
<title>Limitations and possible reasons</title>
<p>We have shown that state representation and value could be learned in the value-RNN with fixed random feedback with a relatively small number of simple RNN units and observation inputs in simple simulated tasks. These simplicities enabled us to derive an intuitive understanding of how the feedback alignment could occur. However, in our models without the non-negativity constraint, as the number of RNN units increased, the performance of the models initially improved but then degraded when the number of RNN units increased beyond around 25. In contrast, in the original value-RNN with backprop <sup><xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup>, the ability to develop belief-state-like representation was reported to improve as the number of RNN units increased to 100 or 50.</p>
<p>There are several possible reasons for this difference. First, as a performance measure we used the sum of squared errors between the values developed by the value-RNN and the values estimated according to the definition (expected discounted cumulative future rewards) between cue and reward, while the previous studies focused on the similarity between the representation developed by the value-RNN and the handcrafted belief states. Second, there was a difference in the way of weight update. Specifically, as mentioned in the Results, while the previous studies used the BPTT <sup><xref ref-type="bibr" rid="c44">44</xref></sup>, which considers the recursive influence of the recurrent weights in a way that lacks causality, our models used an online learning rule, which only considers the influence of the recurrent weights at the previous time step.</p>
<p>Last but not least, there was a difference in the RNN unit. Specifically, we used a simple sigmoidal function, whereas the previous studies used the “Gated Recurrent Unit (GRU) cell” <sup><xref ref-type="bibr" rid="c77">77</xref></sup>. RNN with simple nonlinear unit is known to have the “vanishing gradient problem” <sup><xref ref-type="bibr" rid="c78">78</xref></sup>: through repetitive learning, the gradient of the loss function becomes so small that update becomes invisible. This issue could be alleviated by using an RNN unit having a memory, called the Long Short-Term Memory (LSTM) unit <sup><xref ref-type="bibr" rid="c79">79</xref></sup>. The GRU cell was suggested to have a memory function similar to the LSTM unit <sup><xref ref-type="bibr" rid="c77">77</xref></sup>. We focused on resolving the biological implausibility of the backprop, and sticked to the simple sigmoidal unit. However, gated unit similar to the LSTM unit has actually been proposed to be implemented in cortical microcircuits <sup><xref ref-type="bibr" rid="c80">80</xref></sup>, and incorporating the features of real neuron into value-RNN could enhance the computational power as we will discuss below.</p>
</sec>
<sec id="s3d">
<title>Biological details and future perspectives</title>
<p>Our RNN unit did not incorporate neuronal spiking and its effects on plasticity <sup><xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c81">81</xref>, <xref ref-type="bibr" rid="c82">82</xref></sup>, as well as neuronal morphology with nonlinear dendritic computations <sup><xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c83">83</xref>, <xref ref-type="bibr" rid="c84">84</xref></sup>. Importantly, recent studies suggest that dendritic mechanisms <sup><xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref></sup>, potentially together with burst-dependent plasticity <sup><xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c39">39</xref></sup>, can realize credit assignment without backprop in supervised learning, and also in unsupervised learning <sup><xref ref-type="bibr" rid="c85">85</xref>, <xref ref-type="bibr" rid="c86">86</xref></sup>. Dendritic mechanisms have their own specific features, or constraints, and so having them is different from increasing the number of layers of neural network, and it was argued <sup><xref ref-type="bibr" rid="c41">41</xref></sup> that adding such biological constraints enables learning in deep neural networks. Moreover, recent model of hippocampus <sup><xref ref-type="bibr" rid="c26">26</xref></sup> has shown that a network of multi-compartment units could learn complex representations. Given these, it would be interesting to explore if incorporating biological details into RNN unit can improve the performance of value-RNN.</p>
<p>A different alternative to backprop is the Associative Reward-Penalty (A<sub>R-P</sub>) algorithm <sup><xref ref-type="bibr" rid="c87">87</xref>–<xref ref-type="bibr" rid="c89">89</xref></sup>, in which the hidden units behave stochastically, and thereby the gradient could be estimated, in effect, through stochastic sampling without explicit information of the downstream weights. More recent work <sup><xref ref-type="bibr" rid="c90">90</xref></sup> demonstrated that noise-induced learning of back projections could achieve better alignment and performance compared with the case of fixed random feedback in a feed-forward network. These mechanisms can be biologically implemented because neurons and neural networks can exhibit noisy or chaotic behavior <sup><xref ref-type="bibr" rid="c91">91</xref>–<xref ref-type="bibr" rid="c93">93</xref></sup>, and are expected to potentially improve the performance of value-RNN.</p>
<p>Regarding the connectivity, in our models, recurrent/feed-forward connections could take both positive and negative values. This could be justified because there are both excitatory and inhibitory connections in the cortex and the net connection sign between two units can be positive or negative depending on whether excitation or inhibition exceeds the other. However, recent studies have shown that feed-forward and recurrent neural networks conforming to Dale’s law can perform well depending on the architecture, initialization, and update rules <sup><xref ref-type="bibr" rid="c94">94</xref>, <xref ref-type="bibr" rid="c95">95</xref></sup>. Integration of these models and ours, also with other connectivity features <sup><xref ref-type="bibr" rid="c96">96</xref></sup>, may be a fruitful direction.</p>
<p>More specific to the cortico-basal ganglia circuit, existences of D1/D2 DA receptors and D1-direct and D2-indirect basal ganglia pathways <sup><xref ref-type="bibr" rid="c97">97</xref>–<xref ref-type="bibr" rid="c100">100</xref></sup>, as well as cortical areas and cell types <sup><xref ref-type="bibr" rid="c101">101</xref>–<xref ref-type="bibr" rid="c104">104</xref></sup>, were also not incorporated. Furthermore, circuit/synaptic mechanisms of how TD-RPE is calculated in DA neurons (c.f., <sup><xref ref-type="bibr" rid="c105">105</xref>, <xref ref-type="bibr" rid="c106">106</xref></sup>) and/or how it can be learned (c.f., <sup><xref ref-type="bibr" rid="c107">107</xref></sup>) were unspecified. Future studies are expected to incorporate these factors.</p>
</sec>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Value-RNN with backprop (VRNNbp)</title>
<p>We constructed a value-RNN model based on the previous proposals <sup><xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup> but with several differences. We assumed that the activities of neurons in the RNN at time <italic>t</italic>+1 were determined by the activities of these neurons and neurons representing observation (cue, reward, or nothing) at time <italic>t</italic>:
<disp-formula>
<graphic xlink:href="609100v2_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula>
<graphic xlink:href="609100v2_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula>
<graphic xlink:href="609100v2_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
were the value weights. The error between this estimated value and the true value, <italic>v<sub>true</sub></italic>(<italic>t</italic>), was defined as:
<disp-formula>
<graphic xlink:href="609100v2_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Parameters <italic>w<sub>j</sub></italic>, <italic>A<sub>ij</sub></italic>, and <italic>B<sub>ik</sub></italic> that minimize the squared error <italic>ε</italic>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup> could be found by a gradient descent / error-backpropagation (backprop) method, i.e., by updating them in the directions of −∂(<italic>ε</italic>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>)/∂<italic>w<sub>j</sub></italic>, −∂(<italic>ε</italic>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>)/∂<italic>A<sub>ij</sub></italic>, and −∂(<italic>ε</italic>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>)/∂<italic>B<sub>ik</sub></italic>. −∂(<italic>ε</italic>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>)/∂<italic>w<sub>j</sub></italic> was calculated as follows:
<disp-formula>
<graphic xlink:href="609100v2_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In the last line, since <italic>ε</italic>(<italic>t</italic>) was unavailable as <italic>v<sub>true</sub></italic>(<italic>t</italic>) was unknown, it was approximated by the TD-RPE:
<disp-formula>
<graphic xlink:href="609100v2_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
−∂(<italic>ε</italic>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>)/∂<italic>A<sub>ij</sub></italic> was calculated as follows:
<disp-formula>
<graphic xlink:href="609100v2_ueqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<disp-formula>
<graphic xlink:href="609100v2_ueqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<p>Similarly, −∂(<italic>ε</italic>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>)/∂<italic>B<sub>ik</sub></italic> was calculated as follows:
<disp-formula>
<graphic xlink:href="609100v2_ueqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
According to these, the update rule for the value-RNN was determined as follows:
<disp-formula>
<graphic xlink:href="609100v2_ueqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>a</italic> was the learning rate. In each simulation, the elements of <bold>A</bold> and <bold>B</bold>, as well as the elements of <bold><italic>x</italic></bold>, were initialized to pseudo standard normal random numbers, and the elements of <bold><italic>w</italic></bold> were initialized to 0.</p>
</sec>
<sec id="s4b">
<title>Value-RNN with fixed random feedback (VRNNrf)</title>
<p>We considered an implementation of the value-RNN described above in the cortico-basal ganglia-DA system (<xref rid="fig1" ref-type="fig">Fig. 1</xref>):</p>
<p><bold><italic>x</italic></bold> : activities of neurons in a cortical region with rich recurrent connections</p>
<p><bold>A</bold> : recurrent connection strengths among <bold><italic>x</italic></bold>
<list list-type="simple">
<list-item><p><bold><italic>ο</italic></bold> : activities of neurons in a cortical region processing sensory inputs</p></list-item>
</list>
<bold>B</bold> : feed-forward connection strengths from <bold><italic>o</italic></bold> to <bold><italic>x</italic></bold></p>
<p><italic>f</italic> : sigmoidal relationship from the input to the output of the cortical neurons</p>
<p><bold><italic>w</italic></bold> : connection strengths from cortical neurons <bold><italic>x</italic></bold> to a group of striatal neurons</p>
<p><italic>v</italic> : activity of the group of striatal neurons</p>
<p><italic>δ</italic> : activity of a group of DA neurons / released DA</p>
<p>The update rule for <bold><italic>w</italic></bold>
<disp-formula>
<graphic xlink:href="609100v2_ueqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
could be naturally implemented as cortico-striatal synaptic plasticity, which depends on DA (<italic>δ</italic>(<italic>t</italic>)) and pre-synaptic (cortical) neuronal activity (<italic>x<sub>j</sub></italic>(<italic>t</italic>)). However, an issue emerged in implementation of the update rules for <bold>A</bold> and <bold>B</bold>:
<disp-formula>
<graphic xlink:href="609100v2_ueqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Specifically, <italic>w<sub>i</sub></italic> included in the rightmost of these update rules (for the strengths of cortico-cortical synapses <italic>A<sub>ij</sub></italic> and <italic>B<sub>ik</sub></italic>) is the connection strength from cortical neuron <italic>x<sub>i</sub></italic> to striatal neurons, i.e., the strength of the cortico-striatal synapses (located within the striatum), which is considered to be unavailable at the cortico-cortical synapses (located within the cortex).</p>
<p>As mentioned in the Introduction, this is an example of the long-standing difficulty in biological implementation of backprop, and recently a potential solution for this difficulty, i.e., replacement of the downstream connection strengths in the update rule for upstream connections with fixed random strengths, has been demonstrated in supervised learning of feed-forward and recurrent networks <sup><xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>. The value-RNN, which we considered here, differed from supervised learning considered in these previous studies in two ways: i) it was TD learning, apparent in the approximation of the true error <italic>ε</italic>(<italic>t</italic>) by the TD-RPE <italic>δ</italic>(<italic>t</italic>) in the derivation described above, and ii) it used a scalar error (TD-RPE) rather than a vector error. But we expected that the feedback alignment mechanism could still work at least to some extent, and explored it in this study. Specifically, we examined a modified value-RNN with fixed random feedback (VRNNrf), in which the update rules for <bold>A</bold> and <bold>B</bold> were modified as follows:
<disp-formula>
<graphic xlink:href="609100v2_ueqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>w<sub>i</sub></italic> in the update rules of the value-RNN with backprop (VRNNbp) was replaced with a fixed random parameter <italic>c<sub>i</sub></italic>. Notably, these modified update rules for the cortico-cortical connections <bold>A</bold> and <bold>B</bold> required only pre-synaptic activities (<italic>x<sub>j</sub></italic>(<italic>t</italic>−1), <italic>o<sub>k</sub></italic>(<italic>t</italic>−1)), post-synaptic activities (<italic>x<sub>i</sub></italic>(<italic>t</italic>)), TD-RPE-representing DA (<italic>δ</italic>(<italic>t</italic>)), and fixed random strengths (<italic>c<sub>i</sub></italic>), which would all be available at the cortico-cortical synapses given that VTA DA neurons project not only to the striatum but also to the cortex and random <italic>c<sub>i</sub></italic> could be provided by intrinsic heterogeneity. In each simulation, the elements of <bold><italic>c</italic></bold> were initialized to pseudo standard normal random numbers.</p>
</sec>
<sec id="s4c">
<title>Revised value-RNN models with further biological constraints</title>
<p>In the later part of this study, we examined revised value-RNN models with further biological constraints. Specifically, we considered models, in which the value weights and the activities of neurons in the RNN were constrained to be non-negative. In order to do so, the update rule for <bold><italic>w</italic></bold> was modified to:
<disp-formula>
<graphic xlink:href="609100v2_ueqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where max(<italic>q</italic><sub>1</sub>, <italic>q</italic><sub>2</sub>) returned the maximum of <italic>q</italic><sub>1</sub> and <italic>q</italic><sub>2</sub>. Also, the sigmoidal input-output function was replaced with
<disp-formula>
<graphic xlink:href="609100v2_ueqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and the elements of <bold><italic>x</italic></bold> were initialized to pseudo uniform [0 1] random numbers. The backprop-based update rules for <bold>A</bold> and <bold>B</bold> in VRNNbp were replaced with
<disp-formula>
<graphic xlink:href="609100v2_ueqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We referred to the model with these modifications to VRNNbp as revVRNNbp.</p>
<p>As a revised value-RNN with fixed random feedback (VRNNrf), in addition to the abovementioned modifications of the update of <bold><italic>w</italic></bold>, the sigmoidal input-output function, and the initialization of <bold><italic>x</italic></bold>, the fixed random feedback <bold><italic>c</italic></bold> was assumed to be non-negative. Specifically, the elements of <bold><italic>c</italic></bold> were set to pseudo uniform [0 1] random numbers. Moreover, the update rules for <bold>A</bold> and</p>
<p><bold>B</bold> were replaced with
<disp-formula>
<graphic xlink:href="609100v2_ueqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
so that the originally non-monotonic dependence on <italic>x<sub>i</sub></italic>(<italic>t</italic>) (post-synaptic activity) became monotonic + saturation (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). These update rules with non-negative <italic>c<sub>i</sub></italic> could be said to be Hebbian with additional modulation by TD-RPE (Hebbian under positive TD-RPE). We referred to the model with these modifications to VRNNrf as bioVRNNrf.</p>
</sec>
<sec id="s4d">
<title>Simulation of the tasks</title>
<p>In the Pavlovian cue-reward association task, at time 1 of each trial, cue observation was received by the RNN, and at time 4, reward observation was received. Trial was pseudo-randomly ended at time 7, 8, 9, or 10, and the next trial started from the next time-step. Reward size was <italic>r</italic> = 1. The tasks with probabilistic structures (task 1 and task 2) were implemented in the same way except that reward timing was not time 4 but time 3 or 5 with equal probabilities, specifically, 50% and 50% in task 1 and 30% and 30% in task 2, and there was no reward in the remaining 40% of trials in task 2.</p>
<p>The cue or reward state/timing, mentioned in the text and marked in the figures, was defined to be the timing when the RNN received the cue or reward observation, respectively. Specifically, if <bold><italic>o</italic></bold>(<italic>t</italic>) = (1 0)<sup>T</sup> or <bold><italic>o</italic></bold>(<italic>t</italic>) = (0 1)<sup>T</sup> at time <italic>t</italic>, <italic>t</italic> + 1 was defined to be a cue or reward timing, respectively. For the agents with punctate (CSC) representation, each timing in the tasks was represented by a 10-dimensional one-hot vector, starting from (1 0 0 … 0)<sup>T</sup> for the cue state, with the next state (0 1 0 … 0)<sup>T</sup> and so on.</p>
<p>Unless otherwise mentioned, parameters were set to the following values. Learning rate (<italic>a</italic>): 0.1 (normalization by the squared norm of feature vector was not implemented). Time discount factor (<italic>γ</italic>): 0.8.</p>
</sec>
<sec id="s4e">
<title>Estimation of true state values</title>
<p>As for the Pavlovian cue-reward association task, we defined states by relative timings from the cue, and estimated their (true) state values by simulations according to the definition of state value. Specifically, we generated a sequence of cues and rewards corresponding to 1000 trials, and calculated cumulative discounted future rewards within the sequence:
<disp-formula>
<graphic xlink:href="609100v2_ueqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic><sub>t</sub></italic><sub>_rew</sub> denotes the time-step of each reward counted from the starting state, starting from -2, -1, …, and +6 time steps from a cue. We repeated this 1000 times, generating 1000 sequences (i.e., 1000 simulations of 1000 trials), with different sets of pseudo-random numbers, and calculated an average over these 1000 sequences so as to estimate the expected cumulative discounted future rewards, i.e., state value (by definition) for each state (−2, -1, …, and +6 time steps from cue). Using these estimated true state values, we calculated TD-RPE at each state (−2, -1, …, and +5 time steps from cue).</p>
<p>In a similar manner, we defined states and estimated true state values, and also calculated TD-RPE, for tasks 1 and 2 that had probabilistic structures. As for task 1, we defined the following states: -2, -1, .., and +2 time steps from cue (i.e., states visited (entered) before knowing whether reward was given at the early timing (= +2 time step from cue)), +3, 4, 5, and 6 time steps from cue after reception of reward at the early timing, and +3, 4, 5, and 6 time steps from cue after no reception of reward at the early timing (in total 5 + 4 + 4 = 13 states) (<xref ref-type="fig" rid="fig4">Fig. 4Bc</xref>, left-top). We generated 10000 sequences of cues and rewards corresponding to 1000 trials (i.e., 10000 simulations of 1000 trials), and for each state, calculated cumulative discounted future rewards within the sequence for each of 10000 simulations and took an average to obtain the expected cumulative discounted future rewards (i.e., estimation of state value) (<xref ref-type="fig" rid="fig4">Fig. 4Bc</xref>, left-bottom). Using the estimated state values, we calculated TD-RPE (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>, left).</p>
<p>As for task 2, we defined the following states: -2, -1, .., and +2 time steps from cue (i.e., states visited (entered) before knowing whether reward was given at the early timing), +3, 4, 5, and 6 time steps from cue after reception of reward at the early timing, +3 and 4 time steps from cue after no reception of reward at the early timing (states visited (entered) before knowing whether reward was given at the late timing (= +4 time step from cue)), +5 and 6 time steps from cue after reception of reward at the late timing, and +5 and 6 time steps from cue after no reception of reward at both early and late timings (in total 5 + 4 + 2 + 2 + 2 = 15 states) (<xref ref-type="fig" rid="fig4">Fig. 4Bc</xref>, right-top). We estimated state values of these states (<xref ref-type="fig" rid="fig4">Fig. 4Bc</xref>, right-bottom), and also calculated TD-RPE (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>, right), in similar manners to the above.</p>
</sec>
<sec id="s4f">
<title>Analyses, software, and code availability</title>
<p>SEM (Standard error of the mean) was approximated by SD (standard deviation)/√<italic>N</italic> (number of samples). Linear regression and principal component analysis (PCA) were conducted by using R (functions lm and prcomp). Simulations were conducted by using MATLAB, and pseudo-random numbers were implemented by using rand, randn, and randperm functions. All the codes will be made available at GitHub upon publication of this work in a journal.</p>
</sec>
</sec>
<sec id="s5">
<title>Author contributions</title>
<p>Conceptualization: KM; Formal analysis: KM, TT; Investigation: KM, TT, AyK; Writing – original draft: KM; Writing – review &amp; editing: KM, TT, AyK, ArK</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>The authors thank Dr. Kenji Doya for valuable suggestions. KM was supported by Grants-in-Aid for Scientific Research 23H03295 and 23K27985 from Japan Society for the Promotion of Science (JSPS) and the Naito Foundation. AyK was supported by JSPS Overseas Research Fellowships. ArK was partially funded by Digital Futures (KTH) grant and StratNeuro SRA.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Starkweather</surname>, <given-names>C.K.</given-names></string-name>, <string-name><surname>Babayan</surname>, <given-names>B.M.</given-names></string-name>, <string-name><surname>Uchida</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Gershman</surname>, <given-names>S.J</given-names></string-name></person-group>. <article-title>Dopamine reward prediction errors reflect hidden-state inference across time</article-title>. <source>Nat Neurosci</source> <volume>20</volume>, <fpage>581</fpage>–<lpage>589</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hennig</surname>, <given-names>J.A.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Emergence of belief-like representations through reinforcement learning</article-title>. <source>PLoS Comput Biol</source> <volume>19</volume>, <fpage>e1011067</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Montague</surname>, <given-names>P.R.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Sejnowski</surname>, <given-names>T.J</given-names></string-name></person-group>. <article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title>. <source>J Neurosci</source> <volume>16</volume>, <fpage>1936</fpage>–<lpage>1947</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schultz</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Montague</surname>, <given-names>P.R</given-names></string-name></person-group>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source> <volume>275</volume>, <fpage>1593</fpage>–<lpage>1599</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niv</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Schoenbaum</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>Dialogues on prediction errors</article-title>. <source>Trends Cogn Sci</source> <volume>12</volume>, <fpage>265</fpage>–<lpage>272</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>J.Y.</given-names></string-name>, <string-name><surname>Haesler</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vong</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lowell</surname>, <given-names>B.B.</given-names></string-name> &amp; <string-name><surname>Uchida</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title>. <source>Nature</source> <volume>482</volume>, <fpage>85</fpage>–<lpage>88</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinberg</surname>, <given-names>E.E.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>A causal link between prediction errors, dopamine neurons and learning</article-title>. <source>Nat Neurosci</source> <volume>16</volume>, <fpage>966</fpage>–<lpage>973</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reynolds</surname>, <given-names>J.N.</given-names></string-name>, <string-name><surname>Hyland</surname>, <given-names>B.I.</given-names></string-name> &amp; <string-name><surname>Wickens</surname>, <given-names>J.R</given-names></string-name></person-group>. <article-title>A cellular mechanism of reward-related learning</article-title>. <source>Nature</source> <volume>413</volume>, <fpage>67</fpage>–<lpage>70</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Flajolet</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Greengard</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Surmeier</surname>, <given-names>D.J</given-names></string-name></person-group>. <article-title>Dichotomous dopaminergic control of striatal synaptic plasticity</article-title>. <source>Science</source> <volume>321</volume>, <fpage>848</fpage>–<lpage>851</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yagishita</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>A critical time window for dopamine actions on the structural plasticity of dendritic spines</article-title>. <source>Science</source> <volume>345</volume>, <fpage>1616</fpage>–<lpage>1620</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russek</surname>, <given-names>E.M.</given-names></string-name>, <string-name><surname>Momennejad</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Botvinick</surname>, <given-names>M.M.</given-names></string-name>, <string-name><surname>Gershman</surname>, <given-names>S.J.</given-names></string-name> &amp; <string-name><surname>Daw</surname>, <given-names>N.D</given-names></string-name></person-group>. <article-title>Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title>. <source>PLoS Comput Biol</source> <volume>13</volume>, <fpage>e1005768</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stachenfeld</surname>, <given-names>K.L.</given-names></string-name>, <string-name><surname>Botvinick</surname>, <given-names>M.M.</given-names></string-name> &amp; <string-name><surname>Gershman</surname>, <given-names>S.J</given-names></string-name></person-group>. <article-title>The hippocampus as a predictive map</article-title>. <source>Nat Neurosci</source> <volume>20</volume>, <fpage>1643</fpage>–<lpage>1653</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Qian</surname>, <given-names>L.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>The role of prospective contingency in the control of behavior and dopamine signals during associative learning</article-title>. <source>bioRxiv</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Langdon</surname>, <given-names>A.J.</given-names></string-name>, <string-name><surname>Sharpe</surname>, <given-names>M.J.</given-names></string-name>, <string-name><surname>Schoenbaum</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Niv</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Model-based predictions for dopamine</article-title>. <source>Curr Opin Neurobiol</source> <volume>49</volume>, <fpage>1</fpage>–<lpage>7</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keiflin</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Pribut</surname>, <given-names>H.J.</given-names></string-name>, <string-name><surname>Shah</surname>, <given-names>N.B.</given-names></string-name> &amp; <string-name><surname>Janak</surname>, <given-names>P.H</given-names></string-name></person-group>. <article-title>Ventral Tegmental Dopamine Neurons Participate in Reward Identity Predictions</article-title>. <source>Curr Biol</source> <volume>29</volume>, <fpage>93</fpage>–<lpage>103.e103</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Redish</surname>, <given-names>A.D.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Kurth-Nelson</surname>, <given-names>Z</given-names></string-name></person-group>. <article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling</article-title>. <source>Psychol Rev</source> <volume>114</volume>, <fpage>784</fpage>–<lpage>805</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gershman</surname>, <given-names>S.J.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>C.E.</given-names></string-name>, <string-name><surname>Norman</surname>, <given-names>K.A.</given-names></string-name>, <string-name><surname>Monfils</surname>, <given-names>M.H.</given-names></string-name> &amp; <string-name><surname>Niv</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Gradual extinction prevents the return of fear: implications for the discovery of state</article-title>. <source>Front Behav Neurosci</source> <volume>7</volume>, <fpage>164</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shimomura</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kato</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Morita</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Rigid reduced successor representation as a potential mechanism for addiction</article-title>. <source>Eur J Neurosci</source> <volume>53</volume>, <fpage>3768</fpage>–<lpage>3790</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Nagase</surname>, <given-names>A.M.</given-names></string-name> &amp; <string-name><surname>Morita</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>A Reinforcement Learning Approach to Understanding Procrastination: Does Inaccurate Value Approximation Cause Irrational Postponing of a Task?</article-title> <source>Front Neurosci</source> <volume>15</volume>, <issue>660595</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sato</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Shimomura</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Morita</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Opponent learning with different representations in the cortico-basal ganglia pathways can develop obsession-compulsion cycle</article-title>. <source>PLoS Comput Biol</source> <volume>19</volume>, <fpage>e1011206</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gershman</surname>, <given-names>S.J.</given-names></string-name> &amp; <string-name><surname>Niv</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Learning latent structure: carving nature at its joints</article-title>. <source>Curr Opin Neurobiol</source> <volume>20</volume>, <fpage>251</fpage>–<lpage>256</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niv</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Learning task-state representations</article-title>. <source>Nat Neurosci</source> <volume>22</volume>, <fpage>1544</fpage>–<lpage>1553</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>George</surname>, <given-names>T.M.</given-names></string-name>, <string-name><surname>de Cothi</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Stachenfeld</surname>, <given-names>K.L.</given-names></string-name> &amp; <string-name><surname>Barry</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Rapid learning of predictive maps with STDP and theta phase precession</article-title>. <source>Elife</source> <volume>12</volume>, <elocation-id>e80663</elocation-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bono</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zannone</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pedrosa</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Clopath</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Learning predictive cognitive maps with spiking neurons during behavior and replays</article-title>. <source>Elife</source> <volume>12</volume>, <elocation-id>e80671</elocation-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Aronov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Abbott</surname>, <given-names>L.F.</given-names></string-name> &amp; <string-name><surname>Mackevicius</surname>, <given-names>E.L</given-names></string-name></person-group>. <article-title>Neural learning rules for generating flexible predictions and computing the successor representation</article-title>. <source>Elife</source> <volume>12</volume>, <elocation-id>e80680</elocation-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cone</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Clopath</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Latent representations in hippocampal network model co-evolve with behavioral exploration of task structure</article-title>. <source>Nat Commun</source> <volume>15</volume>, <issue>687</issue> (<year>2024</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amari</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>A Theory of Adaptive Pattern Classifiers</article-title>. <source>IEEE Transactions on Electronic Computers <bold>EC</bold></source><bold>-</bold><volume>16</volume>, <fpage>299</fpage>–<lpage>307</lpage> (<year>1967</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rumelhart</surname>, <given-names>D.E.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G.E.</given-names></string-name> &amp; <string-name><surname>Williams</surname>, <given-names>R.J</given-names></string-name></person-group>. <article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source> <volume>323</volume>, <fpage>533</fpage>–<lpage>536</lpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doya</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Complementary roles of basal ganglia and cerebellum in learning and motor control</article-title>. <source>Curr Opin Neurobiol</source> <volume>10</volume>, <fpage>732</fpage>–<lpage>739</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Doherty</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title>. <source>Science</source> <volume>304</volume>, <fpage>452</fpage>–<lpage>454</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grossberg</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Competitive learning: from interactive activation to adaptive resonance</article-title>. <source>Cognitive Science</source> <volume>11</volume>, <fpage>23</fpage>–<lpage>63</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crick</surname>, <given-names>F</given-names></string-name></person-group>. <article-title>The recent excitement about neural networks</article-title>. <source>Nature</source> <volume>337</volume>, <fpage>129</fpage>–<lpage>132</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lillicrap</surname>, <given-names>T.P.</given-names></string-name>, <string-name><surname>Cownden</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tweed</surname>, <given-names>D.B.</given-names></string-name> &amp; <string-name><surname>Akerman</surname>, <given-names>C.J</given-names></string-name></person-group>. <article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title>. <source>Nat Commun</source> <volume>7</volume>, <issue>13276</issue> (<year>2016</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guerguiev</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lillicrap</surname>, <given-names>T.P.</given-names></string-name> &amp; <string-name><surname>Richards</surname>, <given-names>B.A</given-names></string-name></person-group>. <article-title>Towards deep learning with segregated dendrites</article-title>. <source>Elife</source> <volume>6</volume>, <elocation-id>e22901</elocation-id> (<year>2017</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sacramento</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>R.P.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Dendritic cortical microcircuits approximate the backpropagation algorithm</article-title><source>. in Advances in Neural Information Processing Systems</source> <volume>31</volume> <italic>(NeurIPS 2018)</italic> (<year>2018</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whittington</surname>, <given-names>J.C.R.</given-names></string-name> &amp; <string-name><surname>Bogacz</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Theories of Error Back-Propagation in the Brain</article-title>. <source>Trends Cogn Sci</source> <volume>23</volume>, <fpage>235</fpage>–<lpage>250</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lillicrap</surname>, <given-names>T.P.</given-names></string-name>, <string-name><surname>Santoro</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Marris</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Akerman</surname>, <given-names>C.J.</given-names></string-name> &amp; <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name></person-group> <article-title>Backpropagation and the brain</article-title>. <source>Nat Rev Neurosci</source> <volume>21</volume>, <fpage>335</fpage>–<lpage>346</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Payeur</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Guerguiev</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zenke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B.A.</given-names></string-name> &amp; <string-name><surname>Naud</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits</article-title>. <source>Nat Neurosci</source> <volume>24</volume>, <fpage>1010</fpage>–<lpage>1019</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greedy</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>H.W.</given-names></string-name>, <string-name><surname>Pemberton</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mellor</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Costa</surname>, <given-names>R.P.</given-names></string-name></person-group> <article-title>Single-phase deep learning in cortico-cortical networks</article-title>. <source>Advances in Neural Information Processing Systems (NeurIPS 2022)</source> <volume>35</volume> (<year>2022</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname>, <given-names>Y.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Inferring neural activity before plasticity as a foundation for learning beyond backpropagation</article-title>. <source>Nat Neurosci</source> <volume>27</volume>, <fpage>348</fpage>–<lpage>358</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pagkalos</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Makarov</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Poirazi</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Leveraging dendritic properties to advance machine learning and neuro-inspired computing</article-title>. <source>Curr Opin Neurobiol</source> <volume>85</volume>, <fpage>102853</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>J.M</given-names></string-name></person-group>. <article-title>Local online learning in recurrent networks with random feedback</article-title>. <source>Elife</source> <volume>8</volume> (<year>2019</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wärnberg</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Kumar</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Feasibility of dopamine as a vector-valued feedback signal in the basal ganglia</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>120</volume>, <fpage>e2221994120</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rumelhart</surname>, <given-names>D.E.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G.E.</given-names></string-name> &amp; <string-name><surname>Williams</surname>, <given-names>R.J.</given-names></string-name> . in  (ed. <string-name><given-names>D.E.</given-names> <surname>Rumelhart</surname></string-name>, <string-name><surname>McClelland</surname>, <given-names>J.L.</given-names></string-name>, <collab>The PDP Group</collab></person-group><chapter-title>Learning Internal Representations by Error Propagation</chapter-title>. <source>Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations</source> <fpage>318</fpage>–<lpage>362</lpage> (<publisher-name>MIT Press, Cambridge</publisher-name>, <year>1985</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ludvig</surname>, <given-names>E.A.</given-names></string-name>, <string-name><surname>Sutton</surname>, <given-names>R.S.</given-names></string-name> &amp; <string-name><surname>Kehoe</surname>, <given-names>E.J</given-names></string-name></person-group>. <article-title>Evaluating the TD model of classical conditioning</article-title>. <source>Learn Behav</source> <volume>40</volume>, <fpage>305</fpage>–<lpage>319</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sutton</surname>, <given-names>R.S.</given-names></string-name> &amp; <string-name><surname>Barto</surname>, <given-names>A.G</given-names></string-name></person-group>. <source>Reinforcement Learning: An Introduction (Second Edition)</source> (<publisher-name>MIT Press, Cambridge</publisher-name>, <publisher-loc>MA</publisher-loc>, <year>2018</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feldman</surname>, <given-names>D.E</given-names></string-name></person-group>. <article-title>Synaptic mechanisms for plasticity in neocortex</article-title>. <source>Annu Rev Neurosci</source> <volume>32</volume>, <fpage>33</fpage>–<lpage>55</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Williams</surname>, <given-names>S.M.</given-names></string-name> &amp; <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. <article-title>Widespread origin of the primate mesofrontal dopamine system</article-title>. <source>Cereb Cortex</source> <volume>8</volume>, <fpage>321</fpage>–<lpage>345</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Broussard</surname>, <given-names>J.I.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Dopamine Regulates Aversive Contextual Learning and Associated In Vivo Synaptic Plasticity in the Hippocampus</article-title>. <source>Cell Rep</source> <volume>14</volume>, <fpage>1930</fpage>–<lpage>1939</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brozoski</surname>, <given-names>T.J.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Rosvold</surname>, <given-names>H.E.</given-names></string-name> &amp; <string-name><surname>Goldman</surname>, <given-names>P.S</given-names></string-name></person-group>. <article-title>Cognitive deficit caused by regional depletion of dopamine in prefrontal cortex of rhesus monkey</article-title>. <source>Science</source> <volume>205</volume>, <fpage>929</fpage>–<lpage>932</lpage> (<year>1979</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sawaguchi</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Goldman-Rakic</surname>, <given-names>P.S</given-names></string-name></person-group>. <article-title>D1 dopamine receptors in prefrontal cortex: involvement in working memory</article-title>. <source>Science</source> <volume>251</volume>, <fpage>947</fpage>–<lpage>950</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Durstewitz</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Seamans</surname>, <given-names>J.K.</given-names></string-name> &amp; <string-name><surname>Sejnowski</surname>, <given-names>T.J</given-names></string-name></person-group>. <article-title>Dopamine-mediated stabilization of delay-period activity in a network model of prefrontal cortex</article-title>. <source>J Neurophysiol</source> <volume>83</volume>, <fpage>1733</fpage>–<lpage>1750</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Wang</surname>, <given-names>X</given-names></string-name></person-group>. <article-title>Effects of neuromodulation in a cortical network model of object working memory dominated by recurrent inhibition</article-title>. <source>J Comput Neurosci</source> <volume>11</volume>, <fpage>63</fpage>–<lpage>85</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Floresco</surname>, <given-names>S.B.</given-names></string-name> &amp; <string-name><surname>Magyar</surname>, <given-names>O</given-names></string-name></person-group>. <article-title>Mesocortical dopamine modulation of executive functions: beyond working memory</article-title>. <source>Psychopharmacology (Berl)</source> <volume>188</volume>, <fpage>567</fpage>–<lpage>585</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsetsenis</surname>, <given-names>T.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Midbrain dopaminergic innervation of the hippocampus is sufficient to modulate formation of aversive memories</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>118</volume>, <fpage>e2111069118</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>H.R.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>A Unified Framework for Dopamine Signals across Timescales</article-title>. <source>Cell</source> <volume>183</volume>, <fpage>1600</fpage>–<lpage>1616.e1625</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Doherty</surname>, <given-names>J.P.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Critchley</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Dolan</surname>, <given-names>R.J</given-names></string-name></person-group>. <article-title>Temporal difference models and reward-related learning in the human brain</article-title>. <source>Neuron</source> <volume>38</volume>, <fpage>329</fpage>–<lpage>337</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Otani</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Daniel</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Roisin</surname>, <given-names>M.P.</given-names></string-name> &amp; <string-name><surname>Crepel</surname>, <given-names>F</given-names></string-name></person-group>. <article-title>Dopaminergic modulation of long-term synaptic plasticity in rat prefrontal neurons</article-title>. <source>Cereb Cortex</source> <volume>13</volume>, <fpage>1251</fpage>–<lpage>1256</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sayegh</surname>, <given-names>F.J.P.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Ventral tegmental area dopamine projections to the hippocampus trigger long-term potentiation and contextual learning</article-title>. <source>Nat Commun</source> <volume>15</volume>, <fpage>4100</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takahashi</surname>, <given-names>Y.K.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Expectancy-related changes in firing of dopamine neurons depend on orbitofrontal cortex</article-title>. <source>Nat Neurosci</source> <volume>14</volume>, <fpage>1590</fpage>–<lpage>1597</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Starkweather</surname>, <given-names>C.K.</given-names></string-name>, <string-name><surname>Gershman</surname>, <given-names>S.J.</given-names></string-name> &amp; <string-name><surname>Uchida</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>The Medial Prefrontal Cortex Shapes Dopamine Reward Prediction Errors under State Uncertainty</article-title>. <source>Neuron</source> <volume>98</volume>, <fpage>616</fpage>–<lpage>629.e616</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Takahashi</surname>, <given-names>Y.K.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Expectancy-related changes in firing of dopamine neurons depend on hippocampus</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.07.19.549728</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Samejima</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ueda</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Doya</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Kimura</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Representation of action-specific reward values in the striatum</article-title>. <source>Science</source> <volume>310</volume>, <fpage>1337</fpage>–<lpage>1340</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beier</surname>, <given-names>K.T.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Circuit Architecture of VTA Dopamine Neurons Revealed by Systematic Input-Output Mapping</article-title>. <source>Cell</source> <volume>162</volume>, <fpage>622</fpage>–<lpage>634</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Rainnie</surname>, <given-names>D.G</given-names></string-name></person-group>. <article-title>Bidirectional regulation of synaptic plasticity in the basolateral amygdala induced by the D1-like family of dopamine receptors and group II metabotropic glutamate receptors</article-title>. <source>J Physiol</source> <volume>592</volume>, <fpage>4329</fpage>–<lpage>4351</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sias</surname>, <given-names>A.C.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Dopamine projections to the basolateral amygdala drive the encoding of identity-specific reward memories</article-title>. <source>Nat Neurosci</source> <volume>27</volume>, <fpage>728</fpage>–<lpage>736</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Headley</surname>, <given-names>D.B.</given-names></string-name>, <string-name><surname>Kyriazi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nair</surname>, <given-names>S.S.</given-names></string-name> &amp; <string-name><surname>Pare</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>Gamma Oscillations in the Basolateral Amygdala: Localization, Microcircuitry, and Behavioral Correlates</article-title>. <source>J Neurosci</source> <volume>41</volume>, <fpage>6087</fpage>–<lpage>6101</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Britt</surname>, <given-names>J.P.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Synaptic and behavioral profile of multiple glutamatergic inputs to the nucleus accumbens</article-title>. <source>Neuron</source> <volume>76</volume>, <fpage>790</fpage>–<lpage>803</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>I.B.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Persistent enhancement of basolateral amygdala-dorsomedial striatum synapses causes compulsive-like behaviors in mice</article-title>. <source>Nat Commun</source> <volume>15</volume>, <issue>219</issue> (<year>2024</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rigotti</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ostojic</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fusi</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Salzman</surname>, <given-names>C.D</given-names></string-name></person-group>. <article-title>Abstract Context Representations in Primate Amygdala and Prefrontal Cortex</article-title>. <source>Neuron</source> <volume>87</volume>, <fpage>869</fpage>–<lpage>881</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stalnaker</surname>, <given-names>T.A.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Dopamine neuron ensembles signal the content of sensory prediction errors</article-title>. <source>Elife</source> <volume>8</volume>, <elocation-id>e49315</elocation-id> (<year>2019</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>R.S.</given-names></string-name>, <string-name><surname>Sagiv</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Engelhard</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Witten</surname>, <given-names>I.B.</given-names></string-name> &amp; <string-name><surname>Daw</surname>, <given-names>N.D</given-names></string-name></person-group>. <article-title>A feature-specific prediction error model explains dopaminergic heterogeneity</article-title>. <source>Nat Neurosci</source> <volume>27</volume>, <fpage>1574</fpage>–<lpage>1586</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avvisati</surname>, <given-names>R.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Distributional coding of associative learning in discrete populations of midbrain dopamine neurons</article-title>. <source>Cell Rep</source> <volume>43</volume>, <issue>114080</issue> (<year>2024</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watabe-Uchida</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ogawa</surname>, <given-names>S.K.</given-names></string-name>, <string-name><surname>Vamanrao</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Uchida</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>Whole-brain mapping of direct inputs to midbrain dopamine neurons</article-title>. <source>Neuron</source> <volume>74</volume>, <fpage>858</fpage>–<lpage>873</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carta</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>C.H.</given-names></string-name>, <string-name><surname>Schott</surname>, <given-names>A.L.</given-names></string-name>, <string-name><surname>Dorizan</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Khodakhah</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Cerebellar modulation of the reward circuitry and social behavior</article-title>. <source>Science</source> <volume>363</volume> (<year>2019</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marr</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>A theory of cerebellar cortex</article-title>. <source>J Physiol</source> <volume>202</volume>, <fpage>437</fpage>–<lpage>470</lpage> (<year>1969</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Cho</surname>, <given-names>K.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</article-title>. <source>arXiv</source> <pub-id pub-id-type="doi">10.48550/arXiv.1406.1078</pub-id> (<year>2014</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hochreiter</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>The vanishing gradient problem during learning recurrent neural nets and problem solutions</article-title>. <source>International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</source> <volume>6</volume>, <fpage>107</fpage>–<lpage>30</lpage> 115 (<year>1998</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hochreiter</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Schmidhuber</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Long short-term memory</article-title>. <source>Neural Comput</source> <volume>9</volume>, <fpage>1735</fpage>–<lpage>1780</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Costa</surname>, <given-names>R.P.</given-names></string-name>, <string-name><surname>Assael</surname>, <given-names>Y.M.</given-names></string-name>, <string-name><surname>Shillingford</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>De Freitas</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Vogels</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Cortical microcircuits as gated-recurrent neural networks</article-title>. <source>Advances in Neural Information Processing Systems</source> (<year>2017</year>).</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shouval</surname>, <given-names>H.Z.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S.S.</given-names></string-name> &amp; <string-name><surname>Wittenberg</surname>, <given-names>G.M</given-names></string-name></person-group>. <article-title>Spike timing dependent plasticity: a consequence of more fundamental learning rules</article-title>. <source>Front Comput Neurosci</source> <volume>4</volume>, <issue>19</issue> (<year>2010</year>).</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gjorgjieva</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Audet</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Pfister</surname>, <given-names>J.P</given-names></string-name></person-group>. <article-title>A triplet spike-timing-dependent plasticity model generalizes the Bienenstock-Cooper-Munro rule to higher-order spatiotemporal correlations</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>108</volume>, <fpage>19383</fpage>–<lpage>19388</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poirazi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Brannon</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Mel</surname>, <given-names>B</given-names></string-name></person-group>. <article-title>Pyramidal neuron as two-layer neural network</article-title>. <source>Neuron</source> <volume>37</volume>, <fpage>989</fpage>–<lpage>999</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morita</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Possible role of dendritic compartmentalization in the spatial working memory circuit</article-title>. <source>J Neurosci</source> <volume>28</volume>, <fpage>7699</fpage>–<lpage>7724</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Körding</surname>, <given-names>K.P.</given-names></string-name> &amp; <string-name><surname>König</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Supervised and unsupervised learning with two sites of synaptic integration</article-title>. <source>J Comput Neurosci</source> <volume>11</volume>, <fpage>207</fpage>–<lpage>215</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Illing</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ventura</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bellec</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Local plasticity rules can learn deep representations using self-supervised contrastive predictions</article-title>. <source>Advances in Neural Information Processing Systems (NeurIPS 2021)</source> <volume>34</volume> (<year>2021</year>).</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Barto</surname>, <given-names>A.G.</given-names></string-name> &amp; <string-name><surname>Jordan</surname>, <given-names>M.I</given-names></string-name></person-group>. <article-title>Gradient Following Without Back-Propagation in Layered Networks</article-title>. <conf-name>Proceedings of the First Annual International Conference on Neural Networks</conf-name> Vol. <volume>II</volume> <fpage>629</fpage>–<lpage>636</lpage> <year>1987</year>.</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mazzoni</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Andersen</surname>, <given-names>R.A.</given-names></string-name> &amp; <string-name><surname>Jordan</surname>, <given-names>M.I</given-names></string-name></person-group>. <article-title>A more biologically plausible learning rule for neural networks</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>88</volume>, <fpage>4433</fpage>–<lpage>4437</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mazzoni</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Andersen</surname>, <given-names>R.A.</given-names></string-name> &amp; <string-name><surname>Jordan</surname>, <given-names>M.I</given-names></string-name></person-group>. <article-title>A more biologically plausible learning rule than backpropagation applied to a network model of cortical area 7a</article-title>. <source>Cereb Cortex</source> <volume>1</volume>, <fpage>293</fpage>–<lpage>307</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Max</surname>, <given-names>K.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Learning efficient backprojections across cortical hierarchies in real time</article-title>. <source>Nature Machine Intelligence</source> <volume>6</volume>, <fpage>619</fpage>–<lpage>630</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Faisal</surname>, <given-names>A.A.</given-names></string-name>, <string-name><surname>Selen</surname>, <given-names>L.P.</given-names></string-name> &amp; <string-name><surname>Wolpert</surname>, <given-names>D.M</given-names></string-name></person-group>. <article-title>Noise in the nervous system</article-title>. <source>Nat Rev Neurosci</source> <volume>9</volume>, <fpage>292</fpage>–<lpage>303</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Aihara</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Matsumoto</surname>, <given-names>G.</given-names></string-name> . in  (ed. <string-name><given-names>A.V.</given-names> <surname>Holden</surname></string-name></person-group><chapter-title>Chaotic oscillations and bifurcations in squid giant axons</chapter-title>. in <source>Chaos</source> (ed. ) (<publisher-name>Princeton University Press</publisher-name>, <year>1986</year>).</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Vreeswijk</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Sompolinsky</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science</source> <volume>274</volume>, <fpage>1724</fpage>–<lpage>1726</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Cornford</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Learning to live with Dale’s principle: ANNs with separate excitatory and inhibitory units</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2020.11.02.364968</pub-id> (<year>2021</year>).</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cornford</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ghosh</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Learning better with Dale’s Law: A Spectral Perspective</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.06.28.546924</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mastrogiuseppe</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Ostojic</surname>, <given-names>S.</given-names></string-name></person-group>.  <article-title>Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks</article-title>. <source>Neuron</source> <volume>99</volume>, <fpage>609</fpage>–<lpage>623.e629</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gerfen</surname>, <given-names>C.R.</given-names></string-name> &amp; <string-name><surname>Surmeier</surname>, <given-names>D.J</given-names></string-name></person-group>. <article-title>Modulation of Striatal Projection Systems by Dopamine</article-title>. <source>Annu Rev Neurosci</source> <volume>34</volume>, <fpage>441</fpage>–<lpage>466</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collins</surname>, <given-names>A.G.</given-names></string-name> &amp; <string-name><surname>Frank</surname>, <given-names>M.J</given-names></string-name></person-group>. <article-title>Opponent actor learning (OpAL): modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title>. <source>Psychol Rev</source> <volume>121</volume>, <fpage>337</fpage>–<lpage>366</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mikhael</surname>, <given-names>J.G.</given-names></string-name> &amp; <string-name><surname>Bogacz</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Learning Reward Uncertainty in the Basal Ganglia</article-title>. <source>PLoS Comput Biol</source> <volume>12</volume>, <fpage>e1005062</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lowet</surname>, <given-names>A.S.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>An opponent striatal circuit for distributional reinforcement learning</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2024.01.02.573966</pub-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wall</surname>, <given-names>N.R.</given-names></string-name>, <string-name><surname>De La Parra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Callaway</surname>, <given-names>E.M.</given-names></string-name> &amp; <string-name><surname>Kreitzer</surname>, <given-names>A.C</given-names></string-name></person-group>. <article-title>Differential innervation of direct-and indirect-pathway striatal projection neurons</article-title>. <source>Neuron</source> <volume>79</volume>, <fpage>347</fpage>–<lpage>360</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morita</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Differential cortical activation of the striatal direct and indirect pathway cells: reconciling the anatomical and optogenetic results by using a computational method</article-title>. <source>J Neurophysiol</source> <volume>112</volume>, <fpage>120</fpage>–<lpage>146</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hooks</surname>, <given-names>B.M.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Topographic precision in sensory and motor corticostriatal projections varies across cell type and cortical area</article-title>. <source>Nat Commun</source> <volume>9</volume>, <fpage>3549</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morita</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Im</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Kawaguchi</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Differential striatal axonal arborizations of the intratelencephalic and pyramidal-tract neurons: analysis of the data in the MouseLight database</article-title>. <source>Front Neural Circuits</source> <volume>13</volume>, <issue>71</issue> (<year>2019</year>).</mixed-citation></ref>
<ref id="c105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Distributed and Mixed Information in Monosynaptic Inputs to Dopamine Neurons</article-title>. <source>Neuron</source> <volume>91</volume>, <fpage>1374</fpage>–<lpage>1389</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c106"><label>106.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morita</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Kawaguchi</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>A Dual Role Hypothesis of the Cortico-Basal-Ganglia Pathways: Opponency and Temporal Difference Through Dopamine and Adenosine</article-title>. <source>Front Neural Circuits</source> <volume>12</volume>, <issue>111</issue> (<year>2019</year>).</mixed-citation></ref>
<ref id="c107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cone</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Shouval</surname>, <given-names>H.Z</given-names></string-name></person-group>. <article-title>Learning to express reward prediction error-like dopaminergic activity requires plastic representations of time</article-title>. <source>Nat Commun</source> <volume>15</volume>, <fpage>5856</fpage> (<year>2024</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104101.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work models reinforcement-learning experiments using a recurrent neural network. It examines if the detailed credit assignment necessary for back-propagation through time can be replaced with random feedback. In this <bold>useful</bold> study the authors show that it yields a satisfactory approximation but the evidence to support that it holds in general is <bold>incomplete</bold>. As only short temporal delays are used and the examples simulated are overly simple, the approximation would need to be tested on more complex task and with larger networks.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104101.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Can a plastic RNN serve as a basis function for learning to estimate value. In previous work this was shown to be the case, with a similar architecture to that proposed here. The learning rule in previous work was back-prop with an objective function that was the TD error function (delta) squared. Such a learning rule is non-local as the changes in weights within the RNN, and from inputs to the RNN depends on the weights from the RNN to the output, which estimates value. This is non-local, and in addition, these weights themselves change over learning. The main idea in this paper is to examine if replacing the values of these non-local changing weights, used for credit assignment, with random fixed weights can still produce similar results to those obtained with complete bp. This random feedback approach is motivated by a similar approach used for deep feed-forward neural networks.</p>
<p>This work shows that this random feedback in credit assignment performs well but is not as well as the precise gradient-based approach. When more constraints due to biological plausibility are imposed performance degrades. These results are not surprising given previous results on random feedback. This work is incomplete because the delay times used were only a few time steps, and it is not clear how well random feedback would operate with longer delays. Additionally, the examples simulated with a single cue and a single reward are overly simplistic and the field should move beyond these exceptionally simple examples.</p>
<p>Strengths:</p>
<p>• The authors show that random feedback can approximate well a model trained with detailed credit assignment.</p>
<p>
• The authors simulate several experiments including some with probabilistic reward schedules and show results similar to those obtained with detailed credit assignments as well as in experiments.</p>
<p>
• The paper examines the impact of more biologically realistic learning rules and the results are still quite similar to the detailed back-prop model.</p>
<p>Weaknesses:</p>
<p>• The authors also show that an untrained RNN does not perform as well as the trained RNN. However, they never explain what they mean by an untrained RNN. It should be clearly explained. These results are actually surprising. An untrained RNN with enough units and sufficiently large variance of recurrent weights can have a high-dimensionality and generate a complete or nearly complete basis, though not orthonormal (e.g: Rajan&amp;Abbott 2006). It should be possible to use such a basis to learn this simple classical conditioning paradigm. It would be useful to measure the dimensionality of network dynamics, in both trained and untrained RNN's.</p>
<p>• The impact of the article is limited by using a network with discrete time-steps, and only a small number of time steps from stimulus to reward. What is the length of each time step? If it's on the order of the membrane time constant, then a few time steps are only tens of ms. In the classical conditioning experiments typical delays are of the order to hundreds of milliseconds to seconds. Authors should test if random feedback weights work as well for larger time spans. This can be done by simply using a much larger number of time steps.</p>
<p>• In the section with more biologically constrained learning rules, while the output weights are restricted to only be positive (as well as the random feedback weights), the recurrent weights and weights from input to RNN are still bi-polar and can change signs during learning. Why is the constraint imposed only on the output weights? It seems reasonable that the whole setup will fail if the recurrent weights were only positive as in such a case most neurons will have very similar dynamics, and the network dimensionality would be very low. However, it is possible that only negative weights might work. It is unclear to me how to justify that bipolar weights that change sign are appropriate for the recurrent connections and inappropriate for the output connections. On the other hand, an RNN with excitatory and inhibitory neurons in which weight signs do not change could possibly work.</p>
<p>• Like most papers in the field this work assumes a world composed of a single cue. In the real world there many more cues than rewards, some cues are not associated with any rewards, and some are associated with other rewards or even punishments. In the simplest case, it would be useful to show that this network could actually work if there are additional distractor cues that appear at random either before the CS, or between the CS and US. There are good reasons to believe such distractor cues will be fatal for an untrained RNN, but might work with a trained RNN, either using BPPT or random feedback. Although this assumption is a common flaw in most work in the field, we should no longer ignore these slightly more realistic scenarios.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104101.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Tsurumi et al. show that recurrent neural networks can learn state and value representations in simple reinforcement learning tasks when trained with random feedback weights. The traditional method of learning for recurrent network in such tasks (backpropagation through time) requires feedback weights which are a transposed copy of the feed-forward weights, a biologically implausible assumption. This manuscript builds on previous work regarding &quot;random feedback alignment&quot; and &quot;value-RNNs&quot;, and extends them to a reinforcement learning context. The authors also demonstrate that certain non-negative constraints can enforce a &quot;loose alignment&quot; of feedback weights. The author's results suggest that random feedback may be a powerful tool of learning in biological networks, even in reinforcement learning tasks.</p>
<p>Strengths:</p>
<p>The authors describe well the issues regarding biologically plausible learning in recurrent networks and in reinforcement learning tasks. They take care to propose networks which might be implemented in biological systems and compare their proposed learning rules to those already existing in literature. Further, they use small networks on relatively simple tasks, which allows for easier intuition into the learning dynamics.</p>
<p>Weaknesses:</p>
<p>The principles discovered by the authors in these smaller networks are not applied to deeper networks or more complicated tasks, so it remains unclear to what degree these methods can scale up, or can be used more generally.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104101.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The paper studies learning rules in a simple sigmoidal recurrent neural network setting. The recurrent network has a single layer of 10 to 40 units. It is first confirmed that feedback alignment (FA) can learn a value function in this setting. Then so-called bio-plausible constraints are added: (1) when value weights (readout) is non-negative, (2) when the activity is non-negative (normal sigmoid rather than downscaled between -0.5 and 0.5), (3) when the feedback weights are non-negative, (4) when the learning rule is revised to be monotic: the weights are not downregulated. In the simple task considered all four biological features do not appear to impair totally the learning.</p>
<p>Strengths:</p>
<p>(1) The learning rules are implemented in a low-level fashion of the form: (pre-synaptic-activity) x (post-synaptic-activity) x feedback x RPE. Which is therefore interpretable in terms of measurable quantities in the wet-lab.</p>
<p>(2) I find that non-negative FA (FA with non negative c and w) is the most valuable theoretical insight of this paper: I understand why the alignment between w and c is automatically better at initialization.</p>
<p>(3) The task choice is relevant since it connects with experimental settings of reward conditioning with possible plasticity measurements.</p>
<p>Weaknesses:</p>
<p>(4) The task is rather easy, so it's not clear that it really captures the computational gap that exists with FA (gradient-like learning) and simpler learning rule like a delta rule: RPE x (pre-synpatic) x (post-synaptic). To control if the task is not too trivial, I suggest adding a control where the vector c is constant c_i=1.</p>
<p>(5) Related to point 3), the main strength of this paper is to draw potential connection with experimental data. It would be good to highlight more concretely the prediction of the theory for experimental findings. (Ideally, what should be observed with non-negative FA that is not expected with FA or a delta rule (constant global feedback) ?).</p>
<p>(6a) Random feedback with RNN in RL have been studied in the past, so it is maybe worth giving some insights how the results and the analyzes compare to this previous line of work (for instance in this paper [1]). For instance, I am not very surprised that FA also works for value prediction with TD error. It is also expected from the literature that the RL + RNN + FA setting would scale to tasks that are more complex than the conditioning problem proposed here, so is there a more specific take-home message about non-negative FA? or benefits from this simpler toy task?</p>
<p>
(6b) Related to task complexity, it is not clear to me if non-negative value and feedback weights would generally scale to harder tasks. If the task in so simple that a global RPE signal is sufficient to learn (see 4 and 5), then it could be good to extend the task to find a substantial gap between: global RPE, non-negative FA, FA, BP. For a well chosen task, I expect to see a performance gap between any pair of these four learning rules. In the context of the present paper, this would be particularly interesting to study the failure mode of non-negative FA and the cases where it does perform as well as FA.</p>
<p>(7) I find that the writing could be improved, it mostly feels more technical and difficult than it should. Here are some recommendations:</p>
<p>
(7a) for instance the technical description of the task (CSC) is not fully described and requires background knowledge from other paper which is not desirable.</p>
<p>
(7b) Also the rationale for the added difficulty with the stochastic reward and new state is not well explained.</p>
<p>
(7c) In the technical description of the results I find that the text dives into descriptive comments of the figures but high-level take home messages would be helpful to guide the reader. I got a bit lost, although I feel that there is probably a lot of depth in these paragraphs.</p>
<p>(8) Related to the writing issue and 5), I wished that &quot;bio-plausibility&quot; was not the only reason to study positive feedback and value weights. Is it possible to develop a bit more specifically what and why this positivity is interesting? Is there an expected finding with non-negative FA both in the model capability? or maybe there is a simpler and crisp take-home message to communicate the experimental predictions to the community would be useful?</p>
<p>(1) <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-020-17236-y">https://www.nature.com/articles/s41467-020-17236-y</ext-link></p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104101.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Tsurumi</surname>
<given-names>Takayuki</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kato</surname>
<given-names>Ayaka</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6306-6600</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Kumar</surname>
<given-names>Arvind</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8044-9195</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Morita</surname>
<given-names>Kenji</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2192-4248</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>Can a plastic RNN serve as a basis function for learning to estimate value. In previous work this was shown to be the case, with a similar architecture to that proposed here. The learning rule in previous work was back-prop with an objective function that was the TD error function (delta) squared. Such a learning rule is non-local as the changes in weights within the RNN, and from inputs to the RNN depends on the weights from the RNN to the output, which estimates value. This is non-local, and in addition, these weights themselves change over learning. The main idea in this paper is to examine if replacing the values of these non-local changing weights, used for credit assignment, with random fixed weights can still produce similar results to those obtained with complete bp. This random feedback approach is motivated by a similar approach used for deep feed-forward neural networks.</p>
<p>This work shows that this random feedback in credit assignment performs well but is not as well as the precise gradient-based approach. When more constraints due to biological plausibility are imposed performance degrades. These results are not surprising given previous results on random feedback. This work is incomplete because the delay times used were only a few time steps, and it is not clear how well random feedback would operate with longer delays. Additionally, the examples simulated with a single cue and a single reward are overly simplistic and the field should move beyond these exceptionally simple examples.</p>
<p>Strengths:</p>
<p>• The authors show that random feedback can approximate well a model trained with detailed credit assignment.</p>
<p>• The authors simulate several experiments including some with probabilistic reward schedules and show results similar to those obtained with detailed credit assignments as well as in experiments.</p>
<p>• The paper examines the impact of more biologically realistic learning rules and the results are still quite similar to the detailed back-prop model.</p>
<p>Weaknesses:</p>
<p>• The authors also show that an untrained RNN does not perform as well as the trained RNN. However, they never explain what they mean by an untrained RNN. It should be clearly explained. These results are actually surprising. An untrained RNN with enough units and sufficiently large variance of recurrent weights can have a high-dimensionality and generate a complete or nearly complete basis, though not orthonormal (e.g: Rajan&amp;Abbott 2006). It should be possible to use such a basis to learn this simple classical conditioning paradigm. It would be useful to measure the dimensionality of network dynamics, in both trained and untrained RNN's.</p>
</disp-quote>
<p>Thank you for pointing out the lack of explanation about untrained RNN. Untrained RNN in our simulations (except Fig. 6D/6E-gray-dotted) was randomly initialized RNN (i.e., connection weights were drawn from a pseudo normal distribution) that was used as initial RNN for training of value-RNNs. As you suggested, the performance of untrained RNN indeed improved as the number of units increased (Fig. 2J), and its highest part was almost comparable to the highest performance of trained value-RNNs (Fig. 2I). In the revision we will show the dimensionality of network dynamics (as you have suggested), and eigenvalue spectrum of the network.</p>
<disp-quote content-type="editor-comment">
<p>• The impact of the article is limited by using a network with discrete time-steps, and only a small number of time steps from stimulus to reward. What is the length of each time step? If it's on the order of the membrane time constant, then a few time steps are only tens of ms. In the classical conditioning experiments typical delays are of the order to hundreds of milliseconds to seconds. Authors should test if random feedback weights work as well for larger time spans. This can be done by simply using a much larger number of time steps.</p>
</disp-quote>
<p>Thank you for pointing out this important issue, for which our explanation was lacking and our examination was insufficient. We do not consider that single time step in our models corresponds to the neuronal membrane time constant. Rather, for the following reasons, we assume that the time step corresponds to several hundreds of milliseconds:</p>
<p>- We assume that single RNN unit corresponds to a small neuron population that intrinsically (for genetic/developmental reasons) share inputs/outputs and are mutually connected via excitatory collaterals.</p>
<p>- Cortical activity is suggested to be sustained not only by fast synaptic transmission and spiking but also, even predominantly, by slower synaptic neurochemical dynamics (Mongillo et al., 2008, Science &quot;Synaptic Theory of Working Memory&quot; <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/10.1126/science.1150769">https://www.science.org/doi/10.1126/science.1150769</ext-link>).</p>
<p>- In line with such theoretical suggestion, previous research examining excitatory interactions between pyramidal cells, to which one of us (the corresponding author Morita) contributed by conducting model fitting (Morishima, Morita, Kubota, Kawaguchi, 2011, J Neurosci, <ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/31/28/10380">https://www.jneurosci.org/content/31/28/10380</ext-link>), showed that mean recovery time constant from facilitation for recurrent excitation among one of the two types of cortico-striatal pyramidal cells was around 500 milliseconds.</p>
<p>If single time step corresponds to 500 milliseconds, three time steps from cue to reward in our simulations correspond to 1.5 sec, which matches the delay in the conditioning task used in Schultz et al. 1997 Science. Nevertheless, as you pointed out, it is necessary to examine whether our random feedback models can work for longer delays, and we will examine it in our revision.</p>
<disp-quote content-type="editor-comment">
<p>• In the section with more biologically constrained learning rules, while the output weights are restricted to only be positive (as well as the random feedback weights), the recurrent weights and weights from input to RNN are still bi-polar and can change signs during learning. Why is the constraint imposed only on the output weights? It seems reasonable that the whole setup will fail if the recurrent weights were only positive as in such a case most neurons will have very similar dynamics, and the network dimensionality would be very low. However, it is possible that only negative weights might work. It is unclear to me how to justify that bipolar weights that change sign are appropriate for the recurrent connections and inappropriate for the output connections. On the other hand, an RNN with excitatory and inhibitory neurons in which weight signs do not change could possibly work.</p>
</disp-quote>
<p>Our explanation and examination about this issue were insufficient, and thank you for pointing it out and giving us helpful suggestion. In the Discussion (Line 507-510) of the original manuscript, we described &quot;Regarding the connectivity, in our models, recurrent/feed-forward connections could take both positive and negative values. This could be justified because there are both excitatory and inhibitory connections in the cortex and the net connection sign between two units can be positive or negative depending on whether excitation or inhibition exceeds the other.&quot; However, we admit that the meaning of this description was not clear, and more explicit modeling will be necessary as you suggested.</p>
<p>Therefore in our revision, we will examine models, in which inhibitory units (modeling fast-spiking (FS) GABAergic cells) will be incorporated, and neuron will follow Dale’s law.</p>
<disp-quote content-type="editor-comment">
<p>• Like most papers in the field this work assumes a world composed of a single cue. In the real world there many more cues than rewards, some cues are not associated with any rewards, and some are associated with other rewards or even punishments. In the simplest case, it would be useful to show that this network could actually work if there are additional distractor cues that appear at random either before the CS, or between the CS and US. There are good reasons to believe such distractor cues will be fatal for an untrained RNN, but might work with a trained RNN, either using BPPT or random feedback. Although this assumption is a common flaw in most work in the field, we should no longer ignore these slightly more realistic scenarios.</p>
</disp-quote>
<p>Thank you very much for this insightful comment. In our revision, we will examine situations where there exist not only reward-associated cue but also randomly appeared distractor cues.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>Tsurumi et al. show that recurrent neural networks can learn state and value representations in simple reinforcement learning tasks when trained with random feedback weights. The traditional method of learning for recurrent network in such tasks (backpropagation through time) requires feedback weights which are a transposed copy of the feed-forward weights, a biologically implausible assumption. This manuscript builds on previous work regarding &quot;random feedback alignment&quot; and &quot;value-RNNs&quot;, and extends them to a reinforcement learning context. The authors also demonstrate that certain non-negative constraints can enforce a &quot;loose alignment&quot; of feedback weights. The author's results suggest that random feedback may be a powerful tool of learning in biological networks, even in reinforcement learning tasks.</p>
<p>Strengths:</p>
<p>The authors describe well the issues regarding biologically plausible learning in recurrent networks and in reinforcement learning tasks. They take care to propose networks which might be implemented in biological systems and compare their proposed learning rules to those already existing in literature. Further, they use small networks on relatively simple tasks, which allows for easier intuition into the learning dynamics.</p>
<p>Weaknesses:</p>
<p>The principles discovered by the authors in these smaller networks are not applied to deeper networks or more complicated tasks, so it remains unclear to what degree these methods can scale up, or can be used more generally.</p>
</disp-quote>
<p>In our revision, we will examine more biologically realistic models with excitatory and inhibitory units, as well as more complicated tasks with distractor cues. We will also consider whether/how the depth of networks can be increased, though we do not currently have concrete idea on this last point. Thank you also for giving us the detailed insightful 'recommendations for authors'. We will address also them in our revision.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>Summary:</p>
<p>The paper studies learning rules in a simple sigmoidal recurrent neural network setting. The recurrent network has a single layer of 10 to 40 units. It is first confirmed that feedback alignment (FA) can learn a value function in this setting. Then so-called bio-plausible constraints are added: (1) when value weights (readout) is non-negative, (2) when the activity is non-negative (normal sigmoid rather than downscaled between -0.5 and 0.5), (3) when the feedback weights are non-negative, (4) when the learning rule is revised to be monotic: the weights are not downregulated. In the simple task considered all four biological features do not appear to impair totally the learning.</p>
<p>Strengths:</p>
<p>(1) The learning rules are implemented in a low-level fashion of the form: (pre-synaptic-activity) x (post-synaptic-activity) x feedback x RPE. Which is therefore interpretable in terms of measurable quantities in the wet-lab.</p>
<p>(2) I find that non-negative FA (FA with non negative c and w) is the most valuable theoretical insight of this paper: I understand why the alignment between w and c is automatically better at initialization.</p>
<p>(3) The task choice is relevant since it connects with experimental settings of reward conditioning with possible plasticity measurements.</p>
<p>Weaknesses:</p>
<p>(4) The task is rather easy, so it's not clear that it really captures the computational gap that exists with FA (gradient-like learning) and simpler learning rule like a delta rule: RPE x (pre-synpatic) x (post-synaptic). To control if the task is not too trivial, I suggest adding a control where the vector c is constant c_i=1.</p>
</disp-quote>
<p>Thank you for this insightful comment. We have realized that this is actually an issue that would need multilateral considerations. A previous study of one of us (Wärnberg &amp; Kumar, 2023 PNAS) assumed that DA represents a vector error rather than a scalar RPE, and thus homogeneous DA was considered as negative control because it cannot represent vector error other than the direction of (1, 1, .., 1). In contrast, the present work assumed that DA represents a scalar RPE, and then homogeneous DA (i.e., constant feedback) would not be said as a failure mode because it can actually represent a scalar RPE and FA to the direction of (1, 1, .., 1) should in fact occur. And this FA to (1, 1, ..., 1) may actually be interesting because it means that if heterogeneity of DA inputs is not large and the feedback is not far from (1, 1, ..., 1), states are learned to be represented in such a way that simple summation of cortical neuronal activity approximates value, thereby potentially explaining why value is often correlated with regional activation (fMRI BOLD signal) of not only striatal but also cortical regions (which I have been considering as an unresolved mystery). But on the other hand, the case with constant feedback is the same as the simple delta rule, as you pointed out, and then what could be obtained from the present analyses would be that FA is actually occurring behind the successful operation of such a simple rule. Anyway we will make further examinations and considerations on this issue.</p>
<disp-quote content-type="editor-comment">
<p>(5) Related to point 3), the main strength of this paper is to draw potential connection with experimental data. It would be good to highlight more concretely the prediction of the theory for experimental findings. (Ideally, what should be observed with non-negative FA that is not expected with FA or a delta rule (constant global feedback) ?).</p>
</disp-quote>
<p>In response to this insightful comment, we considered concrete predictions of our models. In the FA model, the feedback vector c and the value-weight vector w are initially at random (on average orthogonal) relationships and become gradually aligned, whereas in the non-negative model, the vectors c and w are loosely aligned from the beginning. We considered how the vectors c and w can be experimentally measured. Each element of the feedback vector c is multiplied with TD-RPE, modulating the degree of update in each pyramidal cell (more accurately, pyramidal cell population that corresponds to single RNN unit). Thus each element of c could be measured as the magnitude of response of each pyramidal cell to DA stimulation. The element of the value-weight vector w corresponding to a given pyramidal cell could be measured, if striatal neuron that receives input from that pyramidal cell can be identified (although technically demanding), as the magnitude of response of the striatal neuron to activation of the pyramidal cell.</p>
<p>Then, the abovementioned predictions can be tested by (i) identify cortical, striatal, and VTA regions that are connected by meso-cortico-limbic pathway and cortico-striatal-VTA pathway, (ii) identify pairs of cortical pyramidal cells and striatal neurons that are connected, (iii) measure the responses of identified pyramidal cells to DA stimulation, as well as the responses of identified striatal neurons to activation of the connected pyramidal cells, and (iv) test whether the DA-&gt;pyramidal responses and the pyramidal-&gt;striatal responses are associated across pyramidal cells, and whether such associations develop through learning. We will elaborate this tentative idea, and also other ideas, in our revision.</p>
<disp-quote content-type="editor-comment">
<p>(6a) Random feedback with RNN in RL have been studied in the past, so it is maybe worth giving some insights how the results and the analyzes compare to this previous line of work (for instance in this paper [<ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-020-17236-y]">https://www.nature.com/articles/s41467-020-17236-y]</ext-link>). For instance, I am not very surprised that FA also works for value prediction with TD error. It is also expected from the literature that the RL + RNN + FA setting would scale to tasks that are more complex than the conditioning problem proposed here, so is there a more specific take-home message about non-negative FA? or benefits from this simpler toy task?</p>
</disp-quote>
<p>In reply to this suggestion, we will explore how our results compare to the previous studies including the paper [<ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-020-17236-y">https://www.nature.com/articles/s41467-020-17236-y</ext-link>], and explore benefits of our models. At preset, we think of one possible direction. According to our results (Fig. 6E), under the non-negativity constraint, the model with random feedback and monotonic plasticity rule (bioVRNNrf) performed better, on average, than the model with backprop and non-monotonic plasticity rule (revVRNNbp) when the number of units was large, though the difference in the performance was not drastic. We will explore reasons for this, and examine if this also applies to cases with more realistic models, e.g., having separate excitatory and inhibitory units (as suggested by other reviewer).</p>
<disp-quote content-type="editor-comment">
<p>(6b) Related to task complexity, it is not clear to me if non-negative value and feedback weights would generally scale to harder tasks. If the task in so simple that a global RPE signal is sufficient to learn (see 4 and 5), then it could be good to extend the task to find a substantial gap between: global RPE, non-negative FA, FA, BP. For a well chosen task, I expect to see a performance gap between any pair of these four learning rules. In the context of the present paper, this would be particularly interesting to study the failure mode of non-negative FA and the cases where it does perform as well as FA.</p>
</disp-quote>
<p>In reply to this comment and also other reviewer's comment, we will examine the performance of the different models in more complex tasks, e.g., having distractor cues or longer delays. We will also see whether or not the better performance of bioVRNNrf than revVRNNbp mentioned in the previous point applies to the different tasks.</p>
<disp-quote content-type="editor-comment">
<p>(7) I find that the writing could be improved, it mostly feels more technical and difficult than it should. Here are some recommendations:</p>
<p>(7a) for instance the technical description of the task (CSC) is not fully described and requires background knowledge from other paper which is not desirable.</p>
<p>(7b) Also the rationale for the added difficulty with the stochastic reward and new state is not well explained.</p>
<p>(7c) In the technical description of the results I find that the text dives into descriptive comments of the figures but high-level take home messages would be helpful to guide the reader. I got a bit lost, although I feel that there is probably a lot of depth in these paragraphs.</p>
</disp-quote>
<p>Thank you for your helpful suggestions. We will thoroughly revise our writings.</p>
<disp-quote content-type="editor-comment">
<p>(8) Related to the writing issue and 5), I wished that &quot;bio-plausibility&quot; was not the only reason to study positive feedback and value weights. Is it possible to develop a bit more specifically what and why this positivity is interesting? Is there an expected finding with non-negative FA both in the model capability? or maybe there is a simpler and crisp take-home message to communicate the experimental predictions to the community would be useful?</p>
</disp-quote>
<p>We will make considerations on whether/how the non-negative constraints could have any benefits other than biological plausibility, in particular, in theoretical aspects or applications using neuro-morphic hardware, while we will also elaborate the links to biology and concretize the model's predictions.</p>
</body>
</sub-article>
</article>