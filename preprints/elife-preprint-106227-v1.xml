<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106227</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106227</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106227.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Developmental Biology</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>MorphoNet 2.0: An innovative approach for qualitative assessment and segmentation curation of large-scale 3D time-lapse imaging datasets</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Gallean</surname>
<given-names>Benjamin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Laurent</surname>
<given-names>Tao</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Biasuz</surname>
<given-names>Kilian</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Clement</surname>
<given-names>Ange</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Faraj</surname>
<given-names>Noura</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4925-2009</contrib-id>
<name>
<surname>Lemaire</surname>
<given-names>Patrick</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2787-0885</contrib-id>
<name>
<surname>Faure</surname>
<given-names>Emmanuel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>emmanuel.faure@lirmm.fr</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013yean28</institution-id><institution>Laboratoire d’informatique, de robotique et de microélectronique de Montpellier, LIRMM, Université de Montpellier, CNRS</institution></institution-wrap>, <city>Montpellier</city>, <country country="FR">France</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01xpc6869</institution-id><institution>Centre de Recherche de Biologie cellulaire de Montpellier, CRBM, Université de Montpellier, CNRS</institution></institution-wrap>, <city>Montpellier</city>, <country country="FR">France</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/051escj72</institution-id><institution>Montpellier Ressources Imagerie, Biocampus, Université de Montpellier, CNRS, INSERM</institution></institution-wrap>, <city>Montpellier</city>, <country country="FR">France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Graña</surname>
<given-names>Martin</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Institut Pasteur de Montevideo</institution>
</institution-wrap>
<city>Montevideo</city>
<country>Uruguay</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Walczak</surname>
<given-names>Aleksandra M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>CNRS</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors contributed equally to this work.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-05-12">
<day>12</day>
<month>05</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106227</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-02-21">
<day>21</day>
<month>02</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-03-01">
<day>01</day>
<month>03</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.02.21.639560"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Gallean et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Gallean et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106227-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Thanks to recent promising advances in AI, automated segmentation of imaging datasets has made significant strides. However, the evaluation and curation of 3D and 3D+t datasets remain extremely challenging and highly resource-intensive. We present MorphoNet 2.0, a major conceptual and technical evolution in order to easily perform segmentation, self-evaluation and correction of 3D. The application is accessible to non-programming biologists through user-friendly graphical interfaces and works on all major operating systems. We showcase its power in enhancing segmentation accuracy and boosting interpretability across five previously published segmented datasets. This new approach is crucial for producing ground-truth datasets of discovery-level scientific quality, critical for training and benchmarking advanced AI-driven segmentation tools, as well as for competitive challenges.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Recent advances in optical time-lapse microscopy now enable the 3D capture of life’s temporal dynamics, from molecular assemblies to entire organisms<sup><xref ref-type="bibr" rid="c1">1</xref></sup>, in cells, embryos, and aggregates. These technologies have thus become crucial for cell and developmental biology. The automated segmentation and tracking of these complex datasets is, however, necessary for their interpretation and quantification<sup><xref ref-type="bibr" rid="c1">1</xref></sup>.</p>
<p>Recent progress in the 3D segmentation of animal and plant cells<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, or organelles<sup><xref ref-type="bibr" rid="c3">3</xref></sup> has made this process highly efficient, despite the persistence of algorithm-dependent systematic errors<sup><xref ref-type="bibr" rid="c4">4</xref></sup>. The processing of time-lapse datasets, however, remains challenging. The accumulation over hundreds of time points of even few residual segmentation errors can strongly affect data interpretation and notably disrupt long-term cell tracking. Following the excitement generated by this new generation of imaging systems, which offers the ability to image living systems at unprecedented spatial and temporal scales, a glass ceiling has now been reached due to the quality of these reconstructed data, which fails to scale to real research exploitations.</p>
<p>Beyond the reconstruction requirements of individual research projects, proper metrics are essential for evaluating the performance of bioimage analysis algorithms<sup><xref ref-type="bibr" rid="c5">5</xref></sup>. However, the training and benchmarking of next-generation AI-based segmentation tools fundamentally depend on the availability of accurate ground truth data.<sup><xref ref-type="bibr" rid="c6">6</xref></sup>. The critical bottleneck is the lack of a massive amount of 3D high-quality reconstructed data to train these systems.</p>
<p>We have decided to take on this challenge and develop a tool to produce high-precision and high-quality 3D datasets for AI. Two major barriers must be overcome to reach this milestone. The first concerns the <bold>evaluation</bold> of dataset reconstructions: how can we measure the accuracy of segmentations obtained by automated algorithms for these complex 3D and 3D+t datasets? The second focuses on the <bold>curation</bold> of reconstructed data: how can we correct and significantly enhance segmentations to achieve reliable reconstructions that are essential for major scientific breakthroughs?</p>
<p>2D imaging has long been a cornerstone of biological research, driving significant scientific discoveries with visual validation being largely adequate, as it could be performed directly on a 2D screen. The traditional method involves visually comparing the reconstruction by overlaying it with the acquired data, as seen in classic software like ImageJ<sup><xref ref-type="bibr" rid="c7">7</xref></sup>, Napari<sup><xref ref-type="bibr" rid="c8">8</xref></sup> or Ilastik<sup><xref ref-type="bibr" rid="c9">9</xref></sup>. When discrepancies arise, experts can perform curation and manually adjust the pixel values to align with the desired class (e.g., a cell).</p>
<p>In the context of 3D and 3D+t data, the technique encounters significant obstacles that hinder reliable scientific analysis. The 3D structure of images, inherently unsuited to the 2D digital environment, demands numerous compromises that restrict their interpretability. Projecting 3D objects onto a 2D screen causes data masking, requiring experts to perform extensive manipulations to verify accuracy. Additionally, the high number and density of 3D objects further complicate the process, as visual interference from overlapping objects increases during these adjustments. As a result, voxel-level (3D pixel) curation becomes almost unfeasible using conventional methods, compelling experts to revert to working in 2D. This approach becomes very time-consuming and therefore does not allow to obtain a satisfactory reconstruction quality for scientific use. A final obstacle is caused by the much larger size of 3D datasets, which significantly lengthens processing time for each curation action. This computational burden becomes daunting for 3D + t time-lapse datasets, as error propagation between consecutive time points rapidly increases the number of segmentation and object tracking errors needing correction. Restricting the application of advanced image processing tasks to subsets of the image may alleviate this issue.</p>
<p>An unsupervised objective assessment can be derived from an <italic>a priori</italic> knowledge of expected spatial (e.g. smoothness of contours, shape regularity, volume…) or temporal (e.g. stability and smooth evolution, lifetime of objects…) features of the objects<sup><xref ref-type="bibr" rid="c10">10</xref></sup>. Features of the object boundaries (e.g. contrast between inside and outside of an object) can also be computed<sup><xref ref-type="bibr" rid="c10">10</xref></sup>. More sophisticated strategies have been proposed when no statistically-relevant a priori knowledge can be drawn<sup><xref ref-type="bibr" rid="c11">11</xref></sup>. Since curation is typically carried out by expert biologists with limited programming skills, the computation of image features and the projection of relevant metrics onto individual segmented objects should be accessible through user-friendly interfaces. An alternative approach for interacting with 3D segmented images is therefore necessary.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>We previously developed a novel concept of morphogenetic web browser, MorphoNet<sup><xref ref-type="bibr" rid="c12">12</xref></sup>, to <bold>visualize</bold> and <bold>interact</bold> with 3D+t segmented datasets through their meshed representations. This user-friendly web solution required no installation and was suited for datasets of moderate size (up to a thousand cells over a few hundred time points). Since its release, this platform has been used in a variety of morphological studies in plant and animal systems<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup> and has been instrumental to interpret the relationship between gene expression and complex evolving cell shapes<sup><xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c17">17</xref></sup>. This web version was restricted to visualizing and interacting with segmented datasets through their precomputed 3D dual meshes. This approach offered significant benefits in terms of online 3D rendering efficiency and data management but it also came with limitations. For example meshes are simplified representations of the segmented object that may not capture fine details of the original volumetric data and that cannot be validated without comparison to the original raw intensity images. Meshed representations can also make it challenging to perform the precise computations required for detailed geometric and topological analyses<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. Exploration of larger datasets was nonetheless constrained by the computational resource limitations of internet browsers.</p>
<p>We present here MorphoNet 2.0, a major conceptual evolution of the platform, designed to offer powerful tools for the <bold>reconstruction</bold>, <bold>evaluation</bold>, and <bold>curation</bold> of 3D and 3D+t datasets. This innovative approach leverages the richness and redundancy of information embedded within these complex datasets. It addresses the two previously identified challenges: <bold>evaluating</bold> 3D segmented data without relying on manual ground truth by fully harnessing the data’s richness, and enabling semi-automated <bold>curation</bold> of 3D data, now achievable with just a few clicks. This enables us to achieve data quality at a level suitable for scientific discovery. We showcase the impact of these advancements by revisiting and enhancing five published animal and plant datasets previously regarded as ground truth<sup><xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<p>We feature a new standalone application running on all major operating systems that exploits the resources of the user’s local machine to explore, segment, assess and manually curate very large 3D and 3D+t datasets. Like the web version, the MorphoNet application uses the power and versatility of the Unity game engine and includes its main features<sup><xref ref-type="bibr" rid="c12">12</xref></sup>. By overcoming web-based limitations, the application can handle more complex datasets including heavier segmented voxel images and up to several tens of thousands of objects on a research laptop (<xref rid="fig1" ref-type="fig">Fig. 1h</xref>, and Supp. Table 1). The standalone application allows users to directly explore private data stored on their own computer, without need for an upload to the MorphoNet server. Subsequent data sharing with other researchers or a wider public in an open science process is facilitated by the MorphoNet server upload functions of the standalone.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Illustration of the visualization of datasets of various complexity and nature in the MorphoNet standalone application</title>
<p>a-f. Visualization of a 64-cell stage Phallusia mammillata embryo with labeled cell nuclei and cell membranes. a-c intensity images showing the nuclei (a), the membranes (b) or both (c). d. same as c. with additional nuclei segmentation obtained with the Binarize plugin (see Supp. Mat. for the full description of curation plugins). e. same as b. With additional membrane segmentation obtained with the Cellpose 3D plugin. f. same as c. with a combination of several rendering possibilities of cell and nuclei segmentations.</p><p>g. Multi-colored shaders allow the simultaneous visualization of the expression patterns of multiple genes extracted from the ANISEED<sup><xref ref-type="bibr" rid="c17">17</xref></sup> database and of tissue fate information. Ascidian embryo<sup><xref ref-type="bibr" rid="c16">16</xref></sup> at stage 15 (mid neurula); cells with a single color are coloured with larval tissue fate; multi-coloured cells are coloured with both larval tissue fate and the expression of selected genes.</p><p>h. Visualization of a 6 days post-fertilization Platynereis dumerilii embryo<sup><xref ref-type="bibr" rid="c20">20</xref></sup> imaged by whole-body serial block face scanning electron microscopy followed by the automated whole-cell segmentation of 16000 cells.</p><p>i-k. Visualization of a cell cycle 14 Drosophila melanogaster embryo imaged with SiMView microscopy and segmented with RACE<sup><xref ref-type="bibr" rid="c21">21</xref></sup>. i. Projection on each segmented cell of the mean image intensity. j. Projection on each segmented cell of the ratio between the length of the major and the minor axes of the ellipse that has the same normalized second central moments as the segmented cell. k. projection of the cell volume.</p></caption>
<graphic xlink:href="639560v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>MorphoNet is designed with a strong emphasis on user-friendliness, making its use highly intuitive for experimental biologists with no coding experience (See Supp. Movies). Every feature and interface element has been crafted to ensure a smooth, straightforward user experience, allowing both beginners and experts to utilize its capabilities efficiently. Additionally, the solution is fully open-source allowing bio-image analysis to develop their own features (See Supp. Table 2).</p>
<p>One of the key features of this new standalone application is the automatic integration of a duality between the 3D segmented images and their corresponding meshes (<xref rid="fig2" ref-type="fig">Fig 2</xref>.). This feature is achieved by linking Python, dedicated to high-performance image processing, with the Unity Game Engine, fully optimized for seamless interaction with the dual meshes. This integration enables access to state-of-the-art reconstruction<sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup> tools, including those leveraging AI libraries, while also addressing the challenges of interacting with 3D images by harnessing the powerful interactive capabilities of game engines.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>MorphoNet Standalone Schema:</title>
<p>From the local data (loaded from the green box), the MorphoNet Standalone application computes first the dual meshes for each of the segmented objects (in the module python in the yellow box). Then, using the 3D viewer (in the blue box), users identify detection, segmentation or tracking issues using, if necessary, the cell lineage information, the raw images and/or properties computed on the segmented objects. Errors are then corrected by choosing and executing the appropriate image processing plugin from the curation menu. Finally, new meshes are computed from the result of the plugin execution to update the visualization.</p></caption>
<graphic xlink:href="639560v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To ensure users can consistently assess segmentation quality, raw intensity images are also available by superimposition. This provides access to the entire workflow, from raw image acquisition to meshed segmentation, allowing simultaneous exploration and visualization of both meshed and voxel images (<xref rid="fig1" ref-type="fig">Fig. 1a-f</xref>).</p>
<p>We also leveraged Unity’s scene management tools to implement simultaneous visualization of multiple scenes. This feature enables the display of interactive cell lineages in a dedicated, fully connected window, offering essential insights into cellular trajectories within 3D+t datasets.</p>
<sec id="s2a">
<title>Automatic error detection</title>
<p>Efficient dataset curation requires the rapid identification of segmentation errors in large 3D or time-lapse datasets containing thousands of segmented objects. MorphoNet 2.0 tackles this critical challenge by introducing unsupervised metrics for objective, automated assessment, eliminating the subjectivity, inefficiency, and inconsistency inherent in manual visual inspections. These metrics leverage prior knowledge of generic data properties - such as shape regularity, contour smoothness, and temporal stability of shapes and volumes - to deliver scalable and consistent analysis. Beyond improving curation efficiency, these metrics offer quantitative scores that enable systematic comparisons across datasets, experiments, and tools. Integrated into MorphoNet, they support benchmarking, algorithm refinement, and reproducibility. Using the scikit-image library<sup><xref ref-type="bibr" rid="c22">22</xref></sup>, we automatically compute object properties from both segmented and intensity images. These properties quickly help identify outliers, which often correspond to segmentation errors. While the traditional approach uses some metrics to evaluate global distributions, it is crucial for curation purposes to apply these properties at the level of each individual segmented object. Thus, these properties, including shape metrics like volume, convexity, and elongation, and intensity metrics such as mean voxel intensity within or around segmented objects, can be easily projected onto meshed objects for visualization (<xref rid="fig1" ref-type="fig">Fig. 1i-k</xref>).</p>
<p>Segmentation quality assessment is further enhanced by calculating three categories of properties for each dataset: 1) Morphological features – including volume, convexity, roughness, and elongation, 2) Intensity-based measurements – within and around each object in the original acquisition images, such as mean intensity, homogeneity, or deviation at segmentation borders; 3) Temporal features – such as object lifetime or cell lineage distances<sup><xref ref-type="bibr" rid="c16">16</xref></sup>.</p>
<p>By projecting these values onto segmented object meshes, outliers are readily identified as prime candidates for curation. These values can also be exported and plotted to reveal distributions, providing an unsupervised assessment of overall segmentation quality. This process is exemplified in various use cases described below.</p>
</sec>
<sec id="s2b">
<title>Biocuration</title>
<p>The execution time of algorithms in 3D image processing limits the optimization of their parameters. Additionally, the inherent heterogeneity within 3D images frequently prevents achieving consistently high-quality results across the entire dataset. To tackle these challenges, we defined in Morphonet the notion of multi-type representation layers, able to combine and interoperate 3D image intensity layers with mesh-based layers. This dual representation enables seamless interaction with meshes, facilitating the rapid identification and precise selection of objects requiring curation. By focusing processing efforts on these selected regions rather than the entire image, MorphoNet significantly reduces the time and effort required, streamlining the curation workflow for complex 3D+t datasets.</p>
<p>The meshed versions of the segmentations are used for 3D rendering, for the identification and selection of objects needing curation, and to launch the needed curation algorithms, which are performed on the segmented images (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). A significant acceleration of operations on 3D images is achieved by targeting the processing activities on a subpart of the image, for instance on the bounding box of an object or of a set of objects. This approach enables results to be generated in just seconds for most tasks, even with highly complex 3D data. Given the heterogeneity within 3D images, it is challenging to find a single set of parameters that works effectively across the entire image. This approach makes it easy to run image processing algorithms with different parameters in different subregions of the image.</p>
<p>Editions of objects necessitating curation can then proceed using dedicated plugins. The curation module of MorphoNet has an expandable open-source Python plugin architecture and provides user-friendly graphical interfaces accessible to experimental biologists with limited programming skills. To support advanced image processing techniques powered by deep learning, which have an increasing impact on image analysis, the default installation package includes training libraries like Scikit-learn, PyTorch, and TensorFlow. Plugins can take the raw intensity image into account to perform 3D image processing tasks as exemplified by the integration of popular segmentation tools, such as Stardist<sup><xref ref-type="bibr" rid="c19">19</xref></sup> and Cellpose<sup><xref ref-type="bibr" rid="c2">2</xref></sup>. The full list of plugins can be found in the Supp. Mat.. Users and developers can expand the current plugin list by creating their own through an easy-to-use python template (See Supp. Table 2).</p>
<p>To help the user identify suitable plug-ins, we grouped them into seven categories. The first five are used to create or edit segmentations of 3D datasets, the last two are dedicated to the temporal propagation of corrections in 3D+t datasets. All categories will be exemplified in the five use cases below. Supp. Fig. 1. illustrates how the majority of segmentation and tracking errors can be corrected using plugins, single handedly or in combination. For example, an under-segmentation can be corrected by running locally the <italic>Cellpose</italic> plugin as in use case 2 or the Temporal Propagation plugin (<italic>Prope</italic>) as in use case 5.
<list list-type="simple">
<list-item><p>1- <italic>De novo Segmentation</italic> plugins : these plugins create a <italic>de novo</italic> segmentation from the original intensity images (or from a specified region of interest of the intensity images). This category includes segmentation tools such as Cellpose 3D or Stardist as well as simpler intensity thresholding. As described below, MorphoNet plugins allow to specifically target the application of these tools to error-rich regions.</p></list-item>
<list-item><p>2- <italic>De novo Seed</italic> plugins : these plugins automatically create seeds (i.e. 3D points in the 3D images), which can then be used to initiate segmentation tasks. Additionally, seeds can also be added manually using the interface.</p></list-item>
<list-item><p>3- <italic>Segmentation from Seeds</italic> plugins : these plugins are used to perform local segmentation using predefined seeds, for example by using a watershed algorithm.</p></list-item>
<list-item><p><italic>4</italic>- <italic>Segmentation correction</italic> plugins : these plugins are used to correct the main classes of segmentation errors on selected segmented objects. This includes the fusion of over-segmented cells, the resolution of small artifactual cells or the splitting of under-segmented cells.</p></list-item>
<list-item><p>5- <italic>Shape transform</italic> plugins : these plugins are used to edit the shape of selected segmented objects. This includes classical morphological operators (such as dilate, erode, etc.) or the manual deformation of a cell’s shape.</p></list-item>
<list-item><p>6- <italic>Propagate Segmentation</italic> plugins: these plugins are used to propagate an accurate segmentation at a specific time point to erroneous segmentations at later or earlier time points. Examples: Propagate eroded cell masks to correct under segmentation errors.</p></list-item>
<list-item><p><italic>7</italic>- <italic>Edit Temporal Links</italic> plugins: these plugins are used to curate cell lineage trees by creating or editing temporal information. Examples: Create temporal links using the spatial overlap between homologous objects at successive time points.</p></list-item>
</list>
</p>
</sec>
<sec id="s2c">
<title>Use cases</title>
<p>To highlight the efficiency of MorphoNet plugins in detecting and correcting the main types of errors across a broad range of organisms, we present five examples from previously published fluorescent live imaging datasets, representing various levels of quality and complexity. These examples collectively showcase MorphoNet’s powerful segmentation quality assessment and error detection capabilities in different contexts, ranging from low-quality nuclear segmentations to high-quality whole-cell segmentations. While these examples frequently involve the use of Cellpose, the current state-of-the-art tool for 3D dataset segmentation, the focus is not on emphasizing the tool’s power but on demonstrating how advanced tools can be enhanced through integration into MorphoNet’s sophisticated graphical plugin interfaces.</p>
<p>The first use-case introduces two unsupervised metrics for assessing dataset quality in the absence of ground truth on a time-lapse light-sheet microscopy movie of a developing <italic>Tribolium castaneum</italic> embryo. Using these metrics, we demonstrate the power of an iterative segmentation and training strategy. By applying a Cellpose model, trained on a curated subpopulation, to the entire dataset, we improve segmentation quality.</p>
<p>The second use-case presents unsupervised whole-cell morphological metrics to assess the quality of several independent segmentations of a time-lapse confocal movie of a developing starfish embryo, up to the 512-cell stage, with fluorescently labeled cell plasma membranes. Originally, a bio-image analyst segmented the movie using a complex workflow. We demonstrate that running an imported, custom-trained Cellpose 3D model through MorphoNet’s graphical interfaces greatly increases the number of cells that can be successfully segmented.</p>
<p>The third use-case focuses on detecting and resolving regions of under-segmented cells in a confocal 3D dataset of <italic>Arabidopsis thaliana</italic> plant apical shoot meristem, with fluorescent plasma membrane and whole-cell segmentation of 1800 cells. It demonstrates the identification of a large region with heavily under-segmented cells, which is successfully addressed by targeting Cellpose to the specific region of interest.</p>
<p>The fourth use-case addresses the correction of low-quality automated cell nuclei segmentation in a time-lapse confocal <italic>Caenorhabditis elegans</italic> embryo with poor-quality nuclei labeling up to the 350-cell stage, despite accurate manual tracking. It demonstrates how object edition plugins can be used to correct individual nuclei segmentations and propagate these corrections over time.</p>
<p>The final use-case illustrates the efficient detection and correction in the cell lineage of rare residual errors that escaped previous scrutiny in a published time-lapse multiview light-sheet imaging dataset of a developing <italic>Phallusia mammillata</italic> embryo. The dataset features labeled cell membranes and high-quality whole-cell segmentation and tracking. This polishing work is crucial for generating ground truths for studies of natural variation.</p>
<p>All the evaluation and curation steps for the use cases are detailed in the Supp. Mat.</p>
</sec>
<sec id="s2d">
<title>Use-case 1: Assessing and improving segmentation quality of <italic>Tribolium castaneum</italic> Embryo using unsupervised nuclei intensity metrics and iterative training on partial annotation</title>
<p>This use-case introduces two unsupervised metrics for assessing dataset quality without ground truth, demonstrating their application in evaluating segmentation performance. It also highlights the effectiveness of an iterative segmentation and training approach, where applying a Cellpose model trained on a curated subset significantly improves overall segmentation quality.</p>
<sec id="s2d1">
<title>Dataset description</title>
<p>This dataset features a 59-time-step light-sheet microscopy acquisition of labeled cell nuclei from a developing <italic>Tribolium castaneum</italic> embryo, containing over 7000 cells per time step. It includes two types of ground truth: a <italic>gold truth</italic> (GT), which is expert manual tracking of 181 nuclei over all time steps, and a <italic>silver truth</italic> (ST), generated through a complex pipeline combining the outputs of up to 16 top-performing algorithms selected from 39 submissions. The ST provides automated segmentation masks using a modified label fusion approach for improved accuracy in dense datasets. The GT and ST are independent and do not share common elements.</p>
</sec>
<sec id="s2d2">
<title>Type of errors</title>
<p>This dataset serves as a benchmark in the fifth Cell Tracking Challenge<sup><xref ref-type="bibr" rid="c6">6</xref></sup> to evaluate and compare AI-based image segmentation methods and is therefore regarded as a reliable ground truth. The manual tracking GT is of high quality (expert annotation), but the automated ST is of lesser quality. To evaluate the segmentation quality of the dataset at first time point, the intensity image and the two ground truths (ST and GT) were simultaneously displayed in separate MorphoNet channels (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). Systematic visual inspection identified 12/181 GT points without corresponding nucleus in the ST data; 8 misplaced segmentation where the GT point is not inside the segmentation; 56 over segmentations and many suspicious nuclear shapes.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Unsupervised quality assessment and curation of the <italic>Tribolium castaneum</italic> embryo</title>
<p>a. The 5 steps of the curation pipeline.</p><p>b. View of the original intensity images (in green) of the first time step of the published data<sup><xref ref-type="bibr" rid="c6">6</xref></sup>. Both Ground Truth channel (GT, red point) and Silver Truth (ST, white nucleus segmentation) are shown at the top. Blue segmentation corresponds to the match between ST and GT. Bottom, zoom at the GT region (ROI) without the intensity images.</p><p>c. Projection for the ST of the distance between the gravity center of the intensity inside the segmentation and the centroid of the segmentation. Color bar at the bottom of d.</p><p>d. Same as c. for the curated pipeline.</p><p>e. Projection for the ST of the deviation of the intensity at the border of the segmentation. Color bar at the bottom of f.</p><p>f. Same as e. for the curated pipeline.</p><p>g. Comparative histogram of the <italic>intensity_offset</italic> property distribution between the ST, the Step 1 and the Step 5 for the 181 curated nuclei (left) and the whole image (right).</p><p>h. Same as g for the distribution of the <italic>intensity_border_variation</italic> property.</p><p>i. Sams as g. For the distribution of the <italic>nuclei volume</italic> property.</p></caption>
<graphic xlink:href="639560v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2d3">
<title>Error identification</title>
<p>Using the <italic>Match</italic> plugin, each GT position was automatically associated with a corresponding nucleus in the ST, leaving 11 out of 181 positions unassigned (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>), indicating approximately 6% of missed nuclei in the ST. To identify inaccurately shaped segmentations, we used two signal intensity-based MorphoNet metrics. The first metric measures the distance between the geometric center of a segmented object and the center of mass of the signal intensity (Supp. Fig. 2 et Supp. Mat. for the full properties description), with 32 of the 56 over-segmented nuclei falling into the upper quartile of this metric’s distribution (<xref rid="fig3" ref-type="fig">Fig 3c-d</xref> and Supp. Fig. 3). The second metric evaluates deviations in the signal intensity along the segmentation boundary, with 29 out of 56 over-segmented nuclei identified in the upper quartile (<xref rid="fig3" ref-type="fig">Fig. 3e-f</xref> and Supp. Fig. 2). These errors were visualized in 3D by projecting both metrics onto each nucleus, enabling spatial identification of potential segmentation issues (Supp. Fig. 3).</p>
</sec>
<sec id="s2d4">
<title>Error correction</title>
<p>Due to the high number of errors in the published ST, a de novo nucleus segmentation pipeline was created in MorphoNet with minimal manual intervention. The five-step process included initial segmentation using the standard Cellpose nuclei model, manual curation of 181 GT nuclei, iterative training of a custom Cellpose model, and refinement of segmentation using geometric properties. Key steps involved correcting segmentation errors with various plugins, extending the Cellpose nuclei model using curated data, and refining non-convex shapes. The pipeline significantly improved segmentation quality compared to the published ST and standard Cellpose model (<xref rid="fig3" ref-type="fig">Fig. 3i</xref>). Analysis of the curated nuclei revealed a more heterogeneous volume distribution, fewer over-segmentations, and better alignment of intensity-based metrics (<xref rid="fig3" ref-type="fig">Fig. 3g-h</xref>), showing substantial improvement over the original segmentation pipeline.</p>
</sec>
</sec>
<sec id="s2e">
<title>Use case 2 : Evaluating membrane segmentation quality using <italic>smoothness</italic> metrics and simplifying a segmentation workflow for <italic>Patiria miniata</italic> starfish embryo</title>
<p>This use case thus introduces new unsupervised morphological metrics (the <italic>smoothness</italic> property) to assess segmentation quality and demonstrates that using a custom-trained 3D Cellpose model via graphical interfaces allows extending segmentation to areas of the dataset with lower intensity/noise ratio. Visualization of metrics such as smoothness and cell volumes simplifies the identification of cells that need further manual curation.</p>
<sec id="s2e1">
<title>Dataset description</title>
<p>This dataset comprises 300 3D stacks from time-lapse, two-channel confocal imaging of 12 <italic>Patiria miniata</italic> wild-type and compressed embryos, captured between the 128- and 512-cell stages<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. Fluorescent labeling highlights cell membranes and nuclei, though imaging was limited to about half of each embryo due to poor penetration and the use of low magnification air objectives. The published whole-cell segmentation relied on a complex, multi step workflow adapted from the CartoCel<sup><xref ref-type="bibr" rid="c24">24</xref></sup> requiring advanced bioanalysis expertise. For this use case, a particularly challenging dataset - a compressed wild-type embryo at the 512-cell stage - was selected for evaluation and curation.</p>
</sec>
<sec id="s2e2">
<title>MorphoNet segmentation with the advanced Cellpose plugin</title>
<p>We explored Cellpose’s potential to improve segmentation quality in challenging regions of a 3D dataset. Initial segmentation using the Cyto2 model identified 1187 cells, with a bimodal size distribution and 939 cells smaller than 1000 voxels (<xref rid="fig4" ref-type="fig">Fig. 4i</xref>). Using the Deli plugin (<xref rid="fig4" ref-type="fig">Fig. 4j</xref>) reduced the count to 211 cells; however, analysis with the <italic>smoothness</italic> property (<xref rid="fig4" ref-type="fig">Fig. 4g</xref>) revealed rough cell surfaces. To address this, we further extend the training of the <italic>Cyto2</italic> model on the published 3D database<sup><xref ref-type="bibr" rid="c23">23</xref></sup> (see Supp. Mat. Training Protocol). The retrained model produced 284 cells with again a bimodal size distribution (<xref rid="fig4" ref-type="fig">Fig. 4i</xref>) which was corrected with the <italic>Deli</italic> plugin. This much simpler procedure than the original segmentation pipeline, produced smooth cells (<xref rid="fig4" ref-type="fig">Fig. 4h</xref>), with a similar size distribution as the published ground truth. While 11 cells were missing or under-segmented compared to the published data (<xref rid="fig4" ref-type="fig">Fig. 4f</xref>), this new approach recovered 48 additional cells (<xref rid="fig4" ref-type="fig">Fig. 4e</xref>). while simplifying the segmentation process compared to the original pipeline.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Starfish whole-cell segmentation using Cellpose</title>
<p>a. Animal and Lateral view of the maximum intensity projection from the published dataset<sup><xref ref-type="bibr" rid="c23">23</xref></sup>.</p><p>b. Segmented ground truth image.</p><p>c. Result of the segmentation using Cellpose Cyto2 model followed by the removal of small cells (&lt;1000 voxels) with the Deli plugin</p><p>d. Result of a Cellpose segmentation with a model trained on P. miniata dataset followed by the Deli plugin.</p><p>e. New cells created (different between b and d)</p><p>f. Under segmentation and missing cells generated by d. compared to b.</p><p>g. Vegetal view of c with smoothness representation.</p><p>h. opposite view of d with smoothness representation.</p><p>i. Cell size distribution in the published segmentation (b), Cellpose cyto2 model (c) and Cellpose with P. miniata model (d).</p><p>j. Identical as i after application of the Deli plugin.</p><p>Colors in b-f, represent cell volume in µm<sup>3</sup>.</p><p>Colors in g,h represent Smoothness.</p></caption>
<graphic xlink:href="639560v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s2f">
<title>Use case 3: Interactive targeted segmentation using CellPose for resolving under-segmentation in <italic>Arabidopsis thaliana</italic> shoot apical meristem whole-cell segmentation</title>
<p>This use case demonstrates the power of interactive selection with targeted segmentation methods that can resolve under-segmentation issues in complex datasets.</p>
<sec id="s2f1">
<title>Dataset description</title>
<p>This dataset<sup><xref ref-type="bibr" rid="c25">25</xref></sup> consists of a 19-time steps time-lapse confocal acquisition of a live <italic>Arabidopsis thaliana</italic> shoot apical meristem with fluorescently-labeled cell membranes. Cell numbers ranged from 600 cells at the first time point to around 1800 cells at the last (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>). We use the last time step (19) with the higher number of cells to illustrate the curation procedure.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Curation of a segmented shoot apical meristem of <italic>Arabidopsis thaliana</italic>.</title>
<p>Visualization of the time step n°19 for several types of curation using MorphoNet.</p><p>a. 3D view of an Arabidopsis thaliana shoot apical meristem<sup><xref ref-type="bibr" rid="c25">25</xref></sup>.</p><p>b. Published 3D intensity images.</p><p>c. Published 3D segmentation of an Arabidopsis thaliana shoot apical meristem<sup><xref ref-type="bibr" rid="c25">25</xref></sup> obtained using MARS-ALT<sup><xref ref-type="bibr" rid="c26">26</xref></sup>.</p><p>d. Comparative histogram based on the cell volume between the published segmentation (c), the result of the cyto2 prediction (e) and the final curated version (i) + Deli plugin for cells &lt;1000 voxels. X and Y axes are in log scale.</p><p>e. Result of the Cellpose<sup><xref ref-type="bibr" rid="c28">28</xref></sup> 3D MorphoNet plugin using the pretrained cyto2 model.</p><p>f. Result using the Cellpose 3D MorphoNet plugin using the model trained over the 10 first time steps with the Cellpose training plugin with the XY planes.</p><p>g. Result of the Cellpose 3D MorphoNet plugin using the model trained over the 10 first time steps with the Cellpose training plugin with each plane of the 3D images.</p><p>h. Manually selected masks using morphonet on published data (c).</p><p>i. Result of the Cellpose 3D MorphoNet on the selected masks (h) using the model trained over the 10 first time steps with the Cellpose training plugin with each plane of the 3D images.</p></caption>
<graphic xlink:href="639560v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2f2">
<title>Error detection</title>
<p>The analysis of the published segmentation uncovered multiple under- and over-segmentation errors in the deep cell layers, caused by poor image quality in the inner regions of the meristem (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). These errors hindered accurate cell tracking over time. Observing the size homogeneity of shoot apical meristem cells (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>), we hypothesized that larger-than-expected cells might indicate under-segmentation errors, while smaller cells could result from over-segmentation. By projecting the automatically computed cell volumes as a color map onto the segmentations, we identified a large under-segmented region containing 20 cells in the deep meristem layer (highlighted in red in <xref rid="fig5" ref-type="fig">Fig. 5c</xref>), along with a few small over-segmented cells (<xref rid="fig5" ref-type="fig">Fig. 5i</xref>).</p>
</sec>
<sec id="s2f3">
<title>Error correction</title>
<p>The large number of fused cells in the under-segmented region prompted us to test whether our interactive Cellpose plugin could outperform the original MARS/ALT software<sup><xref ref-type="bibr" rid="c26">26</xref></sup>. The standard pretrained <italic>cyto2</italic> model of Cellpose<sup><xref ref-type="bibr" rid="c2">2</xref></sup> produced numerous very small over-segmented cells (<xref rid="fig5" ref-type="fig">Fig. 5d</xref> and Supp. Fig. 5), suggesting it was not well-suited for this dataset<sup><xref ref-type="bibr" rid="c27">27</xref></sup>. To improve performance, we used the high-quality MARS/ALT segmentations from the first ten time points and employed our Cellpose train plugin to extend the <italic>cyto2</italic> model training. We tested two training modalities: “2D” training, using only the XY planes, and “3D” training, incorporating XY, XZ, and YZ planes. The segmentations after additional 2D training still contained many over-segmented cells (<xref rid="fig5" ref-type="fig">Fig. 5f</xref>), a problem that was significantly reduced with 3D training (<xref rid="fig5" ref-type="fig">Fig. 5g</xref>). Despite this, the heterogeneity of signal intensities still caused over-segmentation in more superficial regions of the meristem. We thus targeted the Cellpose plugin only to the large under-segmented region (<xref rid="fig5" ref-type="fig">Fig. 5h</xref>). and removed small over-segmented cells using the <italic>Deli</italic> segmentation correction plugin (<xref rid="fig5" ref-type="fig">Fig. 5i</xref>). This targeted approach was fast and accurate (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>) and allowed us to preserve the accurate segmentations from the original published data while curating the identified errors. With this approach, we successfully curated 20 under-segmented regions and generated 98 new cells.</p>
</sec>
</sec>
<sec id="s2g">
<title>Use-case 4: Improving segmentation quality using object editing plugins for <italic>Caenorhabditis elegans</italic> cell lineage</title>
<p>This use case illustrates how object editing plugins can be employed to improve segmentation quality in challenging datasets despite poor-quality nuclei labeling.</p>
<sec id="s2g1">
<title>Dataset description</title>
<p>This dataset<sup><xref ref-type="bibr" rid="c29">29</xref></sup> consists of a live 3D time lapse confocal acquisition of a developing <italic>Caenorhabditis elegans</italic> embryo with fluorescently-labeled cell nuclei (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>), over 195 time steps between the 4- and 362-cell stages. This dataset was included in the Cell Tracking Challenge<sup><xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Curation of a <italic>Caenorhabditis elegans</italic> embryo dataset</title>
<p>a. Cell lineage viewer of all time steps colored by clonal cells of the published segmented dataset<sup><xref ref-type="bibr" rid="c29">29</xref></sup>.</p><p>b. Comparative histogram based on the axis ratio between the published data and the curation.</p><p>c. 3D View of the intensity images at t=150.</p><p>d. 3D View of the published segmented dataset at t=150. Colors represent the ratio of the longest axis on the shortest axis of the shape.</p><p>e. Automatic selection (in gray) of the nuclei with an axis ratio&gt;2.9.</p><p>f. Result of the Fusoc plugin applied on all selected nuclei. Colors represent the nuclei volume in µm<sup>3</sup>.</p><p>g. Same as f. But only previously selected nuclei are shown.</p><p>h. Result of the Gaumi plugin applied independently on regions fused with 4,3 or 2 nuclei. Colors represent the axis ratio as in d.</p></caption>
<graphic xlink:href="639560v1_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2g2">
<title>Error detection</title>
<p>Although the manually curated published cell lineage appeared accurate (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>), some touching nuclei were elongated with flat vertical contact interfaces, likely due to poor imaging quality. To identify these segmentation artifacts systematically, we calculated the axis ratio for each segmented object and projected the values onto the meshed dual of the object (<xref rid="fig6" ref-type="fig">Fig. 6b</xref>). The distribution revealed two peaks: a sharp one around 2, containing most nuclei, and a broader one above 3. Nuclei with an axis ratio greater than 2.9 were flagged for potential correction.</p>
</sec>
<sec id="s2g3">
<title>Error correction</title>
<p>We curated these errors for a specific time point by first applying a fusion correction plugin (<xref rid="fig6" ref-type="fig">Fig. 6f and g</xref>) to all selected nuclei pairs with an axis ratio greater than 2.9 (<xref rid="fig6" ref-type="fig">Fig. 6e</xref>). This was followed by a separation plugin (<xref rid="fig6" ref-type="fig">Fig. 6h</xref>) that divides fused objects into distinct nuclei using a Gaussian mixture applied to intensity values. We initially addressed the more complex cases manually, which included two groups of four 4 nuclei and one group of 3 fused nuclei. Next, we automatically curated the remaining 8 simpler cases of 2 fused nuclei by selecting nuclei with a volume greater than 80 µm³. As anticipated, the axis ratio property of the curated set showed a unimodal distribution, centered around 2 (<xref rid="fig6" ref-type="fig">Fig. 6b</xref>). The entire time step at t=150 was fully curated with just 20 plugin actions (<xref rid="fig6" ref-type="fig">Fig. 6h</xref>).</p>
</sec>
</sec>
<sec id="s2h">
<title>Use-case 5: Enhancing cell lineage accuracy by detecting segmentation errors in <italic>Phallusia mammillata</italic> embryos</title>
<p>This use case showcases how MorphoNet can efficiently detect and correct segmentation errors in complex 3D+t datasets of developing ascidian embryos, enhancing the accuracy of cell lineage reconstructions and analysis of cell division timing variations.</p>
<sec id="s2h1">
<title>Dataset description</title>
<p>This dataset<sup><xref ref-type="bibr" rid="c16">16</xref></sup> comprises 64 time steps of multi-view light-sheet microscopy of developing ascidian embryos (64–298-cell stages), segmented using the ASTEC algorithm<sup><xref ref-type="bibr" rid="c16">16</xref></sup>. Despite the high-quality segmentation and tracking, residual errors such as multi-component cells, over- and under-segmentations, delayed or missed divisions, and shape inaccuracies remained, with no tools available at the time of the publication to systematically validate or correct the 10,000 cell snapshots.</p>
</sec>
<sec id="s2h2">
<title>Error detection</title>
<p>MorphoNet was used to polish the cell lineage to a sufficient level to study natural variation in cell division timing. The main challenge was identifying rare errors within a cell lineage that appeared accurate by visual inspection. To address common lineage residual errors such as missing or delayed divisions and broken lineage links, we developed a set of segmentation error identification metrics. These metrics were visualized by projecting their results onto the lineage using an interactive viewer linked to the 3D dataset (<xref rid="fig7" ref-type="fig">Fig. 7a and b</xref>). All identified errors stemmed from segmentation issues rather than tracking, and their correction was streamlined by the bidirectional connection between the lineage and embryo representation windows, allowing direct navigation between the two.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Curation of a <italic>Phallusia mammillata</italic> ascidian embryo <italic>named Astec-Pm9 in the publication</italic><sup><xref ref-type="bibr" rid="c16">16</xref></sup>.</title>
<p>a. Cell lineage of the cell b7.1 after the execution of the Disco and Deli plugins on the published data. Projection of the volume property, the <italic>colormap between b and c represents the cell volume in µm</italic><sup><xref ref-type="bibr" rid="c3">3</xref></sup>.</p><p>b. <italic>Scatter view of the corresponding segmented embryo described in a. at t=28 colored by cell volume. Colormap identical as a</italic>.</p><p>c. <italic>Same view as c. with the activation of the “highlight” mode which focuses on the selected cell and shows other cells in transparents colors</italic>.</p><p>d. <italic>Several snapshots of the cell b8.21 at different time points with its associated cell lineage. Colormap represents the cell volume in µm</italic><sup><xref ref-type="bibr" rid="c3">3</xref></sup> <italic>which points to a missing division</italic>.</p><p>e. <italic>Cell lineage of the bilateral cells b7.11 and b7.11*. Colorbar shows the lineage distance between the bilateral symmetrical cells. Black region represents snapshots with no matches between bilateral symmetrical cells</italic>.</p><p>f. <italic>Cell lineage of the bilateral cells a7.5 and b8.6. Colorbar shows the compactness property. The property highlight that the delay of division between A7.5* and A7.5 is due a under-segmented error of A7.5*. B8.6 and B8.6* have expected behavior</italic>.</p><p>g. <italic>Result of the Fuse plugin applied on an over-segmented cell</italic>.</p><p>h. <italic>Top line: Several snapshots of B7.7* cell under-segmented (between time point 31 and 33) from the original segmented embryo. Bottom: result of the Propi plugin applied backward from time t=34 where both cells are well separated</italic>.</p><p>i. <italic>Example of the result of the Wata plugin from a manual added seed (red dot) in the empty space</italic>.</p><p>j. <italic>Result of the Copy-Paste plugin from the selected cell (in gray) on the left side of the embryo to the right side (the new cell appears with a mesh shader in blue). Colormap represents cell volume in µm</italic><sup><xref ref-type="bibr" rid="c3">3</xref></sup>.</p><p>k. <italic>Comparison of the lifetime of bilateral symmetrical cells. The X axis shows the number of in time points separating the division of bilateral cell pairs. The Y axis corresponds to the number of cell (in log)</italic></p></caption>
<graphic xlink:href="639560v1_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>State-of-the-art segmentation methods often produce systematic errors, such as very small segmented objects, which we identified using geometric properties like cell volume (e.g., cell a7.1, <xref rid="fig7" ref-type="fig">Fig. 7a</xref>). Property projection, combined with scatter visualization, facilitates the identification of cells even in dense, interior regions (<xref rid="fig7" ref-type="fig">Fig. 7b</xref>), while the highlight mode enhances this process by isolating the cell of interest (<xref rid="fig7" ref-type="fig">Fig. 7c</xref>). Occasional missing or false divisions disrupt accurate cell histories. During early ascidian embryogenesis, stable cell volumes allowed volume projections onto the lineage to identify rapid variations, revealing missed divisions (<xref rid="fig7" ref-type="fig">Fig. 7d</xref>).</p>
<p>Ascidian embryos exhibit bilateral symmetry, with homologous cells dividing almost synchronously. A l<italic>ineage distance</italic> property<sup><xref ref-type="bibr" rid="c16">16</xref></sup> revealed missed divisions (e.g., b7.11* in <xref rid="fig7" ref-type="fig">Fig. 7e</xref>). To distinguish segmentation errors from natural variability, a <italic>compactness</italic> property was used to detect inaccuracies in division timing by tracking cell rounding during mitosis (e.g., <xref rid="fig7" ref-type="fig">Fig. 7f</xref>).</p>
<p>Using MorphoNet, an expert identified approximately 20 isolated cell lineage errors in a dataset of 185 cell snapshots in just 2 hours.</p>
</sec>
<sec id="s2h3">
<title>Error correction</title>
<p>The 41 multi-component labels were separated using the <italic>Disco</italic> plugin, identifying 7 potential cells via the <italic>volume</italic> property analysis (<xref rid="fig7" ref-type="fig">Fig. 7b</xref>). The others, considered as small artifacts, were merged with neighboring cells sharing the largest surface area using the <italic>Deli</italic> plugin. Division timing issues often stem from over-segmentation, under-segmentation, or missing cells. Over-segmentations were resolved with the <italic>Fuse</italic> plugin (<xref rid="fig7" ref-type="fig">Fig. 7g</xref>), while under-segmentations were corrected using <italic>Propagate segmentation</italic> plugins by tracing back from the first accurate segmentation of sister cells to their mother’s division point (<xref rid="fig7" ref-type="fig">Fig. 7h</xref>). For missing cells, seeds were added with seed generator plugins and segmented using local seeded watershed algorithms (<xref rid="fig7" ref-type="fig">Fig. 7i</xref>). If imaging quality was too poor, the bilaterally symmetrical cell served as a mirror-image proxy, using the Copy-Paste plugin to replicate, rotate, and scale the cell to the missing position (<xref rid="fig7" ref-type="fig">Fig. 7j</xref>).</p>
<p>Using a combination of using 264 MorphoNet plugin actions, 185 errors were corrected in 8 hours actions, leading to a better estimation of the natural variability in cell division timing (Fig.7k)</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Recent advancements in optical time-lapse microscopy allow for the 3D capture of dynamic biological processes but face challenges in automating the segmentation and tracking of large, complex datasets. Residual segmentation errors in time-lapse datasets disrupt data interpretation and hinder long-term cell tracking. Moreover, accurate, high-quality 3D data is critical for training next-generation AI-based segmentation tools, yet the availability of such datasets remains limited.</p>
<p>To address these challenges, <bold>MorphoNet 2.0</bold> was developed as a standalone application for reconstructing, evaluating, and curating 3D and 3D+t datasets. Building on its previous web-based version, MorphoNet 2.0 integrates voxel images with meshed representations, leveraging both Unity Game Engine and Python for enhanced interactivity and processing power. Key features include tools for assessing segmentation quality through unsupervised metrics (e.g., volume, smoothness, and temporal stability), automated error detection, and visualization of segmented objects alongside raw intensity images.</p>
<p>Conventional image curation tools struggle with the complexities of interacting with 3D voxel images. In contrast, MorphoNet 2.0 introduces a novel approach using dual representation layers, enabling efficient biocuration by isolating problem areas and significantly accelerating dataset refinement. MorphoNet 2.0 is user-friendly, open-source, and supports both scientific discovery and AI training by producing high-precision 3D datasets and enabling reproducible, scalable data analysis.</p>
<p>By showcasing 5 use-cases of fluorescence datasets in which cell membranes or nuclei were labeled, we demonstrated that the tool’s high versatility and user-friendliness enables biologists without programming skills to efficiently and intuitively detect and handle a broad range of errors (under-segmentation, over-segmentation, missing objects, lineage errors).</p>
<p>MorphoNet has a high potential to adapt to evolving datasets and segmentation challenges. First, its open-source Python plugin architecture fosters community-driven improvements. These could target cellular datasets as exemplified by the 5 use-cases presented. They could also open MorphoNet to other imaging modalities, including multi-modal datasets combining for instance fluorescence and electron microscopy. Additional plugins could accommodate new AI tools or automate the training of segmentation models, through data augmentation, feature selection, or hyperparameter optimization. Plugins for widely used platforms such as Napari or Fiji will also broaden the user-base and interoperability of the tool, as would also the development of flexible export options to integrate MorphoNet outputs with other analytical pipelines or visualization software. The MorphoNet eco-system could be further enlarged by the creation of a centralised repository for community-developed plugins, by the organisation of MorphoNet-based bio-image analysis challenges to stimulate community engagement and by the provision of curated datasets to use as benchmarks for testing and validating new segmentation algorithms.</p>
<p>Curation could also be improved by the introduction of cloud-based, multi-user capabilities to enable experts to work on the same dataset simultaneously. For now, web browsers impose constraints on the use of computing resources, which could be lifted in the near future, for example through the development of WebGPU.</p>
<p>Curation capabilities will likely also be enhanced by the implementation of adaptive machine learning models that integrate past user corrections to suggest or automate future edits. In the context of 3D segmentation tasks, manual expertise and curation for voxel-wise segmentation is labor-intensive and expensive. Full annotation of large datasets may thus not be feasible. Partial annotations allow datasets to be created with less time and resources while still providing valuable information. Including algorithms such as Sketchpose<sup><xref ref-type="bibr" rid="c30">30</xref></sup> could leverage weakly-supervised learning to generalize from the partially annotated data and infer segmentation patterns in the unlabeled portions of the dataset, a strategy we initiated with the Tribolium dataset. This will make it possible to train models on diverse datasets without the burden of full annotations.</p>
<p>By addressing major challenges in 3D and 3D+t dataset assessment and curation, MorphoNet 2.0 provides a versatile platform for improving segmentation quality and generating reliable ground truths. Its user-friendliness, adaptability and extensibility position it as a valuable tool for advancing quantitative bio-image analysis, with significant potential for enhancement and broader application in the future.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by grants of the Occitanie Region (ESR-PREMAT-213) and of the French National Infrastructure France BioImaging (ANR-10-INBS-04) to EF, and by the Cell-Whisper (ANR-19-CE13-0020) and scEmbryo-Mech (ANR-21-CE13-0046) ANR projects coordinated by PL. EF and PL were CNRS staff scientists. NF was an assistant professor at UM. KB was a UM Phd Student supported by the EpiGenMed Labex (ProjetIA-10-LABX-0012) and a post-doc funded by the scEmbryo-Mech project. BG,TL were contract CNRS engineers. AC was a UM Master student. We thank Christophe Godin, Vanessa Barone and Volker Baecker for their valuable feedback and advice.</p>
</ack>
<sec id="d1e1391" sec-type="additional-information">
<title>Additional information</title>
<sec id="s4">
<title>Author Contributions</title>
<p>B.G. contributed to the development of the website, the 3D viewer and the python API. TL contributed to the development of the 3D viewer and the python API. KB contributed to its featuring and to the help pages. AC and NF contributed to the <italic>Deform</italic> plugin. P.L. contributed to the development of features and to writing the manuscript. E.F. conceived the concept of MorphoNet, developed the code, contributed to its featuring and wrote the manuscript. All authors contributed to discussions on the structure of the manuscript.</p>
</sec>
</sec>
<sec id="suppd1e1391" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary File 1.</label>
<media xlink:href="639560v1_supp1.docx"/>
</supplementary-material>
<supplementary-material id="video1">
<label>Movie 1.</label>
<media xlink:href="639560v1_video1.mp4"/>
</supplementary-material>
<supplementary-material id="video2">
<label>Movie 2.</label>
<media xlink:href="639560v1_video2.mp4"/>
</supplementary-material>
<supplementary-material id="video3">
<label>Movie 3.</label>
<media xlink:href="639560v1_video3.mp4"/>
</supplementary-material>
<supplementary-material id="video4">
<label>Movie 4.</label>
<media xlink:href="639560v1_video4.mp4"/>
</supplementary-material>
<supplementary-material id="video5">
<label>Movie 5.</label>
<media xlink:href="639560v1_video5.mp4"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDole</surname>, <given-names>K.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>In Toto Imaging and Reconstruction of Post-Implantation Mouse Development at the Single-Cell Level</article-title>. <source>Cell</source> <volume>175</volume>, <fpage>859</fpage>–<lpage>876.e33</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Stringer</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Cellpose 2.0: how to train your own model</article-title>. <source>Nat. Methods</source> <volume>19</volume>, <fpage>1634</fpage>–<lpage>1641</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cutler</surname>, <given-names>K. J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation</article-title>. <source>Nat. Methods</source> <volume>19</volume>, <fpage>1438</fpage>–<lpage>1448</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Jin</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Azizi</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Blumberg</surname>, <given-names>A. J</given-names></string-name></person-group>. <article-title>Cellstitch: 3D cellular anisotropic image segmentation via optimal transport</article-title>. <source>BMC Bioinformatics</source> <volume>24</volume>, (<year>2023</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Anonymous</collab></person-group> <article-title>Where imaging and metrics meet</article-title>. <source>Nat. Methods</source> <volume>21</volume>, <fpage>151</fpage>–<lpage>151</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maška</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The Cell Tracking Challenge: 10 years of objective benchmarking</article-title>. <source>Nat. Methods</source> <volume>20</volume>, <fpage>1010</fpage>–<lpage>1020</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Rasband</surname>, <given-names>W. S.</given-names></string-name> &amp; <string-name><surname>Eliceiri</surname>, <given-names>K. W</given-names></string-name></person-group>. <article-title>NIH Image to ImageJ: 25 years of image analysis</article-title>. <source>Nat. Methods</source> <volume>9</volume>, <fpage>671</fpage>–<lpage>675</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Sofroniew</surname>, <given-names>N.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>napari: a multi-dimensional image viewer for Python</article-title>. <source>Zenodo</source> <pub-id pub-id-type="doi">10.5281/zenodo.7276432</pub-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berg</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>ilastik: interactive machine learning for (bio)image analysis</article-title>. <source>Nat. Methods</source> <volume>16</volume>, <fpage>1226</fpage>–<lpage>1232</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Correia</surname>, <given-names>P. L.</given-names></string-name> &amp; <string-name><surname>Pereira</surname>, <given-names>F</given-names></string-name></person-group>. <article-title>Stand-Alone Objective Segmentation Quality Evaluation</article-title>. <source>EURASIP J. Adv. Signal Process</source>. <volume>2002</volume>, <fpage>1</fpage>–<lpage>12</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Valindria</surname>, <given-names>V. V.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Reverse Classification Accuracy: Predicting Segmentation Performance in the Absence of Ground Truth</article-title>. <source>IEEE Trans. Med. Imaging</source> <volume>36</volume>, <fpage>1597</fpage>–<lpage>1606</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leggio</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>MorphoNet: an interactive online morphological browser to explore complex multi-scale data</article-title>. <source>Nat. Commun</source>. <volume>10</volume>, <fpage>2812</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manni</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Morphological Study and 3D Reconstruction of the Larva of the Ascidian Halocynthia roretzi</article-title>. <source>J. Mar. Sci. Eng</source>. <volume>10</volume>, <issue>11</issue> (<year>2022</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chung</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A single oscillating proto-hypothalamic neuron gates taxis behavior in the primitive chordate Ciona</article-title>. <source>Curr. Biol</source>. <volume>33</volume>, <fpage>3360</fpage>–<lpage>3370.e4</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Refahi</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A multiscale analysis of early flower development in Arabidopsis provides an integrated view of molecular regulation and growth control</article-title>. <source>Dev. Cell</source> <volume>56</volume>, <fpage>540</fpage>–<lpage>556.e8</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guignard</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Contact area–dependent cell communication and the morphological invariance of ascidian embryogenesis</article-title>. <source>Science</source> <volume>369</volume>, <fpage>eaar5663</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dardaillon</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>ANISEED 2019: 4D exploration of genetic data for an extended range of tunicates</article-title>. <source>Nucleic Acids Res</source>. <volume>48</volume>, <fpage>D668</fpage>–<lpage>D675</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Theis</surname>, <given-names>S</given-names></string-name>, <string-name><surname>Mendieta-Serrano</surname>, <given-names>MA</given-names></string-name>, <string-name><surname>Chapa-y-Lazo</surname>, <given-names>B</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>J</given-names></string-name>, <string-name><surname>Saunders</surname>, <given-names>TE</given-names></string-name></person-group> <article-title>CellMet: Extracting 3D shape metrics from cells and tissues</article-title> | <source>bioRxiv</source>. <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2024.10.11.617843v1">https://www.biorxiv.org/content/10.1101/2024.10.11.617843v1</ext-link>. <year>2024</year></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Weigert</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Haase</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sugawara</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Myers</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy</article-title>. in <year>2020</year> <conf-name>IEEE Winter Conference on Applications of Computer Vision (WACV)</conf-name> <fpage>3655</fpage>–<lpage>3662</lpage> (<publisher-name>IEEE, Snowmass Village, CO</publisher-name>, <publisher-loc>USA</publisher-loc>, <volume>2020</volume>). doi:<pub-id pub-id-type="doi">10.1109/WACV45572.2020.9093435</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vergara</surname>, <given-names>H. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Whole-body integration of gene expression and single-cell morphology</article-title>. <source>Cell</source> <volume>184</volume>, <fpage>4819</fpage>–<lpage>4837.e22</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stegmaier</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Real-Time Three-Dimensional Cell Segmentation in Large-Scale Microscopy Data of Developing Embryos</article-title>. <source>Dev. Cell</source> <volume>36</volume>, <fpage>225</fpage>–<lpage>240</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walt</surname>, <given-names>S</given-names></string-name>. <string-name><surname>van der</surname></string-name> <etal>et al.</etal></person-group> <article-title>scikit-image: image processing in Python</article-title>. <source>PeerJ</source> <volume>2</volume>, <fpage>e453</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barone</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Local and global changes in cell density induce reorganisation of 3D packing in a proliferating epithelium</article-title>. <source>Development</source> <volume>151</volume>, <fpage>dev202362</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andrés-San Román</surname>, <given-names>J. A.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>CartoCell, a high-content pipeline for 3D image analysis, unveils cell morphology patterns in epithelia</article-title>. <source>Cell Rep. Methods</source> <volume>3</volume>, <fpage>100597</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willis</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cell size and growth regulation in the Arabidopsis thaliana apical stem cell niche</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>. <volume>113</volume>, <fpage>E8238</fpage>–<lpage>E8246</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernandez</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Imaging plant growth in 4D: robust tissue reconstruction and lineaging at cell resolution</article-title>. <source>Nat Meth</source> <volume>7</volume>, <fpage>547</fpage>–<lpage>553</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kar</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Benchmarking of deep learning algorithms for 3D instance segmentation of confocal image datasets</article-title>. <source>PLOS Comput. Biol</source>. <volume>18</volume>, <fpage>e1009879</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Michaelos</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Pachitariu</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>. <source>Nat. Methods</source> <volume>18</volume>, <fpage>100</fpage>–<lpage>106</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>J. I.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Automated analysis of embryonic gene expression with cellular resolution in C. elegans</article-title>. <source>Nat. Methods</source> <volume>5</volume>, <fpage>703</fpage>–<lpage>709</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Cazorla</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Munier</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Morin</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Weiss</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Sketchpose: Learning to Segment Cells with Partial Annotations</article-title>. <source>HAl Open Science</source> <elocation-id>hal-04330824</elocation-id> <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="https://hal.science/hal-04330824v1">https://hal.science/hal-04330824v1</ext-link></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106227.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Graña</surname>
<given-names>Martin</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Institut Pasteur de Montevideo</institution>
</institution-wrap>
<city>Montevideo</city>
<country>Uruguay</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work presents an <bold>important</bold> technical advancement with the release of MorphoNet 2.0, a user-friendly, standalone platform for 3D+T segmentation and analysis in biological imaging. The authors provide <bold>convincing</bold> evidence of the tool's capabilities through illustrative use cases, though broader validation against current state-of-the-art tools would strengthen its position. The software's accessibility and versatility make it a resource that will be of value for the bioimaging community, particularly in specialized subfields.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106227.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors present a substantial improvement to their existing tool, MorphoNet, intended to facilitate assessment of 3D+t cell segmentation and tracking results, and curation of high-quality analysis for scientific discovery and data sharing. These tools are provided through a user-friendly GUI, making them accessible to biologists who are not experienced coders. Further, the authors have re-developed this tool to be a locally installed piece of software instead of a web interface, making the analysis and rendering of large 3D+t datasets more computationally efficient. The authors evidence the value of this tool with a series of use cases, in which they apply different features of the software to existing datasets and show the improvement to the segmentation and tracking achieved.</p>
<p>While the computational tools packaged in this software are familiar to readers (e.g., cellpose), the novel contribution of this work is the focus on error correction. The MorphoNet 2.0 software helps users identify where their candidate segmentation and/or tracking may be incorrect. The authors then provide existing tools in a single user-friendly package, lowering the threshold of skill required for users to get maximal value from these existing tools. To help users apply these tools effectively, the authors introduce a number of unsupervised quality metrics that can be applied to a segmentation candidate to identify masks and regions where the segmentation results are noticeably different from the majority of the image.</p>
<p>This work is valuable to researchers who are working with cell microscopy data that requires high-quality segmentation and tracking, particularly if their data are 3D time-lapse and thus challenging to segment and assess. The MorphoNet 2.0 tool that the authors present is intended to make the iterative process of segmentation, quality assessment, and re-processing easier and more streamlined, combining commonly used tools into a single user interface.</p>
<p>One of the key contributions of the work is the unsupervised metrics that MorphoNet 2.0 offers for segmentation quality assessment. These metrics are used in the use cases to identify low-quality instances of segmentation in the provided datasets, so that they can be improved with plugins directly in MorphoNet 2.0. However, not enough consideration is given to demonstrating that optimizing these metrics leads to an improvement in segmentation quality. For example, in Use Case 1, the authors report their metrics of interest (Intensity offset, Intensity border variation, and Nuclei volume) for the uncurated silver truth, the partially curated and fully curated datasets, but this does not evidence an improvement in the results. Additional plotting of the distribution of these metrics on the Gold Truth data could help confirm that the distribution of these metrics now better matches the expected distribution.</p>
<p>Similarly, in Use Case 2, visual inspection leads us to believe that the segmentation generated by the Cellpose + Deli pipeline (shown in Figure 4d) is an improvement, but a direct comparison of agreement between segmented masks and masks in the published data (where the segmentations overlap) would further evidence this.</p>
<p>We would appreciate the authors addressing the risk of decreasing the quality of the segmentations by applying circular logic with their tool; MorphoNet 2.0 uses unsupervised metrics to identify masks that do not fit the typical distribution. A model such as StarDist can be trained on the &quot;good&quot; masks to generate more masks that match the most common type. This leads to a more homogeneous segmentation quality, without consideration for whether these metrics actually optimize the segmentation</p>
<p>In Use case 5, the authors include details that the errors were corrected by &quot;264 MorphoNet plugin actions ... in 8 hours actions [sic]&quot;. The work would benefit from explaining whether this is 8 hours of human work, trying plugins and iteratively improving, or 8 hours of compute time to apply the selected plugins.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106227.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This article presents Morphonet 2.0, a software designed to visualise and curate segmentations of 3D and 3D+t data. The authors demonstrate their capabilities on five published datasets, showcasing how even small segmentation errors can be automatically detected, easily assessed, and corrected by the user. This allows for more reliable ground truths, which will in turn be very much valuable for analysis and training deep learning models. Morphonet 2.0 offers intuitive 3D inspection and functionalities accessible to a non-coding audience, thereby broadening its impact.</p>
<p>Strengths:</p>
<p>The work proposed in this article is expected to be of great interest to the community by enabling easy visualisation and correction of complex 3D(+t) datasets. Moreover, the article is clear and well written, making MorphoNet more likely to be used. The goals are clearly defined, addressing an undeniable need in the bioimage analysis community. The authors use a diverse range of datasets, successfully demonstrating the versatility of the software.</p>
<p>We would also like to highlight the great effort that was made to clearly explain which type of computer configurations are necessary to run the different datasets and how to find the appropriate documentation according to your needs. The authors clearly carefully thought about these two important problems and came up with very satisfactory solutions.</p>
<p>Weaknesses:</p>
<p>There is still one concern: the quantification of the improvement of the segmentations in the use cases and, therefore, the quantification of the potential impact of the software. While it appears hard to quantify the quality of the correction, the proposed work would be significantly improved if such metrics could be provided.</p>
<p>The authors show some distributions of metrics before and after segmentations to highlight the changes. This is a great start, but there seem to be two shortcomings: first, the comparison and interpretation of the different distributions does not appear to be trivial. It is therefore difficult to judge the quality of the improvement from these. Maybe an explanation in the text of how to interpret the differences between the distributions could help. A second shortcoming is that the before/after metrics displayed are the metrics used to guide the correction, so, by design, the scores will improve, but does that accurately represent the improvement of the segmentation? It seems to be the case, but it would be nice to maybe have a better assessment of the improvement of the quality.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106227.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>A very thorough technical report of a new standalone, open-source software for microscopy image processing and analysis (MorphoNet 2.0), with a particular emphasis on automated segmentation and its curation to obtain accurate results even with very complex 3D stacks, including timelapse experiments.</p>
<p>Strengths:</p>
<p>The authors did a good job of explaining the advantages of MorphoNet 2.0, as compared to its previous web-based version and to other software with similar capabilities. What I particularly found more useful to actually envisage these claimed advantages is the five examples used to illustrate the power of the software (based on a combination of Python scripting and the 3D game engine Unity). These examples, from published research, are very varied in both types of information and image quality, and all have their complexities, making them inherently difficult to segment. I strongly recommend the readers to carefully watch the accompanying videos, which show (although not thoroughly) how the software is actually used in these examples.</p>
<p>Weaknesses:</p>
<p>Being a technical article, the only possible comments are on how methods are presented, which is generally adequate, as mentioned above. In this regard, and in spite of the presented examples (chosen by the authors, who clearly gave them a deep thought before showing them), the only way in which the presented software will prove valuable is through its use by as many researchers as possible. This is not a weakness per se, of course, but just what is usual in this sort of report. Hence, I encourage readers to download the software and give it time to test it on their own data (which I will also do myself).</p>
<p>In conclusion, I believe that this report is fundamental because it will be the major way of initially promoting the use of MorphoNet 2.0 by the objective public. The software itself holds the promise of being very impactful for the microscopists' community.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106227.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gallean</surname>
<given-names>Benjamin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Laurent</surname>
<given-names>Tao</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Biasuz</surname>
<given-names>Kilian</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Clement</surname>
<given-names>Ange</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Faraj</surname>
<given-names>Noura</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lemaire</surname>
<given-names>Patrick</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4925-2009</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Faure</surname>
<given-names>Emmanuel</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2787-0885</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>eLife Assessment</bold></p>
<p>This work presents an important technical advancement with the release of MorphoNet 2.0, a user-friendly, standalone platform for 3D+T segmentation and analysis in biological imaging. The authors provide convincing evidence of the tool's capabilities through illustrative use cases, though broader validation against current state-of-the-art tools would strengthen its position. The software's accessibility and versatility make it a resource that will be of value for the bioimaging community, particularly in specialized subfields.</p>
</disp-quote>
<p>We would like to thank the editors and reviewers for their careful and constructive evaluation of our manuscript “MorphoNet 2.0: An innovative approach for qualitative assessment and segmentation curation of large-scale 3D time-lapse imaging datasets”. We are grateful for the positive assessment of MorphoNet 2.0 as a valuable and accessible tool for the bioimaging community, and for the recognition of its technical advancements, particularly in the context of complex 3D+t segmentation tasks.</p>
<p>The reviewers have highlighted several important points that we will address in the revised manuscript. These include:</p>
<p>- The need for a clearer demonstration that improvements in unsupervised quality metrics correspond to actual improvements in segmentation quality. In response, we will provide comparisons with gold standard annotations where available and clarify how to interpret metric distributions.</p>
<p>
- The potential risk of circular logic when using unsupervised metrics to guide model training. We now explicitly discuss this limitation and emphasize the importance of external validation and expert input.</p>
<p>
- The value of comparing MorphoNet 2.0 to other tools such as FIJI and napari. We will include a comparative table to help readers understand MorphoNet’s positioning and complementarity.</p>
<p>
- The importance of clearer documentation and terminology. We will overhaul the help pages, standardize plugin naming, and add a glossary-style table to the manuscript.</p>
<p>
- Suggestions for future developments, such as mesh export and interoperability with napari, which we will explore for the revision.</p>
<p>We appreciate the detailed feedback on both scientific and editorial aspects, including corrections to figures and text, and we will integrate all suggested revisions to improve the manuscript’s clarity and impact. We are confident that these changes will strengthen the manuscript and enhance the utility of MorphoNet 2.0 for the community.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>The authors present a substantial improvement to their existing tool, MorphoNet, intended to facilitate assessment of 3D+t cell segmentation and tracking results, and curation of high-quality analysis for scientific discovery and data sharing. These tools are provided through a user-friendly GUI, making them accessible to biologists who are not experienced coders. Further, the authors have re-developed this tool to be a locally installed piece of software instead of a web interface, making the analysis and rendering of large 3D+t datasets more computationally efficient. The authors evidence the value of this tool with a series of use cases, in which they apply different features of the software to existing datasets and show the improvement to the segmentation and tracking achieved.</p>
<p>While the computational tools packaged in this software are familiar to readers (e.g., cellpose), the novel contribution of this work is the focus on error correction. The MorphoNet 2.0 software helps users identify where their candidate segmentation and/or tracking may be incorrect. The authors then provide existing tools in a single user-friendly package, lowering the threshold of skill required for users to get maximal value from these existing tools. To help users apply these tools effectively, the authors introduce a number of unsupervised quality metrics that can be applied to a segmentation candidate to identify masks and regions where the segmentation results are noticeably different from the majority of the image.</p>
<p>This work is valuable to researchers who are working with cell microscopy data that requires high-quality segmentation and tracking, particularly if their data are 3D time-lapse and thus challenging to segment and assess. The MorphoNet 2.0 tool that the authors present is intended to make the iterative process of segmentation, quality assessment, and re-processing easier and more streamlined, combining commonly used tools into a single user interface.</p>
</disp-quote>
<p>We sincerely thank the reviewer for their thorough and encouraging evaluation of our work. We are grateful that they highlighted both the technical improvements of MorphoNet 2.0 and its potential impact for the broader community working with complex 3D+t microscopy datasets. We particularly appreciate the recognition of our efforts to make advanced segmentation and tracking tools accessible to non-expert users through a user-friendly and locally installable interface, and for pointing out the importance of error detection and correction in the iterative analysis workflow. The reviewer’s appreciation of the value of integrating unsupervised quality metrics to support this process is especially meaningful to us, as this was a central motivation behind the development of MorphoNet 2.0. We hope the tool will indeed facilitate more rigorous and reproducible analyses, and we are encouraged by the reviewer’s positive assessment of its utility for the community.</p>
<disp-quote content-type="editor-comment">
<p>One of the key contributions of the work is the unsupervised metrics that MorphoNet 2.0 offers for segmentation quality assessment. These metrics are used in the use cases to identify low-quality instances of segmentation in the provided datasets, so that they can be improved with plugins directly in MorphoNet 2.0. However, not enough consideration is given to demonstrating that optimizing these metrics leads to an improvement in segmentation quality. For example, in Use Case 1, the authors report their metrics of interest (Intensity offset, Intensity border variation, and Nuclei volume) for the uncurated silver truth, the partially curated and fully curated datasets, but this does not evidence an improvement in the results. Additional plotting of the distribution of these metrics on the Gold Truth data could help confirm that the distribution of these metrics now better matches the expected distribution.</p>
<p>Similarly, in Use Case 2, visual inspection leads us to believe that the segmentation generated by the Cellpose + Deli pipeline (shown in Figure 4d) is an improvement, but a direct comparison of agreement between segmented masks and masks in the published data (where the segmentations overlap) would further evidence this.</p>
</disp-quote>
<p>We agree that demonstrating the correlation between metric optimization and real segmentation improvement is essential. We will add new analysis comparing the distributions of the unsupervised metrics with the gold truth data before and after curation. Additionally, we will provide overlap scores where ground truth annotations are available, confirming the improvement. We will also explicitly discuss the limitation of relying solely on unsupervised metrics without complementary validation.</p>
<disp-quote content-type="editor-comment">
<p>We would appreciate the authors addressing the risk of decreasing the quality of the segmentations by applying circular logic with their tool; MorphoNet 2.0 uses unsupervised metrics to identify masks that do not fit the typical distribution. A model such as StarDist can be trained on the &quot;good&quot; masks to generate more masks that match the most common type. This leads to a more homogeneous segmentation quality, without consideration for whether these metrics actually optimize the segmentation</p>
</disp-quote>
<p>We thank the reviewer for this important and insightful comment. It raises a crucial point regarding the risk of circular logic in our segmentation pipeline. Indeed, relying on unsupervised metrics to select “good” masks and using them to train a model like StarDist could lead to reinforcing a particular distribution of shapes or sizes, potentially filtering out biologically relevant variability. This homogenization may improve consistency with the chosen metrics, but not necessarily with the true underlying structures.</p>
<p>We fully agree that this is a key limitation to be aware of. We will revise the manuscript to explicitly discuss this risk, emphasizing that while our approach may help improve segmentation quality according to specific criteria, it should be complemented with biological validation and, when possible, expert input to ensure that important but rare phenotypes are not excluded.</p>
<disp-quote content-type="editor-comment">
<p>In Use case 5, the authors include details that the errors were corrected by &quot;264 MorphoNet plugin actions ... in 8 hours actions [sic]&quot;. The work would benefit from explaining whether this is 8 hours of human work, trying plugins and iteratively improving, or 8 hours of compute time to apply the selected plugins.</p>
</disp-quote>
<p>We will clarify that the “8 hours” refer to human interaction time, including exploration, testing, and iterative correction using plugins.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>This article presents Morphonet 2.0, a software designed to visualise and curate segmentations of 3D and 3D+t data. The authors demonstrate their capabilities on five published datasets, showcasing how even small segmentation errors can be automatically detected, easily assessed, and corrected by the user. This allows for more reliable ground truths, which will in turn be very much valuable for analysis and training deep learning models. Morphonet 2.0 offers intuitive 3D inspection and functionalities accessible to a non-coding audience, thereby broadening its impact.</p>
<p>Strengths:</p>
<p>The work proposed in this article is expected to be of great interest to the community by enabling easy visualisation and correction of complex 3D(+t) datasets. Moreover, the article is clear and well written, making MorphoNet more likely to be used. The goals are clearly defined, addressing an undeniable need in the bioimage analysis community. The authors use a diverse range of datasets, successfully demonstrating the versatility of the software.</p>
<p>We would also like to highlight the great effort that was made to clearly explain which type of computer configurations are necessary to run the different datasets and how to find the appropriate documentation according to your needs. The authors clearly carefully thought about these two important problems and came up with very satisfactory solutions.</p>
</disp-quote>
<p>We would like to sincerely thank the reviewer for their positive and thoughtful feedback. We are especially grateful that they acknowledged the clarity of the manuscript and the potential value of MorphoNet 2.0 for the community, particularly in facilitating the visualization and correction of complex 3D(+t) datasets. We also appreciate the reviewer’s recognition of our efforts to provide detailed guidance on hardware requirements and access to documentation—two aspects we consider crucial to ensuring the tool is both usable and widely adopted. Their comments are very encouraging and reinforce our commitment to making MorphoNet 2.0 as accessible and practical as possible for a broad range of users in the bioimage analysis community.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>There is still one concern: the quantification of the improvement of the segmentations in the use cases and, therefore, the quantification of the potential impact of the software. While it appears hard to quantify the quality of the correction, the proposed work would be significantly improved if such metrics could be provided.</p>
<p>The authors show some distributions of metrics before and after segmentations to highlight the changes. This is a great start, but there seem to be two shortcomings: first, the comparison and interpretation of the different distributions does not appear to be trivial. It is therefore difficult to judge the quality of the improvement from these. Maybe an explanation in the text of how to interpret the differences between the distributions could help. A second shortcoming is that the before/after metrics displayed are the metrics used to guide the correction, so, by design, the scores will improve, but does that accurately represent the improvement of the segmentation? It seems to be the case, but it would be nice to maybe have a better assessment of the improvement of the quality.</p>
</disp-quote>
<p>We thank the reviewer for this constructive and important comment. We fully agree that assessing the true quality improvement of segmentation after correction is a central and challenging issue. While we initially focused on changes in the unsupervised quality metrics to illustrate the effect of the correction, we acknowledge that interpreting these distributions may not be straightforward, and that relying solely on the metrics used to guide the correction introduces an inherent bias in the evaluation.</p>
<p>To address the first point, we will revise the manuscript to provide clearer guidance on how to interpret the changes in metric distributions before and after correction, with additional examples to make this interpretation more intuitive.</p>
<p>Regarding the second point, we agree that using independent, external validation is necessary to confirm that the segmentation has genuinely improved. To this end, we will include additional assessments using complementary evaluation strategies on selected datasets where ground truth is accessible, to compare pre- and post-correction segmentations with an independent reference. These results reinforce the idea that the corrections guided by unsupervised metrics generally lead to more accurate segmentations, but we also emphasize their limitations and the need for biological validation in real-world cases.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>Summary:</p>
<p>A very thorough technical report of a new standalone, open-source software for microscopy image processing and analysis (MorphoNet 2.0), with a particular emphasis on automated segmentation and its curation to obtain accurate results even with very complex 3D stacks, including timelapse experiments.</p>
<p>Strengths:</p>
<p>The authors did a good job of explaining the advantages of MorphoNet 2.0, as compared to its previous web-based version and to other software with similar capabilities. What I particularly found more useful to actually envisage these claimed advantages is the five examples used to illustrate the power of the software (based on a combination of Python scripting and the 3D game engine Unity). These examples, from published research, are very varied in both types of information and image quality, and all have their complexities, making them inherently difficult to segment. I strongly recommend the readers to carefully watch the accompanying videos, which show (although not thoroughly) how the software is actually used in these examples.</p>
</disp-quote>
<p>We sincerely thank the reviewer for their thoughtful and encouraging feedback. We are particularly pleased that the reviewer appreciated the comparative analysis of MorphoNet 2.0 with both its earlier version and existing tools, as well as the relevance of the five diverse and complex use cases we selected. Demonstrating the software’s versatility and robustness across a variety of challenging datasets was a key goal of this work, and we are glad that this aspect came through clearly. We also appreciate the reviewer’s recommendation to watch the accompanying videos, which we designed to provide a practical sense of how the tool is used in real-world scenarios. Their positive assessment is highly motivating and reinforces the value of combining scripting flexibility with an interactive 3D interface.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>Being a technical article, the only possible comments are on how methods are presented, which is generally adequate, as mentioned above. In this regard, and in spite of the presented examples (chosen by the authors, who clearly gave them a deep thought before showing them), the only way in which the presented software will prove valuable is through its use by as many researchers as possible. This is not a weakness per se, of course, but just what is usual in this sort of report. Hence, I encourage readers to download the software and give it time to test it on their own data (which I will also do myself).</p>
</disp-quote>
<p>We fully agree that the true value of MorphoNet 2.0 will be demonstrated through its practical use by a wide range of researchers working with complex 3D and 3D+t datasets. In this regard, we will improve the user documentation and provide a set of example datasets to help new users quickly familiarize themselves with the platform. We are also committed to maintaining and updating MorphoNet 2.0 based on user feedback to further support its usability and impact.</p>
<disp-quote content-type="editor-comment">
<p>In conclusion, I believe that this report is fundamental because it will be the major way of initially promoting the use of MorphoNet 2.0 by the objective public. The software itself holds the promise of being very impactful for the microscopists' community.</p>
</disp-quote>
</body>
</sub-article>
</article>