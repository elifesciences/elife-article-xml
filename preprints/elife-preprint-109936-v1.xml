<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109936</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109936</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109936.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Cell Biology</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>AutoMorphoTrack: A modular framework for quantitative analysis of organelle morphology, motility, and interactions at single-cell resolution</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5960-0479</contrib-id>
<name>
<surname>Bayati</surname>
<given-names>Armin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>abayati@mgh.harvard.edu</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0000-4227-4710</contrib-id>
<name>
<surname>Schumacher</surname>
<given-names>Jackson G</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8582-8891</contrib-id>
<name>
<surname>Chen</surname>
<given-names>Xiqun</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>xchen17@mgh.harvard.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002pd6e78</institution-id><institution>Department of Neurology, Massachusetts General Hospital and Harvard Medical School</institution></institution-wrap>, <city>Boston</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03zj4c476</institution-id><institution>Aligning Science Across Parkinson’s (ASAP) Collaborative Research Network</institution></institution-wrap>, <city>Chevy Chase</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Sero</surname>
<given-names>Julia E</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/002h8g185</institution-id><institution>University of Bath</institution>
</institution-wrap>
<city>Bath</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2026-02-04">
<day>04</day>
<month>02</month>
<year>2026</year>
</pub-date>
<volume>15</volume>
<elocation-id>RP109936</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2026-01-07">
<day>07</day>
<month>01</month>
<year>2026</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-11-12">
<day>12</day>
<month>11</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.07.19.665650"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2026, Bayati et al</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>Bayati et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109936-v1.pdf"/>
<abstract>
<p>Quantitative imaging of organelle dynamics provides crucial insights into cellular function, state, and organization; however, existing analysis workflows often require advanced coding expertise and multiple software tools. AutoMorphoTrack is an open-source Python toolkit that automates organelle detection, morphology classification, motility tracking, and colocalization from multichannel fluorescence microscopy image stacks. The platform includes adaptive segmentation, organelle trajectory reconstruction, and pixel-level overlap quantification within a unified, reproducible framework that can be executed as an interactive Jupyter notebook, a modular Python package, or through AI-assisted natural-language commands. Each analysis step outputs publication-ready images, time-lapse videos, and standardized quantitative data tables. To complement the main pipeline, an accompanying script— AMTComparison.py—is provided to demonstrate how AutoMorphoTrack’s outputs can be extended for comparative analysis across individual neurons or experimental conditions. Together, these tools provide an accessible framework for high-content, reproducible quantification of subcellular morphology, motility, and interactions at single-cell resolution.</p>
</abstract>
<abstract abstract-type="graphical">
<title>Graphical Abstract</title>
<fig id="ufig1" position="float" orientation="portrait" fig-type="figure">
<graphic xlink:href="665650v4_ufig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03zj4c476</institution-id>
<institution>Aligning Science Across Parkinson's (ASAP)</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Chen</surname>
<given-names>Xiqun</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>New package function and supplemental materials.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Quantitative live-cell imaging has become indispensable for studying subcellular organization, revealing how organelles interact, adapt, and reorganize in response to physiological and pathological cues <sup><xref ref-type="bibr" rid="c1">1</xref></sup>. In neurons, organelle dynamics reflect the balance between energy production, trafficking, and degradation, processes that are tightly regulated through mitochondrial morphology, lysosomal positioning, and their physical and functional interactions <sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c3">3</xref></sup>. While microscopy captures these processes at high temporal and spatial resolution, translating those images into interpretable, quantitative information remains challenging. Most analysis workflows, although very powerful in their outputs, require coding/programming knowledge or rely on proprietary software that lacks transparency and reproducibility <sup><xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<p>AutoMorphoTrack was developed to bridge this divide by providing an open-source, automated, and reproducible image analysis framework specifically designed for cell biologists. The toolkit performs every major stage of organelle quantification: detection, classification, tracking, motility measurement, and colocalization, all within a flexible structure that can be executed in three ways: as a step-by-step Jupyter notebook for interactive analysis, as a standalone Python package for batch processing, or through AI-assisted natural-language prompts. Each function produces both visual and quantitative outputs, allowing users to verify segmentation accuracy, track organelle behavior across time, and extract shape and motion parameters.</p>
<p>The central goal of AutoMorphoTrack is transparency without complexity. Every processing step is exposed to the user through editable parameters—such as threshold sensitivity, eccentricity, and minimum object size—so that segmentation and classification can be optimized without altering the source code. The system outputs standardized image and data formats, including <monospace>.png</monospace>, <monospace>.mp4</monospace>, and <monospace>.csv</monospace>, ensuring compatibility with downstream visualization or statistical tools. This standardization enables the easy comparison of results from different experiments or cell types.</p>
<p>To illustrate how AutoMorphoTrack data can be extended beyond single-sample analysis, we provide an independent companion script, <monospace>AMTComparison.py</monospace>, which demonstrates a flexible approach to comparing results between neurons or experimental conditions. Rather than embedding statistical testing directly into the package, this separate script shows how exported outputs can be analyzed using customizable statistical methods, including t-tests, Welch’s tests, or nonparametric alternatives. This design preserves analytical freedom while maintaining full compatibility with external platforms such as GraphPad Prism, R, or custom Python workflows.</p>
<p>By combining automation, reproducibility, and flexibility, AutoMorphoTrack transforms high-resolution microscopy into a quantitative, interpretable, and extensible dataset. It provides intricate information for examining how organelle morphology, movement, and interaction differ across samples/conditions, allowing computational analysis to provide biological insight.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Software Architecture and Workflow Overview</title>
<p>AutoMorphoTrack was developed in Python (version 3.9 or later) as an image-analysis framework that performs organelle detection, morphological classification, motility tracking, and colocalization within a single, reproducible pipeline (<xref rid="tbl1" ref-type="table">Table 1</xref>). Each analytical component is organized as an independent module (<xref rid="tbl2" ref-type="table">Table 2</xref>) that can be executed sequentially to generate standardized outputs (<xref rid="tbl3" ref-type="table">Table 3</xref>) or imported individually for custom workflows. All modules rely on open-source libraries—<monospace>NumPy (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_008633">RRID:SCR_008633</ext-link>), pandas (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_018214">RRID:SCR_018214</ext-link>), matplotlib (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_008624">RRID:SCR_008624</ext-link>), seaborn (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_018132">RRID:SCR_018132</ext-link>), OpenCV (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_015526">RRID:SCR_015526</ext-link>), scikit-image (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_021142">RRID:SCR_021142</ext-link>), SciPy (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_008058">RRID:SCR_008058</ext-link>),</monospace> and <monospace>tifffile (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_023338">RRID:SCR_023338</ext-link>)</monospace>—and run efficiently on most CPU-based computers.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Functional Steps in the AutoMorphoTrack Workflow</title></caption>
<graphic xlink:href="665650v4_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_tbl1a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>AutoMorphoTrack Package Structure</title></caption>
<graphic xlink:href="665650v4_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_tbl2a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><title>Representative Outputs Generated by AutoMorphoTrack</title></caption>
<graphic xlink:href="665650v4_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>The pipeline can be executed as (i) an interactive Jupyter Notebook (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_018315">RRID:SCR_018315</ext-link>), (ii) a Python package for automated batch analyses, or (iii) an AI-assisted natural-language interface that interprets plain-text prompts. Each mode implements an identical coding language to ensure that results are comparable across execution environments.</p>
</sec>
<sec id="s2b">
<title>Input Data and Preprocessing</title>
<p>AutoMorphoTrack is designed to analyze multichannel fluorescence microscopy data containing two distinct organelle populations. Although originally optimized for mitochondria (Channel 0) and lysosomes (Channel 1), the framework is adaptable to any pair of fluorescently labeled structures, such as endosomes, peroxisomes, autophagosomes, Golgi vesicles, or other dynamic compartments, by redefining the input channels and adjusting detection parameters.</p>
<p>Input files should be supplied as multichannel .tif image stacks obtained from live-cell microscopy. Each stack is parsed to verify dimensional consistency and channel count prior to analysis. Individual channels are converted to 8-bit grayscale to standardize intensity across datasets, and optional spatial upscaling (typically 2×) can be applied to improve segmentation accuracy in low-resolution or dimly labeled samples.</p>
</sec>
<sec id="s2c">
<title>Adaptability</title>
<p>User-defined parameters enable flexible adaptation to diverse imaging sources. The minimum particle size parameter excludes background noise and camera artifacts; the threshold sensitivity factor (<monospace>thr_factor</monospace>) fine-tunes segmentation stringency relative to the Otsu threshold; and channel assignment variables (<monospace>MITO_CHANNEL</monospace>, <monospace>LYSO_CHANNEL</monospace>) allow users to specify which channels correspond to the organelle of interest. This parameter transparency ensures that AutoMorphoTrack can be applied across various imaging systems and experimental contexts—from conventional confocal microscopy to high-throughput spinning-disk or super-resolution platforms—without requiring modifications to the underlying source code, making it easier for biologists to utilize it in their everyday experiments.</p>
<p>Preprocessed frames serve as the standardized input for all subsequent analytical steps, generating consistent data structures that enable direct comparison across organelle types, cell models, or experimental conditions.</p>
</sec>
<sec id="s2d">
<title>Cell Culturing and Preparation</title>
<p>Human induced pluripotent stem cell (iPSC)–derived Neural Progenitor Cells (NPCs) were obtained from STEMCELL Technologies (Catalog #200-0620) and expanded in Neural Progenitor Medium (STEMCELL, Catalog #05833) according to the manufacturer’s instructions. After two passages, NPCs underwent midbrain differentiation using the Midbrain Neuron Differentiation Kit (STEMCELL, Catalog #100-0038) supplemented with 200 ng/mL Human Recombinant Sonic Hedgehog (Shh; STEMCELL, Catalog #78065) for 7 days. Dopaminergic NPCs were then plated on poly-D-lysine and Matrigel–coated glass-bottom 6-, 24-, or 96-well plates (Thermo Scientific, Nunc) for Western blotting, fluorescence microscopy, and quantitative imaging, respectively.</p>
<p>Differentiated NPCs were matured into neurons using the Midbrain Neuron Maturation Kit (STEMCELL, Catalog #100-0041) for two weeks. Following maturation, neurons were maintained in BrainPhys Neuronal Medium supplemented with the SM1 Kit (STEMCELL, Catalog #05792). For each biological replicate, neurons derived from independent differentiation events were plated on glass coverslips (Thomas Scientific, Catalog #1217N79) pre-coated with Matrigel (Corning, Catalog #356234) in 12- or 24- well plates. After completing each replicate, new NPC stocks were thawed and differentiated independently to ensure biological reproducibility.</p>
<p>Fluorescence imaging was performed using a Nikon C2 Confocal Microscope equipped with NIS-Elements software (Nikon, <ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:RRID:SCR_014329">RRID:SCR_014329</ext-link>). For live-cell imaging, dopaminergic neurons were plated on glass-bottom dishes (MatTek, P35G-1.5-14-C) and allowed to mature for 2 weeks. Before imaging, neurons were stained with MitoTracker Red (Invitrogen, M7510) and LysoTracker Green (Invitrogen, L7526) for 30 minutes in BrainPhys medium. Live imaging was conducted using a Nikon C2 confocal microscope equipped with a stage-top environmental chamber (37°C, 5% CO<sub>2</sub>) and a 63× oil-immersion objective lens. Time-lapse images were acquired at 5 s intervals for 40 consecutive frames in multichannel mode and exported as .tif stacks for downstream processing with AutoMorphoTrack.</p>
</sec>
<sec id="s2e">
<title>Step 1 – Organelle Detection</title>
<p>Segmentation is performed using adaptive Otsu thresholding combined with morphological opening, small-object removal, and border clearing. Detected objects are stored as binary masks for mitochondria and lysosomes. Corresponding color overlays (<monospace>Mito_Frame0.png, Lyso_Frame0.png</monospace>) and detection videos (<monospace>*_Detection.mp4</monospace>) are produced automatically (<xref rid="tbl2" ref-type="table">Table 2</xref>). These visual outputs enable real-time verification of segmentation accuracy and are subsequently reused for morphology, tracking, and colocalization analyses.</p>
</sec>
<sec id="s2f">
<title>Step 2 – Lysosome Counting</title>
<p>The lysosomal counting module enumerates and labels individual lysosomes per frame, providing a quantitative measure of vesicle abundance over time. Outputs include labeled images (<monospace>Lyso_Frame0_Count.png</monospace>), temporal plots (<monospace>Lyso_Count_Plot.png</monospace>), and the tabulated dataset Lysosome_Counts.csv. Together, these files lysosomal turnover and temporal heterogeneity, forming the first dynamic parameter set in the pipeline (<xref rid="tbl2" ref-type="table">Table 2</xref>, Step 2).</p>
</sec>
<sec id="s2g">
<title>Step 3 – Mitochondrial Morphology Classification</title>
<p>Mitochondrial networks are quantified by classifying individual mitochondria as elongated or punctate based on user-adjustable thresholds for area and eccentricity. The resulting labeled frame (<monospace>Morphology_Frame0_Labeled.png</monospace>) and summary tables (<monospace>Morphology_Summary.csv</monospace>) provide a rapid overview of morphological states across time. Because all parameters are adjustable, users can tune detection sensitivity to match imaging contrast or magnification, thereby ensuring consistency across experiments.</p>
</sec>
<sec id="s2h">
<title>Step 4 – Shape Feature Extraction</title>
<p>Beyond binary classification of mitochondria, AutoMorphoTrack computes shape descriptors—area, eccentricity, circularity, solidity, aspect ratio, and orientation—for every segmented organelle. The outputs (<monospace>Mito_ShapeMetrics.csv</monospace>, <monospace>Lyso_ShapeMetrics.csv</monospace>) provide frame-by-frame quantification of structural properties. AutoMorphoTrack then generates plots (<monospace>Shape_ViolinPlots.png</monospace>) that summarize morphological properties within and across organelle populations.</p>
</sec>
<sec id="s2i">
<title>Step 5 – Organelle Tracking and Motility Analysis</title>
<p>To capture organelle dynamics, coordinates from consecutive frames are linked using nearest-neighbor matching implemented via <monospace>scipy.spatial.cKDTree</monospace>. This approach reconstructs trajectories for each detected organelle, from which total displacement and mean velocity are computed. This step exports individual coordinate tables (<monospace>*_Tracks.csv</monospace>), scatter and distribution plots (<monospace>Motility_Scatter.png</monospace>, <monospace>Motility_Distributions.png</monospace>), and cumulative overlays (<monospace>Cumulative_Mito.png</monospace>, <monospace>Cumulative_Lyso.png</monospace>). These metrics describe the spatial range and kinetics of organelle movement.</p>
</sec>
<sec id="s2j">
<title>Step 6 – Colocalization Analysis</title>
<p>Colocalization between mitochondria and lysosomes is quantified using both Manders’ coefficients (M1, M2) and the Pearson correlation coefficient, computed directly from binary masks to ensure spatial precision. Visual overlays highlight overlapping pixels in bright blue, while numerical results are stored in <monospace>Colocalization.csv</monospace> and summarized graphically in <monospace>Colocalization_MetricsPlot.png</monospace>. The consistent data structure across analyses facilitates direct integration with motility and morphology outputs.</p>
</sec>
<sec id="s2k">
<title>Step 7 – Integrated Correlation Summary</title>
<p>All per-frame and per-object datasets are merged into an integrated summary table (<monospace>Integrated_Merged_Data.csv</monospace>) using <monospace>pandas.merge()</monospace>. Pearson correlation coefficients are calculated among morphology, motility, and colocalization parameters to generate a comprehensive overview of organelle behavior. The resulting heatmap (<monospace>Integrated_CorrelationMatrix.png</monospace>) reveals relationships such as whether elongated mitochondria exhibit unique motility or colocalization profiles compared to punctate ones.</p>
</sec>
<sec id="s2l">
<title>Step 8 – Comparative Analysis (Optional External Script)</title>
<p>While AutoMorphoTrack explicitly focuses on generating quantitative descriptors and outputs, comparative analyses between cells or experimental conditions can be conducted using the companion script <monospace>AMTComparison.py</monospace>. This external code illustrates how users can extend AutoMorphoTrack data into statistical frameworks of their choice. The script automatically loads exported .csv files generated by AutoMorphoTrack, produces violin plots for key metrics, and selects statistical tests (Student’s t, Welch’s, or Mann-Whitney U) based on normality and variance. It reports effect sizes (Cohen’s d, Hedges’ g, or rank-biserial r) alongside significance. Because optimal comparative strategies depend on study design, this step remains optional and modular.</p>
</sec>
<sec id="s2m">
<title>Software Organization and Package Structure</title>
<p>The internal architecture of AutoMorphoTrack consists of discrete Python modules that correspond to each analytical stage (<xref rid="tbl2" ref-type="table">Table 2</xref>). The core modules (<monospace>detection.py</monospace>, <monospace>morphology.py</monospace>, <monospace>tracking.py</monospace>, <monospace>motility.py</monospace>, <monospace>colocalization.py</monospace>, and <monospace>summary.py</monospace>) define the main computational pipeline, while supportive scripts (<monospace>utils.py</monospace>, <monospace>lyso_count.py</monospace>, <monospace>shape_features.py</monospace>) manage input/output parameters, visualization, and shared functions. The optional <monospace>AMTComparison.py</monospace> script is included with the manuscript (and can be found on Github) as a standalone example of downstream analysis. Data analysis following the use of AutoMorphoTrack is entirely dependent on the user’s preference; therefore, the <monospace>AMTComparison.py</monospace> is not integrated into the AutoMorphoTrack package.</p>
</sec>
<sec id="s2n">
<title>Summary of Outputs, Functions, and Package Design</title>
<p>Comprehensive overviews of AutoMorphoTrack’s outputs, functional steps, and module organization are provided in <xref rid="tbl1" ref-type="table">Tables 1</xref>, <xref rid="tbl2" ref-type="table">2</xref>, and 3. Together, these tables detail how the code converts raw multichannel image stacks into structured, quantitative datasets ready for visualization, comparison, and interpretation.</p>
</sec>
<sec id="s2o">
<title>AI-Assisted Execution and Natural-Language Interface</title>
<p>AutoMorphoTrack and its accompanying comparison script (AMTComparison.py) were designed for seamless integration with large-language-model (LLM)–based chat interfaces, enabling users to execute and modify analyses through natural language commands instead of directly dealing with coding languages. Both the Jupyter notebook and Python package versions can be used interactively with AI chatbots (such as ChatGPT, GPT-4, or similar LLM platforms) that support Python code interpretation and file manipulation.</p>
<p>Through these interfaces, users can request specific analytical actions, for example, “segment mitochondria and lysosomes from this TIFF stack,” “calculate velocity distributions,” or “compare morphology metrics between neurons.” The chatbot interprets the prompt, executes the corresponding code block from the AutoMorphoTrack notebook or package, and returns the generated visual or quantitative outputs. This interaction provides an intuitive layer of automation that enables experimental biologists with limited coding experience to perform reproducible, high-content image analyses without manually modifying the source code.</p>
<p>This AI-assisted execution mode ensures full reproducibility, as every operation is logged within the notebook environment, and the underlying Python commands remain visible to the user. By combining open-source design with conversational accessibility, AutoMorphoTrack provides an interactive, educational, and reproducible framework for computational image analysis.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>AutoMorphoTrack produces a sequence of results that quantify the morphology, dynamics, and interactions of subcellular organelles at single-cell resolution. Each analytical stage generates both visual and quantitative outputs that can be interpreted independently or as part of an integrated cellular profile. The following results illustrate the complete workflow applied to individual image stacks and its extension to comparative analyses between two image stacks. The application of AutoMorphoTrack to additional image stacks, along with their corresponding outputs, is presented in <xref rid="figs1" ref-type="fig">Figures S1-8</xref>.</p>
<sec id="s3a">
<title>Detection, Counting, and Morphological Classification</title>
<p>The workflow begins with adaptive Otsu thresholding to segment mitochondria and lysosomes from multichannel fluorescence image stacks. A composite fluorescence image illustrates the original dataset, followed by independent detection maps showing mitochondria in red and lysosomes in green (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Each lysosome is labeled and enumerated per frame, producing a count map and a temporal plot of lysosome numbers across the recording (<xref rid="fig1" ref-type="fig">Figure 1B</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Automated detection, lysosomal counting, and mitochondrial morphology classification.</title><p>(<bold>A</bold>) Composite multichannel fluorescence image showing mitochondria and lysosomes, followed by independent detection maps for mitochondria. Scale bar = 10 µm. (<bold>B</bold>) Each lysosome is labeled and counted per frame, with a corresponding temporal plot illustrating vesicle abundance across the recording. (<bold>C</bold>) Mitochondrial morphology is classified as elongated or punctate based on area and eccentricity thresholds, shown as a labeled frame (frame 0) and quantitative summary chart. Together, these panels depict the initial detection and morphological analysis workflow implemented in AutoMorphoTrack.</p></caption>
<graphic xlink:href="665650v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Mitochondrial morphology is automatically classified as elongated or punctate according to area and eccentricity thresholds. The resulting morphology map and quantitative summary chart capture both elongated and fragmented mitochondrial populations (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). These steps collectively form the foundation of AutoMorphoTrack’s detection and morphology modules.</p>
</sec>
<sec id="s3b">
<title>Shape Profiling and Feature Extraction</title>
<p>Beyond binary classification, AutoMorphoTrack extracts shape descriptors for each segmented organelle. Distributions of parameters such as circularity, solidity, eccentricity, aspect ratio, and area summarize the range of morphological states of organelles (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). Violin plots comparing the same descriptors for mitochondria and lysosomes (<xref rid="fig2" ref-type="fig">Figure 2B</xref>) provide a continuous morphological profile that complements prior classification metrics.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Quantitative shape profiling of organelles.</title><p>(<bold>A</bold>) Continuous shape descriptors extracted for all detected organelles are summarized as parameter distributions showing variability in area, circularity, solidity, eccentricity, and aspect ratio within a single neuron. (<bold>B</bold>) Comparative violin plots illustrate differences in these shape metrics between mitochondria and lysosomes, providing a continuous morphological quantification beyond categorical classification.</p></caption>
<graphic xlink:href="665650v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3c">
<title>Organelle Tracking and Motility Analysis</title>
<p>Centroid-based nearest-neighbor linkage reconstructs organelle paths across frames. Cumulative-track overlays display the independent motion of lysosomes and mitochondria, as well as their composite trajectories when both populations are visualized together (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). Corresponding velocity and displacement distributions (<xref rid="fig3" ref-type="fig">Figure 3B</xref>) depict organelle motility characteristics, while a scatter plot (<xref rid="fig3" ref-type="fig">Figure 3C</xref>) summarizes individual organelle trajectories. Together, these panels quantify the dynamic behavior of organelles within single neurons.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Organelle tracking and motility characterization.</title><p>(<bold>A</bold>) Centroid-based trajectory reconstructions show cumulative lysosomal and mitochondrial movements and a composite overlay of both populations. (<bold>B</bold>) Quantitative motility metrics are summarized as velocity and displacement distributions. (<bold>C</bold>) A scatter plot of individual trajectories, together capturing the dynamic range of organelle transport within a neuron.</p></caption>
<graphic xlink:href="665650v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3d">
<title>Colocalization and Interaction Metrics</title>
<p>Spatial overlap between mitochondria and lysosomes is quantified using both Manders’ and Pearson’s coefficients. A representative overlay highlights regions of direct spatial proximity (<xref rid="fig4" ref-type="fig">Figure 4A</xref>), while a metrics plot summarizes Manders M1/M2 and Pearson <italic>r</italic> coefficients across time (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). These results capture the extent and stability of mitochondria–lysosome interactions within individual neurons, across the timeframe in which the cell is observed.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Mitochondria–lysosome colocalization within single neurons.</title><p>(<bold>A</bold>) Representative overlay highlighting regions of spatial overlap between mitochondria and lysosomes. (<bold>B</bold>) Quantitative summary plot showing Manders M1/M2 and Pearson <italic>r</italic> coefficients across time. These analyses quantify the frequency and degree of inter-organelle contact within the cell.</p></caption>
<graphic xlink:href="665650v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3e">
<title>Integrated Correlation Summary</title>
<p>To assess how structural and functional properties interrelate, AutoMorphoTrack merges all outputs into an integrated dataset. A single correlation matrix (<xref rid="fig5" ref-type="fig">Figure 5</xref>) illustrates the pairwise relationships between morphological descriptors, motility parameters, and colocalization indices, providing a multidimensional representation of organelle behavior within one neuron.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Integrated correlation of morphology, motility, and colocalization metrics.</title><p>Comprehensive correlation matrix displaying pairwise Pearson relationships among morphological descriptors, motility parameters, and colocalization indices derived from AutoMorphoTrack outputs. The matrix reveals coordinated trends linking structural and dynamic features within a single neuron.</p></caption>
<graphic xlink:href="665650v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3f">
<title>Cross-Neuron Comparative Analysis</title>
<p>Two different image stacks were analyzed in parallel to demonstrate the reproducibility and extensibility of AutoMorphoTrack outputs.</p>
<p><xref rid="fig6" ref-type="fig">Figure 6</xref> compares lysosomal abundance (<xref rid="fig6" ref-type="fig">Figure 6A</xref>) and mitochondrial morphology profiles (<xref rid="fig6" ref-type="fig">Figure 6B</xref>) between the two neurons, revealing cell-to-cell variability in lysosomal turnover and mitochondrial morphology.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Lysosome and mitochondrial comparisons between neurons.</title><p>(<bold>A</bold>) Comparative plots showing differences in lysosomal abundance per frame and (<bold>B</bold>) mitochondrial morphology classification between two neurons. Variations highlight cell-specific differences in vesicle turnover and mitochondrial network organization.</p></caption>
<graphic xlink:href="665650v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig7" ref-type="fig">Figure 7</xref> summarizes differences in shape metrics, showing violin plots for area (<xref rid="fig7" ref-type="fig">Figure 7A</xref>), circularity (<xref rid="fig7" ref-type="fig">Figure 7B</xref>), solidity (<xref rid="fig7" ref-type="fig">Figure 7C</xref>), eccentricity (<xref rid="fig7" ref-type="fig">Figure 7D</xref>), and aspect ratio (<xref rid="fig7" ref-type="fig">Figure 7E</xref>). These comparisons highlight distinct geometric characteristics that distinguish the organelles across the two image stacks.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Comparative shape metrics across neurons.</title><p>Violin plots comparing (<bold>A</bold>) circularity, (<bold>B</bold>) aspect ratio, (<bold>C</bold>) solidity, (<bold>D</bold>) eccentricity, and (<bold>E</bold>) area between Neuron 1 and Neuron 2. Distinct parameter distributions reveal structural heterogeneity and variability in organelle geometry across cells.</p></caption>
<graphic xlink:href="665650v4_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Motility differences are illustrated in <xref rid="fig8" ref-type="fig">Figure 8</xref>, where the distributions of mean velocity (<xref rid="fig8" ref-type="fig">Figure 8A</xref>) and total displacement (<xref rid="fig8" ref-type="fig">Figure 8B</xref>) reveal neuron-specific patterns of organelle movement.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Comparative motility parameters between neurons.</title><p>Distributions of total displacement (<bold>A</bold>) and mean velocity (<bold>B</bold>) for mitochondria and lysosomes from two neurons. Differences demonstrate cell-specific organelle transport dynamics and variability in overall motility behavior.</p></caption>
<graphic xlink:href="665650v4_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Variability in inter-organelle coupling/colocalization is shown in <xref rid="fig9" ref-type="fig">Figure 9</xref>, which presents comparative analyses of Manders M1 (<xref rid="fig9" ref-type="fig">Figure 9A</xref>), Manders M2 (<xref rid="fig9" ref-type="fig">Figure 9B</xref>), Pearson <italic>r</italic> (<xref rid="fig9" ref-type="fig">Figure 9C</xref>), and percent overlap (<xref rid="fig9" ref-type="fig">Figure 9D</xref>) between neurons. These comparisons demonstrate distinct levels of mitochondria–lysosome colocalization and coordination.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Comparative colocalization metrics between neurons.</title><p>Violin plots showing differences in Manders M1 (<bold>A</bold>), Manders M2 (<bold>B</bold>), Pearson <italic>r</italic> (<bold>C</bold>), and percent overlap (<bold>D</bold>) between Neuron 1 and Neuron 2. Divergent distributions indicate variability in mitochondria–lysosome coupling efficiency across cells.</p></caption>
<graphic xlink:href="665650v4_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finally, <xref rid="fig10" ref-type="fig">Figure 10</xref> visualizes the difference between the two neurons’ integrated correlation matrices, highlighting parameters whose relationships shift most strongly between cells.</p>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Differential correlation matrix between neurons.</title><p>Subtraction map of correlation matrices from Neuron 1 and Neuron 2, highlighting parameters whose inter-relationships differ most strongly between cells. The heatmap summarizes shifts in coordination among morphology, motility, and interaction features, revealing neuron-specific patterns of subcellular organization.</p></caption>
<graphic xlink:href="665650v4_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>This study introduces AutoMorphoTrack, an automated, open-source pipeline for quantitative analysis of organelle morphology, motility, and spatial interactions. By combining transparent modular design with reproducible data handling, AutoMorphoTrack bridges a significant gap between high-content microscopy and quantitative cell biology. Previous image analysis tools, including ImageJ plugins such as LIM Tracker and MTrack, have enabled the segmentation or tracking of subcellular structures; however, each typically focuses on a single task or organelle class <sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>.</p>
<p>AutoMorphoTrack integrates and extends these capabilities within a unified, reproducible workflow. The toolkit enables non-programming users to perform sophisticated image analyses that were previously restricted to computational specialists.</p>
<p>The analyses performed here demonstrate that AutoMorphoTrack accurately segments and tracks subcellular structures while preserving biologically relevant variability. Within single image stacks (featuring single neurons), the pipeline captured continuous morphological diversity and revealed distinct populations of elongated and punctate mitochondria. Such heterogeneity reflects the balance between mitochondrial fusion and fission, processes that maintain organelle quality control and metabolic adaptability. Similarly, lysosomal counts and motility trajectories varied dynamically within individual cells, consistent with the notion that lysosomal positioning is actively regulated by microtubule transport and local metabolic cues. The ability of AutoMorphoTrack to simultaneously visualize and quantify these events allows the identification of subtle dynamic features that would otherwise remain qualitative.</p>
<p>When applied to two independent image stacks, AutoMorphoTrack revealed substantial cell-to-cell differences in every major dimension of organelle organization. Morphology-based comparisons showed that the overall distribution of mitochondrial shapes varies between neurons, suggesting differences in network connectivity, mitochondrial dynamics, or energetic demand. Shape-descriptor comparisons further revealed that structural parameters, such as circularity, solidity, eccentricity, and aspect ratio, diverge across cells, emphasizing that even morphologically similar neurons maintain unique subcellular geometries. These differences are not artifacts of imaging or segmentation but reflect genuine biological heterogeneity at the single-cell level, as all these variabilities are controlled for through consistent thresholding and size parameters in AutoMorphoTrack.</p>
<p>Motility analyses uncovered additional layers of diversity. The distributions of mean velocity and total displacement indicated that one neuron harbored more mobile organelles than the other, pointing to differences in cytoskeletal organization or intracellular trafficking. When comparing neurons undergoing different experimental treatments, these changes can provide mechanistic insights into the minute effects caused by the experimental variable. The inclusion of colocalization analyses further extended these insights by quantifying mitochondria–lysosome interactions, a critical component of organelle quality control. Divergent values in Manders’ coefficients and Pearson correlations between neurons suggest cell-specific differences that can be further adapted to neurons captured in different experimental conditions.</p>
<p>Beyond its biological insights, AutoMorphoTrack offers a methodological advance in how image-derived data are structured and analyzed. Each processing step produces standardized outputs that can be directly used for downstream statistical analysis or integration within other computational frameworks. The optional comparison script, <monospace>AMTComparison.py</monospace>, exemplifies this extension. By remaining external to the core package, it allows users to adopt or modify statistical approaches suited to their experimental design while maintaining full reproducibility.</p>
<p>Overall, AutoMorphoTrack transforms subcellular imaging into a quantitative, reproducible, and scalable workflow that connects structural, kinetic, and interaction data within single cells. It not only enables the identification of subtle differences in organelle morphology and dynamics but also provides a reproducible framework for comparing these parameters across individual conditions or treatments. By merging accessibility with analytical depth, AutoMorphoTrack advances the study of organelle behavior from descriptive observation to quantitative, statistically rigorous insight.</p>
</sec>
<sec id="s5">
<title>Conclusion</title>
<p>AutoMorphoTrack provides an end-to-end solution for quantitative, reproducible, and accessible analysis of organelle morphology, motility, and spatial interactions. By unifying segmentation, tracking, and correlation analysis within a single open-source framework, the toolkit enables consistent, high-content evaluation of subcellular organization across imaging modalities and experimental designs. The inclusion of standardized outputs and an adaptable comparison workflow allows users to extend the analysis toward inter-neuron or inter-condition comparisons without modifying the underlying code.</p>
<p>Through its modular design, AutoMorphoTrack can be readily adapted to any pair of fluorescently labeled organelles, extending its relevance beyond mitochondria and lysosomes to other compartments such as endosomes, peroxisomes, or autophagosomes. The capacity to quantify and compare these parameters across individual cells or conditions lays the groundwork for studying how subcellular coordination varies in health and disease. As imaging datasets continue to expand in complexity, AutoMorphoTrack provides a transparent and reproducible foundation for bridging cell biology with computational image analysis—transforming qualitative microscopy into a quantitative framework for mechanistic discovery.</p>
</sec>

</body>
<back>
<sec id="s10" sec-type="supplementary">
<title>Supplementary Figures</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figures 1-8.</label>
<caption><title>AutoMorphoTrack outputs of other analyzed image stacks.</title><p>Functional Steps in the AutoMorphoTrack workflow were run on additional image stacks to verify the validity and applicability of the package. (<bold>A</bold>) Shows channel isolation, thresholding, and organelle segmentation. (<bold>B</bold>) Organelle segmentation enables the quantification of lysosomes across multiple frames. (<bold>C</bold>) The segmented mitochondrial channel is used to quantify mitochondrial morphology (Elongated vs Punctate). (<bold>D</bold> and <bold>E</bold>) Analysis of organelle morphology and structural profiling across different measures. (<bold>F</bold> and <bold>G</bold>). The trajectory of organelles and the cumulative path taken is quantified and visualized, along with displacement and velocity. (<bold>H</bold>) The colocalization of mitochondria and lysosomes is visualized and quantified across various approaches. (<bold>I</bold>) A comparative analysis of the image stack is conducted across all the measures discussed earlier.</p></caption>
<graphic xlink:href="665650v4_figs1.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_figs2.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_figs3.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_figs4.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_figs5.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_figs6.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_figs7.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="665650v4_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s6" sec-type="data-availability">
<title>Code and Data Availability</title>
<p>Outputs generated using AutoMorphoTrack can be found at: Bayati, Armin (2025), “AutoMorphoTrack: A modular framework for quantitative analysis of organelle morphology, motility, and interactions at single-cell resolution”, Mendeley Data, V1, doi: 10.17632/hjnzjcfwjm.1.</p>
<p>Package can be accessed from: <ext-link ext-link-type="uri" xlink:href="https://github.com/abayatibrain/AMTpackage.git">https://github.com/abayatibrain/AMTpackage.git</ext-link>.</p>
<p>Jupyter Notebook version of this code can be accessed from: <ext-link ext-link-type="uri" xlink:href="https://github.com/abayatibrain/automorphotrack.git">https://github.com/abayatibrain/automorphotrack.git</ext-link>.</p>
<p>Comparison code, used to compare outputs of AutoMorphoTrack generated from analyzing different image stacks can be found at: <ext-link ext-link-type="uri" xlink:href="https://github.com/abayatibrain/AMTcomparison.git">https://github.com/abayatibrain/AMTcomparison.git</ext-link>.</p>
</sec>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s7">
<title>Funding</title>
<p>This research was funded in part by Aligning Science Across Parkinson’s No. ASAP-237603 through the Michael J. Fox Foundation for Parkinson’s Research (MJFF).</p>
</sec>
<sec id="s8">
<title>Author Contributions</title>
<p>
<bold>A.B.</bold> conceptualized and developed AutoMorphoTrack, designed the computational architecture, implemented the image-analysis pipeline, and performed all experiments, analyses, and figure generation. <bold>J.G.S.</bold> contributed to the writing and editing of the manuscript. <bold>X.C.</bold> supervised the project.</p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>AutoMorphoTrack Outputs</label>
<media xlink:href="supplements/665650_file02.zip"/>
</supplementary-material>
<supplementary-material id="supp2">
<label>AutoMorphoTrack Comparison Package</label>
<media xlink:href="supplements/665650_file03.zip"/>
</supplementary-material>
<supplementary-material id="supp3">
<label>AutoMorphoTrack Comparison Outputs</label>
<media xlink:href="supplements/665650_file04.zip"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>E</given-names></string-name>, <string-name><surname>van Tartwijk</surname> <given-names>FW</given-names></string-name>, <string-name><surname>Kaminski</surname> <given-names>CF</given-names></string-name></person-group>. <article-title>Advances in the study of organelle interactions and their role in neurodegenerative diseases have been enabled by super-resolution microscopy</article-title>. <source>Neurobiol Dis</source>. <year>2021</year> <month>Nov</month>;<volume>159</volume>:<fpage>105475</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.nbd.2021.105475</pub-id>. Epub 2021 Aug 11. PMID: <pub-id pub-id-type="pmid">34390833</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shah</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chacko</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Joseph</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Ananthanarayanan</surname> <given-names>V</given-names></string-name></person-group>. <article-title>Mitochondrial dynamics, positioning and function mediated by cytoskeletal interactions</article-title>. <source>Cell Mol Life Sci</source>. <year>2021</year> <month>Apr</month>;<volume>78</volume>(<issue>8</issue>):<fpage>3969</fpage>–<lpage>3986</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00018-021-03762-5</pub-id>. Epub 2021 Feb 12. PMID: <pub-id pub-id-type="pmid">33576841</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC11071877</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barral</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Staiano</surname> <given-names>L</given-names></string-name>, <string-name><surname>Guimas Almeida</surname> <given-names>C</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Current methods to analyze lysosome morphology, positioning, motility, and function</article-title>. <source>Traffic</source>. <year>2022</year> May;<volume>23</volume>(<issue>5</issue>):<fpage>238</fpage>–<lpage>269</lpage>. doi: <pub-id pub-id-type="doi">10.1111/tra.12839</pub-id>. Epub 2022 Apr 24. PMID: <pub-id pub-id-type="pmid">35343629</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC9323414</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bagheri</surname> <given-names>N</given-names></string-name>, <string-name><surname>Carpenter</surname> <given-names>AE</given-names></string-name>, <string-name><surname>Lundberg</surname> <given-names>E</given-names></string-name>, <string-name><surname>Plant</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Horwitz</surname> <given-names>R</given-names></string-name></person-group>. <article-title>The new era of quantitative cell imaging-challenges and opportunities</article-title>. <source>Mol Cell</source>. <year>2022</year> <month>Jan</month> <day>20</day>;<volume>82</volume>(<issue>2</issue>):<fpage>241</fpage>–<lpage>247</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.molcel.2021.12.024</pub-id>. PMID: <pub-id pub-id-type="pmid">35063094</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC10339817</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paul-Gilloteaux</surname> <given-names>P</given-names></string-name>, <string-name><surname>Tosi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hériché</surname> <given-names>JK</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Bioimage analysis workflows: community resources to navigate through a complex ecosystem</article-title>. <source>F1000Res</source>. <year>2021</year> <month>Apr</month> <day>26</day>;<volume>10</volume>:<fpage>320</fpage>. doi: <pub-id pub-id-type="doi">10.12688/f1000research.52569.1</pub-id>. PMID: <pub-id pub-id-type="pmid">34136134</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC8182692</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miura</surname> <given-names>K</given-names></string-name>, <string-name><surname>Nørrelykke</surname> <given-names>SF.</given-names></string-name></person-group> <article-title>Reproducible image handling and analysis</article-title>. <source>EMBO J</source>. <year>2021</year> <month>Feb</month> <day>1</day>;<volume>40</volume>(<issue>3</issue>):<fpage>e105889</fpage>. doi: <pub-id pub-id-type="doi">10.15252/embj.2020105889</pub-id>. Epub 2021 Jan 22. PMID: <pub-id pub-id-type="pmid">33480052</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC7849301</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aragaki</surname> <given-names>H</given-names></string-name>, <string-name><surname>Ogoh</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kondo</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Aoki</surname> <given-names>K</given-names></string-name></person-group>. <article-title>LIM Tracker: a software package for cell tracking and analysis with advanced interactivity</article-title>. <source>Sci Rep</source>. <year>2022</year> <month>Feb</month> <day>17</day>;<volume>12</volume>(<issue>1</issue>):<fpage>2702</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-022-06269-6</pub-id>. PMID: <pub-id pub-id-type="pmid">35177675</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC8854686</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kapoor</surname> <given-names>V</given-names></string-name>, <string-name><surname>Hirst</surname> <given-names>WG</given-names></string-name>, <string-name><surname>Hentschel</surname> <given-names>C</given-names></string-name>, <string-name><surname>Preibisch</surname> <given-names>S</given-names></string-name>, <string-name><surname>Reber</surname> <given-names>S.</given-names></string-name></person-group> <article-title>MTrack: Automated Detection, Tracking, and Analysis of Dynamic Microtubules</article-title>. <source>Sci Rep</source>. <year>2019</year> <month>Mar</month> <day>7</day>;<volume>9</volume>(<issue>1</issue>):<fpage>3794</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-018-37767-1</pub-id>. PMID: <pub-id pub-id-type="pmid">30846705</pub-id>; PMCID: <pub-id pub-id-type="pmcid">PMC6405942</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109936.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sero</surname>
<given-names>Julia E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/002h8g185</institution-id><institution>University of Bath</institution>
</institution-wrap>
<city>Bath</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This work describes a <bold>useful</bold> computational tool for automated morphometry of dynamic organelles from microscope images. However, the supporting evidence and novelty of the manuscript as presented are <bold>incomplete</bold> and could be improved. The work will be of interest to microscopists and bioimage analysts who are non-experts but wish to improve quantitative analysis of cellular structures.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109936.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors develop a Python-based analysis framework for cellular organelle segmentation, feature extraction, and analysis for live-cell imaging videos. They demonstrate that their pipeline works for two organelles (mitochondria and lysosomes) and provide a step-by-step overview of the AutoMorphoTrack package.</p>
<p>Strengths:</p>
<p>The authors provide evidence that the package is functional and can provide publication-quality data analysis for mitochondrial and lysosomal segmentation and analysis.</p>
<p>Weaknesses:</p>
<p>(1) I was enthusiastic about the manuscript as a good end-to-end cell/organelle segmentation and quantification pipeline that is open-source, and is indeed useful to the field. However, I'm not certain AutoMorphoTrack fully fulfills this need. It appears to stitch together basic FIJI commands in a Python script that an experienced user can put together within a day. The paper reads as a documentation page, and the figures seem to be individual analysis outputs of a handful of images. Indeed, a recent question on the <ext-link ext-link-type="uri" xlink:href="http://image.sc">image.sc</ext-link> forum prompted similar types of analysis and outputs as a simple service to the community, and with seemingly better results and integrated organelle identity tracking (which is necessary in my opinion for live imaging). I believe this is a better fit in the methods section of a broader work. <ext-link ext-link-type="uri" xlink:href="https://forum.image.sc/t/how-to-analysis-organelle-contact-in-fiji-with-time-series-data/116359/5">https://forum.image.sc/t/how-to-analysis-organelle-contact-in-fiji-with-time-series-data/116359/5</ext-link>.</p>
<p>(2) The authors do not discuss or compare to any other pipelines that can accomplish similar analyses, such as Imaris, CellProfiler, or integrate options for segmentation, etc., such as CellPose, StarDist.</p>
<p>(3) Although LLM-based chatbot integration seems to have been added for novelty, the authors do not demonstrate in the manuscript, nor provide instructions for making this easy-to-implement, given that it is directed towards users who do not code, presumably.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109936.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>AutoMorphoTrack provides an end-to-end workflow for organelle-scale analysis of multichannel live-cell fluorescence microscopy image stacks. The pipeline includes organelle detection/segmentation, extraction of morphological descriptors (e.g., area, eccentricity, &quot;circularity,&quot; solidity, aspect ratio), tracking and motility summaries (implemented via nearest-neighbor matching using cKDTree), and pixel-level overlap/colocalization metrics between two channels. The manuscript emphasizes a specific application to live imaging in neurons, demonstrated on iPSC-derived dopaminergic neuronal cultures with mitochondria in channel 0 and lysosomes in channel 1, while asserting adaptability to other organelle pairs.</p>
<p>The tool is positioned for cell biologists, including users with limited programming experience, primarily through two implemented modes of use: (i) a step-by-step Jupyter notebook and (ii) a modular Python package for scripted or batch execution, alongside an additional &quot;AI-assisted&quot; mode that is described as enabling analyses through natural-language prompts.</p>
<p>The motivation and general workflow packaging are clear, and the notebook-plus-modules structure is a reasonable engineering choice. However, in its current form, the manuscript reads more like a convenient assembly of standard methods than a validated analytical tool. Key claims about robustness, accuracy, and scope are not supported by quantitative evidence, and the 'AI-assisted' framing is insufficiently defined and attributes to the tool capabilities that are provided by external LLM platforms rather than by AutoMorphoTrack itself. In addition, several figure, metric, and statistical issues-including physically invalid plots and inconsistent metric definitions-directly undermine trust in the quantitative outputs.</p>
<p>Strengths:</p>
<p>(1) Clear motivation: lowering the barrier for organelle-scale quantification for users who do not routinely write custom analysis code.</p>
<p>(2) Multiple entry points: an interactive notebook together with importable modules, emphasizing editable parameters rather than a fully opaque black box.</p>
<p>(3) End-to-end outputs: automated generation of standardized visualizations and tables that, if trustworthy, could help users obtain quantitative summaries without assembling multiple tools.</p>
<p>Weaknesses:</p>
<p>(1) &quot;AI-assisted / natural-language&quot; functionality is overstated.</p>
<p>The manuscript implies an integrated natural-language interface, but no such interface is implemented in the software. Instead, users are encouraged to use external chatbots to help generate or modify Python code or execute notebook steps. This distinction is not made clearly and risks misleading readers.</p>
<p>(2) No quantitative validation against trusted ground truth.</p>
<p>There is no systematic evaluation of segmentation accuracy, tracking fidelity, or interaction/overlap metrics against expert annotations or controlled synthetic data. Without such validation, accuracy, parameter sensitivity, and failure modes cannot be assessed.</p>
<p>(3) Limited benchmarking and positioning relative to existing tools.</p>
<p>The manuscript does not adequately compare AutoMorphoTrack to established platforms that already support segmentation, morphometrics, tracking, and colocalization (e.g., CellProfiler) or to mitochondria-focused toolboxes (e.g., MiNA, MitoGraph, Mitochondria Analyzer). This is particularly problematic given the manuscript's implicit novelty claims.</p>
<p>(4) Core algorithmic components are basic and likely sensitive to imaging conditions.</p>
<p>Heavy reliance on thresholding and morphological operations raises concerns about robustness across varying SNR, background heterogeneity, bleaching, and organelle density; these issues are not explored.</p>
<p>(5) Multiple figure, metric, and statistical issues undermine confidence.</p>
<p>The most concerning include:</p>
<p>
(i) &quot;Circularity (4πA/P²)&quot; values far greater than 1 (Figures 2 and 7, and supplementary figures), which is inconsistent with the stated definition and strongly suggests a metric/label mismatch or computational error.</p>
<p>(ii) A displacement distribution extending to negative values (Figure 3B). This is likely a plotting artifact (e.g., KDE boundary bias), but as shown, it is physically invalid and undermines confidence in the motility analysis.</p>
<p>(iii) Colocalization/overlap metrics that are inconsistently defined and named, with axis ranges and terminology that can mislead (e.g., Pearson r reported for binary masks without clarification).</p>
<p>(iv) Figure legends that do not match the displayed panels, and insufficient reporting of Ns, p-values, sampling units, and statistical assumptions.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109936.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>AutoMorphoTrack is a Python package for quantitatively evaluating organelle shape, movement, and colocalization in high-resolution live cell imaging experiments. It is designed to be a beginning-to-end workflow from segmentation through metric graphing, which is easy to implement. The paper shows example results from their images of mitochondria and lysosomes within cultured neurons, demonstrating how it can be used to understand organelle processing.</p>
<p>Strengths:</p>
<p>The text is well-written and easy to follow. I particularly appreciate tables 1 and 2, which clearly define the goals of each module, the tunable parameters, and the input and outputs. I can see how the provided metrics would be useful to other groups studying organelle dynamics. Additionally, because the code is open-source, it should be possible for experienced coders to use this as a backbone and then customize it for their own purposes.</p>
<p>Weaknesses:</p>
<p>Unfortunately, I was not able to install the package to test it myself using any standard install method. This is likely fixable by the authors, but until a functional distribution exists, the utility of this tool is highly limited. I would be happy to re-review this work after this is fixed.</p>
<p>The authors claim that there is &quot;AI-Assisted Execution and Natural-Language Interface&quot;. However, this is never defended in any of the figures, and from quickly reviewing the .py files, there does not seem to be any built-in support or interface for this. Without significantly more instructions on how to connect this package to a (free) LLM, along with data to prove that this works reproducibly to produce equivalent results, this section should be removed.</p>
<p>Additionally, I have a few suggestions/questions:</p>
<p>(1) Red-green images are difficult for colorblind readers. I recommend that the authors change all raw microscopy images to a different color combination.</p>
<p>(2) For all of the velocity vs displacement graphs (Figure 3C and subpart G of every supplemental figure), there is a diagonal line clearly defining a minimum limit of detected movement. Is this a feature of the dataset (drift /shakiness /etc) or some sort of minimum movement threshold in the tracking algorithm? This should be discussed in the text.</p>
<p>(3) Integrated Correlation Summary (Figure 5) - Pearson is likely the wrong metric for most of these metric pairs because even interesting relationships may be non-linear. Please replace with Spearman correlation, which is less dependent on linearity.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109936.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bayati</surname>
<given-names>Armin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5960-0479</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Schumacher</surname>
<given-names>Jackson G</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0000-4227-4710</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Xiqun</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8582-8891</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1</bold></p>
</disp-quote>
<p>We thank the reviewer for their thoughtful and constructive assessment of AutoMorphoTrack and for recognizing its potential utility as an open-source end-to-end workflow for organelle analysis.</p>
<disp-quote content-type="editor-comment">
<p>(1) Novelty and relationship to existing tools / FIJI workflows</p>
</disp-quote>
<p>We appreciate this concern and agree that many of the underlying image-processing operations (e.g., thresholding, morphological cleanup, region properties) are well-established. Our goal with AutoMorphoTrack is not to introduce new segmentation algorithms, but rather to provide a curated, reproducible, and extensible end-to-end workflow that integrates segmentation, morphology, tracking, motility, and colocalization into a single, transparent pipeline tailored for live-cell organelle imaging.</p>
<p>While an experienced user could assemble similar analyses ad hoc using FIJI or custom scripts, our contribution lies in:</p>
<p>Unifying these steps into a single workflow with consistent parameterization and outputs</p>
<p>Generating standardized, publication-ready visualizations and tables at each step,</p>
<p>Enabling batch and longitudinal analyses across cells and conditions, and</p>
<p>Lowering the barrier for users who do not routinely write custom analysis code.</p>
<p>We note that the documentation-style presentation of the manuscript is intentional, as it serves both as a methods paper and a practical reference for users implementing the workflow. We agree, however, that the manuscript currently overemphasizes step-by-step execution at the expense of positioning. In revision, we will more explicitly frame AutoMorphoTrack as a workflow integration and usability contribution, rather than a fundamentally new algorithmic advance.</p>
<p>We will also cite and discuss the image.sc example referenced by the reviewer, clarifying conceptual overlap and differences in scope.</p>
<disp-quote content-type="editor-comment">
<p>(2) Comparison to existing pipelines (Imaris, CellProfiler, CellPose, StarDist)</p>
</disp-quote>
<p>We agree and thank the reviewer for highlighting this omission. In the revised manuscript, we will expand the related-work and positioning section to explicitly compare AutoMorphoTrack with established commercial (e.g., Imaris) and open-source (e.g., CellProfiler, MiNA, MitoGraph) platforms, as well as learning-based segmentation tools such as CellPose and StarDist.</p>
<p>Rather than claiming superiority, we will clarify trade-offs, emphasizing that AutoMorphoTrack prioritizes:</p>
<p>Transparency and parameter interpretability,</p>
<p>Lightweight dependencies suitable for small live-imaging datasets</p>
<p>Direct integration of morphology, tracking, and colocalization in a single workflow, and</p>
<p>Ease of modification for domain-specific use cases.</p>
<disp-quote content-type="editor-comment">
<p>(3) AI / chatbot integration</p>
</disp-quote>
<p>We appreciate this critique and agree that the current description is insufficiently precise. AutoMorphoTrack does not implement a native natural-language interface. Instead, our intent was to convey that the workflow can be executed and modified with assistance from external large language models (LLMs) in a notebook-based environment.</p>
<p>In revision, we will revise this section to:</p>
<p>Clearly distinguish AutoMorphoTrack’s functionality from that of external LLM tools,</p>
<p>Remove any implication of a built-in AI interface, and</p>
<p>Provide concrete, reproducible examples of how non-coding users may interact with the pipeline using natural-language prompts mediated by external tools.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2</bold></p>
</disp-quote>
<p>We thank the reviewer for their detailed and technically rigorous evaluation. We appreciate the recognition of the workflow’s motivation and structure, and we agree that several aspects of validation, positioning, and quantitative reporting must be strengthened.</p>
<disp-quote content-type="editor-comment">
<p>(1) AI-assisted / natural-language functionality</p>
</disp-quote>
<p>We agree with this critique. AutoMorphoTrack does not provide a native natural-language execution layer, and the manuscript currently overstates this aspect. In revision, we will explicitly scope any reference to AI assistance as external, optional support for code generation and parameter editing, with clearly documented examples and stated limitations.</p>
<p>We agree that conflating external LLM capabilities with the software itself risks misleading readers, and we will correct this accordingly.</p>
<disp-quote content-type="editor-comment">
<p>(2) Lack of quantitative validation</p>
</disp-quote>
<p>We fully agree that the current manuscript lacks formal quantitative validation. In the revised version, we will add a dedicated validation section including:</p>
<p>Segmentation accuracy compared to expert annotations using overlap metrics (e.g., Dice / IoU),</p>
<p>Tracking fidelity assessed using manually annotated tracks and/or synthetic ground truth,</p>
<p>Sensitivity analyses for key parameters (e.g., thresholding and linking distance), and</p>
<p>Explicit discussion of failure modes and quality-control indicators.</p>
<p>We acknowledge that without such validation, claims of robustness are not sufficiently supported.</p>
<disp-quote content-type="editor-comment">
<p>(3) Benchmarking and positioning relative to existing tools</p>
</disp-quote>
<p>We agree and will substantially strengthen AutoMorphoTrack’s benchmarking and positioning relative to existing platforms. Rather than framing novelty algorithmically, we will clarify that the primary contribution is a reproducible, integrated workflow designed specifically for two-organelle live imaging in neurons, with transparent parameters and standardized outputs.</p>
<p>We note that our goal is not to exhaustively benchmark against all available tools, but rather to provide representative comparisons that clarify operating regimes, assumptions, and trade-offs. We will add a comparative table and/or qualitative comparison highlighting strengths, assumptions, and limitations relative to existing tools.</p>
<disp-quote content-type="editor-comment">
<p>(4) Core algorithms and robustness</p>
</disp-quote>
<p>We agree that reliance on threshold-based segmentation introduces sensitivity to imaging conditions. In revision, we will:</p>
<p>Explicitly discuss the operating regime and assumptions under which AutoMorphoTrack performs reliably,</p>
<p>Clarify that the framework is modular and can accept alternative segmentation backends, and</p>
<p>Include guidance on when outputs should be treated with caution.</p>
<disp-quote content-type="editor-comment">
<p>(5) Figure, metric, and statistical issues</p>
</disp-quote>
<p>We thank the reviewer for identifying several critical issues and agree that these undermine confidence. In revision, we will correct all figure, metric-definition, and reporting inconsistencies, including:</p>
<p>Resolving circularity values exceeding 1 by correcting computation and/or labeling errors,</p>
<p>Revising physically invalid displacement plots and clarifying kernel-density limitations,</p>
<p>Ensuring colocalization metrics are consistently defined, named, and interpreted, with explicit clarification of whether calculations are intensity- or mask-based,</p>
<p>Correcting figure legends to match displayed panels, and</p>
<p>Clearly reporting sample size, sampling units, and statistical assumptions, including handling of multiple comparisons where applicable.</p>
<disp-quote content-type="editor-comment">
<p>(6) Value-added demonstration</p>
</disp-quote>
<p>We agree that the manuscript would benefit from a clearer demonstration of value-added use cases. In revision, we will include at least one realistic example showing how AutoMorphoTrack enables a complete, reproducible analysis workflow with reduced setup burden compared to manually assembling multiple tools.</p>
<disp-quote content-type="editor-comment">
<p>(7) Editorial suggestions</p>
</disp-quote>
<p>We agree and will streamline the Results section to reduce procedural repetition and focus more on validation, limitations, and quality-control guidance.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3</bold></p>
</disp-quote>
<p>We thank the reviewer for their positive assessment of clarity and organization, and for the constructive practical feedback.</p>
<disp-quote content-type="editor-comment">
<p>Installation issues</p>
</disp-quote>
<p>We appreciate the detailed report of installation failures and acknowledge that the current packaging and distribution are inadequate. Prior to revision, we will:</p>
<p>Fix the package structure to support standard installation methods,</p>
<p>Ensure all required files (e.g., setup configuration, README) are correctly included,</p>
<p>Test installation on clean environments across platforms, and</p>
<p>Correct broken links to notebooks and documentation.</p>
<p>We agree that without a functional installation pathway, the utility of the tool is severely limited.</p>
<disp-quote content-type="editor-comment">
<p>AI-assisted claims</p>
</disp-quote>
<p>We agree with the reviewer and echo our responses above. The AI-assisted description will be clarified and appropriately scoped in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Additional suggestions</p>
</disp-quote>
<p>Color accessibility: We will revise all figures to use colorblind-safe palettes.</p>
<p>Velocity–displacement diagonal: We will explicitly explain the origin of this relationship, including whether it reflects dataset properties, tracking assumptions, or minimum detectable motion.</p>
<p>Integrated correlation metric: We agree that Spearman correlation is more appropriate for many of these relationships and will replace Pearson correlations accordingly.</p>
<p>Supplementary movies: We agree that providing raw movies would improve interpretability and will add representative examples as supplementary material.</p>
</body>
</sub-article>
</article>