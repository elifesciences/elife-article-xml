<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108329</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108329</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108329.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Ecology</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title><italic>vassi</italic> – verifiable, automated scoring of social interactions in animal groups</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6352-3560</contrib-id>
<name>
<surname>Nührenberg</surname>
<given-names>Paul</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>paul.nuehrenberg@uni-konstanz.de</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bose</surname>
<given-names>Aneesh PH</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6131-9734</contrib-id>
<name>
<surname>Jordan</surname>
<given-names>Alex</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026stee22</institution-id><institution>Behavioural Evolution Research Group, Max Planck Institute of Animal Behavior</institution></institution-wrap>, <city>Konstanz</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>2</label><institution>International Max Planck Research School</institution>, <city>Radolfzell</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0546hnb39</institution-id><institution>Centre for the Advanced Study of Collective Behavior, University of Konstanz</institution></institution-wrap>, <city>Konstanz</city>, <country country="DE">Germany</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02yy8x990</institution-id><institution>Department of Wildlife, Fish &amp; Environmental Studies, Swedish University of Agricultural Sciences (SLU)</institution></institution-wrap>, <city>Umeå</city>, <country country="SE">Sweden</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3588-7820</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Wassum</surname>
<given-names>Kate M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution>
</institution-wrap>
<city>Los Angeles</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-11-03">
<day>03</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108329</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-07-15">
<day>15</day>
<month>07</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-07-21">
<day>21</day>
<month>07</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.07.15.664909"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Nührenberg et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Nührenberg et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108329-v1.pdf"/>
<abstract>
<p>Behavioral biologists, from neuroscientists to ethologists, rely on observation and scoring of behavior. In the past decade, numerous methods have emerged to automate this scoring through machine learning approaches. Yet, these methods are typically specified towards laboratory settings with only two animals, or employed in cases with well-separated behavioral categories. Here, we introduce the <italic>vassi</italic> Python package, focusing on supervised classification of directed social interactions and cases in which continuous variation in behavior means categories are less distinct. Our package is broadly applicable across species and social settings, including single individuals, pairs and groups, and implements a validation tool to separate behavioral edge cases. <italic>vassi</italic> has comparable performance to existing approaches on a behavioral classification benchmark, the CALMS21 mouse resident-intruder dataset, and we demonstrate its applicability on a novel, more naturalistic and complex dataset of cichlid fish groups. Our approach highlights future challenges in extending supervised behavioral classification to more naturalistic settings, and offers a methodological framework to overcome these challenges.</p>
</abstract>
<abstract abstract-type="plain-language-summary">
<title>Lay Summary</title>
<p><italic>vassi</italic> (<italic>verifiable, automated scoring of social interactions</italic>) is a flexible, Python-based framework for automated behavioral classification and its verification through interactive visualization. <italic>vassi</italic> enables researchers to quantify directed social interactions in animal groups in naturalistic settings, bridging the gap between traditional ethology and modern computational tools.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The study of animal behavior has wide-ranging impacts across fields including conservation, animal welfare, neuroethology, genetics, and evolutionary biology. At the intersection of these fields lies the discipline of ethology, which at its core is concerned with the phenomenological, causal, ontogenetic, and evolutionary aspects of behavior in wild, freely moving animals (<xref ref-type="bibr" rid="c48">Tinbergen, 1963</xref>). Traditionally, ethologists have recorded behavioral events and intervals during experiments or observations using pen, paper, and a stopwatch (<xref ref-type="bibr" rid="c2">Altmann, 1974</xref>; <xref ref-type="bibr" rid="c5">Barton &amp; Johnson, 1990</xref>), an approach that can be error-prone for various reasons. Observing multiple animals simultaneously requires humans to multitask, which is typically associated with performance costs (<xref ref-type="bibr" rid="c36">Poljac et al., 2018</xref>). The collection of temporal measurements of behavior during live observations is also likely affected by the speed-accuracy tradeoff, a well-studied phenomenon in humans and across taxa (e.g., reviewed in <xref ref-type="bibr" rid="c23">Heitz, 2014</xref>). Jeanne <xref ref-type="bibr" rid="c2">Altmann (1974)</xref> highlighted the challenges of manual behavioral sampling and scoring, sharing an anecdote that emphasized these difficulties: <italic>“[…] with two observers, one 15-minute sample per hour was near the upper limit of our capacity when obtaining an accurate record, with some 5 dozen social behavior categories, of who did what to whom and in what order, as well as keeping track of most nonsocial behavior, durations, and time-out periods.”</italic> Even when human observers are able to keep track of complex animal interactions, there can be considerable variation among observers, particularly with respect to start and stop times (<xref ref-type="bibr" rid="c41">Segalin et al., 2021</xref>). Moreover, animal behaviors may vary at subtle levels that are difficult for observers to reliably recognize or measure – they may occur at very short or very long time-scales (<xref ref-type="bibr" rid="c6">Berman, 2018</xref>), exist in sensory modalities outside of human scope (e.g., olfaction; <xref ref-type="bibr" rid="c32">Nielsen et al., 2015</xref>), or comprise signals that are too complex for an observer to interpret directly (<xref ref-type="bibr" rid="c33">Patricelli &amp; Hebets, 2016</xref>). While some of these difficulties can be overcome in the laboratory through careful experimental design, observation of animals in more natural conditions, for example in larger groups, in complex environments, or indeed under natural conditions, can compound these issues.</p>
<p>In response to these challenges the sub-field of computational ethology has emerged, in which researchers employ data-driven methods to measure and analyze the behavior of animals (outlined in <xref ref-type="bibr" rid="c4">Anderson &amp; Perona, 2014</xref>). Powered by the rapid expansion of video recording as a methodology, supervised machine-learning algorithms can be trained on trajectory, posture, and video data of animals to automatically detect and classify behaviors of interest (implemented in tools like JAABA, SimBA, MARS or A-SOiD; <xref ref-type="bibr" rid="c25">Kabra et al., 2013</xref>; <xref ref-type="bibr" rid="c18">Goodwin et al., 2024</xref>; <xref ref-type="bibr" rid="c41">Segalin et al., 2021</xref>; <xref ref-type="bibr" rid="c47">Tillmann et al., 2024</xref>), leveraging data from recent methods for single and multi-animal detection, tracking and posture estimation that achieve unprecedented detail in raw measurements of animal movement (e.g., DeepLabCut, DeepPoseKit, idtracker.ai or sleap; <xref ref-type="bibr" rid="c31">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="c20">Graving et al., 2019</xref>; <xref ref-type="bibr" rid="c39">Romero-Ferrero et al., 2019</xref>; <xref ref-type="bibr" rid="c35">Pereira et al., 2022</xref>). In brief, a workflow to automate behavioral scoring may consist of the following steps: Keypoints that mark an animal’s body parts can be precisely tracked throughout a video, describing the animal’s posture over time. This data can then be used to compute a high-dimensional description of the animal’s behavior (i.e., spatiotemporal and postural features; implemented for example in JAABA or SimBA). Additional features can also be computed in relation to other tracked animals to capture key aspects of social behavior, such as relative body poses, distances and angles. Subsequently, a machine-learning model (e.g., a decision tree classifier or an artificial neural network) may be fitted to this high-dimensional data, with known behavioral labels as the target variable (supervised learning, see e.g., <xref ref-type="bibr" rid="c4">Anderson &amp; Perona, 2014</xref>). Such models trained on ground-truth behavioral data can then be applied to classify novel tracking-derived data, typically from members of the same species and in the same study contexts. Hence, researchers only need to manually score a small, but representative subsample of their entire video data, and can apply a fitted model on the remaining, potentially much larger dataset. This workflow eliminates some of the problems that are associated with manual scoring, such as errors due to scorer fatigue, or scorer bias and disagreement in the case of multiple scorers, while achieving or even exceeding the scoring accuracy of expert annotators (e.g., <xref ref-type="bibr" rid="c41">Segalin et al., 2021</xref>).</p>
<p>Nevertheless, these approaches face their own challenges (outlined in <xref ref-type="bibr" rid="c26">Kennedy, 2022</xref>), many of which are exacerbated as datasets become more complex with multiple animals in naturalistic settings. To facilitate the scoring, analysis and communication of animal behavior, researchers often impose distinct categories on potentially continuous behavior, which can result in disagreement as to when a behavior starts and stops, or when one behavioral category transitions to another (<xref ref-type="bibr" rid="c17">Gomez-Marin et al., 2014</xref>; <xref ref-type="bibr" rid="c41">Segalin et al., 2021</xref>). The natural variation in timescales over which behavior occurs can lead to inconsistent scoring of behavioral states, both within and among research groups, which in turn can lead to inconsistent classification by supervised models trained on these data (see <xref ref-type="bibr" rid="c42">Siegford et al. (2023)</xref> for a review on factors that impede effective automated behavioral analyses). These problems do not necessarily imply a drawback of the computational methodology, but rather highlight characteristics of the data – a behavioral state may correspond to a specific function (or ‘meaning’; <xref ref-type="bibr" rid="c38">Rendall et al., 2009</xref>) despite its underlying continuous variation, and the same or divergent forms (or postures; <xref ref-type="bibr" rid="c6">Berman, 2018</xref>) may have different effects on conspecifics depending on context or history. In African cichlid fish for example, a lateral posture can be used among rival males as part a threat display, or between males and females as part of a courtship display, and shares an underlying genetic architecture (<xref ref-type="bibr" rid="c3">Alward et al., 2020</xref>). As such, behavioral categorization may not only depend on the kinematic structure, but also on the identity of the participants themselves. Automated methods may therefore need to resolve the identity and directionality of social behaviors between particular actors and recipients, and depending on the context, may need to account for interactions among many individuals within a group. Perhaps because of these potential difficulties in generating robust behavioral categories, combined with a strong focus on model species, many available approaches are highly specified towards certain conditions (commonly two interacting animals in simple experimental arenas), which may complicate the generalization of such approaches and their implementation across contexts (<xref ref-type="bibr" rid="c4">Anderson &amp; Perona, 2014</xref>; <xref ref-type="bibr" rid="c19">Goodwin et al., 2020</xref>; <xref ref-type="bibr" rid="c26">Kennedy, 2022</xref>).</p>
<p>As the call for more naturalistic experiments becomes stronger (<xref ref-type="bibr" rid="c8">Bordes et al., 2023</xref>; <xref ref-type="bibr" rid="c43">Smith &amp; Pinter-Wollman, 2021</xref>), so too does the need for adequate quantitative tools. Here, we present the <italic>vassi</italic> Python package (<italic>verifiable, automated scoring of social interactions</italic>), enabling researchers to perform behavioral classification outside the boundaries of the existing tool-landscape. With continuous advances in animal tracking tools, such as their expansion to naturalistic settings (with aerial drones or SCUBA diving; <xref ref-type="bibr" rid="c27">Koger et al., 2023</xref>; <xref ref-type="bibr" rid="c15">Francisco et al., 2020</xref>), researchers can generate more complex tracking datasets by increasing variables such as the number of tracked animals or the diversity of behavioral interactions. With our package, we provide software that is applicable to such diverse use-cases; <italic>vassi</italic> does not impose constraints on the number of individuals that may socially interact in a group, and allows measurement of the direction of behavioral interaction. The package also allows users to set or automatically tune the confidence threshold for behavioral categories, and to review the resulting behavioral predictions to verify their correctness. The latter aspect becomes essential in cases where behavioral categories themselves are not easily distinguishable and require further downstream verification of potentially important edge-cases. We validated our package on an existing benchmark dataset, the CALMS21 dataset of dyadic mouse interactions, demonstrating classification performance on par with other, more specialized software published alongside the benchmark (<xref ref-type="bibr" rid="c44">Sun et al., 2021</xref>). We further tested the package on a novel dataset of larger groups of freely interacting fish, showcasing its applicability in more naturalistic and complex scenarios with subtle behavioral variation.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>CALMS21 dataset</title>
<p>We validated the <italic>vassi</italic> package on an established benchmark for behavioral classification, the CALMS21 dataset (<xref ref-type="bibr" rid="c44">Sun et al., 2021</xref>). This dataset consists of individually tracked dyadic resident-intruder interactions in mice and corresponding behavioral annotations that assign each video frame to one of four categories (‘attack’, ‘investigate’, ‘mount’, or ‘other’). We trained an XGBoost classifier (<xref ref-type="bibr" rid="c9">Chen &amp; Guestrin, 2016</xref>) to distinguish between these categories using the data processing and sampling pipeline implemented by our package (see methods section for details, XGBoost classifier with default parameters, number of boosting rounds set to 1000). We used this classification model for predictions on the CALMS21 test dataset (<xref rid="fig1" ref-type="fig">Figure 1</xref>). Then, we evaluated the model on this dataset by computing macro F1 scores, a commonly used performance metric for classification on imbalanced datasets that is calculated as the harmonic mean of precision and recall for each category, and then averaged across categories (<xref ref-type="bibr" rid="c13">Fawcett, 2006</xref>). Our base model achieved a per-frame macro F1 score of 0.804 ± 0.001 (mean and standard deviation of 20 pipeline runs with different random states) on the three foreground categories (‘attack’, ‘investigate’ and ‘mount’), and 0.840 ± 0.001 on all four categories (<xref rid="fig1" ref-type="fig">Figure 1C and D</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>CALMS21 test dataset results.</title> <p>Behavioral categories are abbreviated: ‘attack’ – ‘att’, ‘investigation’ – ‘inv’ and ‘mount’ – ‘mnt’. All values represent means of 20 pipeline runs with different random states, standard deviations are shown as error bars if applicable. <bold>A:</bold> Behavioral timeline with ground-truth intervals (annotations, lower bars, blue) and predictions (upper bars, orange) for the four behavioral categories. Lines represent per-category model outputs (classification probabilities after smoothing). The last resident-intruder sequence of the test dataset is visualized, see <xref ref-type="fig" rid="figS5">SI Figure 5</xref> for all 19 test dataset sequences. <bold>B:</bold> Per-frame confusion matrix of the four behavioral categories. Within each cell, upper values show the proportion of frames in agreement with annotated, ground-truth data (normalized across each row), lower values show the absolute frame counts. Results were visualized after model output smoothing and thresholding. <bold>C and D:</bold> Per-frame unweighted average (macro) F1 scores of raw model outputs (‘model’), after smoothing (‘smooth’), and after thresholding (‘thresh’), across only the behavioral foreground categories and across all four categories. <bold>E:</bold> Per-frame F1 scores for each category, calculated on raw model outputs, after smoothing, and after thresholding. <bold>C and E:</bold> Horizontal lines mark the F1 scores of the baseline model as reported in <xref ref-type="bibr" rid="c44">Sun et al. (2021)</xref>.</p></caption>
<graphic xlink:href="664909v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We then transformed the raw model outputs (i.e., classification probabilities for each category and video frame) into accessible data in tabular form as behavioral events between a start and stop timestamp. We used two post-processing steps implemented by our package for this data transformation, temporal smoothing of model outputs and thresholding (see methods section for details). This post-processing entailed two sets of so-called hyperparameters that can affect predictive performance: Firstly, the kind (e.g., mean or median) and size of a temporal smoothing window when applying a filter to model outputs, and secondly, category-specific decision thresholds. We conducted hyperparameter optimization to find optimal values for these additional parameters. Smoothing model outputs using these values improved the quality of classifications (per-frame F1 score of 0.838 ± 0.001 on foreground categories, 0.866 ± 0.001 on all categories), outperforming the primary results without smoothing (<xref rid="fig1" ref-type="fig">Figure 1C and D</xref>). Thresholding these smoothed results with category-specific decision thresholds yielded an additional gain in classification performance on the CALMS21 test dataset (per-frame F1 score of 0.840 ± 0.001 on foreground categories, 0.867 ± 0.001 on all categories; <xref rid="fig1" ref-type="fig">Figure 1C and D</xref>).</p>
<p>We additionally computed these F1 scores for raw model outputs, smoothed results and thresholded results on two other levels, i.e., the annotated behavioral intervals and the predicted intervals (count of behavioral interactions, <xref rid="figS1" ref-type="fig">SI Figure 1</xref>). We found that smoothing model outputs lowered the performance metrics on the test dataset when assessing classification performance based on annotated intervals (<xref rid="figS1" ref-type="fig">SI Figure 1B and C</xref>). This was likely balanced during hyperparameter optimization by a strongly increased classification performance after smoothing when focusing on the predicted intervals (as shown in <xref rid="figS1" ref-type="fig">SI Figure 1E and F</xref> for the test dataset). Category-specific thresholding could partially recover the annotation-based F1 scores, but lowered classification performance when focusing on predicted behavioral intervals. See <xref rid="tblS1" ref-type="table">SI Table 1</xref> for detailed results.</p>
</sec>
<sec id="s2b">
<title>Social cichlids dataset</title>
<p>We also tested our package on a novel dataset of social interactions in groups of cichlid fish. This dataset presented a more complex scenario, requiring not only accurate classification of dyadic behaviors but also proper disentanglement of interactions among multiple individuals, each engaging with only one partner at a time. The social cichlids dataset presents further challenges due to a higher number of behavioral foreground categories (six categories, see <xref rid="tbl1" ref-type="table">Table 1</xref>) and through the types of annotated social behaviors. These include behavioral categories that, although distinctly defined, may overlap in spatiotemporal features. For example, the categories ‘approach’, ‘bite/dart’ and ‘chase’ all entail that the focal individual moves directedly towards a recipient fish. Threat or submissive displays (‘frontal display’, ‘lateral display’ and ‘quiver’) do not require direct contact between individuals, and are generally more inconspicuous than other categories such as ‘chase’. Nevertheless, distinct behaviors that overlap kinematically and can be performed between actors and recipients over long distances represent natural behavioral variation that is valuable to quantify.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Ethogram of <italic>N. multifasciatus</italic> that was used for the behavioral scoring of the social cichlids dataset.</title></caption>
<graphic xlink:href="664909v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>To train a model that can distinguish these behaviors from each other and from the behavioral background category ‘none’, we fitted an XGBoost classifier on the training subset of the social cichlids dataset (again, with number of boosting rounds set to 1000, following the procedure outlined in the methods section). We also optimized hyperparameters for the post-processing of classification results (temporal smoothing and thresholding). Similar as described above, we assessed classification performance with F1 scores on three levels: video frames, annotated intervals and predicted intervals. However, in contrast to the CALMS21 classification, we focused mainly on behavior counts (i.e., predicted intervals) instead of per-frame results, as annotations were much sparser in this dataset (as visualized in <xref rid="fig2" ref-type="fig">Figure 2A</xref>). We found that classification performance on this dataset was lower overall (see <xref rid="fig3" ref-type="fig">Figure 3B and C</xref>; macro F1 across all categories: 0.217 ± 0.004), but improved with both model output smoothing (0.375 ± 0.007) and thresholding (0.453 ± 0.008). Classification errors mainly occurred through false positive predictions when the true category was the behavioral background category ‘none’, and much less between the behavioral foreground categories (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). See <xref rid="figS2" ref-type="fig">SI Figure 2</xref>, <xref rid="figS3" ref-type="fig">SI Figure 3</xref> and <xref ref-type="table" rid="tblS2">SI Table 2</xref> for additional results based on video frames and annotated intervals.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Qualitative classification results of the social cichlids dataset.</title> <p><bold>A:</bold> Behavioral timelines for one focal fish (actor) of the test dataset and its three most frequent interaction partners (recipients). Colored bars denote ground-truth intervals (annotations, lower bars, blue) and predictions (upper bars, orange) for the six behavioral categories. Lines represent per-category model outputs (classification probabilities after smoothing). For recipient 2 and 3, intervals and lines are offset along both x and y axes. The behavioral background category was excluded for a clearer visualization with sparse behavioral data. <bold>B and C:</bold> Annotated and predicted interaction network of one group (15 fish), split by behavioral category. Edge line strength represents interaction counts. Note that this visualization contains data that is not part of the test dataset since the full dataset was split by individual fish and not groups. For visualization purposes, we instead used 5-fold cross validation to fit five independent classifiers that were used for predictions on the full dataset. See <xref rid="figS4" ref-type="fig">SI Figure 4</xref> for correlation tests between annotated and predicted behavior counts of all dyads from the test dataset. Note: The actor and three recipients of the behavioral timelines are part of the interaction networks, marked by node color (black and grey for actor and recipients, respectively).</p></caption>
<graphic xlink:href="664909v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Classification results of the social cichlids test dataset.</title> <p>All values represent means of 20 pipeline runs with different random states, standard deviations are shown as error bars if applicable. <bold>A:</bold> Confusion matrix of predicted intervals and their true category (i.e., category of annotated interval with longest overlap). <bold>B:</bold> Unweighted average (macro) F1 score of raw model outputs (‘model’), after smoothing (‘smooth’), and after thresholding (‘thresh’), calculated across all behavioral categories on predicted intervals. <bold>C:</bold> Per-category F1 scores for each of the same (post-)processing steps. Note that all results were computed based on predicted behavioral intervals (i.e., interaction counts). For both video frame and annotation interval based results, see <xref rid="figS2" ref-type="fig">SI Figure 2</xref> and <xref rid="figS3" ref-type="fig">SI Figure 3</xref>, respectively.</p></caption>
<graphic xlink:href="664909v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Despite the relatively high rates of false positive predictions in this test scenario, we found that our automated behavioral scoring pipeline could still provide otherwise inaccessible data, both in quantity (e.g., when applying classifiers on new videos) and quality. For example, interaction networks of the recorded groups based on either annotated or predicted behavior counts were visually similar (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). To test if our model predictions were reliable proxies for observed behaviors (i.e., ground-truth annotations), we tested the correlation between these two networks for each behavioral category by correlating annotated and predicted behavior counts across all dyads of the designated test dataset. We found moderate to strong correlations between annotated and predicted counts for most categories (Pearson correlation coefficients; lowest for ‘quiver’, r = 0.19, and chase, r = 0.46, all other between 0.56 and 0.76, see <xref ref-type="table" rid="tblS3">SI Table 3</xref>).</p>
<p>Other approaches to quantify social interactions in animal groups often use behavioral proxies such as the cumulative time spent within a certain distance (e.g., association networks; <xref ref-type="bibr" rid="c10">Davis et al., 2018</xref>). However, these proxies cannot capture whether social interactions occur, nor the types of interactions that occur, when animals are in proximity to each other. We compared our classification method to such a non-qualitative proxy – association time, defined as the cumulative duration that individuals spent within three body lengths of one another. For this, we tested if ground-truth behavior counts correlated more strongly with counts resulting from classifier predictions than with association time, accounting for non-independence of both correlations using the William’s test with a shared variable (psych package; William <xref ref-type="bibr" rid="c51">Revelle, 2024</xref>; R v4.4.3; <xref ref-type="bibr" rid="c37">R Core Team, 2025</xref>). We found that this was the case for all six behavioral foreground categories (P &lt; 0.001 for all categories). See <xref rid="figS4" ref-type="fig">SI Figure 4</xref> for a visualizations of these correlations and <xref ref-type="table" rid="tblS3">SI Table 3</xref> for statistical results and a sensitivity analysis for alternative proximity thresholds.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Novel methods in computational ethology have proven invaluable in describing the structure of animal behavior. With the advent of animal tracking approaches that can gather high-resolution posture and movement data in the wild (<xref ref-type="bibr" rid="c11">Dell et al., 2014</xref>), these methods can now be applied in contexts closer to the original goal of ethology, that is to measure the behavior of freely interacting animals in natural contexts. Here we implemented and tested the <italic>vassi</italic> Python package to automatically score the directed social behavior of multiple interacting animals. With our package, we were able to achieve classification results on a benchmark dataset comparable to those from existing tools (e.g., outperforming the baseline model that was published alongside the CALMS21 dataset; <xref ref-type="bibr" rid="c44">Sun et al., 2021</xref>). Additionally, we introduced a more naturalistic and complex test case with groups of cichlid fish (15 individuals per group). This dataset possesses properties that are commonly associated with <italic>in-situ</italic> observations of animals – directed interactions (i.e., incoming and outgoing social behaviors) between multiple individuals, intermittent and noisy trajectory data (e.g., fish can freely enter shelters or visually occlude each other), and continuous behavior types ranging from conspicuous aggression involving contact between individuals to subtle threat or submission displays. Testing our package on this novel dataset, we showed that the implemented behavioral classification workflow generalizes across use-cases, including different social settings (pairs versus groups) and species (mice and fish). Albeit having a lower classification performance on the cichlids data, we were able to validate the applicability of our workflow to this more complicated dataset and use the inbuilt reviewing process to verify and update ambiguous cases at need. For this, we implemented an interactive tool to inspect behavioral datasets that can be used to display video sequences of behavioral interactions, both ground-truth annotations and model predictions. Misclassifications occurred mainly between the behavioral foreground categories and the background category through false positive predictions, which may introduce systematic errors into behavioral datasets resulting from such classification workflows. However, when comparing our classification results to an alternative behavioral proxy that is commonly used for within-group behavioral interactions – association time (see e.g., <xref ref-type="bibr" rid="c50">Whitehead, 2008</xref>), we found that the automatically scored behaviors provide a more accurate, and importantly, a categorical representation of behavioral interactions. Proximity or association time alone, for example, are often not sufficient to capture relevant interaction networks (discussed in <xref ref-type="bibr" rid="c12">Farine, 2015</xref>). We further demonstrated that the post-processing routine of model output smoothing and subsequent thresholding is robust and improved classification performance on both datasets.</p>
<p>Our package does not pose any restrictions on the directionality of interactions or number of interacting individuals, and both tracking and behavioral data can be used without specific pre-processing strategies. Users can import individual trajectories and assign them to groups (and encompassing, datasets), which in turn can be annotated with behavioral observations from tools such as BORIS (<xref ref-type="bibr" rid="c16">Friard &amp; Gamba, 2016</xref>). Since our package is primarily scripting-oriented, users can load their raw data (e.g., trajectories from a CSV file), and our package then provides all functionality that is required to train classification models on this data. Similarly, users can further apply these models to new data and score their entire datasets in an automated way. Classification results are provided in tabular form, similar to the behavioral annotations that users are familiar with. This design integrates with existing Python libraries for machine learning such as scikit-learn and XGBoost (<xref ref-type="bibr" rid="c9">Chen &amp; Guestrin, 2016</xref>; <xref ref-type="bibr" rid="c34">Pedregosa et al., 2011</xref>), which makes the pipeline that we implemented in the <italic>vassi</italic> package extensible and open for future innovation in this field. For example, the process of model fitting is not hidden behind multiple layers of code but directly exposed, users can therefore easily exchange, test and evaluate different model types. Similarly, our package provides a broadly applicable feature extraction workflow with spatiotemporal features such as keypoint distances or posture angles. Although the current implementation focuses on the two case studies with data derived from 2D-tracking, this workflow can be extended to 3D data without requiring modifications of the overall classification pipeline.</p>
<p><italic>vassi</italic> provides a compromise between accessibility (i.e., providing a fully featured graphical user interface, GUI) and extensibility or integration with other software (e.g., by providing a well-documented application programming interface, API). The package includes a widget-based GUI to facilitate interactive visualization of classification results to help improve overall classification performance through iterative validation and re-training (e.g., implemented in JAABA and A-SOiD; <xref ref-type="bibr" rid="c25">Kabra et al., 2013</xref>; <xref ref-type="bibr" rid="c47">Tillmann et al., 2024</xref>). This GUI focuses on two main aspects that become essential in more complex behavioral scenarios: First, the inspection of behavioral predictions (or annotations) across entire projects through an interactive, feature-rich table that supports filtering, editing and sorting; and second, the generation of high-quality video snippets of behavioral sequences. This interactive tool can also be used as a standalone application to make large behavioral video datasets accessible, and may help researchers to comply with best practices demanded for this research area (e.g., data transparency and sharing; <xref ref-type="bibr" rid="c30">Luxem et al., 2023</xref>). The remaining functionality of our package (i.e., data handling, feature extraction, model training and predictions, post-processing of classification results, hyperparameter optimization) is accessible via its API that allows users to implement their own behavioral scoring pipelines without the necessity of writing custom code.</p>
<p>Our package is not the first example of a behavioral classification workflow, existing tools such as JAABA, SimBA, MARS or A-SOiD implement similar methods and functionality (<xref ref-type="bibr" rid="c25">Kabra et al., 2013</xref>; <xref ref-type="bibr" rid="c18">Goodwin et al., 2024</xref>; <xref ref-type="bibr" rid="c41">Segalin et al., 2021</xref>; <xref ref-type="bibr" rid="c47">Tillmann et al., 2024</xref>). The general approach is conserved across these existing tools, using animal trajectories (here, always derived from video data) and behavioral annotations to extract spatiotemporal features serving as input to train supervised machine-learning models that can then automatically classify behaviors in further data. Although implementation details differ between these tools, they share two main limitations, firstly the number of individuals that can interact with each other in the video observations (for example, limiting to only one or two behaving individuals), and secondly, a limited perspective of directedness of behavior in dyadic animal interactions (i.e., certain behaviors can only occur in one direction between two individuals). These two limitations preclude studying social behavior as recursive responses to social stimuli between individuals, as naturally occurs in animal groups. Both limitations arise from software design, most of the tools mentioned above treat multiple animals as one ‘entity’ by combining their keypoints in a fixed order before feature computation. This order then imposes the direction in which behaviors can be classified, both MARS and A-SoiD, for example, only consider the behaviors of one individual (the resident mice in the CALMS21 dataset). With these tools, workarounds for classifying behaviors in multi-animal settings, such as reloading projects with switched trajectories, may be feasible for two individuals but become impractical as the number of tracked animals increases. Only JAABA, the oldest of the existing tools, demonstrated its utility on a dataset of animal groups (multiple fruit flies, <italic>Drosophila melanogaster</italic>) by presenting an alternative in which the number of individuals is not restricted, and behavioral classifiers are trained on the dyadic spatiotemporal features of a focal individual and its nearest neighbor in each video frame. This however assumes that interactions always happen between nearest neighbors, which does not generalize to all behavior types, for example threat displays that might be displayed over larger distances.</p>
<p>An alternative pathway for automated behavioral classification uses video data directly. The tools DeepEthogram (<xref ref-type="bibr" rid="c7">Bohnslav et al., 2021</xref>) and DeepAction (C. <xref ref-type="bibr" rid="c21">Harris et al., 2023</xref>) employ convolutional neural networks to extract relevant pixel-based features from video frames, which is then used for behavioral classification. This is an efficient end-to-end approach from videos to behavioral scoring results, however, it causes stronger limitations on the type of videos that can be analyzed when compared to the more common workflow with posture tracking and the calculation of spatiotemporal features. For example, disentangling directed behavioral interactions of two visually similar animals is not possible with this approach, but can be done when both animals are individually tracked. This is especially problematic when analyzing videos of multiple interacting animals for which the classification of directed behavioral interactions would not only require visually distinct individuals, but also that the classification model can learn a representation of each animal and uses this information for its behavioral predictions.</p>
<p>The <italic>vassi</italic> package overcomes these challenges and provides an automated behavioral scoring pipeline from animal tracking data that can be used to classify social and non-social behaviors in various contexts – single individuals, dyadic interactions, and animal groups. As such, the package broadens the applicability of methods that continue to gain importance in the subfields of neurobiology and computational ethology to other research areas of behavioral biology, particularly in cases where behaviors vary along a continuum or in which multiple animals are interacting at once. <italic>vassi</italic> further enables researchers to visualize and validate behavioral events, contributing to the adoption of reproducible, computational methods in behavioral studies, which may help to alleviate the replication crisis in field-based ecological research (<xref ref-type="bibr" rid="c14">Filazzola &amp; Cahill Jr, 2021</xref>). Overall, this approach both identifies emerging challenges in the application of computational methods to more naturalistic settings, and provides a framework for meeting these challenges.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Implementation</title>
<p>The <italic>vassi</italic> package implements a complete data pipeline to automate the scoring of social behavior in animal groups. Users need to prepare raw data in the form of individual trajectories and matching behavioral annotations. This entails tracking one or more keypoints (e.g., body parts) of at least one focal individual, along with corresponding timestamps for when each individual starts and stops a behavioral event or social behavior towards another individual. While pose estimation methods like DeepLabCut or sleap (<xref ref-type="bibr" rid="c31">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="c35">Pereira et al., 2022</xref>) can provide detailed tracking information, event-logging software such as BORIS (<xref ref-type="bibr" rid="c16">Friard &amp; Gamba, 2016</xref>) can be used to obtain the required behavioral data.</p>
<p>As outlined step-by-step below, the package provides multiple core features, including trajectory preprocessing, feature engineering and extraction, dataset sampling for classifier training or validation (e.g., splitting and k-fold for cross-validation), and post-processing of classification results. Additionally, an interactive visual inspection tool enables users to render video snippets of behavioral sequences for both raw input data (trajectories and behavioral annotations) and model predictions (see <xref rid="fig4" ref-type="fig">Figure 4</xref> and SI Video 1). Refer to the online documentation at <ext-link ext-link-type="uri" xlink:href="http://vassi.readthedocs.io/">http://vassi.readthedocs.io/</ext-link> and our GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/pnuehrenberg/vassi">https://github.com/pnuehrenberg/vassi</ext-link>) for example scripts and two case studies.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Interactive validation tool.</title> <p><bold>A:</bold> An interactive table widget enables the inspection and editing of entire behavioral datasets from within the JupyterLab coding environment. Columns can be sorted (with multiple sorting, in the example: 1 – ‘Actor’ – ascending, 2 – ‘Recipient’ – ascending, 3 – ‘Probability’ – descending) and interactively filtered through selection, value ranges or quantile ranges (accessible via respective pop-ups). Active filters are depicted with the blue tick button. All fields are editable, but only allow valid entries if applicable (e.g., existing behavioral categories). Boolean columns are exposed as check boxes. Free text input is also possible, e.g., for comments. Action buttons can link to other widgets, as for example, video playback. <bold>B:</bold> Video playback interface to visualize behavioral sequences. Each row in the interactive table (i.e., annotations or predictions depending on use case) can be used to render a corresponding video, optionally with overlays for tracking data and a behavioral label. Videos are played directly in the interface and can be looped, stopped and maximized. <bold>C – E:</bold> Rendering options to configure video output. In the example, both actor and recipient are highlighted with red and blue, respectively.</p></caption>
<graphic xlink:href="664909v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4b">
<title>Handling trajectory data</title>
<p>Our package provides functionality to handle and process trajectory data. For example, users can load and save of trajectory data from or to structured HDF5 files (a standard for multidimensional numerical data; <xref ref-type="bibr" rid="c45">The HDF Group, n.d.</xref>). Data access and manipulation is facilitated through object-oriented Python structures that provide methods such as temporal resampling, interpolation (e.g., to impute missing tracking data for occluded individuals) or time-window slicing (e.g., to select relevant intervals or temporally align multiple trajectories). These trajectory objects also allow direct data access, implementing common indexing operations similar to the widely-used Python packages NumPy and pandas (C. R. <xref ref-type="bibr" rid="c22">Harris et al., 2020</xref>; <xref ref-type="bibr" rid="c46">The pandas development team, 2024</xref>).</p>
</sec>
<sec id="s4c">
<title>Feature engineering and extraction</title>
<p>Our package offers a user-friendly workflow to calculate spatiotemporal features from individual trajectories or dyads (i.e., ordered trajectory pairs). Features such as ‘posture angles’, ‘keypoint distances’, ‘speed’, or ‘target velocity’ can be configured through a single YAML file (a human-friendly data serialization language), allowing users to reproduce their feature extraction workflows across different datasets or studies. The features are implemented as Python functions with a common set of inputs. For example, ‘keypoint distances’ can be used to calculate within-animal keypoint distances if only one trajectory is specified, or alternatively to calculate between-animal distances with two trajectories specified. Users can define these features as either ‘individual’ or ‘dyadic’ in their configuration and only need to provide the relevant keypoints or keypoint pairs. In the case of temporal features, such as ‘speed’ (i.e., displacement over time), users can specify a temporal step over which the feature is calculated. Feature extraction workflows can be further augmented with data transformations such as standardization or discretization, and are compatible with data transformations implemented in the scikit-learn library (<xref ref-type="bibr" rid="c34">Pedregosa et al., 2011</xref>). Our package additionally implements a time-window transformation to compute sliding time-window representations of the extracted features, allowing workflows similar to JAABA or MARS (<xref ref-type="bibr" rid="c25">Kabra et al., 2013</xref>; <xref ref-type="bibr" rid="c41">Segalin et al., 2021</xref>)</p>
</sec>
<sec id="s4d">
<title>Training classifiers, prediction and post-processing</title>
<p>To simplify handling entire datasets that combine tracking data with behavioral annotations, our package provides functionality through Python objects for common operations such as data validation, sampling, or subsampling. These objects reflect the inherent structure of such datasets: ‘Groups’ are initialized with trajectories and focus on either ‘individuals’ (for non-social behavior) or ‘dyads’ (for social behavior), a ‘dataset’ consists of at least one of such groups. Users can annotate each of these data objects with intervals of one or more behavioral categories. Using the feature extraction workflow described above, sampling annotated datasets results in a feature table with corresponding target categories, while sampling unannotated datasets returns feature tables alone, suitable for the classification of new data or for unsupervised learning. This design integrates seamlessly with machine learning libraries like scikit-learn or XGBoost (<xref ref-type="bibr" rid="c34">Pedregosa et al., 2011</xref>; <xref ref-type="bibr" rid="c9">Chen &amp; Guestrin, 2016</xref>), allowing users to train classification models without writing custom code.</p>
<p>Our package enables users to directly apply such classification models on entire datasets or their subcomponents (i.e., groups, individuals, or dyads). This prediction step is made accessible to users through a common code interface: raw model outputs (per-frame category probabilities) are transformed into behavioral predictions by grouping consecutive frames of the same category into intervals. The resulting data is provided in tabular form as pandas DataFrames and can be directly used in down-stream statistical analyses.</p>
<p>Additionally, users can apply two post-processing steps to classification results. Model outputs can be temporally smoothed by applying one or more smoothing methods (for example filters implemented in SciPy’s signal module; <xref ref-type="bibr" rid="c49">Virtanen et al., 2020</xref>), reducing noise in the time-series of category probabilities. This may help to reduce the number of short behavioral predictions and limits the amount of frequent back-and-forth transitions between predicted behavioral categories. In addition, users can specify a probability threshold for each category, only keeping predictions above their category’s respective threshold.</p>
</sec>
<sec id="s4e">
<title>Optimization of hyperparameters</title>
<p>Predictive performance in machine learning often depends on hyperparameters that define model structure and optimization routines, for example decision tree depth or learning rate (<xref ref-type="bibr" rid="c53">Yang &amp; Shami, 2020</xref>). Pre- and postprocessing workflows can also introduce hyperparameters, such as category-specific subsampling frequencies or decision thresholds for classification outputs (<xref ref-type="bibr" rid="c40">Sauer et al., 2024</xref>).</p>
<p>With the <italic>vassi</italic> package, we provide methods to optimize the parameters arising from the two implemented post-processing steps: (I) the amount or duration of temporal smoothing to reduce temporal noise in classification results, and (II) category-specific decision thresholds. For binary classification, this threshold typically defaults to 0.5, while in multi-class tasks the category with the highest probability is selected. However, such general rules may not be optimal for all datasets. Our package enables users to tune these hyperparameters with optuna (a hyperparameter optimization framework; <xref ref-type="bibr" rid="c1">Akiba et al., 2019</xref>) to maximize predictive performance with regard to their data and use case.</p>
</sec>
<sec id="s4f">
<title>Model validation</title>
<p>Users can choose any classification model compatible with the scikit-learn classification API (<xref ref-type="bibr" rid="c34">Pedregosa et al., 2011</xref>). Therefore, comparing classification results across models can help to identify the best-performing model for a given dataset or guide the selection of post-processing routines to improve performance. In classification tasks, recall and precision are key metrics for model evaluation (<xref ref-type="bibr" rid="c24">Juba &amp; Le, 2019</xref>). Recall measures the proportion of true positives correctly identified for a category at a given decision threshold, while precision reflects the proportion of predicted positives that genuinely belong to the target category. Both metrics depend on the decision threshold and are interacting through the relationship between true positive and false positive predictions.</p>
<p>A commonly used composite metric is the F1 score, the harmonic mean of precision and recall. Certain behavioral categories that occur only sparsely in a behavioral dataset may be as critical to classify accurately as other, more common categories. Therefore, the macro F1 score (an unweighted average across categories) can be particularly useful to evaluate classification with imbalanced datasets (e.g., <xref ref-type="bibr" rid="c44">Sun et al., 2021</xref>; <xref ref-type="bibr" rid="c47">Tillmann et al., 2024</xref>). Our package provides methods to compute this macro F1 score for test data on three different levels: (I) per-frame, assessing classification performance based on the cumulative duration of each behavioral category, (II) on annotated behavioral intervals, focusing on the recall of ground-truth intervals, and (III), on predicted behavioral intervals, evaluating the precision of predicted intervals. The two interval-based scores measure performance while considering interval counts rather than cumulative durations.</p>
<p>Another tool for assessing classification performance are confusion matrices, showing true and false predictions and the amount of misclassification between categories. Our package implements methods to compute confusion matrices directly on the classification results of annotated data. Equivalently to the F1 score, confusion matrices can be generated on three levels, per-frame, on annotation intervals, and on prediction intervals.</p>
</sec>
<sec id="s4g">
<title>Visual inspection of behavioral sequences</title>
<p>We provide users with two visual inspection tools for behavioral sequences. The first is a timeline-plot (Gantt chart) to render behavioral sequences for one individual or dyad at a time, also implemented for example in BORIS, SimBA or TIBA (<xref ref-type="bibr" rid="c16">Friard &amp; Gamba, 2016</xref>; <xref ref-type="bibr" rid="c18">Goodwin et al., 2024</xref>; <xref ref-type="bibr" rid="c29">Kraus et al., 2024</xref>). Users can augment this visualization with raw and smoothed model outputs (i.e., classification probabilities for each class) and choose whether to include predicted or annotated intervals, or both.</p>
<p>The second inspection tool is a feature-rich table that enables data filtering, sorting and editing and can be used to render video snippets of behavioral sequences (<xref rid="fig4" ref-type="fig">Figure 4</xref>). Each row in this table describes one behavioral interval, either annotation or prediction, depending on the input. The interactivity allows users to inspect entire datasets, for example, by filtering intervals by category (e.g., showing 2 out of 6 categories), and out of these only keep the bottom 10% by classification performance (applying a conditional quantile filter). A workflow such as this enables efficient validation of behavioral predictions, while, in the example case, focusing on likely misclassifications. Users can add columns to the table, for instance for comments or Boolean indicators (e.g., to mark if a row is considered valid or rejected). From each row, a widget can be opened to render the correspondent video sequence. We enable users to adjust options of the graphical display of these video sequences, for example by selecting the tracked keypoints or segments to visualize, the rendering resolution or cropping the video around actor and recipient individuals. See <italic>SI Video 1</italic> for an exemplary use of this tool.</p>
</sec>
<sec id="s4h">
<title>Case studies</title>
<p>We tested the <italic>vassi</italic> package in two case studies, firstly with the CALMS21 benchmark dataset of mouse resident-intruder interactions (<xref ref-type="bibr" rid="c44">Sun et al., 2021</xref>), and secondly with a more complex dataset of social behavior in groups of cichlid fish. In both cases, we fitted an XGBoost classifier to distinguish between behavioral categories in dyadic social interactions. These categories included defined social behaviors directed from an actor to a recipient (foreground categories) and a background category encompassing all other behaviors.</p>
</sec>
<sec id="s4i">
<title>CALMS21 dataset</title>
<p>The CALMS21 dataset supplies benchmarks for multiple tasks, but we focused solely on the first task ‘classical classification’. This benchmark comprises 89 resident-intruder video sequences, with 19 designated as the test set. We adhered to this split, using the first 70 sequences for model training and hyperparameter tuning. Each video frame is categorized into one of four behavioral categories: three social behaviors (‘attack’, ‘mount,’ and ‘investigation’) as foreground categories, and a fourth (‘other’) as the background category assigned to unlabeled frames. Interactions are only one-directional, from resident to intruder. The dataset also includes posture tracking data for resident and intruder mice, with seven body parts tracked per mouse (keypoints on nose, left and right ear, back of neck, left and right hip, and base of tail). Both the behavioral annotations and tracking data were used without modification.</p>
<p>The CALMS21 dataset contains imbalanced categories (see <xref ref-type="table" rid="tblS4">SI Table 4</xref>) and large datasets can pose computational challenges. To address both, we randomly subsampled the overrepresented categories (‘investigation’ and ‘other’) during classifier training, limited to 30000 samples each. Subsampling was stratified by behavioral intervals to maintain proportional representation of all behavioral annotations. <xref ref-type="table" rid="tblS4">SI Table 4</xref> provides an overview of category frequencies in the training data.</p>
<p>During data sampling, we computed movement and posture features for each selected video frame (see <xref ref-type="table" rid="tblS6">SI Table 6</xref> for a full list). Using this data, we trained an XGBoost classifier with the number of boosting rounds set to 1000 to allow for a more complex model but otherwise used default parameters. We used category-specific sample weights via the scikit-learn method ‘compute_sample_weight’ to further mitigate imbalanced categories. The fitted classifier was then applied to the 19 video sequences of the CALMS21 test dataset to evaluate the performance of our package’s classification pipeline. The model generated raw output in the form of classification probabilities for each category across all frames of all sequences. In post-processing, we selected the category with the highest probability for each frame and grouped consecutive frames of the same category into behavioral intervals. These intervals were arranged in a table with columns for ‘group’ (i.e., sequence ID), ‘category’, ‘start’, and ‘stop’, the endpoint of behavioral results in the <italic>vassi</italic> package.</p>
<p>We calculated the per-frame macro F1 score for the three foreground categories to enable comparisons with <xref ref-type="bibr" rid="c44">Sun et al. (2021)</xref>. However, we also deemed the background category’s accuracy important for model evaluation and therefore additionally calculated the per-frame macro F1 score with all four categories. We further computed two interval-based F1 scores: one focusing on the recall of annotations (i.e., how many annotation intervals were overlapping primarily with predictions of the same category) and another assessing the precision of predictions (i.e., how many predicted intervals had the same category as the annotation with highest temporal overlap). We also evaluated model performance with confusion matrices, both on the per-frame level and on annotated and predicted intervals.</p>
<p>Subsequently, we tuned two sets of post-processing hyperparameters to improve classification results. We sought to find the optimal duration of output smoothing when applying a filter (e.g., mean or median) to classification probabilities, and secondly, the optimal decision threshold for each category. We used k-fold cross-validation (k = 5) on the original CALMS21 training dataset, splitting the dataset so that each of the 70 video sequences was used for validation in exactly one of the five dataset folds. For each fold, we sampled training data using the same features and category frequencies as before (see <xref ref-type="table" rid="tblS4">SI Table 4</xref>) to fit a total of five XGBoost classifiers. These classifiers were then used for prediction on the respective test data of each fold. Subsequently, we used optuna (<xref ref-type="bibr" rid="c1">Akiba et al., 2019</xref>), a hyperparameter optimization framework, to tune the specified parameters within a predefined search space. Finally, we applied these two optimized post-processing steps to all predictions on the CALMS21 test dataset, and compared the results to the baseline without post-processing and the results from <xref ref-type="bibr" rid="c44">Sun et al. (2021)</xref>.</p>
</sec>
<sec id="s4j">
<title>Social cichlids dataset</title>
<p>We further tested the <italic>vassi</italic> package with a more complex dataset of socially interacting cichlid fish. This dataset was collected as part of a larger study (Bose et al., in prep) on <italic>Neolamprologus multifasciatus</italic>, a shell-dwelling cichlid fish of Lake Tanganyika that lives in social groups and exhibits a range of social behaviors (<xref ref-type="bibr" rid="c28">Kohler, 1998</xref>).</p>
<p>We selected nine videos of captive <italic>N. multifasciatus</italic> groups of 15 individuals each, filmed top-down with a GoPro Hero7 action camera at a resolution of 2704 × 1520 pixels and 30 frames per second. The video recordings captured the aquarium setups that housed the groups of fish during the experiments. These aquaria were enriched with sand as bottom substrate and 30 empty <italic>Neothauma tanganyicense</italic> snail shells as shelters. As part of the overarching study, we tracked individual fish in these videos using a custom object detection and posture estimation workflow (using the detectron2 framework; <xref ref-type="bibr" rid="c52">Wu et al., 2019</xref>; and Python scripts and user interfaces adapted from <xref ref-type="bibr" rid="c15">Francisco et al., 2020</xref>). As a result, each fish was tracked with three keypoints along the main body axis (head, dorsal center, tail) when visible and detected by the detectron2 model. For all other frames, the keypoints were linearly interpolated between the previous and the next detection. Additionally, one experimenter scored the social behaviors of all fish (‘approach’, ‘lateral display’, ‘frontal display’, ‘bite/dart, ‘chase’, ‘quiver’ as foreground categories, see <xref rid="tbl1" ref-type="table">Table 1</xref> for descriptions) using the event logging software BORIS (<xref ref-type="bibr" rid="c16">Friard &amp; Gamba, 2016</xref>), always in relation to a recipient individual. The trajectories and behavioral annotations are made available alongside the <italic>vassi</italic> package and were used without further modification.</p>
<p>We followed the same workflow as with the CALMS21 dataset but implemented changes where differences in dataset structure demanded so. For example, we allowed interactions between individuals to be bi-directional, which resulted in 210 directed dyads for group each group of 15 individuals (1862 dyads across the nine videos, one group had only 14 individuals). Out of these dyads, we randomly selected 20% of individuals as actors (i.e., the first individual in a directed dyad; 375 dyads) for testing, the remaining 1487 dyads were used for data sampling and the training of classifiers. Similar to the CALMS21 dataset, our social cichlids dataset also consisted of imbalanced behavioral categories, and we chose to subsample the overrepresented categories ‘none’ and ‘frontal display’ for training classifiers (see <xref ref-type="table" rid="tblS5">SI Table 5</xref>). Note that since each individual could interact with all others in its group, but only with one at a time, the background category ‘none’ was substantially larger in this dataset. To address this, in addition to random sampling across all ‘none’ samples, we also selected samples where an individual showed a behavior towards a recipient fish, but then specifically sampled another of its close neighbors (to whom the behavior was not directed) to obtain samples that might be difficult to classify. The number of tracked keypoints (three keypoints, see above) also differed from the CALMS21 dataset, hence, the composition and number of extracted features were different as well. Since the posture information of the tracked fish was less detailed than in case of the CALMS21 dataset, we further transformed the features using a sliding time-window aggregation on a 3 second timescale (93 video frames) with three consecutive windows (first, second and third second), for which descriptive statistics such as the mean, median and quantile values were computed.</p>
<p>After fitting a XGBoost classifier on the training data of the social cichlids dataset, we also optimized two post-processing hyperparameters (temporal smoothing and decision thresholds). Here, instead of splitting the training dataset by videos during 5-fold cross-validation, we split groups by dyads, the smallest behaviorally independent element in this dataset. Note that this was in fact equivalently implemented to the cross-validation splits for the CALMS21 dataset, as video sequences in the CALMS21 dataset consist of ‘groups’ containing exactly one directed dyad (i.e., resident to intruder). After the optimal values of these hyperparameters were determined for the social cichlids dataset, we applied both post-processing steps with these values on the social cichlids test dataset. Classification performance was evaluated with F1 scores (as described before) and compared between the baseline without post-processing and results with temporal smoothing and adjusted decision rules.</p>
</sec>
<sec id="s4k">
<title>Software development</title>
<p>We developed the <italic>vassi</italic> package in Python 3 (version &gt;=3.12), building on established methods for data handling and manipulation (NumPy, pandas, h5py and SciPy), and machine learning packages (scikit-learn, XGBoost and optuna). The package was implemented with a focus on interactive coding in the JupyterLab environment. All steps of the behavioral scoring pipeline are exposed in an object-oriented way through classes and their respective methods. The inspection tool for model validation was implemented with the ipywidgets and ipyvuetify packages. All components of the <italic>vassi</italic> package can be installed with a single command and across operating systems (tested on Windows 11, Ubuntu 24.04.2 and macOS Sequoia 15.3.1). The documentation of the package and example scripts are available online at <ext-link ext-link-type="uri" xlink:href="http://vassi.readthedocs.io/">http://vassi.readthedocs.io/</ext-link>.</p>
<p>Software development and tests were conducted on consumer-grade hardware, a laptop with an Intel Core i7-1165G7 CPU and 16 GB of RAM (for both Windows and Ubuntu), a workstation with an Intel Core i9-9900X CPU and 64 GB of RAM (Ubuntu) and a MacBook Pro 2024 with an Apple M4 Pro chip and 24 GB of RAM (macOS). Hyperparameter tuning was performed on the computing cluster ‘Raven’ of the Max Planck Computing and Data Facility.</p>
</sec>
</sec>
</body>
<back>
<sec id="s8">
<title>Supplementary Information</title>
<sec id="s8a">
<title>Supplementary Tables</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>SI Table 1:</label>
<caption><title>CALMS21 test dataset evaluation results.</title>
<p>The values report means and standard deviations of 20 pipeline runs with different random states. F1 scores were calculated on three levels, i.e., frames, annotated intervals and predicted intervals (both evaluated as counts), and for three processing steps (raw model outputs, classification results after probability smoothing and after thresholding). Classification performance was evaluated separately for each behavioral category and as unweighted average (Macro F1) across all categories and across the three behavioral foreground categories ‘attack’, ‘investigation’, and ‘mount’. All scores are reported as means and standard deviations of 20 pipeline runs with different random states.</p></caption>
<graphic xlink:href="664909v1_tblS1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>SI Table 2:</label>
<caption><title>Social cichlids test dataset evaluation results.</title>
<p>All values report means and standard deviations of 20 pipeline runs with different random states. F1 scores were calculated on three levels, i.e., frames, annotated intervals and predicted intervals (both evaluated as counts), and for three processing steps (raw model outputs, classification results after probability smoothing and after thresholding). Classification performance was evaluated separately for each behavioral category and as unweighted average (Macro F1) across all categories and across the six behavioral foreground categories (excluding ‘none’).</p></caption>
<graphic xlink:href="664909v1_tblS2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>SI Table 3:</label>
<caption><title>Correlation tests between correlations of ground-truth behavioral interaction counts (annotations) and two potential proxies: (1) predicted counts as resulting from the classification pipeline and, (2) association time, the cumulative duration that two individuals spend within a defined distance threshold (3 average body length, and 1 and 5 body length as a sensitivity analysis).</title> <p>We used Williams’ tests between two dependent correlations that share one variable (two tailed) to assess whether correlations with predicted counts were different in strength compared to equivalent correlations with association time. Note that the correlation coefficients with predicted counts are repeated for each association threshold.</p></caption>
<graphic xlink:href="664909v1_tblS3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS4" orientation="portrait" position="float">
<label>SI Table 4:</label>
<caption><title>Overview of sample counts (video frames) and proportions by category in the CALMS21 training and test dataset and after subsampling for model training.</title>
<p>The ‘sampling frequency’ column shows category specific (sub-)sampling strategies employed for this dataset: full sampling (value of 1.0, all frames) and sampling with a target count (for both ‘investigation’ and ‘other’, proportion in parentheses is the realized subsampling frequency). The resulting samples that were used for model training (‘subsampled training data’) can differ from the target count due to constrains from stratified sampling by behavioral intervals.</p></caption>
<graphic xlink:href="664909v1_tblS4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS5" orientation="portrait" position="float">
<label>SI Table 5:</label>
<caption><title>Overview of category counts (video frames per dyad) and proportions by category in the full social cichlids dataset, when split into training dataset (selecting all dyads where a subset of 80% of all individuals where ‘actors’) and test dataset (the remaining dyads), and after subsampling for model training.</title>
<p>The categories were either fully sampled (sampling frequency of 1.0), or subsampled with a given frequency. Note that for the behavioral background category ‘none’, we first randomly selected 1% of all available samples (asterisk in table), and then added more samples where an actor was interacting with a different individual. For these additional ‘none’ samples, we sampled the first and second non-interacting closest neighbors with 0.1x the sampling frequency of the actor’s actual interaction, and the third, fourth and fifth neighbors with 0.05x the sampling frequency. Also note that the category ‘none’ contains more than 99% of all samples because individuals can only interact with one other at a time, but groups consist of 15 individuals.</p></caption>
<graphic xlink:href="664909v1_tblS5.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS6" orientation="portrait" position="float">
<label>SI Table 6:</label>
<caption><title>Spatiotemporal features that were extracted to train classifiers for the CALMS21 dataset.</title>
<p>In total, 201 feature values were extracted to describe the spatiotemporal movement patterns of dyadic mice interactions. Features are either individual or dyadic and result in at least one value per feature and combination of postural elements (i.e., keypoints or segments). Some features are temporal and were calculated for different time steps (video frames). For example, ‘speed’ (an individual feature) was calculated for three keypoints and for three steps, resulting in a total of 9 values. In comparison, ‘target velocity’ is a dyadic feature and was calculated for two keypoints of the actor mouse and along six target vectors between keypoints of the actor and recipient mouse. In comparison to ‘speed’ (a scalar feature with one value), ‘target velocity’ itself is a vector of two components, i.e., the projection and rejection of the keypoint displacement onto a target vector. For the three temporal steps, this results in 3 × 2 × 6 × 2 = 72 values.</p></caption>
<graphic xlink:href="664909v1_tblS6.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s8b">
<title>Supplementary Figures</title>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>SI Figure 1:</label>
<caption><title>Additional CALMS21 test dataset validation results.</title> <p>All values represent means of 20 pipeline runs with different random states, standard deviations are shown as error bars if applicable. <bold>A – D:</bold> Evaluation based on annotated behavioral intervals. <bold>A:</bold> Confusion matrix for annotated intervals and their predicted category (i.e., category of predicted interval with longest overlap). Proportional values are normalized across rows, absolute counts are shown below in parentheses. <bold>B:</bold> Macro F1 scores calculated on the annotated intervals across all categories for three (post-)processing steps – raw model outputs (‘model’), after probability smoothing (‘smooth’) and after thresholding (‘thresh’). <bold>C:</bold> F1 scores calculated for each category and the three processing steps. <bold>D – E:</bold> Corresponding evaluation based on predicted behavioral intervals. <bold>D:</bold> Confusion matrix for predicted intervals and their true category (i.e., category of annotated interval with longest overlap). <bold>E and F:</bold> Macro F1 and category F1 scores calculated on predicted intervals. Behavioral categories are abbreviated: ‘attack’ – ‘att’, ‘investigation’ – ‘inv’ and ‘mount’ – ‘mnt’.</p></caption>
<graphic xlink:href="664909v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>SI Figure 2:</label>
<caption><title>Additional social cichlids test dataset validation results based on annotated intervals.</title> <p>The predicted category that corresponds to a behavioral annotation is selected as the category of the predicted interval with longest overlap. All values represent means of 20 pipeline runs with different random states, standard deviations are shown as error bars if applicable. <bold>A:</bold> Confusion matrix depicting recall of annotations. Note that the ‘none’ column represents false negatives (i.e., missed predictions). Proportional values are normalized across rows, absolute counts are shown below in parentheses. <bold>B and C:</bold> Macro F1 and category F1 scores calculated on annotated intervals for three (post-)processing steps – raw model outputs (‘model’), after probability smoothing (‘smooth’) and after thresholding (‘thresh’).</p></caption>
<graphic xlink:href="664909v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" orientation="portrait" fig-type="figure">
<label>SI Figure 3:</label>
<caption><title>Additional social cichlids test dataset validation results based on video frames.</title> <p>All values represent means of 20 pipeline runs with different random states, standard deviations are shown as error bars if applicable. <bold>A:</bold> Confusion matrix across all video frames of the test dataset. Proportional values are normalized across rows, absolute counts are shown below in parentheses. Note the disproportionate number of frames that belong to the behavioral background category ‘none’. <bold>B and C:</bold> Macro F1 and category F1 scores calculated on video frames for three (post-)processing steps – raw model outputs (‘model’), after probability smoothing (‘smooth’) and after thresholding (‘thresh’).</p></caption>
<graphic xlink:href="664909v1_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" orientation="portrait" fig-type="figure">
<label>SI Figure 4:</label>
<caption><title>Visualization of correlations between ground-truth, annotated behavioral interaction counts and two potential behavioral proxies.</title> <p><bold>A – F:</bold> Correlations with predicted counts, split by behavioral foreground categories. <bold>G – L:</bold> Correlations with association time, the cumulative duration that two individuals spent within a distance of three average body lengths. The correlation strengths of all correlations with predicted counts are significantly higher than the corresponding correlation strengths with association time (William’s correlation tests with dependent correlations that share one variable; P &lt; 0.001 in all cases). For statistical estimates and a sensitivity analysis with other association distance thresholds (1 and 5 body lengths), see <xref ref-type="table" rid="tblS3">SI Table 3</xref>.</p></caption>
<graphic xlink:href="664909v1_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" orientation="portrait" fig-type="figure">
<label>SI Figure 5:</label>
<caption><title>Classification results of all 19 resident-intruder sequences in the CALMS21 test dataset after post-processing (i.e., category-specific output smoothing and thresholding).</title> <p>Upper bars (orange) show predicted intervals, lower bars (blue) ground-truth annotations. Lines represent model outputs – classification probabilities for each category – after smoothing. All sequences are visually aligned with the longest sequence ‘0’. Note the skewed distribution of annotated ‘attack’ intervals: Only five of 19 sequences have annotations for this behavioral category, but in those at a relatively high frequency.</p></caption>
<graphic xlink:href="664909v1_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank all members of the Behavioral Evolution Research Group for feedback and valuable discussion. This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC 2117 – 422037984, by the Max Planck Institute of Animal Behavior and Vetenskapsrådet (grant number 2023-03866 to AB).</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6" sec-type="ethics-statement">
<title>Ethics</title>
<p>We did not conduct any animal experiments that were designed for the purpose of creating the presented methodology. The two case studies were performed on either existing data (CALMS21 mice dataset; <xref ref-type="bibr" rid="c44">Sun et al., 2021</xref>) or on data collected for an overarching study on cichlid fish that was conducted in accordance with the Directive 2010/63/EU on the protection of animals used for scientific purposes and the Tierschutzgesetz (TierSchG) Baden-Württemberg, approved by permit no. G 19/79.</p>
</sec>
<sec id="s5" sec-type="data-availability">
<title>Code and data availability</title>
<p>All code and data is open-source and publicly available. The <italic>vassi</italic> package is under version control on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/pnuehrenberg/vassi">https://github.com/pnuehrenberg/vassi</ext-link>, example scripts (including scripts to reproduce all results and figures) and code documentation are also available online (<ext-link ext-link-type="uri" xlink:href="http://vassi.readthedocs.io/">http://vassi.readthedocs.io/</ext-link>). The social cichlids dataset (videos, trajectories and behavioral annotations), all source files of the <italic>vassi</italic> package, and all additional data or configuration files can be accessed at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17617/3.3R0QYI">https://doi.org/10.17617/3.3R0QYI</ext-link>.</p>
</sec>
<sec id="s7">
<title>Author contributions</title>
<p>
<bold>PN:</bold> Conceptualization, Methodology, Software, Validation, Formal Analysis, Data Curation, Writing – Original Draft, Writing - Review &amp; Editing, Visualization. <bold>AB:</bold> Conceptualization, Validation, Resources, Data Curation, Writing - Review &amp; Editing. <bold>AJ:</bold> Conceptualization, Resources, Writing - Review &amp; Editing, Supervision, Funding acquisition.</p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Video 1.</label>
<caption><title>Example usage of the interactive validation tool.</title> <p>See Figure 4 for more information. The video and subtitles are also available in the online documentation at https://vassi.readthedocs.io. Note that Figure 1 as shown in the video depicts classification results from a preliminary classifier that was trained with a slightly different subsampling strategy.</p></caption>
<media xlink:href="Video1.mp4"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Akiba</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Sano</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Yanase</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ohta</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Koyama</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Optuna: A Next-generation Hyperparameter Optimization Framework</article-title>. <conf-name>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</conf-name>, <fpage>2623</fpage>–<lpage>2631</lpage>. <pub-id pub-id-type="doi">10.1145/3292500.3330701</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altmann</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1974</year>). <article-title>Observational Study of Behavior: Sampling Methods</article-title>. <source>Behaviour</source> <pub-id pub-id-type="doi">10.1163/156853974X00534</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alward</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Laud</surname>, <given-names>V. A.</given-names></string-name>, <string-name><surname>Skalnik</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>York</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Juntti</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Fernald</surname>, <given-names>R. D.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Modular genetic control of social status in a cichlid fish</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>45</issue>), <fpage>28167</fpage>–<lpage>28174</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.2008925117</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Toward a Science of Computational Ethology</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>1</issue>), <fpage>18</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.005</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Barton</surname>, <given-names>L. E.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>H. A.</given-names></string-name></person-group> (<year>1990</year>). <chapter-title>Observational Technology: An Update</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>S. R.</given-names> <surname>Schroeder</surname></string-name></person-group> (Ed.), <source>Ecobehavioral Analysis and Developmental Disabilities: The Twenty-First Century</source> (pp. <fpage>201</fpage>–<lpage>227</lpage>). <publisher-name>Springer</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-1-4612-3336-7_11</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berman</surname>, <given-names>G. J.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Measuring behavior across scales</article-title>. <source>BMC Biology</source>, <volume>16</volume>(<issue>1</issue>), <fpage>23</fpage>. <pub-id pub-id-type="doi">10.1186/s12915-018-0494-7</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bohnslav</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Wimalasena</surname>, <given-names>N. K.</given-names></string-name>, <string-name><surname>Clausing</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>Y. Y.</given-names></string-name>, <string-name><surname>Yarmolinsky</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Cruz</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kashlan</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Chiappe</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Orefice</surname>, <given-names>L. L.</given-names></string-name>, <string-name><surname>Woolf</surname>, <given-names>C. J.</given-names></string-name>, &amp; <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name></person-group> (<year>2021</year>). <article-title>DeepEthogram, a machine learning pipeline for supervised behavior classification from raw pixels</article-title>. <source>eLife</source>, <volume>10</volume>, <elocation-id>e63377</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.63377</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bordes</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Miranda</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Müller-Myhsok</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Schmidt</surname>, <given-names>M. V.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Advancing social behavioral neuroscience by integrating ethology and comparative psychology methods through machine learning</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>151</volume>, <fpage>105243</fpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2023.105243</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Guestrin</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2016</year>). <article-title>XGBoost: A Scalable Tree Boosting System</article-title>. <conf-name>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</conf-name>, <fpage>785</fpage>–<lpage>794</lpage>. <pub-id pub-id-type="doi">10.1145/2939672.2939785</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davis</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Crofoot</surname>, <given-names>M. C.</given-names></string-name>, &amp; <string-name><surname>Farine</surname>, <given-names>D. R.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Estimating the robustness and uncertainty of animal social networks using different observational methods</article-title>. <source>Animal Behaviour</source>, <volume>141</volume>, <fpage>29</fpage>–<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2018.04.012</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dell</surname>, <given-names>A. I.</given-names></string-name>, <string-name><surname>Bender</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Couzin</surname>, <given-names>I. D.</given-names></string-name>, <string-name><surname>de Polavieja</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Noldus</surname>, <given-names>L. P. J. J.</given-names></string-name>, <string-name><surname>Pérez-Escudero</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Straw</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Wikelski</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Brose</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Automated image-based tracking and its application in ecology</article-title>. <source>Trends in Ecology &amp; Evolution</source>, <volume>29</volume>(<issue>7</issue>), <fpage>417</fpage>–<lpage>428</lpage>. <pub-id pub-id-type="doi">10.1016/j.tree.2014.05.004</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Farine</surname>, <given-names>D. R.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Proximity as a proxy for interactions: Issues of scale in social network analysis</article-title>. <source>Animal Behaviour</source>, <volume>104</volume>, <fpage>e1</fpage>–<lpage>e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2014.11.019</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fawcett</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2006</year>). <article-title>An introduction to ROC analysis</article-title>. <source>Pattern Recognition Letters</source>, <volume>27</volume>(<issue>8</issue>), <fpage>861</fpage>–<lpage>874</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2005.10.010</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Filazzola</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Cahill Jr</surname>, <given-names>J. F.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Replication in field ecology: Identifying challenges and proposing solutions</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>12</volume>(<issue>10</issue>), <fpage>1780</fpage>–<lpage>1792</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.13657</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Francisco</surname>, <given-names>F. A.</given-names></string-name>, <string-name><surname>Nührenberg</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Jordan</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>High-resolution, non-invasive animal tracking and reconstruction of local environment in aquatic ecosystems</article-title>. <source>Movement Ecology</source>, <volume>8</volume>(<issue>1</issue>), <fpage>27</fpage>. <pub-id pub-id-type="doi">10.1186/s40462-020-00214-w</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friard</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Gamba</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2016</year>). <article-title>BORIS: A free, versatile open-source event-logging software for video/audio coding and live observations</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>7</volume>(<issue>11</issue>), <fpage>1325</fpage>– <lpage>1330</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.12584</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gomez-Marin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Paton</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Kampff</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>R. M.</given-names></string-name>, &amp; <string-name><surname>Mainen</surname>, <given-names>Z. F.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Big behavioral data: Psychology, ethology and the foundations of neuroscience</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>11</issue>), <fpage>1455</fpage>–<lpage>1462</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3812</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodwin</surname>, <given-names>N. L.</given-names></string-name>, <string-name><surname>Choong</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Hwang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pitts</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bloom</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Islam</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y. Y.</given-names></string-name>, <string-name><surname>Szelenyi</surname>, <given-names>E. R.</given-names></string-name>, <string-name><surname>Tong</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Newman</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Miczek</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wright</surname>, <given-names>H. R.</given-names></string-name>, <string-name><surname>McLaughlin</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Norville</surname>, <given-names>Z. C.</given-names></string-name>, <string-name><surname>Eshel</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Heshmati</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nilsson</surname>, <given-names>S. R. O.</given-names></string-name>, &amp; <string-name><surname>Golden</surname>, <given-names>S. A.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Simple Behavioral Analysis (SimBA) as a platform for explainable machine learning in behavioral neuroscience</article-title>. <source>Nature Neuroscience</source>, <volume>27</volume>(<issue>7</issue>), <fpage>1411</fpage>–<lpage>1424</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-024-01649-9</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodwin</surname>, <given-names>N. L.</given-names></string-name>, <string-name><surname>Nilsson</surname>, <given-names>S. R. O.</given-names></string-name>, &amp; <string-name><surname>Golden</surname>, <given-names>S. A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Rage Against the Machine: Advancing the study of aggression ethology via machine learning</article-title>. <source>Psychopharmacology</source>, <volume>237</volume>(<issue>9</issue>), <fpage>2569</fpage>–<lpage>2588</lpage>. <pub-id pub-id-type="doi">10.1007/s00213-020-05577-x</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Graving</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Chae</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Naik</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Koger</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Costelloe</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Couzin</surname>, <given-names>I. D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title>. <source>eLife</source>, <volume>8</volume>, <elocation-id>e47994</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Finn</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Kieseler</surname>, <given-names>M.-L.</given-names></string-name>, <string-name><surname>Maechler</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Tse</surname>, <given-names>P. U.</given-names></string-name></person-group> (<year>2023</year>). <article-title>DeepAction: A MATLAB toolbox for automated classification of animal behavior in video</article-title>. <source>Scientific Reports</source>, <volume>13</volume>(<issue>1</issue>), <fpage>2688</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-023-29574-0</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Millman</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Van Der Walt</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Gommers</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Virtanen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cournapeau</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wieser</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Kern</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Picus</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hoyer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Van Kerkwijk</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Brett</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Haldane</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Del Río</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Wiebe</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>P.</given-names></string-name>, <etal>…</etal> <string-name><surname>Oliphant</surname>, <given-names>T. E.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Array programming with NumPy</article-title>. <source>Nature</source>, <volume>585</volume>(<issue>7825</issue>), <fpage>357</fpage>–<lpage>362</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heitz</surname>, <given-names>R. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The speed-accuracy tradeoff: History, physiology, methodology, and behavior</article-title>. <source>Frontiers in Neuroscience</source>, <volume>8</volume>. <pub-id pub-id-type="doi">10.3389/fnins.2014.00150</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Juba</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Le</surname>, <given-names>H. S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Precision-Recall versus Accuracy and the Role of Large Data Sets</article-title>. <conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name> <pub-id pub-id-type="doi">10.1609/aaai.v33i01.33014039</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kabra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Robie</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Rivera-Alba</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Branson</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2013</year>). <article-title>JAABA: Interactive machine learning for automatic annotation of animal behavior</article-title>. <source>Nature Methods</source>, <volume>10</volume>(<issue>1</issue>), <fpage>64</fpage>–<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kennedy</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The what, how, and why of naturalistic behavior</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>74</volume>, <fpage>102549</fpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2022.102549</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koger</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Deshpande</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kerby</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Graving</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Costelloe</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Couzin</surname>, <given-names>I. D.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Quantifying the movement, behaviour and environmental context of group-living animals using drones and computer vision</article-title>. <source>Journal of Animal Ecology</source>, <volume>92</volume>(<issue>7</issue>), <fpage>1357</fpage>–<lpage>1371</lpage>. <pub-id pub-id-type="doi">10.1111/1365-2656.13904</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kohler</surname>, <given-names>U.</given-names></string-name></person-group> (<year>1998</year>). <source>Zur Struktur und Evolution des Sozialsystems von Neolamprologus multifasciatus (Cichlidae, Pisces), dem kleinsten Schneckenbuntbarsch des Tanganjikasees</source>. <publisher-name>Shaker</publisher-name>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kraus</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Aichem</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Lein</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Jordan</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Schreiber</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2024</year>). <article-title>TIBA: A web application for the visual analysis of temporal occurrences, interactions, and transitions of animal behavior</article-title>. <source>PLOS Computational Biology</source>, <volume>20</volume>(<issue>10</issue>), <fpage>e1012425</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1012425</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luxem</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Bradley</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Krishnan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yttri</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name><surname>Laubach</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Open-source tools for behavioral video analysis: Setup, methods, and best practices</article-title>. <source>eLife</source>, <volume>12</volume>, <elocation-id>e79305</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.79305</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mamidanna</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cury</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>V. N.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature Neuroscience</source>, <volume>21</volume>(<issue>9</issue>), <fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nielsen</surname>, <given-names>B. L.</given-names></string-name>, <string-name><surname>Jezierski</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Bolhuis</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Amo</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Rosell</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Oostindjer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Christensen</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>McKeegan</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wells</surname>, <given-names>D. L.</given-names></string-name>, &amp; <string-name><surname>Hepper</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Olfaction: An Overlooked Sensory Modality in Applied Ethology and Animal Welfare</article-title>. <source>Frontiers in Veterinary Science</source>, <volume>2</volume>. <pub-id pub-id-type="doi">10.3389/fvets.2015.00069</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patricelli</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Hebets</surname>, <given-names>E. A.</given-names></string-name></person-group> (<year>2016</year>). <article-title>New dimensions in animal communication: The case for complexity</article-title>. <source>Current Opinion in Behavioral Sciences</source>, <volume>12</volume>, <fpage>80</fpage>–<lpage>89</lpage>. <pub-id pub-id-type="doi">10.1016/j.cobeha.2016.09.011</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Michel</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Grisel</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Blondel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Prettenhofer</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Weiss</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Dubourg</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Vanderplas</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Passos</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cournapeau</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Brucher</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Perrot</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Duchesnay</surname>, <given-names>É.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>Journal of Machine Learning Research</source>, <volume>12</volume>(<issue>85</issue>), <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Tabris</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Matsliah</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ravindranath</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Papadoyannis</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Normand</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Deutsch</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z. Y.</given-names></string-name>, <string-name><surname>McKenzie-Smith</surname>, <given-names>G. C.</given-names></string-name>, <string-name><surname>Mitelut</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Castro</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>D’Uva</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kislin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sanes</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Kocher</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S. S.-H.</given-names></string-name>, <string-name><surname>Falkner</surname>, <given-names>A. L.</given-names></string-name>, <etal>…</etal> <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title>. <source>Nature Methods</source>, <volume>19</volume>(<issue>4</issue>), <fpage>486</fpage>–<lpage>495</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-022-01426-1</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poljac</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kiesel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Müller</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2018</year>). <article-title>New perspectives on human multitasking</article-title>. <source>Psychological Research</source>, <volume>82</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>3</lpage>. <pub-id pub-id-type="doi">10.1007/s00426-018-0970-2</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="book"><person-group person-group-type="author"><collab>R Core Team</collab></person-group>. (<year>2025</year>). <source>R: A Language and Environment for Statistical Computing</source>. <publisher-name>R Foundation for Statistical Computing</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rendall</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Owren</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Ryan</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>What do animal signals mean?</article-title> <source>Animal Behaviour</source>, <volume>78</volume>(<issue>2</issue>), <fpage>233</fpage>–<lpage>240</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2009.06.007</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Romero-Ferrero</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bergomi</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Hinz</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Heras</surname>, <given-names>F. J. H.</given-names></string-name>, &amp; <string-name><surname>de Polavieja</surname>, <given-names>G. G.</given-names></string-name></person-group> (<year>2019</year>). <article-title>idtracker.ai: Tracking all individuals in small or large collectives of unmarked animals</article-title>. <source>Nature Methods</source>, <volume>16</volume>(<issue>2</issue>), <fpage>179</fpage>–<lpage>182</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0295-5</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Sauer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Boulesteix</surname>, <given-names>A.-L.</given-names></string-name>, <string-name><surname>Hanßum</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hodiamont</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bausewein</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Ullmann</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Beyond algorithm hyperparameters: On preprocessing hyperparameters and associated pitfalls in machine learning applications</article-title> <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2412.03491</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segalin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Karigo</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hui</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zelikowsky</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Kennedy</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The Mouse Action Recognition System (MARS) software pipeline for automated analysis of social behaviors in mice</article-title>. <source>eLife</source>, <volume>10</volume>, <elocation-id>e63720</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.63720</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Siegford</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Steibel</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Benjamin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brown-Brandl</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Dórea</surname>, <given-names>J. R. R.</given-names></string-name>, <string-name><surname>Morris</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Norton</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Psota</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Rosa</surname>, <given-names>G. J. M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The quest to develop automated systems for monitoring animal behavior</article-title>. <source>Applied Animal Behaviour Science</source>, <volume>265</volume>, <fpage>106000</fpage>. <pub-id pub-id-type="doi">10.1016/j.applanim.2023.106000</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>J. E.</given-names></string-name>, &amp; <string-name><surname>Pinter-Wollman</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Observing the unwatchable: Integrating automated sensing, naturalistic observations and animal social network analysis in the age of big data</article-title>. <source>Journal of Animal Ecology</source>, <volume>90</volume>(<issue>1</issue>), <fpage>62</fpage>–<lpage>75</lpage>. <pub-id pub-id-type="doi">10.1111/1365-2656.13362</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Karigo</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Chakraborty</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mohanty</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Wild</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Yue</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Kennedy</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions</article-title> <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2104.02710</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>The HDF Group</collab></person-group>. (n.d.). <article-title>Hierarchical Data Format, version 5</article-title> <source>GitHub</source> <ext-link ext-link-type="uri" xlink:href="https://github.com/HDFGroup/hdf5">https://github.com/HDFGroup/hdf5</ext-link> <year>no date</year></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>The pandas development team</collab></person-group>. (<year>2024</year>). <article-title>pandas-dev/pandas: Pandas (Version v2.2.3)</article-title>. <source>Zenodo</source>. <pub-id pub-id-type="doi">10.5281/zenodo.13819579</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tillmann</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Hsu</surname>, <given-names>A. I.</given-names></string-name>, <string-name><surname>Schwarz</surname>, <given-names>M. K.</given-names></string-name>, &amp; <string-name><surname>Yttri</surname>, <given-names>E. A.</given-names></string-name></person-group> (<year>2024</year>). <article-title>A-SOiD, an active-learning platform for expert-guided, data-efficient discovery of behavior</article-title>. <source>Nature Methods</source>, <volume>21</volume>(<issue>4</issue>), <fpage>703</fpage>–<lpage>711</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-024-02200-1</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tinbergen</surname>, <given-names>N.</given-names></string-name></person-group> (<year>1963</year>). <article-title>On aims and methods of Ethology</article-title>. <source>Zeitschrift Für Tierpsychologie</source>, <volume>20</volume>(<issue>4</issue>), <fpage>410</fpage>–<lpage>433</lpage>. <pub-id pub-id-type="doi">10.1111/j.1439-0310.1963.tb01161.x</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Virtanen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gommers</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Oliphant</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Haberland</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Reddy</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Cournapeau</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Burovski</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Weckesser</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Bright</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>van der Walt</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Brett</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Millman</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Mayorov</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>A. R. J.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kern</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Larson</surname>, <given-names>E.</given-names></string-name>, <etal>…</etal> <collab>SciPy 1.0 Contributors</collab></person-group>. (<year>2020</year>). <article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title>. <source>Nature Methods</source>, <volume>17</volume>, <fpage>261</fpage>–<lpage>272</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Whitehead</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2008</year>). <source>Analyzing Animal Societies: Quantitative Methods for Vertebrate Social Analysis</source>. <publisher-name>University of Chicago Press</publisher-name>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>William</given-names> <surname>Revelle</surname></string-name></person-group>. (<year>2024</year>). <source>psych: Procedures for Psychological, Psychometric, and Personality Research</source>. <publisher-name>Northwestern University</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=psych">https://CRAN.R-project.org/package=psych</ext-link></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Kirillov</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Massa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lo</surname>, <given-names>W.-Y.</given-names></string-name>, &amp; <string-name><surname>Girshick</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2019</year>). <italic>Detectron2</italic> [Computer software]. <ext-link ext-link-type="uri" xlink:href="https://github.com/facebookresearch/detectron2">https://github.com/facebookresearch/detectron2</ext-link></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Shami</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>On hyperparameter optimization of machine learning algorithms: Theory and practice</article-title>. <source>Neurocomputing</source>, <volume>415</volume>, <fpage>295</fpage>–<lpage>316</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2020.07.061</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108329.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3588-7820</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents vassi, a Python package that streamlines the preparation of training data for machine-learning-based classification of social behaviors in animal groups. This package is a <bold>valuable</bold> resource for researchers with computational expertise, implementing a framework for the detection of directed social interactions within a group and an interactive tool for reviewing and correcting behavior detections. However, the strength of evidence that the method is widely applicable remains <bold>incomplete</bold>, performance on benchmark dyadic datasets is comparable to existing approaches, and performance scores on collective behavioral datasets are low. While the package can analyze behavior in large groups of animals, it only outputs dyadic interactions within these groups and does not account for behaviors where more than two animals may be interacting.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108329.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this manuscript, Nührenberg et al., describe vassi, a Python package for mutually exclusive behavioral classification of social behaviors. This package imports and organizes trajectory data and manual behavior labels, and then computes feature representations for use with available Python machine learning-based classification tools. These representations include all possible dyadic interactions within an animal group, enabling classification of social behaviors between pairs of animals at a distance. The authors validate this package by reproducing the behavior classification performance on a previously published dyadic mouse dataset, and demonstrate its use on a novel cichlid group dataset. The authors have created a package that is agnostic to the mechanism of tracking and will reduce the barrier of data preparation for machine learning, which can be a stumbling block for non-experts. The package also evaluates the classification performance with helpful visualizations and provides a tool for inspection of behavior classification results.</p>
<p>Strengths:</p>
<p>(1) A major contribution of this paper was creating a framework to extend social behavior classification to groups of animals such that the actor and receiver can be any member of the group, regardless of distance. To implement this framework, the authors created a Python package and an extensive documentation site, which is greatly appreciated. This package should be useful to researchers with a knowledge of Python, virtual environments, and machine learning, as it relies on scripts rather than a GUI interface and may facilitate the development of new machine learning algorithms for behavior classification.</p>
<p>(2) The authors include modules for correctly creating train and test sets, and evaluation of classifier performance. This is extremely useful. Beyond evaluation, they have created a tool for manual review and correction of annotations. And they demonstrate the utility of this validation tool in the case of rare behaviors where correct classification is difficult, but the number of examples to review is reasonable.</p>
<p>(3) The authors provide well-commented step-by-step instructions for the use of the package in the documentation.</p>
<p>Weaknesses:</p>
<p>(1) While the classification algorithm was not the subject of the paper, as the authors used off-the-shelf methods and were only able to reproduce the performance of the CALMS21 dyadic dataset, they did not improve upon previously published results. Furthermore, the results from the novel cichlid fish dataset, including a macro F1 score of 0.45, did not compellingly show that the workflow described in the paper produces useful behavioral classifications for groups of interacting animals performing rare social behaviors. I commend the authors for transparently reporting the results both with the macro F1 scores and the confusion matrices for the classifiers. The mutually exclusive, all-vs-all data annotation scheme of rare behaviors results in extremely unbalanced datasets such that categorical classification becomes a difficult problem. To try to address the performance limitation, the authors built a validation tool that allows the user to manually review the behavior predictions.</p>
<p>(2) The pipeline makes a few strong assumptions that should be made more explicit in the paper.</p>
<p>First, the behavioral classifiers are mutually exclusive and one-to-one. An individual animal can only be performing one behavior at any given time, and that behavior has only one recipient. These assumptions are implicit in how the package creates the data structure, and should be made clearer to the reader. Additionally, the authors emphasize that they have extended behavior classification to animal groups, but more accurately, they have extended behavioral classification to all possible pairs within a group.</p>
<p>Second, the package expects comprehensive behavior labeling of the tracking data as input. Any frames not manually labeled are assumed to be the background category. Additionally, the package will interpolate through any missing segments of tracking data and assign the background behavioral category to those trajectory segments as well. The effects of these assumptions are not explored in the paper, which may limit the utility of this workflow for naturalistic environments.</p>
<p>(3) Finally, the authors described the package as a tool for biologists and ethologists, but the level of Python and machine learning expertise required to use the package to develop a novel behavior classification workflow may be beyond the ability of many biologists. More accessible example notebooks would help address this problem.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108329.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors present a novel supervised behavioral analysis pipeline (vassi), which extends beyond previously available packages with its innate support of groups of any number of organisms. Importantly, this program also allows for iterative improvement upon models through revised behavioral annotation.</p>
<p>Strengths:</p>
<p>vassi's support of groups of any number of animals is a major advancement for those studying collective social behavior. Additionally, the built-in ability to choose different base models and iteratively train them is an important advancement beyond current pipelines. vassi is also producing behavioral classifiers with similar precision/recall metrics for dyadic behavior as currently published packages using similar algorithms.</p>
<p>Weaknesses:</p>
<p>vassi's performance on group behaviors is potentially too low to proceed with (F1 roughly 0.2 to 0.6). Different sources have slightly different definitions, but an F1 score of 0.7 or 0.8 is often considered good, while anything lower than 0.5 can typically be considered bad. There has been no published consensus within behavioral neuroscience (that I know of) on a minimum F1 score for use. Collective behavioral research is extremely challenging to perform due to hand annotation times, and there needs to be a discussion in the field as to the trade-off between throughput and accuracy before these scores can be either used or thrown out the door. It would also be useful to see the authors perform a few rounds of iterative corrections on these classifiers to see if performance is improved.</p>
<p>While the interaction networks in Figure 2b-c look visually similar based on interaction pairs, the weights of the interactions appear to be quite different between hand and automated annotations. This could lead to incorrect social network metrics, which are increasingly popular in collective social behavior analysis. It would be very helpful to see calculated SNA metrics for hand versus machine scoring to see whether or not vassi is reliable for these datasets.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108329.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Nührenberg</surname>
<given-names>Paul</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6352-3560</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bose</surname>
<given-names>Aneesh PH</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jordan</surname>
<given-names>Alex</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6131-9734</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the reviewers and editors for their assessment and for identifying the main issues of our framework for automated classification of social interactions in animal groups. Based on the reviewers’ feedback, we would like to briefly summarize three areas in which we aim to improve both our manuscript and the software package.</p>
<p>Firstly, we will revise our manuscript to better define the scope of our classification pipeline. As reviewer #1 correctly points out, our framework is built around the scoring and analysis of dyadic interactions within groups, rather than emergent group-level or collective behavior. This structure more faithfully reflects the way that researchers score social behaviors within groups, following focal individuals while logging all directed interactions of interest (e.g., grooming, aggression or courtship), and with whom these interactions are performed. Indeed, animal groups are often described as social networks of interconnected nodes (individuals), in which the connections between these nodes are derived from pairwise metrics, for example proximity or interaction frequency. For this reason, <italic>vassi</italic> does not aim to classify higher-level group behavior (i.e., the emergent, collective state of all group members) but rather the pair-wise interactions typically measured. Our classification pipeline replicates this structure, and therefore produces raw data that is familiar to researchers that study social animal groups with a focus on pairwise interactions. Since this may be seen as a limitation when studying group-level behavior (with more than two individuals involved, usually undirected), we will make this distinction between different forms of social interaction more clear in the introduction.</p>
<p>Secondly, we acknowledge the low performance of our classification pipeline on the cichlid group dataset. We included analyses in the first version of our manuscript that, in our opinion, can justify the use of our pipeline in such cases (comparison to proximity networks), but we understand the reviewers' concerns. Based on their comments, we will perform additional analyses to further assess whether the use of <italic>vassi</italic> on this dataset results in valid behavioral metrics. This may, for example, include a comparison of per-individual SNA metrics between pipeline results and ground truth, or equivalent comparisons on the level of group structure (e.g., hierarchy derived from aggression counts). We thank reviewer #2 for these suggestions. As the reviewers further point out, there is no consensus yet on when the performance of behavioral classifiers is sufficient for reliable downstream analyses, and although this manuscript does not have the scope to discuss this for the field, it may help to substantiate discussion in future research.</p>
<p>Finally, we appreciate the reviewers feedback on <italic>vassi</italic> as a methodological framework and will address the remaining software-related issues by improving the documentation and accessibility of our example scripts. This will reduce the technical hurdle to use <italic>vassi</italic> in further research. Additionally, we aim to incorporate a third dataset to demonstrate how our framework can be used for iterative training on a sparsely annotated dataset of groups, while broadening the taxonomic scope of our manuscript.</p>
</body>
</sub-article>
</article>