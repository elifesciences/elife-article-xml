<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107803</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107803</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107803.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Immunology and Inflammation</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>Restoring data balance via generative models of T cell receptors for antigen-binding prediction</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0004-4882-8250</contrib-id>
<name>
<surname>Loffredo</surname>
<given-names>Emanuele</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2771-7437</contrib-id>
<name>
<surname>Pastore</surname>
<given-names>Mauro</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>mpastore@ictp.it</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1852-7789</contrib-id>
<name>
<surname>Cocco</surname>
<given-names>Simona</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n2">‡</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4459-0204</contrib-id>
<name>
<surname>Monasson</surname>
<given-names>Rémi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n2">‡</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02en5vm52</institution-id><institution>Laboratory of Physics of the Ecole Normale Supérieure, CNRS UMR 8023 and PSL Research, Sorbonne Université</institution></institution-wrap>, <city>Paris</city>, <country country="FR">France</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/009gyvm78</institution-id><institution>The Abdus Salam International Centre for Theoretical Physics</institution></institution-wrap>, <city>Trieste</city>, <country country="IT">Italy</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bitbol</surname>
<given-names>Anne-Florence</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution>
</institution-wrap>
<city>Lausanne</city>
<country country="CH">Switzerland</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moses</surname>
<given-names>Alan M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Toronto</institution>
</institution-wrap>
<city>Toronto</city>
<country country="CA">Canada</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally to this work</p></fn>
<fn id="n2" fn-type="equal"><label>‡</label><p>These authors also contributed equally to this work</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-09-15">
<day>15</day>
<month>09</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107803</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-06-13">
<day>13</day>
<month>06</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-05-18">
<day>18</day>
<month>05</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.10.602897"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Loffredo et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Loffredo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107803-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Unveiling specificity in T cell recognition of antigens represents a major step to understand the immune system response. Many supervised machine learning approaches have been designed to build sequence-based predictive models of such specificity using binding and non-binding receptor-antigen data. Due to the scarcity of known specific T cell receptors for each antigen compared to the abundance of non-specific ones, available datasets are heavily imbalanced and make the goal of achieving solid predictive performances very challenging. Here, we propose to restore data balance through data augmentation using generative unsupervised models. We then use these augmented data to train supervised models for prediction of peptide-specific T cell receptors, or binding pairs of peptide and T cell receptor sequences. We show that our pipeline yields increased performance in prediction tasks of T cell receptors specificity. More broadly, our pipeline provides a general framework that could be used to restore balance in other computational problems involving biological sequence data.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>section &quot;Background and relevance&quot; added to introduce the problem of class imbalance in machine learning; other sections re-arranged to improve on presentation; figures and tables updated; bibliography updated; appendix updated.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Performances of Machine Learning (ML) approaches are well known to crucially depend on the composition and quality of training data. In the case of classification, imbalance in the training dataset – which occurs when one or more classes are significantly under-represented – poses serious challenges during the learning process, as most algorithms are designed to optimize performances over all data, leading to biased models that mostly focus on the majority classes. Yet, the correct characterization of rare examples is of primarily relevance, in particular in many biologicallyrelated applications, such as molecular biology (<xref ref-type="bibr" rid="c87">Wang et al., 2006</xref>; <xref ref-type="bibr" rid="c92">Yang et al., 2012</xref>; <xref ref-type="bibr" rid="c11">Cheng et al., 2015</xref>; <xref ref-type="bibr" rid="c78">Song et al., 2021</xref>; <xref ref-type="bibr" rid="c4">Ansari and White, 2024</xref>), automated medical diagnostics (<xref ref-type="bibr" rid="c40">Krawczyk et al., 2016</xref>; <xref ref-type="bibr" rid="c26">Fotouhi et al., 2019</xref>), ecology (<xref ref-type="bibr" rid="c41">Kyathanahally et al., 2021</xref>; <xref ref-type="bibr" rid="c10">Chen et al., 2025</xref>), etc. Among these, adaptive immune receptors data, which are the focus of this work, represent a typical scenario where machine learning approaches might be limited due to the imbalance: we will address this question in the context of predicting the specificity of T cell receptors (<xref ref-type="bibr" rid="c18">Deng et al., 2023</xref>; <xref ref-type="bibr" rid="c53">Mason and Reddy, 2024</xref>).</p>
<p>Cytotoxic T lymphocytes (T cells) play a crucial role in the adaptive immune response of organisms against pathogens and/or malfunctioning cells (<xref ref-type="bibr" rid="c93">Zhang and Bevan, 2011</xref>). Short pathogen protein regions (peptide antigens) interact with the Major Histocompatibility Complex proteins (MHC) and form peptide-MHC epitopes (pMHC). Binding of CD8+ T cell receptor (TCR) with pMHC enables the killer machinery against such pathogen. Given the high specificity of the interaction, TCRs bind a limited number of presented pMHC. Achieving a reliable prediction of TCR-pMHC binding from sequence data represents a major goal in the field, in particular for the development of vaccines and the improvement of personalized immunotherapies.</p>
<p>Over the recent years, much progress in this direction has been made with computational approaches (<xref ref-type="bibr" rid="c76">Sim, 2024</xref>; <xref ref-type="bibr" rid="c55">Meysman et al., 2023</xref>; <xref ref-type="bibr" rid="c29">Ghoreyshi and George, 2023</xref>; <xref ref-type="bibr" rid="c88">Weber et al., 2023</xref>; <xref ref-type="bibr" rid="c61">Nagano et al., 2025</xref>), benefiting both from the power of ML methods and the large-scale amount of experimentally tested data. Such works typically use the sequences of the Complementarity-Determining Region-3 beta (CDR3<italic>β</italic>) and alpha (CDR3<italic>α</italic>) chains paired with peptide sequences to reveal TCR-pMHC binding affinity. The CDR3 region is the most variable one in TCRs and is recognized to be the major actor influencing TCR specificity for peptide binding. Though recent works have shown that use of both <italic>α</italic> and <italic>β</italic> chains leads to better predictions (<xref ref-type="bibr" rid="c15">Dash et al., 2017</xref>; <xref ref-type="bibr" rid="c60">Montemurro et al., 2021</xref>), many works still focus on <italic>β</italic> chains solely, because they primarily drive the immune response (<xref ref-type="bibr" rid="c80">Springer et al., 2021</xref>) and are more abundant in most databases.</p>
<p>Predicting TCRs specificity is a computationally challenging problem for several reasons. On the one hand, CDR3<italic>β</italic> sequences binding different target epitopes exhibit very strong similarities and it is hard to identify the features determining binding specificity. On the other hand, ML predictive models are trained on labelled (experimentally tested) TCR-pMHC sequence data through supervised learning to obtain accurate predictions. Public databases containing TCR specificity, such as IEDB (<xref ref-type="bibr" rid="c84">Vita et al., 2018</xref>), VDJdb (<xref ref-type="bibr" rid="c74">Shugay et al., 2017</xref>; <xref ref-type="bibr" rid="c5">Bagaev et al., 2019</xref>), Mc-PAS TCR (<xref ref-type="bibr" rid="c81">Tickotsky et al., 2017</xref>) and PIRD (<xref ref-type="bibr" rid="c94">Zhang et al., 2019</xref>), mostly include limited amount of TCR sequences with positive interaction, <italic>i</italic>.<italic>e</italic>. known to bind some peptides of interest. Considerable efforts have been undertaken to build <italic>ad hoc</italic> negative sequence datasets, for instance by mismatch pairing and/or taking bulk (unlabelled) data from healthy donors. While the way the negative data are chosen may potentially bias predictions (<xref ref-type="bibr" rid="c83">Ursu et al., 2024</xref>), their number exceed in quantity, by several orders of magnitude, the amount of experimentally tested positive binding pairs. As a result, binding prediction models are trained from datasets with strong imbalance between the positive and negative classes.</p>
<p>The present work aims first at analyzing how much TCR specificity predictions are hindered by the imbalance issue. We then propose a two-step pipeline to restore data balance using a generative framework for specific TCR sequences. Our findings highlight that it is worth to carefully restore balance in the composition of training datasets in order to improve performances. Furthermore, since our pipeline is quite general and can handle various sequence-based data, its relevance reaches beyond the TCR field, to any sequence-based classification problem where imbalance is of concern (<xref ref-type="bibr" rid="c87">Wang et al., 2006</xref>; <xref ref-type="bibr" rid="c92">Yang et al., 2012</xref>; <xref ref-type="bibr" rid="c11">Cheng et al., 2015</xref>; <xref ref-type="bibr" rid="c78">Song et al., 2021</xref>; <xref ref-type="bibr" rid="c4">Ansari and White, 2024</xref>).</p>
<p>The paper is organized as follows. In Background and relevance, we present a general overview on the problem of learning with class imbalance. In Results, we introduce our pipeline (detailed in Materials and methods) to tackle this problem in the context of CDR3<italic>β</italic> binding predictions, and we give evidence on how it improves performances on both peptide-specific and pan-specific tasks. In Discussion and conclusions, we draw our conclusions.</p>
</sec>
<sec id="s2">
<title>Background and relevance</title>
<sec id="s2a">
<title>Effect of class imbalance on decision boundary</title>
<p>Class imbalance is a widespread problem affecting both bio- and non bio-related sources of data and is notably an Achilles’ heel in applying machine learning methods to achieve reliable predictions (<xref ref-type="bibr" rid="c24">Fernández et al., 2018</xref>). Informally speaking, naive ML models adapt to the most represented class in the data and are less accurate in capturing the features of under represented examples, degrading the quality of predictions whenever the minority class is of crucial interest. From a geometrical perspective, supervised architectures solve classification tasks by learning a decision boundary in the feature space of input data: the effect of class imbalance can be envisioned as shifting away the decision boundary from the optimal one, at the expenses of classes with fewer data points.</p>
<p>To visualize this effect in a simple setting, let us first focus on generalized linear architectures, which classify input points by learning separating hyperplanes. The high-dimensional (in data dimension and size of the training set) asymptotics of learning and generalization can be fully tackled theoretically within the framework of equilibrium statistical physics (<xref rid="c17" ref-type="bibr">Del Giudice, P. et al., 1989</xref>; <xref ref-type="bibr" rid="c49">Loureiro et al., 2021b</xref>; <xref ref-type="bibr" rid="c64">Pesce et al., 2023</xref>; <xref ref-type="bibr" rid="c14">Dandi et al., 2023</xref>; <xref ref-type="bibr" rid="c51">Mannelli et al., 2023</xref>; <xref ref-type="bibr" rid="c47">Loffredo et al., 2024</xref>; <xref ref-type="bibr" rid="c65">Pezzicoli et al., 2025</xref>). This analysis, which has been proven relevant in realistic cases (<xref ref-type="bibr" rid="c48">Loureiro et al., 2021a</xref>; <xref ref-type="bibr" rid="c64">Pesce et al., 2023</xref>; <xref ref-type="bibr" rid="c47">Loffredo et al., 2024</xref>), shows that classifiers trained on imbalanced datasets learn a sub-optimal decision hyperplane, with a systematic offset from the best attainable one, as well as errors on the orientation (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Effects of imbalanced data on ML classifiers.</title>
<p><bold>a)</bold> 2D visualization of a two-class training dataset, with the majority class (yellow) significantly outnumbering the minority one (blue). Data are generated from a Gaussian mixture; the decision boundary (DB) separating the classes passes perpendicularly through the midpoint of the line segment connecting the centers of the two clusters (dark line); a linear classifier trained under imbalance learns a sub-optimal decision boundary (light line), which leads to low predictive capabilities (see App. 2 for more details). <bold>b)</bold> Accuracy and AUC scores for a predictive model trained to distinguish <monospace>YVLDHLIVV</monospace>- and <monospace>YLQPRTFLL</monospace>-specific CDR3<italic>β</italic> sequences from bulk CDR3<italic>β</italic>s as a function of the fraction <italic>ρ</italic><sub><italic>N</italic></sub> of background data in the training set. In practice we fix the class size of peptide-specific sequences and vary the size of the background sequences class to change <italic>ρ</italic><sub><italic>N</italic></sub>. Performances (evaluated on a balanced test set) are optimal when the two class sizes are of roughly equal sizes, <italic>i</italic>.<italic>e</italic>. when <italic>ρ</italic><sub><italic>N</italic></sub> ≃ 0.5. <bold>c)</bold> Graphical visualization of the imbalanced composition of TCRs datasets, in a two-class setting where <italic>P, N</italic> represent the class sizes. Our work proposes to restore class balance (<italic>i</italic>.<italic>e. P</italic> = <italic>N</italic>, straight line) by introducing a generative model able to sample new sequences compatible with the positive class, for which few data are experimentally available.</p></caption>
<graphic xlink:href="602897v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This picture qualitatively holds for deeper architectures, for two essential reasons: (i) the last layer of a deep classifier is usually fully-connected and can be thought of as a linear classifier in the space of the features learned by the previous layers; (ii) in certain regimes, such as the infinitewidth limit and lazy training (<xref ref-type="bibr" rid="c62">Neal, 1996</xref>; <xref ref-type="bibr" rid="c89">Williams, 1996</xref>; <xref ref-type="bibr" rid="c42">Lee et al., 2018</xref>; <xref ref-type="bibr" rid="c27">de G. Matthews et al., 2018</xref>; <xref ref-type="bibr" rid="c36">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="c6">Bietti and Mairal, 2019</xref>; <xref ref-type="bibr" rid="c12">Chizat et al., 2019</xref>), deep models have been proven equivalent to kernel machines, which embed the data in a randomized feature space and operate there as linear classifiers (<xref ref-type="bibr" rid="c21">Dietrich et al., 1999</xref>; <xref ref-type="bibr" rid="c28">Gerace et al., 2021</xref>; <xref ref-type="bibr" rid="c1">Aguirre-López et al., 2025</xref>).</p>
</sec>
<sec id="s2b">
<title>Metrics for performance assessment</title>
<p>An important issue is how to fairly assess performance under imbalance. Consider for instance a dataset with two classes having imbalance ratio 1:9 in the data. Suppose we split the data along a canonical training/test cross-validation procedure, maintaining on average the same imbalance ratio in each subset, and that we naively assess performances using the standard measure of accuracy – which counts the number of correctly classified examples in the test set. Even a null model identifying everything as the majority class would have high accuracy (≈ 90%), yet such result is clearly meaningless in quantifying how good the model can distinguish the two data classes. Similar warped assessments are obtained from other evaluation metrics that explicitly depend on the test set composition. Furthermore, imbalance can have an implicit and more subtle impact on evaluation metrics insensitive to some model parameters that are however used at inference level. This is the case of measures obtained by thresholding the predictor of the model and integrating parametric curves of the threshold, such as the area under the Receiver Operating Characteristic curve, or the area under the Precision-Recall curve. These estimators are not sensitive to the bias of the model (the offset of the hyperplane for linear classifiers in <xref rid="fig1" ref-type="fig">Fig. 1a</xref>), which is often the parameter most impacted by imbalance during training (<xref ref-type="bibr" rid="c47">Loffredo et al., 2024</xref>).</p>
<p>Many studies have discussed the most useful metrics to adopt in this context (<xref ref-type="bibr" rid="c25">Forman and Scholz, 2010</xref>), with no clear agreement in the literature on which one is less prone to imbalance biases (<xref ref-type="bibr" rid="c45">Lobo et al., 2008</xref>; <xref ref-type="bibr" rid="c71">Saito and Rehmsmeier, 2015</xref>; <xref ref-type="bibr" rid="c68">Richardson et al., 2024</xref>). Here, we follow the recent work <xref rid="c47" ref-type="bibr">Loffredo et al. (2024)</xref>, where a theoretical comparison of different evaluation metrics was performed, suggesting that it is best to use metrics that do not depend explicitly on the imbalance on the test set, i.e. that equally weight the majority and minority contributions regardless of the imbalance ratio, or to use an explicitly balanced test set. Consequently, we hereafter evaluate model performances measuring the accuracy (ACC) and Area Under the Receiver Operating Characteristic curve (AUC) always on a balanced test set, regardless of the training set imbalance (see details in Materials and methods). In <xref rid="fig1" ref-type="fig">Fig. 1b</xref> we plot the behavior of these metrics for models trained to classify CDR3<italic>β</italic> binders to two different epitopes from background sequences with varied imbalance ratios. We observe that the best performance are, not surprisingly, obtained when the training data are (close to) being balanced.</p>
</sec>
<sec id="s2c">
<title>Mitigation approaches</title>
<p>A plethora of mitigation strategies have been proposed to cure the effect of imbalance in the training data and improve performances. Some of these approaches act at the algorithmic level (<xref ref-type="bibr" rid="c24">Fernández et al., 2018</xref>), and proceed by reweighting the loss (to boost minority data), adapting the optimizer to account for imbalance, or averaging over ensembles (e.g. bagging), etc… Data-level methods seek to restore data balance through downsampling majority data or data augmentation of the minority class. Here, we focus on the latter set of methods for mainly two reasons: first, simple algorithmic strategies such as class reweighting in the training loss can be thought as equivalent to random oversampling of the minority class; second, developing generative models to augment data has an interest <italic>per se</italic> for the design of synthetic sequences. In practice, starting with a given dataset composition with two classes – the majority and minority one – we combine majority undersampling and minority oversampling in order to reach some intermediate common size for both classes, see <xref rid="fig1" ref-type="fig">Fig. 1c</xref> and Materials and methods for details. In a recent theoretical study, we have benchmarked different strategies to produce a balanced dataset (<xref ref-type="bibr" rid="c47">Loffredo et al., 2024</xref>), and showed that such a mixed approach offered the highest predictive performances. In this work we will follow this line, showing that also for sequence-based problems and specifically for TCR specificity predictions, mixed strategies yield better results.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Learning pipeline</title>
<p>We now present our mixed mitigation strategy, which combines data removal (subsampling of the negative class) and, crucially, data augmentation (oversampling of the positive class), see <xref rid="fig1" ref-type="fig">Fig. 1c</xref>. We implement this mixed strategy for both</p>
<list list-type="order">
<list-item><p><italic>peptide-specific</italic> models, trained on one or more selected epitopes and for which the peptide sequence is only used as a label during the training. This setting corresponds to multi-class classification of CDR3<italic>β</italic> sequences, with classes identified by the binding peptides.</p></list-item>
<list-item><p><italic>pan-specific</italic> models, where combinations of peptides and TCR sequences are presented during training, together with binary labels expressing if the peptide-TCR pairs are binding or not. This setting corresponds to binary classification of CDR3<italic>β</italic> + peptide binding/non-binding pairs.</p></list-item>
</list>
<p>Though pan-specific models are harder to build as they require more data than their peptidespecific counterparts, they can, in principle, leverage the diversity of the peptide space to capture the common underlying features of TCR-peptide interactions and potentially recognize binding to new, unseen antigens. We explore this last possibility by studying the out-of-distribution performances of our predictive models.</p>
<p>The learning pipeline, fully presented in <xref rid="fig2" ref-type="fig">Fig. 2</xref> for both peptide- and pan-specific cases, consists of two steps. First, data balance is restored by randomly subsampling the negative data class and by augmenting the positive class of sequences through data generation. To do so, we learn an unsupervised generative model over the limited number of peptide-specific CDR3<italic>β</italic> sequences in the training set and enlarge the positive class by generating surrogate sequences. The idea of using unsupervised generative model for data augmentation to mitigate imbalance or data scarcity was recently proposed in a variety of settings (<xref rid="c95" ref-type="bibr">Zięba et al., 2015</xref>; <xref ref-type="bibr" rid="c85">Wan et al., 2017</xref>; <xref ref-type="bibr" rid="c22">Douzas and Bacao, 2018</xref>; <xref ref-type="bibr" rid="c58">Mirza et al., 2021</xref>; <xref ref-type="bibr" rid="c59">Mondal et al., 2023</xref>; <xref ref-type="bibr" rid="c2">Ai et al., 2023</xref>; <xref ref-type="bibr" rid="c47">Loffredo et al., 2024</xref>), and was successfully applied to biological-sequences related tasks, <italic>e</italic>.<italic>g</italic>. in <xref rid="c52" ref-type="bibr">Marouf et al. (2020)</xref>. We consider two generative models, namely</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Learning pipelines for peptide-specific (top) and pan-specific (bottom) models.</title>
<p><bold>(Top)</bold>: peptide-specific models. Column <bold>a)</bold>: Data consists of few CDR3<italic>β</italic> sequences, known to bind some epitopes (colored symbols and segments) and of many ‘negative’ sequences (yellow). Column <bold>b)</bold>: A generative model is trained over peptide-specific CDR3<italic>β</italic> sequences, here, corresponding to the orange epitope. After training, Gibbs sampling of the inferred probability landscape allows us to generate putative peptide-specific sequences. Column <bold>c)</bold>: A supervised CNN architecture is trained over (natural and generated) peptide-specific CDR3<italic>β</italic>s and background CDR3<italic>β</italic>s; after learning, the network is used as predictive model for TCR specificity over in- and out-of-distribution (black sequences) test data. <bold>(Bottom)</bold>: Pan-specific models. Column <bold>a)</bold>: Compared to the pipeline above, input data are joint sequences of peptides (left, lighter color) and of TCR (right). Background sequences are obtained through mismatch pairing. Column <bold>b)</bold>: The generative models produce putative binding pairs of peptide and TCR sequences. Column <bold>c)</bold>: Supervised classifier trained to carry out TCR-epitope binding predictions.</p></caption>
<graphic xlink:href="602897v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<list list-type="bullet">
<list-item><p>Restricted Boltzmann Machines (RBMs), two-layer architectures extracting latent features from sequence data;</p></list-item>
<list-item><p>Bidirectional Encoder Representation Transformer (BERT)-like architectures trained over CDR3<italic>β</italic> sequences to learn their <italic>grammatical structure</italic>.</p></list-item>
</list>
<p>For peptide-specific predictions we separately train a model for each epitope, while for pan-specific predictions a collective model encompassing all the epitopes receives as input peptide and CDR3<italic>β</italic> data, see Generative models for data augmentation.</p>
<p>Second, we use the natural and generated data to train a supervised model for TCR-epitope binding predictions. Models existing so far range from random forests (<xref ref-type="bibr" rid="c30">Gielis et al., 2019</xref>; <xref ref-type="bibr" rid="c66">Pham et al., 2023</xref>) to neural network architectures of different complexities, <italic>e</italic>.<italic>g</italic>. convolutional neural networks (<xref ref-type="bibr" rid="c60">Montemurro et al., 2021</xref>; <xref ref-type="bibr" rid="c75">Sidhom et al., 2021</xref>), long-short term memory networks and autoencoders (<xref ref-type="bibr" rid="c79">Springer et al., 2020</xref>; <xref ref-type="bibr" rid="c50">Lu et al., 2021</xref>); unsupervised algorithms have also been adapted to this task, such as SONIA (<xref ref-type="bibr" rid="c72">Sethna et al., 2020</xref>) and its more precise variant soNNia (<xref ref-type="bibr" rid="c35">Isacchini et al., 2021</xref>), diffRBM (<xref ref-type="bibr" rid="c8">Bravi et al., 2023</xref>), which implements transfer learning within Restricted Boltzmann Machines, and, in the context of Large Language Models, Transformers-based approaches (<xref ref-type="bibr" rid="c91">Yadav et al., 2024</xref>; <xref ref-type="bibr" rid="c90">Wu et al., 2024</xref>; <xref ref-type="bibr" rid="c54">Meynard-Piganeau et al., 2024</xref>).</p>
<p>Hereafter, we resort to a one-dimensional convolutional neural network (CNN) architecture, trained over positive (natural and generated) and negative sequences – once balance has been restored between classes. The architectures for peptide- and pan-specific models are slightly different, see <xref rid="fig2" ref-type="fig">Fig. 2</xref>, and are detailed in Model architecture for supervised predictions. Last of all, the predictive power of the model is tested over its ability to discriminate positive against negative CDR3<italic>β</italic> sequences (the in-distribution test set) or against other new peptide-specific CDR3<italic>β</italic> (the out-of-distribution test set).</p>
</sec>
<sec id="s3b">
<title>Peptide-specific models for in-distribution predictions benefit from data augmentation</title>
<p>We present below results for a case where the predictive model has to learn three different classes of peptide-specific receptors (labelled with <italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub> and <italic>p</italic><sub>3</sub>) and bulk receptors (referred to as <italic>b</italic>). To assess if an unsupervised model that generates CDR3<italic>β</italic> sequences to augment the peptide-specific classes size can yield better performances than naive undersampling of each CDR3<italic>β</italic> class down to the lowest available class size, we consider two different strategies to restore balance in the dataset:</p>
<list list-type="bullet">
<list-item><p>A first protocol, in which data are unaltered and each class size 𝒟 is given by
<disp-formula id="eqn1">
<graphic xlink:href="602897v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<italic>i</italic>.<italic>e</italic>. data points in over-represented classes are randomly under sampled down to the common size 𝒟.</p></list-item>
<list-item><p>A second protocol, in which small-size classes are augmented using the generative model pipeline up to a target size 𝒢 common to all classes, with 𝒢 ≤ 10 𝒟 for computational reasons.</p></list-item>
</list>
<p>Notice that, for both protocols, the bulk class size 𝒟 (<italic>b</italic>), which is orders of magnitude larger than any 𝒟 (<italic>p</italic><sub><italic>i</italic></sub>), is randomly under sampled to match the final common size of the positive data. We use as generative framework both a RBM- and a BERT-based sampling strategy, to assess the effects of balancing through data augmentation regardless of the generative model. These data are then used to train our classifier. Notice that the last layer of the CNN architecture, which outputs the binding predictions, is designed to have as many units as the number of classes, <italic>i</italic>.<italic>e</italic>. of peptide labels plus the background label, with softmax activation function. This allows us to carry out multiclass classification, as the network is able to predict specificity towards more than one target.</p>
<p><xref rid="fig3" ref-type="fig">Fig. 3</xref> shows results for multiple experiments related to different triplets of peptide-specific CDR3<italic>β</italic> sequences, confirming that the quality of predictions increases with the pipeline depicted in <xref rid="fig2" ref-type="fig">Fig. 2</xref> (top row). In particular, the gain in performance is larger for multiclass experiments where one (or more) class contains few sequences, <italic>e</italic>.<italic>g</italic>. 𝒟 (<italic>p</italic><sub>1</sub>) ≪ 𝒟 (<italic>p</italic><sub>2</sub>), 𝒟 (<italic>p</italic><sub>3</sub>). For example, experimental data for the epitopes <monospace>AMFWSVPTV</monospace>, <monospace>VTEHDTLLY</monospace> and <monospace>GLCTLVAML</monospace> have relative sizes 1.5% − 2.4% − 96.1%, which introduces a strong bias towards the <monospace>GLCTLVAML</monospace>-specific CDR3<italic>β</italic>s. Restoring balance by generating new <monospace>AMFWSVPTV</monospace>-specific CDR3<italic>β</italic> sequences, we are able to obtain a 20% performance increase compared to restoring balance by undersampling only. This gain is milder when pepitde-specific classes have more sequences and are more balanced among each other, as the case of epitopes <monospace>ELAGIGILTV</monospace> (9%), <monospace>LLWNGPMAV</monospace> (21%) and <monospace>GLCTLVAML</monospace> (70%) shows. Relative abundances of the initial dataset for the combinations reported in <xref rid="fig3" ref-type="fig">Fig. 3</xref> can be obtained from the absolute class sizes reported in App. 1.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>In-distribution performances of peptide-specific models.</title>
<p>AUC and ACC scores of the predictive models for a multiclass classification task involving three peptide-specific sets of CDR3<italic>β</italic> and background CDR3<italic>β</italic> sequences, evaluated over a balanced test set of sequences of the same classes (in-distribution case). We compare performances for different training datasets, whose balance is restored through undersampling solely, or by generating new CDR3<italic>β</italic> sequences via an RBM- or BERT-based generative architecture. The baseline scores refer to an imbalanced training dataset, whose composition can be derived from the class sizes of each epitope as reported in App. 1; 250,000 background CDR3<italic>β</italic>s are used. Results confirm the benefit of both restoring balance in the training dataset and enlarging the peptide-specific CDR3<italic>β</italic> space through generative models. Dashed black lines indicate random performance levels.</p></caption>
<graphic xlink:href="602897v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Notice that, to obtain the increase of performances we observe in <xref rid="fig3" ref-type="fig">Fig. 3</xref>, it is crucial to balance the dataset with generative models powerful enough to capture non-trivial features in the distribution of the data, a task that both RBMs and BERT-like models are able to do; for more on this aspect, see App. 3.</p>
<p>Similarly with what we will report in the next section for pan-specific models, performances of the peptide-specific ones can also be assessed in an out-of-distribution setting, in which the test set includes sequences binding to unseen epitopes. As expected, model accuracy strongly depends on the similarity between the out-of-distribution and training data distributions, see App. 4.</p>
</sec>
<sec id="s3c">
<title>Pan-specific models for in-distribution predictions benefit from data augmentation</title>
<p>Pan-specific models, which take as inputs both the CDR3<italic>β</italic> and the peptide sequences, have gained interest, as they offer the possibility to identify binding patterns across different epitopes, and could potentially be used for binding predictions with rare or even novel peptides.</p>
<p>Based on the results from peptide-specific models, we now probe the benefit of restoring balance through the use of a pan-specific generative model, which we train over the full dataset from <xref ref-type="bibr" rid="c31">Grazioli et al. (2022)</xref> (see App. 1 for its composition), including abundant peptide-specific groups of CDR3<italic>β</italic>s. We then generate new peptide-specific sequences through Gibbs sampling initialized with natural sequences. The model takes as input peptide and CDR3<italic>β</italic> pairs of sequences and proposes random mutations across the sequence pair to sample new ones. We observe that only a small fraction (~ 3%) of generated data shows mutations across the peptide sequence: data augmentation overwhelmingly consists in the production of new CDR3<italic>β</italic> attached to the same peptides as in the training dataset.</p>
<p>An important hyperparameter is the size of peptide-associated groups after generation of new TCR sequences. In practice, we decide to set a threshold group size 𝒢 for data augmentation, above which peptide-specific classes are deemed as <italic>sufficiently populated</italic> and are thus not enlarged through the generative model. Groups having less than 𝒢 datapoints are all enlarged up to the threshold value. 𝒢 cannot be chosen arbitrarily large, as the generation of new data points is computationally expensive, and limited by the quality of training data and of the unsupervised model.</p>
<p>Once all positive data have been generated, we restore balance at class level by undersampling negative peptide-CDR3<italic>β</italic> pairs. Undersampling is randomly performed within each group, so that we are guaranteed that the dataset contains, on average, equal numbers of positive and negative pairs for each group, a requirement that we have observed to be important performance-wise.</p>
<p>We show in <xref rid="fig4" ref-type="fig">Fig. 4</xref> that data generation leads to increase of performance for a vast majority of the peptides involved in the dataset. Even peptide-specific groups that have not been directly enlarged (red circles) can show a gain, arguably because the CNN architecture is leveraging the information of generated peptide and CDR3<italic>β</italic> pairs to transfer learning across all the different peptide groups. The scores also show that the generative model is particularly promising for heavily under represented groups (small triangles), with performance gains up to 20%.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Pan-specific predictive model results.</title>
<p>Differences in AUC and ACC scores when balance is achieved through undersampling of data only (no generation) or with data augmentation too. In the latter case, new CDR3<italic>β</italic> sequences were generated for under-represented groups of peptides only (triangular dots). Dot sizes are proportional to the raw group size of natural sequence pairs in the dataset. Here, 𝒢 = 400 (for more details on the choice of this threshold, see App. 5).</p></caption>
<graphic xlink:href="602897v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3d">
<title>Out-of-distribution performance on close enough sequences is positively impacted by data augmentation</title>
<p>Having established that our two-step pipeline is beneficial for in-distribution predictions, we set out to investigate whether this still holds for an out-of-distribution analysis. In this context, test data points are sampled from an external distribution – which may share only few features with the training data distributions. Consequently, out-of-distribution predictions can be very challenging. We expect performances to drop consistently, depending on how much the out-of-distribution data are distant from the training data in feature space.</p>
<p>In addition to predictions for peptide and CDR3<italic>β</italic> sequence data, we analyze out-of-distribution performances of our approach on synthetic data, which offer a realistic scenario where the ground truth is available and it is thus possible to quantitatively “measure” out-of-to in-distribution distances.</p>
<sec id="s3d1">
<title>Binding predictions on unseen epitopes</title>
<p>We now ask whether our pan-specific model generalizes well on out-of-distribution TCR-peptide sequences, <italic>i</italic>.<italic>e</italic>. is able to capture the general properties underlying the binding process of receptors to antigens. As out-of-distribution test set we aggregate all the peptide-specific CDR3<italic>β</italic> sequences excluded from the training set during data pre-processing because they were strongly under represented (see Materials and methods); yet, we retain only groups that contains &gt; 20 CDR3<italic>β</italic> sequences to have significant statistics. We report their abundances in Tab. 1.</p>
<p>Performances for this hard out-of-distribution setting are unsurprisingly worse than in the indistribution case, in agreement with previous studies (<xref ref-type="bibr" rid="c13">Croce et al., 2024</xref>). Nonetheless, we observe a correlation between the scores and the similarity between the unseen epitopes and the ones in the training data; in fact, clustering the CDR3<italic>β</italic> sequences based on the Levenshtein distance of their target epitope to the closest epitope in the dataset, we find that performances drop from AUC = 0.68 for close epitopes to random for distant epitopes (Levenshtein distance 11), respectively. We report the AUC/ACC metric results grouped by Levenshtein distance for all tested cases in App. 4.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Out-of-distribution performances evaluated with AUC and ACC metrics across a test set composed of wild type binders to the target epitope and CDR3<italic>β</italic> sequences sampled from other unseen epitopes.</title>
<p>For each prediction, we separately train a classifier with an enlarged training set containing also the synthetic binder of the target out-of-distribution epitope, generated through the pan-specific model (trained on the in-distribution dataset only, but with the target epitope given during the generative step, as explained in the text). The columns labeled with [u] refer to scores obtained balancing the training set by only undersampling the negative class, for comparison. The column d[in] represents the Levenshtein distance from the closest in-distribution epitope, showing that scores degrade when moving away from in-distribution data.</p></caption>
<graphic xlink:href="602897v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Improving out-of-distribution classification is crucial for the task of TCR-peptide binding predictions, as information about the binding properties of new antigens are rarely available. Here, we explore the possibility of predicting antigen binding towards unseen epitopes, exploiting the pan-specific generative model (<italic>trained on the in-distribution dataset</italic>) and <italic>a partial knowledge of the out-of-distribution dataset</italic> (the sequence of the new epitope). In practice, we use the generative architecture to sample CDR3<italic>β</italic>s binding to the target epitope; then, we include these data in the training set and test the performance on the natural out-of-distribution binder sequence data.</p>
<p>As a proof of concept, we evaluated this procedure on 8 different epitopes, whose natural CDR3<italic>β</italic> sequence have to be distinguished from other CDR3<italic>β</italic> with a different out-of-distribution specificity (see Tab. 1). We observe an improvement of performances for epitopes at small distance from their in-distribution counterparts.</p>
<p>Minimal model of out-of-distribution classification of TCR-peptide binding pairs To better understand how the properties of out-of-distribution data impact binding predictions, we repeat this analysis on controlled, synthetic data. We resort to dimeric lattice proteins (LPs) compounds, whose native folds are shown in <xref rid="fig5" ref-type="fig">Fig. 5a</xref>. LPs are synthetic proteins defined as selfavoiding paths over a 3 × 3 × 3 lattice cube, whose vertices carry the 27 amino acids. The two LP structures in the dimer represent, in order, the epitope and the TCR.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Out-of-distribution predictions on synthetic LP dimers for pan-specific case.</title>
<p><bold>a)</bold> The two structures of the proteins making the dimer, with amino acids defining a strong binding interaction, represented by the dotted lines. <bold>b)</bold> Histogram of single structure folding scores (the lower the better), computed according to the ground truth of the model (see App. 6, Eq. (16)). In practice we take MSA of binding sequences used for training and compute the folding scores in their native structure or as if they are in an out-of-distribution close structure or not close structure. The three distributions confirm the vicinity of the sequence data orange structures to the green ones, compared to blue ones. <bold>c)</bold> tSNE visualization of in-distribution training data (binder and non binders, green and red respectively) and out-of-distribution hold-out data (close and not close structures binders) over the embeddings of our CNN architecture. In this layer the classification is linear and we can see a clear decision boundary separating green and red data points. Accuracy on in-distribution data is ACC = 0.99. The close out-of-distribution data are similar to training data, hence the model performs well on these ones (ACC = 0.98); conversely, it gives poorer performances on the not close out-of-distribution (blue data points, ACC = 0.82).</p></caption>
<graphic xlink:href="602897v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Following <xref ref-type="bibr" rid="c46">Loffredo et al. (2023)</xref>, we build dimeric LPs starting from single monomers running Monte Carlo (MC) evolution, and collect sequence data for multiple dimeric LPs. Spanning multiple dimeric structures, by changing the conformation of the self-avoiding paths on the lattice, allowed us to model different group specificities; details in App. 6. We collect MSAs of binding and non-binding pairs constituting the two classes in our dataset, and balanced both at class and group levels. A CNN classifier similar to the one introduced for natural data above is then trained over these data and reaches perfect classification in discriminating binding vs non-binding pairs – regardless of the group specificity, showing the simplicity of this in-distribution task.</p>
<p>To assess out-of-distribution performances we produce an additional dataset of binding pairs as follows. For each group of dimers in the training dataset, we collect sequence data corresponding to its closest dimer, <italic>i</italic>.<italic>e</italic>. with highest structural similarity. This dataset is referred to as <italic>close out-of-distribution</italic>, as we expect it to be very close to the training data in the feature space of the classifier. Similarly, we repeat the procedure with randomly picked dimers, which share few similarities with the dimeric structures defining the training data; we label such data as <italic>not close out-of-distribution</italic>. The structural similarity based on the ground truth folding scores is reported in <xref rid="fig5" ref-type="fig">Fig. 5b</xref>: for all in-distribution binding pairs sequences we plot scores for native, close and not close dimer structures; the orange distribution is effectively closer to the green one than the blue one, confirming a stronger structural similarity.</p>
<p>In <xref rid="fig5" ref-type="fig">Fig. 5c</xref> we report the tSNE 2d projections of the embeddings of the supervised architecture trained over the binary classification task for binding (green) versus non-binding (red) pairs of sequences, which allows us to visualize the neat decision boundary, giving very high ACC = 0.99. Within this feature space, on top of green and red points, we project out-of-distribution data points for the close and not-close cases. As expected, the latter is harder to classify as they share less features with the training data of the model (ACC = 0.98 and ACC = 0.82, respectively). Therefore, even in the framework of artificial data, we observe that out-of-distribution predictive performances depend on the degree of similarity with the in-distribution dataset.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Discussion and conclusions</title>
<p>In this work we have introduced a framework that combines the use of unsupervised and supervised computational approaches to achieve reliable predictions of TCR specificity. Our pipeline relies on a two-step procedure, where we first leverage unsupervised network architectures to learn the probability distribution of peptide-specific CDR3<italic>β</italic> sequences or peptide-CDR3<italic>β</italic> binding pairs from which new informative sequence data are generated afterwards. Second, the generated data allow us to train supervised models for the final predictive task over balanced datasets, avoiding biases induced by class imbalance. We emphasize that restoring balance by augmenting the under-represented positive data class through generation yields better performances than downsampling the negative, over-represented class alone. The gain in performance is found not only for in-distribution data, but also for out-of-distribution tests. In this latter situation, the quality of predictions however decreases with the dissimilarity between the tested epitope and the ones present in the test set. This effect, and more generally the reasons why restoring balance improves classification performances, can be geometrically interpreted in the feature space of the classifier – see Introduction and App. 2.</p>
<p>The proposed pipeline resorts to unsupervised learning to balance the training set of a supervised classifier. A natural benchmark for performance is the direct use of the unsupervised model alone to classify the data. By fixing a threshold on the score that the model assigns to test sequences, measuring how likely they are sampled from the distribution of the positive data used for training, we may obtain predictions for class membership from the unsupervised model alone. This ‘unsupervised classification’ procedure generally yields worse generalization results than the full pipeline proposed here, as reported in App. 8. The decision boundary obtained by fixing a threshold on the score of the unsupervised model, which is trained on positive data only, does not coincides with the surface separating positive and negative features in sequence space, see App. 2.</p>
<p>Restoring balance in the data is crucial to improve predictive power not only when one designs peptide-specific models, a case in which the imbalance is present at class level only, but also for pan-specific models, for which the amount of data associated to each epitope may largely vary, with some peptide being heavily under–represented. An alternative approach could be that of combining peptide- and pan-specific models not only for the supervised predictive task – as already proposed in <xref ref-type="bibr" rid="c38">Jensen and Nielsen (2023)</xref> – but also for data generation; in this way, a limited number of generative models would be trained separately over the most abundant peptide-specific classes, while a single generative model would be trained over peptide and CDR3<italic>β</italic> pairs of sequences. Depending on the peptide, one of the two generative models could then be adopted. Furthermore, the effect of group imbalance could be reduced introducing a re-weighting factor for each group in the loss function of the generative model during training – inversely proportional to the group size.</p>
<p>The training data used in this study for peptide- and pan-specific models have been collected from publicly available databases, which curate and provide T cell receptor sequences and their cognate targets published in literature. Despite recent advantages and efforts to make available also negative assays of TCR-epitope bindings, such resources remain biased towards positive interactions since negative interactions are rarely reported in experiments. To obtain negatively/non– interacting pairs of sequences practitioners resort to various strategies, including the reshuffling of epitopes and CDR3<italic>β</italic> pairs. However, recent works have shown that the production of negative data may induce biases and impact the predictive power of models (<xref ref-type="bibr" rid="c19">Dens et al., 2023</xref>; <xref ref-type="bibr" rid="c83">Ursu et al., 2024</xref>). Alternatively, negative data could be derived using pair complexes with low binding affinity generated through synthetic lattice-based receptor (<xref ref-type="bibr" rid="c3">Akbar et al., 2022</xref>; <xref ref-type="bibr" rid="c69">Robert et al., 2022</xref>). Together with the increasing amount of available negative assays, we believe that these studies will contribute to better understand and limit the sources of biases stemming from negative samples within the context of TCR-epitope binding analysis.</p>
<p>In conclusion, our results demonstrate the benefit of reducing imbalance for both peptide– and pan–specific models, while suggesting to be more important when there is an heavy imbalance in the initial dataset of natural sequences. Preprocessing the dataset to restore balance is effective across multiple strategies, ranging from simple undersampling of abundant sequences to the generation of new peptide-specific CDR3<italic>β</italic> sequences with unsupervised architectures, including energy-based models and Transformers. The robustness of these gains suggests that even better TCR specificity predictions could be achieved with more powerful generative models and/or hyperparameter optimization. For instance, our learning pipeline could be easily extended by including the possibility to feed the CDR3<italic>α</italic> chain, VJ annotation or MHC class information, likely leading to performance improvement as shown in <xref ref-type="bibr" rid="c75">Sidhom et al. (2021)</xref>; <xref ref-type="bibr" rid="c60">Montemurro et al. (2021)</xref>. Similarly, better performances could be reached by simultaneously training the generative and the classifier models, rather than one after the other. In addition, to study the impact of the training set composition on the performances alone, we did not finetune any hyper parameter and we trained all our models with the same number of total iterations over the batch, <italic>i</italic>.<italic>e</italic>. by rescaling the epochs according to the training set size.</p>
<p>To end with, let us emphasize that our generative models allow for the possibility of producing new, putative CDR3<italic>β</italic> sequences with desired binding specificity that could be tested experimentally. Thanks to its methodological simplicity and flexibility, our learning framework is not limited to the context of T cell receptors specificity predictions, and could be applied to other sequence– based computational problems where heavy imbalance impedes the proper training of predictive models (<italic>e</italic>.<italic>g</italic>. <xref rid="c32" ref-type="bibr">Haque et al. (2014)</xref>; <xref ref-type="bibr" rid="c56">Ming et al. (2023)</xref>; <xref rid="c67" ref-type="bibr">Rana et al. (2022)</xref>; <xref rid="c44" ref-type="bibr">Li et al. (2022)</xref>).</p>
</sec>
<sec id="s5">
<title>Materials and methods</title>
<sec id="s5a">
<title>Generative models for data augmentation</title>
<p>Generally speaking, unsupervised machine learning aims to learn an energy landscape by inferring parameters of a user-defined probabilistic model <italic>P</italic><sub>model</sub> over the data presented (in our case, the peptide specific CDR3<italic>β</italic> sequences, or the peptide + CDR3<italic>β</italic> binding sequences). The model is trained (or fine-tuned) separately over each class and is then asked to generate new sequences compatible with real ones by sampling from the learned probability landscape. To generate new sequences we make use of Restricted Boltzmann Machines (RBMs) (<xref ref-type="bibr" rid="c82">Tubiana et al., 2019</xref>; <xref ref-type="bibr" rid="c8">Bravi et al., 2023</xref>) and of Large Language Models (LLMs), specifically we use architectures based on Bidirectional Encoder Representations from Transformer (BERT) adapted to handle CDR3<italic>β</italic> sequences (<xref ref-type="bibr" rid="c90">Wu et al., 2024</xref>).</p>
<sec id="s5a1">
<title>Restricted Boltzmann Machines</title>
<p>RBMs are bipartite graphical models including a set of <italic>L</italic> visible units <bold>v</bold> = (<italic>v</italic><sub>1</sub>, ⋯, <italic>v</italic><sub><italic>L</italic></sub>) and <italic>M</italic> hidden (or latent) units <bold>z</bold> = (<italic>z</italic><sub>1</sub>, ⋯, <italic>z</italic><sub><italic>M</italic></sub>). Only connections between visible and latent units are allowed through the interaction weights <inline-formula><inline-graphic xlink:href="602897v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. RBMs define a joint probability distribution over <bold>v</bold> and <bold>z</bold> as the Gibbs distribution
<disp-formula id="eqn2">
<graphic xlink:href="602897v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>The joint probability above is specified by a set of parameters whose values are inferred from the data (here the biological sequences). They consist of i) the set of single-site biases <inline-formula><inline-graphic xlink:href="602897v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> acting on visible units that capture the amino acid usage at each sequence position; ii) the potentials <inline-formula><inline-graphic xlink:href="602897v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> acting on hidden units, here assumed to have dReLU shape (<xref ref-type="bibr" rid="c82">Tubiana et al., 2019</xref>), and iii) the set of weights <inline-formula><inline-graphic xlink:href="602897v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> coupling the hidden and visible layers.</p>
<p>The probability of data <italic>P</italic> (<bold>v</bold>) is defined as the marginal of the joint probability over the data itself
<disp-formula id="eqn3">
<graphic xlink:href="602897v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>All the parameters in the model are learned by maximizing the marginal log-likelihood
<disp-formula id="eqn4">
<graphic xlink:href="602897v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>D</italic> is our dataset and ‖ <italic>D</italic> ‖ its size. Due to the nature of RBMs, all data needs to have the same dimension to be accepted: in practice, we align the CDR3<italic>β</italic> receptors before feeding them into the model.</p>
<p>From the definition of the model reported in Eq. (2), let us define the RBM energy as
<disp-formula id="eqn5">
<graphic xlink:href="602897v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where we introduced the shorthand notation <inline-formula><inline-graphic xlink:href="602897v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. For a generic parameter <inline-formula><inline-graphic xlink:href="602897v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the rule to infer its value is given by the gradientascent equations stemming from log-likelihood maximization,
<disp-formula id="eqn6">
<graphic xlink:href="602897v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where ⟨⋅⟩<sub><italic>m</italic></sub> stands for the average over the model, ⟨<italic>u</italic>(<bold>v</bold>) <sub><italic>m</italic></sub>⟩ = Σ<sub>v</sub> <italic>P</italic> (<bold>v</bold>)<italic>u</italic>(<bold>v</bold>). In addition, regularization terms can be added to control the values of inferred parameters, by enforcing sparsity on the weights through <italic>L</italic><sub>1</sub>-penalty to avoid overfitting and by controlling their norm through <inline-formula><inline-graphic xlink:href="602897v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>-penalty to prevent divergences. In practice we resort to a <inline-formula><inline-graphic xlink:href="602897v2_inline7a.gif" mimetype="image" mime-subtype="gif"/></inline-formula> regularization scheme that consists in adding to the log-likelihood of data the following penalty term
<disp-formula id="eqn7">
<graphic xlink:href="602897v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where λ sets the regularization strength. It has been suggested that such regularization also helps to improve the generative properties of RBMs. The parameters used for learning of peptide-specific CDR3<italic>β</italic> distributions are:
<disp-formula id="eqn8">
<graphic xlink:href="602897v2_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Once the full set of parameters has been inferred from the training data, we can sample from the probability distribution <italic>P</italic> (<bold>v</bold>) to obtain new data (in our case, new peptide specific CDR3<italic>β</italic> receptors). Here we sample using Alternate Gibbs Sampling (AGS), which consists in alternatively sampling from the RBM’s visible layer while keeping the hidden layer fixed from <italic>P</italic> (<bold>v</bold>|<bold>z</bold>) and vice versa from <italic>P</italic> (<bold>z</bold>|<bold>v</bold>). The Monte Carlo Markov Chain (MCMC) is initialized starting from sequence data in the dataset and new sequences are collected after some steps of thermalization to avoid sampling data correlated with real ones.</p>
</sec>
</sec>
<sec id="s5b">
<title>BERT-based architectures</title>
<p>We consider a generative model of amino acid sequences based on the Bidirectional Encoder Representations from Transformer model architecture (<xref ref-type="bibr" rid="c20">Devlin et al., 2019</xref>). Transformers models are build through a series of attention blocks and feed-forward layers, whose aim is to capture interactions across the input sequence through learning how strongly the embedding of each token in the input is affected by the other tokens (the context). Stacking multiple attention blocks allow the model to learn complex structures within the embedding of the input data, such as semantic, causal and grammatical properties of the data.</p>
<p>The model takes as input for training CDR3<italic>β</italic> sequences for the peptide-specific case and pairs of peptide and CDR3<italic>β</italic> sequences for the pan-specific case; the sequences are formatted via the tokenizer retrieved from <xref rid="c90" ref-type="bibr">Wu et al. (2024)</xref>, which contains a total of 26 tokens spanning the 20 amino acids and some special tokens, such as the prefix and suffix token and the masking one. An additional token &amp; is included for the pan-specific case to model the peptide – CDR3<italic>β</italic> separation in input data (<italic>e</italic>.<italic>g</italic>. <monospace>GILGFVLT</monospace> &amp; <monospace>CASSLDGTVQYF</monospace>). Sequences do not need to be aligned, and the inputs are padded with an attention mask. The input tokens are thus padded through the embedding layer to get a continuous representation vector of the data: such embedded vector (together with a positional encoding vector that retains the positional information of amino acids in the input sequence) goes through the model attention blocks. The output feature vector leaves in a <italic>L</italic> × 768 dimensional space, where <italic>L</italic> is the input length, and is fed to a task-specific head for masked language modelling.</p>
<p>The training over our dataset is carried out performing masked language modelling (MLM) objective, where we mask a fraction of input token in the dataset and train the architecture to predict the correct ones. This allows the model to learn the grammar underlying the CDR3<italic>β</italic> and peptide sequence patterns. For the peptide-specific case, the generative model is obtained by fine-tuning the TCR-BERT model from <xref rid="c90" ref-type="bibr">Wu et al. (2024)</xref> through few epochs of MLM objective over the peptidespecific CDR3<italic>β</italic> data; this leverages the transfer-learning ability of the model that has trained over unlabelled TCR sequences to capture distinctive features of the peptide-specific CDR3<italic>β</italic> sequences. For the pan-specific model, we train the model from BERT weights over the full training dataset; at variance with BERT model, we set the maximal positional embbeding length allowed to 64, due to the shortness of sequence data compared to text data.</p>
<p>For the (fine-)tuning procedure, given a TCR dataset ℳ and a masking pattern <inline-formula><inline-graphic xlink:href="602897v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, we minimize the following training loss
<disp-formula id="eqn9">
<graphic xlink:href="602897v2_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>θ</italic> is the set of all parameters in the model and (<italic>i, r</italic>) run over the residue position and sequences, respectively. The conditional probabilities <italic>p</italic> for each (<italic>i, r</italic>) are computed using the softmaxnormalized model output values - the <italic>logits</italic> - for each symbol in the alphabet. For each input sequence, we mask 15% of amino acids as done in the original training <xref ref-type="bibr" rid="c20">Devlin et al. (2019)</xref>. All hyperparameters during fine-tuning are the same as the ones used for training in <xref rid="c20" ref-type="bibr">Devlin et al. (2019)</xref>.</p>
<p>Once the model has been (fine-)tuned on the MLM downstream task, we leverage its generative power following the iterative masking scheme proposed in <xref ref-type="bibr" rid="c86">Wang and Cho (2019)</xref>; <xref ref-type="bibr" rid="c73">Sgarbossa et al. (2023)</xref> for protein sequences. In practice, each step of this procedure works as follows:</p>
<list list-type="order">
<list-item><p>we randomly mask with probability <italic>q</italic> each entry of the CDR3<italic>β</italic> sequence or we leave it unchanged with probability 1 − <italic>q</italic>. This defines a masking pattern <inline-formula><inline-graphic xlink:href="602897v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula>;</p></list-item>
<list-item><p>we feed the masked sequence to the model and replace masked entries sampling from the softmax (normalized) distribution of the model logits at inverse temperature <italic>β</italic>;</p></list-item>
<list-item><p>we repeat steps (i) and (ii) for <italic>T</italic> times and then we store a new sample sequence.</p></list-item>
</list>
<p>In practice, we set <italic>q</italic> = 0.2, <italic>β</italic> = 1 and <italic>T</italic> = 20 and we repeat this scheme many times starting from the last configuration to obtain the desired number of TCR sequences. At the end we merge all the generated CDR3<italic>β</italic> and eventually drop duplicates: the remaining samples constitute the new set of sequences that will augment peptide specific sets.</p>
<p>Note that our protocol does not allow insertions or deletions across the sequence, as only residue tokens can be masked: in this way, the length distribution of training and generated data remains the same.</p>
</sec>
<sec id="s5c">
<title>Model architecture for supervised predictions</title>
<p>We then want to train a classifier from the training data (CDR3<italic>β</italic> sequences, or CDR3<italic>β</italic>-peptipe pairs, with the corresponding class labels). Many architectures, of widely varying complexity, were implemented in the context of TCR specificity predictions. Hereafter, we consider one architecture, and expect our results on restoring balance through generative models of CDR3<italic>β</italic> to hold for other network architecture chosen for the supervised step (<xref ref-type="bibr" rid="c47">Loffredo et al., 2024</xref>).</p>
<p>In practice, we implemented a 1-D Convolutional Neural Network (CNN) to make binary and multiclass predictions on TCRs specificity. Our architecture takes as input raw sequences and returns a single value corresponding to the target class label assigned by the model (<italic>e</italic>.<italic>g</italic>. if the CDR3<italic>β</italic> considered binds one of the epitopes or it belongs to the bulk repertoires). CDR3<italic>β</italic> sequences are first padded to a max length <italic>L</italic> = 30, then translated into score matrices using the BLOSUM50 encoding matrix with no gap or special amino acid included, see <xref ref-type="bibr" rid="c33">Henikoff and Henikoff (1992)</xref>. Hence, each sequence is mapped into a score matrix of size <italic>L</italic> × 20. The encoded input is then fed into the network architecture, consisting of five convolutional layers having 16 filters and different kernel sizes {1, 3, 5, 7, 9}. The resulting feature vectors go through a batch-normalization layer to reduce overfitting and are then concatenated to pass through a dense layer with 16 hidden neurons. An additional batch-normalization layer is applied and the resulting 16 dimensional feature vector is used to visualize the embeddings constructed by the model starting from raw data. The final classification is performed over such embeddings by feeding the feature vector to a layer with a number of neurons equal to the number of classes in the dataset and having softmax activation function (for binary classification tasks, we actually use one neuron and sigmoid activation function). We use the ReLU activation function throughout the network. Adam optimizer with learning rate <italic>η</italic> = 0.001 and categorical (or binary) cross-entropy loss function are used for learning with a batch size of 128 samples.</p>
<p>Performances are evaluated using accuracy (ACC) and Area Under the receiver operating characteristic Curve (AUC) metrics on balanced test sets, unless otherwise specified. The former is defined as
<disp-formula id="eqn10">
<graphic xlink:href="602897v2_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>In the case of binary classification, the receiver operating characteristic (ROC) curve is defined as the parametric curve in the False Positive Rate–True Positive Rate plane as a function of the threshold used to assign the binary label to the output of the sigmoid activation function of the readout layer. In the case of multiclass classification, AUC is defined as a uniform average of the binary AUCs of all possible combinations of classes (one-vs-one):
<disp-formula id="eqn11">
<graphic xlink:href="602897v2_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p><italic>c</italic> being the number of classes.</p>
</sec>
</sec>
<sec id="s6">
<title>Datasets collection</title>
<p>Supervised and unsupervised models developed in this work have been scored using multiple type of synthetic and real data. In all cases – after preprocessing – positive and negative data were merged together in a final dataset with a proportion <italic>ρ</italic><sub><italic>P</italic></sub> :<italic>ρ</italic><sub><italic>N</italic></sub> that can be tuned. The dataset was then split into three parts for training, validation and test. Subsampling from the previous test set, we also construct a balanced test set having the same numbers of positive and negative examples. We refer to performance on the latter set as in-distribution performance. We then assess model predictions on unseen (balanced) examples to define out-of-distribution performance.</p>
<p>We provide below details on TCR-peptide binding data we used to analyze peptide- and panspecific models with our pipeline.</p>
<sec id="s6a">
<title>TCR-peptide data for peptide-specific models</title>
<p>Sequence data for TCR-peptide specificity predictions were retrieved from the Immune Epitope Database (IEDB) (<xref ref-type="bibr" rid="c84">Vita et al., 2018</xref>) as of June 2023, filtering the Database entries as follows. We focus on human host immune response and set the pMHC restriction to the HLA-A*02:01 complex, limiting the peptide length to 8 − 11 amino acids. Paired CDR3<italic>β</italic>-epitope sequences were thus identified as those for which a T cell assay was reported ‘Positive’ or ‘Positive-High’ and never ‘Negative’. We collected in this way sequences of CDR3<italic>β</italic> with specificity to a set of different epitopes, listed in Tab. 2. A summary of the resulting dataset, with the abundances of CDR3<italic>β</italic> sequences for each peptide, can be found in App. 1.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>List of the epitopes selected to collect CDR3<italic>β</italic> specific sequences in order to form the database used in the analysis of peptide specific models in Results.</title></caption>
<graphic xlink:href="602897v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Background sequences of CDR3<italic>β</italic> considered as those non-self reactive are taken from the database assembled by <xref ref-type="bibr" rid="c35">Isacchini et al. (2021)</xref>, which merges together (i) unique clones from the 743 donors of the cohort in <xref ref-type="bibr" rid="c23">Emerson et al. (2017)</xref> and (ii) the dataset of CDR3<italic>β</italic> sequences from healthy donors in <xref ref-type="bibr" rid="c16">Dean et al. (2015)</xref>. The full dataset is then sub-sampled at random for computational purpose and we retain 10<sup>6</sup> background sequences.</p>
<p>Whenever sequence are fed into the RBM learning, they are required to have the same input length. Therefore, CDR3<italic>β</italic> sequences are aligned with an Hidden Markov Model (HMM) using as alignment profile the one built in <xref ref-type="bibr" rid="c7">Bravi et al. (2021)</xref>. Aligned CDR3<italic>β</italic> sequences have fixed length of 20 amino acids. To train the supervised model with RBM-generated sequences we drop all the gaps inserted by the alignment procedure, as our deep classifier can handle sequences of different lengths.</p>
<p>As anticipated in the introduction, the assembled dataset suffers from imbalance between peptide-specific CDR3<italic>β</italic>s and the large abundance of bulk unlabelled data. In addition, some epitopes (<italic>e</italic>.<italic>g</italic>. GLCTLVAML, <monospace>LLWNGPMAV</monospace>, <monospace>YLQPRTFLL</monospace>) have thousands of known CDR3<italic>β</italic> binders, while others have hundreds of positively tested CDR3<italic>β</italic>s only, which results in further imbalance between the epitope-associated classes.</p>
<p>To avoid misleading predictions due to imbalance biases, we balance the training set in such a way that all classes are equally represented (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). To evaluate performances in a fair way, we consider different metrics over a balanced test set containing all classes in the same proportion in order to factor out any source of bias. We use ACC and AUC metrics to assess our performances (see Model architecture for supervised predictions).</p>
</sec>
<sec id="s6b">
<title>TCR-peptide data for pan-specific models</title>
<p>The dataset used to train and test pan-specific models is taken from <xref ref-type="bibr" rid="c31">Grazioli et al. (2022)</xref>, and contains both positive and negative interacting pairs of CDR3<italic>β</italic> and peptide sequences spanning 118 different epitopes. Natural data are taken from publicly available sources, namely IEDB, VDJdb, McPAS-TCR, MIRA (<xref ref-type="bibr" rid="c39">Klinger et al., 2015</xref>; <xref ref-type="bibr" rid="c63">Nolan et al., 2025</xref>), and from 10X Genomics assays. Among them, we only retain peptides with at least 130 positive binder CDR3<italic>β</italic> representatives to obtain a sufficiently large dataset for training both the generative and the classifier models. This results in a total of 126,882 experimentally tested pairs of interacting CDR3<italic>β</italic> and peptide sequences, and a collection of 405,176 non-interacting sequence pairs. These negative examples consist for a minor part of experimentally assessed ones, while a large part of them is obtained by random mismatching of CDR3<italic>β</italic> and peptide sequences from the positive class; we further enlarge the negative set by pairing peptides with CDR3<italic>β</italic>s randomly chosen from the previous bulk repertoire. A summary of this dataset, with the absolute and relative abundances of CDR3<italic>β</italic> sequences for each peptide, can be found in App. 1.</p>
<p>The resulting dataset is plagued with two sources of imbalance:</p>
<list list-type="order">
<list-item><p>positively interacting pairs are strongly under-represented compared to negatively interacting pairs – we refer to this as <italic>class-level</italic> imbalance;</p></list-item>
<list-item><p>within the positive class, few peptides are strongly over-represented compared to others – we refer to this as <italic>group-level</italic> imbalance (<xref ref-type="bibr" rid="c70">Sagawa et al., 2019</xref>; <xref ref-type="bibr" rid="c34">Idrissi et al., 2022</xref>).</p></list-item>
</list>
<p>Notice that there is possibly group imbalance within the negative class based on the different ways to assemble negative sequence pairs through mismatches. We qualitatively observe that this imbalance has minor effects on performances and ignore it in the following.</p>
<p>The group imbalance could, in principle, also hinder the generative process of new peptidespecific CDR3<italic>β</italic> binding pairs for pan-specific models. In fact, a pan-specific generative model trained on peptide and CDR3<italic>β</italic> sequences pairs might effectively learn only the most-represented instances of epitopes, i.e. the most abundant groups in the positive class, making our pipeline ineffective. Comparison of this generative approach with multiple peptide-specific generative models (one for each epitope) shows that the impact of group imbalance is negligible, see App. 7.</p>
</sec>
</sec>
</body>
<back>
<sec id="s7" sec-type="data-availability">
<title>Code and data availability</title>
<p>The code to reproduce the analysis in this paper can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/Eloffredo/RestoringTCR">https://github.com/Eloffredo/RestoringTCR</ext-link>. Sequence data for TCR-peptide predictions are public and were retrieved from the Immune Epitope Database (IEDB) <xref ref-type="bibr" rid="c84">Vita et al. (2018)</xref> as of June 2023 and from TChard <xref ref-type="bibr" rid="c31">Grazioli et al. (2022)</xref>, available at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/6962043">https://zenodo.org/records/6962043</ext-link>; utilities to collect them can be found in the above GitHub repository.</p>
<sec id="s7b">
<title>Author contributions statement</title>
<p>E.L., M.P., S.C., R.M., designed research; E.L., M.P., S.C., R.M. performed research; E.L., M.P. analyzed data; E.L., M.P., S.C., R.M. wrote the paper</p>
</sec>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We acknowledge funding from the CNRS - University of Tokyo “80 Prime” Joint Research Program and from the Agence Nationale de la Recherche (ANR-19 Decrypted CE30-0021-01 to S.C. and R.M.). E.L. thanks Andrea Di Gioacchino for interesting suggestions during the early stages of this work and his help with the use of the alignment software of receptor sequences. M.P. thanks Riccardo Capelli for discussions. The authors thank Victor Greiff, Eugen Ursu and Aygul Minnegalieva for comments and for the careful reading of the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aguirre-López</surname> <given-names>F</given-names></string-name>, <string-name><surname>Franz</surname> <given-names>S</given-names></string-name>, <string-name><surname>Pastore</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Random features and polynomial rules</article-title>. <source>SciPost Phys</source>. <year>2025</year>; <volume>18</volume>:<fpage>039</fpage>. <ext-link ext-link-type="uri" xlink:href="https://scipost.org/10.21468/SciPostPhys.18.1.039">https://scipost.org/10.21468/SciPostPhys.18.1.039</ext-link>, doi: <pub-id pub-id-type="doi">10.21468/SciPostPhys.18.1.039</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Ai</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>P</given-names></string-name>, <string-name><surname>He</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Pan</surname> <given-names>L</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>Z.</given-names></string-name></person-group> <article-title>Generative Oversampling for Imbalanced Data via Majority-Guided VAE</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Ruiz</surname> <given-names>F</given-names></string-name>, <string-name><surname>Dy</surname> <given-names>J</given-names></string-name>, <string-name><surname>van de Meent</surname> <given-names>JW</given-names></string-name></person-group>. <conf-name>Proceedings of The 26th International Conference on Artificial Intelligence and Statistics of Proceedings of Machine Learning Research PMLR</conf-name>, vol. <volume>206</volume>; <year>2023</year>. p. <fpage>3315</fpage>–<lpage>3330</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v206/ai23a.html">https://proceedings.mlr.press/v206/ai23a.html</ext-link>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akbar</surname> <given-names>R</given-names></string-name>, <string-name><surname>Robert</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Weber</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Widrich</surname> <given-names>M</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>R</given-names></string-name>, <string-name><surname>Pavlovic</surname> <given-names>M</given-names></string-name>, <string-name><surname>Scheffer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Chernigovskaya</surname> <given-names>M</given-names></string-name>, <string-name><surname>Snapkov</surname> <given-names>I</given-names></string-name>, <string-name><surname>Slabodkin</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>BB</given-names></string-name>, <string-name><surname>Miho</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lund-Johansen</surname> <given-names>F</given-names></string-name>, <string-name><surname>Andersen</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Hochreiter</surname> <given-names>S</given-names></string-name>, <string-name><surname>Haff</surname> <given-names>IH</given-names></string-name>, <string-name><surname>Klambauer</surname> <given-names>G</given-names></string-name>, <string-name><surname>Sandve</surname> <given-names>GK</given-names></string-name></person-group>, and VG. <article-title>In silico proof of principle of machine learning-based antibody design at unconstrained scale</article-title>. <source>mAbs</source>. <year>2022</year>; <volume>14</volume>(<issue>1</issue>):<fpage>2031482</fpage>. <pub-id pub-id-type="doi">10.1080/19420862.2022.2031482</pub-id>, doi: <pub-id pub-id-type="doi">10.1080/19420862.2022.2031482</pub-id>, pMID: <pub-id pub-id-type="pmid">35377271</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ansari</surname> <given-names>M</given-names></string-name>, <string-name><surname>White</surname> <given-names>AD</given-names></string-name></person-group>. <article-title>Learning peptide properties with positive examples only</article-title>. <source>Digital Discovery</source>. <year>2024</year>; <volume>3</volume>:<fpage>977</fpage>–<lpage>986</lpage>. <pub-id pub-id-type="doi">10.1039/D3DD00218G</pub-id>, doi: <pub-id pub-id-type="doi">10.1039/D3DD00218G</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bagaev</surname> <given-names>DV</given-names></string-name>, <string-name><surname>Vroomans</surname> <given-names>RMA</given-names></string-name>, <string-name><surname>Samir</surname> <given-names>J</given-names></string-name>, <string-name><surname>Stervbo</surname> <given-names>U</given-names></string-name>, <string-name><surname>Rius</surname> <given-names>C</given-names></string-name>, <string-name><surname>Dolton</surname> <given-names>G</given-names></string-name>, <string-name><surname>Greenshields-Watson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Attaf</surname> <given-names>M</given-names></string-name>, <string-name><surname>Egorov</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Zvyagin</surname> <given-names>IV</given-names></string-name>, <string-name><surname>Babel</surname> <given-names>N</given-names></string-name>, <string-name><surname>Cole</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Godkin</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Sewell</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Kesmir</surname> <given-names>C</given-names></string-name>, <string-name><surname>Chudakov</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Luciani</surname> <given-names>F</given-names></string-name>, <string-name><surname>Shugay</surname> <given-names>M.</given-names></string-name></person-group> <article-title>VDJdb in 2019: database extension, new analysis infrastructure and a T-cell receptor motif compendium</article-title>. <source>Nucleic Acids Research</source>. <year>2019</year> 10; <volume>48</volume>(<issue>D1</issue>):<fpage>D1057</fpage>–<lpage>D1062</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkz874</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/nar/gkz874</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bietti</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mairal</surname> <given-names>J.</given-names></string-name></person-group> <chapter-title>On the Inductive Bias of Neural Tangent Kernels</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Wallach</surname> <given-names>H</given-names></string-name>, <string-name><surname>Larochelle</surname> <given-names>H</given-names></string-name>, <string-name><surname>Beygelzimer</surname> <given-names>A</given-names></string-name>, <string-name><surname>d’Alché-Buc</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fox</surname> <given-names>E</given-names></string-name>, <string-name><surname>Garnett</surname> <given-names>R</given-names></string-name>, editors</person-group>. <source>Advances in Neural Information Processing Systems</source>, vol. <volume>32</volume> <publisher-name>Curran Associates, Inc</publisher-name>.; <year>2019</year>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2019/file/c4ef9c39b300931b69a36fb3dbb8d60e-Paper.pdf">https://proceedings.neurips.cc/paper/2019/file/c4ef9c39b300931b69a36fb3dbb8d60e-Paper.pdf</ext-link>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bravi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Balachandran</surname> <given-names>VP</given-names></string-name>, <string-name><surname>Greenbaum</surname> <given-names>BD</given-names></string-name>, <string-name><surname>Walczak</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Mora</surname> <given-names>T</given-names></string-name>, <string-name><surname>Monasson</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cocco</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Probing T-cell response by sequence-based probabilistic modeling</article-title>. <source>PLOS Computational Biology</source>. <year>2021</year> 09; <volume>17</volume>(<issue>9</issue>):<fpage>1</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009297</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009297</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bravi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Di Gioacchino</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fernandez-de Cossio-Diaz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Walczak</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Mora</surname> <given-names>T</given-names></string-name>, <string-name><surname>Cocco</surname> <given-names>S</given-names></string-name>, <string-name><surname>Monasson</surname> <given-names>R.</given-names></string-name></person-group> <article-title>A transferlearning approach to predict antigen immunogenicity and T-cell receptor specificity</article-title>. <source>eLife</source>. <year>2023</year> <month>sep</month>; <volume>12</volume>:<elocation-id>e85126</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.85126</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/eLife.85126</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chaudhuri</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ahuja</surname> <given-names>K</given-names></string-name>, <string-name><surname>Arjovsky</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lopez-Paz</surname> <given-names>D.</given-names></string-name></person-group> <article-title>Why does Throwing Away Data Improve Worst-Group Error?</article-title> In: <person-group person-group-type="editor"><string-name><surname>Krause</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brunskill</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cho</surname> <given-names>K</given-names></string-name>, <string-name><surname>Engelhardt</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sabato</surname> <given-names>S</given-names></string-name>, <string-name><surname>Scarlett</surname> <given-names>J</given-names></string-name></person-group>. <conf-name>Proceedings of the 40th International Conference on Machine Learning of Proceedings of Machine Learning Research</conf-name>, vol. <volume>202</volume> <year>2023</year>. p. <fpage>4144</fpage>–<lpage>4188</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v202/chaudhuri23a.html">https://proceedings.mlr.press/v202/chaudhuri23a.html</ext-link>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kyathanahally</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Reyes</surname> <given-names>M</given-names></string-name>, <string-name><surname>Merkli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Merz</surname> <given-names>E</given-names></string-name>, <string-name><surname>Francazi</surname> <given-names>E</given-names></string-name>, <string-name><surname>Hoege</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pomati</surname> <given-names>F</given-names></string-name></person-group>, <article-title>Baity-Jesi Producing plankton classifiers that are robust to dataset shift</article-title>. <source>Limnology and Oceanography: Methods</source>. <year>2025</year>; <volume>23</volume>(<issue>1</issue>):<fpage>39</fpage>–<lpage>66</lpage>. <ext-link ext-link-type="uri" xlink:href="https://aslopubs.onlinelibrary.wiley.com/doi/abs/10.1002/lom3.10659">https://aslopubs.onlinelibrary.wiley.com/doi/abs/10.1002/lom3.10659</ext-link>, doi: <pub-id pub-id-type="doi">10.1002/lom3.10659</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheng</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>S</given-names></string-name>, <string-name><surname>Guan</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Computationally predicting protein-RNA interactions using only positive and unlabeled examples</article-title>. <source>Journal of Bioinformatics and Computational Biology</source>. <year>2015</year>; <volume>13</volume>(<issue>03</issue>):<fpage>1541005</fpage>. <pub-id pub-id-type="doi">10.1142/S021972001541005X</pub-id>, doi: <pub-id pub-id-type="doi">10.1142/S021972001541005X</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Chizat</surname> <given-names>L</given-names></string-name>, <string-name><surname>Oyallon</surname> <given-names>E</given-names></string-name>, <string-name><surname>Bach</surname> <given-names>F.</given-names></string-name></person-group> <chapter-title>On Lazy Training in Differentiable Programming</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Wallach</surname> <given-names>H</given-names></string-name>, <string-name><surname>Larochelle</surname> <given-names>H</given-names></string-name>, <string-name><surname>Beygelzimer</surname> <given-names>A</given-names></string-name>, <string-name><surname>d’Alché-Buc</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fox</surname> <given-names>E</given-names></string-name>, <string-name><surname>Garnett</surname> <given-names>R</given-names></string-name>, editors</person-group>. <source>Advances in Neural Information Processing Systems</source>, vol. <volume>32</volume> <publisher-name>Curran Associates, Inc</publisher-name>.; <year>2019</year>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf">https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf</ext-link>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Croce</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bobisse</surname> <given-names>S</given-names></string-name>, <string-name><surname>Moreno</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Schmidt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Guillame</surname> <given-names>P</given-names></string-name>, <string-name><surname>Harari</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gfeller</surname> <given-names>D.</given-names></string-name></person-group> <article-title>Deep learning predictions of TCRepitope interactions reveal epitope-specific chains in dual alpha T cells</article-title>. <source>Nature Communications</source>. <year>2024</year> <month>Apr</month>; <volume>15</volume>(<issue>1</issue>):<fpage>3211</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-024-47461-8</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41467-024-47461-8</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dandi</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Stephan</surname> <given-names>L</given-names></string-name>, <string-name><surname>Krzakala</surname> <given-names>F</given-names></string-name>, <string-name><surname>Loureiro</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zdeborová</surname> <given-names>L</given-names></string-name></person-group>. <chapter-title>Universality laws for Gaussian mixtures in generalized linear models</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Oh</surname> <given-names>A</given-names></string-name>, <string-name><surname>Naumann</surname> <given-names>T</given-names></string-name>, <string-name><surname>Globerson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Saenko</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hardt</surname> <given-names>M</given-names></string-name>, <string-name><surname>Levine</surname> <given-names>S</given-names></string-name>, editors</person-group>. <source>Advances in Neural Information Processing Systems</source>, vol. <volume>36</volume> <publisher-name>Curran Associates, Inc</publisher-name>.; <year>2023</year>. p. <fpage>54754</fpage>–<lpage>54768</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dash</surname> <given-names>P</given-names></string-name>, <string-name><surname>Fiore-Gartland</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Hertz</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>S</given-names></string-name>, <string-name><surname>Souquette</surname> <given-names>A</given-names></string-name>, <string-name><surname>Crawford</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Clemens</surname> <given-names>EB</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>THO</given-names></string-name>, <string-name><surname>Kedzierska</surname> <given-names>K</given-names></string-name>, <string-name><surname>La Gruta</surname> <given-names>NL</given-names></string-name>, <string-name><surname>Bradley</surname> <given-names>P</given-names></string-name>, <string-name><surname>Thomas</surname> <given-names>PG</given-names></string-name></person-group>. <article-title>Quantifiable predictive features define epitope-specific T cell receptor repertoires</article-title>. <source>Nature</source>. <year>2017</year> <month>Jul</month>; <volume>547</volume>(<issue>7661</issue>):<fpage>89</fpage>–<lpage>93</lpage>. <pub-id pub-id-type="doi">10.1038/nature22383</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/nature22383</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dean</surname> <given-names>J</given-names></string-name>, <string-name><surname>Emerson</surname> <given-names>RO</given-names></string-name>, <string-name><surname>Vignali</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sherwood</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Rieder</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Carlson</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Robins</surname> <given-names>HS</given-names></string-name></person-group>. <article-title>Annotation of pseudogenic gene segments by massively parallel sequencing of rearranged lymphocyte receptor loci</article-title>. <source>Genome Medicine</source>. <year>2015</year> <month>Nov</month>; <volume>7</volume>(<issue>1</issue>):<fpage>123</fpage>. <pub-id pub-id-type="doi">10.1186/s13073-015-0238-z</pub-id>, doi: <pub-id pub-id-type="doi">10.1186/s13073-015-0238-z</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Del Giudice</surname>, <given-names>P</given-names></string-name>, <string-name><surname>Franz</surname>, <given-names>S</given-names></string-name>, <string-name><surname>Virasoro</surname>, <given-names>M A.</given-names></string-name></person-group> <article-title>Perceptron beyond the limit of capacity</article-title>. <source>J Phys France</source>. <year>1989</year>; <volume>50</volume>(<issue>2</issue>):<fpage>121</fpage>–<lpage>134</lpage>. <pub-id pub-id-type="doi">10.1051/jphys:01989005002012100</pub-id>, doi: <pub-id pub-id-type="doi">10.1051/jphys:01989005002012100</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deng</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ly</surname> <given-names>C</given-names></string-name>, <string-name><surname>Abdollahi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Prinz</surname> <given-names>I</given-names></string-name>, <string-name><surname>Bonn</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Performance comparison of TCR-pMHC prediction tools reveals a strong data dependency</article-title>. <source>Frontiers in Immunology</source>. <year>2023</year>; <volume>14</volume>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2023.1128326">https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2023.1128326</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fimmu.2023.1128326</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dens</surname> <given-names>C</given-names></string-name>, <string-name><surname>Laukens</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bittremieux</surname> <given-names>W</given-names></string-name>, <string-name><surname>Meysman</surname> <given-names>P.</given-names></string-name></person-group> <article-title>The pitfalls of negative data bias for the T-cell epitope specificity challenge</article-title>. <source>Nature Machine Intelligence</source>. <year>2023</year> <month>Oct</month>; <volume>5</volume>(<issue>10</issue>):<fpage>1060</fpage>–<lpage>1062</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-023-00727-0</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s42256-023-00727-0</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K</given-names></string-name></person-group>, <source>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</source>; <year>2019</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</ext-link>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dietrich</surname> <given-names>R</given-names></string-name>, <string-name><surname>Opper</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sompolinsky</surname> <given-names>H.</given-names></string-name></person-group> <article-title>Statistical Mechanics of Support Vector Networks</article-title>. <source>Phys Rev Lett</source>. <year>1999</year> 04; <volume>82</volume>:<fpage>2975</fpage>–<lpage>2978</lpage>. <ext-link ext-link-type="uri" xlink:href="https://link.aps.org/doi/10.1103/PhysRevLett.82.2975">https://link.aps.org/doi/10.1103/PhysRevLett.82.2975</ext-link>, doi: <pub-id pub-id-type="doi">10.1103/PhysRevLett.82.2975</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Douzas</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bacao</surname> <given-names>F.</given-names></string-name></person-group> <article-title>Effective data generation for imbalanced learning using conditional generative adversarial networks</article-title>. <source>Expert Systems with Applications</source>. <year>2018</year>; <volume>91</volume>:<fpage>464</fpage>–<lpage>471</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0957417417306346">https://www.sciencedirect.com/science/article/pii/S0957417417306346</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.eswa.2017.09.030</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Emerson</surname> <given-names>RO</given-names></string-name>, <string-name><surname>DeWitt</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Vignali</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gravley</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Osborne</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Desmarais</surname> <given-names>C</given-names></string-name>, <string-name><surname>Klinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Carlson</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Hansen</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Rieder</surname> <given-names>M</given-names></string-name>, <string-name><surname>Robins</surname> <given-names>HS</given-names></string-name></person-group>. <article-title>Immunosequencing identifies signatures of cytomegalovirus exposure history and HLA-mediated effects on the T cell repertoire</article-title>. <source>Nature Genetics</source>. <year>2017</year> <month>May</month>; <volume>49</volume>(<issue>5</issue>):<fpage>659</fpage>–<lpage>665</lpage>. <pub-id pub-id-type="doi">10.1038/ng.3822</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/ng.3822</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fernández</surname> <given-names>A</given-names></string-name>, <string-name><surname>García</surname> <given-names>S</given-names></string-name>, <string-name><surname>Galar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Prati</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Krawczyk</surname> <given-names>B</given-names></string-name>, <string-name><surname>Herrera</surname> <given-names>F.</given-names></string-name></person-group> <source>Learning from Imbalanced Data Sets</source>. <publisher-name>Springer Cham</publisher-name>; <year>2018</year>. doi: <pub-id pub-id-type="doi">10.1007/978-3-319-98074-4</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Forman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Scholz</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement</article-title>. <source>SIGKDD Explor Newsl</source>. <year>2010</year> <month>Nov</month>; <volume>12</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>57</lpage>. <pub-id pub-id-type="doi">10.1145/1882471.1882479</pub-id>, doi: <pub-id pub-id-type="doi">10.1145/1882471.1882479</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fotouhi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Asadi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kattan</surname> <given-names>MW</given-names></string-name></person-group>. <article-title>A comprehensive data level analysis for cancer diagnosis on imbalanced data</article-title>. <source>Journal of Biomedical Informatics</source>. <year>2019</year>; <volume>90</volume>:<fpage>103089</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1532046418302302">https://www.sciencedirect.com/science/article/pii/S1532046418302302</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.jbi.2018.12.003</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>de G Matthews</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Hron</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rowland</surname> <given-names>M</given-names></string-name>, <string-name><surname>Turner</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Ghahramani</surname> <given-names>Z.</given-names></string-name></person-group> <article-title>Gaussian Process Behaviour in Wide Deep Neural Networks</article-title>. <source>In: International Conference on Learning Representations</source>; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=H1-nGgWC">https://openreview.net/forum?id=H1-nGgWC</ext-link>-.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gerace</surname> <given-names>F</given-names></string-name>, <string-name><surname>Loureiro</surname> <given-names>B</given-names></string-name>, <string-name><surname>Krzakala</surname> <given-names>F</given-names></string-name>, <string-name><surname>Mézard</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zdeborová</surname> <given-names>L</given-names></string-name></person-group>. <article-title>Generalisation error in learning with random features and the hidden manifold model</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source>. <year>2021</year> 12; <volume>2021</volume>(<issue>12</issue>):<fpage>124013</fpage>. <pub-id pub-id-type="doi">10.1088/1742-5468/ac3ae6</pub-id>, doi: <pub-id pub-id-type="doi">10.1088/1742-5468/ac3ae6</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ghoreyshi</surname> <given-names>ZS</given-names></string-name>, <string-name><surname>George</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Quantitative approaches for decoding the specificity of the human T cell repertoire</article-title>. <source>Frontiers in Immunology</source>. <year>2023</year>; Volume <volume>14</volume> - 2023. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2023.1228873">https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2023.1228873</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fimmu.2023.1228873</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gielis</surname> <given-names>S</given-names></string-name>, <string-name><surname>Moris</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bittremieux</surname> <given-names>W</given-names></string-name>, <string-name><surname>De Neuter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ogunjimi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Laukens</surname> <given-names>K</given-names></string-name>, <string-name><surname>Meysman</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Detection of Enriched T Cell Epitope Specificity in Full T Cell Receptor Sequence Repertoires</article-title>. <source>Frontiers in Immunology</source>. <year>2019</year>; Volume <volume>10</volume> - 2019. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2019.02820">https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2019.02820</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fimmu.2019.02820</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grazioli</surname> <given-names>F</given-names></string-name>, <string-name><surname>Mösch</surname> <given-names>A</given-names></string-name>, <string-name><surname>Machart</surname> <given-names>P</given-names></string-name>, <string-name><surname>Li</surname> <given-names>K</given-names></string-name>, <string-name><surname>Alqassem</surname> <given-names>I</given-names></string-name>, <string-name><surname>O’Donnell</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Min</surname> <given-names>MR</given-names></string-name></person-group>. <article-title>On TCR binding predictors failing to generalize to unseen peptides</article-title>. <source>Frontiers in Immunology</source>. <year>2022</year>; <volume>13</volume>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2022.1014256">https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2022.1014256</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fimmu.2022.1014256</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haque</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Skinner</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Holder</surname> <given-names>LB</given-names></string-name></person-group>. <article-title>Imbalanced Class Learning in Epigenetics</article-title>. <source>Journal of Computational Biology</source>. <year>2014</year>; <volume>21</volume>(<issue>7</issue>):<fpage>492</fpage>–<lpage>507</lpage>. <pub-id pub-id-type="doi">10.1089/cmb.2014.0008</pub-id>, doi: <pub-id pub-id-type="doi">10.1089/cmb.2014.0008</pub-id>, pMID: <pub-id pub-id-type="pmid">24798423</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henikoff</surname> <given-names>S</given-names></string-name>, <string-name><surname>Henikoff</surname> <given-names>JG</given-names></string-name></person-group>. <article-title>Amino acid substitution matrices from protein blocks</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1992</year>; <volume>89</volume>(<issue>22</issue>):<fpage>10915</fpage>–<lpage>10919</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.89.22.10915">https://www.pnas.org/doi/abs/10.1073/pnas.89.22.10915</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.89.22.10915</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Idrissi</surname> <given-names>BY</given-names></string-name>, <string-name><surname>Arjovsky</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pezeshki</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lopez-Paz</surname> <given-names>D.</given-names></string-name></person-group> <article-title>Simple data balancing achieves competitive worst-group-accuracy</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Schölkopf</surname> <given-names>B</given-names></string-name>, <string-name><surname>Uhler</surname> <given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>K</given-names></string-name></person-group>. <conf-name>Proceedings of the First Conference on Causal Learning and Reasoning of Proceedings of Machine Learning Research</conf-name>, vol. <volume>177</volume> <year>2022</year>. p. <fpage>336</fpage>–<lpage>351</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v177/idrissi22a.html">https://proceedings.mlr.press/v177/idrissi22a.html</ext-link>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Isacchini</surname> <given-names>G</given-names></string-name>, <string-name><surname>Walczak</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Mora</surname> <given-names>T</given-names></string-name>, <string-name><surname>Nourmohammad</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Deep generative selection models of T and B cell receptor repertoires with soNNia</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2021</year>; <volume>118</volume>(<issue>14</issue>):<fpage>e2023141118</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.2023141118">https://www.pnas.org/doi/abs/10.1073/pnas.2023141118</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.2023141118</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Jacot</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gabriel</surname> <given-names>F</given-names></string-name>, <string-name><surname>Hongler</surname> <given-names>C.</given-names></string-name></person-group> <chapter-title>Neural Tangent Kernel: Convergence and Generalization in Neural Networks</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Bengio</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wallach</surname> <given-names>H</given-names></string-name>, <string-name><surname>Larochelle</surname> <given-names>H</given-names></string-name>, <string-name><surname>Grauman</surname> <given-names>K</given-names></string-name>, <string-name><surname>Cesa-Bianchi</surname> <given-names>N</given-names></string-name>, <string-name><surname>Garnett</surname> <given-names>R</given-names></string-name>, editors</person-group>. <source>Advances in Neural Information Processing Systems</source>, vol. <volume>31</volume> <publisher-name>Curran Associates, Inc</publisher-name>.; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf</ext-link>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jacquin</surname> <given-names>H</given-names></string-name>, <string-name><surname>Gilson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shakhnovich</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cocco</surname> <given-names>S</given-names></string-name>, <string-name><surname>Monasson</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Benchmarking Inverse Statistical Approaches for Protein Structure and Design with Exactly Solvable Models</article-title>. <source>PLOS Computational Biology</source>. <year>2016</year> 05; <volume>12</volume>(<issue>5</issue>):<fpage>1</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004889</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004889</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jensen</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Nielsen</surname> <given-names>M.</given-names></string-name></person-group> <article-title>NetTCR 2.2 - Improved TCR specificity predictions by combining pan- and peptide-specific training strategies, loss-scaling and integration of sequence similarity</article-title>. <source>eLife</source>. <year>2023</year> <month>Oct</month>; <volume>12</volume>:<elocation-id>RP93934</elocation-id>. <pub-id pub-id-type="doi">10.1101/2023.10.12.562001</pub-id>, doi: <pub-id pub-id-type="doi">10.1101/2023.10.12.562001</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pepin</surname> <given-names>F</given-names></string-name>, <string-name><surname>Wilkins</surname> <given-names>J</given-names></string-name>, <string-name><surname>Asbury</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wittkop</surname> <given-names>T</given-names></string-name>, <string-name><surname>Zheng</surname> <given-names>J</given-names></string-name>, <string-name><surname>Moorhead</surname> <given-names>M</given-names></string-name>, <string-name><surname>Faham</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Multiplex Identification of Antigen-Specific T Cell Receptors Using a Combination of Immune Assays and Immune Receptor Sequenc-ing</article-title>. <source>PLOS One</source>. <year>2015</year> 10; <volume>10</volume>(<issue>10</issue>):<fpage>1</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0141561</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/jour-nal.pone.0141561</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krawczyk</surname> <given-names>B</given-names></string-name>, <string-name><surname>Galar</surname> <given-names>M</given-names></string-name>, <string-name><given-names>Lukasz</given-names> <surname>Jelen</surname></string-name>, <string-name><surname>Herrera</surname> <given-names>F.</given-names></string-name></person-group> <article-title>Evolutionary undersampling boosting for imbalanced classifi-cation of breast cancer malignancy</article-title>. <source>Applied Soft Computing</source>. <year>2016</year>; <volume>38</volume>:<fpage>714</fpage>–<lpage>726</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1568494615005815">https://www.sciencedirect.com/science/article/pii/S1568494615005815</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.asoc.2015.08.060</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kyathanahally</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Hardeman</surname> <given-names>T</given-names></string-name>, <string-name><surname>Merz</surname> <given-names>E</given-names></string-name>, <string-name><surname>Bulas</surname> <given-names>T</given-names></string-name>, <string-name><surname>Reyes</surname> <given-names>M</given-names></string-name>, <string-name><surname>Isles</surname> <given-names>P</given-names></string-name>, <string-name><surname>Pomati</surname> <given-names>F</given-names></string-name>, <string-name><surname>Baity-Jesi</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Deep Learning Clas-sification of Lake Zooplankton</article-title>. <source>Frontiers in Microbiology</source>. <year>2021</year>; <volume>12</volume>. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/microbiology/articles/10.3389/fmicb.2021.746297">https://www.frontiersin.org/journals/microbiology/articles/10.3389/fmicb.2021.746297</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fmicb.2021.746297</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Lee</surname> <given-names>J</given-names></string-name>, <string-name><surname>Sohl-dickstein</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pennington</surname> <given-names>J</given-names></string-name>, <string-name><surname>Novak</surname> <given-names>R</given-names></string-name>, <string-name><surname>Schoenholz</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bahri</surname> <given-names>Y.</given-names></string-name></person-group> <article-title>Deep Neural Networks as Gaussian Pro-cesses</article-title>. <source>In: International Conference on Learning Representations</source>; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=B1EA-M-0Z">https://openreview.net/forum?id=B1EA-M-0Z</ext-link>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>H</given-names></string-name>, <string-name><surname>Helling</surname> <given-names>R</given-names></string-name>, <string-name><surname>Tang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wingreen</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Emergence of Preferred Structures in a Simple Model of Protein Folding</article-title>. <source>Science</source>. <year>1996</year>; <volume>273</volume>(<issue>5275</issue>):<fpage>666</fpage>–<lpage>669</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/abs/10.1126/science.273.5275.666">https://www.science.org/doi/abs/10.1126/science.273.5275.666</ext-link>, doi: <pub-id pub-id-type="doi">10.1126/science.273.5275.666</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>W</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Li</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zheng</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>P</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>B.</given-names></string-name></person-group> <article-title>Protein-Protein Interaction Sites Prediction Based on an Under-Sampling Strategy and Random Forest Algorithm</article-title>. <source>IEEE/ACM Transactions on Computational Biology and Bioinformatics</source>. <year>2022</year>; <volume>19</volume>(<issue>6</issue>):<fpage>3646</fpage>–<lpage>3654</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TCBB.2021.3123269</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lobo</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Jiménez-Valverde</surname> <given-names>A</given-names></string-name>, <string-name><surname>Real</surname> <given-names>R.</given-names></string-name></person-group> <article-title>AUC: a misleading measure of the performance of predictive distribution models</article-title>. <source>Global Ecology and Biogeography</source>. <year>2008</year>; <volume>17</volume>(<issue>2</issue>):<fpage>145</fpage>–<lpage>151</lpage>. <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1466-8238.2007.00358.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1466-8238.2007.00358.x</ext-link>, doi: <pub-id pub-id-type="doi">10.1111/j.1466-8238.2007.00358.x</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Loffredo</surname> <given-names>E</given-names></string-name>, <string-name><surname>Vesconi</surname> <given-names>E</given-names></string-name>, <string-name><surname>Razban</surname> <given-names>R</given-names></string-name>, <string-name><surname>Peleg</surname> <given-names>O</given-names></string-name>, <string-name><surname>Shakhnovich</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cocco</surname> <given-names>S</given-names></string-name>, <string-name><surname>Monasson</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Evolutionary dynamics of a lattice dimer: a toy model for stability vs. affinity trade-offs in proteins</article-title>. <source>Journal of Physics A: Mathematical and Theoretical</source>. <year>2023</year> <month>oct</month>; <volume>56</volume>(<issue>45</issue>):<fpage>455002</fpage>. <pub-id pub-id-type="doi">10.1088/1751-8121/acfddc</pub-id>, doi: <pub-id pub-id-type="doi">10.1088/1751-8121/acfddc</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Loffredo</surname> <given-names>E</given-names></string-name>, <string-name><surname>Pastore</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cocco</surname> <given-names>S</given-names></string-name>, <string-name><surname>Monasson</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Restoring balance: principled under/oversampling of data for optimal classification</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Salakhutdinov</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kolter</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Heller</surname> <given-names>K</given-names></string-name>, <string-name><surname>Weller</surname> <given-names>A</given-names></string-name>, <string-name><surname>Oliver</surname> <given-names>N</given-names></string-name>, <string-name><surname>Scarlett</surname> <given-names>J</given-names></string-name>, <string-name><surname>Berkenkamp</surname> <given-names>F</given-names></string-name></person-group>. <conf-name>Proceedings of the 41st International Conference on Machine Learning of Proceedings of Machine Learning Research</conf-name>, vol. <volume>235</volume> <year>2024</year>. p. <fpage>32643</fpage>–<lpage>32670</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v235/loffredo24a.html">https://proceedings.mlr.press/v235/loffredo24a.html</ext-link>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Loureiro</surname> <given-names>B</given-names></string-name>, <string-name><surname>Gerbelot</surname> <given-names>C</given-names></string-name>, <string-name><surname>Cui</surname> <given-names>H</given-names></string-name>, <string-name><surname>Goldt</surname> <given-names>S</given-names></string-name>, <string-name><surname>Krzakala</surname> <given-names>F</given-names></string-name>, <string-name><surname>Mezard</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zdeborová</surname> <given-names>L</given-names></string-name></person-group>. <chapter-title>Learning curves of generic features maps for realistic datasets with a teacher-student model</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Ranzato</surname> <given-names>M</given-names></string-name>, <string-name><surname>Beygelzimer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dauphin</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Liang</surname> <given-names>PS</given-names></string-name>, <string-name><surname>Vaughan</surname> <given-names>JW</given-names></string-name>, editors</person-group>. <source>Advances in Neural Information Processing Systems</source>, vol. <volume>34</volume> <publisher-name>Cur-ran Associates, Inc</publisher-name>.; <year>2021</year>. p. <fpage>18137</fpage>–<lpage>18151</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/2021/file/9704a4fc48ae88598dcbdcdf57f3fdef-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2021/file/9704a4fc48ae88598dcbdcdf57f3fdef-Paper.pdf</ext-link>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Loureiro</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sicuro</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gerbelot</surname> <given-names>C</given-names></string-name>, <string-name><surname>Pacco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Krzakala</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zdeborová</surname> <given-names>L</given-names></string-name></person-group>. <chapter-title>Learning Gaussian Mixtures with Generalized Linear Models: Precise Asymptotics in High-dimensions</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Ranzato</surname> <given-names>M</given-names></string-name>, <string-name><surname>Beygelzimer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dauphin</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Liang</surname> <given-names>PS</given-names></string-name>, <string-name><surname>Vaughan</surname> <given-names>JW</given-names></string-name>, editors</person-group>. <source>Advances in Neural Information Processing Systems</source>, vol. <volume>34</volume> <publisher-name>Cur-ran Associates, Inc</publisher-name>.; <year>2021</year>. p. <fpage>10144</fpage>–<lpage>10157</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/2021/file/543e83748234f7cbab21aa0ade66565f-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2021/file/543e83748234f7cbab21aa0ade66565f-Paper.pdf</ext-link>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Jiang</surname> <given-names>P</given-names></string-name>, <string-name><surname>Xiao</surname> <given-names>X</given-names></string-name>, <string-name><surname>Bernatchez</surname> <given-names>C</given-names></string-name>, <string-name><surname>Heymach</surname> <given-names>JV</given-names></string-name>, <string-name><surname>Gibbons</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Reuben</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>T.</given-names></string-name></person-group> <article-title>Deep learning-based prediction of the T cell receptor–antigen binding specificity</article-title>. <source>Nature Machine Intelligence</source>. <year>2021</year> <month>Oct</month>; <volume>3</volume>(<issue>10</issue>):<fpage>864</fpage>–<lpage>875</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-021-00383-2</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s42256-021-00383-2</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Mannelli</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Gerace</surname> <given-names>F</given-names></string-name>, <string-name><surname>Rostamzadeh</surname> <given-names>N</given-names></string-name>, <string-name><surname>Saglietti</surname> <given-names>L</given-names></string-name></person-group>, <source>Bias-inducing geometries: an exactly solvable data model with fairness implications</source>; <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2205.15935">https://arxiv.org/abs/2205.15935</ext-link>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marouf</surname> <given-names>M</given-names></string-name>, <string-name><surname>Machart</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bansal</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kilian</surname> <given-names>C</given-names></string-name>, <string-name><surname>Magruder</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Krebs</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Bonn</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Realistic in silico generation and augmentation of single-cell RNA-seq data using generative adversarial networks</article-title>. <source>Nature Communications</source>. <year>2020</year> <month>Jan</month>; <volume>11</volume>(<issue>1</issue>):<fpage>166</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-14018-z</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41467-019-14018-z</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mason</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>ST</given-names></string-name></person-group>. <article-title>Predicting adaptive immune receptor specificities by machine learning is a data gen-eration problem</article-title>. <source>Cell Systems</source>. <year>2024</year>; <volume>15</volume>(<issue>12</issue>):<fpage>1190</fpage>–<lpage>1197</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S2405471224003442">https://www.sciencedirect.com/science/article/pii/S2405471224003442</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cels.2024.11.008</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meynard-Piganeau</surname> <given-names>B</given-names></string-name>, <string-name><surname>Feinauer</surname> <given-names>C</given-names></string-name>, <string-name><surname>Weigt</surname> <given-names>M</given-names></string-name>, <string-name><surname>Walczak</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Mora</surname> <given-names>T.</given-names></string-name></person-group> <article-title>TULIP: A transformer-based unsupervised language model for interacting peptides and T cell receptors that generalizes to unseen epitopes</article-title>. <source>Proceed-ings of the National Academy of Sciences</source>. <year>2024</year>; <volume>121</volume>(<issue>24</issue>):<fpage>e2316401121</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.2316401121">https://www.pnas.org/doi/abs/10.1073/pnas.2316401121</ext-link>, doi: <pub-id pub-id-type="doi">10.1073/pnas.2316401121</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meysman</surname> <given-names>P</given-names></string-name>, <string-name><surname>Barton</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bravi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Cohen-Lavi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Karnaukhov</surname> <given-names>V</given-names></string-name>, <string-name><surname>Lilleskov</surname> <given-names>E</given-names></string-name>, <string-name><surname>Montemurro</surname> <given-names>A</given-names></string-name>, <string-name><surname>Nielsen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Mora</surname> <given-names>T</given-names></string-name>, <string-name><surname>Pereira</surname> <given-names>P</given-names></string-name>, <string-name><surname>Postovskaya</surname> <given-names>A</given-names></string-name>, <string-name><surname>Martínez</surname> <given-names>MR</given-names></string-name>, <string-name><surname>de Cossio-Diaz</surname> <given-names>JF</given-names></string-name>, <string-name><surname>Vujkovic</surname> <given-names>A</given-names></string-name>, <string-name><surname>Walczak</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Weber</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yin</surname> <given-names>R</given-names></string-name>, <string-name><surname>Eug-ster</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>V.</given-names></string-name></person-group> <article-title>Benchmarking solutions to the T-cell receptor epitope prediction problem: IMMREP22 workshop report</article-title>. <source>ImmunoInformatics</source>. <year>2023</year>; <volume>9</volume>:<fpage>100024</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S2667119023000046">https://www.sciencedirect.com/science/article/pii/S2667119023000046</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.immuno.2023.100024</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ming</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Yuan</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Xia</surname> <given-names>H.</given-names></string-name></person-group> <article-title>HostNet: improved sequence representation in deep neural networks for virus-host prediction</article-title>. <source>BMC Bioinformatics</source>. <year>2023</year> <month>Dec</month>; <volume>24</volume>(<issue>1</issue>):<fpage>455</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-023-05582-9</pub-id>, doi: <pub-id pub-id-type="doi">10.1186/s12859-023-05582-9</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mirny</surname> <given-names>L</given-names></string-name>, <string-name><surname>Shakhnovich</surname> <given-names>E.</given-names></string-name></person-group> <article-title>Protein Folding Theory: From Lattice to All-Atom Models</article-title>. <source>Annual Review of Biophysics</source>. <year>2001</year>; <volume>30</volume>:<fpage>361</fpage>–<lpage>396</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.annualreviews.org/content/journals/10.1146/annurev.biophys.30.1.361">https://www.annualreviews.org/content/journals/10.1146/annurev.biophys.30.1.361</ext-link>, doi: <pub-id pub-id-type="doi">10.1146/annurev.biophys.30.1.361</pub-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mirza</surname> <given-names>B</given-names></string-name>, <string-name><surname>Haroon</surname> <given-names>D</given-names></string-name>, <string-name><surname>Khan</surname> <given-names>B</given-names></string-name>, <string-name><surname>Padhani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Syed</surname> <given-names>TQ</given-names></string-name></person-group>. <article-title>Deep Generative Models to Counter Class Imbalance: A Model-Metric Mapping With Proportion Calibration Methodology</article-title>. <source>IEEE Access</source>. <year>2021</year>; <volume>9</volume>:<fpage>55879</fpage>–<lpage>55897</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3071389</pub-id>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Mondal</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Singhal</surname> <given-names>L</given-names></string-name>, <string-name><surname>Tiwary</surname> <given-names>P</given-names></string-name>, <string-name><surname>Singla</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ap</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Minority Oversampling for Imbalanced Data via Class-Preserving Regularized Auto-Encoders</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Ruiz</surname> <given-names>F</given-names></string-name>, <string-name><surname>Dy</surname> <given-names>J</given-names></string-name>, <string-name><surname>van de Meent</surname> <given-names>JW</given-names></string-name>,</person-group>. <conf-name>Proceedings of The 26th International Conference on Artificial Intelligence and Statistics of Proceedings of Machine Learning Research</conf-name>, vol. <volume>206</volume> <year>2023</year>. p. <fpage>3440</fpage>–<lpage>3465</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v206/mondal23a.html">https://proceedings.mlr.press/v206/mondal23a.html</ext-link>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Montemurro</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schuster</surname> <given-names>V</given-names></string-name>, <string-name><surname>Povlsen</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Bentzen</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Jurtz</surname> <given-names>V</given-names></string-name>, <string-name><surname>Chronister</surname> <given-names>WD</given-names></string-name>, <string-name><surname>Crinklaw</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hadrup</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Winther</surname> <given-names>O</given-names></string-name>, <string-name><surname>Peters</surname> <given-names>B</given-names></string-name>, <string-name><surname>Jessen</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Nielsen</surname> <given-names>M.</given-names></string-name></person-group> <article-title>NetTCR-2.0 enables accurate prediction of TCR-peptide binding by using paired TCRα and β sequence data</article-title>. <source>Communications Biology</source>. <year>2021</year> <month>Sep</month>; <volume>4</volume>(<issue>1</issue>):<fpage>1060</fpage>. <pub-id pub-id-type="doi">10.1038/s42003-021-02610-3</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s42003-021-02610-3</pub-id>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nagano</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Pyo</surname> <given-names>AGT</given-names></string-name>, <string-name><surname>Milighetti</surname> <given-names>M</given-names></string-name>, <string-name><surname>Henderson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Shawe-Taylor</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chain</surname> <given-names>B</given-names></string-name>, <string-name><surname>Tiffeau-Mayer</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Contrastive learning of T cell receptor representations</article-title>. <source>Cell Systems</source>. <year>2025</year> <month>Jan</month>; <volume>16</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1016/j.cels.2024.12.006</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/j.cels.2024.12.006</pub-id>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Neal</surname> <given-names>RM</given-names></string-name></person-group>. <source>Priors for Infinite Networks</source>: <publisher-name>Springer</publisher-name> <publisher-loc>New York</publisher-loc>; <year>1996</year>. p. <fpage>29</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.1007/978-1-4612-0745-0_2</pub-id>, doi: <pub-id pub-id-type="doi">10.1007/978-1-4612-0745-0_2</pub-id>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nolan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vignali</surname> <given-names>M</given-names></string-name>, <string-name><surname>Klinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dines</surname> <given-names>JN</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>IM</given-names></string-name>, <string-name><surname>Svejnoha</surname> <given-names>E</given-names></string-name>, <string-name><surname>Craft</surname> <given-names>T</given-names></string-name>, <string-name><surname>Boland</surname> <given-names>K</given-names></string-name>, <string-name><surname>Pesesky</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Gittelman</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Snyder</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Gooley</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Semprini</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cerchione</surname> <given-names>C</given-names></string-name>, <string-name><surname>Nicolini</surname> <given-names>F</given-names></string-name>, <string-name><surname>Mazza</surname> <given-names>M</given-names></string-name>, <string-name><surname>Delmonte</surname> <given-names>OM</given-names></string-name>, <string-name><surname>Dobbs</surname> <given-names>K</given-names></string-name>, <string-name><surname>Carreño-Tarragona</surname> <given-names>G</given-names></string-name>, <string-name><surname>Barrio</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>A large-scale database of T-cell receptor beta sequences and binding associations from natural and synthetic exposure to SARS-CoV-2</article-title>. <source>Frontiers in Immunology</source>. <year>2025</year>; Volume <volume>16</volume> - 2025. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2025.1488851">https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2025.1488851</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fimmu.2025.1488851</pub-id>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Pesce</surname> <given-names>L</given-names></string-name>, <string-name><surname>Krzakala</surname> <given-names>F</given-names></string-name>, <string-name><surname>Loureiro</surname> <given-names>B</given-names></string-name>, <string-name><surname>Stephan</surname> <given-names>L.</given-names></string-name></person-group> <article-title>Are Gaussian Data All You Need? The Extents and Limits of Uni-versality in High-Dimensional Generalized Linear Estimation</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Krause</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brunskill</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cho</surname> <given-names>K</given-names></string-name>, <string-name><surname>Engelhardt</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sabato</surname> <given-names>S</given-names></string-name>, <string-name><surname>Scarlett</surname> <given-names>J</given-names></string-name></person-group>. <conf-name>Proceedings of the 40th International Conference on Machine Learning of Proceedings of Machine Learning Research </conf-name>, vol. <volume>202</volume> <year>2023</year>. p. <fpage>27680</fpage>–<lpage>27708</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v202/pesce23a.html">https://proceedings.mlr.press/v202/pesce23a.html</ext-link>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Pezzicoli</surname> <given-names>FS</given-names></string-name>, <string-name><surname>Ros</surname> <given-names>V</given-names></string-name>, <string-name><surname>Landes</surname> <given-names>FP</given-names></string-name>, <string-name><surname>Baity-Jesi</surname> <given-names>M</given-names></string-name></person-group>, <source>Class Imbalance in Anomaly Detection: Learning from an Exactly Solvable Model</source>; <year>2025</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2501.11638">https://arxiv.org/abs/2501.11638</ext-link>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pham</surname> <given-names>MDN</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>TN</given-names></string-name>, <string-name><surname>Tran</surname> <given-names>LS</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>QTB</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>TPH</given-names></string-name>, <string-name><surname>Pham</surname> <given-names>TMQ</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>HN</given-names></string-name>, <string-name><surname>Giang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Phan</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>V.</given-names></string-name></person-group> <article-title>epiTCR: a highly sensitive predictor for TCR–peptide binding</article-title>. <source>Bioinformatics</source>. <year>2023</year> 04; <volume>39</volume>(<issue>5</issue>):<fpage>btad284</fpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btad284</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btad284</pub-id>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rana</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sowmya</surname> <given-names>A</given-names></string-name>, <string-name><surname>Meijering</surname> <given-names>E</given-names></string-name>, <string-name><surname>Song</surname> <given-names>Y.</given-names></string-name></person-group> <article-title>Imbalanced classification for protein subcellular localization with multilabel oversampling</article-title>. <source>Bioinformatics</source>. <year>2022</year> 12; <volume>39</volume>(<issue>1</issue>):<fpage>btac841</fpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac841</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btac841</pub-id>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richardson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Trevizani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Greenbaum</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Carter</surname> <given-names>H</given-names></string-name>, <string-name><surname>Nielsen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Peters</surname> <given-names>B.</given-names></string-name></person-group> <article-title>The receiver operating characteristic curve accurately assesses imbalanced datasets</article-title>. <source>Patterns</source>. <year>2024</year>; <volume>5</volume>(<issue>6</issue>):<fpage>100994</fpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S2666389924001090">https://www.sciencedirect.com/science/article/pii/S2666389924001090</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.patter.2024.100994</pub-id>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robert</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Akbar</surname> <given-names>R</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>R</given-names></string-name>, <string-name><surname>Pavlovic</surname> <given-names>M</given-names></string-name>, <string-name><surname>Widrich</surname> <given-names>M</given-names></string-name>, <string-name><surname>Snapkov</surname> <given-names>I</given-names></string-name>, <string-name><surname>Slabodkin</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chernigovskaya</surname> <given-names>M</given-names></string-name>, <string-name><surname>Scheffer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Smorodina</surname> <given-names>E</given-names></string-name>, <string-name><surname>Rawat</surname> <given-names>P</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>BB</given-names></string-name>, <string-name><surname>Vu</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Mathisen</surname> <given-names>IF</given-names></string-name>, <string-name><surname>Prósz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Abram</surname> <given-names>K</given-names></string-name>, <string-name><surname>Olar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Miho</surname> <given-names>E</given-names></string-name>, <string-name><surname>Haug</surname> <given-names>DTT</given-names></string-name>, <string-name><surname>Lund-Johansen</surname> <given-names>F</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Unconstrained generation of synthetic antibody–antigen structures to guide machine learn-ing methodology for antibody specificity prediction</article-title>. <source>Nature Computational Science</source>. <year>2022</year> <month>Dec</month>; <volume>2</volume>(<issue>12</issue>):<fpage>845</fpage>–<lpage>865</lpage>. <pub-id pub-id-type="doi">10.1038/s43588-022-00372-4</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s43588-022-00372-4</pub-id>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Sagawa</surname> <given-names>S</given-names></string-name>, <string-name><surname>Koh</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Hashimoto</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Liang</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization</article-title>. <source>CoRR</source>. <year>2019</year>; abs/1911.08731. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1911.08731">http://arxiv.org/abs/1911.08731</ext-link>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saito</surname> <given-names>T</given-names></string-name>, <string-name><surname>Rehmsmeier</surname> <given-names>M.</given-names></string-name></person-group> <article-title>The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets</article-title>. <source>PLOS One</source>. <year>2015</year> 03; <volume>10</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0118432</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0118432</pub-id>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sethna</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Isacchini</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dupic</surname> <given-names>T</given-names></string-name>, <string-name><surname>Mora</surname> <given-names>T</given-names></string-name>, <string-name><surname>Walczak</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Elhanati</surname> <given-names>Y.</given-names></string-name></person-group> <article-title>Population variability in the generation and selection of T-cell repertoires</article-title>. <source>PLOS Computational Biology</source>. <year>2020</year> 12; <volume>16</volume>(<issue>12</issue>):<fpage>1</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008394</pub-id>, doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008394</pub-id>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sgarbossa</surname> <given-names>D</given-names></string-name>, <string-name><surname>Lupo</surname> <given-names>U</given-names></string-name>, <string-name><surname>Bitbol</surname> <given-names>AF</given-names></string-name></person-group>. <article-title>Generative power of a protein language model trained on multiple sequence alignments</article-title>. <source>eLife</source>. <year>2023</year> <month>feb</month>; <volume>12</volume>:<elocation-id>e79854</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.79854</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/eLife.79854</pub-id>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shugay</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bagaev</surname> <given-names>DV</given-names></string-name>, <string-name><surname>Zvyagin</surname> <given-names>IV</given-names></string-name>, <string-name><surname>Vroomans</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Crawford</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Dolton</surname> <given-names>G</given-names></string-name>, <string-name><surname>Komech</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Sycheva</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Koneva</surname> <given-names>AE</given-names></string-name>, <string-name><surname>Egorov</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Eliseev</surname> <given-names>AV</given-names></string-name>, <string-name><surname>Van Dyk</surname> <given-names>E</given-names></string-name>, <string-name><surname>Dash</surname> <given-names>P</given-names></string-name>, <string-name><surname>Attaf</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rius</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ladell</surname> <given-names>K</given-names></string-name>, <string-name><surname>McLaren</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Matthews</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Clemens</surname> <given-names>EB</given-names></string-name>, <string-name><surname>Douek</surname> <given-names>DC</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>VDJdb: a curated database of T-cell receptor sequences with known antigen speci-ficity</article-title>. <source>Nucleic Acids Research</source>. <year>2017</year> 09; <volume>46</volume>(<issue>D1</issue>):<fpage>D419</fpage>–<lpage>D427</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkx760</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/nar/gkx760</pub-id>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sidhom</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Larman</surname> <given-names>HB</given-names></string-name>, <string-name><surname>Pardoll</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Baras</surname> <given-names>AS</given-names></string-name></person-group>. <article-title>DeepTCR is a deep learning framework for revealing sequence concepts within T-cell repertoires</article-title>. <source>Nature Communications</source>. <year>2021</year> <month>Mar</month>; <volume>12</volume>(<issue>1</issue>):<fpage>1605</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-21879-w</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41467-021-21879-w</pub-id>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sim</surname> <given-names>MJW</given-names></string-name></person-group>. <article-title>TCRs and AI: the future is now</article-title>. <source>Nature Reviews Immunology</source>. <year>2024</year> <month>Jan</month>; <volume>24</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>3</lpage>. <pub-id pub-id-type="doi">10.1038/s41577-023-00974-7</pub-id>, doi: <pub-id pub-id-type="doi">10.1038/s41577-023-00974-7</pub-id>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Skipper</surname> <given-names>JCA</given-names></string-name>, <string-name><surname>Gulden</surname> <given-names>PH</given-names></string-name>, <string-name><surname>Hendrickson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Harthun</surname> <given-names>N</given-names></string-name>, <string-name><surname>Caldwell</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Shabanowitz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Engelhard</surname> <given-names>VH</given-names></string-name>, <string-name><surname>Hunt</surname> <given-names>DF</given-names></string-name>, <string-name><surname>Slingluff Jr</surname> <given-names>CL</given-names></string-name></person-group>. <article-title>Mass-spectrometric evaluation of HLA-A*0201-associated peptides identifies dominant naturally processed forms of CTL epitopes from MART-1 and gp100</article-title>. <source>International Journal of Cancer</source>. <year>1999</year>; <volume>82</volume>(<issue>5</issue>):<fpage>669</fpage>–<lpage>677</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/(SICI)1097-0215(19990827)82:5%3C669::AID-IJC9%3E3.0.CO;2-%23">https://doi.org/10.1002/(SICI)1097-0215(19990827)82:5%3C669::AID-IJC9%3E3.0.CO;2-%23</ext-link></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname> <given-names>H</given-names></string-name>, <string-name><surname>Bremer</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Hinds</surname> <given-names>EC</given-names></string-name>, <string-name><surname>Raskutti</surname> <given-names>G</given-names></string-name>, <string-name><surname>Romero</surname> <given-names>PA</given-names></string-name></person-group>. <article-title>Inferring Protein Sequence-Function Relationships with Large-Scale Positive-Unlabeled Learning</article-title>. <source>Cell Systems</source>. <year>2021</year>; <volume>12</volume>(<issue>1</issue>):<fpage>92</fpage>–<lpage>101.e8.</lpage> <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S2405471220304142">https://www.sciencedirect.com/science/article/pii/S2405471220304142</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.cels.2020.10.007</pub-id>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Springer</surname> <given-names>I</given-names></string-name>, <string-name><surname>Besser</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tickotsky-Moskovitz</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dvorkin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Louzoun</surname> <given-names>Y.</given-names></string-name></person-group> <article-title>Prediction of Specific TCR-Peptide Binding From Large Dictionaries of TCR-Peptide Pairs</article-title>. <source>Frontiers in Immunology</source>. <year>2020</year>; Volume <volume>11</volume> - 2020. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2020.01803">https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2020.01803</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fimmu.2020.01803</pub-id>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Springer</surname> <given-names>I</given-names></string-name>, <string-name><surname>Tickotsky</surname> <given-names>N</given-names></string-name>, <string-name><surname>Louzoun</surname> <given-names>Y.</given-names></string-name></person-group> <article-title>Contribution of T Cell Receptor Alpha and Beta CDR3, MHC Typing, V and J Genes to Peptide Binding Prediction</article-title>. <source>Frontiers in Immunology</source>. <year>2021</year>; Volume <volume>12</volume> - 2021. <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2021.664514">https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2021.664514</ext-link>, doi: <pub-id pub-id-type="doi">10.3389/fimmu.2021.664514</pub-id>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tickotsky</surname> <given-names>N</given-names></string-name>, <string-name><surname>Sagiv</surname> <given-names>T</given-names></string-name>, <string-name><surname>Prilusky</surname> <given-names>J</given-names></string-name>, <string-name><surname>Shifrut</surname> <given-names>E</given-names></string-name>, <string-name><surname>Friedman</surname> <given-names>N.</given-names></string-name></person-group> <article-title>McPAS-TCR: a manually curated catalogue of pathology-associated T cell receptor sequences</article-title>. <source>Bioinformatics</source>. <year>2017</year> 05; <volume>33</volume>(<issue>18</issue>):<fpage>2924</fpage>–<lpage>2929</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btx286</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btx286</pub-id>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tubiana</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cocco</surname> <given-names>S</given-names></string-name>, <string-name><surname>Monasson</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Learning protein constitutive motifs from sequence data</article-title>. <source>eLife</source>. <year>2019</year> <month>mar</month>; <volume>8</volume>:<elocation-id>e39397</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.39397</pub-id>, doi: <pub-id pub-id-type="doi">10.7554/eLife.39397</pub-id>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ursu</surname> <given-names>E</given-names></string-name>, <string-name><surname>Minnegalieva</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rawat</surname> <given-names>P</given-names></string-name>, <string-name><surname>Chernigovskaya</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tacutu</surname> <given-names>R</given-names></string-name>, <string-name><surname>Sandve</surname> <given-names>GK</given-names></string-name>, <string-name><surname>Robert</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Greiff</surname> <given-names>V.</given-names></string-name></person-group> <article-title>Training data composition determines machine learning generalization and biological rule discovery</article-title>. <source>bioRxiv</source>. <year>2024</year>; <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2024/06/19/2024.06.17.599333">https://www.biorxiv.org/content/early/2024/06/19/2024.06.17.599333</ext-link>, doi: <pub-id pub-id-type="doi">10.1101/2024.06.17.599333</pub-id>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vita</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mahajan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Overton</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Dhanda</surname> <given-names>SK</given-names></string-name>, <string-name><surname>Martini</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cantrell</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Wheeler</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Sette</surname> <given-names>A</given-names></string-name>, <string-name><surname>Peters</surname> <given-names>B.</given-names></string-name></person-group> <article-title>The Immune Epitope Database (IEDB): 2018 update</article-title>. <source>Nucleic Acids Research</source>. <year>2018</year> 10; <volume>47</volume>(<issue>D1</issue>):<fpage>D339</fpage>–<lpage>D343</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gky1006</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/nar/gky1006</pub-id>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Wan</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>He</surname> <given-names>H.</given-names></string-name></person-group> <article-title>Variational autoencoder based synthetic data generation for imbalanced learning</article-title>. <conf-name>2017 IEEE Symposium Series on Computational Intelligence</conf-name>; <year>2017</year>. p. <fpage>1</fpage>–<lpage>7</lpage>. doi: <pub-id pub-id-type="doi">10.1109/SSCI.2017.8285168</pub-id>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Wang</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cho</surname> <given-names>K</given-names></string-name></person-group>, <source>BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</source>; <year>2019</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1902.04094">https://arxiv.org/abs/1902.04094</ext-link>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>C</given-names></string-name>, <string-name><surname>Meraz</surname> <given-names>RF</given-names></string-name>, <string-name><surname>Holbrook</surname> <given-names>SR</given-names></string-name></person-group>. <article-title>PSoL: a positive sample only learning algorithm for finding non-coding RNA genes</article-title>. <source>Bioinformatics</source>. <year>2006</year> 08; <volume>22</volume>(<issue>21</issue>):<fpage>2590</fpage>–<lpage>2596</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btl441</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btl441</pub-id>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Weber</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pélissier</surname> <given-names>A</given-names></string-name>, <string-name><surname>Martínez</surname> <given-names>MR</given-names></string-name></person-group>, <source>T cell receptor binding prediction: A machine learning revolution</source>; <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2312.16594">https://arxiv.org/abs/2312.16594</ext-link>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Williams</surname> <given-names>C.</given-names></string-name></person-group> <chapter-title>Computing with Infinite Networks</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Mozer</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Jordan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Petsche</surname> <given-names>T</given-names></string-name>, editors</person-group>. <source>Advances in Neu-ral Information Processing Systems</source>, vol. <volume>9</volume> <publisher-name>MIT Press</publisher-name>; <year>1996</year>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/1996/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf">https://proceedings.neurips.cc/paper/1996/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf</ext-link>.</mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Wu</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Yost</surname> <given-names>K</given-names></string-name>, <string-name><surname>Daniel</surname> <given-names>B</given-names></string-name>, <string-name><surname>Belk</surname> <given-names>J</given-names></string-name>, <string-name><surname>Xia</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Egawa</surname> <given-names>T</given-names></string-name>, <string-name><surname>Satpathy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Zou</surname> <given-names>J.</given-names></string-name></person-group> <article-title>TCR-BERT: learning the grammar of T-cell receptors for flexible antigen-binding analyses</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Knowles</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Mostafavi</surname> <given-names>S</given-names></string-name></person-group>. <conf-name>Proceedings of the 18th Machine Learning in Computational Biology meeting of Proceedings of Machine Learning Research</conf-name>, vol. <volume>240</volume> <year>2024</year>. p. <fpage>194</fpage>–<lpage>229</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v240/wu24b.html">https://proceedings.mlr.press/v240/wu24b.html</ext-link>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yadav</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vora</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Sundar</surname> <given-names>D</given-names></string-name>, <string-name><surname>Dhanjal</surname> <given-names>JK</given-names></string-name></person-group>. <article-title>TCR-ESM: Employing protein language embeddings to predict TCR-peptide-MHC binding</article-title>. <source>Computational and Structural Biotechnology Journal</source>. <year>2024</year> <month>Dec</month>; <volume>23</volume>:<fpage>165</fpage>–<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1016/j.csbj.2023.11.037</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/j.csbj.2023.11.037</pub-id>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname> <given-names>P</given-names></string-name>, <string-name><surname>Li</surname> <given-names>XL</given-names></string-name>, <string-name><surname>Mei</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Kwoh</surname> <given-names>CK</given-names></string-name>, <string-name><surname>Ng</surname> <given-names>SK</given-names></string-name></person-group>. <article-title>Positive-unlabeled learning for disease gene identification</article-title>. <source>Bioinfor-matics</source>. <year>2012</year> 08; <volume>28</volume>(<issue>20</issue>):<fpage>2640</fpage>–<lpage>2647</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bts504</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/bioinformat-ics/bts504</pub-id>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bevan</surname> <given-names>MJ</given-names></string-name></person-group>. <article-title>CD8+ T Cells: Foot Soldiers of the Immune System</article-title>. <source>Immunity</source>. <year>2011</year> <month>Aug</month>; <volume>35</volume>(<issue>2</issue>):<fpage>161</fpage>–<lpage>168</lpage>. <pub-id pub-id-type="doi">10.1016/j.immuni.2011.07.010</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/j.immuni.2011.07.010</pub-id>.</mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname> <given-names>W</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wei</surname> <given-names>X</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>K</given-names></string-name>, <string-name><surname>Du</surname> <given-names>W</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Guo</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>C</given-names></string-name>, <string-name><surname>Luo</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>F</given-names></string-name>, <string-name><surname>Gao</surname> <given-names>F</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Li</surname> <given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>R</given-names></string-name>, <string-name><surname>Saksena</surname> <given-names>NK</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>PIRD: Pan Immune Repertoire Database</article-title>. <source>Bioinformatics</source>. <year>2019</year> 08; <volume>36</volume>(<issue>3</issue>):<fpage>897</fpage>–<lpage>903</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz614</pub-id>, doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btz614</pub-id>.</mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zieba</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tomczak</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Gonczarek</surname> <given-names>A.</given-names></string-name></person-group> <chapter-title>RBM-SMOTE: Restricted Boltzmann Machines for Synthetic Minority Over-sampling Technique</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Nguyen</surname> <given-names>NT</given-names></string-name>, <string-name><surname>Trawinski</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kosala</surname> <given-names>R</given-names></string-name>, editors</person-group>. <source>Intelligent Information and Database Sys-tems Cham</source>: <publisher-name>Springer International Publishing</publisher-name>; <year>2015</year>. p. <fpage>377</fpage>–<lpage>386</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-319-15702-3_37</pub-id>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<title>Appendix 1</title>
<sec id="s8">
<title>Composition of the datasets used in the analysis of the main text</title>
<p>We report in Tab. 1, 2 the composition of the datasets used in the study of peptide- and pan-specific models in Results. Both were collected following Datasets collection.</p>
<table-wrap id="tbl1A1" orientation="portrait" position="float">
<label>Appendix 1—table 1.</label>
<caption><title>List of peptide-specific classes in the dataset used in the analysis of peptide-specific models.</title>
<p>We report sizes of the peptide-specific classes defined by peptide sequences in the TCR dataset we used to study peptide-specific models in Results. Imbalance ratios for the 5 cases reported in <xref rid="fig3" ref-type="fig">Fig. 3</xref> can be obtained from the relative abundances of the corresponding triplets of classes.</p></caption>
<graphic xlink:href="602897v2_tbl1_A1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl1A2" orientation="portrait" position="float">
<label>Appendix 1—table 2.</label>
<caption><title>List of peptide-specific classes in the dataset used in the analysis of pan-specific models.</title>
<p>We report sizes and imbalance ratio details on the peptide-specific classes defined by peptide sequences in the TCR dataset we used to study pan-specific models in Results. Only few peptide-specific classes have more than 1% of CDR3<italic>β</italic> binding sequences, causing heavily imbalance. Details refer to the dataset before training/test splitting.</p></caption>
<graphic xlink:href="602897v2_tbl2_A1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</app>
<app id="app2">
<title>Appendix 2</title>
<sec id="s9">
<title>Geometrical interpretation of restoring balance</title>
<p>Supervised architectures solve the classification problem through the learning of a decision boundary in the high dimensional embedded feature space of input data. In this regard, achieving good performances means finding a good feature map where the input data points are well clustered based on their class identities. How does our restoring balance procedure via generative models of sequences affect the supervised learning process? We aim to visualize such effect to seek for a geometrical interpretation of re-balancing data points among different classes.</p>
<p>In particular, our supervised architectures used for both peptide- and pan-specific analyses employ a fully connected read out layer at the end of multiple convolutional layers. Thus, the model applies a series of transformations to map the input dataset into an higher dimensional space, which is passed to a fully connected layer where an hyperplane or a set of intersecting hyperplanes are found to perform binary or multi-class classification, respectively. Thanks to their simplicity, models with a single dense layer like Support Vector Machines (SVMs) allow for a mathematical formulation and some predictions on the model performance can be derived at theoretical level; within this framework, also the issue of learning under imbalanced datasets can be studied. For example, in <xref ref-type="bibr" rid="c9">Chaudhuri et al. (2023)</xref> the authors showed on common imbalanced benchmark datasets that undersampling helps classification as opposed to imbalanced learning; also, in <xref ref-type="bibr" rid="c47">Loffredo et al. (2024)</xref> we characterized the performances of an SVM under imbalance and studied the benefit of restoring data balance via under- and oversampling on synthetic data, showing that augmenting the under represented classes yields best performances. The key finding of such studies – that focus on binary classification – proves that learning under imbalance shifts the optimal decision boundary towards the under represented class and tilts it away from the optimal direction (see <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, main text).</p>
<p>To visualize this effect, we consider a simple 1-hidden-layer network with linear activation and sigmoidal output, which we refer to as 1-Dense Network (1DN), and the CNN model considered above. In the two cases, the input-output map is given by
<disp-formula id="eqn12">
<graphic xlink:href="602897v2_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>W</italic> are the weights of the fully connected readout layer and the feature vector <italic>H</italic> is given by <italic>H</italic> = <italic>W</italic> <sup>(0)</sup><italic>X</italic> (1DN) or <italic>H</italic> = <italic>f</italic><sub>CNN</sub>(<italic>X</italic>) (CNN). We train both our supervised CNN model and the 1DN model with a binary classification task (<monospace>LLWNGPMAV</monospace>-specific and bulk CDR3<italic>β</italic>s) using hinge loss. After training on the same dataset, we feedforward the test datapoints in the two models: we call <italic>H</italic><sub>±</sub> the two class centers of the test set,
<disp-formula id="eqn13">
<graphic xlink:href="602897v2_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the mean is performed over the two test set classes. The weight vector represents the direction of the decision boundary and should be aligned to the distance vector connecting the classes, <italic>H</italic><sub>+</sub> − <italic>H</italic><sub>−</sub>; we quantify this alignment computing the normalized dot product
<disp-formula id="eqn14">
<graphic xlink:href="602897v2_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
for both models, <italic>φ</italic><sup>CNN</sup> and <italic>φ</italic><sup>1DN</sup>. Our hypotheses on the geometrical effect of learning with imbalance implies that we should find a value of <italic>φ</italic> closer to 1 when the model is trained over balanced datasets. Thus we run experiments on different training set compositions, tuning the fraction of negative examples (bulk CDR3<italic>β</italic>s) in the range [50%, 75%], averaging the quantity <italic>φ</italic> over 50 trials. Results on the simple 1DN and the deep CNN are in agreement, with the dot product dropping from <italic>φ</italic><sup>1DN</sup> = 0.87 (ACC = 0.62) to <italic>φ</italic><sup>1DN</sup> = 0.78 (ACC = 0.5) for the 1DN and from <italic>φ</italic><sup>CNN</sup> = 0.73 (ACC = 0.66) to <italic>φ</italic><sup>CNN</sup> = 0.62 (ACC = 0.5) for the CNN.</p>
</sec>
</app>
<app id="app3">
<title>Appendix 3</title>
<sec id="s10">
<title>On the choice of the generative model</title>
<p>Our learning framework is based on the idea that restoring balance through a combination of undersampling the strongly over-represented classes and augmenting under-represented classes via generative models. We believe such approach can yield better performances as the supervised network is provided with new informative peptide-specific CDR3<italic>β</italic> sequences; yet, the quality of generated sequences is crucial to prevent a loss of information. To discuss this point, we benchmark our specificity prediction performances with a simple model that requires zero training: a profile model (PM) that generates new CDR3<italic>β</italic> sequences by independently sampling each site of the sequence based on the peptide-specific class sequence profile (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). Since this model captures only first-order statistics of the sequences we expect it to generate less informative CDR3<italic>β</italic>s and thus we end with poor specificity predictions (see Tab. 1 for the scores obtained using this method for the cases reported in <xref rid="fig3" ref-type="fig">Fig. 3</xref> of the main text).</p>
<p>The quality of data generation is also based on the stringency of the Gibbs sampling procedure, <italic>i</italic>.<italic>e</italic>. how easily we allow the generative process to accept random mutations. The degree of randomness in the sampling scheme is set by the temperature value <italic>β</italic> in the softmax outputs of the model: rescaling the model logits by <italic>β</italic>, low values of <italic>β</italic> flatten the softmax output distribution so that amino acids mutations are randomly accepted regardless from the underlying CDR3<italic>β</italic> distribution learned. To visualize this effect, we take a peptide-specific classification task and restore balance by generating random CDR3<italic>β</italic> sequences (see <xref rid="fig2" ref-type="fig">Fig. 2</xref>); at some point (<italic>ρ</italic><sub>rand.</sub> &gt; 50%) the random sequences completely take over the CDR3<italic>β</italic> natural sequences and make the specificity prediction problem impossible to solve, yielding random predictions.</p>
<fig id="fig1_A3" position="float" fig-type="figure">
<label>Appendix 3—figure 1.</label>
<caption><title>Sequence logos of peptide-specific CDR3<italic>β</italic> classes.</title>
<p>We report the sequence logos profile for peptide-specific classes of CDR3<italic>β</italic> sequences, after alignment to maximal length <italic>L</italic> = 20. The PM is learned over such profiles for each class. Sequence logos show high conservation of the CASS motif and of the last amino acid, while there is more variability in the central region which is indeed responsible for the binding affinity.</p></caption>
<graphic xlink:href="602897v2_fig1_A3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2_A3" position="float" fig-type="figure">
<label>Appendix 3—figure 2.</label>
<caption><title>Performance drop due to randomness effect.</title>
<p>Here we report AUC and ACC scores for a <monospace>VTEHDTLLY</monospace>-specific model that has been trained over a dataset containing a fraction of random sequences <italic>ρ</italic><sub>rand.</sub>. When the generative model is not good enough and is adding noise to the under represented class of data, we can observe the performances drop down to a random classifier.</p></caption>
<graphic xlink:href="602897v2_fig2_A3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl1_A3" orientation="portrait" position="float">
<label>Appendix 3—table 1.</label>
<caption><title>Performances scores for peptide-specific models with PM generative model.</title>
<p>The supervised architecture is the same and it is trained as in <xref rid="fig3" ref-type="fig">Fig. 3</xref> of the main text, on the same datasets with balance restored via generation of new CDR3<italic>β</italic> samples.</p></caption>
<graphic xlink:href="602897v2_tbl1_A3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</app>
<app id="app4">
<title>Appendix 4</title>
<sec id="s11">
<title>More on out-of-distribution tasks</title>
<sec id="s11a">
<title>Performances of peptide-specific models strongly depend on the choice of the out-of-distribution data</title>
<p>Similarly to what done for the pan-specific framework in Results, we study out-of-distribution performances for single-epitope TCR specificity. Our peptide-specific model has learned the key properties of CDR3<italic>β</italic> binding to a selected epitope as reported in the main text, and it remains to be seen to what extent such features can be transferred to predict binding towards other target epitopes against bulk repertoires. We start by considering synthetic data.</p>
<sec id="s11a1">
<title>Lattice proteins</title>
<p>We take data from <xref ref-type="bibr" rid="c46">Loffredo et al. (2023)</xref>, where the authors built dimeric LPs starting from single monomers running Monte Carlo (MC) evolution at variable intra-dimer interaction strengths (see App. 6 for more details). We collect sequence data in the form of Multiple Sequence Alignments (MSA) at three steps during MC evolution, namely at beginning, intermediate and endpoint of evolution: the MSAs will constitute non binder, weak binder and strong binder data, respectively. We train a supervised model over strong and weak binder classes and use non binders as out-of-distribution data, which are thus closer to weak binders than to strong ones. We plot in <xref rid="fig1" ref-type="fig">Fig. 1a</xref> the binding probability distributions for these three classes of binders.</p>
<fig id="fig1_A4" position="float" fig-type="figure">
<label>Appendix 4—figure 1.</label>
<caption><title>Out-of-distribution analysis on synthetic Lattice-Protein dimers.</title>
<p><bold>a)</bold> Densities of binding scores 𝒫 <sub>bind</sub> for strong, weak and non-binding compounds. The y-axis is cut for visualization purpose, as “Non binder” compounds concentrates around zero score. <bold>b)</bold> Receiver Operating Characteristic (ROC) curves for in-distribution (Strong - Weak) and out-of-distribution (Strong - Non binder and Weak - Non binder) test sets. The classification weak-vs-non binders has worst performances. <bold>c)</bold> tSNE visualization of the embeddings (last feature layer) produced by our CNN architecture for in-distribution test data (strong and weak binders) and out-of-distribution hold-out data (non binders). Classification is carried out from linear combinations of the embeddings of the input data points; better separation of clusters reflects higher model performances.</p></caption>
<graphic xlink:href="602897v2_fig1_A4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The trained classifier achieve great performances on the in-distribution test set for strong-vs-weak binders classification (AUC = 0.95) and do even better for the out-of-distribution strong-vs-non binder classification (AUC = 0.98). Performance are much poorer for the out-of-distribution weak-vs-non-binder classification (AUC = 0.69), as shown in <xref rid="fig1" ref-type="fig">Fig. 1b</xref>; note that for this task we switched labels, <italic>i</italic>.<italic>e</italic>. weak binders are presented as strong ones and non-binders are deemed as weak, otherwise we would get AUC = 0.31).</p>
<p>We display in <xref rid="fig1" ref-type="fig">Fig. 1c</xref> the 2d projections of the embedded sequences in the last layer of our architecture (before linear classification) using tSNE. The tSNE visualization shows that the predictive power of the model depends on the location of the out-of-distribution cluster in the feature space compared to the in-distribution data. The out-of-distribution sequence distribution has a large overlap with the one of weak binders, which makes discrimination hard. Conversely, in this feature space, strong binders are well separated from the rest and hence the decision boundary learned over strong-vs-weak is efficient even against out-of-distribution data.</p>
<sec id="s11a1a">
<title>Natural CDR3<italic>β</italic></title>
<p>To assess if the results derived in the controlled framework of synthetic data also holds for natural TCRs, we consider the epitope <monospace>ELAGIGILTV</monospace>, which is 2 mutations away from the primary epitope <monospace>AAGIGILTV</monospace> expressed on the surface of Melanoma-cancer responsible cells (<xref ref-type="bibr" rid="c77">Skipper et al., 1999</xref>). Our dataset includes 2,082 sequences of CDR3<italic>β</italic> experimentally labelled as binders to this epitope. We train the CNN model to distinguish peptide-specific sequences from bulk ones. We then select as out-of-distribution sequences (i) CDR3<italic>β</italic>s that positively bind the epitope <monospace>EAAGIGILTV</monospace>, one mutation away from the wildtype (WT) <monospace>ELAGIGILTV</monospace>; (ii) we also select CDR3<italic>β</italic> binding a very different peptide having Levenshtein distance 8 from our WT, namely <monospace>VQELYSPIFLIV</monospace>. We expect that our model be predictive for out-of-distribution specificity predictions for the first epitope and not for the second one. Results confirm this guess with values of AUC equal to, respectively, 0.79 and 0.54. Similarly to the case of synthetic data in <xref rid="fig1" ref-type="fig">Fig. 1c</xref>, this difference in performance is visualized in <xref rid="fig2" ref-type="fig">Fig. 2</xref> by the distinct locations of the corresponding sequence distributions in the feature space of the last embedding layer of our classifier.</p>
<fig id="fig2_A4" position="float" fig-type="figure">
<label>Appendix 4—figure 2.</label>
<caption><title>Out-of-distribution performances are related to the distribution of features.</title>
<p>tSNE visualization of the feature vectors (in the second-last layer of the classifier architecture) of out-of-distribution data for a model trained over the WT epitope <monospace>ELAGIGILTV</monospace>, with a balanced dataset containing 1500 CDR3<italic>β</italic> per class. Top: As the WT epitope, <monospace>EAAGIGILTV</monospace> is responsible for the Melanoma cancer and thus is targeted by TCRs sharing similar features. Bottom: the <monospace>VQELYSPIFLIV</monospace> peptide is 8 mutations away from the WT and is involved in SARS-CoV-2 infections. Our model predictions are reliable in the first epitope (AUC = 0.79), and are not for the second peptide (AUC = 0.54). The tSNE plots support the claim that out-of-distribution specificity predictions drop when the feature vectors of the test data are far away from the training ones.</p></caption>
<graphic xlink:href="602897v2_fig2_A4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
</sec>
<sec id="s11b">
<title>Performances of pan-specific models strongly depend on the choice of the out-of-distribution data</title>
<p>More generally, as we already discussed, the performance degradation is related to how much we move away from the in-distribution data. We report the table of out-of-distribution performances as a function of in-to out-epitopes Levenshtein distance (see Tab. 1).</p>
<table-wrap id="tbl1_A4" orientation="portrait" position="float">
<label>Appendix 4—table 1.</label>
<caption><title>Pan-specific prediction scores for out-of-distribution tasks.</title>
<p>The AUC (second column) and ACC (fourth column) scores are averaged across many out-of-distribution epitopes, with balance restored through pan-specific generation. Out-of-distribution CDR3<italic>β</italic> are grouped according to the Levenshtein distance of their associated epitope to the closest one in the training dataset (first column). Predictions worsen as peptides get further away from the ones in the training dataset. The third and fifth columns show the standard deviations of the scores.</p></caption>
<graphic xlink:href="602897v2_tbl1_A4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</sec>
</app>
<app id="app5">
<title>Appendix 5</title>
<sec id="s12">
<title>Hyperparameter tuning in pan-specific models</title>
<p>In <xref rid="fig4" ref-type="fig">Fig. 4</xref> of the main text we show results for a specific value of the threshold size 𝒢, above which peptide-specific classes are not augmented. The choice of the threshold value can affect the performances and it is task-dependent. Here, we report in <xref rid="fig1" ref-type="fig">Fig. 1</xref> results of AUC scores before and after data augmentation through the pan-specific model has been applied. Undersampling the populated classes is done down to size 5000 CDR3<italic>β</italic> sequences. As we can see, despite quantitative values depend on 𝒢, the picture confirms the overall concept that generative methods of peptide-specific CDR3<italic>β</italic> sequences help specificity predictions, particularly for under represented classes (small triangles).</p>
<fig id="fig1_A5" position="float" fig-type="figure">
<label>Appendix 5—figure 1.</label>
<caption><title>AUC scores dependencies on the hyperparameter choice <italic>M</italic><sub><italic>S</italic></sub>.</title>
<p>We report results of numerical experiments for the TCR-peptide binding prediction task, comparing AUC performances before and after the generative model has been used (x and y axis, respectively). The data point size is proportional to the size of natural peptide-specific sequences in the dataset. All the training parameters are fixed for all experiments; we rescale the number of training epochs with the training dataset size so that for each experiment we minimize the loss function exactly the same number of times: this factors out all elements, but the dependence of performances on the threshold value 𝒢.</p></caption>
<graphic xlink:href="602897v2_fig1_A5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
<app id="app6">
<title>Appendix 6</title>
<sec id="s13">
<title>Details on Synthetic Lattice Protein dimer data</title>
<p>In the text we considered Lattice Proteins (LPs) as a fully controlled setting, where the ground truth distribution of data is known and its properties are tunable <italic>ad hoc</italic>. LPs consist of a computationally tractable model introduced to study the folding and binding properties of proteins (<xref ref-type="bibr" rid="c43">Li et al., 1996</xref>; <xref ref-type="bibr" rid="c57">Mirny and Shakhnovich, 2001</xref>). A LP monomer is defined as a self-avoiding path over a 3 × 3 × 3 lattice cube, whose conformation defines a structure <italic>S</italic>. There are 𝒩 = 103, 406 distinct structures (up to global symmetries) on the cube. The probability that a sequence <bold>v</bold> of <italic>L</italic> = 27 amino acids folds into structure <italic>S</italic> is
<disp-formula id="eqn15">
<graphic xlink:href="602897v2_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the sum runs over a representative subset of all distinct structures. The energy of the sequence in a structure, <italic>E</italic>(<bold>v</bold>|<italic>S</italic>), is given by
<disp-formula id="eqn16">
<graphic xlink:href="602897v2_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>c</italic><sup><italic>S</italic></sup> is the contact map of the structure (<inline-formula><inline-graphic xlink:href="602897v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula>if <italic>i, j</italic> are in contact, <inline-formula><inline-graphic xlink:href="602897v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> otherwise) and <italic>E</italic><sub>MJ</sub> is the Miyazawa-Jernigan energy matrix, a proxy for the effective interaction energy between pairs of amino acids.</p>
<p>Sequence data for LP dimers are obtained from <xref ref-type="bibr" rid="c46">Loffredo et al. (2023)</xref>, where two monomer sequences <bold>v</bold><sub>1</sub>, <bold>v</bold><sub>2</sub> – folded in, respectively, structures <italic>S</italic><sub>1</sub>, <italic>S</italic><sub>2</sub> – form a dimeric complex via the interaction energy
<disp-formula id="eqn17">
<graphic xlink:href="602897v2_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the sum runs over all sites of both structures. The index <italic>π</italic> in (17) labels a specific orientation of the interaction. In analogy with (15), the probability that the sequences <bold>v</bold><sub>1</sub>, <bold>v</bold><sub>2</sub> fold into the dimer <italic>S</italic><sub>1</sub> + <italic>S</italic><sub>2</sub> is
<disp-formula id="eqn18">
<graphic xlink:href="602897v2_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the sum runs over all possible orientations. Given two structures <italic>S</italic><sub>1</sub>, <italic>S</italic><sub>2</sub>, sequences are collected through MCMC dynamics by accepting or rejecting a mutation at each evolution step based on the total probability
<disp-formula id="eqn19">
<graphic xlink:href="602897v2_eqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>The MSAs are collected at steps <italic>t</italic> = 0, <italic>t</italic> = 100, <italic>t</italic> = 500 and represent non binder, weak and strong binder dimers used in this work. We refer the interested reader to <xref ref-type="bibr" rid="c46">Loffredo et al. (2023)</xref> for an extensive presentation of the LP model and details on how to obtain sequence data. In particular to generate the synthetic dataset used for the pan–specific model we select 12 pairs of structures and for each of them collect an MSA of binding and non binding sequences with the same length to ensure balance. The pairs of structures entering the dataset used in this work have the following ids: 1100 +1701; 249 +7801; 2789 +7511; 333 +794; 3412 + 9422; 456 + 259; 514 + 3894; 5809 + 6682; 9432 + 8754.</p>
<p>The out-of-distribution data for close structures is collected by first finding the closest structures to those used in the in-distribution dataset by minimizing the mean energy in (16)over the in-distribution binder MSA across all the structures available in <xref ref-type="bibr" rid="c37">Jacquin et al. (2016)</xref>; <xref ref-type="bibr" rid="c46">Loffredo et al. (2023)</xref>. We then generate the MSA for the selected close structure pair (2237 + 304), and this constitutes the close out-of-distribution dataset. Next we pick a random pair of structures and collect a MSA that will be our not close out-of-distribution dataset.</p>
</sec>
</app>
<app id="app7">
<title>Appendix 7</title>
<sec id="s14">
<title>Comparison of peptide-vs pan-specific generative models</title>
<p>Here we provide more details on the data augmentation in the pan-specific framework. In fact, it can be achieved in two ways.</p>
<list list-type="order">
<list-item><p>By analogy with the peptide-specific case, we can design a single generative model that is trained on varied peptide and CDR3<italic>β</italic> pairs of sequences and produces new pairs. Intuitively, as the number of distinct CDR3<italic>β</italic>s dwarfs the one of epitopes, we expect a pan-specific generative model to first learn how to cluster groups of TCR sequences based on their epitope labels, then to learn the CDR3<italic>β</italic>s distribution within each group. However, such a model could suffer from group imbalance, and would learn effectively only the most-represented instances of epitopes and TCRs.</p></list-item>
<list-item><p>An alternative approach consists in training, separately, a generative model for each peptide-specific class of CDR3<italic>β</italic>s, so that the group imbalance present in the dataset is completely factored out. This second approach is however computationally demanding, as its running time increases linearly with the number of epitopes. In addition, overfitting could be an issue for peptide-specific groups with very little data.</p></list-item>
</list>
<p>As for the classifier, we closely follow the CNN architecture used in <xref ref-type="bibr" rid="c60">Montemurro et al. (2021)</xref>, where two convolutional layers process separately the CDR3<italic>β</italic> and the peptide sequence: the resulting feature vectors are then concatenated to ultimately output the positive or negative binding prediction, see Model architecture for supervised predictions in the main text.</p>
<p>To assess the impact of group imbalance and the performance of the two approaches above, we first build an auxiliary dataset from the pan-specific dataset in <xref ref-type="bibr" rid="c31">Grazioli et al. (2022)</xref> by retaining only CDR3<italic>β</italic>s binding to five selected epitopes (<monospace>AMFWSVPTV</monospace>, <monospace>ELAGIGILTV</monospace>, <monospace>FLYNLLTRV</monospace>, <monospace>GLCTLVAML</monospace>, <monospace>LLWNGPMAV</monospace>), for a total of 9,000 datapoints. <xref rid="fig1" ref-type="fig">Fig. 1</xref> reports the AUC and ACC scores obtained over each peptide and over the aggregate dataset comprising all five peptide-specific groups when CDR3<italic>β</italic> sequence data have been balanced with one global pan-specific generative model or with multiple peptide-specific ones. In the latter case, we have enlarged the CDR3<italic>β</italic> sequence space of the two most under represented group epitopes – <monospace>AMFWSVPTV</monospace> and <monospace>ELAGIGILTV</monospace>. We observe that predictive performances are comparable between the two approaches, suggesting that the pan-specific generative approach is not heavily impacted by group imbalance, and is capable of adequately clustering and modeling each peptide-associated group of TCR sequences.</p>
<p>This robustness stems from our sampling protocol, in which the generative model is carefully initialized with training sequences (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). Upon sampling, the landscape defined by the inferred probability distribution is explored in the proximity of peptide-specific region associated to the initial sequence. This procedure prevents the generative model from jumping towards other peptide-specific groups, which can have much stronger overall weight due to group imbalance. Gibbs sampling schemes that start from randomly chosen sequence pairs preferably falls within such groups, <italic>e</italic>.<italic>g</italic>. peptides with less than 250 binders in the training dataset are on average generated four times less frequently than peptides with more.</p>
<fig id="fig1_A7" position="float" fig-type="figure">
<label>Appendix 7—figure 1.</label>
<caption><title>Performance change based on peptide-vs pan-specific generative model.</title>
<p>Here we.report AUC and ACC scores for a subset of epitopes in the cases where each peptide class in augmented using a peptide-specific generative model or a global one. Given our sampling protocols, the two approaches are equivalent in terms of performance scores while the pan-specific remains computationally advantageous over the peptide-specific approach.</p></caption>
<graphic xlink:href="602897v2_fig1_A7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
<app id="app8">
<title>Appendix 8</title>
<sec id="s15">
<title>Unsupervised classification</title>
<p>The pipeline explained in Results relies on the generative power of an unsupervised ML model trained on the positive examples and used to augment and balance the dataset, successively passed to a supervised classifier for training. To better understand how performances depend on the unsupervised and supervised steps, we report below predictions made by the unsupervised model alone. For simplicity, we consider hereafter the case of binary classification. By fixing a threshold on the scores, we can discriminate between positive and negative examples. This simple procedure is expected to be sub-optimal, as the unsupervised model is trained on the positive class alone and has no knowledge about the distribution of negative examples, which is used only to fix the threshold maximizing the accuracy on the two classes. Thus, a decision boundary based on the score of this model does not necessarily aligns with the separating surface of the two classes (see App. 2 for more details on the geometry of the classification problem).</p>
<p>In <xref rid="fig1" ref-type="fig">Fig. 1</xref>, we report the histograms of the scores assigned by the unsupervised model trained on positive examples of different peptide-CDR3<italic>β</italic> pairs, randomly chosen among the ones in <xref rid="fig4" ref-type="fig">Fig. 4</xref> of the main text. The threshold (gray dashed line) is fixed by maximizing the accuracy on a validation set composed of positive and negative examples of the given peptide. Performances are consistently lower than the ones obtained using both the unsupervised model (to augment the data) and the CNN (to classify).</p>
<fig id="fig1_A8" position="float" fig-type="figure">
<label>Appendix 8—figure 1.</label>
<caption><title>Unsupervised vs. supervised classification.</title>
<p><bold>(Left)</bold> Histograms of the scores assigned by the unsupervised model to balanced test sets of in-distribution epitopes; lower perplexity corresponds to better score. The gray dashed line locates the threshold obtained by maximizing the accuracy of prediction on positive and negative data. Epitopes in the top row are frequent ones and do not get enlarged during training of the supervised classifier. <bold>(Right)</bold> Comparison of the accuracy scores between unsupervised classification only ([BERT]) and the full pipeline ([BERT+CNN]) in <xref rid="fig2" ref-type="fig">Fig. 2</xref> for the same peptides as in the left panel. Values in the column [BERT+CNN] can be obtained from <xref rid="fig4" ref-type="fig">Fig. 4</xref> of the main text.</p></caption>
<graphic xlink:href="602897v2_fig1_A8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107803.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bitbol</surname>
<given-names>Anne-Florence</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution>
</institution-wrap>
<city>Lausanne</city>
<country>Switzerland</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study introduces a data augmentation approach based on generative unsupervised models to address data imbalance in immune receptor modeling. Support for the findings is <bold>solid</bold>, showing that the use of generated data increases the performance of downstream supervised prediction tasks, e.g., TCR-peptide interaction prediction. However, the validation, mainly relying on synthetic data, could be completed, especially regarding unseen epitopes, and given the exclusive focus on CDR3β. The results should be of interest to the communities working on immunology and biological sequence data analysis.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107803.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript presents a deep learning framework for predicting T cell receptor (TCR) binding to antigens (peptide-MHC) using a combination of data augmentation techniques to address class imbalance in experimental datasets, and introduces both peptide-specific and pan-specific models for TCR-MHC-I binding prediction. The authors leverage a large, curated dataset of experimentally validated TCR-MHC-I pairs and apply a data augmentation strategy based on generative modeling to generate new TCR sequences. The approach is evaluated on benchmark datasets, and the resulting models demonstrate improved accuracy and robustness.</p>
<p>Strengths:</p>
<p>The most significant contribution of the manuscript lies in its data augmentation approach to mitigate class imbalance, particularly for rare but immunologically relevant epitope classes. The authors employ a generative strategy based on two deep learning architectures:</p>
<p>(1) a Restricted Boltzmann Machine (RBM) and</p>
<p>(2) a BERT-based language model, which is used to generate new CDR3B sequences of TCRs that are used as synthetic training data for creating a class balance of TCR-pMHC binding pairs.</p>
<p>The distinction between peptide-specific (HLA allele-specific) and pan-specific (generalized across HLA alleles) models is well-motivated and addresses a key challenge in immunogenomics: balancing specificity and generalizability. The peptide-specific models show strong performance on known HLA alleles, which is expected, but the pan-specific model's ability to generalize across diverse HLA types, especially those not represented in training, is critical.</p>
<p>Weaknesses:</p>
<p>The paper would benefit from a more rigorous analysis of the biological validity of the augmented data. Specifically, how do the synthetic CDR3B sequences compare to real CDR3B in terms of sequence similarity, motif conservation? The authors should provide a quantitative assessment (via t-SNE or UMAP projections) of real vs. augmented sequences, or by measuring the overlap in known motif positions, before and after augmentation. Without such validation, the risk of introducing &quot;hallucinated&quot; sequences that distort model learning remains a concern. Moreover, it would strengthen the argument if the authors demonstrated that performance gains are not merely due to overfitting on synthetic data, but reflect genuine generalization to unseen real data. Ultimately, this can only be performed through elaborate experimental wet-lab validation experiments, which may be outside the scope of this study.</p>
<p>While generative modeling for sequence data is increasingly common, the choice of RBM, which is a relatively older architecture, could benefit from stronger justification, especially given the emergence of more powerful and scalable alternatives (e.g., ProGen, ESM, or diffusion-based models). While BERT was used, it will be valuable in the future to explore other architectures for data augmentation.</p>
<p>The manuscript would be more compelling if the authors performed a deeper analysis of the pan-specific model's behavior across HLA supertypes and allele groups. Are the learned representations truly &quot;pan&quot; or merely a weighted average of the most common alleles? The authors should assess whether the pan-specific model learns shared binding motifs (anchor residue preferences) and whether these features are interpretable through attention maps. A failure to identify such patterns would raise concerns about the model's interpretability and biological relevance.</p>
<p>The exclusive focus on CDR3β for TCR modeling is biologically problematic. TCRs are heterodimers composed of α and β chains, and both CDR1, CDR2, and CDR3 regions of both chains contribute to antigen recognition. The CDR3β loop is often more diverse and critical, but CDR3α and the CDR1/2 loops also play significant roles in binding affinity and specificity. By generating only CDR3B sequences and not modeling the full TCR αβ heterodimer, the authors risk introducing a systematic bias toward β-chain-dominated recognition, which will not reflect the full complexity of TCR-peptide-MHC interactions.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107803.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents a thoughtful and well-motivated strategy to address a major challenge in TCR-epitope binding prediction: data imbalance, particularly the scarcity of positive (binding) TCR, peptide pairs. The authors introduce a two-step pipeline combining data balancing, via undersampling and generative augmentation, and a supervised CNN-based classifier. Notably, the use of Restricted Boltzmann Machines (RBMs) and BERT-style transformer models to generate synthetic CDR3β sequences is shown to improve model performance. The proposed method is applied to both peptide-specific and pan-specific settings, yielding notable performance improvements, especially for in-distribution peptides. Generative augmentation also leads to measurable gains for out-of-distribution epitopes, particularly those with high sequence similarity to the training set.</p>
<p>Strengths:</p>
<p>(1) The authors tackle the well-known but under-addressed issue of class imbalance in TCR-epitope binding data, where negatives vastly outnumber positive (binding) pairs. This imbalance undermines classifier reliability and generalization.</p>
<p>(2) The model is tested on both in-distribution (seen epitopes) and out-of-distribution (unseen epitopes) scenarios. Including a synthetic lattice protein benchmark allows the authors to dissect generalization behavior in a controlled environment.</p>
<p>(3) The paper shows a measurable benefit of generative. For example, AUC improvements of up to +0.11 are observed for peptides closely related to those seen during training, demonstrating the method's practical impact.</p>
<p>(4) A direct comparison between RBM- and Transformer-based sequence generators adds value, offering the community guidance on trade-offs between different generative architectures in TCR modeling applications.</p>
<p>Weaknesses:</p>
<p>(1) Generalization degrades with epitope dissimilarity</p>
<p>The performance drops substantially as the test epitope becomes more dissimilar to the training set. This is expected, but it highlights an essential limitation of the generative models: they help only when the test epitope is similar to one already seen. Table 1 shows that the performance gain from generative augmentation decreases as the test epitope becomes more dissimilar to the training epitopes. For epitopes with a Levenshtein distance of 1 from the training set, the average AUC improvement is approximately +0.11. This gain drops to around +0.06 for epitopes at distance 2. It becomes minimal for those at distance 4, indicating an explicit limitation in the model's ability to generalize to more distant epitopes. The authors should quantify more explicitly how far the model can generalize effectively. What is the performance degradation threshold as a function of Levenshtein distance?</p>
<p>(2) What is the minimal number of positive samples needed for data augmentation to help?</p>
<p>The approach has an intrinsic catch-22: generative models require data to learn the underlying distribution and cannot be applied to epitopes with insufficient data. As a result, the method is unlikely to be effective for completely new epitopes. Could the authors quantify the minimum number of real binders needed for effective generative augmentation? This would be particularly relevant for zero-shot or few-shot prediction scenarios, where only 0-10 positive samples are available. Such experiments would help clarify the practical limits of the proposed strategy.</p>
<p>(3) Lack of end-to-end evaluation on unseen epitopes as inputs</p>
<p>The authors frame peptide-specific models as classification over a few known epitopes, a closed-set formulation. While this is useful for evaluating generation effects, it's not representative of the more practical open-set task of predicting binding to truly novel epitopes. A stronger test would include models that take peptides as input (e.g., pan-specific, peptide-conditioned classifiers), including unseen epitopes at test time. Could the authors attempt an evaluation on benchmarks like IMMREP25 or other datasets where test epitopes are excluded from training?</p>
<p>(4) Focus on β-chain limits generalizability</p>
<p>The current pipeline is trained exclusively on CDR3β sequences. However, the field is increasingly moving toward single-cell sequencing, which provides paired α/β TCR chain data. Understanding how the proposed approach performs when both chains are available would be valuable. Could the authors evaluate the performance gains on paired α/β information, even in a small subset of single-cell data?</p>
<p>(5) Synthetic lattice proteins (LPs) have limited biological fidelity</p>
<p>While the LP-based benchmark presented in Figure 5 is a clever and controlled tool for probing model generalization, it remains conceptually and biophysically distant from real TCR-peptide interactions. Its utility as a toy model is valid, but its limitations should be more explicitly acknowledged:</p>
<p>a) Over-simplified binding landscape: The LP system is designed for tractability, with a simplified sequence-structure mapping and fixed lattice constraints. As shown in Figure 5c, the LP binding landscape is linearly separable, in stark contrast to the complex and often degenerate nature of real TCR-epitope interactions, where multiple structurally distinct TCRs can bind the same peptide and vice versa.</p>
<p>b) Absence of immunological context: The LP model abstracts away key biological factors such as MHC restriction, α/β chain pairing, peptide presentation, and structural constraints of the TCR-pMHC complex. These are essential for understanding binding specificity in actual immune repertoires.</p>
<p>c) Overestimation of generalization: While performance drops on more distant LP structures, even these are structurally and statistically more similar to the training data than truly novel biological epitopes. Thus, the LP benchmark likely underestimates the true difficulty of out-of-distribution generalization in real-world TCR prediction tasks.</p>
<p>d) Simplified biophysics: The LP simulations rely on coarse-grained energy models and empirical potentials that do not capture conformational dynamics, side-chain flexibility, or realistic binding energetics of TCR-peptide interfaces.</p>
<p>In summary, while the LP benchmark helps isolate specific generalization behaviors and for sanity-checking model performance under controlled perturbations, its biological relevance is limited. The authors should explicitly frame these assumptions and limitations to prevent overinterpreting results from this synthetic system.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107803.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors present a method to address class imbalance in T cell receptor (TCR)-epitope binding datasets by generating synthetic positive binding examples using generative models, specifically BERT-based architectures and Restricted Boltzmann Machines (RBMs). They hypothesize that improving class balance can enhance model performance in predicting TCR-peptide binding.</p>
<p>Strengths:</p>
<p>(1) Interesting biological as well as technical topic.</p>
<p>(2) Solid technical foundations.</p>
<p>Weaknesses:</p>
<p>(1) Fundamental Biological Oversight:</p>
<p>While the computational strategy of augmenting positive samples via generative models is technically interesting, the manuscript falls short in addressing key biological considerations. Specifically, the authors simulate and evaluate only CDR3β-peptide binding interactions. However, antigen recognition by T cells involves both the α- and β-chains of the TCR. The omission of CDR3α undermines the biological realism and limits the generalizability of the findings.</p>
<p>(2) Validation of Simulated Data:</p>
<p>The central claim of the manuscript is that simulated positive examples improve predictive performance. However, there is no rigorous validation of the biological plausibility or realism of the generated TCR sequences. Without independent evaluation (e.g., testing whether synthetic TCR-peptide pairs are truly binding), it remains unclear whether the performance gains are biologically meaningful or merely reflect artifacts of the generation process.</p>
<p>(3) Risk of Bias and Overfitting:</p>
<p>Training and evaluating models with generated data introduces a risk of circularity and bias. The observed improvements may not reflect better generalization to real-world TCR-epitope interactions but could instead arise from overfitting to synthetic patterns. Additional testing on independent, biologically validated datasets would help clarify this point.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107803.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Loffredo</surname>
<given-names>Emanuele</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0004-4882-8250</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Pastore</surname>
<given-names>Mauro</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2771-7437</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Cocco</surname>
<given-names>Simona</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1852-7789</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Monasson</surname>
<given-names>Rémi</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4459-0204</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We would like to thank editors and reviewers for their time spent on our work, fair assessments and constructive criticism. We plan to address their concerns in the future revision as follows, detailed by topic.</p>
<p>(1) Limitations of focusing on CDR3β only</p>
<p>In its current state, our work tested the proposed pipeline of data augmentation for binding prediction on benchmark datasets limited to peptide+CDR3β sequence pairs only. As pointed out by all the reviewers, the TCR-peptide interaction is more complex and involves also other regions of the receptor (such as the CDR3α chain) and the MHC presenting the peptide as well. To investigate how the inclusion of additional information impacts results, we plan to apply our pipeline in a setting where the generative protocol is extended to generate paired α and β. The supervised classifier will then receive a concatenation of α+β chains as inputs. We will compare the performance of this classifier with the one using β chains only, and add this analysis to the revised manuscript.</p>
<p>(1) Validation of generated sequences and interpretation of the features learned by the generative model</p>
<p>The reliability of the generative model in augmenting the training set with biologically sensible sequences is a crucial assumption of our approach, and we agree with the reviewers raising this as a main concern. Before stating our strategy to improve the soundness of the method, let us first point out a few aspects already considered in the present manuscript:</p>
<list list-type="bullet">
<list-item><p>The test set of the classifier is always composed of real sequences: in this way, an increase in performance due to data augmentation cannot be due to overfitting to synthetic, possibly unrealistic, sequences.</p>
</list-item>
<list-item><p>The generative protocol is initialized from real sequences, and used to generate sequences not too far from them. In this respect, it could be taken as a way to “regularize” the simplest strategy of data augmentation, random oversampling (taking multiple copies of sequences at random to rebalance the data). This procedure avoids generating “wildly hallucinated” sequences with unreliable models. We will better quantify this statement (see below).</p>
</list-item>
</list>
<list list-type="bullet">
<list-item><p>The training protocol is tailored to push the generative model towards learning binding features between peptide and CDR3β sequences (and not merely fitting their local statistics separately). For example, in the pan-specific setting, during training of the generative model on peptide+CDR3β sequences, the masked language modeling task is modified to force the model to recover the missing amino acid using only the other sequence context.</p>
</list-item></list>
<p>We will better stress these points in the revised manuscript. To further validate the generative protocol in the future revision, we will carry out additional sanity checks on the generated data to confirm that the synthetic sequences remain biologically plausible and comparable to real ones.</p>
<p>(1) Assessment of the performance of the pan-specific protocol for out-of-distribution data:</p>
<p>To better clarify how the degradation in performance of a classifier tested on out-of-distribution data is impacted by the dissimilarity between test and training data distribution, we will improve the synthetic analysis currently reported in Table 1, adding confidence intervals for accuracy, quantifying thresholds on the distance for the method to work, providing t-SNE embeddings of in- and out-of distribution data.</p>
<p>(2) Quantification of the threshold for the number of examples per class in order to train the generative model and obtain a performance increase</p>
<p>In the paper, we adopted an operative common-sense threshold of at least 100 sequences per class in order to apply our data augmentation pipeline. We will quantify this effect testing this threshold in the revised manuscript, in order to (i) emphasize the limits of this two-step generative protocol in the low-data regime and to (ii) assess if the generative model falls back to a random oversampling strategy (due to strong overfitting) when few data are available for training.</p>
<p>(3) Motivation for the use of RBMs:</p>
<p>While RBMs have known limitations, their use in our pipeline (together with the more modern TCR-BERT, that we also test) is mainly motivated by the fact that they provide measurable increases in performance with data augmentation despite their simple 2-layer architecture. We stress that simpler generative (profile) models are unable to show this increase, see Appendix 3. In this respect, the RBM provides a minimal generative model allowing us to augment data successfully, and a lower bound to the increase of performance with respect to more complex architectures trained on more data. We will report this point of view in the text.</p>
<p>(4) Clarification on the role of lattice proteins as an oversimplified toy model for protein interaction</p>
<p>We agree with the points raised by Reviewer #2 on the limitations of lattice proteins as a model for protein interaction. Indeed, we used it merely as a toy model for phenomenology, a strategy whose validity has been fairly acknowledged by the reviewer. We will report in the main text all the drastic simplifications and reasons why the reader should take the comparison to real data with great care.</p>
</body>
</sub-article>
</article>