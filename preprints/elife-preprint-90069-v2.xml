<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">90069</article-id>
<article-id pub-id-type="doi">10.7554/eLife.90069</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90069.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Representational drift as a result of implicit regularization</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0000-7648-9744</contrib-id>
<name>
<surname>Ratzon</surname>
<given-names>Aviv</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3677-6321</contrib-id>
<name>
<surname>Derdikman</surname>
<given-names>Dori</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7894-6344</contrib-id>
<name>
<surname>Barak</surname>
<given-names>Omri</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Rappaport Faculty of Medicine, Technion - Israel Institute of Technology</institution>, Haifa 31096, <country>Israel</country></aff>
<aff id="a2"><label>2</label><institution>Network Biology Research Laboratory, Technion - Israel Institute of Technology</institution>, Haifa 32000, <country>Israel</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Peyrache</surname>
<given-names>Adrien</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>McGill University</institution>
</institution-wrap>
<city>Montreal</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Colgin</surname>
<given-names>Laura L</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Texas at Austin</institution>
</institution-wrap>
<city>Austin</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label> Corresponding author; email: <email>aviv.ratzon@hotmail.com</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-10-10">
<day>10</day>
<month>10</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-04-11">
<day>11</day>
<month>04</month>
<year>2024</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP90069</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-06-20">
<day>20</day>
<month>06</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-07-02">
<day>02</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.05.04.539512"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-10-10">
<day>10</day>
<month>10</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.90069.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.90069.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.90069.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.90069.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.90069.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Ratzon et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Ratzon et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-90069-v2.pdf"/>
<abstract>
<p>Recent studies show that, even in constant environments, the tuning of single neurons changes over time in a variety of brain regions. This representational drift has been suggested to be a consequence of continuous learning under noise, but its properties are still not fully understood. To investigate the underlying mechanism, we trained an artificial network on a simplified navigational task. The network quickly reached a state of high performance, and many units exhibited spatial tuning. We then continued training the network and noticed that the activity became sparser with time. Initial learning was orders of magnitude faster than ensuing sparsification. This sparsification is consistent with recent results in machine learning, in which networks slowly move within their solution space until they reach a flat area of the loss function. We analyzed four datasets from different labs, all demonstrating that CA1 neurons become sparser and more spatially informative with exposure to the same environment. We conclude that learning is divided into three overlapping phases: (i) Fast familiarity with the environment; (ii) slow implicit regularization; (iii) a steady state of null drift. The variability in drift dynamics opens the possibility of inferring learning algorithms from observations of drift statistics.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Revisions in the paper made in light of public eLife review. Changes include strengthening experimental results by analyzing three new datasets, clarifying technical details and explanations.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<p>What do we mean when we say that the brain represents the external world? One interpretation is the existence of neurons whose activity is tuned to world variables. Such neurons have been observed in many contexts: place cells [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>] – which are tuned to position in a specific context, visual cells [<xref ref-type="bibr" rid="c3">3</xref>] – which are tuned to specific visual cues, neurons that are tuned to the execution of actions [<xref ref-type="bibr" rid="c4">4</xref>] and more. This tight link between the external world and neural activity might suggest that, in the absence of environmental or behavioral changes, neural activity is constant. In contrast, recent studies show that, even in constant environments, the tuning of single neurons to outside world variables gradually changes over time in a variety of brain regions, even long after good representations of the stimuli were achieved. This phenomenon has been termed <italic>representational drift</italic>, and has changed the way we think about the stability of memory and perception, but its driving forces and properties are still unknown [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>] (see [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>] for an alternative account).</p>
<p>There are at least two immediate theoretical questions arising from the observation of drift – why does it happen, and whether and how behavior is resistant to it [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>]? One mechanistic explanation is that the underlying anatomical substrates are themselves undergoing constant change, such that drift is a direct manifestation of this structural morphing [<xref ref-type="bibr" rid="c14">14</xref>]. A normative interpretation posits that drift is a solution to a computational demand, such as temporal encoding [<xref ref-type="bibr" rid="c15">15</xref>], ‘drop-out’ regularization [<xref ref-type="bibr" rid="c16">16</xref>], exploration of the solution space [<xref ref-type="bibr" rid="c17">17</xref>], or re-encoding during continual learning [<xref ref-type="bibr" rid="c12">12</xref>]. Several studies also address the resistance question, providing possible explanations on how behavior can be robust to such phenomena [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>].</p>
<p>Here, we focus on the mechanistic question, and leverage analyses of drift statistics for this purpose. Specifically, recent studies suggest that representational drift in the CA1 is driven by active experience [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>]. Namely, rate maps decorrelate more when mice are active for a longer time in a given context. This implies that drift is not just a passive process, but rather an active learning one. As drift seems to occur after an adequate representation has formed, it seems fitting to model it as a form of a continuous learning process.</p>
<p>This approach has been recently explored by [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>]. They considered continuous learning in noisy, overparameterized neural networks. Because the system is overparameterized, a manifold of zero-loss solutions exists. [<xref ref-type="bibr" rid="c24">24</xref>] showed that for feedforward neural networks (FNNs) trained using Hebbian learning with added parameter noise, units change their tuning over time. This was due to an <italic>undirected</italic> random walk within the manifold of solutions. The coordinated drift of neighboring place fields was used as evidence to support this view. The phenomenon of undirected motion within the space of solutions seems plausible, as all members of this space achieve equally good performance (<xref rid="fig1" ref-type="fig">Fig 1A</xref> left). However, there may be other properties of the solutions (<xref rid="fig1" ref-type="fig">Fig 1B</xref>) that vary along this manifold, which could potentially bias drift in a certain direction (<xref rid="fig1" ref-type="fig">Fig 1A</xref> right). It is likely that the drift observed in experiments is a combination of both an undirected and directed movement. We will now introduce theoretical results from machine learning that support the possibility of directed drift.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Two types of possible movements within the solution space.</title>
<p>(A) Two options of how drift may look in the solution space. Random walk within the space of equally good solutions that is either undirected (left) or directed (right). (B) The qualitative consequence of the two movement types. For an undirected random walk, all properties of the solution will remain roughly constant (left). For the directed movement there should be a given property that is gradually increasing or decreasing (right).</p></caption>
<graphic xlink:href="539512v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Recent work provided a tractable analytical framework for the learning dynamics of Stochastic Gradient Descent (SGD) with added noise and an overparameterized regime [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>]. These studies showed that, after the network has converged to the zero-loss manifold, a second-order effect biases the random walk along a specific direction within this manifold. This direction reduces an implicit regularizer, determined by the type of noise the network is exposed to. The regularizer is related to the Hessian of the loss – a measure of the flatness of the loss landscape in the vicinity of the solutions. Since this directed movement is a second-order effect, its timescale is orders of magnitude larger than that of the initial convergence.</p>
<p>Consider a biological neural network performing a task. The machine learning (ML) implicit regularization mentioned above requires three components: an overparameterized regime, noise, and SGD. Both biological and artificial networks possess a large number of synapses, or parameters, and hence can reasonably be expected to be overparameterized. Noise can emerge from the external environment or from internal biological elements. It is not reasonable to assume that a precise form of gradient descent is implemented in the brain [<xref ref-type="bibr" rid="c29">29</xref>], thereby casting doubt on the third element. Nevertheless, biologically plausible rules could be considered as noisy versions of gradient descent, as long as there is a coherent improvement in performance [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>]. Motivated by this analogy, we explore representational drift in models and experimental data.</p>
<p>Because drift is commonly observed in spatially-selective cells, we base our analysis on a model which has been shown to contain such cells [<xref ref-type="bibr" rid="c32">32</xref>]. Specifically, we trained artificial neural networks on a predictive coding task in the presence of noise. In this task, an agent moves along a linear track while receiving visual input from the walls, such that the goal is to predict the subsequent input. We observed that hidden layer units became tuned to the latent variable, which is position, in accordance with previous results [<xref ref-type="bibr" rid="c32">32</xref>]. We continued training and found that in addition to the gradual change of tuning curves, similar to [<xref ref-type="bibr" rid="c24">24</xref>], we witnessed that the fraction of active units decreased slowly while their tuning specificity increased. We show that these results align with experimental observations from the CA1 area - namely that when exposed to a novel environment, the number of active cells reduces while their tuning specificity increases long after the environment is already familiar. Finally, we demonstrated the connection between this sparsificiation effect and changes to the Hessian, in accordance with ML theory. That is, changes in activity statistics (sparseness) and representations (drift) are the signatures of the movement in solution space to a flatter area, until flatness saturates. We conclude that learning is divided into three overlapping phases: (i) Fast familiarity with the environment in which representations form; (ii) slow implicit regularization which can be recognized by changes in activity statistics; (iii) a steady state of null drift in which representations gradually change.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Spontaneous sparsification in a predictive coding network</title>
<p>To model representational drift in the CA1 area, we chose a simple model that could give rise to spatially- tuned cells [<xref ref-type="bibr" rid="c32">32</xref>]. In this model, an agent traverses a corridor while slightly modulating its angle with respect to the main axis (<xref rid="fig2" ref-type="fig">Fig 2A</xref>). The walls are textured by a fixed smooth noisy signal, and the agent receives this as input according to its current field of view. The model itself is a single hidden layer feedforward network, with the velocity and visual field as inputs. The desired output is the predicted visual input in the next time step. The model equations are given by:
<disp-formula id="eqn1">
<graphic xlink:href="539512v4_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn2">
<graphic xlink:href="539512v4_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold>m</bold> and <bold>n</bold> are the input and output matrices respectively, <bold>b</bold> is the bias vector, and <italic>σ</italic> is the ReLU activation function. The vector <italic>σ</italic>(<bold>x</bold><sub><italic>t</italic></sub><bold>m</bold><sup><italic>T</italic></sup> + <bold>b</bold>) constitutes the hidden layer, and each element in it is the activation of a given unit for the input at time <italic>t</italic>. The task is for the network’s output, <bold>y</bold>, to match the visual input, <bold>x</bold> of the following time step, resulting in the following loss function:
<disp-formula id="eqn3">
<graphic xlink:href="539512v4_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<p>We train the network using Gradient Descent (GD), while adding update noise to the learning dynamics:
<disp-formula id="eqn4">
<graphic xlink:href="539512v4_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>θ</italic> = (<bold>m, n, b</bold>) is the vectorized parameters-vector, <italic>τ</italic> is the current training step and <inline-formula><inline-graphic xlink:href="539512v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is Gaussian noise. We let the network converge to a good solution, demonstrated by a loss plateau, and continue training for an additional period. Note that this additional period can be orders of magnitude longer than the initial training period. The network quickly converged to a low loss and stayed at the same loss during the additional training period (<xref rid="fig2" ref-type="fig">Fig 2B</xref>). Surprisingly, when looking at the activity of units within the hidden layer, we noticed that it slowly became sparse (see methods for definitions of sparseness). This sparsification did not hurt performance, because individual units became more informative (<xref rid="fig2" ref-type="fig">Fig 2C</xref>), as quantified by the average Spatial Information (SI, see methods). When looking at the rate maps of units, i.e. their tuning to position, one can observe an image similar to representational drift observed in experiments [<xref ref-type="bibr" rid="c5">5</xref>] – namely that neurons changed their tuning over time (<xref rid="fig2" ref-type="fig">Fig 2D</xref>). Additionally, their tuning specificity increased in accordance with the SI increase. By observing the correlation matrix of the rate maps over time, it is apparent that there was a gradual change that slowed down (<xref rid="fig2" ref-type="fig">Fig 2E</xref>). To summarize, we observed a spontaneous sparsification over a timescale much longer than the initial convergence, without introducing any explicit regularization.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Continuous noisy learning leads to drift and spontaneous sparsification.</title>
<p>(A) Illustration of an agent in a corridor receiving high-dimensional visual input from the walls. (B) Loss as a function of training steps (log scale). Zero loss corresponds to a mean estimator. Note the rapid drop in loss at the beginning, after which it remains roughly constant. (C) Mean spatial information (SI, blue) and fraction of units with non-zero activation for at least one input (red) as a function of training steps. (D) Rate maps sampled at four different time points (columns). Maps in each row are sorted according to a different time point. Sorting is done based on the peak tuning value to the latent variable. (E) Correlation of rate maps between different time points along training. Only active units are used.</p></caption>
<graphic xlink:href="539512v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>At first glance, these results might seem inconsistent with the experimental descriptions of drift reported in the literature in which all metrics are stationary while only representations change [<xref ref-type="bibr" rid="c5">5</xref>]. We suggest that there exists an intermediate phase between initial familiarity and stationary activity metrics, which is consistent with the notion of drift, that exhibits a gradual change in activity statistics. It seems that most experimental paradigms require a long pre-exposure, longer than needed to become fully familiarized with the environment, thus missing the suggested effect. We thus analyzed four datasets, from four different labs, in which we believe that the familiarization stage was shorter than in other studies. Some of the analyses were present in the original papers, and others are novel using publicly available data. Three experiments start from a novel environment and one from a relatively short pre-exposure. As shown in <xref rid="fig3" ref-type="fig">Fig 3</xref>, all datasets are consistent with our simulations - namely that the fraction of active cells reduces while the mean SI per cell increases over a long timescale. See Methods section for a full description of the data sets and analyses, along with another paper in which activity statistics are stationary, for comparison.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Experimental data consistent with simulations.</title>
<p>Data from four different labs show sparsification of CA1 spatial code, along with an increase in the information of active cells. Values are normalized to the first recording session in each experiment. Error bars show standard error of the mean. (A) Fraction of place cells and mean SI per animal over 200 minutes [<xref ref-type="bibr" rid="c22">22</xref>]. (B) Number of cells per animal and mean SI over all cells pooled together over 10 days. Note that we calculated the number of active cells rather than fraction of place cells because of the nature of the available data [<xref ref-type="bibr" rid="c33">33</xref>]. (C) Fraction of place cells and mean SI per animal over 11 days [<xref ref-type="bibr" rid="c34">34</xref>]. (D) Fraction of place cells and mean SI per animal over 8 days [<xref ref-type="bibr" rid="c35">35</xref>].</p></caption>
<graphic xlink:href="539512v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Generality of the phenomenon</title>
<p>The theoretical considerations [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>], simulation results and experimental results from multiple labs all suggest very general and robust phenomenona.</p>
<p>To explore the sensitivity of our results to specific modeling choices, we systematically varied many of them. Specifically, we replaced tasks (see below) and simulated different activation functions. Perhaps most important, we varied the learning rules, as SGD is not a biologically plausible one. We used both Adam [<xref ref-type="bibr" rid="c36">36</xref>] and RMSprop [<xref ref-type="bibr" rid="c37">37</xref>], from the ML literature. We also used Stochastic Error-Descent (SED) [<xref ref-type="bibr" rid="c38">38</xref>], which does not require gradient calculation and is more biologically plausible (6). For SGD, we also ran simulations with label noise instead of update noise. Finally, we replaced the task with either a simplified predictive coding, random mappings or smoothed random mapping. The motivation for different tasks is twofold. First, there is some arbitrariness in the predictive task we chose. Second, the interpretation of drift as a result of continuous learning in the presence of noise, suggests that this effect goes beyond the specific phenomenon of place cells. That is, drift within the solution space should occur in every type of task and scenario, and could be identified outside the scope of spatial representations. All 1151 simulations, except a negligible few, demonstrated an initial, fast, phase of convergence to low loss, followed by a much slower phase of directed random motion within the low-loss space.</p>
<p>The results of the simulations supported our main conclusion – sparsification dynamics were not sensitive to most of the parameters. For almost all of the simulations, excluding SGD with label noise, the fraction of active units gradually reduced, long after the loss converged (<xref rid="fig4" ref-type="fig">Fig 4A</xref>, <xref rid="figS1" ref-type="fig">Fig S1</xref> for further breakdown). For label noise, a slow directed effect was observed but the dynamics were qualitatively different – as predicted by theory [<xref ref-type="bibr" rid="c27">27</xref>] and explained in the next section. The fraction of active units did not reduce as much, but the activity of the units did sparsify (<xref rid="figS2" ref-type="fig">Fig S2</xref>). One qualitative difference observed between simulations was that the timescales could vary by orders of magnitude as a function of the noise scale (<xref rid="fig4" ref-type="fig">Fig 4B</xref> bottom, see methods for details). Additionally, apart from simulations that did not converge due to too large timescales, the final sparsity was the same for all networks with the same parameters (<xref rid="fig4" ref-type="fig">Fig 4B</xref> top), in accordance with results from [<xref ref-type="bibr" rid="c24">24</xref>]. In a sense, once noise is introduced, the network is driven to maximal sparsification in a stochastic manner. For Adam, RMSprop and SED sparsification ensued in the absence of any added noise. For SED the explanation is straightforward, as the parameter updates are driven by noise. For Adam and RMSprop, we suggest that in the vicinity of the zero-loss manifold, the second moment acts as noise. In some cases, the networks quickly collapsed to a sparse solution, most likely as a result of the learning rate being too high, in relation to the input statistics [<xref ref-type="bibr" rid="c39">39</xref>]. Importantly, for GD without noise, there was no change after the initial convergence.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Generality of the results.</title>
<p>Summary of 616 simulations with various parameters, excluding SGD with label noise (see <xref rid="tbl2" ref-type="table">Table 2</xref>). (A) Fraction of active units normalized by the first timestep for all simulations. Red line is the mean. Note that all simulations exhibit a stochastic decrease in the fraction of active units. See <xref rid="figS1" ref-type="fig">Fig S1</xref> for further breakdown. (B) Dependence of sparseness (top) and sparsification time scale (bottom) on noise amplitude. Each point is one of 178 simulations with the same parameters except noise variance. (C) Learning a similarity matching task with Hebbian and anti-Hebbian learning using published code from [<xref ref-type="bibr" rid="c24">24</xref>]. Performance of the network (blue) and fraction of active units (red) as a function of training steps. Note that the loss axis does not start at zero, and the dynamic range is small. The background colors indicate which phase is dominant throughout learning (1 - red, 2 - yellow, 3 - green).</p></caption>
<graphic xlink:href="539512v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As a further test of the generality of this phenomenon, we consider the recent simulation from [<xref ref-type="bibr" rid="c24">24</xref>] in which representational drift was shown. The learning rule used in this work was very different from the ones we applied, and more biologically plausible. We simulated that network using the published code and found the same type of dynamics as shown above. Namely, the network initially converged to a good solution followed by a longer period of sparsification (<xref rid="fig4" ref-type="fig">Fig 4C</xref>). Note that in the original publication [<xref ref-type="bibr" rid="c24">24</xref>] the focus was on the stage following this sparsification, in which the network indeed maintained a constant fraction of active cells.</p>
<p>In conclusion, we see that noisy learning leads to three phases under rather general conditions. The first phase is the learning of the task and convergence to the manifold of low-loss solutions. The second phase is directed movement on this manifold, driven by a second-order effect of implicit regularization. The third phase is an undirected random walk within the sub-manifold of low loss and maximum regularization. <xref rid="tbl1" ref-type="table">Table 1</xref> summarizes these three phases and their signature in network metrics. It is important to note that these are not consecutive phases but rather overlapping ones – they occur simultaneously, but due to their different time scales one can identify when each phase is dominant. A rough delineation of when each phase is dominant can be seen in <xref rid="fig4" ref-type="fig">Fig 4C</xref> background colors – in the first phase (red) the loss function converged, in the second phase (yellow) the fraction of active units reduced substantially. In the third phase (red) both were stationary but the tuning of units continued to change, as shown in the original paper [<xref ref-type="bibr" rid="c24">24</xref>]. We speculate that most experimental studies about drift demonstrated only the third phase of null drift, because they familiarized the animals to the environment for a substantial time period prior to recording. We refer to the second phase as a type of drift, because it happens after learning has finished and also features a gradual change in representations.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>The three phases of noisy learning.</title></caption>
<graphic xlink:href="539512v4_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>Parameter ranges for random simulations.</title></caption>
<graphic xlink:href="539512v4_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2c">
<title>Mechanism of sparsification</title>
<p>What are the mechanisms that give rise to this observed sparsification? As illustrated in <xref rid="fig1" ref-type="fig">Fig. 1</xref>, solutions in the zero-loss manifold have identical loss, but might vary in some of their properties. The authors of suggest that noisy learning will slowly increase the flatness of the loss landscape in the vicinity of the solution. This can be demonstrated with a simple example. Consider a two-dimensional loss function. The function is shaped like a valley with a continuous one-dimensional zero-loss manifold at its bottom (<xref rid="fig5" ref-type="fig">Fig 5A</xref>). Crucially, the loss on this one-dimensional manifold is exactly zero, while the vicinity of the manifold becomes systematically flatter in one direction. We simulated gradient descent with added noise on this function from a random starting point (red dot). The trajectory quickly converged to the zero-loss manifold, and began a random walk on it. This walk was clearly biased towards the flatter area of the manifold, as can be seen by the spread of the trajectory. This bias could be comprehended by noting that the gradient was orthogonal to the contour lines of the loss, and therefore had a component directed towards the flat region.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Noisy learning leads to a flat landscape.</title>
<p>(A) Gradient Descent dynamics over a two-dimensional loss function with a one-dimensional zero-loss manifold (colors from blue to yellow denote loss). Note that the loss is identically zero along the horizontal axis, but the left area is flatter. The orange trajectory begins at the red dot. Note the asymmetric extension into the left area. (B) Fraction of active units is highly correlated with the number of non-zero eigenvalues of the Hessian. (C) Update noise reduces small eigenvalues. Log of non-zero eigenvalues at two consecutive time points for learning with update noise. Note that eigenvalues do not correspond to one another when calculated at two different time points, and this plot demonstrates the change in their distribution rather than changes in eigenvalues corresponding to specific directions. The distribution of larger eigenvalues hardly changes, while the distribution of smaller eigenvalues is pushed to smaller values. (D) Label noise reduces the sum over eigenvalues. Same as (C), but for actual values instead of log.</p></caption>
<graphic xlink:href="539512v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In higher dimensions, flatness is captured by the eigenvalues of the Hessian of the loss. Because these eigenvalues are a collection of numbers, different scenarios could lead to minimizing different aspects of this collection. Specifically, according to [<xref ref-type="bibr" rid="c26">26</xref>], update noise should regularize the sum of the log of the non-zero eigenvalues while label noise should do the same for the sum of eigenvalues. In our predictive coding example, where update noise was added, each inactivated unit translates into a set of zero-rows in the Hessian (see methods), and thus also into a set of zero-eigenvalues (<xref rid="fig5" ref-type="fig">Fig 5B</xref>). The slope of the regularizer approaches infinity as the eigenvalue approaches zero, and thus small eigenvalues are driven to zero much faster than large eigenvalues (<xref rid="fig5" ref-type="fig">Fig 5C</xref>). So in this case, update noise leads to an increase in the number of zero eigenvalues, which are manifested as a sparse solution. Another, perhaps more intuitive, way to understand these results is that units below the activation threshold are insensitive to noise perturbations. In other scenarios, in which we simulated with label noise, we indeed observed a gradual decrease in the sum of eigenvalues (<xref rid="fig5" ref-type="fig">Fig 5D</xref>). For a more intuitive demonstration of this phenomenon, see <xref rid="figS2" ref-type="fig">Fig S2</xref>.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We showed that representational drift could arise from ongoing learning in the presence of noise, after a network has already reached good performance. We suggest that learning is divided into three overlapping phases: a fast initial phase, where good performance is achieved, a second slower phase in which <italic>directed</italic> drift along the low-loss manifold leads to an implicit regularization and finally, a third <italic>undirected</italic> phase ensues once the regularizer is minimized. In our results, the directed component was associated with sparsification of the neural code. We verified the existence of this phenomenon in experimental data from four different labs. It is important to note that sparseness was related to flatness of the loss landscape in the specific case of a single hidden layer feedforward neural network and update or label noise. For other architectures and noise types, the change in activity statistics will most likely be different and calls for further work. The CA1 region is known to have little recurrent connections, which possibly explains why these results match.</p>
<p>Interpreting drift as a learning process has recently been suggested by [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>]. Both studies focused on the final phase in which the statistics of the representations were constant. Experimentally, [<xref ref-type="bibr" rid="c7">7</xref>] reported a decrease in activity at the beginning of the experiment, which they suggested was correlated with some behavioral change, but we believe it could also be a result of the directed drift phase. [<xref ref-type="bibr" rid="c40">40</xref>] also reported a slow directed change in representation long after familiarity with the stimuli. There is another consequence of the timescale separation. Unlike in the setting of drift experiments, natural environments are never truly constant. Thus, it is possible that the second phase of learning never stops because the task is slowly changing. This would imply that the second, directed, phase may be the natural regime in which neural networks reside.</p>
<p>Here, we reported directed drift in the space of solutions of neural networks. This drift could be observed by examining changes to the representation of external world variables, and hence is related to the phenomenon of representational drift. Note, however, that representations are not a full description of a network’s behavior [<xref ref-type="bibr" rid="c41">41</xref>]. The statistics of representational changes can be used as a window into changes of network dynamics and function.</p>
<p>The phenomenon of directed drift is very robust to various modeling choices, and also consistent with recent theoretical results [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>] The details of the direction of the drift, however, are dependent on specific choices. Specifically, which aspects of the Hessian are minimized during the second phase of learning, as well as the timescale of this phase, depend on the specifics of the learning rule and the noise in the system. This suggests an exciting opportunity – inferring the learning rule of a network from the statistics of representational drift.</p>
<p>Our explanation of drift invoked the concept of a low-loss manifold – a family of network configurations that have nearly identical performance on a task. The definition of low-loss, however, depends on the specific task and context analyzed. Challenging a system with new inputs could dissociate two configurations that otherwise appear identical [<xref ref-type="bibr" rid="c42">42</xref>]. It will be interesting to explore whether various environmental perturbations could uncover the motion along the low-loss manifold in the CA1 population. An important subject relating to perturbations is that of remapping – the phenomenon in which place cells change their tuning in response to a change in the environment or change in context. One can therefore speculate that, as the network moves to flatter areas of the loss landscape, becoming more robust to noise and thus to perturbations, the probability for remapping given the same environmental change will systematically decrease. A functional interpretation of remapping as latent state (context) inference is given by [<xref ref-type="bibr" rid="c43">43</xref>]. The authors also summarize results from a series of morph experiments and offer a predictions similar to ours (Fig. 7 there) – that discrepancies between remapping probabilities can by explained as testing at different points of training. Beyond such abstract models, for future work, it is also possible to mechanistically model multiple environments and study remapping probabilities through them [<xref ref-type="bibr" rid="c44">44</xref>].</p>
<p>Machine learning has been suggested as a model tool for neuroscience research [<xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c47">47</xref>]. However, the implicit regularization in ML has not been studied to explain representational drift in neuroscience, and may have been done without awareness of this phenomenon. It’s worth noting that this isn’t a phenomenon specific to neural networks, but rather a general property of overparameterized systems that optimize a cost function. Importing insights from this domain into neuroscience shows the utility of studying general phenomena in systems that learn. For example, another complex learning system in which a similar idea has been proposed is evolution – “survival of the flattest” suggests that, under a high mutation rate, the fittest replicators are not just the ones with the highest fitness, but also with a flat fitness function which is more robust to mutations [<xref ref-type="bibr" rid="c48">48</xref>]. One can hope that more such insights will arise as we open our eyes.</p>
</sec>
<sec id="s4">
<title>Code availablity</title>
<p>The code for this project is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Aviv-Ratzon/DriftReg">https://github.com/Aviv-Ratzon/DriftReg</ext-link></p>
<sec id="s4a">
<title>Materials and methods Predictive coding task</title>
<p>The agent is moving in an arena of size (<italic>L</italic><sub><italic>x</italic></sub>, <italic>L</italic><sub><italic>y</italic></sub>), with constant velocity in the <italic>y</italic> direction of <italic>V</italic><sub>0</sub>. The agent’s heading direction is <italic>θ</italic> and it changes at every time step by<inline-formula><inline-graphic xlink:href="539512v4_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, the agent’s visual field has an angle <italic>θ</italic><sub><italic>vis</italic></sub> and is represented as a vector of size <italic>L</italic><sub><italic>vis</italic></sub>. The texture of the walls is generated from a random Gaussian vector of size <italic>L</italic><sub><italic>walls</italic></sub> = 2(<italic>L</italic><sub><italic>x</italic></sub> + <italic>L</italic><sub><italic>y</italic></sub>)<italic>L</italic><sub><italic>vis</italic></sub>, smoothed with a Gaussian filter with <italic>σ</italic><sup>2</sup> = <italic>K</italic><sub><italic>smooth</italic></sub><italic>L</italic><sub><italic>walls</italic></sub>. At each time step the agent receives the visual input from the walls, determined by the intersection points of it’s visual field with the walls. When the agent reaches a distance of <italic>L</italic><sub><italic>y</italic></sub><italic>L</italic><sub><italic>buffer</italic></sub> from the wall, it turns to the opposite direction.</p>
</sec>
<sec id="s4b">
<title>Tuning properties of units</title>
<p>For each unit we calculated a tuning curve. We divided the arena into 100 equal bins and computed the number of time steps in each bin and the mean unit activation. We then obtained the tuning curve by dividing the mean activity for each bin by the occupancy. We treated movement in each direction as a separate location. We calculated the spatial information (SI) of the tuning curves for each unit:
<disp-formula id="eqn5">
<graphic xlink:href="539512v4_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>i</italic> is the index of the bin, <italic>p</italic><sub><italic>i</italic></sub> is the probability of being in the bin, <italic>r</italic><sub><italic>i</italic></sub> is the value of the tuning curve in the bin and <inline-formula><inline-graphic xlink:href="539512v4_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the unit’s mean activity rate. Active unit was defined as a unit with non-zero activation for at least one input.</p>
</sec>
<sec id="s4c">
<title>Measuring sparsity</title>
<p>We measure sparsity using two metrics: Active fraction, and fraction active units. These can be calculated from the activation matrix, where each row corresponds to a single unit, each column corresponds to an input and the values of the cells are the activations of a given unit for a given input. We can then binarize this matrix, giving a value of 1 to cells with non-zero activations. The active fraction is the mean over all cells of the matrix. The fraction of active units is the fraction of rows with at least one non-zero value. Note that “units” refers to hidden layer units.</p>
</sec>
<sec id="s4d">
<title>Simulations</title>
<p>For the random simulations, we train each network for 10<sup>7</sup> training steps while choosing random learning algorithm and parameters. The ranges and relevant values of parameters are specified in <xref rid="tbl2" ref-type="table">Table 2</xref>. For Adam and SED there was no added noise.</p>
<fig id="ufig1" position="float" fig-type="figure">
<graphic xlink:href="539512v4_ufig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4e">
<title>Sparsification timescale</title>
<p>To produce <xref rid="fig4" ref-type="fig">Fig 4B</xref> bottom, we fitted an exponential curve over the curve of fraction of active units for each simulation and extracted the time constant of the exponential. To simplify this fit, we shifted each curve such that it plateaus at zero. We also clipped the length of the curves at the point in which they reach 90% of their final value to avoid fitting noise in the plateau after convergence. Note that we did this calculation to a subset of 178 simulations with the same parameters and varying noise scale. We chose this set of parameters because it exhibited a relatively “nice” exponential curve. As can be seen is <xref rid="fig4" ref-type="fig">Fig 4</xref> A, some simulations exhibit a long plateau in the fraction of active units followed by a sudden start of the sparsification process. This type of plateau followed by a phase transition was described in previous works [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. For some of the simulations finding the timescale of sparsification is not straightforward and rather noisy, thus for simplicity sake we chose to calculate it only over the mentioned subset.</p>
</sec>
<sec id="s4f">
<title>Stochastic Error Descent</title>
<p>The equation for parameter updates under this learning rule is given by:
<disp-formula id="eqn6">
<graphic xlink:href="539512v4_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In this learning rule, the parameters are randomly perturbed at each training step by a Gaussian noise denoted by <italic>ξ</italic><sub><italic>τ</italic></sub> and then updated in proportion to the change in loss.</p>
</sec>
<sec id="s4g">
<title>Experimental data</title>
<p>We present here a detailed description of the analyses performed for each data set. <xref rid="tbl3" ref-type="table">Table 3</xref> summarizes the differences between them, along with an additional publication about CA1 drift [<xref ref-type="bibr" rid="c23">23</xref>] in which stationary statistics were reported.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><title>Description of experimental data sets.</title></caption>
<graphic xlink:href="539512v4_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<sec id="s4g1">
<title>Khatib et al. [<xref ref-type="bibr" rid="c22">22</xref>]</title>
<p>The results presented here were also shown in the paper, and a full description is available there. Only frames where the mice moved faster then <inline-formula><inline-graphic xlink:href="539512v4_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> were analyzed. We calculated the fraction of place cells out of all recognized cells, place cells were classified using a shuffle test. We also used the published code from [<xref ref-type="bibr" rid="c51">51</xref>] to verify that the increase in SI is sustained under bias corrections. Note that we treated the linear track as one-dimensional and separated the two different running directions, bins were 4<italic>cm</italic> in length. The metrics were averaged over cells pooled together from all animals. Data is not publicly available.</p>
</sec>
<sec id="s4g2">
<title>Jercog et al. [<xref ref-type="bibr" rid="c52">52</xref>]</title>
<p>The results presented here are novel. The published data features rate maps and full trajectories, without spike data. Only neurons with an average activity rate of over 0.1<italic>Hz</italic> were included. Because of this, we calculated only the number of active neurons each day rather than the fraction and could not classify which were place cells. We calculated the binned occupancy maps from the trajectories and used them with the published rate maps to calculate SI. Data is available at: <ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/hc/hc-22/about-hc-22">https://crcns.org/data-sets/hc/hc-22/about-hc-22</ext-link></p>
</sec>
<sec id="s4g3">
<title>Karlsson et al. [<xref ref-type="bibr" rid="c53">53</xref>]</title>
<p>The results presented here are novel. The data features spikes and trajectories. We filtered the data to include time bins when the animals moved faster than <inline-formula><inline-graphic xlink:href="539512v4_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and only neurons from CA1. We used the published code from [<xref ref-type="bibr" rid="c51">51</xref>] to calculate the fraction of place cells along with SI, and verified that the increase in SI is sustained under bias corrections. For the occupancy map we treated the entire W-shaped arena as a square and and used bins of approximately 4<italic>cm</italic> length. This was done for the sake of simplifying the analysis, a more accurate method would be to consider separately each linear track and movement direction. Note that in this dataset animals spent varying amounts of time in two different environments. We pooled together for each animal the data of cells from either environment according to experience time in them. For example, if the animal visited environment A on day 4 for the 4th time and had 20 active cells, and visited environment B on day 6 for the 4th time and had 30 active cells, we pooled together the entire 50 cells for day 4 of this animal. Data is available at: <ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/hc/hc-6/about-hc-5">https://crcns.org/data-sets/hc/hc-6/about-hc-5</ext-link></p>
</sec>
<sec id="s4g4">
<title>Sheintuch et al. [<xref ref-type="bibr" rid="c35">35</xref>]</title>
<p>The results presented here were also shown in the paper. The data features various metrics calculated from the activity. We averaged the fraction of place cells and mean SI over animals for each day. Data is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/zivlab/cell_assemblies">https://github.com/zivlab/cell_assemblies</ext-link></p>
</sec>
</sec>
<sec id="s4h">
<title>Label noise</title>
<p>Label noise is introduced to the loss function given by the following formula:
<disp-formula id="eqn7">
<graphic xlink:href="539512v4_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="539512v4_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is Gaussian noise.</p>
</sec>
<sec id="s4i">
<title>Gradient descent dynamics around the zero-loss manifold</title>
<p>The function we used for the two-dimensional example was given by:
<disp-formula id="eqn8">
<graphic xlink:href="539512v4_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
which has zero loss on the <italic>x</italic> and <italic>y</italic> axes. For small enough update noise, GD will converge to the vicinity of this manifold (the axes). We consider a point on the <italic>x</italic> axis: (<italic>x</italic><sub>0</sub>, 0), and calculate the direction of the gradient near that point. Because we are interested in motion along the zero-loss manifold, we consider a small perturbation in the orthogonal direction (<italic>x</italic><sub>0</sub>, 0 + Δ<italic>y</italic>) where <italic>x</italic><sub>0</sub> <italic>&gt;&gt;</italic> 1 and |Δ<italic>y</italic>| <italic>&lt;&lt;</italic> 1. Any component of the gradient in the <italic>x</italic> direction will lead to motion along the manifold. The update step at this point is given by:
<disp-formula id="eqn9">
<graphic xlink:href="539512v4_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
One can observe that the step has a large component in the <italic>y</italic> direction, quickly returning to the manifold. There is also a smaller component in the <italic>x</italic> direction, reducing the value of <italic>x</italic>. Reducing <italic>x</italic> also reduces the Hessian’s eigenvalues:
<disp-formula id="eqn10">
<graphic xlink:href="539512v4_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn11">
<graphic xlink:href="539512v4_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Thus, it becomes clear that the trajectory will have a bias that reduces the curvature in the <italic>y</italic> direction.</p>
<p>For general loss functions and various noise models, rigorous proofs can be found in [<xref ref-type="bibr" rid="c26">26</xref>], and a different approach can be found in [<xref ref-type="bibr" rid="c27">27</xref>]. Here, we will briefly outline the intuition for the general case. Consider again the update rule for GD:
<disp-formula id="eqn12">
<graphic xlink:href="539512v4_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In order to understand the dynamics close to the zero-loss manifold, we consider a point <italic>θ</italic>, for which <italic>L</italic>(<italic>θ</italic>) = 0 expand the loss around it:
<disp-formula id="eqn13">
<graphic xlink:href="539512v4_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We can then take the gradient of this expansion with respect to <italic>θ</italic>:
<disp-formula id="eqn14">
<graphic xlink:href="539512v4_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn15">
<graphic xlink:href="539512v4_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The first term is zero, because the gradient is zero on the manifold. The second term is the largest one, as it linear in <italic>δθ</italic>. Note that the Hessian matrix has zero eigenvalues in directions on the zero-loss manifold, and non-zero eigenvalues in other directions. Thus, the second term corresponds to projecting <italic>δθ</italic> in a direction that is orthogonal to the zero-loss manifold. The third term can be interpreted as the gradient of some auxiliary loss function. Thus, we expect gradient descent to minimize this new loss, which corresponds to a quadratic form with the Hessian. This is the reason for the implicit regularization along the manifold. Note that the auxiliary loss function is defined by <italic>δθ</italic>, and thus different noise statistics will correspond, on average, to different implicit regularizations. In conclusion, the update step will have a large component that moves the parameter vector towards the zero-loss manifold, and a small component that moves the parameter vector on the manifold in a direction that minimizes some measure of the Hessian.</p>
</sec>
<sec id="s4j">
<title>Hessian and sparseness</title>
<p>In the main text, we show that the implicit regularization of the Hessian leads to sparse representations. Here, we show this relationship for a single-hidden layer feed-forward neural network with ReLU activation and Mean Squared Error loss:
<disp-formula id="eqn16">
<graphic xlink:href="539512v4_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The gradient and Hessian at the zero-loss manifold are given by [<xref ref-type="bibr" rid="c54">54</xref>]:
<disp-formula id="eqn17">
<graphic xlink:href="539512v4_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn18">
<graphic xlink:href="539512v4_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="539512v4_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is an indicator vector denoting whether each unit is active for some input <bold>x</bold><sub><italic>i</italic></sub>. Sparseness means that a unit has become inactive for all inputs. All the partial derivatives of input, output and bias weights associated with such a unit are zero, and thus the relevant rows of the Hessian are zero as well. Thus, every inactive unit leads to several zero eigenvalues.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Ron Teichner and Kabir Dabholkar for comments on the manuscript. This research was supported by the ISRAEL SCIENCE FOUNDATION (grants Nos. 2655/18 and 2183/21 to DD, and 1442/21to OB), by the German-Israeli Foundation (GIF I-1477-421.13/2018) to DD, by a grant from the US-Israel Binational Science Foundation (NIMH-BSF CRCNS BSF:2019807, NIMH:R01 MH125544- 01 to DD), by an HFSP research grant (RGP0017/2021) to OB, A Rappaport Institute Collaborative research grant to DD, by Israel PBC-VATAT and by the Technion Center for Machine Learning and Intelligent Systems (MLIS) to DD and OB, by the Prince Center for the Aging Brain, and by a University of Michigan – Israel Partnership for Research and Education Collaborative Research stipend to DK. (data science)</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>John</given-names> <surname>O’keefe</surname></string-name> and <string-name><given-names>Lynn</given-names> <surname>Nadel</surname></string-name>. <article-title>The hippocampus as a cognitive map</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>2</volume>(<issue>4</issue>):<fpage>487</fpage>–<lpage>494</lpage>, <year>1979</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="other"><string-name><given-names>John</given-names> <surname>O’Keefe</surname></string-name> and <string-name><given-names>Jonathan</given-names> <surname>Dostrovsky</surname></string-name>. <article-title>The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat</article-title>. <source>Brain research</source>, <year>1971</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>David H</given-names> <surname>Hubel</surname></string-name> and <string-name><given-names>Torsten N</given-names> <surname>Wiesel</surname></string-name>. <article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title>. <source>The Journal of physiology</source>, <volume>160</volume>(<issue>1</issue>):<fpage>10</fpage>, <year>1962</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>Bruce L</given-names> <surname>McNaughton</surname></string-name>, <string-name><given-names>SJY</given-names> <surname>Mizumori</surname></string-name>, <string-name><given-names>CA</given-names> <surname>Barnes</surname></string-name>, <string-name><given-names>BJ</given-names> <surname>Leonard</surname></string-name>, <string-name><given-names>M</given-names> <surname>Marquis</surname></string-name>, and <string-name><given-names>EJ</given-names> <surname>Green</surname></string-name>. <article-title>Cortical representation of motion during unrestrained spatial navigation in the rat</article-title>. <source>Cerebral Cortex</source>, <volume>4</volume>(<issue>1</issue>):<fpage>27</fpage>–<lpage>39</lpage>, <year>1994</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name>, <string-name><given-names>Laurie D</given-names> <surname>Burns</surname></string-name>, <string-name><given-names>Eric D</given-names> <surname>Cocker</surname></string-name>, <string-name><given-names>Elizabeth O</given-names> <surname>Hamel</surname></string-name>, <string-name><given-names>Kunal K</given-names> <surname>Ghosh</surname></string-name>, <string-name><given-names>Lacey J</given-names> <surname>Kitch</surname></string-name>, <string-name><given-names>Abbas</given-names> <surname>El Gamal</surname></string-name>, and <string-name><given-names>Mark J</given-names> <surname>Schnitzer</surname></string-name>. <article-title>Long-term dynamics of ca1 hippocampal place codes</article-title>. <source>Nature neuroscience</source>, <volume>16</volume>(<issue>3</issue>):<fpage>264</fpage>–<lpage>26</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>Laura N.</given-names> <surname>Driscoll</surname></string-name>, <string-name><given-names>Noah L.</given-names> <surname>Pettit</surname></string-name>, <string-name><given-names>Matthias</given-names> <surname>Minderer</surname></string-name>, <string-name><given-names>Selmaan N.</given-names> <surname>Chettih</surname></string-name>, and <string-name><given-names>Christopher D.</given-names> <surname>Harvey</surname></string-name>. <article-title>Dynamic Reorganization of Neuronal Activity Patterns in Parietal Cortex</article-title>. <source>Cell</source>, <volume>170</volume>(<issue>5</issue>):<fpage>986</fpage>–<lpage>999</lpage>, 8 <year>2017</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>Daniel</given-names> <surname>Deitch</surname></string-name>, <string-name><given-names>Alon</given-names> <surname>Rubin</surname></string-name>, and <string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name>. <article-title>Representational drift in the mouse visual cortex</article-title>. <source>Current biology</source>, <volume>31</volume>(<issue>19</issue>):<fpage>4327</fpage>–<lpage>433</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>Carl E</given-names> <surname>Schoonover</surname></string-name>, <string-name><given-names>Sarah N</given-names> <surname>Ohashi</surname></string-name>, <string-name><given-names>Richard</given-names> <surname>Axel</surname></string-name>, and <string-name><given-names>Andrew J P</given-names> <surname>Fink</surname></string-name>. <article-title>Representational drift in primary olfactory cortex</article-title>. <source>Nature</source>, <volume>594</volume>, <year>2021</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>Gilad A</given-names> <surname>Jacobson</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Rupprecht</surname></string-name>, and <string-name><given-names>Rainer W</given-names> <surname>Friedrich</surname></string-name>. <article-title>Experience-dependent plasticity of odor representations in the telencephalon of zebrafish</article-title>. <source>Current Biology</source>, <volume>28</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>1</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>William A</given-names> <surname>Liberti</surname>, <suffix>3rd</suffix></string-name>, <string-name><given-names>Tobias A</given-names> <surname>Schmid</surname></string-name>, <string-name><given-names>Angelo</given-names> <surname>Forli</surname></string-name>, <string-name><given-names>Madeleine</given-names> <surname>Snyder</surname></string-name>, and <string-name><given-names>Michael M</given-names> <surname>Yartsev</surname></string-name>. <article-title>Publisher correction: A stable hippocampal code in freely flying bats</article-title>. <source>Nature</source>, <volume>606</volume>(<issue>7914</issue>):<fpage>E6</fpage>, <month>June</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>Sadra</given-names> <surname>Sadeh</surname></string-name> and <string-name><given-names>Claudia</given-names> <surname>Clopath</surname></string-name>. <article-title>Contribution of behavioural variability to representational drift</article-title>. <source>Elife</source>, <volume>11</volume>:<issue>e7790</issue>, <year>2022</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>Michael E</given-names> <surname>Rule</surname></string-name>, <string-name><given-names>Timothy</given-names> <surname>O’Leary</surname></string-name>, and <string-name><given-names>Christopher D</given-names> <surname>Harvey</surname></string-name>. <article-title>Causes and consequences of representational drift</article-title>. <source>Curr. Opin. Neurobiol</source>., <volume>58</volume>:<fpage>141</fpage>–<lpage>147</lpage>, <month>October</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>Laura N</given-names> <surname>Driscoll</surname></string-name>, <string-name><given-names>Lea</given-names> <surname>Duncker</surname></string-name>, and <string-name><given-names>Christopher D</given-names> <surname>Harvey</surname></string-name>. <article-title>Representational drift: Emerging theories for continual learning and experimental future directions</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>76</volume>:<issue>10260</issue>, <year>2022</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>Noam E</given-names> <surname>Ziv</surname></string-name> and <string-name><given-names>Naama</given-names> <surname>Brenner</surname></string-name>. <article-title>Synaptic tenacity or lack thereof: spontaneous remodeling of synapses</article-title>. <source>Trends in neurosciences</source>, <volume>41</volume>(<issue>2</issue>):<fpage>89</fpage>–<lpage>9</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>Alon</given-names> <surname>Rubin</surname></string-name>, <string-name><given-names>Nitzan</given-names> <surname>Geva</surname></string-name>, <string-name><given-names>Liron</given-names> <surname>Sheintuch</surname></string-name>, and <string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name>. <article-title>Hippocampal ensemble dynamics timestamp events in long-term memory</article-title>. <source>elife</source>, <volume>4</volume>:<issue>e1224</issue>, <year>2015</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>Kyle</given-names> <surname>Aitken</surname></string-name>, <string-name><given-names>Marina</given-names> <surname>Garrett</surname></string-name>, <string-name><given-names>Shawn</given-names> <surname>Olsen</surname></string-name>, and <string-name><given-names>Stefan</given-names> <surname>Mihalas</surname></string-name>. <article-title>The geometry of representational drift in natural and artificial neural networks</article-title>. <source>PLOS Computational Biology</source>, <volume>18</volume>(<issue>11</issue>):<fpage>e101071</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>David</given-names> <surname>Kappel</surname></string-name>, <string-name><given-names>Stefan</given-names> <surname>Habenschuss</surname></string-name>, <string-name><given-names>Robert</given-names> <surname>Legenstein</surname></string-name>, and <string-name><given-names>Wolfgang</given-names> <surname>Maass</surname></string-name>. <article-title>Network plasticity as bayesian inference</article-title>. <source>PLoS computational biology</source>, <volume>11</volume>(<issue>11</issue>):<fpage>e100448</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>Uri</given-names> <surname>Rokni</surname></string-name>, <string-name><given-names>Andrew G</given-names> <surname>Richardson</surname></string-name>, <string-name><given-names>Emilio</given-names> <surname>Bizzi</surname></string-name>, and <string-name><given-names>H</given-names> <surname>Sebastian Seung</surname></string-name>. <article-title>Motor learning with unstable neural representations</article-title>. <source>Neuron</source>, <volume>54</volume>(<issue>4</issue>):<fpage>653</fpage>–<lpage>66</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>Lee</given-names> <surname>Susman</surname></string-name>, <string-name><given-names>Naama</given-names> <surname>Brenner</surname></string-name>, and <string-name><given-names>Omri</given-names> <surname>Barak</surname></string-name>. <article-title>Stable memory with unstable synapses</article-title>. <source>Nature communications</source>, <volume>10</volume>(<issue>1</issue>):<fpage>444</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>Gianluigi</given-names> <surname>Mongillo</surname></string-name>, <string-name><given-names>Simon</given-names> <surname>Rumpel</surname></string-name>, and <string-name><given-names>Yonatan</given-names> <surname>Loewenstein</surname></string-name>. <article-title>Intrinsic volatility of synaptic con-nections—a challenge to the synaptic trace theory of memory</article-title>. <source>Current opinion in neurobiology</source>, <volume>46</volume>:<fpage>7</fpage>–<lpage>1</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>Yaroslav Felipe Kalle</given-names> <surname>Kossio</surname></string-name>, <string-name><given-names>Sven</given-names> <surname>Goedeke</surname></string-name>, <string-name><given-names>Christian</given-names> <surname>Klos</surname></string-name>, and <string-name><given-names>Raoul-Martin</given-names> <surname>Memmesheimer</surname></string-name>. <article-title>Drifting assemblies for persistent memory: Neuron transitions and unsupervised compensation</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>46</issue>):<fpage>e202383211</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="other"><string-name><given-names>Dorgham</given-names> <surname>Khatib</surname></string-name>, <string-name><given-names>Aviv</given-names> <surname>Ratzon</surname></string-name>, <string-name><given-names>Mariell</given-names> <surname>Sellevoll</surname></string-name>, <string-name><given-names>Omri</given-names> <surname>Barak</surname></string-name>, <string-name><given-names>Genela</given-names> <surname>Morris</surname></string-name>, and <string-name><given-names>Dori</given-names> <surname>Derdikman</surname></string-name>. <article-title>Active experience, not time, determines within-day representational drift in dorsal ca1</article-title>. <source>Neuron</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="other"><string-name><given-names>Nitzan</given-names> <surname>Geva</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Deitch</surname></string-name>, <string-name><given-names>Alon</given-names> <surname>Rubin</surname></string-name>, and <string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name>. <article-title>Time and experience differentially affect distinct aspects of hippocampal representational drift</article-title>. <source>Neuron</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="other"><string-name><given-names>Shanshan</given-names> <surname>Qin</surname></string-name>, <string-name><given-names>Shiva</given-names> <surname>Farashahi</surname></string-name>, <string-name><given-names>David</given-names> <surname>Lipshutz</surname></string-name>, <string-name><given-names>Anirvan M</given-names> <surname>Sengupta</surname></string-name>, <string-name><given-names>Dmitri B</given-names> <surname>Chklovskii</surname></string-name>, and <string-name><given-names>Cengiz</given-names> <surname>Pehlevan</surname></string-name>. <article-title>Coordinated drift of receptive fields in hebbian/anti-hebbian network models during noisy representation learning</article-title>. <source>Nature Neuroscience</source>, pages <fpage>1</fpage>–<lpage>1</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="other"><string-name><given-names>Farhad</given-names> <surname>Pashakhanloo</surname></string-name> and <string-name><given-names>Alexei</given-names> <surname>Koulakov</surname></string-name>. <article-title>Stochastic gradient descent-induced drift of representation in a two-layer neural network</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2302.02563</pub-id>, <year>2023</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="book"><string-name><given-names>Guy</given-names> <surname>Blanc</surname></string-name>, <string-name><given-names>Neha</given-names> <surname>Gupta</surname></string-name>, <string-name><given-names>Gregory</given-names> <surname>Valiant</surname></string-name>, and <string-name><given-names>Paul</given-names> <surname>Valiant</surname></string-name>. <chapter-title>Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process</chapter-title>. <source>In Conference on learning theory</source>, pages <fpage>483</fpage>–<lpage>513</lpage>. <publisher-name>PML</publisher-name>, <year>2020</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="other"><string-name><given-names>Zhiyuan</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Tianhao</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>Sanjeev</given-names> <surname>Arora</surname></string-name>. <article-title>What happens after sgd reaches zero loss?–a mathematical framework</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2110.06914</pub-id>, <year>2021</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>Ning</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Chao</given-names> <surname>Tang</surname></string-name>, and <string-name><given-names>Yuhai</given-names> <surname>Tu</surname></string-name>. <article-title>Stochastic gradient descent introduces an effective landscapedependent regularization favoring flat solutions</article-title>. <source>Physical Review Letters</source>, <volume>130</volume>(<issue>23</issue>):<fpage>23710</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="other"><string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>Dong-Hyun</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Jorg</given-names> <surname>Bornschein</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Mesnard</surname></string-name>, and <string-name><given-names>Zhouhan</given-names> <surname>Lin</surname></string-name>. <article-title>Towards biologically plausible deep learning</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1502.04156</pub-id>, <year>2015</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><given-names>Yuhan Helena</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Arna</given-names> <surname>Ghosh</surname></string-name>, <string-name><given-names>Blake</given-names> <surname>Richards</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Shea-Brown</surname></string-name>, and <string-name><given-names>Guillaume</given-names> <surname>Lajoie</surname></string-name>. <article-title>Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>35</volume>:<fpage>23077</fpage>–<lpage>2309</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><given-names>Owen</given-names> <surname>Marschall</surname></string-name>, <string-name><given-names>Kyunghyun</given-names> <surname>Cho</surname></string-name>, and <string-name><given-names>Cristina</given-names> <surname>Savin</surname></string-name>. <article-title>A unified framework of online learning algorithms for training recurrent neural networks</article-title>. <source>The Journal of Machine Learning Research</source>, <volume>21</volume>(<issue>1</issue>):<fpage>5320</fpage>–<lpage>535</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>Stefano</given-names> <surname>Recanatesi</surname></string-name>, <string-name><given-names>Matthew</given-names> <surname>Farrell</surname></string-name>, <string-name><given-names>Guillaume</given-names> <surname>Lajoie</surname></string-name>, <string-name><given-names>Sophie</given-names> <surname>Deneve</surname></string-name>, <string-name><given-names>Mattia</given-names> <surname>Rigotti</surname></string-name>, and <string-name><given-names>Eric</given-names> <surname>Shea-Brown</surname></string-name>. <article-title>Predictive learning as a network mechanism for extracting low-dimensional latent space representations</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>, <year>2021</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>Pablo E</given-names> <surname>Jercog</surname></string-name>, <string-name><given-names>Yashar</given-names> <surname>Ahmadian</surname></string-name>, <string-name><given-names>Caitlin</given-names> <surname>Woodruff</surname></string-name>, <string-name><given-names>Rajeev</given-names> <surname>Deb-Sen</surname></string-name>, <string-name><given-names>Laurence F</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>Eric R</given-names> <surname>Kandel</surname></string-name>. <article-title>Heading direction with respect to a reference point modulates place-cell activity</article-title>. <source>Nature communications</source>, <volume>10</volume>(<issue>1</issue>):<fpage>233</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>Mattias P</given-names> <surname>Karlsson</surname></string-name> and <string-name><given-names>Loren M</given-names> <surname>Frank</surname></string-name>. <article-title>Network dynamics underlying the formation of sparse, informative representations in the hippocampus</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>(<issue>52</issue>):<fpage>14271</fpage>–<lpage>1428</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><given-names>Liron</given-names> <surname>Sheintuch</surname></string-name>, <string-name><given-names>Nitzan</given-names> <surname>Geva</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Deitch</surname></string-name>, <string-name><given-names>Alon</given-names> <surname>Rubin</surname></string-name>, and <string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name>. <article-title>Organization of hippocampal ca3 into correlated cell assemblies supports a stable spatial code</article-title>. <source>Cell Reports</source>, <volume>42</volume>(<issue>2</issue>, <year>2023</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="other"><string-name><given-names>Diederik P</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>Jimmy</given-names> <surname>Ba</surname></string-name>. <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1412.6980</pub-id>, <year>2014</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name>, <string-name><given-names>Nitish</given-names> <surname>Srivastava</surname></string-name>, and <string-name><given-names>Kevin</given-names> <surname>Swersky</surname></string-name>. <article-title>Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</article-title>. <source>Cited on</source>, <volume>14</volume>(<issue>8</issue>):, <year>2012</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><given-names>Gert</given-names> <surname>Cauwenberghs</surname></string-name>. <article-title>A fast stochastic error-descent algorithm for supervised learning and opti-mization</article-title>. <source>Advances in neural information processing systems</source>, <volume>5</volume>, <year>1992</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><given-names>Rotem</given-names> <surname>Mulayoff</surname></string-name>, <string-name><given-names>Tomer</given-names> <surname>Michaeli</surname></string-name>, and <string-name><given-names>Daniel</given-names> <surname>Soudry</surname></string-name>. <article-title>The implicit bias of minima stability: A view from function space</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>34</volume>:<fpage>17749</fpage>–<lpage>1776</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="other"><string-name><given-names>Nghia D</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>Andrew</given-names> <surname>Lutas</surname></string-name>, <string-name><given-names>Jesseba</given-names> <surname>Fernando</surname></string-name>, <string-name><given-names>Josselyn</given-names> <surname>Vergara</surname></string-name>, <string-name><given-names>Justin</given-names> <surname>McMahon</surname></string-name>, <string-name><given-names>Jordane</given-names> <surname>Dimidschstein</surname></string-name>, and <string-name><given-names>Mark L</given-names> <surname>Andermann</surname></string-name>. <article-title>Cortical reactivations predict future sensory responses</article-title>. <source>bioRxiv</source>, pages <fpage>2022</fpage>–<lpage>1</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><string-name><given-names>Romain</given-names> <surname>Brette</surname></string-name>. <article-title>Is coding a relevant metaphor for the brain?</article-title> <source>Behavioral and Brain Sciences</source>, <volume>42</volume>:<issue>e21</issue>, <year>2019</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><string-name><given-names>Elia</given-names> <surname>Turner</surname></string-name>, <string-name><given-names>Kabir V</given-names> <surname>Dabholkar</surname></string-name>, and <string-name><given-names>Omri</given-names> <surname>Barak</surname></string-name>. <article-title>Charting and navigating the space of solutions for recurrent neural networks</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>34</volume>:<fpage>25320</fpage>–<lpage>2533</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><given-names>Honi</given-names> <surname>Sanders</surname></string-name>, <string-name><given-names>Matthew A</given-names> <surname>Wilson</surname></string-name>, and <string-name><given-names>Samuel J</given-names> <surname>Gershman</surname></string-name>. <article-title>Hippocampal remapping as hidden state inference</article-title>. <source>Elife</source>, <volume>9</volume>:<issue>e5114</issue>, <year>2020</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="other"><string-name><given-names>Isabel IC</given-names> <surname>Low</surname></string-name>, <string-name><given-names>Lisa M</given-names> <surname>Giocomo</surname></string-name>, and <string-name><given-names>Alex H</given-names> <surname>Williams</surname></string-name>. <article-title>Remapping in a recurrent neural network model of navigation and context inference</article-title>. <source>bioRxiv</source>, pages <fpage>2023</fpage>–<lpage>0</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><string-name><given-names>Blake A</given-names> <surname>Richards</surname></string-name>, <string-name><given-names>Timothy P</given-names> <surname>Lillicrap</surname></string-name>, <string-name><given-names>Philippe</given-names> <surname>Beaudoin</surname></string-name>, <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>Rafal</given-names> <surname>Bogacz</surname></string-name>, <string-name><given-names>Amelia</given-names> <surname>Christensen</surname></string-name>, <string-name><given-names>Claudia</given-names> <surname>Clopath</surname></string-name>, <string-name><given-names>Rui Ponte</given-names> <surname>Costa</surname></string-name>, <string-name><given-names>Archy</given-names> <surname>de Berker</surname></string-name>, <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, <etal>et al.</etal> <article-title>A deep learning framework for neuroscience</article-title>. <source>Nature neuroscience</source>, <volume>22</volume>(<issue>11</issue>):<fpage>1761</fpage>–<lpage>177</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="other"><string-name><given-names>Adam H</given-names> <surname>Marblestone</surname></string-name>, <string-name><given-names>Greg</given-names> <surname>Wayne</surname></string-name>, and <string-name><given-names>Konrad P</given-names> <surname>Kording</surname></string-name>. <article-title>Toward an integration of deep learning and neuroscience</article-title>. <source>Frontiers in computational neuroscience</source>, page <fpage>94</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><string-name><given-names>Andrew</given-names> <surname>Saxe</surname></string-name>, <string-name><given-names>Stephanie</given-names> <surname>Nelli</surname></string-name>, and <string-name><given-names>Christopher</given-names> <surname>Summerfield</surname></string-name>. <article-title>If deep learning is the answer, what is the question?</article-title> <source>Nature Reviews Neuroscience</source>, <volume>22</volume>(<issue>1</issue>):<fpage>55</fpage>–<lpage>6</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><given-names>Francisco M</given-names> <surname>Codoñer</surname></string-name>, <string-name><given-names>José-Antonio</given-names> <surname>Darós</surname></string-name>, <string-name><given-names>Ricard V</given-names> <surname>Solé</surname></string-name>, and <string-name><given-names>Santiago F</given-names> <surname>Elena</surname></string-name>. <article-title>The fittest versus the flattest: experimental confirmation of the quasispecies effect with subviral pathogens</article-title>. <source>PLoS pathogens</source>, <volume>2</volume>(<issue>12</issue>):<fpage>e13</fpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="other"><string-name><given-names>Andrew M</given-names> <surname>Saxe</surname></string-name>, <string-name><given-names>James L</given-names> <surname>McClelland</surname></string-name>, and <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>. <article-title>Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1312.6120</pub-id>, <year>2013</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><given-names>Friedrich</given-names> <surname>Schuessler</surname></string-name>, <string-name><given-names>Francesca</given-names> <surname>Mastrogiuseppe</surname></string-name>, <string-name><given-names>Alexis</given-names> <surname>Dubreuil</surname></string-name>, <string-name><given-names>Srdjan</given-names> <surname>Ostojic</surname></string-name>, and <string-name><given-names>Omri</given-names> <surname>Barak</surname></string-name>. <article-title>The interplay between randomness and structure during learning in rnns</article-title>. <source>Advances in neural information processing systems</source>, <volume>33</volume>:<fpage>13352</fpage>–<lpage>1336</lpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><given-names>Liron</given-names> <surname>Sheintuch</surname></string-name>, <string-name><given-names>Alon</given-names> <surname>Rubin</surname></string-name>, and <string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name>. <article-title>Bias-free estimation of information content in temporally sparse neuronal activity</article-title>. <source>PLoS computational biology</source>, <volume>18</volume>(<issue>2</issue>):<fpage>e100983</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="other"><string-name><given-names>PE</given-names> <surname>Jercog</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>ER</given-names> <surname>Kandel</surname></string-name>. <article-title>Hippocampal CA1 neurons recording from mice foraging in three different environments over 10 days</article-title>. <source>CRCNS.org</source>, <year>2019</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="other"><string-name><given-names>M</given-names> <surname>Karlsson</surname></string-name>, <string-name><given-names>M</given-names> <surname>Carr</surname></string-name>, and <string-name><given-names>LM</given-names> <surname>Frank</surname></string-name>. <article-title>Simultaneous extracellular recordings from hippocampal areas ca1 and ca3 (or mec and ca1) from rats performing an alternation task in two wshapped tracks that are geometrically identically but visually distinct</article-title>. <source>CRCNS</source>. DOI: <pub-id pub-id-type="doi">10.6080/K0NK3BZJ</pub-id>, <year>2015</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="other"><string-name><given-names>Mor Shpigel</given-names> <surname>Nacson</surname></string-name>, <string-name><given-names>Rotem</given-names> <surname>Mulayoff</surname></string-name>, <string-name><given-names>Greg</given-names> <surname>Ongie</surname></string-name>, <string-name><given-names>Tomer</given-names> <surname>Michaeli</surname></string-name>, and <string-name><given-names>Daniel</given-names> <surname>Soudry</surname></string-name>. <article-title>The implicit bias of minima stability in multivariate shallow relu networks</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2306.17499</pub-id>, <year>2023</year>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Supplementary</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><title>Noisy learning leads to spontaneous sparsification.</title>
<p>Summary of 516 simulations with three different learning algorithms: Stochastic error descent (SED, [<xref ref-type="bibr" rid="c38">38</xref>]), Stochastic gradient descent (SGD), Adam. All values are normalized to the first time step of each simulation. The red lines indicate mean over all simulations. (A) Fraction active units – number of units with any response. (B) Active fraction – overall activity across all units (see methods).</p></caption>
<graphic xlink:href="539512v4_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2:</label>
<caption><title>Label and update noise impose different regularizations over the Hessian with distinct signatures in activity statistics.</title>
<p>Summary of 362 simulations with either label or update noise added to SGD learning algorithm. All values are normalized to the first time step of each simulation. Lines indicate the mean of simulations and shaded regions indicate one standard deviation. Loss convergence varies between simulations, and is achieved on a scale of no more than 10<sup>5</sup> time steps. (A) Active fraction as a function of training time. Note this metric decreases significantly for both types of noise. (B) Fraction of active units as a function of training time. For label noise the change is much smaller. (C) Sum of the loss Hessian’s eigenvalues as a function of training time. Here the difference is apparent - label noise imposes slow implicit regularization over this metric while update noise does not. (D) Fraction of non-zero eigenvalues in the loss Hessian as a function of training time. As explained in the main text, update noise imposes implicit regularization over the sum of log-eigenvalues, which manifests as a zeroing of eigenvalues over time and thus a reduction in fraction of active units.</p></caption>
<graphic xlink:href="539512v4_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90069.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peyrache</surname>
<given-names>Adrien</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>McGill University</institution>
</institution-wrap>
<city>Montreal</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a new and <bold>important</bold> theoretical account of spatial representational drift in the hippocampus. The evidence supporting the claims is <bold>convincing</bold>, with a clear and accessible explanation of the phenomenon. Overall, this study will likely attract researchers exploring learning and representation in both biological and artificial neural networks.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90069.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors start from the premise that neural circuits exhibit &quot;representational drift&quot; -- i.e., slow and spontaneous changes in neural tuning despite constant network performance. While the extent to which biological systems exhibit drift is an active area of study and debate (as the authors acknowledge), there is enough interest in this topic to justify the development of theoretical models of drift.</p>
<p>The contribution of this paper is to claim that drift can reflect a mixture of &quot;directed random motion&quot; as well as &quot;steady state null drift.&quot; Thus far, most work within the computational neuroscience literature has focused on the latter. That is, drift is often viewed to be a harmless byproduct of continual learning under noise. In this view, drift does not affect the performance of the circuit nor does it change the nature of the network's solution or representation of the environment. The authors aim to challenge the latter viewpoint by showing that the statistics of neural representations can change (e.g. increase in sparsity) during early stages of drift. Further, they interpret this directed form of drift as &quot;implicit regularization&quot; on the network.</p>
<p>The evidence presented in favor of these claims is concise, but on balance I find their evidence persuasive, at least in artificial network models. This paper includes a brief analysis of four independent experiments in Figure 3, which corroborates the main claims of the paper. Future work should dig deeper into the experimental data to provide a finer grained characterization. For example, in addition to quantifying the overall number of active units, it would be interesting to track changes in the signal-to-noise ratio of each place field, the widths of the place fields, et cetera.</p>
<p>To establish the possibility of implicit regularization in artificial networks, the authors cite convincing work from the machine learning community (Blanc et al. 2020, Li et al., 2021). Here the authors make an important contribution by translating these findings into more biologically plausible models and showing that their core assumptions remain plausible. The authors also develop helpful intuition in Figure 5 by showing a minimal model that captures the essence of their result.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90069.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In the manuscript &quot;Representational drift as a result of implicit regularization&quot; the authors study the phenomenon of representational drift (RD) in the context of an artificial network which is trained in a predictive coding framework. When trained on a task for spatial navigation on a linear track, they found that a stochastic gradient descent algorithm led to a fast initial convergence to spatially tuned units, but then to a second very slow, yet directed drift which sparsified the representation while increasing the spatial information. They finally show that this separation of time-scales is a robust phenomenon and occurs for a number of distinct learning rules.</p>
<p>This is a very clearly written and insightful paper, and I think people in the community will benefit from understanding how RD can emerge in such artificial networks. The mechanism underlying RD in these models is clearly laid out and the explanation given is convincing.</p>
<p>It still remains unclear how this mechanism may account for the learning of multiple environments, although this is perhaps a topic for future study. The non-stationarity of the drift in this framework would seem, at first blush, to contrast with what one sees experimentally, but the authors provide compelling evidence that there are continuous changes in network properties during learning and that stationarity may be the hallmark of overfamiliarized environments. Future experimental work may further shed light on differences in RD between novel and familiar environments.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90069.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Single unit neural activity tuned to environmental or behavioral variables gradually changes over time. This phenomenon, called representational drift, occurs even when all external variables remain constant, and challenges the idea that stable neural activity supports the performance of well-learned behaviors. While a number of studies have described representational drift across multiple brain regions, our understanding of the underlying mechanism driving drift is limited. Ratzon et al. propose that implicit regularization - which occurs when machine learning networks continue to reconfigure after reaching an optimal solution - could provide insights into why and how drift occurs in neurons. To test this theory, Ratzon et al. trained a recurrent neural network (RNN) trained to perform the oft-utilized linear track behavioral paradigm and compare the changes in hidden layer units to those observed in hippocampal place cells recorded in awake, behaving animals.</p>
<p>Ratzon et al. clearly demonstrate that hidden layer units in their model undergo consistent changes even after the task is well-learned, mirroring representational drift observed in real hippocampal neurons. They show that the drift occurs across three separate measures: the active proportion of units (referred to as sparsification), spatial information of units, and correlation of spatial activity. They continue to address the conditions and parameters under which drift occurs in their model to assess the generalizability of their findings to non-spatial tasks. Last, they investigate the mechanism through which sparsification occurs, showing that flatness of the manifold near the solution can influence how the network reconfigures. The authors suggest that their findings indicate a three stage learning process: 1) fast initial learning followed by 2) directed motion along a manifold which transitions to 3) undirected motion along a manifold.</p>
<p>Overall, the authors' results support the main conclusion that implicit regularization in machine learning networks mirrors representational drift observed in hippocampal place cells. Their findings promise to open new fields of inquiry into the connection between machine learning and representational drift in other, non-spatial learning paradigms, and to generate testable predictions for neural data.</p>
<p>Strengths:</p>
<p>(1) Ratzon et al. make an insightful connection between well-known phenomena in two separate fields: implicit regularization in machine learning and representational drift in the brain. They demonstrate that changes in a recurrent neural network mirror those observed in the brain, which opens a number of interesting questions for future investigation.</p>
<p>(2) The authors do an admirable job of writing to a large audience and make efforts to provide examples to make machine learning ideas accessible to a neuroscience audience and vice versa. This is no small feat and aids in broadening the impact of their work.</p>
<p>(3) This paper promises to generate testable hypotheses to examine in real neural data, e.g., that drift rate should plateau over long timescales (now testable with the ability to track single-unit neural activity across long time scales with calcium imaging and flexible silicon probes). Additionally, it provides another set of tools for the neuroscience community at large to use when analyzing the increasingly high-dimensional data sets collected today.</p>
<p>Weaknesses:</p>
<p>The revised manuscript addresses all the weaknesses outlined in my initial review. However, there is one remaining (minor) weakness regarding how &quot;sparseness&quot; is used and defined.</p>
<p>Sparseness can mean different things to different fields. For example, for engram studies, sparseness could be measured at the population level by the proportion of active cells, whereas for a physiology study, sparseness might be measured at the neuron level by the change in peak firing rate of each cell as an animal enters that cell's place field. In this manuscript, the idea of &quot;sparseness&quot; is introduced indirectly in the last paragraph of the introduction as &quot;...changes in activity statistics (sparseness)...&quot;, but it is unclear from the preceding text if the referenced &quot;activity statistics&quot; used to define sparseness are the &quot;fraction of active units,&quot; or their &quot;tuning specificity,&quot; or both. While sparseness is clearly defined in the Methods section for the RNN, there is no mention of how it is defined for neural data, and spatial information is not mentioned at all. For clarity, I suggest explicitly defining sparseness for both the RNN and real neural data early in the main text, e.g. &quot;Here, we measure sparseness in neural data by A and B, and by the analogous metric(s) of X and Y in our RNN...&quot; This is a small but important nuance that will enhance the ease of reading for a broad neuroscience audience.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90069.2.sa4</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ratzon</surname>
<given-names>Aviv</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0000-7648-9744</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Derdikman</surname>
<given-names>Dori</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3677-6321</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Barak</surname>
<given-names>Omri</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7894-6344</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>eLife assessment</bold></p>
<p>This study presents a new and valuable theoretical account of spatial representational drift in the hippocampus. The evidence supporting the claims is convincing, with a clear and accessible explanation of the phenomenon. Overall, this study will likely attract researchers exploring learning and representation in both biological and artificial neural networks.</p>
</disp-quote>
<p>We would like to ask the reviewers to consider elevating the assessment due to the following arguments. As noted in the original review, the study bridges two different fields (machine learning and neuroscience), and does not only touch a single subfield (representational drift in neuroscience). In the revision, we also analysed data from four different labs, strengthening the evidence and the generality of the conclusions.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>The authors start from the premise that neural circuits exhibit &quot;representational drift&quot; -- i.e., slow and spontaneous changes in neural tuning despite constant network performance. While the extent to which biological systems exhibit drift is an active area of study and debate (as the authors acknowledge), there is enough interest in this topic to justify the development of theoretical models of drift.</p>
<p>The contribution of this paper is to claim that drift can reflect a mixture of &quot;directed random motion&quot; as well as &quot;steady state null drift.&quot; Thus far, most work within the computational neuroscience literature has focused on the latter. That is, drift is often viewed to be a harmless byproduct of continual learning under noise. In this view, drift does not affect the performance of the circuit nor does it change the nature of the network's solution or representation of the environment. The authors aim to challenge the latter viewpoint by showing that the statistics of neural representations can change (e.g. increase in sparsity) during early stages of drift. Further, they interpret this directed form of drift as &quot;implicit regularization&quot; on the network.</p>
<p>The evidence presented in favor of these claims is concise. Nevertheless, on balance, I find their evidence persuasive on a theoretical level -- i.e., I am convinced that implicit regularization of noisy learning rules is a feature of most artificial network models. This paper does not seem to make strong claims about real biological systems. The authors do cite circumstantial experimental evidence in line with the expectations of their model (Khatib et al. 2022), but those experimental data are not carefully and quantitatively related to the authors' model.</p>
</disp-quote>
<p>We thank the reviewer for pushing us to present stronger experimental evidence. We now analysed data from four different labs. Two of those are novel analyses of existing data (Karlsson et al, Jercog et al). All datasets show the same trend - increasing sparsity and increasing information per cell. We think that the results, presented in the new figure 3, allow us to make a stronger claim on real biological systems.</p>
<disp-quote content-type="editor-comment">
<p>To establish the possibility of implicit regularization in artificial networks, the authors cite convincing work from the machine-learning community (Blanc et al. 2020, Li et al., 2021). Here the authors make an important contribution by translating these findings into more biologically plausible models and showing that their core assumptions remain plausible. The authors also develop helpful intuition in Figure 4 by showing a minimal model that captures the essence of their result.</p>
</disp-quote>
<p>We are glad that these translation efforts are appreciated.</p>
<disp-quote content-type="editor-comment">
<p>In Figure 2, the authors show a convincing example of the gradual sparsification of tuning curves during the early stages of drift in a model of 1D navigation. However, the evidence presented in Figure 3 could be improved. In particular, 3A shows a histogram displaying the fraction of active units over 1117 simulations. Although there is a spike near zero, a sizeable portion of simulations have greater than 60% active units at the end of the training, and critically the authors do not characterize the time course of the active fraction for every network, so it is difficult to evaluate their claim that &quot;all [networks] demonstrated... [a] phase of directed random motion with the low-loss space.&quot; It would be useful to revise the manuscript to unpack these results more carefully. For example, a histogram of log(tau) computed in panel B on a subset of simulations may be more informative than the current histogram in panel A.</p>
</disp-quote>
<p>The previous figure 3A was indeed confusing. In particular, it lumped together many simulations without proper curation. We redid this figure (now Figure 4), and added supplementary figures (Figures S1, S2) to better explain our results. It is now clear that the simulations with a large number of active units were either due to non-convergence, slow timescale of sparsification or simulations featuring label noise in which the fraction of active units is less affected. Regarding the log(tau) calculation, while it could indeed be an informative plot, it could not be calculated in a simple manner for all simulations. This is because learning curves are not always exponential, but sometimes feature initial plateaus (see also Saxe et al 2013, Schuessler et al 2020). We added a more detailed explanation of this limitation in the methods section, and we believe the current figure exemplifies the effect in a satisfactory manner.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>In the manuscript &quot;Representational drift as a result of implicit regularization&quot; the authors study the phenomenon of representational drift (RD) in the context of an artificial network that is trained in a predictive coding framework. When trained on a task for spatial navigation on a linear track, they found that a stochastic gradient descent algorithm led to a fast initial convergence to spatially tuned units, but then to a second very slow, yet directed drift which sparsified the representation while increasing the spatial information. They finally show that this separation of timescales is a robust phenomenon and occurs for a number of distinct learning rules.</p>
<p>Strengths:</p>
<p>This is a very clearly written and insightful paper, and I think people in the community will benefit from understanding how RD can emerge in such artificial networks. The mechanism underlying RD in these models is clearly laid out and the explanation given is convincing.</p>
</disp-quote>
<p>We thank the reviewer for the support.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>It is unclear how this mechanism may account for the learning of multiple environments.</p>
</disp-quote>
<p>There are two facets to the topic of multiple environments. First, are the results of the current paper relevant when there are multiple environments? Second, what is the interaction between brain mechanisms of dealing with multiple environments and the results of the current paper?</p>
<p>We believe the answer to the first question is positive. The near-orthogonality of representations between environments implies that changes in one can happen without changes in the other. This is evident, for instance, in Khatib et al and Geva et al - in both cases, drift seems to happen independently in two environments, even though they are visited intermittently and are visually similar.</p>
<p>The second question is a fascinating one, and we are planning to pursue it in future work. While the exact way in which the brain achieves this near-independence is an open question, remapping is one possible window into this process.</p>
<p>We extended the discussion to make these points clear.</p>
<disp-quote content-type="editor-comment">
<p>The process of RD through this mechanism also appears highly non-stationary, in contrast to what is seen in familiar environments in the hippocampus, for example.</p>
</disp-quote>
<p>The non-stationarity noted by the reviewer is indeed a major feature of our observations, and is indeed linked to familiarity. We divide learning into three phases (now more clearly stated in Table 1 and Figure 4C). The first, rapid phase, consists of improvement of performance - corresponding to initial familiarity with the environment. The third phase, often reported in the literature of representational drift, is indeed stationary and obtained after prolonged familiarity. Our work focuses on the second phase, which is not as immediate as the first one, and can take several days. We note in the discussion that experiments which include a long familiarization process can miss this phase (see also Table 3). Furthermore, we speculate that real life is less stationary than a lab environment, and this second phase might actually be more relevant there.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>Single-unit neural activity tuned to environmental or behavioral variables gradually changes over time. This phenomenon, called representational drift, occurs even when all external variables remain constant, and challenges the idea that stable neural activity supports the performance of well-learned behaviors. While a number of studies have described representational drift across multiple brain regions, our understanding of the underlying mechanism driving drift is limited. Ratzon et al. propose that implicit regularization - which occurs when machine learning networks continue to reconfigure after reaching an optimal solution - could provide insights into why and how drift occurs in neurons. To test this theory, Ratzon et al. trained a Feedforward Network trained to perform the oft-utilized linear track behavioral paradigm and compare the changes in hidden layer units to those observed in hippocampal place cells recorded in awake, behaving animals.</p>
<p>Ratzon et al. clearly demonstrate that hidden layer units in their model undergo consistent changes even after the task is well-learned, mirroring representational drift observed in real hippocampal neurons. They show that the drift occurs across three separate measures: the active proportion of units (referred to as sparsification), spatial information of units, and correlation of spatial activity. They continue to address the conditions and parameters under which drift occurs in their model to assess the generalizability of their findings.</p>
<p>However, the generalizability results are presented primarily in written form: additional figures are warranted to aid in reproducibility.</p>
</disp-quote>
<p>We added figures, and a Github with all the code to allow full reproducibility.</p>
<disp-quote content-type="editor-comment">
<p>Last, they investigate the mechanism through which sparsification occurs, showing that the flatness of the manifold near the solution can influence how the network reconfigures. The authors suggest that their findings indicate a three-stage learning process: 1) fast initial learning followed by 2) directed motion along a manifold which transitions to 3) undirected motion along a manifold.</p>
<p>Overall, the authors' results support the main conclusion that implicit regularization in machine learning networks mirrors representational drift observed in hippocampal place cells.</p>
</disp-quote>
<p>We thank the reviewer for this summary.</p>
<disp-quote content-type="editor-comment">
<p>However, additional figures/analyses are needed to clearly demonstrate how different parameters used in their model qualitatively and quantitatively influence drift.</p>
</disp-quote>
<p>We now provide additional figures regarding parameters (Figures S1, S2).</p>
<disp-quote content-type="editor-comment">
<p>Finally, the authors need to clearly identify how their data supports the three-stage learning model they suggest.</p>
<p>Their findings promise to open new fields of inquiry into the connection between machine learning and representational drift and generate testable predictions for neural data.</p>
<p>Strengths:</p>
<p>(1) Ratzon et al. make an insightful connection between well-known phenomena in two separate fields: implicit regularization in machine learning and representational drift in the brain. They demonstrate that changes in a recurrent neural network mirror those observed in the brain, which opens a number of interesting questions for future investigation.</p>
<p>(2) The authors do an admirable job of writing to a large audience and make efforts to provide examples to make machine learning ideas accessible to a neuroscience audience and vice versa. This is no small feat and aids in broadening the impact of their work.</p>
<p>(3) This paper promises to generate testable hypotheses to examine in real neural data, e.g., that drift rate should plateau over long timescales (now testable with the ability to track single-unit neural activity across long time scales with calcium imaging and flexible silicon probes). Additionally, it provides another set of tools for the neuroscience community at large to use when analyzing the increasingly high-dimensional data sets collected today.</p>
</disp-quote>
<p>We thank the reviewer for these comments. Regarding the hypotheses, these are partially confirmed in the new analyses we provide of data from multiple labs (new Figure 3 and Table 3) - indicating that prolonged exposure to the environment leads to more stationarity.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) Neural representational drift and directed/undirected random walks along a manifold in ML are well described. However, outside of the first section of the main text, the analysis focuses primarily on the connection between manifold exploration and sparsification without addressing the other two drift metrics: spatial information and place field correlations. It is therefore unclear if the results from Figures 3 and 4 are specific to sparseness or extend to the other two metrics. For example, are these other metrics of drift also insensitive to most of the Feedforward Network parameters as shown in Figure 3 and the related text? These concerns could be addressed with panels analogous to Figures 3a-c and 4b for the other metrics and will increase the reproducibility of this work.</p>
</disp-quote>
<p>We note that the results from figures 3 and 4 (original manuscript) are based on abstract tasks, while in figure 2 there is a contextual notion of spatial position. Spatial position metrics are not applicable to the abstract tasks as they are simple random mapping of inputs, and there isn’t necessarily an underlying latent variable such as position. This transition between task types is better explained in the text now. In essence the spatial information and place field correlation changes are simply signatures of the movements in parameter space. In the abstract tasks their change becomes trivial, as the spatial information becomes strongly correlated with sparsity and place fields are simply the activity vectors of units. These are guaranteed to change as long as there are changes in the activity statistics. We present here the calculation of these metrics averaged over simulations for completeness.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<caption>
<title>PV correlation between training time points averaged over 362 simulations.</title>
<p>(B) Mean SI of units normalized to first time step, averaged over 362 simulations. Red line shows the average time point of loss convergence, the shaded area represents one standard deviation.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-90069-sa4-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>(2) Many caveats/exceptions to the generality of findings are mentioned only in the main text without any supporting figures, e.g., &quot;For label noise, the dynamics were qualitatively different, the fraction of active units did not reduce, but the activity of the units did sparsify&quot; (lines 116-117). Supporting figures are warranted to illustrate which findings are &quot;qualitatively different&quot; from the main model, which are not different from the main model, and which of the many parameters mentioned are important for reproducing the findings.</p>
</disp-quote>
<p>We now added figures (S1, S2) that show this exactly. We also added a github to allow full reproduction.</p>
<disp-quote content-type="editor-comment">
<p>(3) Key details of the model used by the authors are not listed in the methods. While they are mentioned in reference 30 (Recanatesi et al., 2021), they need to be explicitly defined in the methods section to ensure future reproducibility.</p>
</disp-quote>
<p>The details of the simulation are detailed in the methods sections. We also added a github to allow full reproducibility.</p>
<disp-quote content-type="editor-comment">
<p>(4) How different states of drift correspond to the three learning stages outlined by the authors is unclear. Specifically, it is not clear where the second stage ends, and the third stage begins, either in real neural data or in the figures. This is compounded by the fact that the third stage - of undirected, random manifold exploration - is only discussed in relation to the introductory Figure 1 and is never connected to the neural network data or actual brain data presented by the authors. Are both stages meant to represent drift? Or is only the second stage meant to mirror drift, while undirected random motion along a manifold is a prediction that could be tested in real neural data? Identifying where each stage occurs in Figures 2C and E, for example, would clearly illustrate which attributes of drift in hidden layer neurons and real hippocampal neurons correspond to each stage.</p>
</disp-quote>
<p>Thanks for this comment, which urged us to better explain these concepts.</p>
<p>The different processes (reduction in loss, reduction in Hessian) happen in parallel with different timescales. Thus, there are no sharp transitions between the phases. This is now explained in the text in relation to figure 4C, where the approximate boundaries are depicted.</p>
<p>The term drift is often used to denote a change in representation without a change in behavior. In this sense, both the second and third phases correspond to drift. Only the third stage is stationary. This is now emphasized in the text and in the new Table 1. Regarding experimental data, apart from the new figure 3 with four datasets, we also summarize in Table 3 the relation between duration of familiarity and stationarity of the data.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p>The reviewers have raised several concerns. They concur that the authors should address the specific points below to enhance the manuscript.</p>
<p>(1) The three different phases of learning should be clearly delineated, along with how they are determined. It remains unclear in which exact phase the drift is observed.</p>
</disp-quote>
<p>This is now clearly explained in the new Table 1 and Figure 4C. Note that the different processes (reduction in loss, reduction in Hessian) happen in parallel with different timescales. Thus, there are no sharp transitions between the phases. This is now explained in the text in relation to figure 4C, where the approximate boundaries are depicted.</p>
<p>The term drift is often used to denote a change in representation without a change in behavior. In this sense, both the second and third phases correspond to drift. Only the third stage is stationary. This is now emphasized in the text and in the new Table 1. Regarding experimental data, apart from the new figure 3 with four datasets, we also summarize in Table 3 the relation between duration of familiarity and stationarity of the data.</p>
<disp-quote content-type="editor-comment">
<p>(2) The term &quot;sparsification&quot; of unit activity is not fully clear. Its meaning should be more explicitly explained, especially since, in the simulations, a significant number of units appear to remain active (Fig. 3A).</p>
</disp-quote>
<p>We now define precisely the two measures we use - Active Fraction, and Fraction Active Units. There is a new section with an accompanying figure in the Methods section. As Figure S2 shows, the noise statistics (label noise vs. update noise) differentially affects these two measures.</p>
<disp-quote content-type="editor-comment">
<p>(3) While the study primarily focuses on one aspect of representational drift-the proportion of active units-it should also explore other features traditionally associated with representational drift, such as spatial information and the correlation between place fields.</p>
</disp-quote>
<p>This absence of features is related to the abstract nature of some of the tasks simulated in our paper. In our original submission the transition between a predictive coding task to more abstract tasks was not clearly explained, creating some confusion regarding the measured metrics. We now clarified the motivation for this transition.</p>
<p>Both the initial simulation and the new experimental data analysis include spatial information (Figures 2,3). The following simulations (Figure 4) with many parameter choices use more abstract tasks, for which the notion of correlation between place cells and spatial information loses its meaning as there is no spatial ordering of the inputs, and every input is encountered only once. Spatial information becomes strongly correlated with the inverse of the active fraction metric. The correlation between place cells is also directly linked to increase in sparseness for these tasks.</p>
<disp-quote content-type="editor-comment">
<p>(4) There should be a clearer illustration of how labeling noise influences learning dynamics and sparsification.</p>
</disp-quote>
<p>This was indeed confusing in the original submission. We removed the simulations with label noise from Figure 4, and added a supplementary figure (S2) illustrating the different effects of label noise.</p>
<disp-quote content-type="editor-comment">
<p>(5) The representational drift observed in this study's simulations appears to be nonstationary, which differs from in vivo reports. The reasons for this discrepancy should be clarified.</p>
</disp-quote>
<p>We added experimental results from three additional labs demonstrating a change in activity statistics (i.e. increase in spatial information and increase in sparseness) over a long period of time. We suggest that such a change long after the environment is already familiar is an indication for the second phase, and stress that this change seems to saturate at some point, and that most drift papers start collecting data after this saturation, hence this effect was missed in previous in vivo reports. Furthermore, these effects are become more abundant with the advent on new calcium imaging methods, as the older electrophysiological regording methods did not usually allow recording of large amounts of cells for long periods of time.
The new Table 3 surveys several experimental papers, emphasizing the degree of familiarity with the environment.</p>
<disp-quote content-type="editor-comment">
<p>(6) A distinctive feature of the hippocampus is its ability to learn different spatial representations for various environments. The study does not test representational drift in this context, a topic of significant interest to the community. Whether the authors choose to delve into this is up to them, but it should at least be discussed more comprehensively, as it's only briefly touched upon in the current manuscript version.</p>
</disp-quote>
<p>There are two facets to the topic of multiple environments. First, are the results of the current paper relevant when there are multiple environments? Second, what is the interaction between brain mechanisms of dealing with multiple environments and the results of the current paper?</p>
<p>We believe the answer to the first question is positive. The near-orthogonality of representations between environments implies that changes in one can happen without changes in the other. This is evident, for instance, in Khatib et al and Geva et al - in both cases, drift seems to happen independently in two environments, even though they are visited intermittently and are visually similar.</p>
<p>The second question is a fascinating one, and we are planning to pursue it in future work. While the exact way in which the brain achieves this near-independence is an open question, remapping is one possible window into this process.</p>
<p>We extended the discussion to make these points clear.</p>
<disp-quote content-type="editor-comment">
<p>(7) The methods section should offer more details about the neural nets employed in the study. The manuscript should be explicit about the terms &quot;hidden layer&quot;, &quot;units&quot;, and &quot;neurons&quot;, ensuring they are defined clearly and not used interchangeably..</p>
</disp-quote>
<p>We changed the usage of these terms to be more coherent and made our code publicly available. Specifically, “units” refer to artificial networks and “neurons” to biological ones.</p>
<disp-quote content-type="editor-comment">
<p>In addition, each reviewer has raised both major and minor concerns. These are listed below and should be addressed where possible.</p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>I recommend that the authors edit the text to soften their claims. For example:</p>
<p>In the abstract &quot;To uncover the underlying mechanism, we...&quot; could be changed to &quot;To investigate, we...&quot;</p>
</disp-quote>
<p>Agree. Done</p>
<disp-quote content-type="editor-comment">
<p>On line 21, &quot;Specifically, recent studies showed that...&quot; could be changed to &quot;Specifically, recent studies suggest that...&quot;</p>
</disp-quote>
<p>Agree. Done</p>
<disp-quote content-type="editor-comment">
<p>On line 100, &quot;All cases&quot; should probably be softened to &quot;Most cases&quot; or more details should be added to Figure 3 to support the claim that every simulation truly had a phase of directed random motion.</p>
</disp-quote>
<p>The text was changed in accordance with the reviewer’s suggestion. In addition, the figure was changed and only includes simulations in which we expected unit sparsity to arise (without label noise). We also added explanations and supplementary figures for label noise.</p>
<disp-quote content-type="editor-comment">
<p>Unless I missed something obvious, there is no new experimental data analysis reported in the paper. Thus, line 159 of the discussion, &quot;a phenomenon we also observed in experimental data&quot; should be changed to &quot;a phenomenon that recently reported in experimental data.&quot;</p>
</disp-quote>
<p>We thank the reviewer for drawing our attention to this. We now analyzed data from three other labs, two of which are novel analyses on existing data. All four datasets show the same trends of sparseness with increasing spatial information. The new Figure 3 and text now describe this.</p>
<disp-quote content-type="editor-comment">
<p>On line 179 of the Discussion, &quot;a family of network configurations that have identical performance...&quot; could be softened to &quot;nearly identical performance.&quot; It would be possible for networks to have minuscule differences in performance that are not detected due to stochastic batch effects or limits on machine precision.</p>
</disp-quote>
<p>The text was changed in accordance with the reviewer’s suggestion.</p>
<disp-quote content-type="editor-comment">
<p>Other minor comments:</p>
<p>Citation 44 is missing the conference venue, please check all citations are formatted properly.</p>
</disp-quote>
<p>Corrected.</p>
<disp-quote content-type="editor-comment">
<p>In the discussion on line 184, the connection to remapping was confusing to me, particularly because the cited reference (Sanders et al. 2020) is more of a conceptual model than an artificial network model that could be adapted to the setting of noisy learning considered in this paper. How would an RNN model of remapping (e.g. Low et al. 2023; Remapping in a recurrent neural network model of navigation and context inference) be expected to behave during the sparsifying portion of drift?</p>
</disp-quote>
<p>We now clarified this section. The conceptual model of Sanders et al includes a specific prediction (Figure 7 there) which is very similar to ours - a systematic change in robustness depending on duration of training. Regarding the Low et al model, using such mechanistic models is an exciting avenue for future research.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>I only have two major questions.</p>
<p>(1) Learning multiple representations: Memory systems in the brain typically must store many distinct memories. Certainly, the hippocampus, where RD is prominent, is involved in the ongoing storage of episodic memories. But even in the idealized case of just two spatial memories, for example, two distinct linear tracks, how would this learning process look? Would there be any interference between the two learning processes or would they be largely independent? Is the separation of time scales robust to the number of representations stored? I understand that to answer this question fully probably requires a research effort that goes well beyond the current study, but perhaps an example could be shown with two environments. At the very least the authors could express their thoughts on the matter.</p>
</disp-quote>
<p>There are two facets to the topic of multiple environments. First, are the results of the current paper relevant when there are multiple environments? Second, what is the interaction between brain mechanisms of dealing with multiple environments and the results of the current paper?</p>
<p>We believe the answer to the first question is positive. The near-orthogonality of representations between environments implies that changes in one can happen without changes in the other. This is evident, for instance, in Khatib et al and Geva et al - in both cases, drift seems to happen independently in two environments, even though they are visited intermittently and are visually similar.</p>
<p>The second question is a fascinating one, and we are planning to pursue it in future work. While the exact way in which the brain achieves this near-independence is an open question, remapping is one possible window into this process.</p>
<p>We extended the discussion to make these points clear.</p>
<disp-quote content-type="editor-comment">
<p>(2) Directed drift versus stationarity: I could not help but notice that the RD illustrated in Fig.2D is not stationary in nature, i.e. the upper right and lower left panels are quite different. This appears to contrast with findings in the hippocampus, for example, Fig.3e-g in (Ziv et al, 2013). Perhaps it is obvious that a directed process will not be stationary, but the authors note that there is a third phase of steady-state null drift. Is the RD seen there stationary? Basically, I wonder if the process the authors are studying is relevant only as a novel environment becomes familiar, or if it is also applicable to RD in an already familiar environment. Please discuss the issue of stationarity in this context.</p>
</disp-quote>
<p>The non-stationarity noted by the reviewer is indeed a major feature of our observations, and is indeed linked to familiarity. We divide learning into three phases (now more clearly stated in Table 1 and Figure 4C). The first, rapid, phase consists of improvement of performance - corresponding to initial familiarity with the environment. The third phase, often reported in the literature of representational drift, is indeed stationary and obtained after prolonged familiarity. Our work focuses on the second phase, which is not as immediate as the first one, and can take several days. We note in the discussion that experiments which include a long familiarization process can miss this phase (see also Table 3). Furthermore, we speculate that real life is less stationary than a lab environment, and this second phase might actually be more relevant there.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>Most of my general recommendations are outlined in the public review. A large portion of my comments regards increasing clarity and explicitly defining many of the terms used which may require generating more figures (to better illustrate the generality of findings) or modifying existing figures (e.g., to show how/where the three stages of learning map onto the authors' data).</p>
<p>Sparsification is not clearly defined in the main text. As I read it, sparsification is meant to refer to the activity of neurons, but this needs to be clearly defined. For example, lines 262-263 in the methods define &quot;sparseness&quot; by the number of active units, but lines 116-117 state: &quot;For label noise, the dynamics were qualitatively different, the fraction of active units did not reduce, but the activity of the units did sparsify.&quot; If the fraction of active units (defined as &quot;sparseness&quot;) did not change, what does it mean that the activity of the units &quot;sparsified&quot;? If the authors mean that the spatial activity patterns of hidden units became more sharply tuned, this should be clearly stated.</p>
</disp-quote>
<p>We now defined precisely the two measures we use - Active Fraction, and Fraction Active Units. There is a new section with an accompanying figure in the Methods section. As Figure S2 shows, the noise statistics (label noise vs. update noise) differentially affects these two measures.</p>
<disp-quote content-type="editor-comment">
<p>Likewise, it is unclear which of the features the authors outlined - spatial information, active proportion of units, and spatial correlation - are meant to represent drift. The authors should clearly delineate which of these three metrics they mean to delineate drift in the main text rather than leave it to the reader to infer. While all three are mentioned early on in the text (Figure 2), the authors focus more on sparseness in the last half of the text, making it unclear if it is just sparseness that the authors mean to represent drift or the other metrics as well.</p>
</disp-quote>
<p>The main focus of our paper is on the non-stationarity of drift. Namely that features (such as these three) systematically change in a directed manner as part of the drift process. This is in The new analyses of experimental data show sparseness and spatial information.</p>
<p>The focus on sparseness in the second half of the paper is because we move to more abstract These are also easy to study in the more abstract tasks in the second part of the paper.  In our original submission the transition between a predictive coding task to more abstract tasks was not clearly explained, creating some confusion regarding the measured metrics. We now clarified the motivation for this transition.</p>
<disp-quote content-type="editor-comment">
<p>It is not clear if a change in the number of active units alone constitutes &quot;drift&quot;, especially since Geva et al. (2023) recently showed that both changes in firing rate AND place field location drive drift, and that the passage of time drives changes in activity rate (or # cells active).</p>
</disp-quote>
<p>Our work did not deal with purely time-dependent drift, but rather focused on experience-dependence. Furthermore, Geva et al study the stationary phase of drift, where we do not expect a systematic change in the total number of cells active. They report changes in the average firing rate of active cells in this phase, as a function of time - which does not contradict our findings.</p>
<disp-quote content-type="editor-comment">
<p>&quot;hidden layer&quot;, &quot;units&quot;, and &quot;neurons&quot; seem to be used interchangeably in the text (e.g., line 81-85). However, this is confusing in several places, in particular in lines 83-85 where &quot;neurons&quot; is used twice. The first usage appears to refer to the rate maps of the hidden layer units simulated by the authors, while the second &quot;neurons&quot; appears to refer to real data from Ziv 2013 (ref 5). The authors should make it explicit whether they are referring to hidden layer units or actual neurons to avoid reader confusion.</p>
</disp-quote>
<p>We changed the usage of these terms to be more coherent. Specifically, “units” refer to artificial networks and “neurons” to biological ones.</p>
<disp-quote content-type="editor-comment">
<p>The authors should clearly illustrate which parts of their findings support their three-phase learning theory. For example, does 2E illustrate these phases, with the first tenth of training time points illustrating the early phase, time 0.1-0.4 illustrating the intermediate phase, and 0.4-1 illustrating the last phase? Additionally, they should clarify whether the second and third stages are meant to represent drift, or is it only the second stage of directed manifold exploration that is considered to represent drift? This is unclear from the main text.</p>
</disp-quote>
<p>The different processes (reduction in loss, reduction in Hessian) happen in parallel with different timescales. Thus, there are no sharp transitions between the phases. This is now explained in the text in relation to figure 4C, where the approximate boundaries are depicted.</p>
<p>The term drift is often used to denote a change in representation without a change in behavior. In this sense, both the second and third phases correspond to drift. Only the third stage is stationary. This is now emphasized in the text and in the new Table 1. Regarding experimental data, apart from the new figure 3 with four datasets, we also summarize in Table 3 the relation between duration of familiarity and stationarity of the data.</p>
<disp-quote content-type="editor-comment">
<p>Line 45 - It appears that the acronym ML is not defined above here anywhere.</p>
</disp-quote>
<p>Added.</p>
<disp-quote content-type="editor-comment">
<p>Line 71: the ReLU function should be defined in the text, e.g., sigma(x) = x if x &gt; 0 else 0.</p>
</disp-quote>
<p>Added.</p>
<disp-quote content-type="editor-comment">
<p>106-107: Figures (or supplemental figures) to demonstrate how most parameters do not influence sparsification dynamics are warranted. As written, it is unclear what &quot;most parameters&quot; mean - all but noise scale. What about the learning rule? Are there any interactions between parameters?</p>
</disp-quote>
<p>We now removed the label noise from Figure 4, and added two supplementary figures to clearly explain the effect of parameters. Figure 4 itself was also redone to clarify this issue.</p>
<disp-quote content-type="editor-comment">
<p>2F middle: should &quot;change&quot; be omitted for SI?</p>
</disp-quote>
<p>The panel was replaced by a new one in Figure 3.</p>
<disp-quote content-type="editor-comment">
<p>116-119: A figure showing how results differ for label noise is warranted.</p>
</disp-quote>
<p>This is now done in Figure S1, S2.</p>
<disp-quote content-type="editor-comment">
<p>124: typo, The -&gt; the</p>
</disp-quote>
<p>Corrected.</p>
<disp-quote content-type="editor-comment">
<p>127-129: This conclusion statement is the first place in the text where the three stages are explicitly outlined. There does not appear to be any support or further explanation of these stages in the text above.</p>
</disp-quote>
<p>We now explain this earlier at the end of the Introduction section, along with the new Table 1 and marking on Figure 4C.</p>
<disp-quote content-type="editor-comment">
<p>132-133 seems to be more of a statement and less of a prediction or conclusion - do the authors mean &quot;the flatness of the loss landscape in the vicinity of the solution predicts the rate of sparsification?&quot;</p>
</disp-quote>
<p>We thank the reviewer for this observation. The sentence was rephrased:</p>
<p>Old: As illustrated in Fig. 1, different solutions in the zero-loss manifold might vary in some of their properties. The specific property suggested from theory is the flatness of the loss landscape in the vicinity of the solution.</p>
<p>New: As illustrated in Fig. 1, solutions in the zero-loss manifold have identical loss, but might vary in some of their properties. The authors of [26] suggest that noisy learning will slowly increase the flatness of the loss landscape in the vicinity of the solution.</p>
<disp-quote content-type="editor-comment">
<p>135: typo, it's -&gt; its</p>
</disp-quote>
<p>Corrected.</p>
<disp-quote content-type="editor-comment">
<p>Line 135-136 &quot;Crucially, the loss on the 136 entire manifold is exactly zero...&quot; This appears to contradict the Figure 4A legend - the loss appears to be very high near the top and bottom edges of the manifold in 4A. Do the authors mean that the loss along the horizontal axis of the manifold is zero?</p>
</disp-quote>
<p>The reviewer is correct. The manifold mentioned in the sentence is indeed the horizontal axis. We changed the text and the figure to make it clearer.</p>
<disp-quote content-type="editor-comment">
<p>Equation 6: This does not appear to agree with equation 2 - should there be an E_t term for an expectation function?</p>
</disp-quote>
<p>Corrected.</p>
<disp-quote content-type="editor-comment">
<p>Line 262-263: &quot;Sparseness means that a unit has become inactive for all inputs.&quot; This should also be stated explicitly as the definition of sparseness/sparsification in the main text.</p>
</disp-quote>
<p>We now define precisely the two measures we use - Active Fraction, and Fraction Active Units. There is a new section with an accompanying figure in the Methods section. As Figure S2 shows, the noise statistics (label noise vs. update noise) differentially affects these two measures.</p>
</body>
</sub-article>
</article>