<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97107</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97107</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97107.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A dynamic generative model can extract interpretable oscillatory components from multichannel neurophysiological recordings</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8807-042X</contrib-id>
<name>
<surname>Das</surname>
<given-names>Proloy</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>He</surname>
<given-names>Mingjian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Purdon</surname>
<given-names>Patrick L</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Anesthesiology, Perioperative and Pain Medicine, Stanford University</institution>, Stanford, CA 94305</aff>
<aff id="a2"><label>2</label><institution>Department of Bioengineering, Stanford University</institution>, Stanford, CA 94305</aff>
<aff id="a3"><label>3</label><institution>Department of Psychology, Stanford University</institution>, Stanford, CA 94305</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><email>proloy@stanford.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-04-18">
<day>18</day>
<month>04</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-07-12">
<day>12</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97107</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-02-29">
<day>29</day>
<month>02</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-02-08">
<day>08</day>
<month>02</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.07.26.550594"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-04-18">
<day>18</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97107.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.97107.1.sa1">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.97107.1.sa0">Reviewer #1 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Das et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Das et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97107-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Modern neurophysiological recordings are performed using multichannel sensor arrays that are able to record activity in an increasingly high number of channels numbering in the 100’s to 1000’s. Often, underlying lower-dimensional patterns of activity are responsible for the observed dynamics, but these representations are difficult to reliably identify using existing methods that attempt to summarize multivariate relationships in a post-hoc manner from univariate analyses, or using current blind source separation methods. While such methods can reveal appealing patterns of activity, determining the number of components to include, assessing their statistical significance, and interpreting them requires extensive manual intervention and subjective judgement in practice. These difficulties with component selection and interpretation occur in large part because these methods lack a generative model for the underlying spatio-temporal dynamics. Here we describe a novel component analysis method anchored by a generative model where each source is described by a bio-physically inspired state space representation. The parameters governing this representation readily capture the oscillatory temporal dynamics of the components, so we refer to it as Oscillation Component Analysis (OCA). These parameters – the oscillatory properties, the component mixing weights at the sensors, and the number of oscillations – all are inferred in a data-driven fashion within a Bayesian framework employing an instance of the expectation maximization algorithm. We analyze high-dimensional electroencephalography and magnetoencephalography recordings from human studies to illustrate the potential utility of this method for neuroscience data.</p>
</abstract>
<abstract abstract-type="teaser">
<title>Significance Statement</title>
<p>Neuroscience studies often involve simultaneous recordings in a large number of sensors in which a smaller number of dynamic components generate the complex spatio-temporal patterns observed in the data. Current blind source separation techniques produce sub-optimal results and are difficult to interpret because these methods lack an appropriate generative model that can guide both statistical inference and interpretation. Here we describe a novel component analysis method employing a dynamic generative model that can decompose high-dimensional multivariate data into a smaller set of oscillatory components are learned in a data-driven way, with parameters that are immediately interpretable. We show how this method can be applied to neurophysiological recordings with millisecond precision that exhibit oscillatory activity such as electroencephalography and magnetoencephalography.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Cortical Oscillation</kwd>
<kwd>Source separation</kwd>
<kwd>Component analysis</kwd>
<kwd>Dynamical models</kwd>
<kwd>State space</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Expanded Results and Discussion section following the suggestion of eLife reviewers.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Human neurophysiological recordings such as scalp electroencephalogram (EEG), magnetoencephalo-gram (MEG), stereoelectroencephalogram (SEEG), Local field potentials (LFP) etc. consist of <italic>∼</italic> 10<sup>2</sup> of sensors that record mixtures of predominantly cortical network oscillations [<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c5">5</xref>]. The network oscillations have distinct spatio-temporal signatures, based on the functional brain areas involved, their interconnections, and the electromagnetic mapping between the source currents and the sensors. However, the source-to-sensor mixing and the further superposition of measurement noise complicates the interpretation of the sensor-level data and its topography [<xref ref-type="bibr" rid="c6">6</xref>]. Given the widespread and growing availability of high-density neural recording technologies [<xref ref-type="bibr" rid="c7">7</xref>], there is clearly a pressing need for analysis tools that can recover underlying dynamic components from highly multivariate data.</p>
<p>This problem fits within a larger class of blind source separation (BSS) problems for which there are a plethora of component analysis algorithms that attempt to extract underlying source activity as linear weighted combinations of the sensor activity. The decomposition weights are designed according to some predefined criterion on the extracted time-series depending on the application. Independent Component Analysis (ICA) [<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c11">11</xref>]) has been particularly popular within neuroscience for a number of reasons. It requires no assumption of the original sources except for statistical independence, i.e., no or minimal mutual information between the components [<xref ref-type="bibr" rid="c12">12</xref>], and in principle requires little to no user intervention to run. However, a drawback of the method is that it assumes that the samples of each component time trace are independent and identically distributed, which is not generally true in physical or physiological applications. This leads to another major drawback of ICA: it relies on the cumulative histograms of sensor recordings and if these histograms are Gaussian, ICA is in principle unable to separate such sources [<xref ref-type="bibr" rid="c13">13</xref>]. This includes, for example, physiological signals such as EEG, MEG, sEEG etc. that are at least approximately Gaussian distributed due to the central limit theorem [<xref ref-type="bibr" rid="c14">14</xref>], since these signals reflect the linear combination of many sources of activity. Finally, given the lack of assumptions on the structure of the underlying signals, there is no guarantee that ICA will extract relevant oscillatory components, or any other highly structured dynamic pattern for that matter.</p>
<p>Given the prominence and ubiquity of oscillations in neurophysiological data, applications of component analysis methods in neuroscience have taken a different approach to emphasize oscillatory dynamics, employing canonical correlation analysis [<xref ref-type="bibr" rid="c15">15</xref>], generalized eigenvalue decomposition [<xref ref-type="bibr" rid="c16">16</xref>], joint decorrelation [<xref ref-type="bibr" rid="c17">17</xref>], or similar methods to identify a set of spatial weights to maximize the signal-to-noise ratio in the extracted component within a narrow band around a given frequency of interest[<xref ref-type="bibr" rid="c18">18</xref>–<xref ref-type="bibr" rid="c21">21</xref>]. The extracted component then inherits the intrinsic oscillatory dynamics around that frequency without the need for a pre-designed narrow-band filter [<xref ref-type="bibr" rid="c22">22</xref>]. These techniques do acknowledge the inherent temporal dynamics of the oscillatory sources, but do so via nonparametric sample correlation or cross-spectrum matrix estimates that are sensitive to noise or artifacts and that require substantial amounts of data to achieve consistency.</p>
<p>Another problem common to all of these component decomposition methods is that they do not directly estimate the source to sensor mixing matrix. Instead, they estimate spatial filters that extract the independent components, which are not directly interpretable. The source to sensor mixing matrix can only be estimated by solving an inverse problem that requires knowledge of the noise covariance matrix [<xref ref-type="bibr" rid="c23">23</xref>], which adds another complication and source of potential error. Finally, the properties of the extracted component time courses are not known a priori and must be assessed after the fact, typically by using nonparametric tests as well as visual inspection.</p>
<p>These difficulties with component selection and interpretation occur in large part because existing blind source separation methods lack a generative model for the underlying spatio-temporal dynamics. With a probabilistic generative model, it is possible to specify a soft constraint on the dynamic properties of the underlying components, while maintaining other desirable properties such as independence between components. Component selection can be handled automatically within a statistical framework under the model, and interpretation is straightforward in principle if the components can be described by a small number of parameters. Here we propose a novel component analysis method that uses a clever state space model [<xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c26">26</xref>] to efficiently represent oscillatory dynamics in terms of latent analytic signals [<xref ref-type="bibr" rid="c27">27</xref>] consisting of both the real and imaginary components of the oscillation. The observed data are then represented as a superposition of these latent oscillations, each weighted by a multi-channel mixing matrix that describes the spatial signature of the oscillation. We estimate the parameters of the model and the mixing matrices using Generalized Expectation Maximization and employ empirical Bayes model selection to objectively determine the number of components. In these ways we address the major shortcomings described above for many component analysis methods. We refer to our novel method as Oscillation Component Analysis (OCA) akin to Independent Component Analysis (ICA). In what follows we describe the model formulation in detail and demonstrate the performance of the method on simulated and experimental data sets including high-density EEG during propofol anesthesia and sleep, as well as resting-state MEG from the Human Connectome Project.</p>
</sec>
<sec id="s2">
<title>Theory</title>
<sec id="s2a">
<title>State space oscillator model</title>
<p>Oscillatory time-series can be described using the following state space representation [<xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c26">26</xref>]:
<disp-formula id="eqn1">
<graphic xlink:href="550594v4_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the the oscillation state, <bold>x</bold><italic><sub>n</sub></italic> = [<italic>x<sub>n,</sub></italic><sub>1</sub><italic>, x<sub>n,</sub></italic><sub>2</sub>]<italic><sup>⊤</sup></italic> is a two dimensional state vector. The stochastic difference equation summarizes oscillatory dynamics using random rotation in two-dimensional state space through a deterministic rotation matrix, <italic>R</italic>, explicitly parameterized by the oscillation frequency, <italic>f</italic> , the sampling rate, <italic>f<sub>s</sub></italic>, and the damping factor, <italic>a</italic> as,
<disp-formula id="eqn2">
<graphic xlink:href="550594v4_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and a stochastic driving noise, assumed to be stationary and Gaussian with variance <italic>σ</italic><sup>2</sup>. These elements of this state vector trace out two time-series that maintains an approximate <italic>π/</italic>2 radian phase difference, and therefore are closely related to the real and imaginary parts of an analytic signal [<xref ref-type="bibr" rid="c27">27</xref>] in the complex plane (see SI Text: Section Oscillation states and analytic signals ). The oscillation states are henceforth called analytic signal with minor abuse of notation. An arbitrary fixed projection (i.e., on the real line) of the state vector realizations generates the observed noisy oscillation time-series at the sensor. Multiple oscillations can be readily incorporated in this state space model by simply considering their linear combination. Recently, several investigators [<xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c30">30</xref>] have utilized this state space representation to extract underlying oscillatory time courses from single channel EEG time traces.</p>
</sec>
<sec id="s2b">
<title>Generalization to multichannel data</title>
<p>In order to represent multichannel neural recordings, we employ this oscillatory state space representation within a blind source separation model [<xref ref-type="bibr" rid="c31">31</xref>]. In the proposed generative model, a sensor-array with <italic>L</italic> sensors records neurophysiological signals produced by superimposition of <italic>M</italic> distinct oscillations supported by underlying brain networks or circuits, which we will refer to as <italic>oscillation sources</italic>.
<disp-formula id="eqn3">
<graphic xlink:href="550594v4_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Each oscillation source is governed by the above-mentioned state space representation, resulting in a structured 2<italic>M</italic> dimensional state space. The structure of the underlying brain networks or circuits governs how each oscillation source is observed at the sensor array, via a <italic>M ×</italic> 2 spatial distribution matrix, <bold>c</bold><italic><sub>∗,m</sub></italic> = [<bold>c</bold><sub>1<italic>,m</italic></sub>, <bold>c</bold><sub>2<italic>,m</italic></sub><italic>, · · · ,</italic> <bold>c</bold><italic><sub>L,m</sub></italic>]. In other words, the <italic>l</italic><sup>th</sup> electrode in the sensor array observes a specific projection of the underlying <italic>m</italic><sup>th</sup> analytic oscillation given by <bold>c</bold><italic><sub>l,m</sub></italic> = [<italic>c<sub>l,m,</sub></italic><sub>1</sub><italic>, c<sub>l,m,</sub></italic><sub>2</sub>], which encodes the amplitude and phase of the <italic>m</italic><sup>th</sup> oscillation time-course at that electrode. This probabilistic generative model for multichannel recordings is motivated by a potential biophysical mechanism of electromagnetic traveling wave generation in brain parenchyma [<xref ref-type="bibr" rid="c32">32</xref>] (see SI Text: Section Mechanistic origin ).</p>
</sec>
<sec id="s2c">
<title>An illustration</title>
<p>In <xref rid="fig1" ref-type="fig">Fig 1(a)</xref>, middle panel, we depict an oscillation state as an analytic signal, <bold>x</bold><italic><sub>n</sub></italic> (black arrows) in 2D state space that is rotating around the origin according to the given state space model of left panel: the red dashed line traces the oscillation states over time. The actual amount of rotation between every pair of consecutive time-points is a random variable centered around 2<italic>πf/f<sub>s</sub></italic> = 0.22<italic>π</italic>, with the spread determined by damping parameter, <italic>a</italic> = 0.99 and process noise covariance. The right panel shows two noisy measurements, reflecting two different projections: one on the real axis (blue traces), another on the line making <italic>π/</italic>4 radian angle to the real axis (orange traces). Because of this angle between the lines of projection, these two measurements maintain approximately <italic>π/</italic>4 radian phase difference throughout the time course of the oscillation.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>From state space oscillator model to oscillation component decomposition: (a) An illustrative example of a multichannel state space oscillation model: A single oscillation realized as an analytic signal <bold>x</bold><italic><sub>n</sub></italic> is measured as two different projections having <italic>π/</italic>4 radian phase difference. (b) Graphical representation of the probabilistic generative model describing the oscillations as dynamic processes, that undergo mixing at the sensors and that are observed with additive Gaussian noise. (c) Graphical representation of the Variational Bayes’ approximation that allows iterative closed form inference. (d) Oscillation component analysis fitting and reconstruction pipeline for experimentally recorded neurophysiological data. The pipeline exposes a number of methods for ease of analysis, i.e., for fitting the OCA hyperparameters fit() method, that accepts the sensor recordings and an initial sensor noise covariance matrix as input, for extracting the oscillation time-courses get sources(), for reconstructing a multichannel signal from any arbitrary subset of oscillation components, apply(), for getting a final noise covariance estimate from the residuals of OCA fitting, get noise covariance() method etc.</p></caption>
<graphic xlink:href="550594v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We note that several earlier works [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c34">34</xref>] similar multivariate oscillator models, albeit from the perspective of a spatio-spectral eigendecomposition of the companion form of a multivariate autoregressive (MVAR) parameter matrix [<xref ref-type="bibr" rid="c35">35</xref>]. As described earlier, we employ this state space form here in the context of a blind source separation problem.</p>
<p>Next, we briefly describe how one can infer the hidden oscillator states from observed multivariate time-series dataset given the oscillation state space model parameters; and potentially adjust the oscillation state space model parameters to the dataset. We defer the mathematical derivations to the Materials and methods section to maintain lucidity of the presentation.</p>
</sec>
</sec>
<sec id="s3">
<title>Learning Algorithm</title>
<sec id="s3a">
<title>Priors</title>
<p>We assume simple prior distributions on the measurement noise, and sensor level mixing coefficients to facilitate stable and unique recovery of the oscillation components. To obtain an <italic>M</italic> - oscillator analysis of <italic>L</italic>-channel data, we consider a Gaussian prior on the spatial mixing components, <bold>c</bold><italic><sub>l,m</sub></italic> with precision <italic>α</italic>, and an inverse-Wishart prior on sensor noise covariance matrix, <bold>R</bold> with scale, <bold>Ψ</bold> and degrees of freedom, <italic>ν</italic>:
<disp-formula id="eqn4">
<graphic xlink:href="550594v4_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We treat the oscillation parameters, (<italic>f</italic> <sup>(<italic>m</italic>)</sup><italic>, a</italic><sup>(<italic>m</italic>)</sup>, (<italic>σ</italic><sup>2</sup>)<sup>(<italic>m</italic>)</sup>) := <bold><italic>θ</italic></bold><sup>(<italic>m</italic>)</sup> and distributional parameters of the assumed priors, <italic>α</italic>, <bold>Ψ</bold><italic>, ν</italic> as hyperparameters. <xref rid="fig1" ref-type="fig">Fig 1(b)</xref> shows the probabilistic graphical model portraying oscillations as a dynamical system evolving in time, with the priors as parent nodes.</p>
</sec>
<sec id="s3b">
<title>Variational Bayes’ inference</title>
<p>Unfortunately, given the priors and the interplay between oscillation source time-courses and sensor-level mixing patterns, exact computation of the log-likelihood, and thus the exact posterior, is intractable. We therefore employ variational Bayes inference [<xref ref-type="bibr" rid="c36">36</xref>], a computationally efficient inference technique to obtain a closed-form approximation to the exact Bayes’ posterior. Originally introduced by [<xref ref-type="bibr" rid="c37">37</xref>], variational Bayesian inference simplifies the inference problem through a restrictive parameterization that reduces the search space for distributions, splitting up the problem into multiple partial optimization steps that are potentially easier to solve [<xref ref-type="bibr" rid="c38">38</xref>]. The name comes from the <italic>negative variational free energy</italic>, also known as evidence lower bound or ELBO [<xref ref-type="bibr" rid="c39">39</xref>], used to assess the quality of the aforementioned approximation and as a surrogate for the log-likelihood (see SI text: Section Negative variational free energy ).</p>
<p>In particular, given the number of oscillatory components <italic>M</italic> and the corresponding hyper-parameters <bold><italic>θ</italic></bold> = [<bold><italic>θ</italic></bold><sup>(1)</sup>, <bold><italic>θ</italic></bold><sup>(2)</sup><italic>, · · · ,</italic> <bold><italic>θ</italic></bold><sup>(<italic>M</italic>)</sup>] and <italic>α,</italic> <bold>Ψ</bold><italic>, ν</italic> we use the following Variational Bayes (VB) decoupling [<xref ref-type="bibr" rid="c38">38</xref>] of the posterior distribution of mixing matrix <bold>C</bold>, oscillation states <bold>x</bold><italic><sub>t</sub></italic> and noise covariance matrix <bold>R</bold>:
<disp-formula id="eqn5">
<graphic xlink:href="550594v4_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This particular choice allows the approximate posteriors of these quantities to be represented in closed-form by multivariate Gaussian, multivariate Gaussian and inverse-Wishart distributions, respectively (see SI text: Section Variational Bayes’ inference ). <xref rid="fig1" ref-type="fig">Fig 1(c)</xref> shows the graphical representation of posterior distribution after the VB decoupling. This essentially allows us to perform an iterative posterior inference procedure, where we cyclically update the posteriors <italic>q</italic> (<italic>{</italic><bold>x</bold><italic><sub>t</sub>} | M</italic> ), <italic>q</italic> (<bold>C</bold> <italic>| M</italic> ), and <italic>q</italic> (<bold>R</bold> <italic>| M</italic> ) using the latest sufficient statistics from other two distributions (see SI text: Section Variational Bayes’ inference for more details).</p>
</sec>
<sec id="s3c">
<title>Generalized EM to update hyperparameters</title>
<p>Since the parameters of the state space model, and of the assumed prior distributions, are not known <italic>a priori</italic>, we then obtain their point estimates using an instance of the generalized Expectation Maximization (GEM) algorithm which utilizes the aforementioned approximate inference in the E-step [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>]. We start the learning algorithm by initializing <bold><italic>θ</italic></bold>, <inline-formula><inline-graphic xlink:href="550594v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, <inline-formula><inline-graphic xlink:href="550594v4_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, <italic>α</italic> to appropriate values (see SI text: Section Initialization of parameters and hyper parameters for details). We then cyclically update the posteriors <italic>q</italic> (<italic>{</italic><bold>x</bold><italic><sub>t</sub>} | M</italic> ), <italic>q</italic> (<bold>C</bold> <italic>| M</italic> ), and <italic>q</italic> (<bold>R</bold> <italic>| M</italic> ) until these iterations stop changing the negative variational free energy. At the end, we update the hyperparameters <bold><italic>θ</italic></bold>, <italic>α</italic> from the current inference, and proceed to the next inference with the updated hyperparameters. These update rules form an outer update loop that refines the hyperparameters, within which operates an inner update step that iteratively improves the approximate posterior distribution of the model parameters. We defer the update rules to SI text: Section Generalized EM algorithm . The algorithm terminates when the hyperparameter adjustments no longer increase the free energy of the model, i.e., we resort to parametric empirical Bayes estimation (see SI text: Section Empirical Bayes inference and model selection ).</p>
</sec>
<sec id="s3d">
<title>Selecting optimal number of oscillations</title>
<p>The learning algorithm assumes that the number of oscillation sources, <italic>M</italic> , is known a-priori, which is rarely the case for experimentally recorded data. In theory, our probabilistic treatment could assign a log-likelihood for a given dataset to every model structure, i.e., the number of oscillation sources, that can be used as a goodness-of-fit score. Unfortunately, the log-likelihood is intractable to evaluate, however the negative variational free energy can be used as a surrogate.</p>
<p>We formalize this idea by considering that the model structure, <italic>M</italic> , is drawn from a discrete uniform distribution over a finite contiguous set, (1<italic>, M<sub>max</sub></italic>):
<disp-formula id="eqn6">
<graphic xlink:href="550594v4_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with <italic>M<sub>max</sub></italic> being the maximal number of oscillation sources. This allows us to compute the posterior on the model structure, <italic>q</italic>(<italic>M</italic> ) within the same variational approximation framework. Using the negative variational free energy expression of the oscillator model it is easy to show that <italic>q</italic>(<italic>M</italic> ) is proportional to the exponential of the negative variational free energy of the <italic>M</italic> -oscillator model (see SI text: Section Model structure posterior ). Once the model posteriors are obtained, one can choose to simply select the model with maximum negative variational free energy (empirical Bayes model selection, see SI text: Section Empirical Bayes inference and model selection ), or locate the <italic>knee</italic> or <italic>jump</italic> point that exhibits a significant local change in <italic>q</italic>(<italic>M</italic> ) (for such an example see the DiffBic function[<xref ref-type="bibr" rid="c41">41</xref>]).</p>
</sec>
<sec id="s3e">
<title>Oscillation component analysis</title>
<p>A preliminary version of this algorithm has been presented previously [<xref ref-type="bibr" rid="c42">42</xref>]. In order to analyze experimental data, we devised the a standardized pipeline as demonstrated in <xref rid="fig1" ref-type="fig">Fig 1(d)</xref> closely following the MNE ICA pipeline [<xref ref-type="bibr" rid="c43">43</xref>]. We refer to this pipeline as OCA. The pipeline, implemented as python class OCA, accepts the sensor recordings either as continuous raw data or a set of discrete epochs, each with an initial sensor noise covariance matrix.</p>
<p>The hyperparameters for the measurement noise covariance prior are derived from the supplied initial sensor noise covariance matrix, and are not updated in this pipeline. The sensor data is standardized (i.e., pre-whitened) against the sensor noise covariance: this step also applies all active signal-space projectors to the data if present such as the average reference [<xref ref-type="bibr" rid="c44">44</xref>]. The pre-whitened data is then decomposed using PCA. The first n_components are then passed to the parameter learning iterations for hyperparameters <bold><italic>θ</italic></bold><italic>, α</italic>, followed by the joint inference of the mixing matrix, oscillation time-courses, and residual noise covariance. The optimal number of oscillations is then selected as described earlier. The final noise covariance is computed from the remaining PCA components that are not supplied to oscillation fitting and the OCA residuals. Once the oscillation hyperparameters and the mixing matrix have been estimated, the oscillation time-courses can be estimated from any continuous recordings or discrete epochs. Finally, the pipeline can also reconstruct a multichannel signal from any arbitrary subset of oscillation components. We emphasize here that the main reason for using PCA here is to perform rotation of the data, so that: 1) rank-deficient data (i.e., average referenced EEG data) can be handled conveniently, by throwing out the zero-variance component. 2) the variance the multi-channel data can be uniformly distributed in the remaining PCs. For these reason, we retain components explaining 99.9% variance of the multichannel recordings, i.e., almost all components with non-zero variance. The PCA step essentially performs rotation of the data in the spatial dimension, leaving the temporal relationships untouched.</p>
</sec>
</sec>
<sec id="s4">
<title>Data Examples</title>
<p>Finally, we apply OCA on a simulation example and several experimentally recorded datasets. We demonstrate the versatility of OCA analysis using two EEG datasets, one under propofol-induced anesthesia, another during sleep, and one resting state MEG dataset. The simulation example shows the utility of such time-domain analysis over frequency-domain analysis in the context of multichannel recordings. In the real data applications, we showcase the implications of OCA as a mechanistically-grounded tool, suggesting various downstream analysis involving the oscillation time courses and their sensor distributions.</p>
<sec id="s4a">
<title>Simulation Example</title>
<p>We first apply the proposed oscillation decomposition method on a synthetic dataset to illustrate how our time-domain approach differs from traditional frequency-domain approach. We generated a synthetic EEG dataset using a 64 channel montage, forward model and noise covariance matrix from a sample dataset distributed with the MNE software package [<xref ref-type="bibr" rid="c43">43</xref>]. To select the active current sources, four regions with 5 mm<sup>2</sup> area were centered on chosen labels in the DKT atlas [<xref ref-type="bibr" rid="c45">45</xref>]: ‘transversetemporal-lh’, ‘precentral-rh’, ‘inferiorparietal-rh’ and ‘caudalmiddlefrontal-lh’. Three time-courses were separately generated at a sampling frequency of 100 Hz according to univariate AR(2) dynamics tuned to generate stable oscillations at 1.6 Hz (slow/delta), 10 Hz (alpha), 12 Hz (alpha) frequencies. The first two regions were simulated with the same slow/delta time-course such that the activity in the ‘precentral-rh’ area (orange, <xref rid="fig2" ref-type="fig">Fig 2A</xref>) lags that of ‘transversetemporal-lh’ area (blue, <xref rid="fig2" ref-type="fig">Fig 2A</xref>) by 10 ms. The last two regions, ‘inferiorparietal-rh’ and ‘caudalmiddlefrontal-lh’, were simulated with two independent alpha time-courses. We projected this source activity to the EEG sensors via a lead-field matrix and add spatially colored noise generated with the covariance structure in <xref rid="fig2" ref-type="fig">Fig 2D</xref> to the sensor data. Two 20 s epochs were chosen for OCA and model selection was performed within the range of (2, 3, 4, 5, 6, 8, 10) oscillations. <xref rid="fig2" ref-type="fig">Fig 2B</xref> presents the two most common frequency-domain visualizations of multichannel data: the top panel shows the power distribution over the scalp within different canonical EEG frequency bands, while the channel-wise power spectral density is shown in bottom panel. <xref rid="fig2" ref-type="fig">Fig 2C</xref> demonstrates how the decomposition obtained by OCA provides an accurate delineation of different oscillation time-courses and effective characterization of their spatial distributions in the sensor level mixing maps. Unlike the frequency-domain techniques, the residual time-series that follows OCA extraction provides the temporally unstructured part of the multichannel recording. The estimated covariance matrix from these residuals closely resembles the original sensor noise covariance matrix that was used to corrupt the recordings (see <xref rid="fig2" ref-type="fig">Fig 2E</xref>). The covariance matrix estimate is strikingly close to the true covariance matrix (i.e., compare with <xref rid="fig2" ref-type="fig">Fig 2D</xref>). OCA provides a measure, <italic>q</italic>(<italic>M</italic> ), for model structure selection, that objectively identifies the existence of 3 oscillation time-courses with different time dynamics (see <xref rid="fig2" ref-type="fig">Fig 2F</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Simulation Study: <bold>A.</bold> four neural sources carrying 3 oscillations, where the orange time-course is a lagged instance of blue time-course, red and green time courses are independent; <bold>B.</bold> power spectral density of the simulated EEG recording, and power distribution over the EEG sensors in different frequency bands; <bold>C.</bold> recovered oscillation components and their sensor level maps; <bold>D.</bold> sensor noise covariance for the simulation; <bold>E.</bold> estimated sensor noise covariance matrix; <bold>F.</bold> model structure selection via model structure posterior <italic>q</italic>(<italic>m</italic>).</p></caption>
<graphic xlink:href="550594v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s4a1">
<title>How does OCA compare to conventional approaches?</title>
<p>We performed additional analyses to compare OCA with Fourier based frequency domain method and ICA in a different realization of a similar synthetic EEG dataset (see <xref rid="fig3" ref-type="fig">Fig 3A</xref>). The frequency domain approach applied here is based on the multitaper method: given a bandwidth parameter of 2 Hz, power spectrum density is computed for individual channels. The oscillatory activity can be identified visually as the peaks in the power spectrum plot in <xref rid="fig3" ref-type="fig">Fig 3B</xref> top panel, and its scalp distribution can be obtained by averaging the power within given frequency bands around the peaks in each channel (as shown in <xref rid="fig3" ref-type="fig">Fig 3B</xref> bottom panels). However, these visualizations do very little to identify underlying oscillatory sources and pose additional questions. For example, the left topographical plot hints to two possible sources, but provides no evidence if they are separated spatially, temporally or both. Similarly, the right two topographical plots are almost identical and show no separability between these the oscillatory sources that generate two distinct peaks in spectrum. Regarding ICA, we use the ‘extended infomax’ [<xref ref-type="bibr" rid="c46">46</xref>] implementation provided by MNE-python 1.2 [<xref ref-type="bibr" rid="c43">43</xref>] to see if ICA can distinguish the three statistically independent sources in these data. The leading 4 ICA components are shown in <xref rid="fig3" ref-type="fig">Fig 3C</xref>. The ICA-identified components mix the underlying independent signals, which is not surprising since all components follow a Gaussian distribution. Meanwhile, OCA is able to recover three distinct oscillatory components consistent with the ground truth. OCA is able to do so because it explicitly models temporal dynamics of the components.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Simulation Study (extended): <bold>A.</bold> four neural sources carrying 3 oscillations, similar to <xref rid="fig2" ref-type="fig">Fig 2</xref>; <bold>B.</bold> power spectral density of the simulated EEG recording, and power distribution over the EEG sensors in the frequency bands (red overlay) around visually identifiable peaks; <bold>C.</bold> recovered ICA components (left, middle and right columns show topographic maps, power spectrum density and time-courses respectively); <bold>D.</bold> recovered OCA components (the topographic maps show the magnitude (left) and phase (right), while line plots show power spectrum density (left) and time-courses (right) respectively.)</p></caption>
<graphic xlink:href="550594v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We repeated similar analyses using real human EEG recording and found a similar result where ICA produced components that mixed slow and alpha band signals whereas OCA identified distinct oscillatory components (see SI text: Section Comparison of OCA to traditional approaches in experimental EEG data) .</p>
</sec>
</sec>
<sec id="s4b">
<title>EEG recording during propofol-induced unconsciousness</title>
<p>Next, we demonstrate utility of OCA on experimental data using EEG recordings from a healthy volunteer undergoing propofol-induced unconsciousness, previously described in [<xref ref-type="bibr" rid="c47">47</xref>]. For induction of unconsciousness, the volunteer underwent a computer-controlled infusion of propofol to achieve monotonically increasing levels of effect-site concentration in steps of 1 µg mL<italic><sup>−</sup></italic><sup>1</sup>. Each target effect-site concentration level was maintained for 14 min. The EEG was recorded using a 64-channel BrainVision MRI Plus system (Brain Products) with a sampling rate of 5000 Hz, bandwidth 0.016–1000 Hz. The volunteers were instructed to close their eyes throughout the study. Here we investigated EEG epochs during maintenance of effect-site concentrations of 0 µg mL<italic><sup>−</sup></italic><sup>1</sup> (i.e., baseline, eyes closed), 2 µg mL<italic><sup>−</sup></italic><sup>1</sup> and 4 µg mL<italic><sup>−</sup></italic><sup>1</sup> for OCA. We selected 10 clean 3.5 s epochs corresponding to those target effect site concentration (see Supplementary Materials).</p>
<p>We fitted OCA models with 20, 25, 30, 35, 45, 50, 55, 60 oscillation components, and selected the model with highest negative variational free-energy as the best OCA model. Following this empirical Bayes criteria we selected OCA models with 30, 30, 50 components for these three conditions, respectively. When the center frequencies were grouped within the canonical frequency bands [<xref ref-type="bibr" rid="c1">1</xref>], we found 17, 17, 29 slow/delta (0.1–4 Hz) oscillation components and 17, 9, 15 alpha (8–13 Hz) oscillation components for the three conditions respectively. <xref rid="fig4" ref-type="fig">Fig 4A–C</xref> shows power spectral densities (PSDs) of reconstructed EEG activity within each band, defined as the cumulative projection of these grouped oscillation components to the sensors. Since each of these oscillations have different sensor distributions, the increasing number of oscillation components can be associated with fragmentation of neuronal networks that support these oscillations.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>OCA of the EEG from a healthy volunteer undergoing propofol-induced unconsciousness. Conditions of target effect site concentration of <bold>A.</bold> &amp; <bold>D.</bold> 0 (i.e. baseline), <bold>B.</bold> &amp; <bold>E.</bold> 2 µg mL<italic><sup>−</sup></italic><sup>1</sup> and <bold>C.</bold> &amp; <bold>F.</bold> 4 µg mL<italic><sup>−</sup></italic><sup>1</sup> are analyzed. Panels <bold>A-C</bold> show the PSDs of reconstructed EEG activity within each canonical band. Panels <bold>D-F</bold> show the three dominant (in terms of sensor wide power) alpha component: the topographic maps show the magnitude (left) and phase (right) distribution of sensor level mixing, the time courses are 1 sec representative example of the extracted oscillations from the selected epochs. The black bars on the right display the coherency measure within alpha band. OCA correctly identifies that the spatial mixing sensor maps of the alpha waves (8 Hz to 12 Hz) are oriented posteriorly at baseline, but gradually become frontally-dominant under propofol. The sensor weights are scaled to have maximum value 1. So, the units of time series traces can be considered to be in µV.</p></caption>
<graphic xlink:href="550594v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We also quantified the coherency of the reconstructed EEG activity within these commonly used frequency bands as the ratio of the power of the strongest component to the total power of the oscillation components within the bands. For the data analyzed here, slow+delta band coherency decreases gradually while the alpha band coherency increases with the increasing propofol effect-site concentration. To investigate this observation further, we visualized three dominant alpha components from each conditions in order of their strengths in <xref rid="fig4" ref-type="fig">Fig 4D-F</xref>. In each column, the left sub-panels consist of two topographic plots: the left one showing the distributions of the strength (scaled between 0 to 1) and the right one showing the relative phase of the estimated analytic oscillation mixing, <bold>c</bold><italic><sub>i,j</sub></italic>. The right sub-panels display examples of the extracted analytic oscillations, i.e., estimated real and imaginary components of 1 s long oscillation segments. The rightmost black bars show the coherency measure within alpha band. Clearly, during the 4 µg mL<italic><sup>−</sup></italic><sup>1</sup> effect site concentration, the amplitude of the leading alpha oscillation component is much bigger than the others, which aligns with our observation of the coherency measure. This finding previously reported analyses showing a rise of slow+delta oscillation power, rapid fragmentation of a slow+delta oscillation network, and emergence of global alpha network during propofol induced anesthesia [<xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c49">49</xref>]. Further, as the propofol effect site concentration increases, the temporal dynamics increasingly resemble a pure sinusoidal signal, as evident from the PSD of the Alpha band reconstruction that becomes more concentrated around the center frequency. Lastly, the topographic plots in <xref rid="fig4" ref-type="fig">Fig 4D-F</xref> illustrate how alpha activity shifts from posterior electrodes to frontal electrode with increasing propofol concentration [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c50">50</xref>].</p>
</sec>
<sec id="s4c">
<title>EEG recordings under different Sleep Stages</title>
<p>The HD-EEG data shown in <xref rid="fig5" ref-type="fig">Fig 5</xref> is from a young adult subject (age 28, Female) undergoing a sleep study, recorded using ANT Neuro eego mylab system. After preprocessing (see Supplementary Materials), we visually selected 10 clean 5 s segments of data that exhibited strong alpha oscillations, each from relaxed wakeful (eyes closed) and REM sleep to apply OCA.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>OCA of the EEG from a healthy young volunteer to compare between (<bold>A.</bold> &amp; <bold>C.</bold>) wakeful resting state and (<bold>B.</bold> &amp; <bold>D.</bold>) rapid eye movement sleep. Panels <bold>A-B</bold> show the PSDs of reconstructed EEG activity within each canonical band. Panels <bold>C-D</bold> show the three dominant (in terms of sensor wide power) alpha component: the topographic maps show the magnitude (left) and phase (right) distribution of sensor level mixing, the time courses are 1 sec representative example of the extracted oscillations from the selected epochs. The rightmost black bars display the coherency measure within alpha band. The contrasting topographic distribution of the alpha components (8–12 Hz), the shape of the oscillation power spectrum and alpha coherence hints at a distinct generating mechanism for alpha waves during stage–2 REM sleep compared to awake eyes closed alpha wave. The sensor weights are scaled to have a maximum value 1 so that the units of time series traces are in µV.</p></caption>
<graphic xlink:href="550594v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We fitted OCA models with 20, 25, 30, 35, 45, 50, 55, 60 oscillation components, and selected the model structure with the highest negative variational free-energy as the optimum OCA model. The empirical Bayes criterion selected 55 and 50 OCA components for relaxed wakeful (eyes closed) and REM sleep, respectively. <xref rid="fig5" ref-type="fig">Fig 5</xref> follows a format similar to <xref rid="fig4" ref-type="fig">Fig 4</xref>: panels A-B show the power spectrum density of the recorded EEG and the reconstructed sensor data from the OCA components grouped in slow, theta and alpha bands; while panels C-D visualizes three of the extracted oscillation components within the alpha band, ordered according to their strengths. The topographical maps show the scalp distribution of these components, while the blue and red traces show 1 s–long oscillation time-series pair corresponding to each component.</p>
<p>Activity within the alpha band (8–13 Hz) could be summarized by a few oscillation components: for example only 7 and 3 OCA components had their center frequency within alpha band during relaxed wakeful (eyes closed) condition and REM sleep, respectively. These alpha oscillation components during relaxed wakeful (eyes closed) condition and REM sleep appeared to be distinct in terms of their temporal dynamics: relaxed wakeful alpha components are more regular (i.e. closer to sinusoidal activity) than REM alpha. But, REM alpha coherency (0.4307) is moderately higher than that of relaxed wakeful alpha (0.3195). The spatial distribution of the REM alpha component appeared to be confined primarily within the posterior channels while the awake alpha is more distributed. To quantify the degree of similarity or difference between the spatial distributions, we calculated the principal angle between the spatial mixing maps of the awake eyes-closed and REM alpha components. The <italic>principal angle</italic> measures the similarity between two subspaces, where an angle of 0 ° indicates that one subspace is a subset of the other, and where an angle of 90 ° indicates that at least one vector in a subspace is orthogonal to the other [<xref ref-type="bibr" rid="c51">51</xref>]. The spatial mixing maps for awake eyes-closed alpha and REM sleep had a principal angle of 44.31 °, suggesting that the subspaces, and thus the underlying cortical generators, were substantially different.</p>
</sec>
<sec id="s4d">
<title>Resting state MEG recording</title>
<p>Finally, we demonstrate the utility of OCA on a MEG dataset from the Human Connectome Project. As requested by the Human Connectome Project, the following text is copied verbatim: MEG data in Section Resting state MEG recording ‘were provided by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.’ The MEG data used here is the resting-state recording from subject 104 012 MEG, session 3.</p>
<p>We fit OCA on 34 clean 5 s epochs, and from among the OCA with 20, 25, 30, 35, 40, 45, 50, 55 components, empirical Bayes criterion selected the OCA with 30 components. Out of these 13 were within the slow+delta band and 10 were alpha oscillations. For this example, we sought to highlight a different downstream analysis of the extracted oscillation components. We chose the three leading slow oscillation components and the three leading alpha oscillation components from the set of identified oscillation components.</p>
<p>For each oscillation, OCA extracts a pair of time traces, i.e., one <italic>real</italic> time trace and one <italic>imaginary</italic> time trace (see SI Text: Section Oscillation states and analytic signals )). These ‘real’ and ‘imaginary’ indices can be utilized to compute the instantaneous phase and instantaneous amplitude of individual oscillation components at every time-point, without having to rely on Hilbert transform [<xref ref-type="bibr" rid="c52">52</xref>].</p>
<disp-formula>
<graphic xlink:href="550594v4_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<p>Further, we pick any one of the slow oscillation components and one of the alpha oscillation components and consider the ordered pair of slow component phase and alpha component amplitude at a single time-point as a sample drawn from the joint distribution of the ‘phase-amplitude’ of these two components. These samples then can be used to quantify cross frequency phase-amplitude coupling between these components via either nonparametric [<xref ref-type="bibr" rid="c53">53</xref>–<xref ref-type="bibr" rid="c55">55</xref>] or parametric modelling [<xref ref-type="bibr" rid="c56">56</xref>, <xref ref-type="bibr" rid="c57">57</xref>]. <xref rid="fig6" ref-type="fig">Fig 6</xref> demonstrates a possible way of investigating the coupling between the slow oscillation component phase and alpha oscillation component amplitude: the conditional mean of the alpha amplitude given slow/delta phase is higher at some specific phases between all three slow waves and OSC012, but overall flat for other alpha oscillation components. This example illustrates how the OCA algorithm can be used to identify specific oscillatory components that show phase-amplitude coupling.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Cross–frequency phase–amplitude coupling in OCA components extracted from resting state MEG recording. The black traces show the conditional mean of a selected alpha component (8–12 Hz) amplitude given another selected slow/delta component (0–4 Hz) phase. The three slow oscillations and three alpha oscillations that explained the highest variance were selected for demonstration purposes. The topographic maps show the magnitude (left) and phase (right) distribution of sensor level mixing of the selected components.</p></caption>
<graphic xlink:href="550594v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>OCA is a novel approach to the multichannel component decomposition problem that can identify a number of independent spatio-temporal components, but does so in the context of underlying temporal dynamics specified by a state space model. In the state space representation used here, the dynamics of an elemental oscillation are parameterized by a central frequency, <italic>f</italic> , a damping parameter, <italic>a</italic> and the second order statistics of stochastic driving noise, <italic>σ</italic><sup>2</sup>. Meanwhile an associated spatial mixing pattern at the sensor level quantifies the contribution of the elemental oscillation to each observed channel.</p>
<p>The OCA learning algorithm uses an instance of the generalized expectation maximization algorithm to iteratively <italic>match</italic> the parameters of the oscillation state space parameters to the second order statistics of the M/EEG data. The goodness-of-fit for the data–driven matching procedure is defined within a Bayesian framework, as is the inference for the oscillation time-courses, their sensor level mixing patterns, and the measurement noise covariance matrix. This same goodness-of-fit metric is further used to determine the number of oscillations present in the multichannel data via empirical Bayes model selection. Once the number of oscillations, oscillation state space parameters, their sensor level mixing patterns, and the measurement noise covariance matrix are estimated, the elemental oscillatory activities can be extracted as a pair of time-courses, i.e., the ‘real’ and ‘imaginary’ parts of the analytical signal, from any given observation.</p>
<p>The oscillator state space parameters discovered in OCA are akin to the ad-hoc parameters, e.g., peak frequency, full-width-half-maxima bandwidth, and peak oscillation power etc. encountered in Fourier/wavelet based frequency-domain nonparametric time series analysis [<xref ref-type="bibr" rid="c58">58</xref>]. However, OCA circumvents a major drawback of frequency-domain methods when searching for neural oscillations from multichannel recordings. Generally, the source-to-sensor mixing complicates the interpretation of the topographies generated by signal processing tools that treat each channel individually [<xref ref-type="bibr" rid="c6">6</xref>]. In a nonparametric frequency-domain approach, the only way to discover this spatially-correlated structure across sensors is to perform eigenvalue analysis on the cross-spectral density on a frequency by frequency basis, also known as global coherence analysis [<xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c59">59</xref>, <xref ref-type="bibr" rid="c60">60</xref>]. If the eigenvalue distribution exhibits sufficient skewness, the associated frequency is deemed coherent and the leading eigenvectors are identified as the ‘principal’ sensor networks at that frequency; in other words a proxy for a strong oscillatory component. As the number of sensors increases this analysis becomes increasingly intractable: in a sensor array with <italic>∼</italic> 10<sup>2</sup> sensors the cross–spectral density matrix has <italic>∼</italic> 10<sup>4</sup> entries. Alternatively, the state space representation used here in OCA provides a convenient and compact way to perform multichannel time-domain modeling and analysis [<xref ref-type="bibr" rid="c61">61</xref>]. The OCA modelling approach decouples the oscillatory dynamics and the spatial mixing pattern effectively by estimating one set of parameters for each discovered oscillation and the associated spatial mixing matrix, thereby avoiding the need for cross–spectral density matrices and frequency parameters for individual channels altogether [<xref ref-type="bibr" rid="c62">62</xref>]. Another major advantage of OCA over global coherence analysis is that OCA automatically identifies the dominant <italic>coherent</italic> oscillations across the channels based on its probabilistic generative model and Bayesian learning approach.</p>
<p>The iterative parameter estimation procedure of OCA is clearly more computationally burdensome compared to conventional Frequency-domain nonparametric methods. However, once the OCA parameters are estimated, OCA can decompose any given multichannel recording segments into the oscillation time-series pairs. In that sense, OCA parameter learning can be viewed as iterative estimation of a pair of optimal spatio–temporal filters [<xref ref-type="bibr" rid="c63">63</xref>] for each oscillation, parameterized by the estimated oscillation state space parameters and spatial maps. These optimal spatio–temporal filters are then applied to the multichannel data to extract the oscillation time courses. As a result, the extracted narrowband activity is endogenous to the data, rather than being imposed by an arbitrary narrowband filter [<xref ref-type="bibr" rid="c22">22</xref>].</p>
<p>This behavior of OCA is similar to oscillatory component extraction methods based on data-driven spatial filtering, where the extracted component inherits the intrinsic oscillatory dynamics around the specified frequency [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>]. In fact, OCA employs the same philosophy as spatio-temporal source separation [<xref ref-type="bibr" rid="c21">21</xref>], where the extracted narrow-band signal and its time-shifted versions are again projected to a temporal coordinate space to further enhance the power within the given frequency band. However, spatio-temporal source separation establishes the temporal constraint via nonparametric sample correlation estimates that are sensitive to noise or artifacts and that require substantial amounts of data for estimation. Another important distinction is that spatio-temporal source separation determines the sensor weights first, and then obtains the temporal filter kernel from the projection of the multichannel data. In contrary, OCA updates the sensor weights and the parameters for the temporal filter iteratively within the Bayesian formulation of the state space framework, thus jointly estimating the spatial and temporal components simultaneously from the data.</p>
<p>The class of ICA–based methods, on the other hand, assumes that the component time courses share no mutual information, i.e., are statistically independent. Without any explicit assumption on the temporal dynamics of the generative process, the properties of the identified components may be difficult to interpret and may also be unreliable. In fact, the identified ICA components typically require subjective visual inspection by experts for their possible interpretation. A number of investigators have recognized these limitations with ICA, in particular the assumption of temporal independence, and have proposed generalizations of the ICA framework to incorporate auto-regressive modeling of source dynamics [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c64">64</xref>]. Despite demonstrating improved source separation performance in naturalistic signals like music [<xref ref-type="bibr" rid="c64">64</xref>], adoption of these methods for the analysis of neurophysiological data has been slow. This may be due in part to the lack of interpretability of the higher order auto-regressive model structures. Alternatively, Brookes et. al. use a combination of Hilbert envelope computation within pre-defined frequency bands and temporal down-sampling to identify meaningful temporally independent time signals, but stop short of trying to disambiguate different oscillatory sources [<xref ref-type="bibr" rid="c13">13</xref>]. Thus, they only assess the connectivity pattern within pre-defined bands, i.e., how different areas of the brain are harmonized through modulation of the oscillations or vice-versa inside those pre-defined bands. The spatial maps recovered from anatomically projected resting state MEG data by this method resemble spatial patterns of fMRI resting state networks. OCA is close to the first variant of ICA, but describes the identified component sources using a two-dimensional state space vector that efficiently represents oscillatory dynamics [<xref ref-type="bibr" rid="c28">28</xref>] in a manner that is easy to interpret. Matsuda and Komaki [<xref ref-type="bibr" rid="c33">33</xref>] described a similar state space model for multivariate time series, but did not apply the model to neurophysiological data, perhaps due to its high dimensionality. The OCA algorithm differs by way of its learning and model selection algorithms, which allow it to select the number of components to extract in a statistically principled and data driven manner. Overall, OCA delivers the best of the previously mentioned eigen-analysis-based source separation methods and ICAs: a decomposition method that can identify independent components, but where each component represents an underlying oscillatory dynamical system. The generative link between each component and the underlying dynamical system makes interpretation of the components straightforward. Here we focus specifically on analyzing neurophysiological data that are exemplified by highly structured oscillatory dynamics. But the methods we describe apply to more general, arbitrary dynamics that can be approximated by linear state space models, and can be equally useful so long as the state space model is interpretable.</p>
<p>There is another important distinction between OCA and the aforementioned blind source separation techniques. These methods directly estimate the sensor weights for a weighted linear combination of sensor recordings that produce the individual temporal components [<xref ref-type="bibr" rid="c23">23</xref>]. In other words, these blind source separation methods provide a backward decoding algorithm. The linear weights are not directly interpretable since they do not specify how the individual temporal components map to the observed data. That uniquely interpretable information is provided by a forward mapping which must be estimated in a separate step. When the number of linear components matches with the number of sensors, this forward mapping can be calculated via simple matrix inversion. However, when there is a reduced set of components the forward mapping can only be obtained by solving an inverse problem that requires knowledge of the source and sensor sample covariance matrix. In contrast, the OCA model involves only the (forward) spatial distribution matrix and our algorithm directly estimates it. In fact, the backward extraction model of OCA involves non-zero weights for time-shifted signals and are never explicitly computed. In that sense, OCA avoids an extraneous transformation step that could inadvertently introduce errors to the spatial mixing pattern estimates.</p>
<p>Another popular time-domain approach for oscillatory signal discovery is the multichannel extension of empirical mode decomposition. Empirical mode decomposition models the time series data as linear combination of intrinsic oscillations called intrinsic mode functions (IMF). Identification of IMFs depends critically on finding the local mean of the local extrema, which is not well-defined in the context of multichannel recordings. Rehman and Mandic [<xref ref-type="bibr" rid="c65">65</xref>] chose to take real-valued projections of the <italic>n</italic>-channel data, with the projections taken along direction vectors uniformly sampled on a unit spherical surface in an <italic>n</italic> dimensional coordinate space. The local extrema of each of the projected signals are extrapolated to obtain a set of multivariate signal envelopes. Clearly, this random sampling in a high-dimensional space is computationally demanding, and the fully nonparametric formulation requires a substantial amount of data, making the procedure sensitive to noise. On the contrary, OCA employs a parametric model that represents oscillatory dynamics in a manner that is statistically efficient and resilient to noise.</p>
</sec>
<sec id="s6">
<title>Conclusion</title>
<p>In summary, starting from a simple probabilistic generative model of neural oscillations, OCA provides a novel data-driven approach for analyzing multichannel synchronization within underlying oscillatory modes that are easier to interpret than conventional frequency-wise, cross-channel coherence. The overall approach adds significantly to existing methodologies for spatio-temporal decomposition by adding a formal representation of dynamics to the underlying generative model. The application of OCA on simulated and real M/EEG data demonstrates its capabilities as a principled dimensionality reduction tool that simultaneously provides a parametric description of the underlying oscillations and their activation pattern over the sensor array.</p>
</sec>
<sec id="s7">
<title>Materials and methods</title>
<sec id="s7a">
<title>M/EEG pre-processing</title>
<p>All prepossessing were performed using MNE-python 1.2[<xref ref-type="bibr" rid="c43">43</xref>] and Eelbrain 0.37[<xref ref-type="bibr" rid="c66">66</xref>], with default setting of the respective functions.</p>
<sec id="s7a1">
<title>EEG recording during propofol-induced unconsciousness</title>
<p>The EEG was bandpass filtered between 0.1 Hz to 40 Hz, followed by downsampling to 100 Hz and average referencing prior to performing OCA. The 0.1 Hz highpass filtering was done to filter out slow drifts in the EEG recordings, which can adversely effect the OCA fitting procedure. All selected epochs had peak-to-peak signal amplitude less than 1000mV. The prior on the noise-covariance was estimated from the same EEG recording, after high-pass filtering above 30 Hz.</p>
</sec>
<sec id="s7a2">
<title>Sleep EEG recording</title>
<p>The EEG data was first down sampled to 100 Hz after band pass filtering within 1 Hz to 40 Hz. The flat and noisy channels are first identified upon visual inspection. We computed neighborhood correlation for the rest of the channels and marked channels with median correlation, <italic>ρ &lt;</italic> 0.4 as bad channels. These channels are dropped for the subsequent analysis.</p>
</sec>
<sec id="s7a3">
<title>MEG recording from Human Connectome Project</title>
<p>We divided the entire resting state MEG recording from subject 104 012 MEG, session 3 into 5 s epochs, and selected 34 epochs with peak-to-peak amplitude less than 4000 fT for OCA. Prior to fitting OCA, we used Signal-space projection [<xref ref-type="bibr" rid="c44">44</xref>] to remove heartbeat and eye movement artifacts from MEG signals. The repaired MEG recordings are then downsampled to 100 Hz after applying appropriate anti-aliasing filter.</p>
</sec>
</sec>
</sec>
<sec id="d1e1336" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1440">
<label>Supporting Info</label>
<media xlink:href="supplements/550594_file08.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<sec id="s8">
<title>Supporting Information</title>
<p><bold>Supporting Text: Mechanistic origin</bold> Traveling wave solution of Maxwell’s equation to the state space model.</p>
<p><bold>Supporting Text: Oscillation states and analytic signals</bold> Relation between oscillation states and analytic signals</p>
<p><bold>Supporting Text: Model parameter estimation and model selection</bold> Detailed derivation of the OCA algorithm.</p>
<p><bold>Supporting Text: Comparison of OCA to traditional approaches in experimental EEG data</bold></p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by National Institutes of Health (Grant no. R01AG054081-01A1) and Tiny Blue Dot Foundation.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="book"><string-name><given-names>G.</given-names> <surname>Buzsaki</surname></string-name>, <source>Rhythms of the Brain</source>. <publisher-name>Oxford University Press</publisher-name>, <year>2006</year>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Buzsáki</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Draguhn</surname></string-name>, “<article-title>Neuronal Oscillations in Cortical Networks</article-title>,” <source>Science</source>, vol. <volume>304</volume>, pp. <fpage>1926</fpage>–<lpage>1929</lpage>, June <year>2004</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Lopes da Silva</surname></string-name>, “<article-title>EEG and MEG: Relevance to Neuroscience</article-title>,” <source>Neuron</source>, vol. <volume>80</volume>, pp. <fpage>1112</fpage>– <lpage>1128</lpage>, <month>Dec.</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><given-names>X.-J.</given-names> <surname>Wang</surname></string-name>, “<article-title>Neurophysiological and Computational Principles of Cortical Rhythms in Cognition</article-title>,” <source>Physiological Reviews</source>, vol. <volume>90</volume>, pp. <fpage>1195</fpage>–<lpage>1268</lpage>, July <year>2010</year>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>R. F.</given-names> <surname>Helfrich</surname></string-name> and <string-name><given-names>R. T.</given-names> <surname>Knight</surname></string-name>, “<article-title>Oscillatory Dynamics of Prefrontal Cognitive Control</article-title>,” <source>Trends in Cognitive Sciences</source>, vol. <volume>20</volume>, pp. <fpage>916</fpage>–<lpage>930</lpage>, <month>Dec.</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Schaworonkow</surname></string-name> and <string-name><given-names>V. V.</given-names> <surname>Nikulin</surname></string-name>, “<article-title>Is sensor space analysis good enough? Spatial patterns as a tool for assessing spatial mixing of EEG/MEG rhythms</article-title>,” <source>NeuroImage</source>, p. 119093, <month>Mar.</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><given-names>I. H.</given-names> <surname>Stevenson</surname></string-name> and <string-name><given-names>K. P.</given-names> <surname>Kording</surname></string-name>, “<article-title>How advances in neural recording affect data analysis</article-title>,” <source>Nature Neuroscience</source>, vol. <volume>14</volume>, pp. <fpage>139</fpage>–<lpage>142</lpage>, <month>Feb.</month> <year>2011</year>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Hyvärinen</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Oja</surname></string-name>, “<article-title>Independent component analysis: Algorithms and applications</article-title>,” <source>Neural Networks</source>, vol. <volume>13</volume>, pp. <fpage>411</fpage>–<lpage>430</lpage>, June <year>2000</year>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>T.-P.</given-names> <surname>Jung</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Makeig</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Humphries</surname></string-name>, <string-name><given-names>T.-W.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>McKeown</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Iragui</surname></string-name>, and <string-name><given-names>T. J.</given-names> <surname>Sejnowski</surname></string-name>, “<article-title>Removing electroencephalographic artifacts by blind source separation</article-title>,” <source>Psychophysiology</source>, vol. <volume>37</volume>, pp. <fpage>163</fpage>–<lpage>178</lpage>, Mar. <year>2000</year>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>J.-F.</given-names> <surname>Cardoso</surname></string-name>, “<article-title>High-Order Contrasts for Independent Component Analysis</article-title>,” <source>Neural Computation</source>, vol. <volume>11</volume>, pp. <fpage>157</fpage>–<lpage>192</lpage>, Jan. <year>1999</year>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Hyvärinen</surname></string-name>, <article-title>Independent component analysis: Recent advances</article-title>, <source>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</source>, vol. <volume>371</volume>, p. <fpage>20110534</fpage>, <month>Feb.</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>A. J.</given-names> <surname>Bell</surname></string-name> and <string-name><given-names>T. J.</given-names> <surname>Sejnowski</surname></string-name>, “<article-title>An Information-Maximization Approach to Blind Separation and Blind Deconvolution</article-title>,” <source>Neural Computation</source>, vol. <volume>7</volume>, pp. <fpage>1129</fpage>–<lpage>1159</lpage>, <month>Nov.</month> <year>1995</year>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>M. J.</given-names> <surname>Brookes</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Woolrich</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Luckhoo</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Price</surname></string-name>, <string-name><given-names>J. R.</given-names> <surname>Hale</surname></string-name>, <string-name><given-names>M. C.</given-names> <surname>Stephenson</surname></string-name>, <string-name><given-names>G. R.</given-names> <surname>Barnes</surname></string-name>, <string-name><given-names>S. M.</given-names> <surname>Smith</surname></string-name>, and <string-name><given-names>P. G.</given-names> <surname>Morris</surname></string-name>, “<article-title>Investigating the electrophysiological basis of resting state networks using magnetoencephalography</article-title>,” <source>Proceedings of the National Academy of Sciences</source>, vol. <volume>108</volume>, pp. <fpage>16783</fpage>–<lpage>16788</lpage>, <month>Oct.</month> <year>2011</year>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="book"><string-name><given-names>R. J.</given-names> <surname>Muirhead</surname></string-name>, <chapter-title>Aspects of Multivariate Statistical Theory</chapter-title>. <source>Wiley Series in Probability and Mathematical Statistics</source>, <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>, <year>1982</year>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="confproc"><string-name><given-names>N.</given-names> <surname>Robinson</surname></string-name>, <string-name><given-names>K. P.</given-names> <surname>Thomas</surname></string-name>, and <string-name><given-names>A. P.</given-names> <surname>Vinod</surname></string-name>, “<article-title>Canonical correlation analysis of EEG for classification of motor imagery</article-title>,” in <conf-name>2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</conf-name>, (Banff, AB), pp. <fpage>2317</fpage>–<publisher-loc>2321</publisher-loc>, IEEE, Oct. <year>2017</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Parra</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Sajda</surname></string-name>, “<article-title>Blind Source Separation via Generalized Eigenvalue Decomposition</article-title>,” <source>Journal of Machine Learning Research</source>, vol. <volume>4</volume>, pp. <fpage>1261</fpage>–<lpage>1269</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>de Cheveigné</surname></string-name> and <string-name><given-names>L. C.</given-names> <surname>Parra</surname></string-name>, “<article-title>Joint decorrelation, a versatile tool for multichannel data analysis</article-title>,” <source>NeuroImage</source>, vol. <volume>98</volume>, pp. <fpage>487</fpage>–<lpage>505</lpage>, Sept. <year>2014</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>V. V.</given-names> <surname>Nikulin</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Nolte</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Curio</surname></string-name>, “<article-title>A novel method for reliable and fast extraction of neuronal EEG/MEG oscillations on the basis of spatio-spectral decomposition</article-title>,” <source>NeuroImage</source>, vol. <volume>55</volume>, pp. <fpage>1528</fpage>–<lpage>1535</lpage>, <month>Apr.</month> <year>2011</year>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>de Cheveigné</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Arzounian</surname></string-name>, “<article-title>Scanning for oscillations</article-title>,” <source>Journal of Neural Engineering</source>, vol. <volume>12</volume>, p. <fpage>066020</fpage>, <month>Dec.</month> <year>2015</year>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><given-names>M. X.</given-names> <surname>Cohen</surname></string-name>, “<article-title>Multivariate cross-frequency coupling via generalized eigendecomposition</article-title>,” <source>eLife</source>, vol. <volume>6</volume>, p. <fpage>e21792</fpage>, <month>Jan.</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><given-names>M. X.</given-names> <surname>Cohen</surname></string-name>, “<article-title>Using spatiotemporal source separation to identify prominent features in multichannel data without sinusoidal filters</article-title>,” <source>European Journal of Neuroscience</source>, vol. <volume>48</volume>, pp. <fpage>2454</fpage>–<lpage>2465</lpage>, <month>Oct.</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Yeung</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Bogacz</surname></string-name>, <string-name><given-names>C. B.</given-names> <surname>Holroyd</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Nieuwenhuis</surname></string-name>, and <string-name><given-names>J. D.</given-names> <surname>Cohen</surname></string-name>, “<article-title>Theta phase resetting and the error-related negativity</article-title>,” <source>Psychophysiology</source>, vol. <volume>44</volume>, pp. <fpage>39</fpage>–<lpage>49</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Haufe</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Meinecke</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Görgen</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dähne</surname></string-name>, <string-name><given-names>J.-D.</given-names> <surname>Haynes</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Blankertz</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Bießmann</surname></string-name>, “<article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title>,” <source>NeuroImage</source>, vol. <volume>87</volume>, pp. <fpage>96</fpage>–<lpage>110</lpage>, <month>Feb.</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="book"><string-name><given-names>N.</given-names> <surname>Wiener</surname></string-name>, <article-title>Nonlinear Problems in Random Theory</article-title>. Aug. <year>1966</year>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="book"><string-name><given-names>A. C.</given-names> <surname>Harvey</surname></string-name>, <source>Forecasting, Structural Time Series Models, and the Kalman Filter</source>. <publisher-loc>Cambridge; New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>, <year>1989</year>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><given-names>A. C.</given-names> <surname>Harvey</surname></string-name> and <string-name><given-names>T. M.</given-names> <surname>Trimbur</surname></string-name>, “<article-title>General Model-Based Filters for Extracting Cycles and Trends in Economic Time Series</article-title>,” <source>Review of Economics and Statistics</source>, vol. <volume>85</volume>, pp. <fpage>244</fpage>–<lpage>255</lpage>, <month>May</month> <year>2003</year>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="book"><string-name><given-names>R. N.</given-names> <surname>Bracewell</surname></string-name>, <source>The Fourier Transform and Its Applications</source>. <chapter-title>McGraw-Hill Series in Electrical and Computer Engineering</chapter-title>, <publisher-loc>Boston</publisher-loc>: <publisher-name>McGraw Hill</publisher-name>, <edition>3</edition>rd ed ed., <year>2000</year>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Matsuda</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Komaki</surname></string-name>, “<article-title>Time series decomposition into oscillation components and phase estimation</article-title>,” <source>Neural Computation</source>, vol. <volume>29</volume>, no. <issue>2</issue>, pp. <fpage>332</fpage>–<lpage>367</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="confproc"><string-name><given-names>A. M.</given-names> <surname>Beck</surname></string-name>, <string-name><given-names>E. P.</given-names> <surname>Stephen</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Purdon</surname></string-name>, “<article-title>State Space Oscillator Models for Neural Data Analysis</article-title>,” in <conf-name>2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</conf-name>, (Honolulu, HI), pp. <fpage>4740</fpage>–<lpage>4743</lpage>, IEEE, July <year>2018</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="preprint"><string-name><given-names>A. M.</given-names> <surname>Beck</surname></string-name>, <string-name><given-names>M.</given-names> <surname>He</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Gutierrez</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Purdon</surname></string-name>, <article-title>An iterative search algorithm to identify oscillatory dynamics in neurophysiological time series</article-title>, <source>bioRxiv</source>, <month>Nov.</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="book"><string-name><given-names>L. C.</given-names> <surname>Parra</surname></string-name>, “<chapter-title>Temporal models in blind source separation</chapter-title>,” in <source>Adaptive Processing of Sequences and Data Structures</source> (<string-name><given-names>J. G.</given-names> <surname>Carbonell</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Siekmann</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Goos</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Hartmanis</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Van Leeuwen</surname></string-name>, <string-name><given-names>C. L.</given-names> <surname>Giles</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Gori</surname></string-name>, eds.), vol. <volume>1387</volume>, pp. <fpage>229</fpage>–<lpage>247</lpage>, <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>, <year>1998</year>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><given-names>V. L.</given-names> <surname>Galinsky</surname></string-name> and <string-name><given-names>L. R.</given-names> <surname>Frank</surname></string-name>, “<article-title>Universal theory of brain waves: From linear loops to nonlinear synchronized spiking and collective brain rhythms</article-title>,” <source>Physical Review Research</source>, vol. <volume>2</volume>, p. 023061, <month>Apr.</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Matsuda</surname></string-name> and <string-name><given-names>F.</given-names> <surname>Komaki</surname></string-name>, “<article-title>Multivariate Time Series Decomposition into Oscillation Components</article-title>,” <source>Neural Computation</source>, vol. <volume>29</volume>, pp. <fpage>2055</fpage>–<lpage>2075</lpage>, <month>Aug.</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><given-names>A. J.</given-names> <surname>Quinn</surname></string-name>, <string-name><given-names>G. G.</given-names> <surname>Green</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Hymers</surname></string-name>, “<article-title>Delineating between-subject heterogeneity in alpha networks with Spatio-Spectral Eigenmodes</article-title>,” <source>NeuroImage</source>, vol. <volume>240</volume>, p. <fpage>118330</fpage>, <month>Oct.</month> <year>2021</year>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Neumaier</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Schneider</surname></string-name>, “<article-title>Estimation of parameters and eigenmodes of multivariate autoregressive models</article-title>,” <source>ACM Transactions on Mathematical Software</source>, vol. <volume>27</volume>, pp. <fpage>27</fpage>–<lpage>57</lpage>, <month>Mar.</month> <year>2001</year>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="book"><string-name><given-names>A.</given-names> <surname>Quinn</surname></string-name> and <string-name><given-names>V.</given-names> <surname>Šmídl</surname></string-name>, <source>The Variational Bayes Method in Signal Processing</source>. <chapter-title>Signals and Communication Technology Ser</chapter-title>, <publisher-loc>New York Boulder</publisher-loc>: <publisher-name>Springer NetLibrary, Inc</publisher-name>., <year>2006</year>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="confproc"><string-name><given-names>G. E.</given-names> <surname>Hinton</surname></string-name> and <string-name><given-names>D.</given-names> <surname>van Camp</surname></string-name>, “<article-title>Keeping the neural networks simple by minimizing the description length of the weights</article-title>,” in <conf-name>Proceedings of the Sixth Annual Conference on Computational Learning Theory - COLT ’93</conf-name>, (Santa Cruz, California, United States), pp. <fpage>5</fpage>–<lpage>13</lpage>, ACM Press, <year>1993</year>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="confproc"><string-name><given-names>H.</given-names> <surname>Attias</surname></string-name>, “<article-title>Inferring Parameters and Structure of Latent Variable Models by Variational Bayes</article-title>,” <source>Proceedings of the Fifteenth Conference on Uncertainity in Artificial Intelligence</source>, pp. <fpage>21</fpage>–<lpage>30</lpage>, July <year>1999</year>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="book"><string-name><given-names>R. M.</given-names> <surname>Neal</surname></string-name> and <string-name><given-names>G. E.</given-names> <surname>Hinton</surname></string-name>, “<chapter-title>A View of the EM Algorithm that Justifies Incremental, Sparse, and other Variants</chapter-title>,” in <source>Learning in Graphical Models</source> (<string-name><given-names>M. I.</given-names> <surname>Jordan</surname></string-name>, ed.), pp. <fpage>355</fpage>–<lpage>368</lpage>, <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Springer Netherlands</publisher-name>, <year>1998</year>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><given-names>A. P.</given-names> <surname>Dempster</surname></string-name>, <string-name><given-names>N. M.</given-names> <surname>Laird</surname></string-name>, and <string-name><given-names>D. B.</given-names> <surname>Rubin</surname></string-name>, “<article-title>Maximum Likelihood from Incomplete Data Via the <italic>EM</italic> Algorithm</article-title>,” <source>Journal of the Royal Statistical Society: Series B (Methodological)</source>, vol. <volume>39</volume>, pp. <fpage>1</fpage>–<lpage>22</lpage>, Sept. <year>1977</year>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="confproc"><string-name><given-names>Q.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Xu</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Fränti</surname></string-name>, “<article-title>Knee Point Detection on Bayesian Information Criterion</article-title>,” in <conf-name>2008 20th IEEE International Conference on Tools with Artificial Intelligence</conf-name>, (Dayton, OH, USA), pp. <fpage>431</fpage>–<lpage>438</lpage>, IEEE, Nov. <year>2008</year>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="confproc"><string-name><given-names>P.</given-names> <surname>Das</surname></string-name> and <string-name><given-names>P. L.</given-names> <surname>Purdon</surname></string-name>, “<article-title>Extracting common oscillatory time-courses from multichannel recordings: Oscillation Component Analysis</article-title>,” in <conf-name>2022 56th Asilomar Conference on Signals, Systems, and Computers</conf-name>, (Pacific Grove, CA, USA), pp. <fpage>602</fpage>–<lpage>606</lpage>, IEEE, Oct. <year>2022</year>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Luessi</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Larson</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Engemann</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Strohmeier</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Brodbeck</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Parkkonen</surname></string-name>, and <string-name><given-names>M. S.</given-names> <surname>Hämäläinen</surname></string-name>, “<article-title>MNE software for processing MEG and EEG data</article-title>,” <source>NeuroImage</source>, vol. <volume>86</volume>, pp. <fpage>446</fpage>–<lpage>460</lpage>, <month>Feb.</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><given-names>M. A.</given-names> <surname>Uusitalo</surname></string-name> and <string-name><given-names>R. J.</given-names> <surname>Ilmoniemi</surname></string-name>, “<article-title>Signal-space projection method for separating MEG or EEG into components</article-title>,” <source>Medical &amp; Biological Engineering &amp; Computing</source>, vol. <volume>35</volume>, pp. <fpage>135</fpage>–<lpage>140</lpage>, <month>Mar.</month> <year>1997</year>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><given-names>R. S.</given-names> <surname>Desikan</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Ségonne</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Fischl</surname></string-name>, <string-name><given-names>B. T.</given-names> <surname>Quinn</surname></string-name>, <string-name><given-names>B. C.</given-names> <surname>Dickerson</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Blacker</surname></string-name>, <string-name><given-names>R. L.</given-names> <surname>Buckner</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Dale</surname></string-name>, <string-name><given-names>R. P.</given-names> <surname>Maguire</surname></string-name>, <string-name><given-names>B. T.</given-names> <surname>Hyman</surname></string-name>, <string-name><given-names>M. S.</given-names> <surname>Albert</surname></string-name>, and <string-name><given-names>R. J.</given-names> <surname>Killiany</surname></string-name>, “<article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title>,” <source>NeuroImage</source>, vol. <volume>31</volume>, pp. <fpage>968</fpage>–<lpage>980</lpage>, July <year>2006</year>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><given-names>T.-W.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Girolami</surname></string-name>, and <string-name><given-names>T. J.</given-names> <surname>Sejnowski</surname></string-name>, “<article-title>Independent Component Analysis Using an Extended Infomax Algorithm for Mixed Subgaussian and Supergaussian Sources</article-title>,” <source>Neural Computation</source>, vol. <volume>11</volume>, pp. <fpage>417</fpage>–<lpage>441</lpage>, <month>Feb.</month> <year>1999</year>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><given-names>P. L.</given-names> <surname>Purdon</surname></string-name>, <string-name><given-names>E. T.</given-names> <surname>Pierce</surname></string-name>, <string-name><given-names>E. A.</given-names> <surname>Mukamel</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Prerau</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Walsh</surname></string-name>, <string-name><given-names>K. F.</given-names> <surname>Wong</surname></string-name>, <string-name><given-names>A. F.</given-names> <surname>Salazar-Gomez</surname></string-name>, <string-name><given-names>P. G.</given-names> <surname>Harrell</surname></string-name>, <string-name><given-names>A. L.</given-names> <surname>Sampson</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Cimenser</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ching</surname></string-name>, <string-name><given-names>N. J.</given-names> <surname>Kopell</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Tavares-Stoeckel</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Habeeb</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Merhar</surname></string-name>, and <string-name><given-names>E. N.</given-names> <surname>Brown</surname></string-name>, “<article-title>Electroencephalogram signatures of loss and recovery of consciousness from propofol</article-title>,” <source>Proc Natl Acad Sci U S A</source>, vol. <volume>110</volume>, pp. <fpage>E1142</fpage>–<lpage>51</lpage>, <month>Mar.</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><given-names>L. D.</given-names> <surname>Lewis</surname></string-name>, <string-name><given-names>V. S.</given-names> <surname>Weiner</surname></string-name>, <string-name><given-names>E. A.</given-names> <surname>Mukamel</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Donoghue</surname></string-name>, <string-name><given-names>E. N.</given-names> <surname>Eskandar</surname></string-name>, <string-name><given-names>J. R.</given-names> <surname>Madsen</surname></string-name>, <string-name><given-names>W. S.</given-names> <surname>Anderson</surname></string-name>, <string-name><given-names>L. R.</given-names> <surname>Hochberg</surname></string-name>, <string-name><given-names>S. S.</given-names> <surname>Cash</surname></string-name>, <string-name><given-names>E. N.</given-names> <surname>Brown</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Purdon</surname></string-name>, “<article-title>Rapid fragmentation of neuronal networks at the onset of propofol-induced unconsciousness</article-title>,” <source>Proceedings of the National Academy of Sciences</source>, vol. <volume>109</volume>, <month>Dec.</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Murphy</surname></string-name>, <string-name><given-names>M.-A.</given-names> <surname>Bruno</surname></string-name>, <string-name><given-names>B. A.</given-names> <surname>Riedner</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Boveroux</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Noirhomme</surname></string-name>, <string-name><given-names>E. C.</given-names> <surname>Landsness</surname></string-name>, <string-name><given-names>J.-F.</given-names> <surname>Brichant</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Phillips</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Massimini</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Laureys</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Tononi</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Boly</surname></string-name>, “<article-title>Propofol Anesthesia and Sleep: A High-Density EEG Study</article-title>,” <source>Sleep</source>, vol. <volume>34</volume>, pp. <fpage>283</fpage>–<lpage>291</lpage>, <month>Mar.</month> <year>2011</year>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Cimenser</surname></string-name>, <string-name><given-names>P. L.</given-names> <surname>Purdon</surname></string-name>, <string-name><given-names>E. T.</given-names> <surname>Pierce</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Walsh</surname></string-name>, <string-name><given-names>A. F.</given-names> <surname>Salazar-Gomez</surname></string-name>, <string-name><given-names>P. G.</given-names> <surname>Harrell</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Tavares-Stoeckel</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Habeeb</surname></string-name>, and <string-name><given-names>E. N.</given-names> <surname>Brown</surname></string-name>, “<article-title>Tracking brain states under general anesthesia by using global coherence analysis</article-title>,” <source>Proceedings of the National Academy of Sciences</source>, vol. <volume>108</volume>, pp. <fpage>8832</fpage>–<lpage>8837</lpage>, <month>May</month> <year>2011</year>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Bjorck</surname></string-name> and <string-name><given-names>G. H.</given-names> <surname>Golub</surname></string-name>, “<article-title>Numerical Methods for Computing Angles Between Linear Subspaces</article-title>,” <source>Mathematics of Computation</source>, vol. <volume>27</volume>, p. <fpage>579</fpage>, July <year>1973</year>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Wodeyar</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Schatza</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Widge</surname></string-name>, <string-name><given-names>U. T.</given-names> <surname>Eden</surname></string-name>, and <string-name><given-names>M. A.</given-names> <surname>Kramer</surname></string-name>, “<article-title>A state space modeling approach to real-time phase estimation</article-title>,” <source>eLife</source>, vol. <volume>10</volume>, p. <fpage>e68803</fpage>, Sept. <year>2021</year>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><given-names>M. X.</given-names> <surname>Cohen</surname></string-name>, “<article-title>Assessing transient cross-frequency coupling in EEG data</article-title>,” <source>Journal of Neuroscience Methods</source>, vol. <volume>168</volume>, pp. <fpage>494</fpage>–<lpage>499</lpage>, <month>Mar.</month> <year>2008</year>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Zhang</surname></string-name>, “<article-title>Permutation Mutual Information: A Novel Approach for Measuring Neuronal Phase-Amplitude Coupling</article-title>,” <source>Brain Topography</source>, vol. <volume>31</volume>, pp. <fpage>186</fpage>–<lpage>201</lpage>, <month>Mar.</month> <year>2018</year>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Martínez-Cancino</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Heng</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Delorme</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Kreutz-Delgado</surname></string-name>, <string-name><given-names>R. C.</given-names> <surname>Sotero</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Makeig</surname></string-name>, “<article-title>Measuring transient phase-amplitude coupling using local mutual information</article-title>,” <source>NeuroImage</source>, vol. <volume>185</volume>, pp. <fpage>361</fpage>–<lpage>378</lpage>, <month>Jan.</month> <year>2019</year>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Soulat</surname></string-name>, <string-name><given-names>E. P.</given-names> <surname>Stephen</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Beck</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Purdon</surname></string-name>, “<article-title>State space methods for phase amplitude coupling analysis</article-title>,” <source>Scientific Reports</source>, vol. <volume>12</volume>, p. <fpage>15940</fpage>, Sept. <year>2022</year>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="confproc"><string-name><given-names>A.</given-names> <surname>Perley</surname></string-name> and <string-name><given-names>T. P.</given-names> <surname>Coleman</surname></string-name>, “<article-title>A Mutual Information Measure of Phase-Amplitude Coupling using High Dimensional Sparse Models</article-title>,” in <conf-name>2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</conf-name>, (Glasgow, Scotland, United Kingdom), pp. 21–24, IEEE, July <year>2022</year>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Donoghue</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Haller</surname></string-name>, <string-name><given-names>E. J.</given-names> <surname>Peterson</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Varma</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Sebastian</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Noto</surname></string-name>, <string-name><given-names>A. H.</given-names> <surname>Lara</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Wallis</surname></string-name>, <string-name><given-names>R. T.</given-names> <surname>Knight</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Shestyuk</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Voytek</surname></string-name>, “<article-title>Parameterizing neural power spectra into periodic and aperiodic components</article-title>,” <source>Nature Neuroscience</source>, vol. <volume>23</volume>, pp. <fpage>1655</fpage>–<lpage>1665</lpage>, <month>Dec.</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="preprint"><string-name><given-names>V. S.</given-names> <surname>Weiner</surname></string-name>, <string-name><given-names>D. W.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Kahali</surname></string-name>, <string-name><given-names>E. P.</given-names> <surname>Stephen</surname></string-name>, <string-name><given-names>R. A.</given-names> <surname>Peterfreund</surname></string-name>, <string-name><given-names>L. S.</given-names> <surname>Aglio</surname></string-name>, <string-name><given-names>M. D.</given-names> <surname>Szabo</surname></string-name>, <string-name><given-names>E. N.</given-names> <surname>Eskandar</surname></string-name>, <string-name><given-names>A. F.</given-names> <surname>Salazar-Gomez</surname></string-name>, <string-name><given-names>A. L.</given-names> <surname>Sampson</surname></string-name>, <string-name><given-names>S. S.</given-names> <surname>Cash</surname></string-name>, <string-name><given-names>E. N.</given-names> <surname>Brown</surname></string-name>, and <string-name><given-names>P. L.</given-names> <surname>Purdon</surname></string-name>, <article-title>Propofol disrupts alpha dynamics in distinct thalamocortical networks underlying sensory and cognitive function during loss of consciousness</article-title>, <source>bioRxiv</source>, <month>Apr.</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="book"><string-name><given-names>P.</given-names> <surname>Mitra</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Bokil</surname></string-name>, <source>Observed Brain Dynamics</source>. <publisher-loc>Oxford ; New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>, <year>2008</year>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="book"><string-name><given-names>N. S.</given-names> <surname>Nise</surname></string-name>, <source>Control Systems Engineering</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>Wiley</publisher-name>, <edition>6</edition>th ed ed., <year>2011</year>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Gunasekaran</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Azizi</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Van Wassenhove</surname></string-name>, and <string-name><given-names>S. K.</given-names> <surname>Herbst</surname></string-name>, “<article-title>Characterizing endogenous delta oscillations in human MEG</article-title>,” <source>Scientific Reports</source>, vol. <volume>13</volume>, p. <fpage>11031</fpage>, July <year>2023</year>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="book"><string-name><given-names>B. D. O.</given-names> <surname>Anderson</surname></string-name> and <string-name><given-names>J. B.</given-names> <surname>Moore</surname></string-name>, <article-title>Optimal Filtering</article-title>. <publisher-loc>Mineola, NY</publisher-loc>: <publisher-name>Dover Publ</publisher-name>, dover ed., unabridged republ ed., <year>2005</year>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="book"><string-name><given-names>B.</given-names> <surname>Pearlmutter</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Parra</surname></string-name>, “<chapter-title>Maximum likelihood blind source separation: A context-sensitive generalization of ICA</chapter-title>,” in <source>Advances in Neural Information Processing Systems</source> (<string-name><given-names>M.</given-names> <surname>Mozer</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Jordan</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Petsche</surname></string-name>, eds.), vol. <volume>9</volume>, <publisher-name>MIT Press</publisher-name>, <year>1996</year>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Rehman</surname></string-name> and <string-name><given-names>D. P.</given-names> <surname>Mandic</surname></string-name>, <article-title>Multivariate empirical mode decomposition</article-title>, <source>Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</source>, vol. <volume>466</volume>, pp. <fpage>1291</fpage>–<lpage>1302</lpage>, <month>May</month> <year>2010</year>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="web"><string-name><given-names>C.</given-names> <surname>Brodbeck</surname></string-name>, <string-name><given-names>T. L.</given-names> <surname>Brooks</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Das</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Reddigari</surname></string-name>, and <string-name><given-names>J. P.</given-names> <surname>Kulasingham</surname></string-name>, “<article-title>Christianbrod-beck/Eelbrain: 0.37</article-title>.” <source>Zenodo</source>, Apr. <year>2022</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97107.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This method paper proposes a <bold>valuable</bold> Oscillation Component Analysis (OCA) approach, in analogy to Independent Component Analysis (ICA), in which source separation is achieved through biophysically inspired generative modeling of neural oscillations. The empirical evidence justifying the approach's advantage is <bold>solid</bold>. This work will be of interest to researchers in the fields of cognitive neuroscience, neural oscillation, and MEG/EEG.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97107.2.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The present paper introduces Oscillation Component Analysis (OCA), in analogy to ICA, where source separation is underpinned by a biophysically inspired generative model. It puts the emphasis on oscillations, which is a prominent characteristic of neurophysiological data.</p>
<p>Strengths:</p>
<p>Overall, I find the idea of disambiguating data-driven decompositions by adding biophysical constrains useful, interesting and worth pursuing. The model incorporates both a component modelling of oscillatory responses that is agnostic about the frequency content (e.g. doesn't need bandpass filtering or predefinition of bands) and a component to map between sensor and latent-space. I feel these elements can be useful in practice.</p>
<p>Weaknesses:</p>
<p>Lack of empirical support: I am missing empirical justification of the advantages that are theoretically claimed in the paper. I feel the method needs to be compared to existing alternatives.</p>
<p>Comments on the revised version: This concern has been addressed in the revised version.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97107.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Das</surname>
<given-names>Proloy</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8807-042X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>He</surname>
<given-names>Mingjian</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Purdon</surname>
<given-names>Patrick L</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The present paper introduces Oscillation Component Analysis (OCA), in analogy to ICA, where source separation is underpinned by a biophysically inspired generative model. It puts the emphasis on oscillations, which is a prominent characteristic of neurophysiological data.</p>
<p>Strengths:</p>
<p>Overall, I find the idea of disambiguating data-driven decompositions by adding biophysical constrains useful, interesting and worth-pursuing. The model incorporates both a component modelling of oscillatory responses that is agnostic about the frequency content (e.g., doesn’t need bandpass filtering or predefinition of bands) and a component to map between sensor and latent space. I feel these elements can be useful in practice.</p>
</disp-quote>
<p>Thank you for the positive evaluation!</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>Lack of empirical support: I am missing empirical justification of the advantages that are theoretically claimed in the paper. I feel the method needs to be compared to existing alternatives.</p>
</disp-quote>
<p>Thank you for bringing up this important issue.  We agree that a direct performance comparison would be important to demonstrate.  We performed additional analyses to compare OCA with ICA and one easy frequency domain exploratory technique in both simulated and real human data (see Section How does OCA compare to conventional approaches? and Supporting Text: Comparison of OCA to traditional approaches in experimental EEG data).  The results of the simulated data are shown in the revised Figure 3.  Although the slow and alpha oscillations in this simulation are statistically independent under the generative model, ICA identifies components that mix these independent signals, as one would expect based on the above discussion (i.e., all components are Gaussian).  Meanwhile, OCA is able to recover distinct slow and alpha components.  We repeated this analysis in real human EEG during propofol-induced unconsciousness and found a similar result where ICA produced components that mixed slow and alpha band signals whereas OCA identified distinct oscillatory components (see Figure S4.1).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Major</p>
<p>Theoretical justification. About the limitation of ICA In M/EEG, lines 24-28 seem to suggest that, almost by necessity (if Gaussianity approximately holds as argued), ICA doesn’t work on these modalities. But a body of work indicates that it does work to a reasonable extent, and that it is useful in practice; see <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/pdf/10.1073/pnas.1112685108?download=true">https://www.pnas.org/doi/pdf/10.1073/pnas.1112685108?download=true</ext-link>. How then this theoretical claim be reconciled with the empirical evidence suggesting otherwise? I am putting this as a major comment because the limitations of ICA are one of the main motivations for this work, so it needs to be well-justified.</p>
</disp-quote>
<p>Thanks for bringing this forward this important point and for suggesting the reference Brookes, et al. Their work actually supports our claim. In the fifth paragraph of the discussion section, Brookes, et al. states “ICA has been used previously and extensively for artifact rejection in MEG; however, its use in identification of oscillatory signals has remained limited. This limitation is likely due to its susceptibility to interference and the fact that amplitude-modulated oscillatory signals exhibit a largely Gaussian statistical distribution (and ICA relies on non-Gaussianity in recovered sources).” For this reason, they use the Hilbert envelope as the input to the ICA procedure rather than the original time-series. These Hilbert envelopes represent the instantaneous amplitude of neural oscillatory activity, i.e., they follow the amplitude modulation of the oscillatory activity. The method does not extract any oscillatory activity or disambiguate different oscillatory sources, but only assess the connectivity pattern within pre-defined bands, i.e., how different areas of the brain are harmonized through modulation of the oscillations or vice-versa inside those pre-defined bands. The paper did not show extracted independent time signals (tICs), focusing instead on the spatial pattern that these tICs activated. In that way, their use of ICA was totally justified.  Overall, our assessment of the limitations of ICA are very well aligned with Brookes, et al. We have added the against our claim in the introduction (see page 3 line 23) and revised the discussion section to refer to this paper (see page 21 lines 426-432).</p>
<disp-quote content-type="editor-comment">
<p>Empirical justification. The synthetic example is good, but I’m not quite sure what to make out of the real data examples. One can see reasonable spectra in the different bands and not-soeasy to interpret spatial topologies. But the main question is how OCA compares to more standard, easier approaches. Could the authors show explicitly how the benefits that were spelled out in the introduction/discussion manifest in practice, when compared to other methods?</p>
</disp-quote>
<p>Thank you for bringing up this important issue.  We agree that a direct performance comparison would be important to demonstrate. We performed additional analyses to compare OCA with ICA and one easy frequency domain exploratory technique in both simulated and real human data (see Section How does OCA compare to conventional approaches? and Supporting Text: Comparison of OCA to traditional approaches in experimental EEG data).  The results of the simulated data are shown in the revised Figure 3 in page 12. Although the slow and alpha oscillations in this simulation are statistically independent under the generative model, ICA identifies components that mix these independent signals, as one would expect based on the above discussion (i.e., all components are Gaussian).  Meanwhile, OCA is able to recover distinct slow and alpha components. We repeated this analysis in real human EEG during propofol-induced unconsciousness and found a similar result where ICA produced components that mixed slow and alpha band signals whereas OCA identified distinct oscillatory components (see Figure S4.1 in Supporting Text: Comparison of OCA to traditional approaches in experimental EEG data).</p>
<disp-quote content-type="editor-comment">
<p>Minor</p>
<p>&quot;a recently-described class of state-space models&quot; -&gt; of the three references, one is from the sixties, another from the eighties, and the last one is 21 years old. Is this really a recent idea?</p>
<p>Maybe rephrase &quot;recently-described&quot;, or else think of more recent references that bring something new?</p>
</disp-quote>
<p>We have amended the wording as suggested. (See page 4, line 53)</p>
<disp-quote content-type="editor-comment">
<p>Lines 72-74. It might be useful to unwrap in *intuitive* terms why the elements of this vector are closely related to the real and imaginary parts of the analytic signal.</p>
</disp-quote>
<p>Thanks for the helpful comment. The sentence now reads:</p>
<p>“These elements of this state vector traces out two time-series that maintains an approximate π/ 2 radian phase difference and therefore are closely related to the real and imaginary parts of an analytic signal…”. (See page 5, lines 72-75)</p>
<disp-quote content-type="editor-comment">
<p>Also, relatedly, I don’t seem to have access to the SI which is supposed to explain this. It doesn’t show up in the BiorXiv preprint either.</p>
</disp-quote>
<p>We are sorry to hear that. BiorXiv merges all the supporting information and posts them under the Supplementary Material.</p>
<disp-quote content-type="editor-comment">
<p>In Eq(1) should it be R(f) instead of R(2 \pi f / f_s) ?</p>
</disp-quote>
<p>Thank you for catching this typo.</p>
<disp-quote content-type="editor-comment">
<p>As I understand from lines 182-195, the input for the method is not channels but PCA components. Since R is learned, presumably the variance of the lower-order PCs (i.e. the latest elements of the diagonal of R) will estimated to be small. This, in turn, would make the likelihood to be heavily weighed on these components (because one basically divides their contribution by their variance). Would this potentially bias the estimation towards these lower-order PCs, at the expense of higher-order PCs. In a different context, this is shown here: <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008580">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008580</ext-link> Maybe it would be worth commenting on this?</p>
</disp-quote>
<p>We agree with reviewer’s initial observations but disagree with the assessment. Our loglikelihood calculation reweights the components appropriately to counter the weighting coming due to spatial whitening, thus negating the above-mentioned bias. The main contribution of the spatial whitening and PCA are to make the learning numerically stable, i.e., it does not encounter underflow or overflow in the iterative steps. We also note that this spatial whitening, and the PCA are also reverted at the end to obtain the spatial components and estimated noise covariance. So, as long as we use all the components with strictly positive variances, we will not bias the log-likelihood one way or other.</p>
</body>
</sub-article>
</article>