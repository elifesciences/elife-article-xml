<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101021</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101021</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101021.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Confidence over competence: Real-time integration of social information in human continuous perceptual decision-making</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4798-3054</contrib-id>
<name>
<surname>Schneider</surname>
<given-names>Felix</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a6">6</xref>
<email>fschneider@dpz.eu</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Calapai</surname>
<given-names>Antonino</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mundry</surname>
<given-names>Roger</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Báez-Mendoza</surname>
<given-names>Raymundo</given-names>
</name>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1165-4646</contrib-id>
<name>
<surname>Gail</surname>
<given-names>Alexander</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1814-4200</contrib-id>
<name>
<surname>Kagan</surname>
<given-names>Igor</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Treue</surname>
<given-names>Stefan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Cognitive Neuroscience Laboratory, German Primate Center</institution></institution-wrap>, <city>Goettingen</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Sensorimotor Group, German Primate Center</institution></institution-wrap>, <city>Goettingen</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Decision and Awareness Group, German Primate Center</institution></institution-wrap>, <city>Goettingen</city>, <country country="DE">Germany</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>Faculty of Biology and Psychology, Georg-August University</institution></institution-wrap>, <city>Goettingen</city>, <country country="DE">Germany</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Cognitive Ethology Laboratory, German Primate Center</institution></institution-wrap>, <city>Goettingen</city>, <country country="DE">Germany</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ehdmg18</institution-id><institution>Leibniz ScienceCampus Primate Cognition</institution></institution-wrap>, <city>Goettingen</city>, <country country="DE">Germany</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>Johann-Friedrich-Blumenbach Institute, Department for Primate Cognition, Georg-August University</institution></institution-wrap>, <city>Goettingen</city>, <country country="DE">Germany</country></aff>
<aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Department of Neurobiology, German Primate Center</institution></institution-wrap>, <city>Goettingen</city>, <country country="DE">Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kolling</surname>
<given-names>Nils</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Stem-cell and Brain Institute (SBRI), U1208 Inserm</institution>
</institution-wrap>
<city>Bron Cedex</city>
<country country="FR">France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-11-11">
<day>11</day>
<month>11</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-10-16">
<day>16</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101021</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-08-18">
<day>18</day>
<month>08</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-08-20">
<day>20</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.08.19.608609"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-11-11">
<day>11</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101021.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.101021.1.sa3">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101021.1.sa2">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.101021.1.sa1">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.101021.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Schneider et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Schneider et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101021-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Human perceptual decision-making is susceptible to social influences. To determine if and how individuals opportunistically integrate real-time social information about noisy stimuli into their judgment, we tracked perceptual accuracy and confidence in social (dyadic) and non-social (solo) settings using a novel continuous perceptual report (CPR) task with peri-decision wagering. In the dyadic setting, most participants showed a higher degree of perceptual confidence. In contrast, average accuracy did not improve compared to solo performance. Underlying these net effects, partners in a dyad exhibit mutual convergence of accuracy and confidence, benefiting less competent or confident individuals, at the expense of the better-performing partner. In conclusion, real-time social information asymmetrically shapes human perceptual decision-making, with most dyads expressing more confidence without a matching gain in overall competence.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Continuous psychophysics</kwd>
<kwd>perceptual confidence</kwd>
<kwd>dyadic interaction</kwd>
<kwd>social information</kwd>
<kwd>motion perception</kwd>
<kwd>perceptual accuracy</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/018mejw64</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>Project-ID 454648639 - SFB 1528 - Cognition of Interaction, subproject A01</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Figures and main text have been revised</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>To navigate dynamic and uncertain environments, people continuously gather and evaluate sensory evidence. This ongoing perceptual decision process is shaped not only by competence – the accuracy of percept – but also confidence: the belief in that accuracy, formed through metacognitive evaluation of current and past decisions. Confidence plays a pivotal role because it governs how individuals act on their perceptions, whether committing to a choice, revising it, or seeking more information (<xref ref-type="bibr" rid="c28">Fleming and Lau, 2014</xref>; <xref ref-type="bibr" rid="c38">Kepecs and Mainen, 2012</xref>; <xref ref-type="bibr" rid="c74">Yeung and Summerfield, 2012</xref>).</p>
<p>In social situations, people routinely adjust their decisions and confidence in response to others’ behavior, feedback, or consensus – even when their own sensory input remains unchanged (<xref ref-type="bibr" rid="c5">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="c8">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="c23">Esmaily et al., 2023</xref>; <xref ref-type="bibr" rid="c59">Pescetelli et al., 2016</xref>; <xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>). These adjustments reflect susceptibility to both informational and normative social influences (<xref ref-type="bibr" rid="c31">Frith and Singer, 2008</xref>; <xref ref-type="bibr" rid="c69">Takagaki and Krug, 2020</xref>; <xref ref-type="bibr" rid="c70">Terenzi et al., 2021</xref>; <xref ref-type="bibr" rid="c71">Toelch and Dolan, 2015</xref>; <xref ref-type="bibr" rid="c73">Van Den Bos et al., 2013</xref>). For instance, by integrating the perceptual report of a partner with one’s own subjective experience, the quality of perceptual judgments can be optimized and result in a collective benefit, but only under certain conditions (<xref ref-type="bibr" rid="c5">Bahrami et al., 2010</xref>, <xref ref-type="bibr" rid="c3">2012a</xref>; <xref ref-type="bibr" rid="c12">Baumgart et al., 2020</xref>). Social information can speed up decision-making by reducing exploration time, but it can also introduce biases and false beliefs (<xref ref-type="bibr" rid="c9">Bang and Frith, 2017</xref>). For example, social conformity biases information uptake towards majority choices (<xref ref-type="bibr" rid="c33">Germar et al., 2016</xref>; <xref ref-type="bibr" rid="c72">Toelch et al., 2018</xref>).</p>
<p>Expressions of confidence in particular have been shown to exert a major influence during social exchange. Judging competence of others is difficult because it requires performance monitoring over time, and accuracy and confidence measures generally covary (<xref ref-type="bibr" rid="c12">Baumgart et al., 2020</xref>; <xref ref-type="bibr" rid="c39">Khalvati et al., 2021</xref>; <xref ref-type="bibr" rid="c59">Pescetelli et al., 2016</xref>). Therefore, people often use confidence signals of a partner as a proxy for assessing competence of others (<xref ref-type="bibr" rid="c9">Bang and Frith, 2017</xref>). Furthermore, confidence signals could serve as a channel for social information transfer. For instance, the expressed confidence range can adapt to specific social partners to achieve more optimal group decisions (<xref ref-type="bibr" rid="c8">Bang et al., 2017</xref>). Access to choices coupled with the confidence reports of another player has been shown to asymmetrically modulate individual decisions: compared to solo performance, dyadic agreement caused greater improvement in accuracy and confidence levels than disagreement reduced these measures (<xref ref-type="bibr" rid="c59">Pescetelli et al., 2016</xref>).</p>
<p>While these and other studies have illuminated the interplay of individual decisions and social influences, they relied on paradigms that treat perceptual decisions and confidence judgments as discrete and sequential – limiting our understanding of how these highly intertwined processes unfold during continuous, real-time decision-making. First, stimuli are typically presented as serial, isolated events rather than ongoing, temporally correlated streams, which are more representative of real-world environments. Second, many studies employed trial-based, discrete paradigms in which perceptual decisions and confidence reports are temporally separated. This separation introduces the possibility that metacognitive judgments draw on information not available at the time of the perceptual experience itself (<xref ref-type="bibr" rid="c54">Navajas et al., 2016</xref>; <xref ref-type="bibr" rid="c74">Yeung and Summerfield, 2012</xref>). Third, decisions were constrained to binary alternatives, e.g., left vs right (<xref ref-type="bibr" rid="c23">Esmaily et al., 2023</xref>; <xref ref-type="bibr" rid="c40">Kiani and Shadlen, 2009</xref>) or first vs second interval (<xref ref-type="bibr" rid="c5">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>), precluding a more graded expression of perceptual experience. Fourth, most studies imposed rigid structure with individual choices preceding social exchange and joint decision-making, assessing interdependent dyadic performance (<xref ref-type="bibr" rid="c5">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="c8">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="c12">Baumgart et al., 2020</xref>). To understand how accuracy and confidence evolve during the course of the dynamic decision-making and how both factors are shaped by the availability of social information, novel experimental methods allowing access to continuous reports are needed.</p>
<p>Recent work began to demonstrate the influence of continuous information exchange during dyadic co-action, although still in a two-alternative and serial design. Dynamic, mutually visible confidence reports elicited greater increase of perceptual confidence during agreement and less reduction during disagreement, compared to static reports (<xref ref-type="bibr" rid="c61">Pescetelli and Yeung, 2020</xref>), and also resulted in a confidence alignment between partners (<xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>). Emergent continuous psychophysics approaches further capture dynamics of perception, quantifying sensorimotor and cognitive processes via continuous perceptual report (CPR) tasks (<xref ref-type="bibr" rid="c17">Bonnen et al., 2017</xref>, <xref ref-type="bibr" rid="c16">2015</xref>; <xref ref-type="bibr" rid="c36">Huk et al., 2018</xref>; <xref ref-type="bibr" rid="c67">Straub and Rothkopf, 2022</xref>). As CPRs can be displayed continuously, they are particularly useful to elucidate interactions that unfold over time, such as the integration of noisy sensory and social information.</p>
<p>Motivated by this goal, we investigated how real-time, unconstrained social information influences the continuous expression of perceptual accuracy and confidence during decision-making about ambiguous visual stimuli. We developed a continuous “peri-decision” wagering paradigm, allowing participants to signal their percept and associated confidence in real-time using simple visual cues observable by others. This contrasts with previous approaches that relied on post-decision confidence measures – such as numerical ratings (<xref ref-type="bibr" rid="c14">Boldt and Yeung, 2015</xref>; <xref ref-type="bibr" rid="c29">Fleming et al., 2010</xref>), post-decision wagering (<xref ref-type="bibr" rid="c53">Moreira et al., 2018</xref>; <xref ref-type="bibr" rid="c58">Persaud et al., 2007</xref>), or opt-out/decline response options (<xref ref-type="bibr" rid="c32">Gail et al., 2004</xref>; <xref ref-type="bibr" rid="c35">Hanks and Summerfield, 2017</xref>; <xref ref-type="bibr" rid="c39">Khalvati et al., 2021</xref>; <xref ref-type="bibr" rid="c40">Kiani and Shadlen, 2009</xref>; <xref ref-type="bibr" rid="c41">Komura et al., 2013</xref>; <xref ref-type="bibr" rid="c65">Smith, 1997</xref>). Notably, our “co-action” design did not enforce payoff interdependence between players: they were free to monitor and opportunistically incorporate their partner’s report, without being compelled to do so.</p>
<p>This approach allowed us to address three key questions: (1) Do humans express perceptual confidence in real-time? (2) Does social context influence both perceptual accuracy and confidence expression? (3) How does the competence and the confidence of a social partner shape the integration of perceptual and social evidence? We hypothesized that participants continuously adjust confidence based on moment-to-moment sensory evaluation and selectively integrate their partner’s cues to interpret ambiguous stimuli, resulting in more accurate and confident reports. We further predicted that social benefit will depend on relative competence, with lower-performing individuals gaining more from interaction with a more skilled partner.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>To assess the integration of social information, we developed a dynamic perceptual decision paradigm (<xref rid="fig1" ref-type="fig">Figure 1A,B</xref>, <xref rid="figs1" ref-type="fig">Supplementary Figure 1A</xref> and <xref ref-type="supplementary-material" rid="vid1">Supplementary Video 1</xref>, Continuous Perceptual Report task, ‘CPR’, see Methods and Glossary) that enables continuous “peri-decision” wagering on the accuracy of perceptual judgments about the direction of a noisy random dot pattern (RDP). The RDP direction and coherence changed frequently, resulting in successive “stimulus states” (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). Participants used a joystick to signal their perceived direction (angle) and confidence (joystick tilt, the deviation from the central position). Their task was to maximize the monetary reward score by following the direction of the RDP as accurately as possible, so that their cursor (partial circle, ‘response arc’) would overlap with occasionally presented small reward targets appearing in the direction of the coherent motion (<xref rid="figs1" ref-type="fig">Supplementary Figure 1B</xref>). Tilting the joystick away from the center shortened the arc length, making it harder to hit the target and obtain the reward. But at the same time, larger tilt could result in a larger reward, because the reward score was calculated as the product of report accuracy and tilt (but was zero if the response arc did not hit the target, <xref rid="fig1" ref-type="fig">Figure 1D</xref>). This reward scheme allowed us to elicit continuous peri-decision wagering, by awarding large rewards for accurate and confident reports, small rewards for less accurate and/or less confident reports, and omitting rewards for inaccurate and overconfident reports.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Continuous perceptual report task.</title>
<p><bold>(A)</bold> Experimental setup: Two participants sat in adjacent experimental booths. Subjects played a motion tracking game with a joystick either alone (‘solo’) or together with a partner (‘dyadic’, mixed order: see <xref rid="figs1" ref-type="fig">Supplementary Figure 1A</xref>). In dyadic experiments, subjects watched the shared visual stimulus on a screen. Joystick responses of both players as well as visual feedback were mutually visible in real-time. <bold>(B)</bold> Random dot pattern (‘RDP’) with circular aperture and blacked out central fixation area was continuously presented for intervals of about 1 minute. Subjects were instructed to look at the central fixation cross. Joystick-controlled cursors (‘response arcs’, color-coded) are located at the edge of the fixation area. The stimulus motion signal was predictive of the location of behaviorally relevant reward targets (gray disc). Alignment of the cursor arc with the target resulted in target collection (‘hit’) and reward. The joystick tilt was linked to size of the cursor: less tilt – wide; more tilt - narrow. <bold>(C)</bold> Example trace of stimulus motion signal and dyadic responses: the stimulus was frequently changing in motion direction and coherence level. Participants tracked the stimulus direction with the joystick to obtain rewards. Darker hues indicate less joystick tilt. Target presentation occurred in pseudorandom time intervals (see <xref rid="figs1" ref-type="fig">Supplementary Figure 1B</xref>). The visual and auditory feedback representing the reward score was provided after each target hit. <bold>(D)</bold> Top: Upon target hit, the reward score was based on joystick tilt and accuracy, individually for each player. Bottom: Each dot corresponds to the accuracy-tilt combination during target presentation. The shaded greyscale background illustrates the non-zero reward map. The non-shaded area denotes missed targets with no reward (reward score = 0). The accuracy and tilt responses of both dyadic players are summarized with the histograms (color-coded). Positive trend of reward scores, shown in <xref rid="figs1" ref-type="fig">Supplementary Figure 1C</xref>, indicates perceptual learning over time.</p></caption>
<graphic xlink:href="608609v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Participants played the game in different experimental settings: alone (<italic>solo)</italic> or simultaneously with a partner (<italic>dyadic</italic>). In dyadic experiments, the joystick direction and tilt of both players are continuously presented to both participants. Here, we define social information as the perceptual report of the partner.</p>
<sec id="s2a">
<title>Humans express perceptual confidence in real-time through peri-decision wagering</title>
<p>Confidence measures have been shown to scale with evidence accumulation time and perceptual performance (<xref ref-type="bibr" rid="c6">Balsdon et al., 2021</xref>, <xref ref-type="bibr" rid="c7">2020</xref>; <xref ref-type="bibr" rid="c40">Kiani and Shadlen, 2009</xref>). Our task design intends to capture perceptual confidence via real-time wagering behavior. In this section, we verify that participants used peri-decision wagering while playing the CPR game.</p>
<p>More accurate participants showed more joystick tilt (Pearson’s correlation of average accuracy and tilt, n = 38 subjects, r = 0.62, p&lt;0.0001). Participants increased their hit rate (fraction of successfully acquired targets) and joystick tilt, while reducing their angular error (i.e., improving accuracy), with higher motion coherence (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). Their response lag after an RDP direction change was on average 643 ms ± 79 ms (Mean ± IQR, coherence pooled; <xref rid="fig2" ref-type="fig">Figure 2B-C</xref>, <xref rid="figs1" ref-type="fig">Supplementary Figure 1D</xref>), with higher motion coherence causing shorter stimulus following responses. Low RDP coherence resulted in a breakdown of motion tracking, increasing the variance of cross-correlation peaks (<xref rid="figs1" ref-type="fig">Supplementary Figure 1D</xref>). Participants also increased their tracking accuracy and joystick tilt within individual stimulus states, except at very low coherences (<xref rid="fig2" ref-type="fig">Figure 2D</xref>, Linear regression of the average time course in each participant, Accuracy: Mean slope = 4.8835e-04; Wilcoxon signed rank test against zero: n = 38 subjects, Z = 5.3731, p &lt; 0.0001; Tilt: Mean slope = 1.0366e-04; Wilcoxon signed rank test against zero: n = 38 subjects, Z = 3.2993, p &lt; 0.001), indicating a continuous evidence accumulation during stable stimulus epochs. Changes in accuracy preceded changes in joystick tilt (<xref rid="fig2" ref-type="fig">Figure 2E</xref>, Cross-correlation, Mean lag = 339.9 ms, Wilcoxon signed rank test against zero: n = 38 subjects, Z = 5.3733, p &lt; 0.0001). These patterns reflect the dynamics of evidence integration, with faster and more reliable responses driving higher levels of confidence.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Solo behavior during the continuous perceptual report.</title>
<p><bold>(A)</bold> Coherence-dependent modulation of hit rates (left), accuracy (center, shown with absolute, target-aligned angular joystick error as dashed blue line) and state-aligned radial joystick tilt (right). Gray lines show averages for individual participants. Red shading illustrates the 99% confidence intervals of the mean across participants. Bold, black lines show the mean across the population. Data were first averaged within-subject, before pooling coherence conditions across-subjects. <bold>(B)</bold> Estimation of the response lag after a stimulus direction change with a cross-correlation between stimulus and joystick signal for an example subject (dashed lines). Low coherence levels resulted in a breakdown of response reliability, indicated by the low cross-correlation coefficients (see also <xref rid="figs1" ref-type="fig">Supplementary Figure 1D</xref>). Here and in other panels, stimulus coherence is color-coded. <bold>(C)</bold> Top: average population response lag. Black data points show individual data; white dot displays population median. Bottom: variability of the population lags, displayed by the standard deviation. <bold>(D)</bold> Average time course of joystick accuracy (left) and tilt (right) during a stable stimulus period aligned to the next stimulus direction change at time point 0, across all subjects. Here, the first 500 ms of each stimulus state as well as all samples after the first target appearance were excluded before averaging. The shaded background illustrates the 95% confidence interval of the mean. We calculated the slopes of the average time course and tested if they were significantly different from zero across subjects (n = 38, Bonferroni-corrected). Statistics is illustrated with color-coded triangles, indicating which coherence condition has a significant slope and in which direction (triangles point up for positive slopes). <bold>(E)</bold> Cross-correlation between the mean-detrended time course of joystick tilt and accuracy, indicating that changes in tilt follow changes in accuracy within half a second. Peak cross-correlation coefficients are marked with a dashed line. Statistics is illustrated with the color-coded triangles, indicating which coherence condition has a significantly shifted from 0 cross-correlation peak across subjects (n = 38, Bonferroni-corrected).</p></caption>
<graphic xlink:href="608609v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Despite high inter-subject variability, we found that motion coherence had a robust impact on all behavioral response measures: joystick accuracy, tilt and hit rates (Linear mixed effects models – see <xref rid="tbls1" ref-type="table">Supplementary Table 1</xref> to <xref rid="tbls4" ref-type="table">Supplementary Table 4</xref>). This suggests that participants adapted their responses to varying stimulus difficulty. We further demonstrate that hit rates, while generally increasing with motion coherence, dropped for 24 of 38 participants (63%) during the most salient RDP coherence, suggesting overconfident or risk-seeking joystick placement.</p>
<p>To directly assess if subjects used joystick tilt as a proxy for perceptual confidence, we adapted the metacognitive performance analysis of the area under the receiver operating characteristics curve (AUC) for confidence ratings to continuous joystick responses (<xref ref-type="bibr" rid="c28">Fleming and Lau, 2014</xref>; <xref ref-type="bibr" rid="c47">Maniscalco and Lau, 2014</xref>, <xref ref-type="bibr" rid="c48">2012</xref>). We used the distributions of joystick response measurements to infer whether there is a relationship between response accuracy and tilt, separately for each RDP coherence level. To that end, we median-split the joystick responses into high and low tilt distributions. We then analyzed the AUC to quantify whether the accuracy of these distributions was different. High accuracy was, indeed, related to more joystick tilt, suggesting an ongoing metacognitive assessment of the perceptual report that is reflected in the tilt of the joystick (<xref rid="figs2" ref-type="fig">Supplementary Figure 2</xref>). Thus, participants optimized joystick placement in both accuracy and tilt, which provides evidence for real-time peri-decision wagering and links joystick tilt to perceptual confidence. Importantly, we ruled out that the short-term fluctuations of instantaneous coherence or random differences in average coherence between stimulus states could explain the variations in joystick tilt within each nominal coherence level. Furthermore, the gradual tilt increase within stable stimulus epochs indicates that this response dimension was dissociated from the fluctuations of motion coherence (<xref rid="fig2" ref-type="fig">Figure 2D</xref>).</p>
<p>In summary, solo CPRs indicate that participants actively wager on their own percept. As joystick tilt was the only response dimension that could be chosen freely via metacognitive assessment of the current and past decisions, we treat it as a proxy measure of subjective perceptual confidence. Usage and range of this response parameter varied widely between participants, suggesting individual confidence ranges. These findings indicate that our CPR game makes it possible to continuously assess participants’ perceptual processes and associated confidence. Next, we examine whether and to what extent participants incorporated the perceptual report of a second player into their own decision-making.</p>
</sec>
<sec id="s2b">
<title>Social setting changes perceptual accuracy and confidence during real-time decision-making</title>
<p>Previous studies have shown that, under certain conditions, two participants are more successful in perceptual decision-making than the more competent player on its own (<xref ref-type="bibr" rid="c4">Bahrami et al., 2012b</xref>; <xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>). We asked whether participants performed the CPR task better when a second player was playing along, and if so, whether differences between participants in perceptual competence or confidence were the driving forces behind any improvement. Both participants reacted to the same RDP and could observe both cursors and feedback of immediate and cumulative scores for themselves and the other player (see Methods). Importantly, the task did not enforce a competitive or cooperative context – the individual payoffs were independent. Participants could freely choose whether to use or ignore the perceptual report of the other player.</p>
<p>We pooled all experimental sessions of each participant according to social context (solo vs dyadic, within-subject). Average response lags after a direction change were significantly different in solo and dyadic experiments (Solo: 643 ms ± 79 ms [Mean ± IQR]; Dyadic: 662 ms ± 105 ms; Wilcoxon signed rank test, n = 34, Z = −2.98, p &lt; 0.01). We also found a small but significant improvement in average individual score between solo and dyadic experiments (Solo: 0.236 ± 0.07 [Mean ± IQR]; Dyadic 0.247 ± 0.05; Wilcoxon signed rank test, n = 34 (subjects), Z = 2.31, p &lt; 0.05). Compared to the solo CPR, 68% of participants (23/34) achieved a higher score when co-acting with a partner (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). On the level of a dyad, the nominal combined score of two co-acting players was higher than that of the same two players working alone in 72% of dyads (Mean difference = 0.023, Wilcoxon signed rank test, n = 50 (dyads), Z= −3.36, p&lt;0.001). Thus, social context seemed to improve overall task outcome as measured by the reward score, although it was not due to an increased number of collected targets (Hit rate Solo: 0.389 ± 0.105 [Mean ± IQR]; Dyadic: 0.391 ± 0.064; Wilcoxon signed rank test, n = 34 (subjects), Z = 0.6582, p = 0.51).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Social modulation: dyadic vs solo.</title>
<p><bold>(A)</bold> Reward score in dyadic vs solo sessions. All solo and (human-human) dyadic sessions were pooled within-subject. Inset: coherence-wise averaging of reward scores. Here and in other panels, stimulus coherence is color-coded. Each subject contributes one data point per stimulus coherence level. The median score across all subjects for each coherence condition is overlaid in brighter color hues. Error bars show 99% confidence intervals of the median in solo and dyadic conditions. <bold>(B)</bold> Social modulation between solo and dyadic experiments, measured as AUC, for state-aligned accuracy (top) and confidence (i.e., tilt, bottom, Wilcoxon rank sum test, Bonferroni-corrected) of individual participants. Coherence was pooled within-subjects. A value of 0.5 corresponds to perfect overlap between solo and dyadic response distributions, 1 and 0 imply perfect separation. See <xref rid="figs3" ref-type="fig">Supplementary Figure 3A</xref> for average performance in dyadic experiments and <xref rid="figs3" ref-type="fig">Supplementary Figure 3C</xref> for examples of social modulation and how the AUC captures the its directionality. <bold>(C)</bold> Performance-dependent social modulation. Schematic: social modulation can increase performance (values above the horizontal dashed line, green) or decrease it (values below the horizontal dashed line, red). First column: statistical comparison between joystick accuracy and confidence in solo and dyadic experiments, for each coherence. Sessions were pooled according to experimental condition within-subject. The percentage of participants with significantly different distribution in solo and dyadic sessions is displayed (Wilcoxon rank sum test, Bonferroni-corrected). The directionality of the significant effect in each subject was established with the AUC. Average AUC per coherence level is shown in gray with 99% confidence intervals (shaded background). Second column: average social modulation displayed for different solo performance quartiles, separately for each corresponding performance measure (hit rate, accuracy, confidence). See <xref rid="figs3" ref-type="fig">Supplementary Figure 3B</xref> for comparison of raw solo joystick responses with social modulation, and <xref rid="figs4" ref-type="fig">Supplementary Figure 4</xref> for quartile grouping across response dimensions.</p></caption>
<graphic xlink:href="608609v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we assessed whether changes in perceptual accuracy or confidence were the driving factors behind reward score improvements. To account for the full distributions of accuracy and confidence, we used an AUC analysis to compare performance between solo and dyadic conditions in each subject. Average response accuracy in dyadic experiments did not change for 53% (18/34) of participants and did decline for most of the others (<xref rid="fig3" ref-type="fig">Figure 3B</xref>; within subjects: Wilcoxon rank-sum test, coherence pooled, number of tests: 34 (subjects), Bonferroni-corrected significance threshold = 0.0015; across subjects: Wilcoxon signed rank test, Z = - 2.25, p &lt; 0.05, Median AUC = 0.485). Thus, the access to reports of others did not improve average CPR competence. However, the social setting altered the confidence reports in 94% (32/34) of participants (<xref rid="fig3" ref-type="fig">Figure 3B</xref>; within subjects: Wilcoxon rank-sum test, coherence pooled, number of tests: 34 (subjects), Bonferroni-corrected significance threshold = 0.0015; across subjects: Wilcoxon signed rank test, Z = 2.48, p &lt; 0.05, Median AUC = 0.57).</p>
<p>In participants with significantly different confidence in solo versus dyadic conditions, 41% - 59% (min and max across coherence levels) wagered more aggressively on their percept when playing with a partner, while 12% - 26% wagered more conservatively (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, first column; Wilcoxon rank-sum test, number of tests: 34 (subjects) * 7 (coherence levels) = 238, Bonferroni-corrected significance threshold = 2.1008e-04). Consequently, by signaling more perceptual confidence on their perceptual report in a social setting, most participants achieved a better score.</p>
<p>To understand how social modulation in each participant is influenced by their initial competence or confidence, we sorted the subjects into quartiles based on their solo performance: hit rate, accuracy, and confidence. Generally, participants with low individual performance tended to improve, whereas high-performing ones showed a decline. Initially, less confident participants increased their wagers, while initially more accurate individuals declined in accuracy (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, second column, see also <xref rid="figs4" ref-type="fig">Supplementary Figure 4</xref>). Notably, the magnitude of social modulation showed an asymmetry, both with respect to response dimensions and when comparing gains vs losses within each dimension. The least accurate participants improved only slightly, while the most accurate declined more; conversely, the least confident participants gained a lot of confidence, while the most confident lost very little.</p>
<p>These patterns might be explained by convergence between participants, which was shown to impact perceptual accuracy of social decisions (<xref ref-type="bibr" rid="c8">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="c46">Mahmoodi et al., 2015</xref>; <xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>). To directly test the convergence hypothesis, we contrasted the absolute confidence and accuracy differences between the two players in dyadic vs solo settings (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Compared to solo experiments, 76% of dyads (38/50) exhibited a smaller difference in confidence when playing together (Wilcoxon signed rank test, n = 50, Z = 3.66, p &lt; 0.001). Similarly, 64% of dyads (32/50) displayed a smaller difference in accuracy (Wilcoxon signed rank test, n = 50, Z = 2.52, p &lt; 0.05). Furthermore, we compared the confidence and the accuracy correlations between the two players, in solo and dyadic contexts. As expected, there was no correlation in solo reports, but the participants’ confidence became significantly correlated when they played together (<xref rid="figs5" ref-type="fig">Supplementary Figure 5</xref>, Pearson’s correlation, n = 50, Accuracy: Solo: r = −0.04, p = 0.76, Dyadic: r = 0.21, p = 0.14; Confidence: Solo: r = 0.11, p = 0.44, Dyadic: r = 0.54, p &lt; 0.001).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Social modulation in human-human dyads.</title>
<p>Left column: schematic depiction of hypothesized effects; middle and right column: actual data for state-aligned confidence and accuracy. <bold>(A)</bold> Absolute difference between partners in solo and dyadic setting for confidence (middle) and accuracy (right). Each dyad is represented by one data point. Dyads show convergence when differences between players in dyadic setting are smaller than differences in solo experiments (see left schematic). <bold>(B)</bold> Social modulation difference between players (better in solo minus worse in solo) as a function of better minus worse inter-player performance difference, for confidence (middle) and accuracy (right). Each dyad is represented by one data point. The solid line illustrates the correlation between solo difference and social modulation difference (Linear regression, n = 50, Confidence: r = −0.535, p &lt; 0.0001; Accuracy: r = −0.26, p = 0.068; Converging dyads only (n = 40): Confidence: r = −0.483, p &lt; 0.01; Accuracy: r = −0.474, p &lt; 0.01). <bold>(C)</bold> Social modulation displayed separately for better and worse solo players. Dyads are connected with a colored line (blue: convergence; red: divergence). Histograms show the overall distribution of social modulations across all participants. Means of the distributions are illustrated with a colored line.</p></caption>
<graphic xlink:href="608609v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>Performance difference between participants determines social effect</title>
<p>The previous analysis, across subjects, indicated that the participants’ solo performance is an important determinant of how their performance will change in dyadic setting. We next asked whether the amplitude and the direction of social modulation in each participant – summarized with AUC – can be explained by within-dyad differences in solo performance. Intuitively, we hypothesized that a larger difference in solo performance between subjects would lead to a stronger convergence, because little additional information could be derived from observing a similar partner (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). Theoretically, when considering the better and the worse solo player in a dyad, both could improve in the dyadic setting (AUC &gt; 0.5), both could worsen (AUC &lt; 0.5), or one can improve while the other worsen (here and elsewhere, “worse”/“better” mean less/more confident/accurate, correspondingly).</p>
<p>Depending on the relative strength of the change, this could lead to dyadic convergence or divergence. Confirming the previous result across subjects, 40/50 dyads showed convergence, in both confidence and accuracy. In line with our prediction, we observe stronger convergence between dyadic participants with larger solo difference, indicated by the negative correlation between the better vs worse inter-player AUC difference and the solo performance difference (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). The initially worse players could exhibit a more beneficial social modulation, relative to the better player, by either improving more or getting less worse (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, left). Indeed, the initially less confident players improved relative to their counterparts (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, middle, Wilcoxon signed rank test, n = 50, Z = −4.18, p &lt; 0.001); conversely, less accurate players were barely affected by the social context while their counterparts got worse (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, right, Wilcoxon signed rank test, n = 50, Z = −4.35, p &lt; 0.001). This asymmetric pattern resulted in an overall positive shift for confidence and slight overall negative shift for accuracy (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, histograms, Wilcoxon signed rank test against 0.5, n = 100; Confidence: Median AUC = 0.5752, Z = 2.80, p &lt; 0.01; Accuracy: Median AUC = 0.486, Z = −3.55, p &lt; 0.001). Importantly, even though we analyzed the confidence and the accuracy separately, there was a strong overlap between these two measures within a dyad: the initially more confident player was also the more accurate one in 43/50 (86%) dyads. This allows interpreting the plots in <xref rid="fig4" ref-type="fig">Figure 4C</xref> across panels: e.g. not only less confident but also less <italic>accurate</italic> participants tend to gain <italic>confidence</italic> (see also <xref rid="figs4" ref-type="fig">Supplementary Figure 4</xref>).</p>
<p>In contrast to earlier studies that found more successful perceptual decision-making in social settings when perceptual sensitivities or confidence match between participants (<xref ref-type="bibr" rid="c5">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>), our data does not reveal systematic “dyadic benefits” for dyads with similar perceptual accuracy or confidence. Instead, there was a positive (but not significant) relationship between the average social modulation within a pair and solo difference between players (<xref rid="figs6" ref-type="fig">Supplementary Figure 6</xref>).</p>
</sec>
<sec id="s2d">
<title>Perceptual accuracy improves with reliable social signaling</title>
<p>As expected, the quality of the solo perceptual report declined in a comparable fashion across participants for low stimulus coherence (<xref rid="fig2" ref-type="fig">Figure 2</xref>). We wondered if this perceptual breakdown led to the relatively small accuracy modulation by the social context we described above (<xref rid="fig3" ref-type="fig">Figure 3</xref> and <xref rid="fig4" ref-type="fig">Figure 4</xref>). Based on earlier work on Bayesian integration and social conformity (<xref ref-type="bibr" rid="c20">De Martino et al., 2017</xref>; <xref ref-type="bibr" rid="c33">Germar et al., 2016</xref>; <xref ref-type="bibr" rid="c39">Khalvati et al., 2021</xref>; <xref ref-type="bibr" rid="c57">Park et al., 2017</xref>), we expected that integrating information from a partner will be weighted by their reports’ accuracy reliability. We hypothesized that participants would integrate more social evidence when it was reliably accurate, regardless of the stimulus noise. Furthermore, we asked whether incorporating social signals into human decision-making requires graded, accuracy-depended confidence signaling by others.</p>
<p>To that end, we developed a computer player, that was programed to accurately represent the nominal RDP direction (± Gaussian noise; note that even 0% coherence had a correct “nominal” direction), with a fixed “confidence” of 0.5 (± Gaussian noise in a.u.) across all coherence levels at all times (<xref rid="figs7" ref-type="fig">Supplementary Figure 7</xref>). In such human-computer (HC) dyads, the computer player was physically impersonated by one of the experimenters who pretended to be the partner. Thus, participants believed that they played the game with another human. The computer player was set up to report motion direction with a constant, human-like latency (508 ms ± Gaussian noise). The computer response was not affected by the cursor of the human participants, resulting in a situation where the social signals might only unilaterally affect the human player. Crucially, unlike real human partners, the computer player did not provide useful information regarding its tracking confidence. With this condition we aimed to evaluate whether human participants would integrate social cues about the motion direction into their own reports, while the partner’s confidence report was uninformative. We nevertheless expected a response accuracy improvement, especially when sensory evidence became degraded at low coherences. Furthermore, we hypothesized that the reliable nature of the computer partner would result in riskier, more eccentric joystick placement in human players.</p>
<p>By accurately representing the stimulus direction, the computer player triggered profound behavioral effects (<xref rid="tbls2" ref-type="table">Supplementary Table 2</xref> to <xref rid="tbls4" ref-type="table">Supplementary Table 4</xref>). In this setting, participants collected more targets with a higher score (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). Compared to human-human dyads, 63% of participants (21/33) improved their accuracy (and only 3 participants became worse) across all coherence levels (<xref rid="fig5" ref-type="fig">Figure 5B-C</xref>, within-subjects: Wilcoxon rank-sum test, coherence pooled, number of tests = 33 (subjects), Bonferroni-corrected significance threshold = 0.0015; across subjects: Wilcoxon signed rank test, Median AUC = 0.56, Z = 3.87, p &lt; 0.001; see <xref rid="figs8" ref-type="fig">Supplementary Figure 8</xref> for comparison to solo behavior). In particular, 55% of participants (18/33) showed a significant accuracy boost at 0% coherence (Wilcoxon rank-sum test, number of tests: 33 (subjects) * 7 (coherence levels) = 231, Bonferroni-corrected significance threshold = 2.1645e-04, Median AUC across subjects at 0% coherence = 0.66). Thus, participants integrated reliable sensory-social direction cues to improve their task performance, especially when stimulus was ambiguous, suggesting a unilateral convergence towards the computer player.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Comparison of social modulation in human-human (HH) dyads and human-computer (HC) dyads.</title>
<p><bold>(A)</bold> Left: average subject-wise score in the two dyadic experiments compared to the score of the same participant in solo experiments. Average score (middle) and hit rate (right) of the population in different experimental conditions. Error bars correspond to the 99% confidence intervals of the mean. <bold>(B)</bold> Comparison of average hit rate (top), accuracy (middle) and confidence (bottom) for each participant and each stimulus coherence (color-coded) in human-human and human-computer dyads. Individual data (subject-wise, averaged across several HH sessions for each subject) are shown in darker hue. Medians are overlaid for each coherence condition with bright colors. Error bars show 99% confidence intervals of the median. <bold>(C)</bold> Statistical comparison of accuracy and confidence in HH and HC dyadic experiments, for each coherence condition. Sessions were pooled according to experimental condition within-subject. The percentage of participants with significantly different distribution in HH vs HC dyadic sessions is displayed (Wilcoxon rank sum test, Bonferroni-corrected). The directionality of the significant effect in each subject was established with the AUC. Average AUC per coherence level is shown in gray with 99% confidence intervals (shaded background). For hit rates, the average difference is displayed.</p></caption>
<graphic xlink:href="608609v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Despite more accurate task performance, most participants did not improve in the confidence dimension (<xref rid="fig5" ref-type="fig">Figure 5B-C</xref>). In fact, compared to an interaction with a real human counterpart, 64% of participants showed less confidence while only 18% improved when playing with the computer player (within-subjects: Wilcoxon rank-sum test, coherence pooled, number of tests = 33 (subjects), Bonferroni-corrected significance threshold = 0.0015, across subjects: Wilcoxon signed rank test, Median AUC = 0.45, Z = −3.15, p &lt; 0.01). Subjects were particularly affected when the task was easy (98% coherence, Median AUC across subjects = 0.36). This too seems to suggest confidence convergence towards the relatively invariant low confidence computer player, even with otherwise reliably accurate direction signaling.</p>
</sec>
<sec id="s2e">
<title>Social modulation of confidence and accuracy co-varies</title>
<p>So far, we analyzed the social modulation of perceptual confidence and accuracy independently. Here we investigate the link between the two response dimensions. In both dyadic conditions, subject-wise social modulation of perceptual confidence correlated with the change in accuracy (<xref rid="fig6" ref-type="fig">Figure 6</xref>, HH: Pearson’s correlation, n = 100, r = 0.76, p &lt; 0.001; HC: n = 33, r = 0.56, p &lt; 0.001), suggesting that the gain or the loss in accuracy leads to reappraisal of confidence. Note however that a substantial fraction of participants had an incongruent social modulation, showing <italic>less</italic> accuracy but <italic>more</italic> confidence (<xref rid="fig6" ref-type="fig">Figure 6</xref>, HH, upper left quadrant).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Relationship between social modulation of accuracy and confidence in each participant.</title>
<p>Each dot represents one player in a dyad. Left: Social modulation in human-human (HH) dyadic condition vs. solo (n = 100); Middle: Human-Computer (HC) dyadic condition vs solo (n = 33); Right: Correlation of confidence vs accuracy difference between two dyadic conditions (HC vs HH, n = 98). Values above 0.5 on each axis correspond to positive social modulation: increased accuracy or confidence in the “first condition” (e.g. HH) compared to the “second condition” (e.g. solo), and vice versa.</p></caption>
<graphic xlink:href="608609v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Despite the positive covariation between social modulation of accuracy and confidence, the human–computer experiment demonstrated that social evidence integration does not require metacognitively sensitive, graded confidence signaling by the partner. However, the apparent dissociation between the <italic>average</italic> improvement of accuracy and the <italic>average</italic> decline of confidence is still grounded in a lawful relationship between two response dimensions. Players who gained substantially more accuracy tended to show confidence increase, or at least less confidence decrease, compared to players who did not change much in accuracy (<xref rid="fig6" ref-type="fig">Figure 6</xref>, HC vs HH: n = 98, r = 0.62, p &lt; 0.001). To conclude, both, individually-varied, coherence-dependent human reports, and reliably accurate direction reports coupled with uninformative confidence expression by the simulated partner, influenced human perceptual decisions and resulted in converging, socially conforming behavior.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we assessed continuous human perceptual decision-making in individual and social settings, with a newly developed paradigm, where subjects wager on the correctness of their motion percept in real-time. Overall, during dyadic co-action, we find higher perceptual confidence but no gain in accuracy. We demonstrate that asymmetric convergence underlies this net effect, with the magnitude and directionality of the social modulation depending on the difference between competence and confidence of the partners.</p>
<p>In contrast to the increase in confidence in the dyadic condition we have observed, previous social perceptual decision studies did not report overall rise in confidence (<xref ref-type="bibr" rid="c8">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="c59">Pescetelli et al., 2016</xref>; <xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>). At the same time, some earlier work demonstrated gains in dyadic competence – i.e. when the group decision-making outperforms even the better of two partners – after partners exchanged their individual confidence and made a joint choice (<xref ref-type="bibr" rid="c5">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="c59">Pescetelli et al., 2016</xref>). Likewise, competence increases even during dyadic co-action – without interdependent payoff or joint decision – especially for participants with similar confidence (<xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>). But despite presenting subjective confidence continuously and saliently as part of the perceptual report, our task did not elicit overall – across all participants – improvement in accuracy or hit rate. The resulting score (which combines accuracy, hit rate, and confidence), however, did improve in the dyadic setting, reflecting a dyadic benefit driven by an overall increase in confidence. This indicates that real-time social feedback boosts confidence in one’s perception, even in the absence of a corresponding enhancement in competence. Such a disconnect between the effects on confidence (which can be construed as subjectively perceived competence) and actual competence is reminiscent of previous work on metacognitive biases in individuals (<xref ref-type="bibr" rid="c42">Kruger and Dunning, 1999</xref>): many less competent participants showed socially-induced increase in confidence without an associated increase – or even with a decrease – in accuracy.</p>
<p>These effects were driven by a mutual convergence between dyadic partners along both the perceptual accuracy and the confidence dimensions. For each participant, the direction and the magnitude of social modulation was determined by the difference in the initial performance of the two players, with larger solo differences between participants resulting in larger difference in social modulation between the partners. Critically, dyadic convergence affects both partners, but often asymmetrically. For instance, the more confident players on average show very little change in confidence but they strongly boost the confidence of their partners. Dyadic confidence convergence (<xref ref-type="bibr" rid="c23">Esmaily et al., 2023</xref>; <xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>) and confidence matching (<xref ref-type="bibr" rid="c8">Bang et al., 2017</xref>) have been described before. An elegant collective decision study showed that an “equality bias” – assigning similar weights to both participants regardless of their respective competence – might underlie mutual convergence (<xref ref-type="bibr" rid="c46">Mahmoodi et al., 2015</xref>). Interestingly, in their study more competent dyad members were better at weighting their partner’s options compared to less competent ones. In contrast, in our experiment more competent players lose more accuracy than is gained by their partners. Furthermore, unlike the previous work, where similar confidence (<xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>) or perceptual sensitivity (<xref ref-type="bibr" rid="c3">Bahrami et al., 2012a</xref>, <xref ref-type="bibr" rid="c5">2010</xref>; <xref ref-type="bibr" rid="c12">Baumgart et al., 2020</xref>) correlated with higher dyadic benefit, we do not find systematic dyadic competence benefits (in our case, nominal “average accuracy” within a dyad) for participants with similar task competence or confidence. We speculate that the mode of social feedback and interaction might underlie these differences. Explicit communication and a joint decision – made together or by one player (<xref ref-type="bibr" rid="c3">Bahrami et al., 2012a</xref>, <xref ref-type="bibr" rid="c5">2010</xref>; <xref ref-type="bibr" rid="c12">Baumgart et al., 2020</xref>), or periods of metacognitive introspection in which prior individual decisions are evaluated (<xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>), might have elicited competence improvements in those studies.</p>
<p>The control experiment with a reliably accurate, simulated dyadic “partner” who exhibited a stable but fairly low level of confidence irrespective of the task difficulty elicited vastly improved accuracy and hit rate, especially when sensory information was ambiguous. At the same time, participants’ confidence reports became more conservative when playing with such “conservative” partner, because participants gravitated towards the report of the simulated partner. Improvements in competence coupled with declining confidence further support dyadic convergence where the direction can be dissociated along the two dimensions. Thus, instead of using the reliably accurate information provided by the computer player to be more accurate <italic>and</italic> more confident, convergence interfered with fully maximizing the reward score. High-accuracy, low-confidence simulated partners have been recently shown to elicit more conservative confidence reports during binary dyadic decision-making (<xref ref-type="bibr" rid="c23">Esmaily et al., 2023</xref>). Beyond these results, our experiments demonstrate that humans do not require sensible confidence expression to recognize and utilize differences in task competence. Our findings indicate a possible dissociation between the accuracy and confidence alignment. At the same time, a positive covariation of accuracy and confidence modulation by the dyadic context suggests a retention of metacognitive sensitivity under social influence. Thus, performance history and temporal reliability might be important factors in addition to explicitly signaled confidence, especially when these information streams are not congruent.</p>
<p>Systematic changes towards group consensus (‘social conformity’) have been shown to bias decision-making towards majority choices (<xref ref-type="bibr" rid="c20">De Martino et al., 2017</xref>; <xref ref-type="bibr" rid="c33">Germar et al., 2016</xref>; <xref ref-type="bibr" rid="c57">Park et al., 2017</xref>; <xref ref-type="bibr" rid="c71">Toelch and Dolan, 2015</xref>). The dyadic convergence we and others observe might be the basis for social conformity in larger group settings. These findings resonate with the work on factual knowledge estimation by Lorenz and colleagues, who showed that social influence can reduce the diversity of opinions, leading to increased confidence without improved group accuracy (<xref ref-type="bibr" rid="c44">Lorenz et al., 2011</xref>). Reinterpreting these results, Farrell argued that information sharing can enhance individual performance and confidence calibration, even if group-level diversity diminishes. This statistical “shrinkage” effect may benefit individuals by reducing extreme errors. Thus, while social influence may undermine the classical “wisdom of crowds” at the aggregate level, it can simultaneously improve individual outcomes and self-assessment (<xref ref-type="bibr" rid="c25">Farrell, 2011</xref>; <xref ref-type="bibr" rid="c44">Lorenz et al., 2011</xref>; <xref ref-type="bibr" rid="c62">Rauhut et al., 2011</xref>). Our results suggest that real-time social feedback may provide similar individual-level benefits, even if it fails to enhance collective perceptual competence.</p>
<p>These nuanced relationships between individual and joint – often only nominally “joint” – success underscore the importance of the actual social context. Similar to many social influence studies such as above, in our dyadic condition participants were not instructed to cooperate or compete with one another, nor incentivized to perform better as a group. Instead, they co-acted under independent reward contingencies. This difference to joint decision studies (<xref ref-type="bibr" rid="c4">Bahrami et al., 2012b</xref>, <xref ref-type="bibr" rid="c3">2012a</xref>, <xref ref-type="bibr" rid="c5">2010</xref>; <xref ref-type="bibr" rid="c12">Baumgart et al., 2020</xref>; <xref ref-type="bibr" rid="c23">Esmaily et al., 2023</xref>; <xref ref-type="bibr" rid="c46">Mahmoodi et al., 2015</xref>; <xref ref-type="bibr" rid="c59">Pescetelli et al., 2016</xref>) is crucial for interpreting our results, since participants could ignore the overt behavior of the other player. Therefore, any social modulation or correlated behavior observed in our experiment can be attributed to a spontaneous, self-regulated process. Supporting a prior perceptual decision co-action study (<xref ref-type="bibr" rid="c60">Pescetelli and Yeung, 2022</xref>), we interpret our findings as evidence that in social situations people spontaneously and opportunistically integrate the judgment of others into their own decisions, even when social interaction is not incentivized or enforced. In line with this argument, humans seem to naturally follow gaze signals and choice preferences of others, suggesting the utilization of others’ thoughts and intentions (<xref ref-type="bibr" rid="c13">Bayliss et al., 2007</xref>; <xref ref-type="bibr" rid="c45">Madipakkam et al., 2019</xref>; <xref ref-type="bibr" rid="c51">Mitsuda and Masaki, 2018</xref>). Furthermore, human co-action seems to result in attentional attraction or withdrawal in some dyads (<xref ref-type="bibr" rid="c22">Dosso et al., 2018</xref>). As next step, it would be very interesting to test whether face-to-face interactions through the transparent shared visual display will induce even stronger social effects compared to separate experimental booths (<xref ref-type="bibr" rid="c37">Isbaner et al., 2025</xref>; <xref ref-type="bibr" rid="c43">Lewen et al., 2025</xref>; <xref ref-type="bibr" rid="c52">Moeller et al., 2023</xref>).</p>
<p>The advent of new techniques such as time-continuous decision-making (<xref ref-type="bibr" rid="c16">Bonnen et al., 2015</xref>; <xref ref-type="bibr" rid="c36">Huk et al., 2018</xref>; <xref ref-type="bibr" rid="c56">Noel et al., 2023</xref>, <xref ref-type="bibr" rid="c55">2022</xref>) and hyperscanning (<xref ref-type="bibr" rid="c2">Babiloni and Astolfi, 2014</xref>; <xref ref-type="bibr" rid="c19">Czeszumski et al., 2020</xref>) allows to ask how evolving decisional variables are represented in neural circuitry underlying flexible behaviors. This is an important step beyond the traditional approach based on discrete, trial-based decisions. Adapting this approach, we demonstrate real-time, dynamic influence of social information on human perceptual decisions. Studies investigating the neuronal correlates of similar perceptual decisions have demonstrated faster and more accurate behavioral responses when the sensory evidence resulted in earlier and more reliable neuronal changes (<xref ref-type="bibr" rid="c24">Fan et al., 2018</xref>; <xref ref-type="bibr" rid="c34">Gold and Stocker, 2017</xref>; <xref ref-type="bibr" rid="c40">Kiani and Shadlen, 2009</xref>). It has also been shown that microstimulation- and optogenetically-elicited inputs can be integrated into perceptual decisions (<xref ref-type="bibr" rid="c27">Fetsch et al., 2018</xref>, <xref ref-type="bibr" rid="c26">2014</xref>; <xref ref-type="bibr" rid="c63">Salzman et al., 1990</xref>). Along these lines, we propose that reliably accurate real-time social information is multiplexed with sensory signals, possibly resulting in enhanced encoding already in cortical neurons representing relevant sensory dimensions.</p>
<p>In summary, our novel CPR task is a powerful new tool for studying the dynamics of decision and confidence formation, in individual and social settings. We show that the presence of a co-acting social partner adaptively changes continuous and graded perceptual decisions, resulting in mutual but asymmetric convergence and often a net dyadic benefit in confidence and the reward score. This is particularly apparent in a confidence boost of a less confident – and often less accurate – dyadic partner. On the other hand, more accurate partners on average lose accuracy. The lawful relationships between confidence and competence modulations demonstrate the importance of concurrently considering these two measures, both within each participant and across interacting partners. These results advance our understanding of how humans evaluate and incorporate social information, especially in real-time decision-making situations that do not permit careful but slow deliberations.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Study design and participants</title>
<p>Data were recorded from 38 human participants (Median age: 26.17 years, IQR 4.01; 13 of which with corrected vision) between January 2022 and August 2023. Prior to the experimental sessions, each participant was trained on two occasions. During the experimental phase, participants played three variations of the experimental paradigm: alone (solo), with a human player (Human-Human dyad), and with a computer player (Human-Computer dyad). The experimental order was mixed and largely determined by the availability of participants (Supplementary Figure 1A). Most participants in this study originated from central Europe or South Asia. All procedures performed in this study were approved by the Ethics Board of the University of Göttingen (Application 171/292).</p>
</sec>
<sec id="s4b">
<title>Experimental setup</title>
<p>Participants sat in separate experimental booths with identical hardware. They were instructed to rest their head on a chinrest, placed 57 cm away from the screen (Asus XG27AQ, 27” LCD). A single-stick joystick (adapted analog multifunctional joystick (Sasse), Resolution: 10-bit, 100 Hz) was anchored to an adjustable platform placed in front of the participants, at a height of 75 cm from the floor. The joystick was calibrated before data acquisition to ensure comparable readouts. Screens were calibrated to be isoluminant. Two speakers (Behringer MS16), one for each setup were used to deliver auditory feedback at 70 dB SPL.</p>
<p>The experimental paradigm was programmed in MWorks (Version 0.10 – <ext-link ext-link-type="uri" xlink:href="https://mworks.github.io">https://mworks.github.io</ext-link>). Two iMac Pro computers (Apple, MacOS Mojave 10.14.6) served as independent servers for each setup booth. These computers were controlled by an iMac Pro (Apple, MacOS Mojave 10.14.6). Custom-made plugins for MWorks were used to generate and display the stimuli, to handle the data acquisition from the joystick (10 ms sampling rate), and to incorporate all data from both servers into a single data file.</p>
</sec>
<sec id="s4c">
<title>Continuous perceptual report (CPR) game</title>
<p>Participants were instructed to maximize monetary outcome in a motion tracking game. In this game, subjects watched a frequently changing random dot pattern on the screen and used a joystick (<xref ref-type="bibr" rid="c68">Szul et al., 2020</xref>) to indicate their current motion direction perception. The joystick controlled an arc-shaped response cursor on the screen (partial circle with fixed eccentricity, Solo: 2 degree of visual angle (‘dva’) radius from the center of the screen; Dyadic: 1.8 dva &amp; 2 dva radius). The angular direction of the joystick was linked to the cursor’s polar center position. In addition, the joystick tilt was permanently coupled to the cursor’s width (see below, 13 – 180 degrees). This resulted a continuous representation of the joystick position along its two axes. By moving the joystick, participants could rotate and shape the cursor. At unpredictable times (1% probability every 10 ms), a small white disc (‘reward target’, diameter: 0.5 dva) appeared on the screen for a duration of 50 ms at 2.5 dva eccentricity, congruently with the motion direction of the stimulus. Whenever a target appeared in line with the cursor, the target was considered collected (‘hit’) and the score of the participant increased. To help participants performing such alignment to the best of their perceptual abilities, a small triangular reference point was added to the center point of the cursor. Throughout the experiment, participants were required to maintain gaze fixation on a central fixation cross (2.5 dva radius tolerance window) or the cursor would disappear and no targets could be collected until fixation was resumed.</p>
<p>In the solo experiments, the cursor was always red. In dyadic conditions, the two cursors, present on screen simultaneously, were red and green (isoluminant at 17.5 cd/m<sup>2</sup> ± 1 cd/m<sup>2</sup>). During dyadic experiments, the position of the two cursors switched between stimulus cycles, with the red cursor always starting above, but not overlapping the green cursor. Each cursor color was permanently associated with one of the two experimental booths. After the mid-session break, participants switched booths, contributing an equal amount of data for each setup (600 reward targets, ∼20 min, up to 17 stimulus cycles). Players initiated new stimulus cycles with a joystick movement. Each stimulus cycle could last up to 75 seconds, during which the RDP’s motion direction and coherence changed at pseudorandomized intervals, resulting in the presentation of 30 stimulus states per cycle.</p>
</sec>
<sec id="s4d">
<title>Random dot pattern (RDP)</title>
<p>We used a circular RDP (8 dva radius) with white dots on a black background. Each dot had a diameter of 0.1 dva, moved with 8 dva/s and had a lifetime of 25 frames (208 ms). The overall dot density was 2.5 dots/dva. The stimulus patch was centrally located on the screen. The central part of the stimulus (5 dva diameter) was blacked out. In this area we presented the fixation cross and the response arc. The RDP motion direction was randomly seeded and set to change instantly by either 15 deg, 45 deg, 90 deg or 135 deg after a pseudo-randomized time interval of 1250 ms to 2500 ms. Whether the signal moved in clockwise or counterclockwise direction was random. Only signal dots altered their direction. The dot coherence changed pseudo-randomly after 10 RDP direction changes to the coherence level that was presented least. Seven coherence conditions were tested: 0, 8, 13, 22, 36, 59, 98%.</p>
</sec>
<sec id="s4e">
<title>Gaze control</title>
<p>Participants were required to maintain gaze fixation at the center of the screen throughout each stimulus cycle. We used a white cross (0.3 dva diameter) as anchor point for the participants’ gaze. The diameter of the fixation window was set to 5 dva. An eye tracker (SR research, EyeLink 1000 Plus) was used to control gaze position in real-time. If the gaze position left the fixation window for more than 300 ms, the player’s arc would disappear from the screen, preventing target collection. In addition to this, an increase of the fixation cross’ size, together with a change in color (white to red), signaled to the participants that fixation was broken. As soon as the gaze entered the fixation window again, visual parameters were reset to the original values and the arc would reappear, allowing the player to continue target collection.</p>
</sec>
<sec id="s4f">
<title>Reward score</title>
<p>Participants were incentivized to maximize their monetary outcome by collecting as many targets as possible with the highest possible score. The minimum polar distance between the arc’s center position and the nominal direction (target center, ‘accuracy’) as well as the angular width of the response cursor (‘joystick tilt’) at the moment of collection were taken into account when calculating the score:
<disp-formula id="eqn1">
<graphic xlink:href="608609v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>RDP<sub>dir</sub></italic>refers to the direction of the random dot pattern and <italic>JS<sub>dir</sub></italic> refers to the direction of the joystick at sample <italic>i</italic>.
<disp-formula id="eqn2">
<graphic xlink:href="608609v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>tilt</italic> varies between 0 and 1 for minimal and maximal radial joystick positions, respectively.
<disp-formula id="eqn3">
<graphic xlink:href="608609v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Thus, narrower and more accurately placed cursors caused higher reward scores.</p>
</sec>
<sec id="s4g">
<title>Feedback signals</title>
<p>Various feedback signals were provided throughout the experiment to inform participants about their short- and long-term performance. All feedback signals were mutually visible.</p>
</sec>
<sec id="s4h">
<title>Immediate feedback</title>
<p>Immediately after a target was collected, visual and acoustic signals were provided simultaneously. The auditory feedback consisted of a 200 ms long sinusoidal pure tone at a frequency determined by the score. Each tone corresponded to a reward range of 12.5%, with lower pitch corresponding to low reward score. We used 8 notes from the C5 major scale (523, 587, 659, 698, 784, 880, 988, 1047 Hz). Sounds were on- and off-ramped using a 50 ms Hanning window. No sound feedback was given for missed targets. In solo experiments, the visual feedback consisted of a 2 dva wide circle, filled in proportion to the score with the same color as the arc’s player. The circle was presented in the center of the screen, behind the fixation cross for 150 ms. In dyadic conditions, the visual feedback consisted of half a disc for each player and both color-coded halves were mutually visible.</p>
</sec>
<sec id="s4i">
<title>Short-term feedback</title>
<p>During each stimulus cycle, a running average of the reward score was displayed for each player with a 0.9 dva wide, color-coded ring around the circumference of the RDP (18.2 dva and 19.4 dva diameter). After every target presentation, the filled portion of the ring updated. To avoid spatial biasing, the polar zero position of the ring changed randomly with every stimulus cycle.</p>
</sec>
<sec id="s4j">
<title>Long-term feedback</title>
<p>Cumulative visual feedback was provided after each stimulus cycle (during the inter-cycle intervals) for 2000 ms. It displayed the total reward score accumulated across all cycles as a colored bar graph located at the center of the screen. A grey bar (2 dva wide, max height: 10 dva) indicated the maximal possible cumulative score after each cycle. A colored bar next to it (same dimensions) showed how much was collected by the player so far. In dyadic experiments, red and green bars would be shown on either side of the grey bar. In solo experiments, a red arc was shown to the left of the grey bar. The configuration of the visual stimuli and task parameters is illustrated in <xref rid="fig1" ref-type="fig">Figure 1</xref> and as a supplementary video file <xref ref-type="supplementary-material" rid="vid1">Supplementary Video 1</xref>.</p>
</sec>
<sec id="s4k">
<title>Statistical analysis</title>
<p>Performance metrics, based on raw joystick tilt and accuracy, were extracted and averaged in time windows of 30 frames (∼250 ms) that were either target- or state-aligned. Target-alignment refers to time windows prior to the first reward target presentation of each stimulus state. Target-aligned data were only considered if the first target appeared at least 1000 ms after the direction change. This analysis approach was chosen to (i) allow adequate time for an intentional response and (ii) to avoid the prediction of motion direction based on earlier target locations. State-alignment refers to a 30 frames time window before a nominal motion direction change of the stimulus (end of the current stimulus state). Stimulus states in which fixation breaks exceeded 10% of the state duration were excluded from the performance analyses.</p>
<p>For population analyses across subjects, joystick response parameters (tilt and accuracy) were first averaged within-subject. Bootstrapped 95% confidence intervals were estimated with 1000 repetitions (Matlab: bootci). For analyses within each subject or a dyad, differences of accuracy and tilt between experimental conditions (solo vs dyadic) were tested with a two-sided Wilcoxon signed rank test (Matlab: signrank, for paired samples) and a two-sided paired Wilcoxon rank sum test (Matlab: ranksum). Bonferroni correction was applied for multiple testing. Whether or not coherence was pooled is indicated for each test. Within-subject effect size and direction was estimated with the area under the receiver operating characteristic (‘AUC’, Matlab: perfcurve). AUC values of 0.5 indicated similar distributions. AUC of 0 and 1 suggested perfectly separated distributions. Directionality of social modulation was inferred by AUC change (larger vs smaller than 0.5). Correlation coefficients were calculated with a Pearson correlation (Matlab: corrcoef). Average response lags between stimulus and response were estimated with the maximum cross-correlation coefficient (Matlab: xcorr). Social modulation differences between dyadic players were compared to the baseline modulation of shuffled dyadic partners (Matlab: randperm). We fitted three Generalized Linear Mixed Models (GLMM; <xref ref-type="bibr" rid="c1">Baayen, 2008</xref>) to the raw joystick responses parameters, which differed in their response variable and the size of the data set analyzed but had identical fixed effects structures and largely identical random effects structures. We fitted one model for the probability of a target hit (model 1a), joystick tilt (model 1b), and joystick accuracy (model 1c) as the response. All three aimed at estimating the extent to which the respective response variable was affected by the fixed effects of experimental condition (solo or human-computer dyad), random dot pattern coherence, stimulus duration, stimulus number, block number, and day number. We hypothesized that the effect of coherence depended on the condition, thus, we included the interaction between these two predictors into the fixed affects part of the model. To avoid pseudo-replication and account for the possibility that the response was influenced by several layers of non-independence, we included three random intercepts effects, namely those of the ID of the participant, the ID of test day (nested in participant; thereafter ‘day ID’), and the ID of the block (nested in participant and day; thereafter ‘block ID’). The reason for including the latter two was that it could be reasonably assumed that the performance of participants varied between test days and also between blocks tested on the same day. To avoid an ‘overconfident model’ and keep type I error rate at the nominal level of 0.05 we included all theoretically random slopes (<xref ref-type="bibr" rid="c10">Barr et al., 2013</xref>; <xref ref-type="bibr" rid="c64">Schielzeth and Forstmeier, 2009</xref>). These were those of condition, coherence, their interaction, stimulus duration, stimulus number, block number, and day number within participant, coherence, stimulus duration, stimulus number, and block number within day ID, and finally coherence, stimulus duration, and stimulus number within block ID. Originally we also included estimates of the correlations among random intercepts and slopes into each model, but do to convergence and identifiability problems (recognizable by absolute correlation parameters being close to 1; <xref ref-type="bibr" rid="c49">Matuschek et al., 2017</xref>) we had to exclude all or several of these estimates from the full models (see <xref rid="tbls1" ref-type="table">Supplementary Table 1</xref> for detailed information).</p>
<p>For each model we conducted a full-null model comparison which aims at avoiding ‘cryptic multiple testing’ and keeping the type one error rate at the nominal level of 0.05 (<xref ref-type="bibr" rid="c30">Forstmeier and Schielzeth, 2011</xref>). As we had a genuine interest in all predictors present in the fixed effects part of each model the null models comprised only the intercept in the fixed effect’s part but were otherwise identical to the respective full model. This full-null model comparison utilized a likelihood ratio test (<xref ref-type="bibr" rid="c21">Dobson, 2001</xref>). Tests of individual effects were also based on likelihood ratio test, comparing a full model with an each in a set of reduced models which lacked fixed effects one at a time.</p>
</sec>
<sec id="s4l">
<title>Model implementation</title>
<p>We fitted all models in R (version 4.3.2; R Core Team, 2023). In model 1a we included the response as a two columns matrix with the number of targets hit and not hit in the first and second column respectively (<xref ref-type="bibr" rid="c1">Baayen, 2008</xref>). The model was fitted with a binomial error structure and logit link function (<xref ref-type="bibr" rid="c50">McCullagh and Nelder, 1989</xref>). In essence, such models model the proportion of targets hit. We are aware that in principle one would need an ‘observation level random effect’ which would link the number of targets hit and not hit in a given stage. However, in a relatively large proportion of stages (19.7%) there was only a single target that appeared and in the majority of stages (47.0%) only two targets appeared, making it unlikely that a respective random effect can be fitted successfully.</p>
<p>For models 1b and 1c, we fitted with a beta error distribution and logit link function (<xref ref-type="bibr" rid="c15">Bolker, 2008</xref>). Models fitted with a beta 1 error distribution cannot cope with values in the response being exactly 0 or 1. Hence, when such values were present in a given response variable we transformed then as suggested by <xref ref-type="bibr" rid="c66">Smithson &amp; Verkuilen, 2006</xref>. Model 1a was fitted using the function glmer of the package lme4 (version 1.134; <xref ref-type="bibr" rid="c11">Bates et al., 2015</xref>), and models 1b and 1c were fitted using the function glmmTMB of the equally named package (version 1.1.8; <xref ref-type="bibr" rid="c18">Brooks et al., 2017</xref>). We determined model stability by dropping levels of the random effects factors, one at a time, fitting the full model to each of the subsets, and finally comparing the range of fixed effects estimates obtained from the subsets with those obtained from the model fitted on the respective full data set. This revealed all models to be of good stability. We estimated 95% confidence limits of model estimates and fitted values by means of parametric bootstraps (N=1000 bootstraps; function bootMer of the package lme4 for model 1 and function simulate of package glmmTMB for models the response was overdispersed (maximum dispersion parameter: 1.0).</p>
</sec>
</sec>
</body>
<back>
<sec id="s10">
<title>Supplementary Figures</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1.</label>
<caption><title>Related to <xref rid="fig1" ref-type="fig">Figure 1</xref> and <xref rid="fig2" ref-type="fig">Figure 2</xref>. Additional information regarding experiments and joystick responses of individual participants.</title>
<p><bold>(A)</bold> Number and identity of experimental sessions for each subject. A session comprised two experimental blocks that were recorded in different setups. The order of the session type (solo or dyadic) was mixed (color-coded). All participants, except two, contributed data to each session type, specifically, solo CPR as well as both dyadic conditions. <bold>(B)</bold> Statistics of target occurrences during stimulus presentation. Distributions of inter-target intervals and target count per stimulus state for an example session. Targets were flashed with a 1% probability every 10 ms. Once a target was presented it remained in the screen for 50 ms followed by a minimum inter-target interval of 300 ms. <bold>(C)</bold> Final reward scores of participants over the course of the individuals’ data acquisition period (gray lines). Reward score increased over time. A linear regression was fitted to the cumulative scores of each experimental block for each participant (black lines). Note that each experimental session comprised two blocks, one in each setup. Independent of the session type (see panel (A) for more details), scores increased over time, likely due to perceptual learning. The final cumulative score is comprised of the hit rate, accuracy and tilt, all of which are affected by number of the experimental block, with later sessions resulting in higher hit rate as well as more accurate and tilted responses (<xref rid="tbls1" ref-type="table">Supplementary Table 1</xref> - <xref rid="tbls4" ref-type="table">Supplementary Table 4</xref>). <bold>(D)</bold> Normalized cross-correlation coefficients between random dot motion direction and joystick response direction illustrated for each subject. Lighter hues indicate higher cross-correlation coefficients at the respective signal lag, darker hues suggest low correlation between stimulus and joystick response at that lag. Cross-correlations broke down consistently with lower stimulus coherence.</p></caption>
<graphic xlink:href="608609v2_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 2:</label>
<caption><title>Related to <xref rid="fig2" ref-type="fig">Figure 2</xref>. Joystick tilt is a proxy measure of perceptual confidence.</title>
<p><bold>(A)</bold> Metacognitive sensitivity of joystick response for one example subject. Left: Distribution of joystick accuracy in stimulus states with low (gray) vs high joystick tilt (colored, median split). Accuracy and tilt were averaged for the last 30 frames (250 ms) prior to a stimulus direction change. Coherence is color-coded. Right: Corresponding receiver-operating characteristics (‘ROC’) between the two distributions for each coherence level (color-coded). A ROC curve along the diagonal would indicate similar accuracy distributions between hits and misses, suggesting no metacognitive sensitivity. <bold>(B)</bold> Population AUC values (black dots) are consistently above 0.5 (p&lt;0.001, Two-sided Wilcoxon signed rank test for distribution with median 0.5), demonstrating that high tilt was more often associated with high accuracy, suggesting metacognitive-sensitive confidence readouts.</p></caption>
<graphic xlink:href="608609v2_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 3:</label>
<caption><title>Related to <xref rid="fig3" ref-type="fig">Figure 3</xref>. Additional information for human-human dyadic performance.</title>
<p><bold>(A)</bold> Performance summary in (human-human) dyadic experiments. Hit rate (left), accuracy (center) and confidence (right) illustrated for each subject contributing human-human dyadic data. Same conventions as in <xref rid="fig2" ref-type="fig">Figure 2A</xref>. <bold>(B)</bold> Comparison of social modulation with response measures of solo experiments. Hit rate differences (left, compared to solo hit rate) and social modulation of accuracy (center, compared to solo accuracy) and tilt (right, compared to solo confidence) are displayed for the entire population. Joystick data was recorded in a normalized fashion for both accuracy (180deg difference = 0; Perfect match of stimulus direction = 1) and confidence (center = 0; Max. joystick tilt = 1). Each participant contributes on data point per coherence condition (color-coded). The median score across all subjects is overlaid for each coherence condition in brighter color hues. Error bars show 99% confidence intervals of the median in solo and dyadic conditions. <bold>(C)</bold> Social modulation of confidence for three example participants. Data for solo (light, left) and human-human dyadic experiments (dark, right) are displayed for each stimulus coherence level (color-coded). Each dot corresponds to the time-window average for a single stimulus state. All sessions of the same experimental condition are pooled. Corresponding AUC values, used to quantify the direction and magnitude of social modulation between dyadic and solo experiments, are shown below. A value of 0.5 corresponds to perfect overlap between solo and dyadic response distributions, 1 and 0 imply perfect separation between experimental conditions.</p></caption>
<graphic xlink:href="608609v2_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 4:</label>
<caption><title>Related to <xref rid="fig3" ref-type="fig">Figure 3</xref>. Social modulation grouped by different metrics.</title>
<p>Illustration of average social modulation for hit rates (first row), accuracy (second row) and confidence (third row) based on differently grouped solo performance quartiles: first column - hit rate quartiles, second column - accuracy quartiles, third column - confidence quartiles. Coherence is color-coded. Same conventions as in <xref rid="fig3" ref-type="fig">Figure 3C</xref>. Social modulation between dyadic and solo experiments was measured by AUC. An AUC value of 0.5 corresponds to perfect overlap between solo and dyadic response distributions. AUC &gt; 0.5 imply better accuracy or higher confidence in dyadic experiments. AUC &lt; 0.5 imply better accuracy or higher confidence in solo experiments.</p></caption>
<graphic xlink:href="608609v2_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 5:</label>
<caption><title>Related to <xref rid="fig4" ref-type="fig">Figure 4</xref>. Across-player correlation in solo and dyadic setting.</title>
<p>Confidence (left) and accuracy (right) of dyadic partners in solo and dyadic settings. Only in the dyadic condition confidence (but not accuracy) correlate significantly between players.</p></caption>
<graphic xlink:href="608609v2_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 6:</label>
<caption><title>Related to <xref rid="fig4" ref-type="fig">Figure 4</xref>. Relationship between social modulation and solo performance difference.</title>
<p>Average (across dyad members) social modulation for confidence (top) and accuracy (bottom) displayed as a function of the absolute solo differences in confidence (left) and accuracy (right) between dyadic partners. Each data point corresponds to one dyad (N=50). Average social modulation did not correlate with absolute solo difference between players.</p></caption>
<graphic xlink:href="608609v2_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 7:</label>
<caption><title>Related to <xref rid="fig5" ref-type="fig">Figure 5</xref>. Additional information regarding computer player performance.</title><p><bold>(A)</bold> Illustration of computer player behavior in an example session. Each dot corresponds to the accuracy-confidence combination during target presentation. Distribution of computer behavior is summarized with histograms. Same conventions as in <xref rid="fig1" ref-type="fig">Figure 1D</xref>. Coherence is color-coded. <bold>(B)</bold> Cross-correlation between stimulus direction changes and cursor responses of computer player. Similar human-like response lag was built in for all coherence conditions (color-coded). See <xref rid="fig2" ref-type="fig">Figure 2C</xref> for comparison. <bold>(C)</bold> Average computer player performance (reward score, hit rate, accuracy, and confidence) as a function of stimulus coherence. Shaded background corresponds to the 99% confidence intervals of the median. See <xref rid="fig2" ref-type="fig">Figure 2A</xref> for comparison to human behavior.</p></caption>
<graphic xlink:href="608609v2_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 8:</label>
<caption><title>Related to <xref rid="fig5" ref-type="fig">Figure 5</xref>. Social modulation in human-computer (HC) dyads vs solo.</title>
<p><bold>(A)</bold> Comparison of average hit rate (top), accuracy (center) and confidence (bottom) for each stimulus coherence in solo and human-computer dyads, color-coded for coherence level. Individual data is shown in darker hues. Each subject contributes one data point per coherence condition. Medians across subjects overlaid for each coherence condition (bright color). Error bars show 99% confidence intervals of the median. Same conventions as in <xref rid="fig5" ref-type="fig">Figure 5</xref>. <bold>(B)</bold> Social modulation of humans between dyadic (human-computer, HC) and solo experiments. <bold>(C)</bold> Population comparison of social modulation (between solo and HH experiments, top) and HH and HC contrast (bottom).</p></caption>
<graphic xlink:href="608609v2_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s11">
<title>Supplementary Tables</title>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Supplementary Table 1:</label>
<caption><title>Random effects structure and sample size of each model</title></caption>
<graphic xlink:href="608609v2_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Supplementary Table 2:</label>
<caption><title>Results of the full model with hit probability as response</title></caption>
<graphic xlink:href="608609v2_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Supplementary Table 3:</label>
<caption><title>Results of the full model with confidence being the response</title></caption>
<graphic xlink:href="608609v2_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls4" orientation="portrait" position="float">
<label>Supplementary Table 4:</label>
<caption><title>Results of the full model with accuracy being the response</title></caption>
<graphic xlink:href="608609v2_tbls4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="vid1">
<label>Supplementary Video 1</label><caption><title>Schematic animation of the continuous perceptual report task.</title><p>Animation is based a solo experiment of an example subject. White random dot pattern moves continuously on black background with frequently changing motion direction and coherence. The white vector indicates the nominal motion direction of the stimulus. It is scaled by motion coherence for illustration purposes. The coherence is indicated in the lower right corner. The red vector shows the behavioral response of the subject: joystick direction (polar angle) and joystick tilt (length). Both joystick response values as well as the resulting score are stated in the lower left corner. The red shading demonstrates the angular width of the response arc.</p></caption>
<media xlink:href="608609v2_vid1.mp4"/>
</supplementary-material>
</sec>
<sec id="s5">
<title>Glossary</title>
<sec id="s5a">
<title>Dyadic condition</title>
<p>Real-time co-action of two players, each having instant access to the partner’s report but without payoff interdependence. One player is always a human participant; the other can be either a real human participant (HH dyad) or a simulated computer agent (HC dyad), which was impersonated by the experimenter. <italic>Hit rate</italic>: Fraction of successfully collected reward targets. Success (hit) was achieved when the joystick-controlled cursor overlapped with the reward target at the time of its appearance.</p>
</sec>
<sec id="s5b">
<title>Joystick accuracy</title>
<p>Inverse normalized tracking error of the joystick. A value of 1 corresponds to a joystick direction response in nominal stimulus direction. A value of 0 indicates a response which is 180 degrees off the nominal stimulus direction. Used as measure for task competence.</p>
</sec>
<sec id="s5c">
<title>Joystick tilt</title>
<p>Refers to the normalized radial displacement of the joystick from the center position. A value of 1 indicates maximum displacement from the joysticks center position. Used as proxy measure for perceptual confidence.</p>
</sec>
<sec id="s5d">
<title>Reward targets</title>
<p>Small stimuli which were briefly presented at pseudo-random times in nominal stimulus direction. Subjects were instructed to collect these targets with their joystick-controlled cursor, to obtain rewards.</p>
</sec>
<sec id="s5e">
<title>Reward score</title>
<p>Normalized monetary payoff. Calculated as the product of normalized accuracy and tilt at time of reward target presentation for hits, and is always zero for misses.</p>
</sec>
<sec id="s5f">
<title>Stimulus state</title>
<p>A pseudorandomized epoch of time with constant nominal direction and coherence of the random dot pattern.</p>
</sec>
</sec>
<sec id="s9" sec-type="data-availability">
<title>Data and code availability</title>
<p>The dataset and MATLAB code generated during this study is available at GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/SocCog-Team/CPR/tree/main/Publications/2024_perceptual_confidence">https://github.com/SocCog-Team/CPR/tree/main/Publications/2024_perceptual_confidence</ext-link>).</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This publication was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 454648639 - SFB 1528 - Cognition of Interaction, subproject A01, and the Leibniz Collaborative Excellence grant K265/2019 “Neurophysiological mechanisms of primate interactions in dynamic sensorimotor settings” (PRIMAINT) and the European Union (ERC Starting Grant, NEUROGROUP, 101041799). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. We thank Fred Wolf for useful discussions.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Author contribution</title>
<p>Conceptualization: F.S., A.C., A.G., I.K., S.T.; Methodology: F.S., A.C., S.T.; Investigation: F.S.; Analysis: F.S., R.M.; Software: F.S.; Visualization: F.S., I.K.; Writing – Original Draft: F.S., A.C., I.K.; Writing – Review &amp; Editing: all authors; Funding Acquisition: A.G, I.K., S.T.; Resources: S.T.; Supervision: I.K, S.T.</p>
</sec>
<sec id="s8">
<title>Lead contact</title>
<p>Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Felix Schneider (<email>fschneider@dpz.eu</email>).</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Baayen</surname> <given-names>RH</given-names></string-name></person-group>. <year>2008</year>. <source>Analyzing Linguistic Data: A Practical Introduction to Statistics using R</source>, <edition>1st ed</edition>. <publisher-name>Cambridge University Press</publisher-name>. doi:<pub-id pub-id-type="doi">10.1017/CBO9780511801686</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Babiloni</surname> <given-names>F</given-names></string-name>, <string-name><surname>Astolfi</surname> <given-names>L</given-names></string-name></person-group>. <year>2014</year>. <article-title>Social neuroscience and hyperscanning techniques: Past, present and future</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source> <volume>44</volume>:<fpage>76</fpage>–<lpage>93</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neubiorev.2012.07.006</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name>, <string-name><surname>Olsen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Roepstorff</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rees</surname> <given-names>G</given-names></string-name>, <string-name><surname>Frith</surname> <given-names>C</given-names></string-name></person-group>. <year>2012a</year>. <article-title>What failure in collective decision-making tells us about metacognition</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>367</volume>:<fpage>1350</fpage>–<lpage>1365</lpage>. doi:<pub-id pub-id-type="doi">10.1098/rstb.2011.0420</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name>, <string-name><surname>Olsen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Roepstorff</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rees</surname> <given-names>G</given-names></string-name>, <string-name><surname>Frith</surname> <given-names>C</given-names></string-name></person-group>. <year>2012b</year>. <article-title>Together, slowly but surely: The role of social interaction and feedback on the build-up of benefit in collective decision-making</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>38</volume>:<fpage>3</fpage>–<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0025708</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name>, <string-name><surname>Olsen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Latham</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Roepstorff</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rees</surname> <given-names>G</given-names></string-name>, <string-name><surname>Frith</surname> <given-names>CD</given-names></string-name></person-group>. <year>2010</year>. <article-title>Optimally Interacting Minds</article-title>. <source>Science</source> <volume>329</volume>:<fpage>1081</fpage>–<lpage>1085</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1185718</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balsdon</surname> <given-names>T</given-names></string-name>, <string-name><surname>Mamassian</surname> <given-names>P</given-names></string-name>, <string-name><surname>Wyart</surname> <given-names>V</given-names></string-name></person-group>. <year>2021</year>. <article-title>Separable neural signatures of confidence during perceptual decisions</article-title>. <source>eLife</source> <volume>10</volume>:<elocation-id>e68491</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.68491</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balsdon</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wyart</surname> <given-names>V</given-names></string-name>, <string-name><surname>Mamassian</surname> <given-names>P</given-names></string-name></person-group>. <year>2020</year>. <article-title>Confidence controls perceptual evidence accumulation</article-title>. <source>Nat Commun</source> <volume>11</volume>:<fpage>1753</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41467-020-15561-w</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Aitchison</surname> <given-names>L</given-names></string-name>, <string-name><surname>Moran</surname> <given-names>R</given-names></string-name>, <string-name><surname>Herce Castanon</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rafiee</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mahmoodi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lau</surname> <given-names>JYF</given-names></string-name>, <string-name><surname>Latham</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name>, <string-name><surname>Summerfield</surname> <given-names>C</given-names></string-name></person-group>. <year>2017</year>. <article-title>Confidence matching in group decision-making</article-title>. <source>Nat Hum Behav</source> <volume>1</volume>:<fpage>0117</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41562-017-0117</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Frith</surname> <given-names>CD</given-names></string-name></person-group>. <year>2017</year>. <article-title>Making better decisions in groups</article-title>. <source>R Soc open sci</source> <volume>4</volume>:<fpage>170193</fpage>. doi:<pub-id pub-id-type="doi">10.1098/rsos.170193</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barr</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Levy</surname> <given-names>R</given-names></string-name>, <string-name><surname>Scheepers</surname> <given-names>C</given-names></string-name>, <string-name><surname>Tily</surname> <given-names>HJ</given-names></string-name></person-group>. <year>2013</year>. <article-title>Random effects structure for confirmatory hypothesis testing: Keep it maximal</article-title>. <source>Journal of Memory and Language</source> <volume>68</volume>:<fpage>255</fpage>–<lpage>278</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jml.2012.11.001</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bates</surname> <given-names>D</given-names></string-name>, <string-name><surname>Mächler</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bolker</surname> <given-names>B</given-names></string-name>, <string-name><surname>Walker</surname> <given-names>S</given-names></string-name></person-group>. <year>2015</year>. <article-title>Fitting Linear Mixed-Effects Models Using lme4</article-title>. <source>J Stat Soft</source> <volume>67</volume>. doi:<pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baumgart</surname> <given-names>KG</given-names></string-name>, <string-name><surname>Byvshev</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sliby</surname> <given-names>A</given-names></string-name>, <string-name><surname>Strube</surname> <given-names>A</given-names></string-name>, <string-name><surname>König</surname> <given-names>P</given-names></string-name>, <string-name><surname>Wahn</surname> <given-names>B</given-names></string-name></person-group>. <year>2020</year>. <article-title>Neurophysiological correlates of collective perceptual decision-making</article-title>. <source>Eur J of Neuroscience</source> <volume>51</volume>:<fpage>1676</fpage>–<lpage>1696</lpage>. doi:<pub-id pub-id-type="doi">10.1111/ejn.14545</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bayliss</surname> <given-names>AP</given-names></string-name>, <string-name><surname>Frischen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fenske</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Tipper</surname> <given-names>SP.</given-names></string-name></person-group> <year>2007</year>. <article-title>Affective evaluations of objects are influenced by observed gaze direction and emotional expression q</article-title>, <source>Cognition</source>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boldt</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yeung</surname> <given-names>N</given-names></string-name></person-group>. <year>2015</year>. <article-title>Shared Neural Markers of Decision Confidence and Error Detection</article-title>. <source>J Neurosci</source> <volume>35</volume>:<fpage>3478</fpage>–<lpage>3484</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.0797-14.2015</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bolker</surname> <given-names>BM</given-names></string-name></person-group>. <year>2008</year>. <source>Ecological models and data in R</source>. <publisher-loc>Princeton, N.J</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonnen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Burge</surname> <given-names>J</given-names></string-name>, <string-name><surname>Yates</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cormack</surname> <given-names>LK</given-names></string-name></person-group>. <year>2015</year>. <article-title>Continuous psychophysics: Target-tracking to measure visual sensitivity</article-title>. <source>Journal of Vision</source> <volume>15</volume>:<fpage>14</fpage>. doi:<pub-id pub-id-type="doi">10.1167/15.3.14</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonnen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Huk</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Cormack</surname> <given-names>LK</given-names></string-name></person-group>. <year>2017</year>. <article-title>Dynamic mechanisms of visually guided 3D motion tracking</article-title>. <source>Journal of Neurophysiology</source> <volume>118</volume>:<fpage>1515</fpage>–<lpage>1531</lpage>. doi:<pub-id pub-id-type="doi">10.1152/jn.00831.2016</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brooks</surname> <given-names>M E</given-names></string-name>, <string-name><surname>Kristensen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Benthem K J</surname>, <given-names>van</given-names></string-name>, <string-name><surname>Magnusson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Berg</surname> <given-names>C W</given-names></string-name>, <string-name><surname>Nielsen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Skaug</surname> <given-names>H J</given-names></string-name>, <string-name><surname>Mächler</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bolker</surname> <given-names>B M.</given-names></string-name></person-group> <year>2017</year>. <article-title>glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling</article-title>. <source>The R Journal</source> <volume>9</volume>:<fpage>378</fpage>. doi:<pub-id pub-id-type="doi">10.32614/RJ-2017-066</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Czeszumski</surname> <given-names>A</given-names></string-name>, <string-name><surname>Eustergerling</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lang</surname> <given-names>A</given-names></string-name>, <string-name><surname>Menrath</surname> <given-names>D</given-names></string-name>, <string-name><surname>Gerstenberger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schuberth</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schreiber</surname> <given-names>F</given-names></string-name>, <string-name><surname>Rendon</surname> <given-names>ZZ</given-names></string-name>, <string-name><surname>König</surname> <given-names>P</given-names></string-name></person-group>. <year>2020</year>. <article-title>Hyperscanning: A Valid Method to Study Neural Inter-brain Underpinnings of Social Interaction</article-title>. <source>Front Hum Neurosci</source> <volume>14</volume>:<fpage>39</fpage>. doi:<pub-id pub-id-type="doi">10.3389/fnhum.2020.00039</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Martino</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bobadilla-Suarez</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nouguchi</surname> <given-names>T</given-names></string-name>, <string-name><surname>Sharot</surname> <given-names>T</given-names></string-name>, <string-name><surname>Love</surname> <given-names>BC.</given-names></string-name></person-group> <year>2017</year>. <article-title>Social Information Is Integrated into Value and Confidence Judgments According to Its Reliability</article-title>. <source>J Neurosci</source> <volume>37</volume>:<fpage>6066</fpage>–<lpage>6074</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3880-16.2017</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dobson</surname> <given-names>A</given-names></string-name></person-group>. <year>2001</year>. <article-title>An Introduction to Generalized Linear Models, Second Edition, Chapman &amp; Hall/CRC Texts in Statistical Science</article-title><source>. Chapman and Hall/CRC</source>. doi:<pub-id pub-id-type="doi">10.1201/9781420057683</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dosso</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Roberts</surname> <given-names>KH</given-names></string-name>, <string-name><surname>DiGiacomo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group>. <year>2018</year>. <article-title>The Influence of Co-action on a Simple Attention Task: A Shift Back to the Status Quo</article-title>. <source>Front Psychol</source> <volume>9</volume>:<fpage>874</fpage>. doi:<pub-id pub-id-type="doi">10.3389/fpsyg.2018.00874</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esmaily</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zabbah</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ebrahimpour</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name></person-group>. <year>2023</year>. <article-title>Interpersonal alignment of neural evidence accumulation to social exchange of confidence</article-title>. <source>eLife</source> <volume>12</volume>:<elocation-id>e83722</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.83722</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>L</given-names></string-name></person-group>. <year>2018</year>. <article-title>Ongoing, rational calibration of reward-driven perceptual biases</article-title>. <source>eLife</source> <volume>7</volume>:<fpage>1</fpage>–<lpage>26</lpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.36018</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Farrell</surname> <given-names>S</given-names></string-name></person-group>. <year>2011</year>. <article-title>Social influence benefits the wisdom of individuals in the crowd</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>108</volume>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1109947108</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fetsch</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Kiani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>WT</given-names></string-name>, <string-name><surname>Shadlen</surname> <given-names>MN</given-names></string-name></person-group>. <year>2014</year>. <article-title>Effects of Cortical Microstimulation on Confidence in a Perceptual Decision</article-title>. <source>Neuron</source> <volume>83</volume>:<fpage>797</fpage>–<lpage>804</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.011</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fetsch</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Odean</surname> <given-names>NN</given-names></string-name>, <string-name><surname>Jeurissen</surname> <given-names>D</given-names></string-name>, <string-name><surname>El-Shamayleh</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Horwitz</surname> <given-names>GD</given-names></string-name>, <string-name><surname>Shadlen</surname> <given-names>MN</given-names></string-name></person-group>. <year>2018</year>. <article-title>Focal optogenetic suppression in macaque area MT biases direction discrimination and decision confidence, but only transiently</article-title>. <source>eLife</source> <volume>7</volume>:<fpage>1</fpage>–<lpage>23</lpage>. doi:<pub-id pub-id-type="doi">10.7554/eLife.36523</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fleming</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Lau</surname> <given-names>HC</given-names></string-name></person-group>. <year>2014</year>. <article-title>How to measure metacognition</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>8</volume>:<fpage>1</fpage>–<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.3389/fnhum.2014.00443</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fleming</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Weil</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Nagy</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Rees</surname> <given-names>G</given-names></string-name></person-group>. <year>2010</year>. <article-title>Relating Introspective Accuracy to Individual Differences in Brain Structure</article-title>. <source>Science</source> <volume>329</volume>:<fpage>1541</fpage>–<lpage>1543</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1191883</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Forstmeier</surname> <given-names>W</given-names></string-name>, <string-name><surname>Schielzeth</surname> <given-names>H</given-names></string-name></person-group>. <year>2011</year>. <article-title>Cryptic multiple hypotheses testing in linear models: overestimated effect sizes and the winner’s curse</article-title>. <source>Behav Ecol Sociobiol</source> <volume>65</volume>:<fpage>47</fpage>–<lpage>55</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s00265-010-1038-5</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frith</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Singer</surname> <given-names>T</given-names></string-name></person-group>. <year>2008</year>. <article-title>The role of social cognition in decision making</article-title>. <source>Phil Trans R Soc B</source> <volume>363</volume>:<fpage>3875</fpage>–<lpage>3886</lpage>. doi:<pub-id pub-id-type="doi">10.1098/rstb.2008.0156</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brinksmeyer</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Eckhorn</surname>, <given-names>R.</given-names></string-name></person-group> <year>2004</year>. <article-title>Perception-related Modulations of Local Field Potential Power and Coherence in Primary Visual Cortex of Awake Monkey during Binocular Rivalry</article-title>. <source>Cerebral Cortex</source> <volume>14</volume>:<fpage>300</fpage>–<lpage>313</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bhg129</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Germar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Albrecht</surname> <given-names>T</given-names></string-name>, <string-name><surname>Voss</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mojzisch</surname> <given-names>A</given-names></string-name></person-group>. <year>2016</year>. <article-title>Social conformity is due to biased stimulus processing: electrophysiological and diffusion analyses</article-title>. <source>Social Cognitive and Affective Neuroscience</source> <volume>11</volume>:<fpage>1449</fpage>–<lpage>1459</lpage>. doi:<pub-id pub-id-type="doi">10.1093/scan/nsw050</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Stocker</surname> <given-names>AA</given-names></string-name></person-group>. <year>2017</year>. <article-title>Visual Decision-Making in an Uncertain and Dynamic World</article-title>. <source>Annu Rev Vis Sci</source> <volume>3</volume>:<fpage>227</fpage>–<lpage>250</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114511</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hanks</surname> <given-names>TD</given-names></string-name>, <string-name><surname>Summerfield</surname> <given-names>C</given-names></string-name></person-group>. <year>2017</year>. <article-title>Perceptual Decision Making in Rodents, Monkeys, and Humans</article-title>. <source>Neuron</source> <volume>93</volume>:<fpage>15</fpage>–<lpage>31</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.003</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huk</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bonnen</surname> <given-names>K</given-names></string-name>, <string-name><surname>He</surname> <given-names>BJ</given-names></string-name></person-group>. <year>2018</year>. <article-title>Beyond Trial-Based Paradigms: Continuous Behavior, Ongoing Neural Activity, and Natural Stimuli</article-title>. <source>J Neurosci</source> <volume>38</volume>:<fpage>7551</fpage>–<lpage>7558</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1920-17.2018</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Isbaner</surname> <given-names>S</given-names></string-name>, <string-name><surname>Mendoza</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Bothe</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fischer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gläscher</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lueschen</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Moeller</surname> <given-names>S</given-names></string-name>, <string-name><surname>Eiteljoerge</surname> <given-names>SFV</given-names></string-name>, <string-name><surname>Penke</surname> <given-names>L</given-names></string-name>, <string-name><surname>Priesemann</surname> <given-names>V</given-names></string-name>, <string-name><surname>Ruß</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schacht</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schneider</surname> <given-names>F</given-names></string-name>, <string-name><surname>Shahidi</surname> <given-names>N</given-names></string-name>, <string-name><surname>Treue</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wibral</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ziereis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fischer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name>, <string-name><surname>Mani</surname> <given-names>N</given-names></string-name></person-group>. <year>2025</year>. <article-title>Dyadic Interaction Platform: A novel tool to study transparent social interactions</article-title>, <source>PsyArXiv</source>. doi:<pub-id pub-id-type="doi">10.31234/osf.io/2rckn_v1</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kepecs</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mainen</surname> <given-names>ZF</given-names></string-name></person-group>. <year>2012</year>. <article-title>A computational framework for the study of confidence in humans and animals</article-title>. <source>Phil Trans R Soc B</source> <volume>367</volume>:<fpage>1322</fpage>–<lpage>1337</lpage>. doi:<pub-id pub-id-type="doi">10.1098/rstb.2012.0037</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khalvati</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kiani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Rao</surname> <given-names>RPN</given-names></string-name></person-group>. <year>2021</year>. <article-title>Bayesian inference with incomplete knowledge explains perceptual confidence and its deviations from accuracy</article-title>. <source>Nat Commun</source> <volume>12</volume>:<fpage>5704</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41467-021-25419-4</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Shadlen</surname> <given-names>MN</given-names></string-name></person-group>. <year>2009</year>. <article-title>Representation of Confidence Associated with a Decision by Neurons in the Parietal Cortex</article-title>. <source>Science</source> <volume>324</volume>:<fpage>759</fpage>–<lpage>764</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1169405</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Komura</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Nikkuni</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hirashima</surname> <given-names>N</given-names></string-name>, <string-name><surname>Uetake</surname> <given-names>T</given-names></string-name>, <string-name><surname>Miyamoto</surname> <given-names>A</given-names></string-name></person-group>. <year>2013</year><article-title>. Responses of pulvinar neurons reflect a subject’s confidence in visual categorization</article-title>. <source>nature NEUROSCIENCE</source>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kruger</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dunning</surname> <given-names>D</given-names></string-name></person-group>. <year>1999</year>. <article-title>Unskilled and unaware of it: How difficulties in recognizing one’s own incompetence lead to inflated self-assessments</article-title>. <source>Journal of Personality and Social Psychology</source> <volume>77</volume>:<fpage>1121</fpage>–<lpage>1134</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0022-3514.77.6.1121</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lewen</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ivanov</surname> <given-names>V</given-names></string-name>, <string-name><surname>Dehning</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ruß</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fischer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Penke</surname> <given-names>L</given-names></string-name>, <string-name><surname>Schacht</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Priesemann</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name></person-group>. <year>2025</year>. <article-title>Continuous dynamics of cooperation and competition in social foraging</article-title>, <source>bioRxiv</source>. doi:<pub-id pub-id-type="doi">10.1101/2025.05.28.655569</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lorenz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rauhut</surname> <given-names>H</given-names></string-name>, <string-name><surname>Schweitzer</surname> <given-names>F</given-names></string-name>, <string-name><surname>Helbing</surname> <given-names>D</given-names></string-name></person-group>. <year>2011</year>. <article-title>How social influence can undermine the wisdom of crowd effect</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>108</volume>:<fpage>9020</fpage>–<lpage>9025</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1008636108</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Madipakkam</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Bellucci</surname> <given-names>G</given-names></string-name>, <string-name><surname>Rothkirch</surname> <given-names>M</given-names></string-name>, <string-name><surname>Park</surname> <given-names>SQ</given-names></string-name></person-group>. <year>2019</year>. <article-title>The influence of gaze direction on food preferences</article-title>. <source>Sci Rep</source> <volume>9</volume>:<fpage>5604</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41598-019-41815-9</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahmoodi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Olsen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>YA</given-names></string-name>, <string-name><surname>Shi</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Broberg</surname> <given-names>K</given-names></string-name>, <string-name><surname>Safavi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Han</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nili Ahmadabadi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Frith</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Roepstorff</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rees</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name></person-group>. <year>2015</year>. <article-title>Equality bias impairs collective decision-making across cultures</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>112</volume>:<fpage>3835</fpage>–<lpage>3840</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1421692112</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Maniscalco</surname> <given-names>B</given-names></string-name>, <string-name><surname>Lau</surname> <given-names>H</given-names></string-name></person-group>. <year>2014</year>. <chapter-title>Signal Detection Theory Analysis of Type 1 and Type 2 Data: Meta-d1, Response-Specific Meta-d1, and the Unequal Variance SDT Model</chapter-title> In: <person-group person-group-type="editor"><string-name><surname>Fleming</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Frith</surname> <given-names>CD</given-names></string-name></person-group>, editors. <source>The Cognitive Neuroscience of Metacognition</source>. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>. pp. <fpage>25</fpage>–<lpage>66</lpage>. doi:<pub-id pub-id-type="doi">10.1007/978-3-642-45190-4_3</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maniscalco</surname> <given-names>B</given-names></string-name>, <string-name><surname>Lau</surname> <given-names>H</given-names></string-name></person-group>. <year>2012</year>. <article-title>A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings</article-title>. <source>Consciousness and Cognition</source> <volume>21</volume>:<fpage>422</fpage>–<lpage>430</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.concog.2011.09.021</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Matuschek</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R</given-names></string-name>, <string-name><surname>Vasishth</surname> <given-names>S</given-names></string-name>, <string-name><surname>Baayen</surname> <given-names>H</given-names></string-name>, <string-name><surname>Bates</surname> <given-names>D</given-names></string-name></person-group>. <year>2017</year>. <article-title>Balancing Type I error and power in linear mixed models</article-title>. <source>Journal of Memory and Language</source> <volume>94</volume>:<fpage>305</fpage>–<lpage>315</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jml.2017.01.001</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McCullagh</surname> <given-names>P</given-names></string-name>, <string-name><surname>Nelder</surname> <given-names>JA</given-names></string-name></person-group>. <year>1989</year>. <source>Generalized Linear Models</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer US</publisher-name>. doi:<pub-id pub-id-type="doi">10.1007/978-1-4899-3242-6</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mitsuda</surname> <given-names>T</given-names></string-name>, <string-name><surname>Masaki</surname> <given-names>S</given-names></string-name></person-group>. <year>2018</year>. <article-title>Subliminal gaze cues increase preference levels for items in the gaze direction</article-title>. <source>Cognition and Emotion</source> <volume>32</volume>:<fpage>1146</fpage>–<lpage>1151</lpage>. doi:<pub-id pub-id-type="doi">10.1080/02699931.2017.1371002</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moeller</surname> <given-names>S</given-names></string-name>, <string-name><surname>Unakafov</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Fischer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gail</surname> <given-names>A</given-names></string-name>, <string-name><surname>Treue</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name></person-group>. <year>2023</year>. <article-title>Human and macaque pairs employ different coordination strategies in a transparent decision game</article-title>. <source>eLife</source> <volume>12</volume>:<elocation-id>e81641</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.81641</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moreira</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Rollwage</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kaduk</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wilke</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kagan</surname> <given-names>I</given-names></string-name></person-group>. <year>2018</year>. <article-title>Post-decision wagering after perceptual judgments reveals bi-directional certainty readouts</article-title>. <source>Cognition</source> <volume>176</volume>:<fpage>40</fpage>–<lpage>52</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cognition.2018.02.026</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Navajas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name>, <string-name><surname>Latham</surname> <given-names>PE</given-names></string-name></person-group>. <year>2016</year>. <article-title>Post-decisional accounts of biases in confidence</article-title>. <source>Current Opinion in Behavioral Sciences</source> <volume>11</volume>:<fpage>55</fpage>–<lpage>60</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cobeha.2016.05.005</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noel</surname> <given-names>J-P</given-names></string-name>, <string-name><surname>Balzani</surname> <given-names>E</given-names></string-name>, <string-name><surname>Avila</surname> <given-names>E</given-names></string-name>, <string-name><surname>Lakshminarasimhan</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Bruni</surname> <given-names>S</given-names></string-name>, <string-name><surname>Alefantis</surname> <given-names>P</given-names></string-name>, <string-name><surname>Savin</surname> <given-names>C</given-names></string-name>, <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name></person-group>. <year>2022</year>. <article-title>Coding of latent variables in sensory, parietal, and frontal cortices during closed-loop virtual navigation</article-title>. <source>eLife</source> <volume>11</volume>:<elocation-id>e80280</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.80280</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noel</surname> <given-names>J-P</given-names></string-name>, <string-name><surname>Bill</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>H</given-names></string-name>, <string-name><surname>Vastola</surname> <given-names>J</given-names></string-name>, <string-name><surname>DeAngelis</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Drugowitsch</surname> <given-names>J</given-names></string-name></person-group>. <year>2023</year>. <article-title>Causal inference during closed-loop navigation: parsing of self- and object-motion</article-title>. <source>Phil Trans R Soc B</source> <volume>378</volume>:<fpage>20220344</fpage>. doi:<pub-id pub-id-type="doi">10.1098/rstb.2022.0344</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Park</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Goïame</surname> <given-names>S</given-names></string-name>, <string-name><surname>O’Connor</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Dreher</surname> <given-names>J-C</given-names></string-name></person-group>. <year>2017</year>. <article-title>Integration of individual and social information for decision-making in groups of different sizes</article-title>. <source>PLoS Biol</source> <volume>15</volume>:<fpage>e2001958</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pbio.2001958</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Persaud</surname> <given-names>N</given-names></string-name>, <string-name><surname>McLeod</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cowey</surname> <given-names>A</given-names></string-name></person-group>. <year>2007</year>. <article-title>Post-decision wagering objectively measures awareness</article-title>. <source>Nat Neurosci</source> <volume>10</volume>:<fpage>257</fpage>–<lpage>261</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1840</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pescetelli</surname> <given-names>N</given-names></string-name>, <string-name><surname>Rees</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bahrami</surname> <given-names>B</given-names></string-name></person-group>. <year>2016</year>. <article-title>The perceptual and social components of metacognition</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>145</volume>:<fpage>949</fpage>–<lpage>965</lpage>. doi:<pub-id pub-id-type="doi">10.1037/xge0000180</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pescetelli</surname> <given-names>N</given-names></string-name>, <string-name><surname>Yeung</surname> <given-names>N</given-names></string-name></person-group>. <year>2022</year>. <article-title>Benefits of spontaneous confidence alignment between dyad members</article-title>. <source>Collective Intelligence</source> <volume>1</volume>:<fpage>263391372211269</fpage>. doi:<pub-id pub-id-type="doi">10.1177/26339137221126915</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pescetelli</surname> <given-names>N</given-names></string-name>, <string-name><surname>Yeung</surname> <given-names>N</given-names></string-name></person-group>. <year>2020</year>. <article-title>The effects of recursive communication dynamics on belief updating</article-title>. <source>Proc R Soc B</source> <volume>287</volume>:<fpage>20200025</fpage>. doi:<pub-id pub-id-type="doi">10.1098/rspb.2020.0025</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rauhut</surname> <given-names>H</given-names></string-name>, <string-name><surname>Lorenz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schweitzer</surname> <given-names>F</given-names></string-name>, <string-name><surname>Helbing</surname> <given-names>D</given-names></string-name></person-group>. <year>2011</year>. <article-title>Reply to Farrell: Improved individual estimation success can imply collective tunnel vision</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>108</volume>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1111007108</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salzman</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Britten</surname> <given-names>KH</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>WT</given-names></string-name></person-group>. <year>1990</year>. <article-title>Cortical microstimulation influences perceptual judgements of motion direction</article-title>. <source>Nature</source> <volume>346</volume>:<fpage>174</fpage>–<lpage>177</lpage>. doi:<pub-id pub-id-type="doi">10.1038/346174a0</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schielzeth</surname> <given-names>H</given-names></string-name>, <string-name><surname>Forstmeier</surname> <given-names>W</given-names></string-name></person-group>. <year>2009</year>. <article-title>Conclusions beyond support: overconfident estimates in mixed models</article-title>. <source>Behavioral Ecology</source> <volume>20</volume>:<fpage>416</fpage>–<lpage>420</lpage>. doi:<pub-id pub-id-type="doi">10.1093/beheco/arn145</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname> <given-names>J</given-names></string-name></person-group>. <year>1997</year>. <article-title>The uncertain response in humans and animals</article-title>. <source>Cognition</source> <volume>62</volume>:<fpage>75</fpage>–<lpage>97</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0010-0277(96)00726-3</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smithson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Verkuilen</surname> <given-names>J</given-names></string-name></person-group>. <year>2006</year>. <article-title>A better lemon squeezer? Maximum-likelihood regression with beta-distributed dependent variables</article-title>. <source>Psychological Methods</source> <volume>11</volume>:<fpage>54</fpage>–<lpage>71</lpage>. doi:<pub-id pub-id-type="doi">10.1037/1082-989X.11.1.54</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Straub</surname> <given-names>D</given-names></string-name>, <string-name><surname>Rothkopf</surname> <given-names>CA</given-names></string-name></person-group>. <year>2022</year>. <article-title>Putting perception into action with inverse optimal control for continuous psychophysics</article-title>. <source>eLife</source> <volume>11</volume>. doi:<pub-id pub-id-type="doi">10.7554/eLife.76635</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szul</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Bompas</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sumner</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>J</given-names></string-name></person-group>. <year>2020</year>. <article-title>The validity and consistency of continuous joystick response in perceptual decision-making</article-title>. <source>Behav Res</source> <volume>52</volume>:<fpage>681</fpage>–<lpage>693</lpage>. doi:<pub-id pub-id-type="doi">10.3758/s13428-019-01269-3</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takagaki</surname> <given-names>K</given-names></string-name>, <string-name><surname>Krug</surname> <given-names>K</given-names></string-name></person-group>. <year>2020</year>. <article-title>The effects of reward and social context on visual processing for perceptual decision-making</article-title>. <source>Current Opinion in Physiology</source> <volume>16</volume>:<fpage>109</fpage>–<lpage>117</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cophys.2020.08.006</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Terenzi</surname> <given-names>D</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bellucci</surname> <given-names>G</given-names></string-name>, <string-name><surname>Park</surname> <given-names>SQ</given-names></string-name></person-group>. <year>2021</year>. <article-title>Determinants and modulators of human social decisions</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source> <volume>128</volume>:<fpage>383</fpage>–<lpage>393</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neubiorev.2021.06.041</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Toelch</surname> <given-names>U</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name></person-group>. <year>2015</year>. <article-title>Informational and Normative Influences in Conformity from a Neurocomputational Perspective</article-title>. <source>Trends in Cognitive Sciences</source> <volume>19</volume>:<fpage>579</fpage>–<lpage>589</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2015.07.007</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Toelch</surname> <given-names>U</given-names></string-name>, <string-name><surname>Pooresmaeili</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name></person-group>. <year>2018</year>. <article-title>Neural substrates of norm compliance in perceptual decisions</article-title>. <source>Sci Rep</source> <volume>8</volume>:<fpage>3315</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41598-018-21583-8</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Den Bos</surname> <given-names>R</given-names></string-name>, <string-name><surname>Jolles</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Homberg</surname> <given-names>JR.</given-names></string-name></person-group> <year>2013</year>. <article-title>Social modulation of decision-making: a cross-species review</article-title>. <source>Front Hum Neurosci</source> <volume>7</volume>. doi:<pub-id pub-id-type="doi">10.3389/fnhum.2013.00301</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yeung</surname> <given-names>N</given-names></string-name>, <string-name><surname>Summerfield</surname> <given-names>C</given-names></string-name></person-group>. <year>2012</year>. <article-title>Metacognition in human decision-making: confidence and error monitoring</article-title>. <source>Phil Trans R Soc B</source> <volume>367</volume>:<fpage>1310</fpage>–<lpage>1321</lpage>. doi:<pub-id pub-id-type="doi">10.1098/rstb.2011.0416</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101021.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kolling</surname>
<given-names>Nils</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Stem-cell and Brain Institute (SBRI), U1208 Inserm</institution>
</institution-wrap>
<city>Bron Cedex</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study developed a novel continuous dot-motion decision-making task, in which participants can see another player's responses as well as their own, to measure perceptual performance and confidence judgments in a social context. The study is a <bold>useful</bold> contribution to social decision-making primarily by introducing a new task and offering <bold>convincing</bold> evidence on how participants are impacted by others' decisions during continuous perceptual choices. The manuscript delivers clear evidence that participants judgements are driven by metacognitive confidence over simpler primary uncertainty.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101021.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper reports an interesting and clever task which allows the joint measurement of both perceptual judgments and confidence (or subjective motion strength) in real / continuous time. The task is used together with a social condition to identify the (incidental, task-irrelevant) impact of another player on decision-making and confidence. The paper is well-written and clear.</p>
<p>Strengths:</p>
<p>The innovation on the task alone is likely to be impactful for the field, extending recent continuous report (CPR) tasks to examine other aspects of perceptual decision-making and allowing more naturalistic readouts. One interesting and novel finding is the observation of dyadic convergence of confidence estimates even when the partner is incidental to the task performance, and that dyads tend to be more risk-seeking (indicating greater confidence) than when playing solo.</p>
<p>One concern with the novel task is whether confidence is disambiguated from a tracking of stimulus strength or coherence. The subjects' task is to track motion direction and use the eccentricity of the joystick to control the arc of a catcher - thus implementing a real-time sensitivity to risk (peri-decision wagering). The variable-width catcher has been used to good effect in other confidence/uncertainty tasks involving learning of the spread of targets (the Nassar papers). But in the context of an RDK task, one simple strategy here is to map eccentricity directly to (subjective) motion coherence - such that the joystick position at any moment in time is a vector with motion direction and strength. The revised version of the paper now includes a comprehensive analysis of the extent to which the metacognitive aspect of the task (the joystick eccentricity) tracks stimulus features such as motion coherence. The finding of a lagged relationship between task accuracy and eccentricity in conjunction with a relative lack of instantaneous relationships with coherence fluctuations, convincingly strengthens the inference that this component of the joystick response is metacognitive in nature, and dynamically tracking changes in performance. This importantly rebuts a more deflationary framing of the metacognitive judgment, in which what the subjects might be doing is tracking two features of the world - instantaneous motion strength and direction.</p>
<p>The claim that the novel task is tracking confidence is also supported by new analyses showing classic statistical features of explicit confidence judgments (scaling with aggregate accuracy, and tracking psychometric function slope) are obtained with the joystick eccentricity measure.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101021.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Schneider et al examine perceptual decision-making in a continuous task setup when social information is also provided to another human (or algorithmic) partner. The authors track behaviour in a visual motion discrimination task and report accuracy, hit rate, wager, and reaction times, demonstrating that choice wager is affected by social information from the partner.</p>
<p>Strengths:</p>
<p>There are many things to like about this paper. The visual psychophysics has been undertaken with much expertise and care to detail. The reporting is meticulous and the coverage of the recent previous literature is reasonable. The research question is novel.</p>
<p>Comments on revisions:</p>
<p>The authors have addressed my suggestions adequately</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101021.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schneider</surname>
<given-names>Felix</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4798-3054</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Calapai</surname>
<given-names>Antonino</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mundry</surname>
<given-names>Roger</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Báez-Mendoza</surname>
<given-names>Raymundo</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gail</surname>
<given-names>Alexander</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1165-4646</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Kagan</surname>
<given-names>Igor</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1814-4200</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Treue</surname>
<given-names>Stefan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1:</bold></p>
<p>Strengths:</p>
<p>The innovation on the task alone is likely to be impactful for the field, extending recent continuous report (CPR) tasks to examine other aspects of perceptual decision-making and allowing more naturalistic readouts. One interesting and novel finding is the observation of dyadic convergence of confidence estimates even when the partner is incidental to the task performance, and that dyads tend to be more risk-seeking (indicating greater confidence) than when playing solo. The paper is well-written and clear.”</p>
</disp-quote>
<p>We thank reviewer 1 for this encouraging evaluation. Below we address the identified weaknesses and recommendations.</p>
<disp-quote content-type="editor-comment">
<p>(1) Do we measure metacognitive confidence?</p>
<p>One concern with the novel task is whether confidence is disambiguated from a tracking of stimulus strength or coherence. […] But in the context of an RDK task, one simple strategy here is to map eccentricity directly to (subjective) motion coherence - such that the joystick position at any moment in time is a vector with motion direction and strength. This would still be an interesting task - but could be solved without invoking metacognition or the need to estimate confidence in one's motion direction decision. […] what the subjects might be doing is tracking two features of the world - motion strength and direction. This possibility needs to be ruled out if the authors want to claim a mapping between eccentricity and decision confidence […].”</p>
</disp-quote>
<p>We thank reviewer 1 for pointing out that the joystick tilt responses of our subjects could potentially be driven by stimulus coherence instead of metacognitive decision confidence. Below, we present four arguments to address this point of concern:</p>
<p>(1.1) Similar physical coherence between high and low confidence states</p>
<p>Nominal motion coherence is a discrete value, but the random noisiness in the stimulus causes the actual frame-by-frame coherence to be distributed around this nominal value. Because of this, subjects might scale their joystick tilt report according to the coherence fluctuations around the nominal value. To check if this was the case, we use a median split to separate stimulus states into states with large versus small joystick tilt, individually for each nominal coherence. For each stimulus state, we extracted the actual instantaneous (frame-to-frame) motion coherence, which is based on the individual movements of dots in the stimulus patch between two frames, recorded in our data files.</p>
<p>First, we compared the motion coherence between stimulus states with large versus small joystick tilt. For each stimulus state, we calculated average instantaneous motion coherence, and analyzed the difference of the medians for the large versus small tilt distributions for each subject and each coherence level. The resulting histograms show the distribution of differences across all 38 subjects for each nominal coherence, and are, except for the coherence of 22%, not significantly different from zero across subjects (Author response image 1). For the 22% coherence condition, the difference amounts to 0.19% – a very small, non-perceptible difference. Thus, we do no find systematic differences between the average motion coherence in states with high versus low joystick tilt.</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<caption>
<title>Histograms of within-subject difference between medians of average coherence distributions with large and small joystick tilt for all subjects.</title>
<p>Coherence is color-coded (cyan – 0%, magenta – 98%). On top, the title of each panel illustrates the number of significant differences (Ranksum test in each subject) without correction for multiple comparisons (see Author response table 1 below). In the second row of the title, we show the result of the population t-test against zero. Only 22% coherence shows a significant bias. Positive values indicate higher average coherence for large joystick tilt.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101021-sa3-fig1.jpg" mimetype="image"/>
</fig>
<table-wrap id="sa3table1">
<label>Author response table 1.</label>
<caption>
<title>List of all individual significantly different coherence distributions between high and low tilt states, without correction for multiple comparisons.</title>
<p>Median differences do not show a consistent bias (i.e. positive values) that would indicate higher average coherence for the large tilts.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101021-sa3-table1.jpg" mimetype="image"/>
</table-wrap>
<disp-quote content-type="editor-comment">
<p>(1.2) Short-term stimulus fluctuations have no effect</p>
<p>[…] But to fully characterise the task behaviour it also seems important to ask how and whether fluctuations in motion energy (assuming that the RDK frames were recorded) during a steady state phase are affecting continuous reporting of direction and eccentricity, prior to asking how social information is incorporated into subjects' behaviour.</p>
</disp-quote>
<p>In addition to the analysis of stimulus coherence and tilt averaged across each stimulus state (1.1), we analyzed moment-to-moment relationship between instantaneous coherence and ongoing reports of accuracy and tilt. Below, we provide evidence that short-term fluctuations in the instantaneous coherence (i.e. the motion energy of the stimulus) do not result in correlated changes in joystick responses, neither for tilt nor accuracy. For each continuous stimulus state, we calculated cross-correlation functions between the instantaneous coherence, tilt and accuracy, and then averaged the cross-correlation across all states of the same nominal coherence, and then across subjects. The resulting average cross-correlation functions are essentially flat. This further supports our interpretation that the joystick reports do not reflect short-term fluctuations of motion energy.</p>
<fig id="sa3fig2">
<label>Author response image 2.</label>
<caption>
<title>Cross-correlation between the length of the resultant vector with joystick accuracy (left) and tilt (right).</title>
<p>Coherence is color-coded. Shaded background illustrates 95% confidence intervals.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101021-sa3-fig2.jpg" mimetype="image"/>
</fig>
<p>(1.3) Joystick tilt changes over time despite stable average stimulus coherence</p>
<p>If perceptual confidence is derived from evidence integration, we should see changes over time even when the stimulus is stable. Here, we have analyzed the average slope of the joystick tilt as a function of time within each stimulus state for each subject and each coherence, to verify if our participants tilted their joystick more with additional evidence. This is illustrated with a violin plot below (Author response image 3). The linear slopes of the joystick tilt progression over the course of stimulus states are different between coherence levels. High coherence causes more tilt over time, resulting in positive slopes for most subjects. In contrast, low/no coherence results mostly in flat or negative slopes. This tilt progression over time indicates that low coherence results in lower confidence, as subjects do not wager more with weak evidence. In contrast, high coherence causes subjects to exhibit more confidence, indicated by positive slope of the joystick tilt.</p>
<fig id="sa3fig3">
<label>Author response image 3.</label>
<caption>
<title>Violin plots showing the fitted slopes of the joystick tilt time course in the last 200 samples (1667 ms) leading up to a next stimulus direction (cf.</title>
<p>Figure 2D). Positive values signify an increase in joystick tilt over time. Each dot shows the average slope for one subject. Coherence is color-coded. The dashed line at zero indicates unchanged joystick tilt over the analyzed time window.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101021-sa3-fig3.jpg" mimetype="image"/>
</fig>
<p>(1.4) Cross-correlation between response accuracy and joystick tilt</p>
<p>Similar to 1.2 above, we have cross-correlated the frame-by-frame changes of joystick accuracy and tilt for each individual stimulus state and each subject. Across subjects, changes in tilt occur later than changes in accuracy, indicating that changes in the quality of the report are followed by changes in the size of the wager. Given that this process is not driven by short-term changes in the motion energy of the stimulus (see 1.2 above), we interpret this as additional evidence for a metacognitive assessment of the quality of the behavioral report (i.e. accuracy) reflected in the size of the wager (our measure for confidence). (See Figure 2E).</p>
<disp-quote content-type="editor-comment">
<p>(2) Peri-decision wagering is different to post-decision wagering</p>
<p>[…] One route to doing this would be to ask whether the eccentricity reports show statistical signatures of confidence that have been established for more classical punctate tasks. Here a key move has been to identify qualitative patterns in the frame of reference of choice accuracy - with confidence scaling positively with stimulus strength for correct decisions, and negatively with stimulus strength for incorrect decisions (the so-called X-pattern, for instance Sanders et al. 2016 Neuron […].</p>
</disp-quote>
<p>We thank reviewer 1 for the constructive feedback. Our behavioral data do not show similar signatures to the previously reported post-decision confidence expression (Desender et al., 2021; Sanders et al., 2016). The previously described patterns show, first of all, that confidence for the incorrect type1 decisions diverges from the correct type1 decisions, declining with stimulus strength (e.g. coherence), as compared to increase for correct decisions. In our task, there is a graded accuracy and (putative) confidence expression, but there are no correct or incorrect decisions – instead, there are hits and misses of the reward targets presented at nominal directions. Instead of a decline for misses, we observe an equally positive scaling with coherence for the confidence, both for hits and misses (Author response image 4A). This is because in our peri-decision wagering task, the expression of confidence causally determines the binary hit or miss outcome. The outcome in our task is a function of the two-dimensional joystick response: higher tilt (confidence) requires a more accurate response to successfully hit a target. Thus, a subject can display a high (but not high enough) level of accuracy and confidence but still remain unsuccessful. If we instead median-split the confidence reports by high and low accuracy (Author response image 4C), we observe a slight separation, especially for higher coherences, but still no clear different in slopes.</p>
<p>We do observe the other two dynamic signatures of confidence (Desender et al., 2021): signature 2 – monotonically increasing accuracy as a function of confidence (Author response image 4), and signature 3 – steeper type 1 psychometric performance (accuracy) for high versus low confidence (Author response image 4D).</p>
<fig id="sa3fig4">
<label>Author response image 4.</label>
<caption>
<title>Confidence (i.</title>
<p>e., joystick tilt, left column) and accuracy reports (right column) for different stimulus coherence, sorted by discrete outcome (hit versus miss, upper row) and the complementary joystick dimension (lower row, based on median split).</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101021-sa3-fig4.jpg" mimetype="image"/>
</fig>
<fig id="sa3fig5">
<label>Author response image 5.</label>
<caption>
<title>Accuracy reports correlate positively with confidence reports.</title>
<p>For each stimulus state, we averaged the joystick response in the time window between 500 ms (60 samples) after a direction change until the first reward target appearance. If there was no target, we took all samples until the next RDP direction change into account. This corresponds to data snippets averaged in Figure 2D. Thus, for each stimulus state, we extracted a single value for joystick accuracy and for tilt (confidence). Subsequently, we fitted a linear regression to the accuracy-confidence scatter within each subject and within each coherence level. The plot above shows the average linear regression between accuracy and confidence across all subjects (i.e., the slopes and intercepts were averaged across n=38 subjects). Coherence is color-coded.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-101021-sa3-fig5.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>(3)  Additional analyses regarding the continuous nature of our data</p>
<p>I was surprised not to see more analysis of the continuous report data as a function of (lagged) task variables. […]</p>
</disp-quote>
<p>Reviewer 1 requested more analyses regarding the continuous nature of our data. We agree that this is a useful addition to our paper, and thank reviewer 1 for this suggestion. To address this point, we revised main Figure 2 and provided additional panels. Panel D illustrates the continuous ramp-up of both accuracy and tilt (confidence) for high coherence levels, suggesting ongoing evidence integration and meta-cognitive assessment. Panel E shows the cross-correlation between frame-by-frame changes in accuracy and tilt (see 1.4 above). Here, we demonstrate that changes in the accuracy precede changes in joystick tilt, characterizing the continuous nature of the perceptual decision-making process.</p>
<disp-quote content-type="editor-comment">
<p>(4) Explicit motivation regarding continuous social experiments</p>
<p>This paper is innovating on a lot of fronts at once - developing a new CPR task for metacognition, and asking exploratory questions about how a social setting influences performance on this novel task. However, the rationale for this combination was not made explicit. Is the social manipulation there to help validate the new task as a measure of confidence as dissociated from other perceptual variables? (see query 1 below). Or is the claim that the social influence can only be properly measured in the naturalistic CPR task, and not in a more established metacognition task?</p>
</disp-quote>
<p>Our rationale for the combination of real-time decision making and social settings was twofold:</p>
<p>i. Primates, including humans, are social species. Naturally, most behavior is centered around a social context and continuously unfolds in real-time. We wanted to showcase a paradigm in which distinct aspects of continuous perceptual decision-making could be assessed over time in individual and social environments.</p>
<p>ii. Human behavior is susceptible to what others think and do. We wanted to demonstrate that the sheer presence of a co-acting social partner affects continuous decision-making, and quantify the extent and direction of social modulation.</p>
<p>We agree that the motivation for combining the new task and this specific type of social co-action should be more clear. We have clarified this aspect in the Introduction, line 92-109. In brief, the continuous, free-flowing nature of the CPR task and real-time availability of social information made this design a very suitable paradigm for assessing unconstrained social influences. We see this study as the first step into disentangling the neural basis of social modulation in primates. See also the response to reviewer 2, point 2, below.</p>
<disp-quote content-type="editor-comment">
<p>(5) Response to minor points</p>
<p>(5.1)  Clarification on behavioral modulation patterns</p>
<p>Lines 295-298, isn't it guaranteed to observe these three behavioral patterns (both participants improving, both getting worse, only one improving while the other gets worse) even in random data?</p>
</disp-quote>
<p>The reviewer is correct. We now simply illustrate these possibilities in Figure 4B and how these patterns could lead to divergence or convergence between the participants (see also line 282). Unlike random data, our results predominantly demonstrate convergence.</p>
<disp-quote content-type="editor-comment">
<p>(5.2) Clarification on AUC distributions</p>
<p>Lines 703-707, it wasn't clear what the AUC values referred to here (also in Figure 3) - what are the distributions that are being compared? I think part of the confusion here comes from AUC being mentioned earlier in the paper as a measure of metacognitive sensitivity (correct vs. incorrect trial distributions), whereas my impression here is that here AUC is being used to investigate differences in variables (e.g., confidence) between experimental conditions.</p>
</disp-quote>
<p>We apologize for the confusion. Indeed, the AUC analysis was used for the two purposes:</p>
<p>(i) To assess the metacognitive sensitivity (line 175, Supplementary Figure 2).</p>
<p>(ii) To assess the social modulation of accuracy and confidence (starting at line 232, Figures 3-6).</p>
<p>We now introduce the second AUC approach for assessing social modulation, and the underlying distributions of accuracy and confidence derived from each stimulus state, separately in each subject, in line 232.</p>
<disp-quote content-type="editor-comment">
<p>(5.3) Clarification of potential ceiling effects</p>
<p>Could the findings of the worse solo player benefitting more than the better solo player (Figure 4c) be partly due to a compressive ceiling effect - e.g., there is less room to move up the psychometric function for the higher-scoring player?</p>
</disp-quote>
<p>We thank the reviewer for this insight. First, even better performing participants were not at ceiling most of the times, even at the highest coherence (cf. Figure 2 and Supplementary Figure 3C). To test for the potential ceiling effect in the better solo players, we correlated their social modulation (expressed as AUC as in Figure 4) to the solo performance. There was no significant negative correlation for the accuracy (p &gt; 0.063), but there was a negative correlation for the confidence (r = - 0.39, p = 0.0058), indicating that indeed low performing “better players in a dyad” showed more positive social modulation. We note however that this correlation was driven mainly by few such initially low performing “better” players, who mostly belonged to the dyads where both participants improved in confidence (green dots, Figure 4B), and that even the highest solo average confidence was at ceiling (&lt;0.95). To conclude, the asymmetric social modulation effect we observe is mainly due to the better players declining (orange and red dots, Figure 4B), rather than due to both players improving but the better player improving less (green dots, Figure 4B).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2:</bold></p>
<p>Strengths:</p>
<p>There are many things to like about this paper. The visual psychophysics has been undertaken with much expertise and care to detail. The reporting is meticulous and the coverage of the recent previous literature is reasonable. The research question is novel.</p>
</disp-quote>
<p>We thank reviewer 2 for this positive evaluation. Below we address the identified weaknesses and recommendations.</p>
<disp-quote content-type="editor-comment">
<p>(1) Streamlining the text to make the paper easier to read</p>
<p>The paper is difficult to read. It is very densely written, with little to distinguish between what is a key message and what is an auxiliary side note. The Figures are often packed with sometimes over 10 panels and very long captions that stick to the descriptive details but avoid clarity. There is much that could be shifted to supplementary material for the reader to get to the main points.</p>
</disp-quote>
<p>We thank reviewer 2 for the honest assessment that our article was difficult to read and understand, and for providing specific examples of confusion. We substantially improved the clarity:</p>
<p>We added a Glossary that defines key terms, including Accuracy and Hit rate.</p>
<p>We replaced the confusing term “eccentricity” with joystick “tilt”.</p>
<p>We simplified Figures 3 and 5, moving some panels into supplementary figures.</p>
<p>We substantially redesigned and simplified our main Figure 4, displaying the data in a more straightforward, less convoluted way, and removing several panels. This change was accompanied by corresponding changes in the text (section starting at line 277).</p>
<p>More generally, we shortened the Introduction, substantially revised the Results and the figure legends, and streamlined the Discussion.</p>
<disp-quote content-type="editor-comment">
<p>(2) Dyadic co-action vs joint dyadic decision making</p>
<p>A third and very important one is what the word &quot;dyadic&quot; refers to in the paper. The subjects do not make any joint decisions. However, the authors calculate some &quot;dyadic score&quot; to measure if the group has been able to do better than individuals. So the word dyadic sometimes refers to some &quot;nominal&quot; group. In other places, dyadic refers to the social experimental condition. For example, we see in Figure 3c that AUC is compared for solo vs dyadic conditions. This is confusing.</p>
<p>[…] my key criticism is that the paper makes strong points about collective decision-making and compares its own findings with many papers in that field when, in fact, the experiments do not involve any collective decision-making. The subjects are not incentivized to do better as a group either. […]</p>
</disp-quote>
<p>The reviewer is correct to highlight these important aspects. We did, in fact, not investigate a situation where two players had to reach a joint decision with interdependent payoff and there was no incentive to collaborate or even incorporate the information provided by the other player. To make the meaning of “dyadic” in our context more explicit, we have clarified the nature of the co-action and independent payoff (e.g. lines 107, 211, 482, 755 - Glossary), and used the term “nominal combined score” (line 224) and “nominal “average accuracy” within a dyad” (line 439).</p>
<p>Concerning the key point about embedding our findings into the literature on collective decision-making, we would like to clarify our motivation. Outside of the recent study by Pescetelli and Yeung, 2022, we are not aware of any perceptual decision-making studies that investigated co-action without any explicit joint task. So naturally, we were stimulated by the literature on collective decisions, and felt it is appropriate to compare our findings to the principles derived from this exciting field.  Besides developing continuous – in time and in “space” (direction) – peri-decision wagering CPR game, the social co-action context is the main novel contribution of our work. Although it is possible to formulate cooperative or competitive contexts for the CPR, we leveraged the free-flowing continuous nature of the task that makes it most readily amendable to study spontaneously emerging social information integration.</p>
<p>We now more explicitly emphasize that most prior work has been done using the joint decision tasks, in contrast to the co-action we study here, in Introduction and Discussion.</p>
<disp-quote content-type="editor-comment">
<p>(3) Addition of relevant literature to Discussion</p>
<p>[…] To see why this matters, look at Lorenz et al PNAS (<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/10.1073/pnas.1008636108">https://www.pnas.org/doi/10.1073/pnas.1008636108</ext-link>) and the subsequent commentary that followed it from Farrell (<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/full/10.1073/pnas.1109947108">https://www.pnas.org/doi/full/10.1073/pnas.1109947108</ext-link>). The original paper argued that social influence caused herding which impaired the wisdom of crowds. Farrell's reanalysis of the paper's own data showed that social influence and herding benefited the individuals at the expense of the crowd demonstrating a form of tradeoff between individual and joint payoff. It is naive to think that by exposing the subjects to social information, we should, naturally, expect them to strive to achieve better performance as a group.</p>
<p>Another paper that is relevant to the relationship between the better and worse performing members of the dyad is Mahmoodi et al PNAS 2015 (<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/10.1073/pnas.1421692112">https://www.pnas.org/doi/10.1073/pnas.1421692112</ext-link>). Here too the authors demonstrate that two people interacting with one another do not &quot;bother&quot; figuring out each others' competence and operate under &quot;equality assumption&quot;. Thus, the lesser competent member turns out to be overconfident, and the more competent one is underconfident. The relevance of this paper is that it manages to explain patterns very similar to Schneider et al by making a much simpler &quot;equality bias&quot; assumption.</p>
</disp-quote>
<p>We thank reviewer 2 for pointing out these highly relevant references, which we have now integrated in the Discussion (lines 430 and 467). Regarding the debate of Lorenz et al and Farell, although it is about very different type of tasks – single-shot factual knowledge estimation, it is very illuminating for understanding the differing perspectives on individual vs group benefit. We fully agree that it is naïve to assume that during independent co-action in our highly demanding task participants would strive to achieve better performance as a group – if anything, we expected less normative and more informational, reliability-driven effects as a way to cope with task demands.</p>
<p>Mahmoodi et al. is a particularly pertinent and elegant study, and the equality bias they demonstrate may indeed underlie the effects we see. We admit that we did not know this paper at the time of our initial writing, but it is encouraging to see the convergence [pun intended] despite task and analysis differences. As highlighted above (2), our novel contributions remain that we observe mutual alignment, or convergence, in real-time without explicitly formulated collective decision task and associated social pressure, and that we separate asymmetric social effects on accuracy and confidence.</p>
<p>Other reviewer-independent changes:</p>
<p>Additional information: Angular error in Figure 2</p>
<p>In panel A of the main Figure 2, we have added the angular error of the solo reports (blue dashed line) to give readers an impression about the average deviation of subjects’ joystick direction from the nominal stimulus direction. We have pointed out that angular error is the basis for accuracy calculation.</p>
<p>Data alignment</p>
<p>In the previous version of the manuscript, we have presented data with different alignments: Accuracy values were aligned to the appearance of the first target in a stimulus state (target-alignment) to avoid the predictive influence of target location within the remaining stimulus state, while the joystick tilt was extracted at the end of each stimulus state (state-alignment) to allow subjects more time to make a deliberate, confidence-guided report (Methods). We realized that this is confusing as it compares the social modulation of the two response dimensions at different points in time. In the revision, we use state-aligned data in most figures and analyses and clearly indicate which alignment type has been used. We kept the target-alignment for the illustration of the angular error in the solo-behavior (Figure 2). Specifically, this has only changed the reporting on accuracy statistics. None of the results have changed fundamentally, but the social modulation on accuracy became even stronger in state-aligned data.</p>
<p>In summary, we hope that these revisions have resulted in an easier-to-understand and convincing article, with clear terminology and concise and important takeaway messages.</p>
<p>We thank both reviewers and the editors again for their time and effort, and look forward to the reevaluation of our work.</p>
<p>References</p>
<p>Desender K, Donner TH, Verguts T. 2021. Dynamic expressions of confidence within an evidence accumulation framework. Cognition 207:104522. doi:10.1016/j.cognition.2020.104522</p>
<p>Pescetelli N, Yeung N. 2022. Benefits of spontaneous confidence alignment between dyad members. Collective Intelligence 1. doi:10.1177/26339137221126915</p>
<p>Sanders JI, Hangya B, Kepecs A. 2016. Signatures of a Statistical Computation in the Human Sense of Confidence. Neuron 90:499–506. doi:10.1016/j.neuron.2016.03.025</p>
</body>
</sub-article>
</article>