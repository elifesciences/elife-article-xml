<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">102800</article-id>
<article-id pub-id-type="doi">10.7554/eLife.102800</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.102800.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Physics of Living Systems</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Advantageous and disadvantageous inequality aversion can be taught through vicarious learning of others’ preferences</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Zhang</surname>
<given-names>Shen</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A2">2</xref>
<email xlink:href="mailto:Shen.Zhang@mail.bnu.edu.cn">Shen.Zhang@mail.bnu.edu.cn</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>FeldmanHall</surname>
<given-names>Oriel</given-names>
</name>
<xref ref-type="aff" rid="A3">3</xref>
<xref ref-type="aff" rid="A4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hétu</surname>
<given-names>Sébastien</given-names>
</name>
<xref ref-type="aff" rid="A5">5</xref>
<xref ref-type="aff" rid="A6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Otto</surname>
<given-names>A Ross</given-names>
</name>
<xref ref-type="aff" rid="A7">7</xref>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/059y0zb32</institution-id><institution>State Key Laboratory of Cognitive Neuroscience and Learning &amp; IDG/McGovern Institute for Brain Research, Beijing Normal University</institution></institution-wrap>, <city>Beijing</city>, <country>China</country></aff>
<aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Department of Neurobiology, German Primate Center</institution></institution-wrap>, <city>Göttingen</city>, <country>Germany</country></aff>
<aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Cognitive, Linguistics and Psychological Sciences, Brown University</institution></institution-wrap>, <city>Providence</city>, <country>United States</country></aff>
<aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Carney Institute for Brain Sciences, Brown University</institution></institution-wrap>, <city>Providence</city>, <country>United States</country></aff>
<aff id="A5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0161xgx34</institution-id><institution>Department of Psychology, Université de Montréal</institution></institution-wrap>, <city>Montréal</city>, <country>Canada</country></aff>
<aff id="A6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0161xgx34</institution-id><institution>Centre Interdisciplinaire de Recherche sur le Cerveau et l’Apprentissage (CIRCA)</institution></institution-wrap>, <addr-line>Montréal</addr-line>, <country>Canada</country></aff>
<aff id="A7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>Department of Psychology, McGill University</institution></institution-wrap>, <city>Montréal</city>, <country>Canada</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Büchel</surname>
<given-names>Christian</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University Medical Center Hamburg-Eppendorf</institution>
</institution-wrap>
<city>Hamburg</city>
<country>Germany</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2025-01-06">
<day>06</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP102800</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-09-12">
<day>12</day>
<month>09</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-08-26">
<day>26</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2405.06500"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Zhang et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Zhang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-102800-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>While enforcing egalitarian social norms is critical for human society, punishing social norm violators often incurs a cost to the self. This cost looms even larger when one can benefit from an unequal distribution of resources, a phenomenon known as advantageous inequity—for example, receiving a higher salary than a colleague with the identical role. In the Ultimatum Game, a classic testbed for fairness norm enforcement, individuals rarely reject (or punish) such unequal proposed divisions of resources because doing so entails a sacrifice of one’s own benefit. Recent work has demonstrated that observing and implementing another’s punitive responses to unfairness can efficiently alter the punitive preferences of an observer. It remains an open question, however, whether such contagion is powerful enough to impart advantageous inequity aversion to individuals—that is, can observing another’s preferences to punish inequity result in increased enforcement of equality norms, even in the difficult case of AI? Using a variant of the Ultimatum Game in which participants are tasked with responding to fairness violations on behalf of another ‘Teacher’—whose aversion to advantageous (versus disadvantageous) inequity was systematically manipulated—we probe whether individuals subsequently increase their punishment unfair after experiencing fairness violations on their own behalf. In two experiments, we found individuals can acquire aversion to advantageous inequity ‘vicariously’ through observing (and implementing) the Teacher’s preferences. Computationally, these learning effects were best characterized by a model which learns the latent structure of the Teacher’s preferences, rather than a simple Reinforcement Learning account. In summary, our study is the first to demonstrate that people can swiftly and readily acquire another’s preferences for advantageous inequity, suggesting in turn that behavioral contagion may be one promising mechanism through which social norm enforcement— which people rarely implement in the case of advantageous inequality—can be enhanced.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>Humans can learn how to navigate through the world by observing the actions of others. For example, individuals can learn complex motor skills by observing and imitating how experts coordinate their movements (<xref ref-type="bibr" rid="c26">Hayes et al., 2008</xref>). Observational learning can also transmit important information about social and moral norms, such as reciprocity (<xref ref-type="bibr" rid="c17">Engelmann &amp; Fischbacher, 2009</xref>), cooperating with others (<xref ref-type="bibr" rid="c50">van Baar et al., 2019</xref>), or context in which punishment is considered appropriate (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>). In some cases, an individual learns from others in a straightforward and unambiguous social context, where the tensions endemic to many moral dilemmas—e.g., benefit one-self versus the collective good—are not directly juxtaposed against one another. And yet, the social world is rarely straightforward, often ambiguous, and moral dilemmas that pit self-benefit over the collective good abound (<xref ref-type="bibr" rid="c22">FeldmanHall &amp; Shenhav, 2019</xref>; <xref ref-type="bibr" rid="c53">Vives &amp; Feldmanhall, 2018</xref>).</p>
<p>Consider the case of inequity for which individuals exhibit a strong distaste: concerns for fairness are well documented in adults (<xref ref-type="bibr" rid="c25">Güth et al., 1982</xref>; <xref ref-type="bibr" rid="c44">Sanfey et al., 2003</xref>), children (<xref ref-type="bibr" rid="c19">Fehr et al., 2008</xref>; <xref ref-type="bibr" rid="c39">McAuliffe &amp; Dunham, 2017</xref>), primates (<xref ref-type="bibr" rid="c10">Brosnan &amp; De Waal, 2003</xref>, <xref ref-type="bibr" rid="c11">2014</xref>; <xref ref-type="bibr" rid="c51">Van Wolkenten et al., 2007</xref>) and even domesticated dogs (<xref ref-type="bibr" rid="c18">Essler et al., 2017</xref>). This aversion to inequity manifests perhaps most famously in the Ultimatum Game (UG), in which one player (the Proposer) decides how to split a sum of money with another player (the Receiver), a role typically assumed by the participant (<xref ref-type="bibr" rid="c25">Güth et al., 1982</xref>; <xref ref-type="bibr" rid="c44">Sanfey et al., 2003</xref>). A Receiver’s acceptance results in both parties receiving the offered money, whereas rejection results in neither party receiving any money—a form of costly punishment. A recurring observation supporting inequity aversion is that people tend to reject disadvantageous offers that unfairly benefit the other party (<xref ref-type="bibr" rid="c11">Brosnan &amp; De Waal, 2014</xref>; <xref ref-type="bibr" rid="c20">Fehr &amp; Schmidt, 1999</xref>).</p>
<p>At the same time, not all inequity is experienced in the same way. For example, when we stand to receive less than our “fair share”, such disadvantageous inequity (DI) engenders feelings of envy, anger, and/or disappointment (<xref ref-type="bibr" rid="c27">Heffner &amp; FeldmanHall, 2022</xref>) which often manifests in punishment of unfair offers in the UG via rejection (<xref ref-type="bibr" rid="c40">McAuliffe et al., 2014</xref>, <xref ref-type="bibr" rid="c41">2017</xref>; <xref ref-type="bibr" rid="c42">Pedersen et al., 2013</xref>; <xref ref-type="bibr" rid="c43">Pillutla &amp; Murnighan, 1996</xref>). In contrast, when we stand to receive a favorable share of resources—albeit one that is unfairly distributed—these advantageous inequitable (AI) offers often engender feelings of guilt or shame (<xref ref-type="bibr" rid="c23">Gao et al., 2018</xref>). Despite these negative emotions, Receivers are much less willing to engage in costly punishment of offers that are advantageously inequitable (<xref ref-type="bibr" rid="c14">Civai et al., 2012</xref>; <xref ref-type="bibr" rid="c29">Hennig-Schmidt et al., 2008</xref>; <xref ref-type="bibr" rid="c37">Luo et al., 2018</xref>).In short, AI versus DI engender markedly different punishment preferences.</p>
<p>A growing body of developmental research demonstrates that this difference in punitive responses to AI versus DI manifests early in the developmental trajectory (<xref ref-type="bibr" rid="c2">Amir et al., 2023</xref>; <xref ref-type="bibr" rid="c9">Blake et al., 2015</xref>; <xref ref-type="bibr" rid="c8">Blake &amp; McAuliffe, 2011</xref>; <xref ref-type="bibr" rid="c39">McAuliffe et al., 2017</xref>). Indeed, punishing AI offers requires the sacrifice a (larger) personal gain to achieve a fairer outcome—mirroring many moral dilemmas in which self-versus other-regarding interests are at odds—AI aversion likely necessitates complex cognitive abilities. More specifically, because the aversion to AI appears to arise from more abstract concerns about fairness (<xref ref-type="bibr" rid="c48">Tomasello, 2019</xref>), AI aversion is thought to impose considerable demands on more sophisticated cognitive processing (<xref ref-type="bibr" rid="c23">Gao et al., 2018</xref>). This may explain, in part, why the developmental trajectory of AI-averse preferences comes online much later relative to DI (<xref ref-type="bibr" rid="c39">McAuliffe et al., 2017</xref>).</p>
<p>Thus, one open question concerns how people acquire inequity-averse preferences. One influential framework posits that we often adapt our behaviors to people around us through a process of conformity (<xref ref-type="bibr" rid="c13">Cialdini &amp; Goldstein, 2004</xref>). Put simply, loyally following the behaviors of another—a social contagion effect—is a powerful motivator of social behavior. Such behavioral contagion effects are observed in diverse decision-making domains such as valuation (Campbell-Meiklejohn et al., 2010), risk-taking (<xref ref-type="bibr" rid="c46">Suzuki et al., 2016</xref>), delay of gratification (<xref ref-type="bibr" rid="c24">Garvert et al., 2015</xref>), moral preferences (<xref ref-type="bibr" rid="c6">Bandura &amp; Mcdonald, 1963</xref>; <xref ref-type="bibr" rid="c52">Vives et al., 2022</xref>), and social norms (<xref ref-type="bibr" rid="c31">Hertz, 2021</xref>). Recently, we demonstrated that punitive responses to DI can be ‘taught’ to participants in the context of the Ultimatum Game, whereby individuals’ preferences for rejecting disadvantageous unfair offers were strengthened as a result of observing another individual’s (a ‘Teacher’) desire to punish these offers (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>).</p>
<p>At the same time, social contagion effects may be far less robust if the behavior demands sacrifice of self-benefit. Since this type of dynamic places behavioral contagion and desire for self-gain in opposition, it remains unclear whether AI aversion can also be acquired ‘vicariously’ through observing another’s preferences. Here, we investigate whether the act of observing the AI-averse preferences of another punitive Receiver enhances an individual’s aversion to AI, even if the rejection of such AI offers requires sacrificing self-benefit. Answering this question not only enriches our understanding of the nature of inequity aversion, but also enable us to better understand the mechanisms underpinning vicarious learning of moral preferences during social interactions.</p>
<p>One can imagine different possible computational mechanisms driving vicarious learning of inequity aversion. One possibility is that observational learning of moral preference is based on simple action-outcome contingencies—on this view, a simple but elegant Reinforcement Learning (RL; <xref ref-type="bibr" rid="c12">Burke et al., 2010</xref>; <xref ref-type="bibr" rid="c16">Diaconescu et al., 2020</xref>; <xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>; <xref ref-type="bibr" rid="c36">Lindström et al., 2019</xref>) model formalizes how individuals adapt their behavior to recently observed outcomes. In its most basic form, RL makes choices on the basis of learned associations between actions and outcomes, and critically, actions are bound to the specific decision context—applied to the UG, the unfairness of the amount offered by the Proposer. However, during social interactions, people might not consider the behaviors of others as resulting from simple action-outcome associations but alternatively, construct and use models of other agents, representing their stable intentions, beliefs, and preferences (<xref ref-type="bibr" rid="c3">Anzellotti &amp; Young, 2020</xref>). Accordingly, we also consider the possibility that moral preferences are immutable across contexts (<xref ref-type="bibr" rid="c4">Bail et al., 2018</xref>; <xref ref-type="bibr" rid="c20">Fehr &amp; Schmidt, 1999</xref>; <xref ref-type="bibr" rid="c47">Taber &amp; Lodge, 2012</xref>), which would suggest that moral preferences are not learned merely as associations (i.e. specific responses tied to different unfairness levels), but rather, through a deeper inference process which models the underlying fairness preferences of the observed individual. Here, across two experiments, we leverage a well-characterized vicarious learning paradigm (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>; <xref ref-type="bibr" rid="c45">Son et al., 2019</xref>; <xref ref-type="bibr" rid="c52">Vives et al., 2022</xref>) to examine the conditions under which individuals are able to learn AI-averse preferences on the basis of exposure to another Receiver’s punitive preferences. In addition, to mechanistically probe how punitive preferences come to be valued in DI and AI contexts, we also characterize trial-by-trial acquisition of punitive behavior with computational models of choice.</p>
</sec>
<sec id="s2">
<title>Experiment 1</title>
<p>In Experiment 1, following the approach of previous experiments (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>), we test if the rejection of advantageously unfair offers can be learned on the basis of exposure to the preferences of another individual (the ‘Teacher’) who has AI-averse preferences. In a between subjectdesign with three phases, participants interacted with other individuals in a repeated Ultimatum Games (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). We assess contagion effects by measuring participants’ DI and AI aversion both before and after observing (and implementing) the preferences of a Teacher’s who exhibits inequity aversion in both DI and AI contexts (“AI-DI-Averse” condition; N =100) and inequity aversion only in a DI context (“DI-Averse” condition; N =100).</p>
<p>First, to assess participants’ baseline fairness preferences across inequity levels, in the Baseline Phase (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) participants acted as a Receiver in several one-shot UGs, responding to offers ranging from extreme DI (e.g., the Proposer keeps 90 cents and offers 10 cents to the Receiver; a 90:10 split) to extreme AI (e.g., the Proposer keeps 10 cents and offers 90 cents to the Receiver; a 10:90 split). On each trial, participants interacted with a different Proposer, and unbeknownst to participants, the offers were pre-determined by the experimenters. Following the typical formulation of the UG (<xref ref-type="bibr" rid="c25">Güth et al., 1982</xref>), participants made the choice between accepting versus rejecting each offer, and also rated the fairness of the offer.</p>
<p>Next, in the Learning Phase (<xref ref-type="fig" rid="fig1">Figure 1d</xref>), participants played a repeated vicarious UG as a third party, in which they accepted or rejected offers on behalf of another Receiver (termed the Teacher in this phase) such that the participant’s decisions did not impact their own payoff but would impact the payoffs to the Proposer and the Teacher. Critically, after each decision, participants received feedback whether the Teachers <italic>would</italic> have preferred acceptance versus rejection (i.e., punishment) of the offer. Thus, through trial-by-trial feedback, the Teacher can signal to participants their preference to punish the Proposer for making unfair offers. Critically, in the ‘DI-Averse’ condition, akin to the typical pattern of human preferences observed in the UG (<xref ref-type="bibr" rid="c25">Güth et al., 1982</xref>; <xref ref-type="bibr" rid="c44">Sanfey et al., 2003</xref>), the Teacher’s preferences exhibited a strong aversion to DI, and thus routinely punishing unfair offers. Specifically, the Teacher was likely to reject DI offers (i.e. 90:10 and 70:30), but not AI offers (i.e. 30:70 and 10:90; see <xref ref-type="fig" rid="fig1">Figure 1b</xref> and <xref ref-type="table" rid="tbl1">Table S1</xref>). However, in the ‘AI-DI-Averse’ condition, the Teacher was likely to reject any unfair offer, regardless of whether it was DI or AI, manifesting typical DI-averse preferences as well as the less commonly observed aversion to AI (i.e., punishing advantageous offers). Feedback from the Teacher was also accompanied by fairness ratings consistent with their preferences (see <xref ref-type="fig" rid="fig1">Figure 1b</xref> and <xref ref-type="table" rid="tbl2">Table S2</xref>).</p>
<p>Finally, to examine contagion (or transmission) of the Teacher’s preferences to the participants, we assessed fairness preferences of the participants for a second time in a Transfer Phase (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). This third phase was identical to the Baseline Phase and thus allowed us to quantify changes in participants’ fairness preferences before and after the Learning Phase.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p><bold>a)</bold> Task Overview. Our main task consists of 3 phases. In the Baseline Phase participants acted as a Receiver, responding to offers of different inequity level and rated their perceived fairness towards the offers on three out of every five trials. While, in the subsequent Learning Phase, participants acted as an Agent, deciding on behalf of the Receiver (Teacher) and Proposer. Again, they rated the fairness on three out of every five trials. Finally, participants made choices in a Transfer Phase which was identical to the Baseline Phase. <bold>b)</bold> Preferences and Fairness Ratings governing the Teacher’s feedback in the Learning Phase (See <xref ref-type="table" rid="tbl1">Table S1</xref> and <xref ref-type="table" rid="tbl2">Table S2</xref>). <bold>c)</bold> Baseline and Transfer phase, in which participants played the Ultimatum game as a Receiver, making choices on their own behalf. <bold>d</bold>In the Learning phase, participants acted as a third party (the agent), making decisions on behalf of the Proposer and the Receiver (Teacher), playing a Vicarious Ultimatum game. In a Vicarious Ultimatum game, the Agent make decisions for the Receiver, if he/she rejects the proposed split, both the Proposer and the Receiver receive nothing. If he/she accepts, the Proposer and the Receiver are rewarded with the proposed split.</p></caption>
<graphic xlink:href="2405.06500v2_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<sec id="s2-1">
<title>Preferences Across Baseline and Transfer Phases</title>
<p>Mirroring preferences typically observed in Western, Educated, Industrialized, Rich, and Democratic (WEIRD) participant populations (<xref ref-type="bibr" rid="c30">Henrich et al., 2006</xref>), punishment choices in the Baseline Phase were DI-averse, but not AI-averse, as we observed similarly high rejections rates during the Baseline Phase for DI offers across the DI-Averse and the AI-DI-Averse conditions (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="table" rid="tbl3">Table S3</xref>; all <italic>Ps</italic> &lt; 0.001). As expected, the rejection rates for AI offers were much lower than DI offers (<xref ref-type="fig" rid="fig2">Figure 2</xref>, e.g. 90:10 vs 10:90, all <italic>Ps</italic> &lt; 0.001, see <xref ref-type="table" rid="tbl3">Table S3</xref> for estimates of rejection rates). Consistent with these Rejection rates, participants rated DI offers as unfair (i.e., lower than the rating scale midpoint of 4, <xref ref-type="table" rid="tbl4">Table S4</xref>; all <italic>Ps</italic> &lt; 0.001) and AI offers were rated as more fair than DI offers (all <italic>Ps</italic> &lt; 0.001)— despite the fact that the offers in the AI and DI contexts represent the same magnitude of inequity (e.g. 90:10 vs 10: 90 splits).</p>
<p>To examine whether exposure to the Teacher’s punishment preferences in the Learning Phase resulted in changes in participants’ fairness preferences, we examined changes in Rejection rates and Fairness rating between the Baseline and Transfer phases (<xref ref-type="fig" rid="fig2">Figure 2</xref>). First, in response to DI offers (90:10 and 70:30 splits), we observed robust increases in punishment rates after observing a Teacher who prefers punishment. That is, after exposure to the Teacher’s punitive preferences in the Learning Phase, participants were more likely to reject offers that unfairly benefitted the Proposer in the Transfer Phase (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), conceptually replicating our previous results (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>). These increases in punishment rates under DI contexts were statistically significant in both the AI-DI-Averse (<xref ref-type="table" rid="tbl5">Table S5</xref>; 90:10 splits: <italic>β</italic> (<italic>SE</italic>)=0.14(0.03), <italic>p</italic> &lt;0.001; 70:30 splits: <italic>β</italic> (<italic>SE</italic>)=0.12(0.03), p&lt;0.001) and DI-Averse conditions (<xref ref-type="table" rid="tbl5">Table S5</xref>, 90:10 splits: <italic>β</italic>(<italic>SE</italic>)=0.18(0.13),<italic>p</italic>&lt;0.001; 70:30 splits: <italic>β</italic>(<italic>SE</italic>)=0.14(0.03), <italic>p</italic>&lt;0.001). These preference changes were consistent with the stronger DI aversion exhibited by the Teacher (in both conditions) than participants in the Baseline Phase. However, we did not observe consistent fairness rating changes for DI offers, presumably because the fairness ratings given by the Teacher was similar to participants’ baseline fairness ratings (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, <xref ref-type="table" rid="tbl6">Table S6</xref>) in both conditions.</p>
<p>Examining punishment choices in response to AI (30:70 and 10:90) splits, we observed that participants in the AI-DI-Averse condition increased their rates of rejection to AI offers (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). That is, participants exposed to the Teacher’s preferences to punish AI offers during the Learning Phase became significantly more likely, in the Transfer Phase, to reject unequal offers that stood to unfairly reap further monetary benefits for the participants (30:70 splits: <italic>β</italic>(<italic>SE</italic>)=0.09(0.03), <italic>p</italic>=0.002; 10:90 splits: <italic>β</italic> (<italic>SE</italic>)=0.12(0.03), <italic>p</italic> &lt;0.001; <xref ref-type="table" rid="tbl5">Table S5</xref>). Importantly, we did not observe transfer of AI-averse preferences in the DI-Averse condition, where the Teacher did not express a desire to punish the AI offers in the Learning phase (30:70 splits: <italic>p</italic>=0.666; 90:10 splits: <italic>p</italic>=0.280; <xref ref-type="table" rid="tbl5">Table S5</xref>). Mirroring changes in rejection rates, participants’ fairness ratings also shifted towards those of the Teacher (<xref ref-type="fig" rid="fig2">Figure 2b</xref>)— specifically, in the AI-DI-Averse condition, participants rated AI offers as more unfair in Transfer compared to Baseline Phase (<xref ref-type="table" rid="tbl6">Table S6</xref>, 30:70 splits: <italic>β</italic>(<italic>SE</italic>)=-0.54(0.12), <italic>p</italic>&lt;0.001; 10:90 splits: <italic>β</italic>(<italic>SE</italic>)= 0.76(0.12), <italic>p</italic>&lt;0.001), while in the DI-Averse condition, participants rated AI offers as more fair (<xref ref-type="table" rid="tbl6">Table S6</xref>, 30:70 splits: <italic>β</italic>(<italic>SE</italic>)=0.32(0.12), <italic>p</italic>&lt;0.001; 10:90 splits: <italic>β</italic>(<italic>SE</italic>)=0.28(0.12), <italic>p</italic>&lt;0.001). Exposure to the Teacher’s behavior in response to “fair” offers (50:50 splits) did not appear to elicit changes in punishment rates in either the AI-DI-Averse (<xref ref-type="table" rid="tbl5">Table S5</xref>, <italic>p</italic>=0.349) or the DI-Averse (<xref ref-type="table" rid="tbl5">Table S5</xref>, <italic>p</italic>=0.517) condition, which suggests against the possibility that participants in the AI-DI-Averse condition were simply increasing punitive preferences, regardless of context. Finally, in line with the rejection rates results, for fair offers (<xref ref-type="table" rid="tbl6">Table S6</xref>, 50:50 splits), we did not observe any change in fairness ratings in the AI-DI-Averse condition (<xref ref-type="table" rid="tbl6">Table S6</xref>, <italic>p</italic>=0.662) and a slight increase in the DI-Averse condition (<italic>p</italic>=0.041).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Behavioral Contagion in Experiment 1</title>
<p><bold>a)</bold> Rejection rates change significantly in DI offers for all conditions, while changes in AI offers were only evident in AI-DI-Averse Condition. <bold>b)</bold> Observing the Teacher’s ratings of AI offers changed fairness ratings in all offer types, while the Teacher’s behaviors in DI offers didn’t. Dashed lines indicate behaviors of the Teacher. Error bars indicate standard error. (†indicates p&lt;0.1, *indicates p&lt;0.05, **indicates p&lt;0.01,***indicates p&lt;0.001)</p></caption>
<graphic xlink:href="2405.06500v2_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s2-2">
<title>Learning another’s preferences</title>
<p>Having demonstrated that participants’ preferences to reject unfair DI and AI offers were altered on the basis of exposure to the Teacher’s preferences, we next examined the trial-by-trial changes in rejection rates during the Learning phase (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). A mixed-effects logistic regression revealed a significant positive effect of trial number on rejection rates of DI Offers for the AI-DI-Averse condition (70:30 splits: <italic>β</italic> (<italic>SE</italic>)=0.36(0.09), <italic>p</italic> &lt;0.001; not significant in 90:10 splits: <italic>β</italic> (<italic>SE</italic>)=0.27(0.20), <italic>p</italic>=0.164; <xref ref-type="table" rid="tbl7">Table S7</xref>) and the DI-Averse condition (90:10 splits: <italic>β</italic> (<italic>SE</italic>)=0.63(0.20), <italic>p</italic>=0.001; 70:30 splits: <italic>β</italic>(<italic>SE</italic>)=0.31(0.09), <italic>p</italic>&lt;0.001), indicating an increase of rejection rates during the Learning phase. In contrast, examining responses to AI offers, we only observed a learning effect—that is, a rejection rate increase—in the AI-DI-Averse condition (30:70 splits: <italic>β</italic>(<italic>SE</italic>)=0.47(0.12),<italic>p</italic>&lt;0.001; 10:90 splits: <italic>β</italic> (<italic>SE</italic>)=0.77(0.18), <italic>p</italic> &lt;0.001) where the Teacher imparted AI-averse preferences, but not in DI-Averse condition (30:70 splits: <italic>p</italic>=0.111; 10:90 splits: <italic>p</italic>=0.429), where the Teacher did not. In other words, participants by and large appeared to adjust their rejection choices and fairness ratings in accordance with the Teacher’s feedback in an incremental fashion.</p>
</sec>
<sec id="s2-3">
<title>Computational Models of Learning Punishment Preferences</title>
<p>Having established that rejection rates increased for both DI offers (in both the DI-Averse and AI-DI-Averse conditions) and AI offers (only in the AI-DI-Averse condition), we then sought to better understand the learning mechanisms underpinning trial-by-trial learning (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). We used computational modelling to formalize two different sets of assumptions about how participants learn from Teachers’ feedback. Under one account, a simple Reinforcement Learning (RL) model proposes that decision-makers learn punishment preferences by observing feedback resulting from actions made in response to specific offers. Previously we have found that this ‘naïve’ model provided a reasonable characterization of participants’ trial-by-trial learning of DI-averse preferences (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>). However, this model may fail to capture a critical facet of learning: participants’ moral preferences may not be learned merely as associations—the type of response being tied to specific offers—but rather, through a deeper inference process which models the underlying fairness preferences of the Teacher. Accordingly, our alternative model assumes that the participant uses trial-by-trial feedback to infer the Teacher’s underlying preferences concerning inequality, which may shift depending on the context (DI versus AI). Following <xref ref-type="bibr" rid="c20">Fehr-Schmidt’s (1999)</xref> simple inequality aversion formalism, in our model—termed the Preference Inference model—the Teacher’s aversion to DI is modeled by an ‘Envy’ parameter, while the ‘Guilt’ parameter captures the Teacher’s aversion to AI (see Methods for model details). Critically, the RL model does not learn the Teacher’s preferences per se, but the value of each action (accept or reject), independently for each offer type. In contrast, the Preference Inference model explicitly represents the extent of the Teacher’s AI- and DI-aversion—i.e., their underlying preferences—by independently updating the envy and guilt parameters using trial-by-trial feedback from the Teacher.</p>
<p>We compared the goodness of fit of six different models to participants’ choices in the Learning Phase: the Preference Inference model which learns the ‘guilt’ and ‘envy’ parameters experientially, a Static Preference model which assumes the ‘guilt’ and ‘envy’ are fixed over the course of learning (baseline model 1, see Methods), three variants of the RL model that make different assumptions about how action values are represented, and a baseline or ‘null’ model which assumed a fixed probability of each action (randomly choosing, baseline model 2). We fit each of the six models via maximum likelihood estimation, penalizing for model complexity (see Methods), and found that the Preference Inference model provided the best characterization of learning (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, <xref ref-type="table" rid="tbl8">Table S8</xref>), suggesting that participants were performing trial-by-trial inference of the Teacher’s underlying inequity preferences, rather than simply learning reinforced associations between experienced offer types and actions. Even the Static Preference model, which does not assume any learning mechanism but rather, assumes fixed preferences with respect to DI and AI, provided a better characterization of learning than any of the three RL models which do assume incremental learning over paired associations.</p>
<p>To examine the learning dynamics underpinning the best-fitting Preference Inference Model, we simulated Learning Phase choice behavior using participants’ estimated parameter values (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, see Methods for details). The close correspondence between the simulated and observed learning curves indicates that the Preference Inference model captures the reinforcement-guided variations in punishment in DI-context and, crucially, the marked differences in learning between DI-Averse and AI-DI-Averse conditions in AI-context (30:70 and 10:90 splits). To better understand how the Preference Inference model accounts for these patterns of change in rejection rates, we examined how model-inferred aversion to DI (‘envy’) and AI (‘guilt’)—the two components of the Fehr-Schmidt inequality aversion model (<xref ref-type="bibr" rid="c20">Fehr &amp; Schmidt, 1999</xref>) representing the latent structure of the Teachers’ preferences— emerge as a function of exposure to the Teacher’s preferences. In both the DI-Averse and AI-DI-Averse conditions, the model inferred a similarly “envious” Teacher (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), while the model only increased its estimate of the Teacher’s ‘guilt’ parameter (<xref ref-type="fig" rid="fig3">Figure 3d</xref>), in the AI-DI-Averse condition where the Teachers’ feedback exhibited AI aversion, mirroring the model’s—and participants’—shift in rejection rates over the course of the Learning Phase and further suggesting that the Preference Inference model captured critical aspect of the participants’ vicarious learning behaviors.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Learning phase behavior in Experiment 1.</title>
<p><bold>a)</bold> Rejection rate changes in Learning Phase. Rejection choices were summarized across participants. For DI Offers, rejection rate increased in both Conditions during learning. While rejection rate only changed (increase) in AI-DI-Averse Condition for AI offers. Sold thin lines denote participants’ rejection choices, dashed lines denote the Teacher’s preferences, and solid thick lines represent predictions of the (bestfitting) Preference Inference Model. <bold>b)</bold> Model comparison, demonstrating that the Preference Inference model provided the best fit to participant’ Learning Phase behavior (AIC: Akaike Information Criterion) <bold>c)</bold> and <bold>d)</bold> Parameters updating for the Preference Inference model. The Preference Inference model captured significant rejection rate increasing in AI offers by updating the guilt parameter in a trial by trial manner.</p></caption>
<graphic xlink:href="2405.06500v2_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s2-4">
<title>Relationship between Learning and Contagion Effects</title>
<p>We next investigated whether the extent of a participants’ vicarious learning was associated with greater contagion of punishment preferences. To do this, we examined whether participants’ changes in rejection rates between Transfer and Baseline, could be explained by the degree to which they vicariously learned, defined as the change in punishment rates between the first and last 5 trials of the Learning phase. Across offer types (<xref ref-type="fig" rid="fig4">Figure 4</xref>), we observed that participants who more strongly adapted their rejection rates to match the Teacher’s preferences in the Learning phase were more likely to adopt the Teachers’ preference while making their own choices in the Transfer phase. Specifically, for DI offers, these predictive relationships were strong in the AI-DI-Averse (90:10 splits: <italic>β</italic>(<italic>SE</italic>)=0.22(0.12), <italic>p</italic>=0.073; 70:30 splits: <italic>β</italic> (<italic>SE</italic>)=0.33(0.08), <italic>p</italic> &lt;0.001; <xref ref-type="table" rid="tbl9">Table S9</xref>) and DI-Averse (90:10 splits: <italic>β</italic>(<italic>SE</italic>)=0.48(0.09), <italic>p</italic>&lt;0.001;70:30 offers: <italic>β</italic>(<italic>SE</italic>)=0.36(0.08), <italic>p</italic>&lt;0.001; <xref ref-type="table" rid="tbl9">Table S9</xref>) conditions. However, for AI offers, we only observed a relationship in the AI-DI-Averse Condition (30:70 splits: <italic>β</italic>(<italic>SE</italic>)=0.26(0.10), <italic>p</italic>&lt;0.010; 10:90 splits: <italic>β</italic>(<italic>SE</italic>)=0.27(0.07), <italic>p</italic>&lt;0.001; <xref ref-type="table" rid="tbl9">Table S9</xref>) but not the DI-Averse condition (a trend in 30:70 splits: <italic>β</italic>(<italic>SE</italic>)=0.25(0.13), <italic>p</italic>=0.058; 10:90 splits: <italic>β</italic>(<italic>SE</italic>)=0.12(0.10), <italic>p</italic>=0.196; <xref ref-type="table" rid="tbl9">Table S9</xref>). As expected, we did not observe any predictive relationships between learning and transfer behavior in response to fair (50:50) offers in either condition (AI-DI-Averse: <italic>β</italic>(<italic>SE</italic>)=0.10 (0.20), <italic>p</italic>=0.625; DI-Averse: <italic>β</italic>(<italic>SE</italic>)=0.12(0.18), <italic>p</italic>=0.503; <xref ref-type="table" rid="tbl9">Table S9</xref>). Taken together, the relationships between the degree of successful learning of the Teachers’ preferences and the magnitude of change in punishment rates (when participants acted on their own behalf) strongly suggests that participants’ changes in punitive preferences—particularly for AI offers—occurred as a result of exposure to another’s preferences.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Relationship between learning and contagion effects in Experiment 1.</title>
<p>Rejection rate changes in Learning phase was indexed by the averaged rejection rate difference between first five and the last five trials in Learning phase. On the DI side, the learning index can predict contagion in both AI-DI-Averse and DI-Averse conditions. while on the AI side, this effect is more salient in the AI-DI-Averse Condition than that in the DI-Averse Condition (†indicates p&lt;0.1, *indicates p&lt;0.05, **indicates p&lt;0.01,***indicates p&lt;0.001)</p></caption>
<graphic xlink:href="2405.06500v2_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>Conceptually replicating our previous findings (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>), Experiment 1 provides evidence for behavioral contagion of DI-averse preferences, but extends this work by revealing that AI-averse preferences—which are believed to be less mutable (<xref ref-type="bibr" rid="c37">Luo et al., 2018</xref>)—can be similarly shaped by exposure to another individual with manifesting a strong aversion to resource divisions that unfairly benefit them. These results suggest that individuals’ moral preferences can be learned even in cases where these preferences conflict with one’s own self-interest. Computationally, this learning process was best characterized by an account that prescribes that individuals build a representation of others’ moral preferences about DI and AI (akin to the <xref ref-type="bibr" rid="c20">Fehr-Schmidt (1999)</xref> model of inequity aversion), rather than by a simple Reinforcement Learning account. This preference inference account predicts that individuals, when exposed to only a fraction of inequity-related punishment preferences, should generalize these inferred preferences to other similar inequity contexts. In Experiment 2, we sought to test this generalization hypothesis more directly, buttressing the idea that the learning and transfer of Inequity-averse preferences observed in Experiment 1 came about as a result of participants modeling the Teachers’ inequality-averse preferences.</p>
</sec>
</sec>
<sec id="s3">
<title>Experiment 2</title>
<p>Experiment 2 provides a more stringent test of whether participants model the Teacher’s underlying inequity preferences. Specifically, if individual indeed learn the Teacher’s latent inequity-averse preferences in the Learning phase, we would expect that feedback-driven learning of Teacher’s punishment preferences on specific (moderate inequity) offers (30:70 splits) should generalize to offers in the same context (10:90 splits) without any direct experience of those offer types. Accordingly, to probe for this sort of generalization—a hallmark of the sort of latent structure learning we attribute to the behavior we observed in Experiment 1—we now eliminate feedback for extreme DI (90:10) and AI (10:90) offers from the Learning phase. If participants’ punishment preferences are informed by modeling inferred inequity-averse preferences of the Teacher, we should expect to see these preferences transfer to participants’ own fairness preferences in a similar, generalized manner in the Transfer Phase.</p>
<p>Mirroring Experiment 1, we employed two conditions governing the Teachers’ preferences in the Learning Phase: the Teacher in the DI-Averse condition only exhibited strong punishment preferences concerning moderate DI offers (30:70 splits), while the Teacher in the AI-DI-Averse condition exhibited strong preferences in response to both moderate DI and moderate AI offers (30:70 and 70:30 splits).</p>
<sec id="s3-1">
<title>Contagion Effects for Extreme Unfair Offers Suggest Generalization</title>
<p>In Experiment 2, we took the same analysis approach as in Experiment 1, examining changes in rejection rates between the Baseline Phase and the Transfer Phase (after participants experienced feedback with moderately unfair offers). Similar to what we observed in Experiment 1 (<xref ref-type="fig" rid="fig5">Figure 5a</xref>), participants increased their rates of rejection of extreme DI (i.e., 90:10) in the Transfer Phase, relative to the Baseline phase (AI-DI-Averse Condition: <italic>β</italic> (<italic>SE</italic>)=0.13(0.03), <italic>p</italic> &lt;0.001; DI-Averse Condition: <italic>β</italic> (<italic>SE</italic>)=0.18(0.03), <italic>p</italic> &lt;0.001; <xref ref-type="table" rid="tbl10">Table S10</xref>), suggesting that participants’ learned (and adopted) DI-averse preferences, generalized from one specific offer type (70:30) to an offer types for which they received no Teacher feedback (90:10). Examining generalization across AI offers, we found a trend that participants in the AI-DI-Averse Condition increased their rejection rates of extreme AI offers (10:90) for which they did not receive any Teacher feedback in the Learning Phase (<italic>β</italic>(<italic>SE</italic>)=0.06(0.03), <italic>p</italic>=0.060). Furthermore, we saw decreases in rejection rates of extreme AI offers for participants in the DI-Averse Condition (β <italic>(SE</italic>)=-0.06(0.03), <italic>p</italic>=0.035). Mirroring the observed rejection rates (<xref ref-type="fig" rid="fig5">Figure 5b</xref>), participant rated these (untrained) extreme unfair DI offers (90:10) as less fair in the Transfer phase in both the AI-DI-Averse (<italic>β</italic>(<italic>SE</italic>)=-0.23(0.13), <italic>p</italic>=0.076; see <xref ref-type="table" rid="tbl11">Table S11</xref>) and DI-Averse conditions (<italic>β</italic>(<italic>SE</italic>)=0.29(0.13), <italic>p</italic>=0.023), and rated extreme AI offers (10:90) as less fair only in AI-DI-Averse Condition (<italic>β</italic>(<italic>SE</italic>)=- 0.84(0.13), <italic>p</italic>&lt;0.001).</p>
<p>We further reasoned that if the contagion effects observed for extremely unfair offers (90:10 and 10:90 splits) resulted from the same latent structure learning process driving the observed contagion for moderately unfair offers (30:70 and 70:30 splits), we should expect the magnitude of contagion effects in moderately unfair offers to relate to the magnitude of contagion effects in extremely unfair offers. Examining changes in punishment rates in AI contexts—between the Transfer and Baseline phases—we observed that participants with larger contagion effects for 30:70 offers also exhibited larger contagion effects for 10:90 offers (<xref ref-type="fig" rid="fig8">Figure S2</xref>). This was observed both in rejection rate changes (AI-DI-Averse Condition: <italic>p</italic>&lt;0.001, DI-Averse Condition: <italic>p</italic>&lt;0.001) and perceived fairness changes (AI-DI-Averse Condition: <italic>p</italic>&lt;0.001, DI-Averse Condition: <italic>p</italic>&lt;0.001). In DI contexts, we found that the same relationships between responses to moderately unfair offers (70:30) and extremely unfair offers (90:10), both for changes in rejection rates (AI-DI-Averse Condition: <italic>p</italic>&lt;0.001; DI-Averse Condition: <italic>p</italic>=0.003) and perceived fairness rating changes (AI-DI-Averse Condition: <italic>p</italic>&lt;0.001; DI-Averse Condition: <italic>p</italic>&lt;0.001).</p>
<p>In short, we find evidence that participants generalized across learning contexts, which in turn shaped their own punitive responses to extreme offers, both in the case of DI and AI offers. In other words, it appears that preferences acquired through contagion extends beyond mere associations between single offers and actions, and instead relies on a mechanism that infers the latent structure of the Teacher’s fairness preferences.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><title>Baseline and Transfer Phase Behavior in Experiment 2.</title>
<p><bold>a)</bold> Contagion in extremely unfair offers. Though no feedback was provided in the Learning phase for 90:10 or 10:90 splits, we observed generalization of punishment preferences in these types of offers. Dashed lines represent the Teacher’s preferences. <bold>b)</bold> Fairness rating changes. We found significant changes from Baseline to Transfer phase in fairness rating for 90:10 in both AI-DI-Averse and DI-Averse Condition, but only in AI-DI-Averse Condition for 10:90 offers. Error bars represent standard errors (†indicates p&lt;0.1, *indicates p&lt;0.05, **indicates p&lt;0.01,***indicates p&lt;0.001)</p></caption>
<graphic xlink:href="2405.06500v2_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s3-2">
<title>Preference Changes in the Learning Phase Suggest Generalization</title>
<p>A primary goal of Experiment 2 was to demonstrate that learning the Teacher’s preferences with respect to moderately unfair offers generalized to extremely unfair offers, where no feedback from the Teacher was provided. Examining the time course of rejection rates in AI-contexts during the Learning phase (<xref ref-type="fig" rid="fig6">Figure 6</xref>) revealed that participants learned over time to punish mildly unfair 30:70 offers, and these punishment preferences generalized to more extreme offers (10:90). We observed a significant increase in rejections rates for 10:90 (AI) offers in the AI-DI-Averse Condition (<xref ref-type="fig" rid="fig6">Figure 6</xref>, β (<italic>SE</italic>)=1.06(0.31), <italic>p</italic>&lt;0.001; mixed-effects logistic regression, see <xref ref-type="table" rid="tbl12">Table S12</xref>), but not in the DI-Averse Condition <italic>(β</italic> (<italic>SE</italic>)=-0.54(0.40), <italic>p</italic>=0.170). We observed significant rejection rate increases for 90:10 (DI) offers in DI-Averse Condition (<italic>β</italic> (<italic>SE</italic>)=0.96(0.27), <italic>p</italic> &lt;0.001) but not in the AI-DI-Averse Condition (<italic>β</italic>(<italic>SE</italic>)=0.36(0.26), <italic>p</italic>=0.116). Finally, we observed significant decreases in fairness ratings over time for 10:90 offers in the AI-DI-Averse Condition (<italic>β</italic>(<italic>SE</italic>)=-0.27(0.09), <italic>p</italic>=0.004; <xref ref-type="table" rid="tbl13">Table S13</xref>), again suggesting that participants generalized fairness ratings across offer types on the basis of inferred fairness preferences attributed to the Teacher. We did not observe this sort of generalization for fairness ratings in the DI-Averse condition, nor did we observe significant changes in fairness ratings over time in DI context (<italic>p</italic>s &gt; 0.131 for all offers; see <xref ref-type="table" rid="tbl13">Table S13</xref>), perhaps owing to the smaller difference between the Teacher’s preferences and participants’ default (Baseline) preferences concerning DI offers. Furthermore, we reasoned that participants’ learning in moderately unfair contexts (for which participants received feedback) should predict their learning in extremely unfair contexts (for which no feedback was provided). To test this, we examined individual differences in changes in punishment between the final 5 trials and the first 5 trials of the Learning phase, finding that learning for moderately unfair (30:70 and 70:30) offers predicted changes in punishment rates for extremely unfair (10:90 splits: p &lt; 0.001 and 90:10 splits: p &lt; 0.001) offers. In other words, participants who increased their punishment more in response to (reinforced) moderately unfair offers also exhibited changes in their preferences for (non-reinforced) rejection of extremely unfair offers.</p>
<p>Finally, following Experiment 1, we fit a series of computational models of Learning phase choice behavior, comparing the goodness-of-fit of the four best-fitting models from Experiment 1 (see Methods). As before, we found that the Preference Inference model best characterized participants’ Learning Phase behavior (<xref ref-type="fig" rid="fig7">Figure S1a</xref>, <xref ref-type="table" rid="tbl14">Table S14</xref>). This is unsurprising, given that this model—by virtue of learning the Teachers’ underlying preferences— assumes the sort of generalization of preferences between offer types that Experiment 2 sought to test directly. We also simulated this model’s Learning Phase behavior using participants’ estimated parameter values and found that the model, mirroring participants’ choices, exhibits clear incremental changes in rejection rates (<xref ref-type="fig" rid="fig6">Figure 6</xref>; thick lines), both for offers where the model received explicit feedback (70:30) and for offers where the model received no feedback (90:10). In other words, like participants, the model generalizes the learned inequity-averse preferences to extreme AI offers (90:10), which stems from the model’s trial-by-trial updating of the parameters governing the Teacher’s preferences (see <xref ref-type="fig" rid="fig7">Figures S1b and c</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Learning Phase Choice Behavior in Experiment 2.</title>
<p>Learning effect were documentedin Extremely unfair offers. Rejection choices were summarized across subjects. Dashed lines indicate Rejection choice of the Teacher. The learning effect were evident for 90:10 offers in DI-Averse condition and 10:90 offers in AI-DI Averse condition. Thin solid lines represnts participants’ rejection choice, thick solid lines show the predictions of the Preference Inference Model , and the dashed lines indicate the Teacher’s preferences (not observed by participants in 90:10 and 10:90 splits).</p></caption>
<graphic xlink:href="2405.06500v2_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s4" sec-type="discussion">
<title>Discussion</title>
<p>While people tend to reject proposed resource allocations where they stand to receive less than their peers (so-called disadvantageous inequity; DI), they are markedly less averse to resource allocations where they stand to unfairly gain more than their peers (advantageous inequity or AI; <xref ref-type="bibr" rid="c9">Blake et al., 2015</xref>; <xref ref-type="bibr" rid="c37">Luo et al., 2018</xref>). Here we considered the possibility that these complex, other-regarding preferences for fairness can be imparted merely by observing (and enacting) the preferences of another person. We investigated, in an Ultimatum Game setting, whether AI-averse preferences can be shaped by learning and the preferences of another individual. We leveraged a well-characterized observational learning paradigm (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>), exposing participants to another individual (the Teacher) exhibiting a strong preference for punishment of advantageously unfair offers, and probed whether these punishment preferences in turn transferred to participants making choices on their own. We found that participants’ own AI-averse preferences shifted towards the preferences of the Teacher they just observed, and the strength of these contagion effects related to the degree of behavior change participants exhibited on behalf of the Teachers, suggesting that they internalized, at least somewhat, these inequity preferences.</p>
<p>While we observed apparent adoption of the Teachers’ inequity-averse preferences, in the face of both AI and DI, previous work has outlined a number of important differences with respect to how individuals respond to the two sorts of inequity. Aversion to DI is thought to arise from negative emotions such as spite (<xref ref-type="bibr" rid="c40">McAuliffe et al., 2014</xref>; <xref ref-type="bibr" rid="c43">Pillutla &amp; Murnighan, 1996</xref>) engendered by consideration of one’s standing relative to others, while AI aversion, in contrast, is thought to stem from concerns about fairness or inequality (<xref ref-type="bibr" rid="c38">McAuliffe et al., 2013</xref>). Hence, the expression of AI aversion may signal, and even enforce, egalitarian social norms. Developmental evidence supports this distinction between AI versus DI aversion. While DI aversion emerges at the tender age of 4, AI aversion does not appear to manifest until about 8 years (<xref ref-type="bibr" rid="c9">Blake et al., 2015</xref>; <xref ref-type="bibr" rid="c8">Blake &amp; McAuliffe, 2011</xref>; <xref ref-type="bibr" rid="c39">McAuliffe et al., 2017</xref>). In fact, AI aversion—which entails trading off self-interest against a social norm enforcement— is not even commonly observed in adults (<xref ref-type="bibr" rid="c9">Blake et al., 2015</xref>; <xref ref-type="bibr" rid="c29">Hennig-Schmidt et al., 2008</xref>; <xref ref-type="bibr" rid="c37">Luo et al., 2018</xref>), suggesting that AI aversion is more difficult (and less likely) to be learned than DI aversion. That we found evidence that AI averse-preferences can be learned suggests that observational learning processes are a potent and promising means by which sophisticated fairness preferences can be imparted.</p>
<p>Mechanistically, we found that participants’ feedback-based learning of punishment preferences was best characterized by a computational model that assumes individuals infer the Teacher’s latent and structured preferences for punishment—rather than a simple Reinforcement Learning (RL) account assuming that individuals learn contextually-bound punishment preferences. To support the interpretation that individuals indeed ‘model’ the fairness preferences of others, in a second experiment we direct test whether participants can infer the Teachers’ inequity-averse preferences across contexts. We found that participants generalized the Teacher’s punitive preferences to other contexts that varied in their unfairness, and this occurred in the both AI and DI contexts, suggesting that the discovery of latent structure is instrumental for generalization.</p>
<p>The representation of others’ beliefs in other interpersonal decision-making tasks has been previously formalized, computationally, by a Bayesian account of theory of mind (ToM) in which the hypothetical beliefs were described by a prior distribution, and participants update this distribution using Bayesian updating (<xref ref-type="bibr" rid="c5">Baker, Chris et al., 2009</xref>; <xref ref-type="bibr" rid="c33">Jara-ettinger, 2019</xref>). This computational framework has been applied to describing behaviors in a group decision making task (<xref ref-type="bibr" rid="c34">Khalvati et al., 2019</xref>). In our Preference-inference model, on each trial, the learner makes a guess about the Teacher’s inequity aversion parameters, then the Teacher’s feedback is subsequently used to further constrain the range of parameter values which could conceivably produce the Teacher’s observed feedback. The learner then updates their initial guess range. Conceptually, this learning mechanism is consistent with the sort of Bayesian updating.</p>
<p>One open question concerning the contagion effects is how the identities— and number—of teachers experienced in the Learning phase bear on the strength of the learning and contagion effects observed. For simplicity, our Learning Phase employed only one distinct Teacher, that included no identity information, which contrasts with many social interactions in daily life, which are almost always accompanied by identifying information or attributes concerning the other (<xref ref-type="bibr" rid="c32">Hester &amp; Gray, 2020</xref>). In contrast, our interactions with others are profoundly influenced by the identities of others—for example, whether they are conservative versus liberal (<xref ref-type="bibr" rid="c35">Leong et al., 2020</xref>), whether they are inversus out-group members (<xref ref-type="bibr" rid="c28">Hein et al., 2010</xref>; <xref ref-type="bibr" rid="c52">Vives et al., 2022</xref>). At the same time, the strength of social influence often increases with the number (or proportion) of individuals in a group expressing a particular preference (<xref ref-type="bibr" rid="c13">Cialdini &amp; Goldstein, 2004</xref>; <xref ref-type="bibr" rid="c45">Son et al., 2019</xref>). However, it may also be the case that social contagion effects require repeated interactions with the same individual (<xref ref-type="bibr" rid="c49">Tsvetkova &amp; Macy, 2014</xref>), which the contagion observed in the present paradigm corroborates. Accordingly, future work should aim to examine the influence of the teacher’s identity—and its concordance with the learner’s identity—as well as seek to understand the how the relative balance of repeated experience with identical teachers versus the number of distinct teachers modulates the strength of contagion effects in AI/DI punishment preferences.</p>
<p>In summary, our study provides an initial demonstration that despite the desire for self-gain, we observe that people can swiftly and readily acquire another’s preferences for advantageous inequity, even when it comes at a monetary cost to the self. Computationally, we find that this contagion of inequity averse-preferences occurs through representing the underlying structure of another’s preferences, rather than a Reinforcement Learning-like process of learning simple context-action associations. Importantly, these inferred preferences are sufficient to induce individuals to change their preferences for punishing advantageous inequity, suggesting that social influence may be one promising route through which social norm enforcement—which is particularly uncommon in the case of AI—can be promoted.</p>
</sec>
<sec id="s5" sec-type="methods">
<title>Methods</title>
<sec id="s5-1">
<sec id="s5-1-1">
<title>Participants</title>
<p>We recruited US-based participants from Amazon Mechanical Turk (<xref ref-type="bibr" rid="c15">Crump et al., 2013</xref>) for both Experiment 1 (<italic>N</italic>=200, <italic>M</italic> age = 37.53 (<italic>SD</italic>=10.88), 75 females) and Experiment 2 (<italic>N</italic>=200, <italic>M</italic> age=37.16 (<italic>SD</italic>=11.79) years old, 80 females). These sample sizes are based on our previous work employing the same per-condition sample sizes (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>)in a punishment-learning task. Participants provided informed consent in accordance with the McGill University Research Ethics Board. Participants were randomly assigned to either the AI-DI-Averse (N = 100) or the DI-Averse (N = 100) condition. As we replicated all key reported results when excluding participants who evidenced some disbelief that the Teacher (see below for procedural details), suggesting that these results are robust to potential disbelief about the task structure as described to them. In Experiment 2, we excluded the data of participants who failed to meet the requirements of each analysis due to missed trials (3 in AI-DI-Averse and 3 in DI-Averse Condition were removed in Contagion effects analysis; 2 in AI-DI-Averse , 5 in DI-Averse Condition were removed for the generalization analysis).</p>
</sec>
<sec id="s5-1-2">
<title>Punishment Learning Paradigm</title>
<p>We used a modified version of the Ultimatum Game (<xref ref-type="bibr" rid="c25">Güth et al., 1982</xref>) to probe participants’ fairness preference, In this task, the proposer, offer an allocation of a total amount (e.g. $1 out of $10). Then, another player, the Receiver, chooses to reject or accept the proposed allocation. If the Receiver accepts, both players receive the proposed amount, if they reject, both player receive nothing. Following our past work examining contagion effects (<xref ref-type="bibr" rid="c21">FeldmanHall et al., 2018</xref>), our task consisted of 3 phases: Baseline, Learning and Transfer (see <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p>
<p>In the Baseline and Transfer phases, participants responded to unfair offers as a receiver in multiple rounds of ultimatum game, and participants were informed that the Proposer on each trial is a different, randomly chosen individual with a unique (fictitious) participant ID (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), which permitted us to measure participants’ preferences and beliefs about fairness irrespective of any particular Proposer. We considered 5 unique offer levels (90:10, 70:30, 50:50, 30:70, 10:90) ranging from extreme DI to extreme AI, including ‘fair’ offers (50:50), which allowed us to measure participants’ baseline rejection tendency. We instructed participants that these Proposers were concurrently participating in this task and these were “real time” offers. Critically, participants were responding to offers made by fictitious players with predetermined offers to ensure that we could observed rejection preferences for each offer level. On 3/5 of Baseline and Transfer Phase trials, participants also rated the fairness of the offer on a 1-7 scale (1 being ‘Strongly unfair’; 7 being ‘Strongly fair’) after making an accept/reject choice. Participants experienced 5 trials of each offer type in both the Baseline and Transfer Phase.</p>
<p>In the Learning phase, participants played an Ultimatum Game, acting as a third party deciding to accept or reject Proposers’ offers for another receiver (the Teacher) receiving offers from Proposers (see <xref ref-type="fig" rid="fig1">Figure 1d</xref>). After the participant chose to accept or reject the offer, the option the Teacher would have preferred was revealed. In the AI-DI-Averse condition, the Teacher indicated preference for rejection of fair (50:50) and AI offers, and uniformly rated these AI offers as unfair (see <xref ref-type="fig" rid="fig1">Figure 1b</xref>). In the DI-Averse condition, the Teacher accepted all fair and AI offers and rated these offers as uniformly fair. In both the DI-Averse and AI-DI-Averse, DI offers were rejected and rated as unfair. The Teachers’ rejection rates and ratings in response to each offer type are provided in <xref ref-type="table" rid="tbl1">Tables S1</xref> and <xref ref-type="table" rid="tbl2">S2</xref>.</p>
<p>Again, on 3/5 of trials, participants rated the fairness of each offer, and on these trials participants also saw the Teachers’ fairness rating of the offer. Like the Baseline and Transfer phases, participants saw a unique, randomly chosen Proposer on every trial, but were informed that there was a single, unvarying Teacher over the entire Learning Phase. In Experiment 1, participants experienced 20 trials for each of the 5 offer types considered (90:10, 70:30, 50:50, 30:70, 10:90). To generate each offer, we further added a uniformly distributed noise in the range of (-10 ,10) to each offer type.</p>
<p>Experiment 2 followed the same procedure as Experiment 1 except for the following changes. First, and most importantly, in the Learning Phase participants did not experience feedback on extreme AI offers (10:90 splits). Second, we added to the Learning Phase 25 more trials each for offer type (70:30 and 30:70 splits), to afford greater opportunity to observe the Teacher’s preferences. Third, participants were instructed that Proposers’ offers (which were predetermined) were generated by previous participants in previous similar experiment. Finally, in all task phases, participants were required to respond within a 3-second deadline when making choices, and a 4-second deadline when providing fairness ratings.</p>
</sec>
<sec id="s5-1-3">
<title>Data Analysis</title>
<p>We used mixed-effects regression, implemented in the ‘lmer’ package for R (<xref ref-type="bibr" rid="c7">Bates et al., 2015</xref>) to estimate the effect of offer level and punishment condition upon contagion rates— the rejection rate change between the Baseline and Transfer phases. To do this the rejection rate change were modeled by interactions between Offer types (factors of five levels: 90:10, 70:30, 50:50, 30:70, 10:90, encoded as 5 columns) and Condition (AI-DI-Averse vs. DI-Averse, encoded as 2 columns), with random intercepts taken over participants (see <xref ref-type="table" rid="tbl7">Table S7</xref> for full coefficient estimates).</p>
<p>We estimated changes in punishment rates in the Learning Phase using mixed-effects logistic regressions in Learning. Specifically, rejection choices were predicted as a function of trial number, offer type, condition, and their resultant interactions, taking the 2-way interactions between Trial number and offer type (random slope) as random effects over participants. To estimate changes in fairness ratings, we estimated a linear mixed-effects model with the same terms.</p>
</sec>
<sec id="s5-1-4">
<title>Computational Models of Learning Phase Behavior</title>
<p>We considered 6 computational models of Learning Phase choice behavior, which we fit to individual participants’ observed sequences of choices and Teacher feedback via Maximum Likelihood Estimation. Importantly, Models 1 (Random choosing) and 2 (Static preference) are baseline models which assume that rejection probabilities are fixed over time, while all other models allow for learning of rejection rates over time in accordance with Teacher feedback.</p>
<sec id="s5-1-4-1">
<title>Model 1 (Random Choosing).</title>
<p>This model assumes that participants reject offers with a fixed probability governed by the parameter <italic>p<sub>offertype</sub></italic> (one for each offer type; 5 free parameters in total).</p>
<disp-formula id="FD1">
<alternatives>
<mml:math id="M1" display="block"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2405.06500v2_eqn1.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(1)</label>
</disp-formula>
</sec>
<sec id="s5-1-4-2">
<title>Model 2 (Static preference):</title>
<p>This model that when choosing for others, the utility of accepting an offer, relative to rejection of the offer, is governed by the Fehr-Schmidt (FS) inequity aversion(<xref ref-type="bibr" rid="c20">Fehr &amp; Schmidt, 1999</xref>; <xref ref-type="bibr" rid="c37">Luo et al., 2018</xref>) function:
<disp-formula id="FD2">
<alternatives>
<mml:math id="M2" display="block"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo>*</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mo>−</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mo>*</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2405.06500v2_eqn2.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(2)</label>
</disp-formula>
<disp-formula id="FD3">
<alternatives>
<mml:math id="M3" display="block"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>
<graphic xlink:href="2405.06500v2_eqn3.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(3)</label>
</disp-formula>
</p>
<p>In this function, <italic>offer</italic> represents the share the Proposer give to the Receiver, <italic>α</italic> parameterizes the Teacher’s disutility for accepting disadvantageous unfair offers (DI aversion), and <italic>β</italic> captures Teacher’s disutility for accepting advantageous offers (AI aversion), which can each range from 0-1.</p>
<p>These action utilities were then transformed to choice probabilities using the softmax choice rule:
<disp-formula id="FD4">
<alternatives>
<mml:math id="M4" display="block"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
<graphic xlink:href="2405.06500v2_eqn4.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(4)</label>
</disp-formula>
</p>
<p>where the inverse temperature (τ) parameter captures decision noise, where a larger τ corresponds to a higher probability of choosing the action with higher utility, and as γ approaches 0, the two options are chosen with equal probability. In total, this model has 3 free parameters.</p>
</sec>
<sec id="s5-1-4-3">
<title>Model 3 (Basic RL):</title>
<p>Model 3 is a simple RL model following that used by <xref ref-type="bibr" rid="c21">FeldmanHall et al. (2018)</xref> only one learning rate, which represents and updates values of the two actions separately for each offer type, using a delta updating rule:
<disp-formula id="FD5">
<alternatives>
<mml:math id="M5" display="block"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2405.06500v2_eqn5.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(5)</label>
</disp-formula>
</p>
<p>where <italic>actiont<sub>t</sub></italic> is the action the participant chose (accept or reject) on the <italic>t</italic>-th trial, <italic>R<sub>t</sub></italic> is the reward on the <italic>t</italic>-th trial, which is defined as 1 (reward obtained) when the action taken was the same as the action the Teacher would have preferred, and 0 (no reward) otherwise. The softmax choice rule was used to translate these action values to predicted choice probabilities. This model has 2 free parameters.</p>
</sec>
<sec id="s5-1-4-4">
<title>Model 4 (Offer-Sensitive RL):</title>
<p>This RL model is a more complex variant of Model 3, and assumes a separate learning rate for each offer type (as in Model 3). In total, this model has 6 free parameters.</p>
</sec>
<sec id="s5-1-4-5">
<title>Model 5 (Offer-Sensitive RL with Separate Initial Values):</title>
<p>This RL model extends Model 3, assuming different initial action values for each offer type. Formally, this model treats <italic>Q<sub>0</sub>(reject, offertype), (offertype</italic> ∈ 90:10, 70: 30, 50: 50, 30: 70,10: 90) as free parameters with values between 0 and 1., resulting in 7 free parameters.</p>
</sec>
<sec id="s5-1-4-6">
<title>Model 6 (Preference Inference):</title>
<p>Model 6 posits that the participant infers the Fehr-Schmidt utility function (Equation 2) governing the Teacher’s preferences, and updates their modeled ‘guilt’ (<italic>α</italic>) and ‘envy’ (β) parameters incrementally from feedback, under the assumption that the Teacher’s indicated choices are made in accordance with each offer’s Fehr-Schmidt utility (more formally, the Teacher rejects the offer when <italic>U<sub>accept</sub></italic>(<italic>offer</italic>) &lt; 0). As <italic>α</italic> and <italic>β</italic> govern the disutility of unfair offers, the model infers the minimal value of <italic>α</italic> (or <italic>β</italic>) would lead to rejection of DI (or AI) offers, and similarly, the maximum values of <italic>α</italic> (or <italic>β</italic>) would lead to acceptance of DI (or AI) offers.</p>
<p>Accordingly, after observing that the Teacher prefers rejection in response to a DI offer, Equation 2 can be transformed to the following inequality:
<disp-formula id="FD6">
<alternatives>
<mml:math id="M6" display="block"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mo>−</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:math>
<graphic xlink:href="2405.06500v2_eqn6.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(6)</label>
</disp-formula>
</p>
<p>where the model can infer a lower bound of <italic>a,</italic> that would lead to the offer’s rejection by solving (6):
<disp-formula id="FD7">
<alternatives>
<mml:math id="M7" display="block"><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mfrac><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>−</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:math>
<graphic xlink:href="2405.06500v2_eqn7.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(7)</label>
</disp-formula>
</p>
<p>The right side of (7) can be denoted as <italic>α<sub>lb</sub></italic> , and then that trial’s estimate of <italic>α</italic> (denoted <italic>α<sub>t</sub></italic>) is updated as follows:
<disp-formula id="FD8">
<alternatives>
<mml:math id="M8" display="block"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math>
<graphic xlink:href="2405.06500v2_eqn8.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(8)</label>
</disp-formula>
</p>
<p>The parameter η governs the rate at which the learner’s estimate of the Teacher’s <italic>α</italic> value is updated, and is constrained to the range [0, 5].</p>
<p>The updating procedure is similar when Teacher indicates acceptance of a DI offer, which implies that <italic>U<sub>accept</sub></italic>(<italic>offer</italic>)&gt;0, and the following inequality:
<disp-formula id="FD9">
<alternatives>
<mml:math id="M9" display="block"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mo>−</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:math>
<graphic xlink:href="2405.06500v2_eqn9.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(9)</label>
</disp-formula>
</p>
<p>which yields a upper bound the for the envy parameter <italic>α</italic>
<disp-formula id="FD10">
<alternatives>
<mml:math id="M10" display="block"><mml:mi>α</mml:mi><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mrow><mml:mtext>offer</mml:mtext></mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>−</mml:mo><mml:mtext>offer</mml:mtext></mml:mrow></mml:mfrac></mml:math>
<graphic xlink:href="2405.06500v2_eqn10.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(10)</label>
</disp-formula>
</p>
<p>This upper bound, <italic>α<sub>ub</sub></italic>, is in turn used to update <italic>α<sub>t</sub></italic>:
<disp-formula id="FD11">
<alternatives>
<mml:math id="M11" display="block"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math>
<graphic xlink:href="2405.06500v2_eqn11.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(10)</label>
</disp-formula>
</p>
<p>In the case of AI offers, the model employs the identical procedure to update the ‘guilt’ parameter <italic>β</italic>, and <italic>α</italic> is updated only in DI offers, while <italic>β</italic> is only updated in AI offers. Following <xref ref-type="bibr" rid="c37">Luo et al. (2018)</xref>, <italic>α</italic> and <italic>β</italic> were restricted to the range of [0, 10]. The initial value of <italic>α</italic> and <italic>β</italic> are taken as free parameters in the range of [0, 10], resulting in a model with a total of 4 free parameters.</p>
<p>All models were fit via Maximum Likelihood Estimation, employing a nonlinear optimization procedure using 100 random start points in the parameter space in order to find the best-fitting parameter values for each participant. We then computed the Akaike Information Criterion (AIC; (<xref ref-type="bibr" rid="c1">Akaike, 1974</xref>) to select the best-fitting models of Learning phase choice behavior, penalizing each model’s goodness-fit-score by its complexity (i.e., number of free parameters). See <xref ref-type="table" rid="tbl8">Table S8</xref> and S14 for parameter estimates and goodness-of-fit metrics.</p>
</sec>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This work was funded by the European Union (ERC Starting Grant, NEUROGROUP, 101041799) and by an NSERC Discovery Grant, a New Researchers Startup Grant from the Fonds de Recherche du Québec - Nature et Technologies, and an infrastructure award from the Canadian Foundation for Innovation. Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.</p>
</ack>
<sec id="s6">
<title>Supplementary Information</title>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><p>Model comparison results in Experiment 2. <bold>a)</bold> AICs of the models considered in experiment 2. <bold>b),c)</bold>. Updating of the ‘guilt’ and ‘envy’ parameters indicates the sanity of the Preference Inference model.</p></caption>
<graphic xlink:href="2405.06500v2_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><p>In Experiment 2, Rejection rates changes in the learning phase (indexed by the rejection rate difference between the first and the last five trials in each offer type) predict the rejection rate change from baseline to transfer phase (the contagion).</p></caption>
<graphic xlink:href="2405.06500v2_fig8.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl1" position="float" orientation="portrait">
<label>Table S1.</label>
<caption><p>Reinforcement rates governing the Teacher’s feedback in the Learning Phase. For example, a rate 90% for 90:10 offers indicates that on 90% of trials with that offer type, the Teacher indicated they would have preferred rejection of that offer.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Offer type</th>
<th align="left" valign="top">AI-DI-Averse</th>
<th align="left" valign="top">DI-Averse</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">90:10</td>
<td align="left" valign="top">90%</td>
<td align="left" valign="top">90%</td>
</tr>
<tr>
<td align="left" valign="top">70:30</td>
<td align="left" valign="top">75%</td>
<td align="left" valign="top">75%</td>
</tr>
<tr>
<td align="left" valign="top">50:50</td>
<td align="left" valign="top">0%</td>
<td align="left" valign="top">0%</td>
</tr>
<tr>
<td align="left" valign="top">30:70</td>
<td align="left" valign="top">75%</td>
<td align="left" valign="top">0%</td>
</tr>
<tr>
<td align="left" valign="top">10:90</td>
<td align="left" valign="top">90%</td>
<td align="left" valign="top">0%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl2" position="float" orientation="portrait">
<label>Table S2.</label>
<caption><p>Fairness rating of the Teacher in the Learning Phase in Experiment 1. The teacher rated 90:10 offers as Strongly unfair (1) or Unfair (2) randomly in both conditions.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl2.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Offer type</th>
<th align="left" valign="top">AI-DI-Averse</th>
<th align="left" valign="top">DI-Averse</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">90:10</td>
<td align="left" valign="top">1 or 2</td>
<td align="left" valign="top">1 or 2</td>
</tr>
<tr>
<td align="left" valign="top">70:30</td>
<td align="left" valign="top">2 or 3</td>
<td align="left" valign="top">2 or 3</td>
</tr>
<tr>
<td align="left" valign="top">50:50</td>
<td align="left" valign="top">6 or 7</td>
<td align="left" valign="top">6 or 7</td>
</tr>
<tr>
<td align="left" valign="top">30:70</td>
<td align="left" valign="top">2 or 3</td>
<td align="left" valign="top">6 or 7</td>
</tr>
<tr>
<td align="left" valign="top">10:90</td>
<td align="left" valign="top">1 or 2</td>
<td align="left" valign="top">6 or 7</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl3" position="float" orientation="portrait">
<label>Table S3.</label>
<caption><p>Baseline Phase Choice Behavior in Experiment 1. Linear mixed model coefficients (fixed effects) indicating effects of condition and Offer type on the baseline Rejection rate.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl3.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">t</th>
<th align="right" valign="top">P</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">0.67</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">23.29</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.34</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">11.85</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.98</td>
<td align="right" valign="top">0.329</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">0.05</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">1.60</td>
<td align="right" valign="top">0.109</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">2.86</td>
<td align="right" valign="top">0.004</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">0.54</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">18.69</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.25</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">8.85</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">1.05</td>
<td align="right" valign="top">0.296</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">0.06</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">1.95</td>
<td align="right" valign="top">0.051</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">3.14</td>
<td align="right" valign="top">0.002</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl4" position="float" orientation="portrait">
<label>Table S4.</label>
<caption><p>Baseline Phase Rating Behavior in Experiment 1. Linear mixed model coefficients (fixed effects) indicating effects of condition and Offer type on the baseline Fairness Ratings (testing if the coefficient is equal to 4).</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl4.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">t</th>
<th align="right" valign="top">P</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">2.06</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">-13.92</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">3.16</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">-5.99</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">5.22</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">8.71</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">3.92</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">-0.55</td>
<td align="right" valign="top">0.583</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">3.19</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">-5.78</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">2.22</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">-12.75</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">3.24</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">-5.42</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">5.04</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">7.42</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">4.08</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">0.55</td>
<td align="right" valign="top">0.583</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">3.45</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">-3.96</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl5" position="float" orientation="portrait">
<label>Table S5.</label>
<caption><p>Contagion effects in rejection rates in Experiment 1, computed as the difference between Transfer and Learning Phase rejection rates. Linear mixed model coefficients (fixed effects) indicating the effects of Condition and Offer Type on the Rejection rate changes from Baseline to Transfer phase.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl5.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">t</th>
<th align="right" valign="top">P</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">4.97</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">4.25</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.94</td>
<td align="right" valign="top">0.349</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">3.10</td>
<td align="right" valign="top">0.002</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">4.47</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">0.18</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">6.56</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">5.12</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.02</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.65</td>
<td align="right" valign="top">0.517</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">-0.01</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">-0.43</td>
<td align="right" valign="top">0.666</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">-0.03</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">-1.08</td>
<td align="right" valign="top">0.280</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl6" position="float" orientation="portrait">
<label>Table S6.</label>
<caption><p>Contagion effects in Fairness ratings in Experiment 1, computed as the difference between Transfer and Learning Phase fairness ratings. Linear mixed model coefficients (fixed effects) indicating the effects of Condition and Offer Type on the Fairness rating changes.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl6.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">t</th>
<th align="right" valign="top">P</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">-0.14</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">-1.15</td>
<td align="right" valign="top">0.252</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">-0.28</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">-2.32</td>
<td align="right" valign="top">0.021</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.05</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.44</td>
<td align="right" valign="top">0.662</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">-0.54</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">-4.42</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">-0.76</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">-6.23</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">-0.06</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">-0.49</td>
<td align="right" valign="top">0.623</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">-0.07</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">-0.57</td>
<td align="right" valign="top">0.566</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.25</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">2.05</td>
<td align="right" valign="top">0.041</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">0.32</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">2.59</td>
<td align="right" valign="top">0.010</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">0.28</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">2.29</td>
<td align="right" valign="top">0.022</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl7" position="float" orientation="portrait">
<label>Table S7.</label>
<caption><p>Mixed-effects logistic regression examining Rejection choices (Reject vs. Accept) during the Learning Phase in Experiment 1, as a function of the interactions between Condition, Offer Type, and Trial Number.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl7.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">P</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">5.32</td>
<td align="right" valign="top">0.54</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.18</td>
<td align="right" valign="top">0.29</td>
<td align="right" valign="top">0.533</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">-5.47</td>
<td align="right" valign="top">0.41</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">-2.73</td>
<td align="right" valign="top">0.35</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">-1.34</td>
<td align="right" valign="top">0.39</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">4.59</td>
<td align="right" valign="top">0.52</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">-0.31</td>
<td align="right" valign="top">0.29</td>
<td align="right" valign="top">0.288</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">-5.61</td>
<td align="right" valign="top">0.43</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">-5.50</td>
<td align="right" valign="top">0.44</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">-6.20</td>
<td align="right" valign="top">0.53</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10xTrial Number</td>
<td align="right" valign="top">0.27</td>
<td align="right" valign="top">0.20</td>
<td align="right" valign="top">0.164</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30xTrial Number</td>
<td align="right" valign="top">0.36</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50xTrial Number</td>
<td align="right" valign="top">0.06</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">0.653</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70xTrial Number</td>
<td align="right" valign="top">0.47</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90xTrial Number</td>
<td align="right" valign="top">0.77</td>
<td align="right" valign="top">0.18</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10xTrial Number</td>
<td align="right" valign="top">0.63</td>
<td align="right" valign="top">0.20</td>
<td align="right" valign="top">0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30xTrial Number</td>
<td align="right" valign="top">0.31</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50xTrial Number</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.15</td>
<td align="right" valign="top">0.423</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70xTrial Number</td>
<td align="right" valign="top">-0.30</td>
<td align="right" valign="top">0.19</td>
<td align="right" valign="top">0.111</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90xTrial Number</td>
<td align="right" valign="top">-0.22</td>
<td align="right" valign="top">0.28</td>
<td align="right" valign="top">0.429</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl8" position="float" orientation="portrait">
<label>Table S8.</label>
<caption><p>Summary of Model Comparison in Experiment 1.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl8.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top">Model</th>
<th align="left" valign="top">nLL</th>
<th align="left" valign="top">AIC</th>
<th align="left" valign="top" colspan="7">Mean Parameter Estimates</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top" rowspan="2">Preference Inference</td>
<td align="left" valign="top" rowspan="2">3765.02</td>
<td align="left" valign="top" rowspan="2">9130.04</td>
<td align="left" valign="top"><italic>α</italic><sub>0</sub></td>
<td align="left" valign="top"><italic>β<sub>0</sub></italic></td>
<td align="left" valign="top"><italic>η</italic></td>
<td align="left" valign="top"><italic>τ</italic></td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">2.01</td>
<td align="left" valign="top">1.38</td>
<td align="left" valign="top">0.14</td>
<td align="left" valign="top">0.83</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">Static preference</td>
<td align="left" valign="top" rowspan="2">4146.96</td>
<td align="left" valign="top" rowspan="2">9493.93</td>
<td align="left" valign="top"><italic>α</italic></td>
<td align="left" valign="top"><italic>β</italic></td>
<td align="left" valign="top"><italic>τ</italic></td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">2.01</td>
<td align="left" valign="top">1.58</td>
<td align="left" valign="top">0.67</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">RL Basic</td>
<td align="left" valign="top" rowspan="2">6519.68</td>
<td align="left" valign="top" rowspan="2">13839.36</td>
<td align="left" valign="top"><italic>η</italic></td>
<td align="left" valign="top"><italic>τ</italic></td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">0.28</td>
<td align="left" valign="top">14.27</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">RL Separate Initial</td>
<td align="left" valign="top" rowspan="2">4097.73</td>
<td align="left" valign="top" rowspan="2">10995.45</td>
<td align="left" valign="top"><italic>η</italic></td>
<td align="left" valign="top"><italic>τ</italic></td>
<td align="left" valign="top"><italic>V</italic><sub>1</sub></td>
<td align="left" valign="top"><italic>V</italic><sub>2</sub></td>
<td align="left" valign="top"><italic>V</italic><sub>3</sub></td>
<td align="left" valign="top"><italic>V</italic><sub>4</sub></td>
<td align="left" valign="top"><italic>V</italic><sub>5</sub></td>
</tr>
<tr>
<td align="left" valign="top">0.06</td>
<td align="left" valign="top">35.14</td>
<td align="left" valign="top">0.72</td>
<td align="left" valign="top">0.44</td>
<td align="left" valign="top">0.11</td>
<td align="left" valign="top">0.20</td>
<td align="left" valign="top">0.26</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">RL Offer Sensitive</td>
<td align="left" valign="top" rowspan="2">5393.92</td>
<td align="left" valign="top" rowspan="2">13187.84</td>
<td align="left" valign="top"><italic>η</italic><sub>1</sub></td>
<td align="left" valign="top"><italic>η</italic><sub>2</sub></td>
<td align="left" valign="top"><italic>η</italic><sub>3</sub></td>
<td align="left" valign="top"><italic>η</italic><sub>4</sub></td>
<td align="left" valign="top"><italic>η</italic><sub>5</sub></td>
<td align="left" valign="top"><italic>τ</italic></td>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">0.35</td>
<td align="left" valign="top">0.16</td>
<td align="left" valign="top">0.74</td>
<td align="left" valign="top">0.44</td>
<td align="left" valign="top">0.48</td>
<td align="left" valign="top">33.68</td>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">Random Choosing</td>
<td align="left" valign="top" rowspan="2">4393.93</td>
<td align="left" valign="top" rowspan="2">10787.85</td>
<td align="left" valign="top"><italic>P<sub>1</sub></italic></td>
<td align="left" valign="top"><italic>P<sub>2</sub></italic></td>
<td align="left" valign="top"><italic>P<sub>3</sub></italic></td>
<td align="left" valign="top"><italic>P<sub>4</sub></italic></td>
<td align="left" valign="top"><italic>P<sub>5</sub></italic></td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">0.80</td>
<td align="left" valign="top">0.50</td>
<td align="left" valign="top">0.05</td>
<td align="left" valign="top">0.16</td>
<td align="left" valign="top">0.24</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="TFN1"><p>Parameters:</p>
<p><italic>α</italic>, <italic>α</italic><sub>0</sub>: Envy parameters</p>
<p><italic>β</italic>, <italic>β</italic><sub>0</sub>: Guilt parameters</p>
<p><italic>η</italic>, <italic>η<sub>i</sub></italic> (i = 1,2,3,4,5): Learning rates</p>
<p><italic>τ</italic>: Inverse temperature.</p>
<p><italic>V<sub>i</sub></italic> (<italic>i</italic> = 1,2,3,4,5): Initial values</p>
<p><italic>p<sub>i</sub></italic> (<italic>i</italic> = 1,2,3,4,5): Random choosing probabilities</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="tbl9" position="float" orientation="portrait">
<label>Table S9.</label>
<caption><p>Relationships between Learning Phase Behavior and Contagion in Experiment 1. Linear Mixed model coefficients (fixed effects) indicating the effects of Condition, Offer Type, and Learning index (Rejection rate change between first and last five trials of Learning phase) on the Contagion effect (Rejection rate changes from Baseline to Transfer phase).</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl9.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.003</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.316</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">0.06</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.052</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.003</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.02</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.528</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">-0.00</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.907</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">-0.03</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.300</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10xLearning</td>
<td align="right" valign="top">0.22</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.073</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30xLearning</td>
<td align="right" valign="top">0.33</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50xLearning</td>
<td align="right" valign="top">0.10</td>
<td align="right" valign="top">0.20</td>
<td align="right" valign="top">0.625</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70xLearning</td>
<td align="right" valign="top">0.26</td>
<td align="right" valign="top">0.10</td>
<td align="right" valign="top">0.010</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90xLearning</td>
<td align="right" valign="top">0.27</td>
<td align="right" valign="top">0.07</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10xLearning</td>
<td align="right" valign="top">0.48</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30xLearning</td>
<td align="right" valign="top">0.36</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50xLearning</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.18</td>
<td align="right" valign="top">0.503</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70xLearning</td>
<td align="right" valign="top">0.25</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.058</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90xLearning</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.10</td>
<td align="right" valign="top">0.196</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl10" position="float" orientation="portrait">
<label>Table S10.</label>
<caption><p>Contagion effects in Experiment 2. Linear mixed model coefficients (fixed effects) indicating the effects of Condition and Offer Type on the Rejection rate changes (from Baseline phase to Transfer phase).</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl10.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.10</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.05</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.078</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.345</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">0.06</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.060</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">0.18</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.002</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">-0.02</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.492</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">-0.03</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.280</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">-0.06</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.035</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl11" position="float" orientation="portrait">
<label>Table S11.</label>
<caption><p>Contagion effect of Fairness rating in Experiment 2. Linear mixed model coefficients (fixed effects) indicating the effects of Condition and Offer Type on the Fairness rating changes from Baseline to Transfer phase.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl11.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">-0.23</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.076</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">-0.41</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">-0.24</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.056</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">-0.53</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">-0.84</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">-0.29</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.023</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">-0.15</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.228</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">0.24</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.060</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.498</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">0.14</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.272</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl12" position="float" orientation="portrait">
<label>Table S12.</label>
<caption><p>Mixed-effects logistic regression examining Rejection choices (Reject vs. Accept) during the Learning Phase in Experiment 2, as a function of the interactions between Condition, Offer Type, and Trial Number.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl12.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">2.35</td>
<td align="right" valign="top">0.50</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">-0.86</td>
<td align="right" valign="top">0.35</td>
<td align="right" valign="top">0.013</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">-3.61</td>
<td align="right" valign="top">0.30</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">-3.32</td>
<td align="right" valign="top">0.34</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">-1.95</td>
<td align="right" valign="top">0.44</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">1.48</td>
<td align="right" valign="top">0.49</td>
<td align="right" valign="top">0.003</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">-1.38</td>
<td align="right" valign="top">0.35</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">-4.47</td>
<td align="right" valign="top">0.34</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">-4.76</td>
<td align="right" valign="top">0.37</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">-5.45</td>
<td align="right" valign="top">0.53</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10xTrial Number</td>
<td align="right" valign="top">0.36</td>
<td align="right" valign="top">0.26</td>
<td align="right" valign="top">0.167</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30xTrial Number</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">0.093</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50xTrial Number</td>
<td align="right" valign="top">0.56</td>
<td align="right" valign="top">0.24</td>
<td align="right" valign="top">0.019</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70xTrial Number</td>
<td align="right" valign="top">0.10</td>
<td align="right" valign="top">0.10</td>
<td align="right" valign="top">0.300</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90xTrial Number</td>
<td align="right" valign="top">1.06</td>
<td align="right" valign="top">0.31</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10xTrial Number</td>
<td align="right" valign="top">0.96</td>
<td align="right" valign="top">0.27</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30xTrial Number</td>
<td align="right" valign="top">0.26</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50xTrial Number</td>
<td align="right" valign="top">-0.14</td>
<td align="right" valign="top">0.27</td>
<td align="right" valign="top">0.593</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70xTrial Number</td>
<td align="right" valign="top">-0.37</td>
<td align="right" valign="top">0.13</td>
<td align="right" valign="top">0.003</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90xTrial Number</td>
<td align="right" valign="top">-0.54</td>
<td align="right" valign="top">0.40</td>
<td align="right" valign="top">0.170</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl13" position="float" orientation="portrait">
<label>Table S13.</label>
<caption><p>Mixed-effects regression examining Fairness ratings during the Learning Phase in Experiment 2, as a function of the interactions between Condition, Offer Type, and Trial Number.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl13.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Predictor</th>
<th align="right" valign="top">Estimate</th>
<th align="right" valign="top">SE</th>
<th align="right" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10</td>
<td align="right" valign="top">2.40</td>
<td align="right" valign="top">0.16</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30</td>
<td align="right" valign="top">3.22</td>
<td align="right" valign="top">0.11</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50</td>
<td align="right" valign="top">5.09</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70</td>
<td align="right" valign="top">3.50</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90</td>
<td align="right" valign="top">2.58</td>
<td align="right" valign="top">0.18</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10</td>
<td align="right" valign="top">2.72</td>
<td align="right" valign="top">0.16</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30</td>
<td align="right" valign="top">3.59</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50</td>
<td align="right" valign="top">5.41</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70</td>
<td align="right" valign="top">4.45</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90</td>
<td align="right" valign="top">4.08</td>
<td align="right" valign="top">0.18</td>
<td align="right" valign="top">&lt;0.001</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 90:10xTrial Number</td>
<td align="right" valign="top">0.05</td>
<td align="right" valign="top">0.06</td>
<td align="right" valign="top">0.424</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 70:30xTrial Number</td>
<td align="right" valign="top">-0.04</td>
<td align="right" valign="top">0.02</td>
<td align="right" valign="top">0.071</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 50:50xTrial Number</td>
<td align="right" valign="top">0.12</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">0.126</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 30:70xTrial Number</td>
<td align="right" valign="top">-0.10</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.002</td>
</tr>
<tr>
<td align="left" valign="top">AI-DI-AversexOffer 10:90xTrial Number</td>
<td align="right" valign="top">-0.27</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">0.004</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 90:10xTrial Number</td>
<td align="right" valign="top">-0.09</td>
<td align="right" valign="top">0.06</td>
<td align="right" valign="top">0.131</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 70:30xTrial Number</td>
<td align="right" valign="top">-0.02</td>
<td align="right" valign="top">0.02</td>
<td align="right" valign="top">0.333</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 50:50xTrial Number</td>
<td align="right" valign="top">0.16</td>
<td align="right" valign="top">0.08</td>
<td align="right" valign="top">0.054</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 30:70xTrial Number</td>
<td align="right" valign="top">0.01</td>
<td align="right" valign="top">0.03</td>
<td align="right" valign="top">0.828</td>
</tr>
<tr>
<td align="left" valign="top">DI-AversexOffer 10:90xTrial Number</td>
<td align="right" valign="top">0.07</td>
<td align="right" valign="top">0.09</td>
<td align="right" valign="top">0.448</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl14" position="float" orientation="portrait">
<label>Table S14.</label>
<caption><p>Summary of Model Comparison in Experiment 2.</p></caption>
<alternatives>
<graphic xlink:href="2405.06500v2_tbl14.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top">Model</th>
<th align="left" valign="top">nLL</th>
<th align="left" valign="top">AIC</th>
<th align="left" valign="top" colspan="7">Mean Parameter Estimates</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top" rowspan="2">Preference Inference</td>
<td align="left" valign="top" rowspan="2">6250.45</td>
<td align="left" valign="top" rowspan="2">14100.90</td>
<td align="left" valign="top"><italic>α</italic><sub>0</sub></td>
<td align="left" valign="top"><italic>β</italic><sub>0</sub></td>
<td align="left" valign="top"><italic>η</italic></td>
<td align="left" valign="top"><italic>τ</italic></td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">1.75</td>
<td align="left" valign="top">1.48</td>
<td align="left" valign="top">0.07</td>
<td align="left" valign="top">0.82</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">Static preference</td>
<td align="left" valign="top" rowspan="2">6522.31</td>
<td align="left" valign="top" rowspan="2">14244.63</td>
<td align="left" valign="top"><italic>α</italic></td>
<td align="left" valign="top"><italic>β</italic></td>
<td align="left" valign="top"><italic>τ</italic></td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">1.74</td>
<td align="left" valign="top">1.53</td>
<td align="left" valign="top">0.78</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">RL Separate Initial</td>
<td align="left" valign="top" rowspan="2">6720.48</td>
<td align="left" valign="top" rowspan="2">16240.97</td>
<td align="left" valign="top"><italic>η</italic></td>
<td align="left" valign="top"><italic>τ</italic></td>
<td align="left" valign="top"><italic>V</italic><sub>1</sub></td>
<td align="left" valign="top"><italic>V</italic><sub>2</sub></td>
<td align="left" valign="top"><italic>V</italic><sub>3</sub></td>
<td align="left" valign="top"><italic>V</italic><sub>4</sub></td>
<td align="left" valign="top"><italic>V</italic><sub>5</sub></td>
</tr>
<tr>
<td align="left" valign="top">0.01</td>
<td align="left" valign="top">38.16</td>
<td align="left" valign="top">0.58</td>
<td align="left" valign="top">0.39</td>
<td align="left" valign="top">0.15</td>
<td align="left" valign="top">0.23</td>
<td align="left" valign="top">0.26</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">Random Choosing</td>
<td align="left" valign="top" rowspan="2">6894.55</td>
<td align="left" valign="top" rowspan="2">15789.11</td>
<td align="left" valign="top"><italic>p</italic><sub>1</sub></td>
<td align="left" valign="top"><italic>p</italic><sub>2</sub></td>
<td align="left" valign="top"><italic>p</italic><sub>3</sub></td>
<td align="left" valign="top"><italic>p</italic><sub>4</sub></td>
<td align="left" valign="top"><italic>p</italic><sub>5</sub></td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">0.62</td>
<td align="left" valign="top">0.43</td>
<td align="left" valign="top">0.08</td>
<td align="left" valign="top">0.15</td>
<td align="left" valign="top">0.22</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="TFN2"><p>Parameters:</p>
<p><italic>α</italic>, <italic>α</italic><sub>0</sub>: Envy parameters</p>
<p><italic>β</italic>, <italic>β</italic><sub>0</sub>: Guilt parameters</p>
<p><italic>η</italic>: Learning rates</p>
<p><italic>τ</italic>: Inverse temperature.</p>
<p><italic>V<sub>i</sub></italic> (<italic>i</italic> = 1,2,3,4,5): Initial values</p>
<p><italic>p<sub>i</sub></italic> (<italic>i</italic> = 1,2,3,4,5): Random choosing probabilities</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akaike</surname>, <given-names>H.</given-names></string-name></person-group> (<year>1974</year>). <article-title>A New Look at the Statistical Model Identification</article-title>. <source>IEEE Transactions on Automatic Control</source>, <volume>19</volume>(<issue>6</issue>), <fpage>716</fpage>–<lpage>723</lpage>. <pub-id pub-id-type="doi">10.1109/TAC.1974.1100705</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amir</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Melnikoff</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Warneken</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Blake</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Corbit</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Callaghan</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Barry</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bowie</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kleutsch</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kramer</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Ross</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Vongsachang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wrangham</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>McAuliffe</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Computational signatures of inequity aversion in children across seven societies</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>152</volume>(<issue>10</issue>), <fpage>2882</fpage>–<lpage>2896</lpage>. <pub-id pub-id-type="doi">10.1037/xge0001385</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anzellotti</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Young</surname>, <given-names>L. L.</given-names></string-name></person-group> (<year>2020</year>). <article-title>The acquisition of person knowledge</article-title>. <source>Annual Review of Psychology</source>, <volume>71</volume>, <fpage>613</fpage>–<lpage>634</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-psych-010419-050844</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bail</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Argyle</surname>, <given-names>L. P.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>T. W.</given-names></string-name>, <string-name><surname>Bumpus</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Fallin Hunzaker</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Merhout</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Volfovsky</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Exposure to opposing views on social media can increase political polarization</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>115</volume>(<issue>37</issue>), <fpage>9216</fpage>–<lpage>9221</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1804840115</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baker</surname></string-name>, <string-name><surname>Chris</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Saxe</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Citation Accessed Action Understanding as Inverse Planning</article-title>. <source>Cognition</source>, <volume>113</volume>(<issue>3</issue>), <fpage>329</fpage>–<lpage>349</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bandura</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Mcdonald</surname>, <given-names>Frederick. J.</given-names></string-name></person-group> (<year>1963</year>). <article-title>Influence of social reinforcement and the behavior of models in shaping children’s moral judgment</article-title>. <source>Influence of Social Reinforcement and the Behavior of Models in Shaping Children’s Moral Judgment</source>., <volume>67</volume>(<issue>3</issue>), <fpage>274</fpage>–<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bates</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mçchler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bolker</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Walker</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Fitting Linear Mixed-Effects Models Using lme4</article-title>. <source>Journal of Statistical Software</source>, <volume>67</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blake</surname>, <given-names>P. R.</given-names></string-name>, &amp; <string-name><surname>McAuliffe</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2011</year>). <article-title>“I had so much it didn’t seem fair” Eight-year-olds reject two forms of inequity</article-title>. <source>Cognition</source>, <volume>120</volume>(<issue>2</issue>), <fpage>215</fpage>–<lpage>224</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2011.04.006</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blake</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>McAuliffe</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Corbit</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Callaghan</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Barry</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bowie</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kleutsch</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kramer</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Ross</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Vongsachang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wrangham</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Warneken</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2015</year>). <article-title>The ontogeny of fairness in seven societies</article-title>. <source>Nature</source>, <volume>528</volume>(<issue>7581</issue>), <fpage>258</fpage>–<lpage>261</lpage>. <pub-id pub-id-type="doi">10.1038/nature15703</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brosnan</surname>, <given-names>S. F.</given-names></string-name>, &amp; <string-name><surname>De Waal</surname>, <given-names>F. B. M.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Monkeys reject unequal pay</article-title>. <source>Nature</source>, <volume>425</volume>(<issue>6955</issue>), <fpage>297</fpage>–<lpage>299</lpage>. <pub-id pub-id-type="doi">10.1038/nature01963</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brosnan</surname>, <given-names>S. F.</given-names></string-name>, &amp; <string-name><surname>De Waal</surname>, <given-names>F. B. M.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Evolution of responses to (un)fairness</article-title>. <source>Science</source>, <volume>346</volume>(<issue>6207</issue>). <pub-id pub-id-type="doi">10.1126/science.1251776</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burke</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Tobler</surname>, <given-names>P. N.</given-names></string-name>, <string-name><surname>Baddeley</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Schultz</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Neural mechanisms of observational learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>107</volume>(<issue>32</issue>), <fpage>14431</fpage>–<lpage>14436</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1003111107</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cialdini</surname>, <given-names>R. B.</given-names></string-name>, &amp; <string-name><surname>Goldstein</surname>, <given-names>N. J.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Social Influence: Compliance and Conformity</article-title>. <source>Annual Review of Psychology</source>, <volume>55</volume>(<issue>1</issue>), <fpage>591</fpage>–<lpage>621</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.psych.55.090902.142015</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Civai</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Crescentini</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Rustichini</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Rumiati</surname>, <given-names>R. I.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Equality versus self-interest in the brain: Differential roles of anterior insula and medial prefrontal cortex</article-title>. <source>NeuroImage</source>, <volume>62</volume>(<issue>1</issue>), <fpage>102</fpage> <lpage>112</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.037</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crump</surname>, <given-names>M. J. C.</given-names></string-name>, <string-name><surname>McDonnell</surname>, <given-names>J. V.</given-names></string-name>, &amp; <string-name><surname>Gureckis</surname>, <given-names>T. M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Evaluating Amazon’s Mechanical Turk as a Tool for Experimental Behavioral Research</article-title>. <source>PLoS ONE</source>, <volume>8</volume>(<issue>3</issue>). <pub-id pub-id-type="doi">10.1371/journal.pone.0057410</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diaconescu</surname>, <given-names>A. O.</given-names></string-name>, <string-name><surname>Stecy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kasper</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Burke</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Nagy</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Mathys</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Tobler</surname>, <given-names>P. N.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Neural arbitration between social and individual learning systems</article-title>. <source>eLife</source>, <volume>9</volume>, <fpage>1</fpage>–<lpage>35</lpage>. <pub-id pub-id-type="doi">10.7554/eLife.54051</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engelmann</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Fischbacher</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Indirect reciprocity and strategic reputation building in an experimental helping game</article-title>. <source>Games and Economic Behavior</source>, <volume>67</volume>(<issue>2</issue>), <fpage>399</fpage>–<lpage>407</lpage>. <pub-id pub-id-type="doi">10.1016/j.geb.2008.12.006</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Essler</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Marshall-Pescini</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Range</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Domestication Does Not Explain the Presence of Inequity Aversion in Dogs</article-title>. <source>Current Biology</source>, <volume>27</volume>(<issue>12</issue>), <fpage>1861</fpage>–<lpage>1865.e3</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2017.05.061</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fehr</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bernhard</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Rockenbach</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Egalitarianism in young children</article-title>. <source>Nature</source>, <volume>454</volume>(<issue>7208</issue>), <fpage>1079</fpage>–<lpage>1083</lpage>. <pub-id pub-id-type="doi">10.1038/nature07155</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fehr</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Schmidt</surname>, <given-names>K. M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>A Theory of Fairness , Competition , and Cooperation</article-title>. <source>The Quarterly Journal of Economics</source>, <volume>114</volume>(<issue>3</issue>), <fpage>817</fpage>–<lpage>868</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>FeldmanHall</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Otto</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name><surname>Phelps</surname>, <given-names>E. A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Learning moral values: Another’s desire to punish enhances one’s own punitive behavior</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>147</volume>(<issue>8</issue>), <fpage>1211</fpage>–<lpage>1224</lpage>. <pub-id pub-id-type="doi">10.1037/xge0000405</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>FeldmanHall</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Shenhav</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Resolving uncertainty in a social world</article-title>. <source>Nature Human Behaviour</source>. <pub-id pub-id-type="doi">10.1038/s41562-019-0590-x</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sáez</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Blue</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hsu</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Zhou</surname>, <given-names>X.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Distinguishing neural correlates of context-dependent advantageous- and disadvantageous-inequity aversion</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>115</volume>(<issue>33</issue>), <elocation-id>E7680–E7689</elocation-id>. <pub-id pub-id-type="doi">10.1073/pnas.1802523115</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garvert</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Moutoussis</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kurth-Nelson</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E. J.</given-names></string-name>, &amp; <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Learning-Induced Plasticity in Medial Prefrontal Cortex Predicts Preference Malleability</article-title>. <source>Neuron</source>, <volume>85</volume>(<issue>2</issue>), <fpage>418</fpage>–<lpage>428</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.033</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Güth</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Schmittberger</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Schwarze</surname>, <given-names>B.</given-names></string-name></person-group> (<year>1982</year>). <article-title>An experimental analysis of ultimatum bargaining</article-title>. <source>Journal of Economic Behavior &amp; Organization</source>, <volume>3</volume>(<issue>4</issue>), <fpage>367</fpage>–<lpage>388</lpage>. <pub-id pub-id-type="doi">10.1016/0167-2681(82)90011-7</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayes</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Ashford</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Bennett</surname>, <given-names>S. J.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Goal-directed imitation: The means to an end</article-title>. <source>Acta Psychologica</source>, <volume>127</volume>(<issue>2</issue>), <fpage>407</fpage>–<lpage>415</lpage>. <pub-id pub-id-type="doi">10.1016/j.actpsy.2007.07.009</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heffner</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>FeldmanHall</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A probabilistic map of emotional experiences during competitive social interactions</article-title>. <source>Nature Communications</source>, <volume>13</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-022-29372-8</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hein</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Silani</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Preuschoff</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Batson</surname>, <given-names>C. D.</given-names></string-name>, &amp; <string-name><surname>Singer</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Neural responses to ingroup and outgroup members’ suffering predict individual differences in costly helping</article-title>. <source>Neuron</source>, <volume>68</volume>(<issue>1</issue>), <fpage>149</fpage>–<lpage>160</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.003</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hennig-Schmidt</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Z. Y.</given-names></string-name>, &amp; <string-name><surname>Yang</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Why people reject advantageous offers-Non-monotonic strategies in ultimatum bargaining. Evaluating a video experiment run in PR China</article-title>. <source>Journal of Economic Behavior and Organization</source>, <volume>65</volume>(<issue>2</issue>), <fpage>373</fpage>–<lpage>384</lpage>. <pub-id pub-id-type="doi">10.1016/j.jebo.2005.10.003</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henrich</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>McElreath</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Barr</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ensminger</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Barrett</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bolyanatz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cardaroas</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Gurven</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gwako</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Henrich</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lesoronol</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Marlowe</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Tracer</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Ziker</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Costly punishment across human societies</article-title>. <source>Science</source>, <volume>312</volume>(<issue>5781</issue>), <fpage>1767</fpage>–<lpage>1770</lpage>. <pub-id pub-id-type="doi">10.1126/science.1127333</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hertz</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Learning how to behave: Cognitive learning processes account for asymmetries in adaptation to social norms</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>, <volume>288</volume>(<issue>1952</issue>). <pub-id pub-id-type="doi">10.1098/rspb.2021.0293</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hester</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Gray</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2020</year>). <article-title>The Moral Psychology of Raceless, Genderless Strangers</article-title>. <source>Perspectives on Psychological Science</source>, <volume>15</volume>(<issue>2</issue>), <fpage>216</fpage>–<lpage>230</lpage>. <pub-id pub-id-type="doi">10.1177/1745691619885840</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jara-ettinger</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>ScienceDirect Theory of mind as inverse reinforcement learning</article-title>. <source>COBEHA</source>, <volume>29</volume>, <fpage>105</fpage>–<lpage>110</lpage>. <pub-id pub-id-type="doi">10.1016/j.cobeha.2019.04.010</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khalvati</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Mirbagheri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Philippe</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sestito</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dreher</surname>, <given-names>J.-C.</given-names></string-name>, &amp; <string-name><surname>Rao</surname>, <given-names>R. P. N.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Modeling Other Minds: Bayesian Inference Explains Human Choices in Group Decision Making</article-title>. <source>Science Advances</source>, in press(<issue>November</issue>), <fpage>aax8783</fpage>. <pub-id pub-id-type="doi">10.1101/419515</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leong</surname>, <given-names>Y. C.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Willer</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Zaki</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Conservative and liberal attitudes drive polarized neural responses to political content</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>44</issue>), <fpage>27731</fpage>–<lpage>27739</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.2008530117</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lindström</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Golkar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jangard</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tobler</surname>, <given-names>P. N.</given-names></string-name>, &amp; <string-name><surname>Olsson</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Lindstrom et al. - 2019— Social threat learning transfers to decision making in humans.pdf</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>116</volume>(<issue>10</issue>), <fpage>4732</fpage>–<lpage>4737</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1810180116</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luo</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hétu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lohrenz</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hula</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ramey</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Sonnier-Netto</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lisinski</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>LaConte</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nolte</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Fonagy</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rahmani</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Montague</surname>, <given-names>P. R.</given-names></string-name>, &amp; <string-name><surname>Ramey</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Early childhood investment impacts social decision-making four decades later</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>), <fpage>4705</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-07138-5</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAuliffe</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Blake</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wrangham</surname>, <given-names>R. W.</given-names></string-name>, &amp; <string-name><surname>Warneken</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Social influences on inequity aversion in children</article-title>. <source>PLoS ONE</source>, <volume>8</volume>(<issue>12</issue>). <pub-id pub-id-type="doi">10.1371/journal.pone.0080966</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAuliffe</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Blake</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Steinbeis</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Warneken</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2017</year>). <article-title>The developmental foundations of human fairness</article-title>. <source>Nature Human Behaviour</source>, <volume>1</volume>(<issue>2</issue>). <pub-id pub-id-type="doi">10.1038/s41562-016-0042</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAuliffe</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Blake</surname>, <given-names>P. R.</given-names></string-name>, &amp; <string-name><surname>Warneken</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Children reject inequity out of spite</article-title>. <source>Biology Letters</source>, <volume>10</volume> (<issue>12</issue>), <fpage>1</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1098/rsbl.2014.0743</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAuliffe</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Dunham</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Fairness overrides group bias in children’s second-party punishment</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>146</volume>(<issue>4</issue>), <fpage>485</fpage>–<lpage>494</lpage>. <pub-id pub-id-type="doi">10.1037/xge0000244</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedersen</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Kurzban</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>McCullough</surname>, <given-names>M. E.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Do humans really punish altruistically? A closer look</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>, <volume>280</volume>(<issue>1758</issue>), <fpage>20122723</fpage>. <pub-id pub-id-type="doi">10.1098/rspb.2012.2723</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pillutla</surname>, <given-names>M. M.</given-names></string-name>, &amp; <string-name><surname>Murnighan</surname>, <given-names>J. K.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Unfairness, anger, and spite: Emotional rejections of ultimatum offers</article-title>. <source>Organizational Behavior and Human Decision Processes</source>, <volume>68</volume>(<issue>3</issue>), <fpage>208</fpage>–<lpage>224</lpage>. <pub-id pub-id-type="doi">10.1006/obhd.1996.0100</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanfey</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Rilling</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Aronson</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Nystrom</surname>, <given-names>L. E.</given-names></string-name>, &amp; <string-name><surname>Cohen</surname>, <given-names>J. D.</given-names></string-name></person-group> (<year>2003</year>). <article-title>The neural basis of economic decision-making in the Ultimatum Game</article-title>. <source>Science</source>, <volume>300</volume>(<issue>5626</issue>), <fpage>1755</fpage>–<lpage>1758</lpage>. <pub-id pub-id-type="doi">10.1126/science.1082976</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Son</surname>, <given-names>J. Y.</given-names></string-name>, <string-name><surname>Bhandari</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>FeldmanHall</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Crowdsourcing punishment: Individuals reference group preferences to inform their own punitive decisions</article-title>. <source>Scientific Reports</source>, <volume>9</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-019-48050-2</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suzuki</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>E. L. S.</given-names></string-name>, <string-name><surname>Bossaerts</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>O’Doherty</surname>, <given-names>J. P.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Behavioral contagion during learning about another agent’s risk-preferences acts on the neural representation of decision-risk</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>14</issue>), <fpage>3755</fpage>–<lpage>3760</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1600092113</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taber</surname>, <given-names>C. S.</given-names></string-name>, &amp; <string-name><surname>Lodge</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Motivated skepticism in the evaluation of political beliefs (2006)</article-title>. <source>Critical Review</source>, <volume>24</volume>(<issue>2</issue>), <fpage>157</fpage>–<lpage>184</lpage>. <pub-id pub-id-type="doi">10.1080/08913811.2012.711019</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tomasello</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Becoming human: A theory of ontogeny</article-title>. In <source>Belknap</source>. <ext-link ext-link-type="uri" xlink:href="https://books.google.com/books?hl=en&amp;lr=&amp;id=ZnhyDwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=tomasello+2019&amp;ots=5fZGGcafP9&amp;sig=gD9NQYvz6NsHaAvUkQEgx5crHik#v=onepage&amp;q=tomasello2019&amp;f=false">https://books.google.com/books?hl=en&amp;lr=&amp;id=ZnhyDwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=tomasello+2019&amp;ots=5fZGGcafP9&amp;sig=gD9NQYvz6NsHaAvUkQEgx5crHik#v=onepage&amp;q=tomasello2019&amp;f=false</ext-link></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsvetkova</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Macy</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The Social Contagion of Generosity</article-title>. <source>PLOS ONE</source>, <volume>9</volume>(<issue>2</issue>), <elocation-id>e87275</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0087275</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Baar</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>L. J.</given-names></string-name>, &amp; <string-name><surname>Sanfey</surname>, <given-names>A. G.</given-names></string-name></person-group> (<year>2019</year>). <article-title>The computational and neural substrates of moral strategies in social decision-making</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), <fpage>1483</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-09161-6</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Wolkenten</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brosnan</surname>, <given-names>S. F.</given-names></string-name>, &amp; <string-name><surname>De Waal</surname>, <given-names>F. B. M.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Inequity responses of monkeys modified by effort</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>104</volume>(<issue>47</issue>), <fpage>18854</fpage>–<lpage>18859</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0707182104</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vives</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Cikara</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>FeldmanHall</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Following Your Group or Your Morals? The In Group Promotes Immoral Behavior While the Out-Group Buffers Against It</article-title>. <source>Social Psychological and Personality Science</source>, <volume>13</volume>(<issue>1</issue>), <fpage>139</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.1177/19485506211001217</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vives</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Feldmanhall</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Tolerance to ambiguous uncertainty predicts prosocial behavior</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>), <fpage>25</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-04631-9</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102800.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This cleverly-designed and potentially <bold>important</bold> work supports our understanding regarding how and whether social behaviours promoting egalitarianism can be learned, even when implementing these norms entails a cost for oneself. However, the evidence supporting the major claims is currently <bold>incomplete</bold>, with major limitations being the statistical approach, the modelling, and over-interpretation. With a strengthening of the supporting evidence, this work will be of interest to a wide range of fields, including cognitive psychology/neuroscience, neuroeconomics, and social psychology, as well as policy making.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102800.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Zhang et al. addressed the question of whether advantageous and disadvantageous inequality aversion can be vicariously learned and generalized. Using an adapted version of the ultimatum game (UG), in three phases, participants first gave their own preference (baseline phase), then interacted with a &quot;teacher&quot; to learn their preference (learning phase), and finally were tested again on their own (transfer phase). The key measure is whether participants exhibited similar choice preferences (i.e., rejection rate and fairness rating) influenced by the learning phase, by contrasting their transfer phase and baseline phase. Through a series of statistical modeling and computational modeling, the authors reported that both advantageous and disadvantageous inequality aversion can indeed be learned (Study 1), and even be generalised (Study 2).</p>
<p>Strengths:</p>
<p>This study is very interesting, it directly adapted the lab's previous work on the observational learning effect on disadvantageous inequality aversion, to test both advantageous and disadvantageous inequality aversion in the current study. Social transmission of action, emotion, and attitude have started to be looked at recently, hence this research is timely. The use of computational modeling is mostly appropriate and motivated. Study 2, which examined the vicarious inequality aversion in conditions where feedback was never provided, is interesting and important to strengthen the reported effects. Both studies have proper justifications to determine the sample size.</p>
<p>Weaknesses:</p>
<p>Despite the strengths, a few conceptual aspects and analytical decisions have to be explained, justified, or clarified.</p>
<p>INTRODUCTION/CONCEPTUALIZATION</p>
<p>
(1) Two terms seem to be interchangeable, which should not, in this work: vicarious/observational learning vs preference learning. For vicarious learning, individuals observe others' actions (and optionally also the corresponding consequence resulting directly from their own actions), whereas, for preference learning, individuals predict, or act on behalf of, the others' actions, and then receive feedback if that prediction is correct or not. For the current work, it seems that the experiment is more about preference learning and prediction, and less so about vicarious learning. The intro and set are heavily around vicarious learning, and later the use of vicarious learning and preference learning is rather mixed in the text. I think either tone down the focus on vicarious learning, or discuss how they are different. Some of the references here may be helpful: Charpentier et al., Neuron, 2020; Olsson et al., Nature Reviews Neuroscience, 2020; Zhang &amp; Glascher, Science Advances, 2020</p>
<p>EXPERIMENTAL DESIGN</p>
<p>
(2) For each offer type, the experiment &quot;added a uniformly distributed noise in the range of (-10 ,10)&quot;. I wonder what this looks like? With only integers such as 25:75, or even with decimal points? More importantly, is it possible to have either 70:30 or 90:10 option, after adding the noise, to have generated an 80:20 split shown to the participants? If so, for the analyses later, when participants saw the 80:20 split, which condition did this trial belong to? 70:30 or 90:10? And is such noise added only to the learning phase, or also to the baseline/transfer phases? This requires some clarification.</p>
<p>(3) For the offer conditions (90:10, 70:30, 50:50, 30:70, 10:90) - are they randomized? If so, how is it done? Is it randomized within each participant, and/or also across participants (such that each participant experienced different trial sequences)? This is important, as the order especially for the learning phase can largely impact the preference learning of the participants.</p>
<p>STATISTICAL ANALYSIS &amp; COMPUTATIONAL MODELING</p>
<p>
(4) In Study 1 DI offer types (90:10, 70:30), the rejection rate for DI-AI averse looks consistently higher than that for DI averse (ie, the blue line is above the yellow line). Is this significant? If so, how come? Since this is a between-subject design, I would not anticipate such a result (especially for the baseline). Also, for the LME results (eg, Table S3), only interactions were reported but not the main results.</p>
<p>(5) I do not particularly find this analysis appealing: &quot;we examined whether participants' changes in rejection rates between Transfer and Baseline, could be explained by the degree to which they vicariously learned, defined as the change in punishment rates between the first and last 5 trials of the Learning phase.&quot; Naturally, the participants' behavior in the first 5 trials in the learning phase will be similar to those in the baseline; and their behavior in the last 5 trials in the learning phase would echo those at the transfer phase. I think it would be stronger to link the preference learning results to the change between the baseline and transfer phase, eg, by looking at the difference between alpha (beta) at the end of the learning phase and the initial alpha (beta).</p>
<p>(6) I wonder if data from the baseline and transfer phases can also be modeled, using a simple Fehr-Schimdt model. This way, the change in alpha/beta can also be examined between the baseline and transfer phase.</p>
<p>(7) I quite liked Study 2 which tests the generalization effect, and I expected to see an adapted computational modeling to directly reflect this idea. Indeed, the authors wrote, &quot;[...] given that this model [...] assumes the sort of generalization of preferences between offer types [...]&quot;. But where exactly did the preference learning model assume the generalization? In the methods, the modeling seems to be only about Study 1; did the authors advise their model to accommodate Study 2? The authors also ran simulation for the learning phase in Study 2 (Figure 6), and how did the preference update (if at all) for offers (90:10 and 10:90) where feedback was not given? Extending/Unpacking the computational modeling results for Study 2 will be very helpful for the paper.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102800.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigates whether individuals can learn to adopt egalitarian norms that incur a personal monetary cost, such as rejecting offers that benefit them more than the giver (advantageous inequitable offers). While these behaviors are uncommon, two experiments demonstrate that individuals can learn to reject such offers through vicarious learning - by observing and acting in line with a &quot;teacher&quot; who follows these norms. The authors use computational modelling to argue that learners adopt these norms through a sophisticated process, inferring the latent structure of the teacher's preferences, akin to theory of mind.</p>
<p>Strengths:</p>
<p>This paper is well-written and tackles a critical topic relevant to social norms, morality, and justice. The findings, which show that individuals can adopt just and fair norms even at a personal cost, are promising. The study is well-situated in the literature, with clever experimental design and a computational approach that may offer insights into latent cognitive processes. Findings have potential implications for policymakers.</p>
<p>Weaknesses:</p>
<p>Note: in the text below, the &quot;teacher&quot; will refer to the agent from which a participant presumably receives feedback during the learning phase.</p>
<p>(1) Focus on Disadvantageous Inequity (DI): A significant portion of the paper focuses on responses to Disadvantageous Inequitable (DI) offers, which is confusing given the study's primary aim is to examine learning in response to Advantageous Inequitable (AI) offers. The inclusion of DI offers is not well-justified and distracts from the main focus. Furthermore, the experimental design seems, in principle, inadequate to test for the learning effects of DI offers. Because both teaching regimes considered were identical for DI offers the paradigm lacks a control condition to test for learning effects related to these offers. I can't see how an increase in rejection of DI offers (e.g., between baseline and generalization) can be interpreted as speaking to learning. There are various other potential reasons for an increase in rejection of DI offers even if individuals learn nothing from learning (e.g. if envy builds up during the experiment as one encounters more instances of disadvantageous fairness).</p>
<p>(2) Statistical Analysis: The analysis of the learning effects of AI offers is not fully convincing. The authors analyse changes in rejection rates within each learning condition rather than directly comparing the two. Finding a significant effect in one condition but not the other does not demonstrate that the learning regime is driving the effect. A direct comparison between conditions is necessary for establishing that there is a causal role for the learning regime.</p>
<p>(3) Correlation Between Learning and Contagion Effects:</p>
<p>
The authors argue that correlations between learning effects (changes in rejection rates during the learning phase) and contagion effects (changes between the generalization and baseline phases) support the idea that individuals who are better aligning their preferences with the teacher also give more consideration to the teacher's preferences later during generalization phase. This interpretation is not convincing. Such correlations could emerge even in the absence of learning, driven by temporal trends like increasing guilt or envy (or even by slow temporal fluctuations in these processes) on behalf of self or others. The reason is that the baseline phase is temporally closer to the beginning of the learning phase whereas the generalization phase is temporally closer to the end of the learning phase. Additionally, the interpretation of these effects seems flawed, as changes in rejection rates do not necessarily indicate closer alignment with the teacher's preferences. For example, if the teacher rejects an offer 75% of the time then a positive 5% learning effect may imply better matching the teacher if it reflects an increase in rejection rate from 65% to 70%, but it implies divergence from the teacher if it reflects an increase from 85% to 90%. For similar reasons, it is not clear that the contagion effects reflect how much a teacher's preferences are taken into account during generalization.</p>
<p>(4) Modeling Efforts: The modelling approach is underdeveloped. The identification of the &quot;best model&quot; lacks transparency, as no model-recovery results are provided, and fits for the losing models are not shown, leaving readers in the dark about where these models fail. Moreover, the reinforcement learning (RL) models used are overly simplistic, treating actions as independent when they are likely inversely related (for example, the feedback that the teacher would have rejected an offer provides feedback that rejection is &quot;correct&quot; but also that acceptance is &quot;an error&quot;, and the later is not incorporated into the modelling). It is unclear if and to what extent this limits current RL formulations. There are also potentially important missing details about the models. Can the authors justify/explain the reasoning behind including these variants they consider? What are the initial Q-values? If these are not free parameters what are their values?</p>
<p>(5) Conceptual Leap in Modeling Interpretation: The distinction between simple RL models and preference-inference models seems to hinge on the ability to generalize learning from one offer to another. Whereas in the RL models learning occurs independently for each offer (hence to cross-offer generalization), preference inference allows for generalization between different offers. However, the paper does not explore RL models that allow generalization based on the similarity of features of the offers (e.g., payment for the receiver, payment for the offer-giver, who benefits more). Such models are more parsimonious and could explain the results without invoking a theory of mind or any modelling of the teacher. In such model versions, a learner learns a functional form that allows to predict the teacher's feedback based on said offer features (e.g., linear or quadratic form). Because feedback for an offer modulates the parameters of this function (feature weights) generalization occurs without necessarily evoking any sophisticated model of the other person. This leaves open the possibility that RL models could perform just as well or even show superiority over the preference learning model, casting doubt on the authors' conclusions. Of note: even the behaviourists knew that as Little Albert was taught to fear rats, this fear generalized to rabbits. This could occur simply because rabbits are somewhat similar to rats. But this doesn't mean little Alfred had a sophisticated model of animals he used to infer how they behave.</p>
<p>(6) Limitations of the Preference-Inference Model: The preference-inference model struggles to capture key aspects of the data, such as the increase in rejection rates for 70:30 DI offers during the learning phase (e.g. Figure 3A, AI+DI blue group). This is puzzling.</p>
<p>Thinking about this I realized the model makes quite strong unintuitive predictions that are not examined. For example, if a subject begins the learning phase rejecting the 70:30 offer more than 50% of the time (meaning the starting guilt parameter is higher than 1.5), then overleaning the tendency to reject will decrease to below 50% (the guilt parameter will be pulled down below 1.5). This is despite the fact the teacher rejects 75% of the offers. In other words, as learning continues learners will diverge from the teacher. On the other hand, if a participant begins learning to tend to accept this offer (guilt &lt; 1.5) then during learning they can increase their rejection rate but never above 50%. Thus one can never fully converge on the teacher. I think this relates to the model's failure in accounting for the pattern mentioned above. I wonder if individuals actually abide by these strict predictions. In any case, these issues raise questions about the validity of the model as a representation of how individuals learn to align with a teacher's preferences (given that the model doesn't really allow for such an alignment).</p>
</body>
</sub-article>
</article>