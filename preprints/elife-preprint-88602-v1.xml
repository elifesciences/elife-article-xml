<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">88602</article-id>
<article-id pub-id-type="doi">10.7554/eLife.88602</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88602.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Disentangled multi-subject and social behavioral representations through a constrained subspace variational autoencoder (CS-VAE)</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Yi</surname>
<given-names>Daiyao</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Musall</surname>
<given-names>Simon</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Churchland</surname>
<given-names>Anne</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Padilla-Coreano</surname>
<given-names>Nancy</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Saxena</surname>
<given-names>Shreya</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Electrical and Computer Engineering, University of Florida</institution>, Gainesville, FL, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Department of Neurophysiology, Institute of Biology 2, RWTH Aachen University</institution>, Aachen, <country>Germany</country></aff>
<aff id="a3"><label>3</label><institution>Department of Neurobiology, University of California</institution>, Los Angeles, Los Angeles, CA, <country>USA</country></aff>
<aff id="a4"><label>4</label><institution>Department of Neuroscience, University of Florida</institution>, Gainesville, FL, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label> Corresponding author; email: <email>shreya.saxena@ufl.edu</email></corresp>
<fn id="n1" fn-type="others"><p>{<email>yidaiyao@ufl.edu</email>, <email>shreya.saxena@ufl.edu</email>}</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-07-17">
<day>17</day>
<month>07</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP88602</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-05-08">
<day>08</day>
<month>05</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-05-04">
<day>04</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.09.01.506091"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Yi et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Yi et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-88602-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Effectively modeling and quantifying behavior is essential for our understanding of the brain. Modeling behavior in naturalistic settings in social and multi-subject tasks remains a significant challenge. Modeling the behavior of different subjects performing the same task requires partitioning the behavioral data into features that are common across subjects, and others that are distinct to each subject. Modeling social interactions between multiple individuals in a freely-moving setting requires disentangling effects due to the individual as compared to social investigations. To achieve flexible disentanglement of behavior into interpretable latent variables with individual and across-subject or social components, we build on a semi-supervised approach to partition the behavioral subspace, and propose a novel regularization based on the Cauchy-Schwarz divergence to the model. Our model, known as the constrained subspace variational autoencoder (CS-VAE), successfully models distinct features of the behavioral videos across subjects, as well as continuously varying differences in social behavior. Our approach vastly facilitates the analysis of the resulting latent variables in downstream tasks such as uncovering disentangled behavioral motifs, the efficient decoding of a novel subject’s behavior, and provides an understanding of how similarly different animals perform innate behaviors.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Updated manuscript.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Effective study of the relationship between neural signals and ensuing behavior relies on our ability to measure and adequately quantify behavior. Historically, behavior has been quantified by a very small number of markers as the subject performs the task, for example, force sensors on levers. However, advancement in hardware and storage capabilities, as well as computational methods applied to video data, has allowed us to increase the quality and capability of behavioral recordings to videos of the entire subject that can be processed and analyzed quickly. It is now widely recognized that understanding the relationship between complex neural activity and high-dimensional behavior is a major step in understanding the brain that has been undervalued in the past [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>]. However, the analysis of high-dimensional behavioral video data across subjects is still a nascent field, due to the lack of adequate tools to efficiently disentangle behavioral features related to different subjects. Moreover, as recording modalities become light-weight and portable, neural and behavioral recordings can be performed in more naturalistic settings, which are difficult for behavioral analysis tools to disentangle due to changing scenes.</p>
<p>Although pose estimation tools that track various body parts in a behavioral video are very popular, they fail to capture smaller movements and rely on the labeler to judge which parts of the scene are important to track [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>]. Unsupervised techniques have gained traction to circumvent these problems. These include directly applying dimensionality reduction methods such as Principal Component Analysis (PCA) and Variational Autoencoders (VAEs) to video data [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>]. However, understanding or segmentation of the latent variables is difficult for any downstream tasks such as motif generation. To combine the best of both worlds, semi-supervised VAEs have been used for the joint estimation of tracked body parts and unsupervised latents that can effectively describe the entire image [<xref ref-type="bibr" rid="c2">2</xref>]. These have not been applied to across-subject data, with the exception of [<xref ref-type="bibr" rid="c10">10</xref>], where the authors directly use a frame of each subject’s video as a context frame to define individual differences; however, this method only works with a <italic>discrete</italic> set of <italic>labeled</italic> sessions or subjects. These methods fail when applied without labeled subject data, or more importantly, when analyzing freely-behaving social behavior, due to continuously shifting image distributions that confound the latent space.</p>
<p>With increasing capabilities to effectively record more naturalistic data in neuroscience, there is a growing demand for behavioral analysis methods that are tailored to these settings. In this work, we model a continuously varying distribution of images, such as in freely moving and multi-subject behavior, by using a novel loss term called the Cauchy-Schwarz Divergence (CSD) [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>]. By applying the CSD loss term, a subset of the latents can be automatically projected on a pre-defined and flexible distribution, thus leading to an unbiased approach towards latent separation. Here, the CSD is an effective variational regularizer that separates the latents corresponding to images with different appearances, thus successfully capturing ‘background’ information of an individual. This background information can be the difference in lighting during the experiment, the difference in appearance across mice in a multi-subject dataset, or the presence of another subject in the same field of view as in a social interaction dataset.</p>
<p>To further demonstrate the utility of our approach, we show that we can recover behavioral motifs from the resulting latents in a seamless manner. We recover (a) the same motifs across different animals performing the same task, and (b) motifs pertaining to social interactions in a freely moving task with two animals. Furthermore, we show the neural decoding of multiple animals in a unified model, with benefits towards the efficient decoding of the behavior of a novel subject. Finally, we compare the commonalities in neural activity across different trials in the same subject to those across subjects for different types of behavior motifs, e.g. task-related and spontaneous.</p>
<sec id="s1a">
<title>Related Works</title>
<p>Pose estimation tools such as DeepLabCut (DLC) and LEAP have been broadly applied to neuroscience experiments to track the body parts of animals performing different tasks, including in the social setting [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>]. These are typically supervised techniques that require extensive manual labeling. Although these methods can be sample-efficient due to the use of transfer learning methods, they still depend inherently on the quality of the manual labels, which can differ across labelers. Moreover, these methods may be missing key information in these behavioral videos that are not captured by tracking the body parts, for example, movements of the face, the whiskers, and smaller muscles that comprise a subject’s movements.</p>
<p>Emerging unsupervised methods have demonstrated significant potential in directly modeling behavioral videos. A pioneer in this endeavor was MoSeq, a behavioral video analysis tool that encodes high dimensional behavior by directly applying PCA to the data [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c9">9</xref>]. Behavenet is similar to MoSeq, but uses autoencoders to more effectively reduce the dimensionality of the representation [<xref ref-type="bibr" rid="c8">8</xref>]. However, the corresponding latent variables in these models are typically not interpretable. To add interpretability, the Partitioned Subspace VAE (PS-VAE) [<xref ref-type="bibr" rid="c2">2</xref>] formulates a semi-supervised approach that uses the labels generated using pose estimation methods such as DLC in order to partition the latent representation into both supervised and unsupervised subspaces. The ‘supervised’ latent subspace captures the parts that are labeled by pose estimation software, while the ‘unsupervised’ latent subspace encodes the parts of the image that have not been accounted for by the supervised space. While PS-VAE is very effective for a single subject, it does not address latent disentaglement in the ‘unsupervised’ latent space, and is not able to model multi-subject or social behavioral data.</p>
<p>Modeling multiple sessions has recently been examined in two approaches: MSPS-VAE and DBE [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c10">10</xref>]. Both of these are confined to modeling head-fixed animals with a pre-specified number of sessions or subjects. In MSPS-VAE, an extension to PS-VAE, a latent subspace is introduced in the model that encodes the static differences across sessions. In DBE, a context frame from each session or subject is used as a static input to generate the behavioral embeddings. Two notable requirements of applying both these methods is the presence of a discrete number of labeled sessions or subjects in the dataset. Therefore, these are not well suited for naturalistic settings where the session / subject identity might not be known a priori, or the scene might be continuously varying, for example, in the case of subjects roaming in an open-field.</p>
</sec>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>CS-VAE Model Structure</title>
<p>Although existing pose estimation methods are capable enough to capture the body position of the animals in both open and contained space, tracking specific actions such as shaking and wriggling still remains a problem. However, a purely unsupervised or semi-supervised model such as a VAE or PS-VAE lacks the ability to extract meaningful and interoperable behaviors from multi-subject or social behavioral videos. One possible solution is to add another set of latent which could capture the variance across individuals and during social interactions. Instead of constraining the data points from different sessions or subjects to distinct parts of the subspace as in [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c10">10</xref>], we directly constrain the latent subspace to a flexible prior distribution using a Cauchy-Schwarz regularizer as detailed in the Methods section. Ideally, this constrained subspace (CS) captures the difference between different animals in the case of a multi-subject task and the social interactions in a freely-behaving setting, while the supervised and unsupervised latents are free to capture the variables corresponding to the individual. The model structure described above is shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. After the input frames go through a series of convolutional layers, the resulting latent splits into three sets. The first set contains the supervised latents, which encodes the specific body position as tracked by supervised tracking methods such as DLC. The unsupervised latents capture the rest of the individual’s behavior that are not captured by supervised latents. The CS latents capture the continuous difference across frames. The prior distribution can be changed to fit different experimental settings (and can be modeled as a discretized state space if so desired, making it close to the MSPS-VAE discussed in the Introduction).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Overview of the Constrained Subspace Variational Autoencoder (CS-VAE). The latent space is divided in three parts: (1) the supervised latents decode the labeled body positions, (2) the unsupervised latents model the individual’s behavior that is not explained by the supervised latents, and (3) the constrained subspace latents model the continuously varying features of the image, e.g., relating to multi-subject or social behavior. After training the network, the generated latents can be applied to several downstream tasks. Here we show two example tasks: (1) Motif generation: we apply state space models such as hidden Markov models (HMM) and switched linear dynamical systems (SLDS), with the behavioral latent variables as the observations; (2) Neural decoding: with neural recordings such as widefield calcium imaging, corresponding behaviors can be efficiently predicted for novel subjects.</p></caption>
<graphic xlink:href="506091v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Modeling Smooth Variations in a Simulated Dataset</title>
<p>We performed a simulation study on the behavioral videos of one of the mice in the ‘Multi-Subject Behavior’ dataset detailed in <xref ref-type="sec" rid="s5">Appendix .1</xref>. We applied a continuously varying contrast ratio throughout the trials (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>) to model smoothly varying lighting differences across the dataset. We then randomly shuffled all the trials and trained a CS-VAE model with a swiss roll as a prior distribution. Here, the <italic>R</italic><sup>2</sup> for the supervised labels was 0.881 ± 0.05 (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>), and the mean squared error (MSE) for reconstructing the entire frame was 0.0067 ± 0.0003, showing that both the images and the labels were fit well.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>(A) Simulated dataset: behavioral videos from one mouse with artificially simulated differences in contrast. (B) Distribution occupied by the 3 CS latents.The constrained latents are distributed according to the pre-defined prior: a Swiss roll distribution. Different contrast ratios separate well in space. (C) Left: <italic>R</italic><sup>2</sup> values for label reconstruction; Right: visualization of label reconstruction for an example trial. Latent traversals for (D) CS latents, each of which captures lower, medium, and higher contrast rate. (E) An example supervised latent captures lever movement, and (F) an example unsupervised latent which captures jaw movement.</p></caption>
<graphic xlink:href="506091v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We show the CS latents recovered by the model in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, which follow the contrast ration distribution. We also show latent traversals in <xref rid="fig2" ref-type="fig">Fig. 2D-F</xref>, which demonstrate that the CS latent successfully captured the contrast changes in the frames (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>), the supervised latent successfully captured the corresponding labeled body part (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>), and the unsupervised latent captured parts of the individual’s body movement with a strong emphasis on the jaw (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>). Thus, we show that smoothly varying changes in the videos are well captured by our model.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Modeling Multi-Subject Behavior</title>
<p>In a multi-subject behavioral task, we would like to disentangle the commonalities in behavior from the differences across subjects. Here, we test the CS-VAE on an experimental dataset with four different mice performing a two-alternative forced choice task (2AFC): head-fixed mice performed a self-initiated visual discrimination task, while the behavior was recorded from two different views (face and body). The behavioral video includes the head-fixed mice as well as experimental equipment such as the levers and the spouts. We labeled the right paw, the spouts, and the levers using DLC [<xref ref-type="bibr" rid="c3">3</xref>]. Neural activity in the form of widefield calcium imaging across the entire mouse dorsal cortex was simultaneously recorded with the behavior. The recording and preprocessing details are in [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>], and the preprocessing steps for the neural data are detailed in [<xref ref-type="bibr" rid="c15">15</xref>].</p>
<sec id="s2c1">
<title>Reconstruction Accuracy</title>
<p>The CS-VAE model results in a mean label reconstruction accuracy <italic>R</italic><sup>2</sup> = 0.926 ± 0.02 (<xref rid="fig3" ref-type="fig">Fig. 3B,C</xref>), with the MSE for frame reconstruction as 0.00232 ± 7.7 · 10<sup><italic>−</italic>5</sup> (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). This was comparable to the results obtained using a PS-VAE model (<italic>R</italic><sup>2</sup> = 0.99 ± 0.004, MSE = 0.13 ± 4.5 · 10<sup><italic>−</italic>7</sup>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Modeling the behavior of four different mice. A. Image reconstruction result for an example frame from each mouse. B. Label reconstruction result for an example trial. C. <italic>R</italic><sup>2</sup> value for label reconstruction for all mice. D. (Left) CS latent and (Right) unsupervised latent distributions for all mice generated using our CS-VAE model. On the left, we see that the CS latent distribution follows the pre-defined prior distribution and is well separated; on the right, we see that the unsupervised latent distribution is well overlapped across mice. E. Unsupervised latent distribution for all mice generated using the comparison PS-VAE model, where the latents from different mice are separate from each other. F. SVM classification accuracy for classifying different mice using the CS-VAE and PS-VAE latents. The unsupervised latents generated by the CS-VAE has low classification accuracy, indicating across-subject representations, and the CS latents have a classification accuracy close to one, indicating good separation.</p></caption>
<graphic xlink:href="506091v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c2">
<title>Disentangled Latent Space Representation</title>
<p>We show latent traversals for each mouse in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, with the base image chosen separately for each mouse (videos in Supplementary Material 3). We see that, even for different mice, the supervised latent can successfully capture the corresponding labeled body part (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>). The example unsupervised latent is shown to capture parts of the jaw of each mouse (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>), and is well-localized, comparable with the example supervised latent. The CS latent dimension encodes many different parts of the image, and has a large effect on the appearance of the mouse, effectively changing the appearance from one mouse to another, signifying that it is useful in the case of modeling mouse-specific differences (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>). We demonstrate the abilities of the CS latent in capturing the appearance of the mouse by directly changing the CS latent from one part of subspace to another (<xref rid="fig4" ref-type="fig">Figure 4D</xref>). The changes in appearance along with the invariance in actions shows the intraoperability between mice by only changing the CS latents in this model (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Latent traversals for behavioral modeling of four different mice for A. an example supervised latent that captures the left spout across all the subjects, B. an example unsupervised latent that captures the chest of the mice, and C. an example CS latent that successfully captures the mouse appearance. D. Changing the value of the CS latent in an example frame leads to a change in subject, while keeping the same action as in the example frame.</p></caption>
<graphic xlink:href="506091v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Ideally, we would like to uncover common across-subject variables using the supervised and unsupervised latents subspaces, and have the individual differences across subjects be encoded in the CS latents. Thus, we expect the unsupervised latents to not be able to classify the individual well. In fact, <xref rid="fig3" ref-type="fig">Fig. 3D,F</xref> show that the unsupervised latents overlap well across the four mice and perform close to chance level (0.25) in a subject-classification task using SVM (details in Appendix <bold>??</bold>). This signifies that unsupervised latents occupy the same values across all four mice and thus effectively capture across-subject behavior. In fact, we tested our latent space by choosing the same base image across the four mice, and found that the supervised and unsupervised latents from different mice can be used interchangeably to change the actions in the videos, also showing interoperability between different mice in these latent subspaces (<xref ref-type="sec" rid="s13">Appendix .9</xref>).</p>
<p>This is in stark contrast to the CS latents, which are well separated across mice and are able to be classified well (<xref rid="fig3" ref-type="fig">Fig. 3D,F</xref>); thus, they effectively encode for individual differences across subjects. Note that our method did not <italic>a prior</italic> know the identity of the subjects, and thus this shows that the CS latents achieve separation in an unsupervised manner. We also note that the CS latents are distributed in the shape of the chosen prior distribution (a circle). The separation in the unsupervised latent space obtained by the baseline PS-VAE shown in <xref rid="fig3" ref-type="fig">Fig. 3E</xref> and the latents’ ability to classify different subjects (<xref rid="fig3" ref-type="fig">Fig. 3F</xref>) further validates the utility of CS-VAE.</p>
<p>Lastly, we trained the model while using prior distributions of different types, to understand the effect on the separability of the resulting latents. The separability was comparable across a number of different prior distributions, such as a swiss roll and a plane, signifying that the exact type of prior distribution does not play a large role.</p>
</sec>
<sec id="s2c3">
<title>Across-Subject Motif Generation</title>
<p>To further show that the supervised and unsupervised latents produced by CS-VAE are interoperable between the different mice, we apply a standard SLDS model (<xref ref-type="sec" rid="s10">Appendix .6</xref>) to uncover the motifs using this across-subject subspace. As seen in the ethograms (left) and the histograms (right) in <xref rid="fig5" ref-type="fig">Fig. 5</xref>, the SLDS using the CS-VAE latents captures common states across different subjects, indicating that the latents are well overlapped across mice. The supervised latents related to equipment in the experiment, here the spout and lever, split the videos into four states (different colors in the ethograms in <xref rid="fig5" ref-type="fig">Fig. 5A</xref>), that we could independently match with ground truth obtained from sensors in these equipment. The histograms show that, as expected, these states occur with a very similar frequency across mice. We also explored the behavioral states related to the right paw. The resulting three states captured the idle vs. slightly moving vs. dramatically moving paw (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). The histograms show that these states also occur with a very similar frequency across mice. Videos for all these states are available in Supplementary Material 2. The inference drawn from supervised latents is directly proportional to the DLC labels. Hence, a similar conclusion can be arrived at by utilizing the DLC pose estimations. Nonetheless, the subsequent outcomes cannot be attained solely based on the poses. We extracted the behavioral states related to the unsupervised latents, which yielded 3 states related to raising of the paws (including grooming) and jaw movements (including licking) that are present in all four mice, as shown in <xref rid="fig5" ref-type="fig">Fig. 5C</xref>. We see that different mice have different tendencies to lick and groom, e.g., mouse 1 and 4 seem to groom more often.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Motif generation for across-subject (supervised and unsupervised) behavioral latents using CS-VAE. SLDS results for CS-VAE latents: A. Supervised latents relating to equipment in the field of view. The equipment actions are similar for each trial. B. Supervised latents relating to tracked body parts. The ethograms for each trial across subjects and between subjects are very similar. The histogram indicates the number of frames occupied by each action per mouse. This further confirms the similarities between the supervised latents across subjects. C. Unsupervised latents also look similar across mice. Here, some example consecutive frames from the ‘raise pow’ motif are shown, which show the mouse grooming. D. As a comparison, SLDS results for the latents generated by a VAE, which failed to produce across-subject motifs.</p></caption>
<graphic xlink:href="506091v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As a baseline, we repeat this exercise on the latents of a single VAE trained to reconstruct the videos of all four mice (<xref rid="fig5" ref-type="fig">Fig. 5D</xref>). We see that the latents obtained by the VAE do not capture actions across subjects, and fail to cluster the same actions from different subjects into the same group.</p>
</sec>
<sec id="s2c4">
<title>Efficient Neural Decoding via Transfer Learning</title>
<p>To understand the relationship between neural activity and behavior, we decoded each behavioral latent with neural data across the dorsal cortex recorded using widefield calcium imaging. The decoding results for the supervised latents were similar across the CS-VAE and the PS-VAE, but we show that the neural data was also able to capture the CS-VAE unsupervised latents well (<xref ref-type="sec" rid="s14">Appendix .10</xref>).</p>
<p>Next, as a final test of interoperability of the individual latents across mice, we used a transfer learning approach. We first trained an LSTM decoding model on 3 of the 4 mice, and then tested that model on the 4<sup>th</sup> mouse while holding the LSTM weights constant but training a new dense layer leading to the LSTM (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>, details in <xref ref-type="sec" rid="s14">Appendix .10</xref>). As a baseline, we compared the performance of an individual LSTM model trained only on the 4<sup>th</sup> mouse’s data. We see in <xref rid="fig6" ref-type="fig">Fig. 6B</xref> that, as the training set of the 4<sup>th</sup> mouse becomes smaller, the transfer learning model outperforms the baseline with regards to both time and accuracy (more results and baseline comparisons in <xref ref-type="sec" rid="s14">Appendix .10</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p>A. Transfer learning model framework. Each of the four mice has a specific dense layer for aligning the neural activities. After the model is trained using three mice, the across-subject Recurrent Neural Network (RNN) layer is fixed and transferred to the fourth mouse. As a comparison, we trained a novel RNN model for the fourth mouse and compared the accuracy with the transfer learning model B. <italic>R</italic><sup>2</sup> and training time trade-off for individual vs. transfer learning model as the size of the training set decreases. As the training set decreases, the transfer learning has a better performance than the individually trained model with regards to both time and <italic>R</italic><sup>2</sup> accuracy.</p></caption>
<graphic xlink:href="506091v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c5">
<title>Neural Correlations across Mice during Spontaneous and Task-Related Behaviors</title>
<p>Here we explore the neural activity correlations while the subjects perform similar spontaneous behaviors vs. task-related behaviors. Across mice, we automatically identify spontaneous behaviors such as grooming and task-related behaviors such as lever pulls. We first separate the behavior from the same motif into small segments and kept the segments that have similar means and standard deviations within and across animals as shown in <xref rid="fig7" ref-type="fig">Fig. 7B</xref>. Next, we explore the commonalities between the neural activity of different mice as they perform these tasks by transforming the neural activity into a common subspace, using Multidimensional Canonical Correlation Analysis (MCCA). Here, we adopt the assumptions in Safaie et al. [<bold>?</bold>] that when the animals perform the same actions, the neural latent will share similar dynamics. We employ MCCA to align the high-dimensional neural activity across multiple subjects[<bold>?</bold>]. To do this, MCCA projects the datasets onto a canonical coordinate space that maximizes correlations between them (<xref rid="fig7" ref-type="fig">Fig. 7 C</xref>. method details in <xref ref-type="sec" rid="s15">Appendix .11</xref>). Finally, we compare the commonalities across different trials in the same subject to those across subjects for different types of behaviors. In <xref rid="fig7" ref-type="fig">Fig. 7D</xref>, we see that for the idle behavior, the neural correlation across mice is much lower than the correlation within the same mouse; however, this does not hold for the task-related behaviors such as lever pull and licking, or the spontaneous behaviors such as grooming. For the grooming behavior, the neural correlations within and across subjects are much higher than for the idle behaviors, and in fact, even higher than the task-related behaviors. This may be due to innate behaviors having common neural information pathways across mice, whereas learnt behaviors may display significant differences across mice. Considering the region-based differences in commonalities, the sensory areas such as the visual and the somatosensory areas are much more highly correlated across mice for all behaviors as compared to motor behaviors. This may be due to the similarities in sensory feedback due to these similar behaviors but is a topic of future exploration.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><p>A.The overall workflow for comparing the neural activities for different subjects performing similar spontaneous behaviors: First, the behavioral videos are encoded into behavior latents by CS-VAE. Then, the behavior latents would be clustered into different motifs. After that, similar behaviors are grouped based on their mean and standard deviation values. We can therefore obtain the corresponding neural activities. Finally, the neural activities from different subjects are aligned using the MCCA. B. Behavior latents are cut into small fragments. Similar behavior fragments are grouped together based on their mean and standard deviation values. The corresponding neural activities are obtained based on the grouping results of the behavior. C. Neural activities are being aligned using MCCA. MCCA aligns the neural activities from different subjects by mapping them into the same feature spaces. D. Correlation score for behavioral-based aligned neural activity. The grooming behavior has higher neural correlation scores for cross-subjects than other behaviors.</p></caption>
<graphic xlink:href="506091v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Modeling Freely-Moving Social Behavior</title>
<p>The dataset consists of a 16 minute video of two adult novel C57BL/6J mice, a female and a male, interacting in a clean cage. Prior to the recording session the mice were briefly socially isolated for 15 minutes to increase interaction time. As preprocessing, we aligned the frame to one mouse and cropped the video (schematic in <xref rid="fig8" ref-type="fig">Fig. 8A</xref>; details in the <xref ref-type="sec" rid="s6">Appendix .2</xref>). We tracked the nose position (<italic>x</italic> and <italic>y</italic> coordinates) of the mouse using DLC. Here, we did not include an unsupervised latent space, since the alignment and supervised labels resulted in the entire individual being explained well using the supervised latents.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8:</label>
<caption><p>A. Image alignment for the social behavior data. B. Model performance on the social behavior dataset. C. Visualization of the CS latents overlaid with the nose-to-tail distance between the two interacting mice. The CS latents separates the frames that contain social interactions from those that do not.</p></caption>
<graphic xlink:href="506091v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2d1">
<title>Reconstruction Accuracy</title>
<p>The CS-VAE model results in a mean label reconstruction accuracy 0.961 ± 0.0017 (<xref rid="fig8" ref-type="fig">Fig. 8B</xref>), with the MSE for frame reconstruction as 1.21 · 10<sup><italic>−</italic>5</sup> (<xref rid="fig8" ref-type="fig">Fig. 8B</xref>). We compared the performance of our model with the VAE and PS-VAE (<xref rid="tbl4" ref-type="table">Table 4</xref>), and the CS-VAE model performed better than the baseline models for both image and label reconstruction. For the VAE, we obtained the <italic>R</italic><sup>2</sup> for nose position prediction by training a multi-layer perceptron (MLP) with a single hidden layer from the VAE latents to the nose position.</p>
</sec>
<sec id="s2d2">
<title>Disentangled Latent Space Representation</title>
<p>We calculated the latent traversals for each latent as in <xref ref-type="sec" rid="s13">Appendix .9</xref>. As shown in the videos in Supplementary Material 4, CS latent 1 captures the second mouse to the front of the tracked mouse, CS latent 2 captures the front and above position of the second mouse, and CS latent 3 captures the position where the second mouse is below the tracked mouse.</p>
<p>To visualize the latent space and understand the relationship to social interactions, we plot the CS latents overlaid with the nose-to-tail distance between the two mice (nose of one mouse to the tail of the other) in <xref rid="fig8" ref-type="fig">Fig. 8C</xref>. We see that the CS latents represent the degree of social interaction very well, with a large separation between different social distances. Furthermore, we trained an MLP with a single hidden layer from different models’ latents to the nose-to-tail distance, and the CS-VAE produces the highest accuracy (<xref rid="tbl4" ref-type="table">Table 4</xref>).</p>
</sec>
<sec id="s2d3">
<title>Motif Generation</title>
<p>We applied a hidden Markov model (HMM) to the CS latents to uncover behavioral motifs. The three clusters cleanly divide the behaviors into social investigation vs. non-social behavior vs. non-social behavior with the aligned mice exploring the environment. To effectively visualize the changes in states, we show the ethogram in <xref rid="fig9" ref-type="fig">Fig. 9A</xref>. Videos related to these behavioral motifs are provided in Supplementary Material 5.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9:</label>
<caption><p>A. Ethogram for the animals’ behavior recovered using hidden Markov models (HMM) applied to the CS latents. B. Different metrics for analysing the behavioral motifs. Here, the three motifs are <italic>a</italic>. social interaction; <italic>b</italic>. non-social interaction with the companion on the upper side of the aligned mouse; <italic>c</italic>. non-social interaction (the aligned mouse exploring the environment with its companion far away). These metrics show the quantitative differences between the different motifs.</p></caption>
<graphic xlink:href="506091v2_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Lastly, we calculated different metrics to quantitatively evaluate the difference between each behavioral motif. The results are shown in <xref rid="fig9" ref-type="fig">Fig 9B</xref>, where we plot the average values for distances and angles between different key points. The lower distance between the two mice in <italic>State a</italic> demonstrates that the mice are close to each other in that state, pointing to social interactions. The smaller nose-to-tail distance for the aligned mouse in <italic>State c</italic> points to this state encoding for the ‘rearing’ of the mouse. The angle between the two mice further reveals the relative position between the two mice; in <italic>State b</italic>, the second mouse is located above the aligned mouse, while the opposite is true for <italic>State c</italic>. These metrics uncover the explicit differences between the different motifs that are discovered by CS-VAE.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>In the field of behavior modeling, there exist three major groups of methods, supervised, unsupervised, and semi-supervised. The supervised methods consist of methods such as DeepLabCut (DLC) [<xref ref-type="bibr" rid="c7">7</xref>], LEAP [<xref ref-type="bibr" rid="c6">6</xref>], AlphaTracker [<xref ref-type="bibr" rid="c5">5</xref>], amongst others. Although these methods capture the positions of the subjects, they lack the ability to model smaller movements and unlabeled behavior, and necessitate tedious labeling. On the other hand, unsupervised methods such as MoSeq [<xref ref-type="bibr" rid="c9">9</xref>] and Behavenet [<xref ref-type="bibr" rid="c8">8</xref>] lack the ability to produce intertpretable behavioral latents. While some semi-supervised methods, for instance, MSPS-VAE [<xref ref-type="bibr" rid="c2">2</xref>] and DBE [<xref ref-type="bibr" rid="c10">10</xref>], succeed in producing interpretable latents and modeling behavior across subjects, they need significant human input, and lack the ability to model freely-moving animals’ behavior. Here, we introduce a constrained generative network called CS-VAE that effectively addresses major challenges in behavioral modeling-disentangling multiple subjects and representing social behaviors.</p>
<p>For multi-subject behavioral modeling, the behavioral latents successfully separates the common activities across animals from the differences across animals. This behavioral generality is highlighted by the across-subject behavioral motifs generated by standard methods, and a higher accuracy while applying transfer learning for the neural decoding task. Furthermore, the SVM classification accuracy approaches 100%, which also indicates that the constrained-subspace latents well separate the differences between the subjects. In the social behavioral task, the constrained latents well capture the presence of social investigations, the environmental exploration, and the relative locations of the two individuals in the behavioral motifs. While our methods succeed in effectively modeling social behavior, it remains a challenge to separate out different kinds of social investigations in an unsupervised manner.</p>
<p>The constrained latents encode smoothly and discretely varying differences in behavioral videos. As seen in this work, in the across-subject scenario, the constrained latents encode the appearance of the different subjects, while in freely-moving scenario, the constrained latents capture social investigation between the subjects. The flexibility of this regularization thus gives it the ability to be fit in different conditions. Future directions include building an end-to-end structure that can captures behavioral motifs in a unsupervised way.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<title>Regularization of Constrained Subspace</title>
<p>We use the Cauchy-Schwarz divergence to regularize our constrained subspace using a chosen prior distribution. The Cauchy-Schwarz divergence <italic>D</italic><sub><italic>CS</italic></sub>(<italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>) between distributions <italic>p</italic><sub>1</sub>(<italic>x</italic>) and <italic>p</italic><sub>2</sub>(<italic>x</italic>) is given by:
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="506091v2_eqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<italic>D</italic><sub><italic>CS</italic></sub>(<italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>) equals zero if and only if the two distributions <italic>p</italic><sub>1</sub>(<italic>x</italic>) and <italic>p</italic><sub>2</sub>(<italic>x</italic>) are the same. By applying the Parzen window estimation technique to <italic>p</italic><sub>1</sub>(<italic>x</italic>) and <italic>p</italic><sub>2</sub>(<italic>x</italic>), we get the entropy form of the Equation [<xref ref-type="bibr" rid="c11">11</xref>]:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="506091v2_eqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="506091v2_eqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <italic>p</italic><sub>1<italic>i</italic></sub> represents the <italic>i</italic>th sample from the distribution <italic>p</italic><sub>1</sub>, i.e., <italic>p</italic><sub>1</sub>(<italic>x</italic><sub><italic>i</italic></sub>). − log(<italic>V</italic> (<italic>p</italic><sub>1</sub>)) and − log(<italic>V</italic> (<italic>p</italic><sub>2</sub>)) are the estimated quadratic entropy of <italic>p</italic><sub>1</sub>(<italic>x</italic>) and <italic>p</italic><sub>2</sub>(<italic>x</italic>), respectively, while − log(<italic>V</italic> (<italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>)) is the estimated cross-entropy of <italic>p</italic><sub>1</sub>(<italic>x</italic>) and <italic>p</italic><sub>2</sub>(<italic>x</italic>). <italic>G</italic> is the kernel applied to the input distribution; here it is chosen to be Gaussian. <italic>N, N</italic><sub>1</sub>, and <italic>N</italic><sub>2</sub> are the number of samples being input into the model while <italic>σ</italic> is the kernel size. The choice of the kernel size depends on the dataset itself; generally, the kernel size should be greater than the number of the groups in the data. <xref ref-type="disp-formula" rid="eqn1">Equation (1)</xref> can be expressed as:
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="506091v2_eqn4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <italic>p</italic><sub>1</sub>(<italic>x</italic>) represents the distribution of our CS latent space, and <italic>p</italic><sub>2</sub>(<italic>x</italic>) the chosen prior distribution. In <xref ref-type="disp-formula" rid="eqn4">Equation (4)</xref>, minimizing <italic>V</italic> (<italic>p</italic><sub>1</sub>) would result in the spreading out of <italic>p</italic><sub>1</sub>(<italic>x</italic>), while maximizing <italic>V</italic> (<italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>) would make the samples in both distributions closer together [<xref ref-type="bibr" rid="c11">11</xref>]. Thus, we minimize this term in the objective function while training the model. However, it may be necessary to stop at an appropriate value, since overly spreading out <italic>p</italic><sub>1</sub>(<italic>x</italic>) may lead to the separation of the samples from the same groups, while making <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub> excessively close may cause mixtures of data points across groups.</p>
<p>In short, the Cauchy-Schwarz divergence measures the distance between <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub>. In our work, we adopt a variety of distributions as a prior distribution <italic>p</italic><sub>2</sub>(<italic>x</italic>), and we aim to project the constrained subspace latents onto the prior distribution (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>).</p>
</sec>
<sec id="s4b">
<title>Optimization</title>
<p>The loss for the CS-VAE derives from that for the PS-VAE, and is given by:
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="506091v2_eqn5.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, the terms ℒ<sub><italic>frames</italic></sub> and ℒ<sub><italic>label</italic></sub> represent the reconstruction loss of the frames and the labels, respectively. The ℒ<sub><italic>KL−s</italic></sub> represents the KL-divergence loss for the supervised latents while ℒ<sub><italic>ICMI</italic></sub>, ℒ<sub><italic>T C</italic></sub>, and ℒ<sub><italic>DW KL</italic></sub> form the decomposed version of the KL loss for the unsupervised latents. Lastly, the ℒ<sub><italic>CS</italic></sub> represents the CS-divergence loss on our constrained latents. <italic>α</italic> is introduced to control the reconstruction quality of the labels, <italic>β</italic> is adopted to assist the model in producing independent unsupervised latents, and <italic>γ</italic> is implemented to control the variability in the constrained latent space for better separation. The detailed explanations and derivations for each term in the objective function are in <xref ref-type="sec" rid="s7">Appendix .3</xref>. Furthermore, the loss terms in <xref ref-type="disp-formula" rid="eqn5">Equation (5)</xref> can be modified to fit various conditions. For a freely-behaving social task, the background for one individual in the container could be the edge of the container as well as the rest of the individuals in the container. The choice of hyperparameters and the loss curves through the training process is shown in <xref ref-type="sec" rid="s9">Appendix .5</xref> and <xref ref-type="sec" rid="s11">.7</xref>, respectively.</p>
</sec>
<sec id="s4c">
<title>Visualization of the latent space</title>
<p>To test how the image varies with a change in the latent, one frame from the trials is randomly chosen as the ‘base image’, and the effect of varying a specific latent at a time is visualized and quantified. This is known as the ‘latent traversal’ [<xref ref-type="bibr" rid="c2">2</xref>]. First, for each latent variable, we find out the maximum value that it occupies across a set of randomly selected trials. We then change that specific latent to achieve its maximum value, and this new set of latents forms the input to the decoder. We obtain the corresponding output from the decoder as the ‘latent traversal’ image. Finally, we visualize the difference between the ‘latent traversal’ image and the base image. The above steps are performed for each latent individually. In videos containing latent traversals (Supplementary Material), we change the latent’s value from its minimum to its maximum across all trials, and input all the corresponding set of latents into the decoder to produce a video.</p>
</sec>
<sec id="s4d">
<title>Behavioral Motif Generation</title>
<p>Clustering methods such as Hidden Markov Models (HMM) and switching linear dynamical systems (SLDS) have been applied in the past to split complex behavioral data into simpler discrete segments [<xref ref-type="bibr" rid="c16">16</xref>] (see <xref ref-type="sec" rid="s10">Appendix .6</xref> for details). We use these approaches to analyze motifs from our latent space, and directly input the latent variables into these models. In the case of multi-subject datasets, our goal is to capture the variance in behavior in a common way in the across-subject latents, i.e., recover the same behavioral motifs in subjects performing the same task. In the case of freely-moving behavior, our goal is to capture motifs related to social behavior.</p>
</sec>
<sec id="s4e">
<title>Efficient Neural Decoding</title>
<p>Decoding neural activity to predict behavior is very useful in the understanding of brain-behavior relationships, as well as in brain-machine interface tasks. However, models to predict high-dimensional behavior using large-scale neural activity can be computationally expensive, and require a large amount of data to fit. In a task with multiple subjects, we can utilize the similarities in brain-behavior relationships to efficiently train models on novel subjects using concepts in transfer learning. Here, we represent across-subject behavior in a unified manner and train an across-subject neural decoder. Armed with this across-subject decoder, we show the decoding power on a novel subject with varying amounts of available data, such that it can be used in a low-data regime. The implementational details for this transfer learning approach can be found in <xref ref-type="sec" rid="s14">Appendix .10</xref>.</p>
</sec>
<sec id="s4f">
<title>Behavior election for innate behaviors studying</title>
<p>While the behavioral features extracted from the previous sections are successful in capturing similar spontaneous behaviors across various animals, the behavioral patterns within the same motifs can exhibit substantial variation. For instance, in the case of the raising paw motif, continuous movement of the paws could be indicative of either grooming or other complex behaviors. To overcome this challenge, we divided the behaviors belonging to the same motif into smaller segments and calculated the corresponding mean and standard deviation of the behavioral latents. Subsequently, we compared these values and retained the segments that exhibited similar mean and standard deviations both within and across animals, as illustrated in <xref rid="fig7" ref-type="fig">Fig. 7B</xref>. These steps were repeated for all the behavioral motifs examined in this study.</p>
<p>In addition to the spontaneous behaviors discussed above, we also selected an ‘idle’ behavior that captured the mouse’s inactivity and a task-related behavior, namely the ‘lever pull’ behavior, which signaled the initiation of each task.</p>
</sec>
</sec>
<sec id="d1e1192" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1278">
<label>Supplementary Info</label>
<media xlink:href="supplements/506091_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Ghazanfar</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Gomez-Marin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>MacIver</surname>, <given-names>M. A.</given-names></string-name> <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <article-title>Neuroscience needs behavior: Correcting a reductionist bias</article-title>. <source>Neuron</source> <volume>93</volume>, <fpage>480</fpage>–<lpage>490</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="other"><string-name><surname>Whiteway</surname>, <given-names>M. R.</given-names></string-name> <etal>et al.</etal> <article-title>Partitioning variability in animal behavioral videos using semi-supervised variational autoencoders</article-title>. <source>bioRxiv</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>Deeplabcut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature Neuroscience</source> <volume>21</volume>, <fpage>1281</fpage>–<lpage>1289</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="other"><string-name><surname>Pereira</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> <article-title>Fast animal pose estimation using deep neural networks</article-title>. <source>bioRxiv</source> (<year>2018</year>).</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="other"><string-name><surname>Chen</surname>, <given-names>Z.</given-names></string-name> <etal>et al.</etal> <article-title>Alphatracker: A multi-animal tracking and behavioral analysis tool</article-title>. <source>bioRxiv</source> (<year>2020</year>).</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="other"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name> <etal>et al.</etal> <article-title>Publisher correction: Sleap: A deep learning system for multi-animal pose tracking</article-title>. <source>Nat Methods</source> (<year>2022</year>).</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="other"><string-name><surname>Lauer</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Multi-animal pose estimation and tracking with deeplabcut</article-title>. <source>bioRxiv</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="book"><string-name><surname>Batty</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal>/person-group&gt;. <chapter-title>Behavenet: nonlinear embedding and bayesian neural decoding of behavioral videos</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Wallach</surname>, <given-names>H.</given-names></string-name></person-group> et al. (eds.) <source>Advances in Neural Information Processing Systems</source>, vol. <volume>32</volume> (<publisher-name>Curran Associates, Inc</publisher-name>., <year>2019</year>).</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><surname>Wiltschko</surname>, <given-names>A. B.</given-names></string-name> <etal>et al.</etal> <article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title>. <source>Nature neuroscience</source> <volume>23</volume>, <fpage>1433</fpage> –<lpage>1443</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="other"><string-name><surname>Shi</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> <article-title>Learning disentangled behavior embeddings</article-title>. <source>In NeurIPS</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="other"><string-name><surname>Santana</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Emigh</surname>, <given-names>M.</given-names></string-name> <string-name><surname>Principe</surname>, <given-names>J.</given-names></string-name> <source>Information theoretic-learning auto-encoder</source> (<year>2016</year>).</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="other"><string-name><surname>Tran</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Pantic</surname>, <given-names>M.</given-names></string-name> <string-name><surname>Deisenroth</surname>, <given-names>M. P.</given-names></string-name> <source>Cauchy-schwarz regularized autoencoder</source> (<year>2021</year>). 2101.02149.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><surname>Wiltschko</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>Mapping sub-second structure in mouse behavior</article-title>. <source>Neuron</source> <volume>88</volume>, <fpage>1121</fpage>–<lpage>1135</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><surname>Musall</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Juavinett</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Gluf</surname>, <given-names>S.</given-names></string-name> <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name> <article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title>. <source>Nature neuroscience</source> <volume>22</volume>, <fpage>1677</fpage> –<lpage>1686</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><surname>Saxena</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Localized semi-nonnegative matrix factorization (locanmf) of widefield calcium imaging data</article-title>. <source>PLOS Computational Biology</source> <volume>16</volume>, <fpage>1</fpage>–<lpage>28</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="book"><string-name><surname>Linderman</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal>/person-group&gt;. <chapter-title>Bayesian Learning and Inference in Re-current Switching Linear Dynamical Systems</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Singh</surname>, <given-names>A.</given-names></string-name> <string-name><surname>Zhu</surname>, <given-names>J.</given-names></string-name></person-group> (eds.) <source>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</source>, vol. <volume>54</volume> <collab>of Proceedings of Machine Learning Research</collab>, <fpage>914</fpage>–<lpage>922</lpage> (<publisher-name>PMLR</publisher-name>, <year>2017</year>).</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<label>5</label>
<title>Appendix</title>
<sec id="s5">
<label>.1</label>
<title>Experimental Methods and Preprocessing for the Multi-Subject Dataset</title>
<p>In our work, we employed a subset of the behavioral dataset detailed in Musall et al., 2019 [<xref ref-type="bibr" rid="c14">14</xref>]. Briefly, the task entailed pressing a lever to initiate the task, after which a visual stimulus was displayed towards the left or the right. After a delay period, the spouts come forward, at which time the mouse makes its decision by licking the spout corresponding to the direction of the visual stimulus (left or right). Finally, the mice receive a juice reward if they choose correctly.</p>
<p>We tested the CS-VAE on the behavioral data for the four mice performing a visual task and randomly chose 388 trials per mouse each of the trials has a 189 number of frames. Each frame was pre-processed and resized to have both the length and width being 128. One example trial for each mouse can be found in Supplementary Material 1.</p>
<p>Before inputting the data into the model, we sorted the trials by the amount of variance in the images, and shuffled the first half (high variance) and the second half (low variance) of the dataset separately. This was done to speed up training by training the model on high-variance trials first. We tested our model by randomly choosing 4 trials from all trials for each mouse 5 times. The same procedure was applied when training the model on the simulation dataset, i.e., the doctored data for one subject.</p>
</sec>
<sec id="s6">
<label>.2</label>
<title>Experimental Methods and Preprocessing for the Freely-Moving Social Behavior Dataset</title>
<p>The dataset consists of a 16-minute video of two adult novel C57BL/6J mice, a female and a male, interacting in a clean cage. Prior to the recording session, the mice were briefly socially isolated for 15 minutes to increase interaction time. This dataset was collected by one of the authors. The original data has 24917 number of frames with length and width being 1920 and 1080, respectively. The example fraction of the video can be found in Supplementary Material 6.</p>
<p>The nose, ears, and tail base of each mouse were manually annotated using AlphaTracker. We kept 19659 number of frames that have the labels for preprocessing and training. We perform several preprocessing steps to align and crop the video as well as the labels based on one of the two mice (Mouse 1, female). All of the preprocessing steps were based on the AlphaTracker labels. For each frame, we first rotate it to ensure that the nose and tailbase for Mouse 1 are on the same horizontal line, with the central point for rotation as the left ear. Next, we aligned the frame such that the left ear of Mouse 1 was at the same location across all frames. Finally, we resize the frame to be 128 × 128 and consequently the AlphaTracker labels. For this dataset, since there was a relatively low number of frames, we obtained the CS-VAE MSE and label <italic>R</italic><sup>2</sup> for the entire dataset.</p>
</sec>
<sec id="s7">
<label>.3</label>
<title>Methodological details of the Partitioned Subspace VAE</title>
<p>The Partitioned Subspace VAE (PS-VAE) was introduced in [<xref ref-type="bibr" rid="c2">2</xref>], and we borrow the notation used in that paper when detailing the CS-VAE. Thus, we include here a full description of the model.</p>
<p>First of all, we define the input frame as <italic>x</italic>, and the corresponding pose estimation tracking label as <italic>y</italic>. The reconstructed variables are termed <inline-formula><alternatives><inline-graphic xlink:href="506091v2_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <italic>ŷ</italic>, respectively. The supervised latent space is denoted as <italic>z</italic><sub><italic>s</italic></sub>, unsupervised latent as <italic>z</italic><sub><italic>u</italic></sub>, and the background latent as <italic>z</italic><sub><italic>b</italic></sub>. In a VAE model, we would like to minimize the distance, typically the KL divergence, between the posterior distribution of the latent variables <italic>p</italic>(<italic>z</italic>|<italic>x</italic>) and a chosen distribution <italic>q</italic>(<italic>z</italic>|<italic>x</italic>). However, since <italic>p</italic>(<italic>z</italic>|<italic>x</italic>) is an unknown distribution, the Evidence Lower Bound (ELBO) is introduced as an alternative method to reduce the KL divergence:
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="506091v2_eqn6.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Following [<xref ref-type="bibr" rid="c2">2</xref>], if we have a finite dataset <inline-formula><alternatives><inline-graphic xlink:href="506091v2_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, and we treat <italic>n</italic> as a random variable with a uniform distribution <italic>p</italic>(<italic>n</italic>) while defining <italic>q</italic>(<italic>z</italic>|<italic>n</italic>) := <italic>q</italic>(<italic>z</italic><sub><italic>n</italic></sub>|<italic>x</italic><sub><italic>n</italic></sub>), we can rewrite the ELBO as:
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="506091v2_eqn7.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We define the loss over frames ℒ<sub><italic>frames</italic></sub> as the first of the two terms above. In the PS-VAE model, there are two inputs: frames <italic>x</italic> and labels <italic>y</italic>. Therefore, in <xref ref-type="disp-formula" rid="eqn7">Equation (7)</xref>, instead of writing the input likelihood as <italic>p</italic>(<italic>x</italic>|<italic>z</italic>), we can now write it as <italic>p</italic>(<italic>x, y</italic>|<italic>z</italic>). A simplifying assumption is made that <italic>x</italic> and <italic>y</italic> are conditionally independent given <italic>z</italic>, and thus we can directly write ℒ<sub><italic>frames</italic>+<italic>labels</italic></sub> as ℒ<sub><italic>frames</italic></sub> + ℒ<sub><italic>labels</italic></sub>, where ℒ<sub><italic>labels</italic></sub> is calculated by replacing <italic>x</italic> with <italic>y</italic> in ℒ<sub><italic>frames</italic></sub>.</p>
<p>After assuming the prior <italic>p</italic>(<italic>z</italic>) has a factorized form: <italic>p</italic>(<italic>z</italic>) = Π<sub><italic>i</italic></sub> <italic>p</italic>(<italic>z</italic><sub><italic>i</italic></sub>), the KL term ℒ<sub><italic>KL</italic></sub> can be split as the addition of <italic>ℓ</italic><sub><italic>KL−s</italic></sub> and <italic>ℓ</italic><sub><italic>KL−u</italic></sub>, i.e., the KL terms for the supervised and unsupervised latents, respectively. We decompose the KL term for the unsupervised latent as the following [<xref ref-type="bibr" rid="c2">2</xref>].
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="506091v2_eqn8.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>j</italic> represents the latent dimension, ℒ<sub><italic>ICMI</italic></sub> is the index-code mutual information, which measures how well the latent encodes the corresponding input data. The term TC is short for total correlation, which measures the interdependency of each latent dimension. The third term, ℒ<sub><italic>DW KL</italic></sub> is the dimension-wise KL, which calculates the KL divergence for each dimension individually. Finally, the resulting subspace is forced to be orthogonal by applying orthogonal weights across all the different latents.</p>
<p>The authors in [<xref ref-type="bibr" rid="c2">2</xref>] introduce an extension to PS-VAE for modeling multi-session data. The Multi-Session PS-VAE (MS-PS-VAE) can only work with a labeled set of discrete sessions, as described in the Introduction. The images from each session are labeled, and the session-specific latents are enforced to be static over time, thus capturing the image-related details. To enforce the background latents to be static over time in a particular session, and to maximize the difference in the background latents across different sessions, the triplet loss is introduced in MS-PS-VAE. As described in the Introduction, this loss term artificially places the latents from the same session together while separating the latents from different sessions. The triplet loss is computed as the following.
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="506091v2_eqn9.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <italic>a</italic> is the anchor point, <italic>p</italic> is the positive point, <italic>n</italic> is the negative point, and <italic>m</italic> is a margin. The function pulls the point <italic>p</italic> towards point <italic>a</italic>, and pushes the point <italic>n</italic> away from point <italic>a</italic>. While training, the data from multiple sessions is included in each mini-batch. The data from each session is split in three, and each third from the same session acts as an anchor and positive point, while the data from another session acts as a negative point. Practically, this requires as many sessions as possible in the same mini-batch during the training for accurate results. As the number of sessions increases, this method becomes computationally intractable, and may lead to unsatisfactory reconstruction results. Moreover, this loss does not allow for varying backgrounds across any one session.</p>
<p>In the MS-PS-VAE model, the triplet loss was applied as a supervised manner to pull the data from the same subject being closer while pushing the different subjects away from each other. This method is only useful when the number of sessions is known, and is not applicable in an open-field setting, for example while modeling freely-moving social behavior as in this manuscript.</p>
<p>Therefore, in this manuscript, we introduce a regularization term that can automatically separate different subjects in the background latent space without specifying the number of sessions or labeling each frame as belonging to a specific session.</p>
</sec>
<sec id="s8">
<label>.4</label>
<title>Model Architecture and Training</title>
<p>Our computational experiments were carried out using TensorFlow and Keras. The image decoder we use is symmetric to the encoder, with both of them containing 14 convolution layers. We applied the Adam optimizer with learning rate as 10<sup><italic>−</italic>4</sup>. For the multi-subject dataset, we fixed our batch size to be 256 and trained for 50 epochs. For the freely-moving social behavior dataset, we trained for 500 epochs with batch size 128.</p>
</sec>
<sec id="s9">
<label>.5</label>
<title>Choice of Hyperparameters</title>
<p>In the multi-subject dataset, four coefficients need to be decided for the objective function as indicated above: {<italic>α, β, σ, γ</italic>}. There is a balance between the choice of <italic>β</italic> and <italic>γ</italic>: properly choosing the values could separate the latent in the unsupervised space and the latents in both unsupervised and background space as well. A large separation of the background latent may potentially lead to unsatisfactory reconstruction results. The choice of kernel size <italic>σ</italic> depends on the dataset, and should be larger than the number of distinct groups in our dataset; since in our current experiments, we have at most four groups, we set <italic>σ</italic> = 15. Moreover, we set <italic>α</italic> to 1000, <italic>β</italic> to 5, <italic>γ</italic> to 500. We set the dimensionality of the supervised latent space equal to the number of tracked video parts, which is 5 in our case. We set the dimensionality of the unsupervised latent space as 2, while that of the background latent space as 2.</p>
<p>In the social behavior task, we track the nose location as the supervised latent, since the other labels do not have a high variance (due to the alignment process). Additionally, we do not need any unsupervised latents to explain the individual’s behavior. The CS latent in this setting has 3 dimensions. Here, <italic>α</italic> is 1200, <italic>γ</italic> is 200, and the kernel size is 20.</p>
<p>The hyperparameters chosen for all three datasets are shown in <xref rid="tbl2" ref-type="table">Tables 2</xref> and <xref rid="tbl3" ref-type="table">3</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Comparison of different models on the freely-moving social behavior dataset</p></caption>
<graphic xlink:href="506091v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Hyperparameter for different dataset</p></caption>
<graphic xlink:href="506091v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>Latent dimensions and the prior distribution for different dataset</p></caption>
<graphic xlink:href="506091v2_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4:</label>
<caption><p>Training size vs <italic>R</italic><sup>2</sup> value for multi-subject dataset</p></caption>
<graphic xlink:href="506091v2_tbl4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s10">
<label>.6</label>
<title>Motif Generation</title>
<p>A switching linear dynamical system (SLDS) consists of discrete latent state <italic>z</italic><sub><italic>t</italic></sub> ∈ {1, 2, ..<italic>K</italic>}, continuous latent state <italic>x</italic><sub><italic>t</italic></sub> ∈ R<sup><italic>M</italic></sup>, and the observation state <italic>y</italic><sub><italic>t</italic></sub> ∈ R<sup><italic>N</italic></sup>. Here, <italic>t</italic> = 1, 2, 3, .., <italic>T</italic> is the time step, <italic>T</italic> is the length of the input signal; <italic>K</italic> is the number of discrete states; <italic>M</italic> is the number of latent dimensions; <italic>N</italic> is the observation dimensions. The discrete latent state <italic>z</italic><sub><italic>t</italic></sub> follows the Markovian dynamics with the state transition matrix expressed as:
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="506091v2_eqn10.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The continuous latent state <italic>x</italic><sub><italic>t</italic></sub> has the following linear dynamical relations that determined by <italic>z</italic><sub><italic>t</italic></sub>.
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="506091v2_eqn11.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <inline-formula><alternatives><inline-graphic xlink:href="506091v2_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the dynamic matrix at state <italic>z</italic><sub><italic>t</italic>+1</sub>; <italic>u</italic><sub><italic>t</italic></sub> is the input at time t, with <inline-formula><alternatives><inline-graphic xlink:href="506091v2_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> being the control matrix; <inline-formula><alternatives><inline-graphic xlink:href="506091v2_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the offset vector and <italic>w</italic><sub><italic>t</italic></sub> being the noise which is generally the zero mean Gaussian. Here, our observation model is in Gaussian case; therefore, the observation <italic>y</italic><sub><italic>t</italic></sub> is expressed as:
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="506091v2_eqn12.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <inline-formula><alternatives><inline-graphic xlink:href="506091v2_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the measurement matrix at state <italic>z</italic><sub><italic>t</italic></sub>; <inline-formula><alternatives><inline-graphic xlink:href="506091v2_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the feedthrough matrix which directly feed the input into the observation; <inline-formula><alternatives><inline-graphic xlink:href="506091v2_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the offset vector and <italic>v</italic><sub><italic>t</italic></sub> is the noise. Here the update was accomplished by the Expectation-Maximization(EM) algorithm. In the E-step, the model updates the hyperparameters. In the M-step, the log-likelihood in Eq.12 is being maximized.</p>
<p>To implement the SLDS, we adopted the open source software from Linderman et al.[<xref ref-type="bibr" rid="c16">16</xref>]. We fit the SLDS using different latent dimensions, where the observation dimension was the order of latent dimension and the number of states was determined by visualizing the videos. We use SLDS’s to model the motifs in the multi-subject dataset since the behaviors are well separated using their dynamics. We use K-means to model the motifs in the freely-moving social behavior dataset since the behaviors are well separated directly in state space. An autoregressive HMM (a simpler model than an SLDS) applied to the CS latents in the social behavior dataset leads to similar results as the K-means.</p>
</sec>
<sec id="s11">
<label>.7</label>
<title>Loss Curves</title>
<p>We show the learning curve for each loss term for both dataset to precisely quantify the model, in <xref rid="fig10" ref-type="fig">Fig. 10</xref>. For the multi-sujbect dataset (<xref rid="fig10" ref-type="fig">Fig. 10A</xref>), for the unsupervised latents, the final loss for dimension-wise KL, total correlation, and the mutual information are 11.7, −4.8, and −4.6, respectively. The final KL loss for the supervised latents is 5.06 and the final CSD loss for the CS latents is 0.1. For the free behaving dataset, the loss curves for each loss term are shown in <xref rid="fig10" ref-type="fig">Fig. 10B</xref>. By the end of the training process, the KL loss for the supervised latents is 7.01 and the CSD loss for the CS latents is 1.15.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10:</label>
<caption><p>Loss curve for A. training the multi-subject dataset B. training the freely behaving dataset with the specified hyperparameters as in <xref rid="tbl1" ref-type="table">Tables 1</xref> and <xref rid="tbl2" ref-type="table">2</xref>.</p></caption>
<graphic xlink:href="506091v2_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s12">
<label>.8</label>
<title>SVM</title>
<p>To further quantify the separation of the latents between different subjects, we applied a supervised classification method to decode the identity of the subject using each latent.</p>
<p>After randomly shuffling all the latents, we split all the trials into training trials and test trials, with each mouse having 368 trials in the training set and 20 trials in the test set, and repeated this 5 times with different random seeds.</p>
</sec>
<sec id="s13">
<label>9</label>
<title>Latent traversal</title>
<p>For the multi-subject dataset, we tested the latent traversal with the same base image to validate the results, shown in <xref rid="fig11" ref-type="fig">Figure 11</xref>. Here, we randomly chose a frame from a mouse and changed each individual latent within different ranges as detailed in the Methods. For example, in <xref rid="fig11" ref-type="fig">Figure 11</xref>, the first row contains the output when the corresponding latent is changed to take on the maximum value from the range of Mouse 1. Similar to the figures in the main text, the upper images are the latent traversal images while the lower ones are the difference between the upper and original images. We see that the base image from Mouse 3 can be flexibly changed to produce a different mouse when changing the CS latent. Moreover, when changing the supervised and unsupervised latents for the different mice, Mouse 3 seems to be flexibly changing with these latents from different mice.</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11:</label>
<caption><p>Latent traversals for the multi-subject dataset for the four mice with the same base image A. an example supervised latent, B. an example unsupervised latent, and C. an example CS latent. We see that the same base image (Mouse 3) is transformed into a different mouse each time when changing the CS latent.</p></caption>
<graphic xlink:href="506091v2_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To better visualize the specialization of each latent, we generated the latent traversal videos for each latent with different base images. For different mice, we, first of all, find the maximum and the minimum value for the specific latent. Then, change the latent within that range with 0.5 per step. Finally, concatenate all the latent traversal images into videos. The videos can be found in Supplementary Material 3.</p>
<p>We performed a similar visualization on the freely-moving social behavior dataset for the CS latents. The latent traversal videos can be found in Supplementary Material 4, and some clips from the videos are shown in <xref rid="fig12" ref-type="fig">Fig 12</xref>.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure 12:</label>
<caption><p>Latent traversals on the CS-latents for the freely-moving social behavior dataset. We see that the latents all encode for social interactions between the two mice.</p></caption>
<graphic xlink:href="506091v2_fig12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig13" position="float" fig-type="figure">
<label>Figure 13:</label>
<caption><p>Neural decoding for CS-VAE vs. PS-VAE.</p></caption>
<graphic xlink:href="506091v2_fig13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s14">
<label>.10</label>
<title>Neural decoding models</title>
<p>The trials were first shuffled and then split into training and testing. Next, we employed the CS-VAE generated latent representations, and choose one example subject to decode the behavior at time <italic>t</italic> using the neural activity recorded between <italic>t</italic> − 0.15s and <italic>t</italic>. We applied four types of models to compare the performance. A linear model which directly maps the neural activities into the behavior. A multilayer perceptron (MLP) with three dense layers to train the decoder. We used the Adam optimizer with learning rate decay from 0.1 with 0.3 decay rate for every 5 step. The batch size was fixed to be 150 and trained for 200 epochs. A LSTM model, which begin with a dense layer followed by a LSTM layer with a drop-out rate being 0.5 and another dense layer at the end. We applied the same training strategy as in MLP model.</p>
<p>We introduced a model based on transfer learning to perform the decoding test on the previously tested subject. The rest of the three mice were the input to the original training model. The procedures were similar to before, after the trials were shuffled and split, we decoded the behavior directly with the raw neural activities with the time window being 0.15<italic>s</italic>. After that, we implemented three perceptron layers for each of the three mice before the output of which went into a recurrent neural network (RNN). The RNN consisted of one long short-term memory (LSTM) layer with a unit number of 64 and a drop-out layer with a rate being 0.5. We applied the Adam optimizer with learning rate decay from 0.1 with 0.3 decay rate for every 5 step. The batch size was 150 and we trained for 200 epochs. After we finished training the original network, we transferred the RNN model to the new model which was applied to train the fourth mouse alone. For the fourth mouse, the trials were split with different training and testing ratios. After applying the same steps to the data, the neural activities then went through a new perceptron layer before going through the pre-trained RNN model. We applied the Adam optimizer with the same learning rate decay procedures as well. We again, trained for 200 epochs with batch size being 128 this time. The trade-off between accuracy and time for different models can be found in <xref rid="tbl4" ref-type="table">Tables 4</xref> and <xref rid="tbl5" ref-type="table">5</xref>.</p>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5:</label>
<caption><p>Training size vs time usage for multi-subject dataset</p></caption>
<graphic xlink:href="506091v2_tbl5.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s15">
<label>.11</label>
<title>Multidimensional Canonical Correlation Analysis (MCCA) for neural signal alignment</title>
<p>In our work, after extracting similar behaviors chunks from different individuals, we then extracted the corresponding neural activity for each subject. To smooth away the discreteness of the neural activity chunks, we shuffled the chunks before concatenating them together. After that, we performed the MCCA for all four subjects on each brain region. For each brain region, we choose the four sets of neural activities being the same length <italic>d, X</italic>1 = {<italic>x</italic>1<sub>1</sub>, <italic>x</italic>1<sub>2</sub>, …, <italic>x</italic>1<sub><italic>n</italic></sub>} <italic>R</italic><sup><italic>n×d</italic></sup>, <italic>X</italic>2 = {<italic>x</italic>2<sub>1</sub>, <italic>x</italic>2<sub>2</sub>, …, <italic>x</italic>2<sub><italic>n</italic></sub>} <italic>R</italic><sup><italic>m×d</italic></sup>, <italic>X</italic>3 = {<italic>x</italic>3<sub>1</sub>, <italic>x</italic>3<sub>2</sub>, …, <italic>x</italic>3<sub><italic>n</italic></sub>} <italic>R</italic><sup><italic>k×d</italic></sup>, and <italic>X</italic>1 = {<italic>x</italic>1<sub>1</sub>, <italic>x</italic>1<sub>2</sub>, …, <italic>x</italic>1<sub><italic>n</italic></sub>} <italic>R</italic><sup><italic>l×d</italic></sup>. Here, we choose the minimum number of region dimensionality in all of the four subjects as the dimension of canonical coordinate space, <italic>minimum</italic> {<italic>n, m, k, l</italic>}, and is annotated as <italic>j</italic>. For each dimension, define the projection weights for each dataset as <italic>a</italic><sub><italic>j</italic></sub> = {<italic>a</italic><sub><italic>j</italic>1</sub>, <italic>a</italic><sub><italic>j</italic>2</sub>, .., <italic>a</italic><sub><italic>jn</italic></sub> }, <italic>b</italic><sub><italic>j</italic></sub> = {<italic>b</italic><sub><italic>j</italic>1</sub>, <italic>b</italic><sub><italic>j</italic>2</sub>, .., <italic>b</italic><sub><italic>jn</italic></sub>}, <italic>c</italic><sub><italic>j</italic></sub> = {<italic>c</italic><sub><italic>j</italic>1</sub>, <italic>c</italic><sub><italic>j</italic>2</sub>, .., <italic>c</italic><sub><italic>jn</italic></sub>}, and <italic>d</italic><sub><italic>j</italic></sub> = {<italic>d</italic><sub><italic>j</italic>1</sub>, <italic>d</italic><sub><italic>j</italic>2</sub>, .., <italic>d</italic><sub><italic>jn</italic></sub>}. The resulting projected datasets are now <italic>d</italic>-dimensional arrays: <italic>u</italic>1<sub><italic>j</italic></sub> = ⟨<italic>a</italic><sub><italic>j</italic></sub>, <italic>X</italic>1⟩, <italic>u</italic>2<sub><italic>j</italic></sub> = ⟨<italic>b</italic><sub><italic>j</italic></sub>, <italic>X</italic>2⟩, <italic>u</italic>3<sub><italic>j</italic></sub> = ⟨<italic>c</italic><sub><italic>j</italic></sub>, <italic>X</italic>3 ⟩, and <italic>u</italic>4<sub><italic>j</italic></sub> = ⟨<italic>d</italic><sub><italic>j</italic></sub>, <italic>X</italic>4⟩. For each of the coordinate spaces, the objective functions can be written as:
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="506091v2_eqn13.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Generally, for each pair of canonical components, the above equation is solved iteratively to find the best projects that can maximize the correlation. During training, the orthogonality between each canonical component is constrained. In our experiment, we calculated the across-subject correlations for each obtained CCs and kept the highest correlation value for each pair, here termed <italic>ρ</italic><sub>1</sub> (<xref ref-type="disp-formula" rid="eqn13">Equation 13</xref>). We performed the above task for each brain region. In addition, we shuffled the chunks ten times and repeated the above steps. We also calculated the canonical component for the same subject having similar behaviors. We applied the same methods as stated above to find similar behavior components and the corresponding neural activities. We divided the obtained neural activities into two parts with the same length and performed the CCA on those two signals. We calculated the correlation between the first two canonical correlation axes as the baseline.</p>
</sec>
<sec id="s16">
<label>.12</label>
<title>Code</title>
<p>The code for training the CS-VAE can be found in Supplementary Material 7. The code can be executed by simply compiling the script ‘train.py’. All the code are available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/saxenalabneuro/Behaivoral-feature-extraction-CS-VAE">https://github.com/saxenalabneuro/Behaivoral-feature-extraction-CS-VAE</ext-link>.</p>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88602.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper is a <bold>valuable</bold> step in multi-subject behavioral modeling using an extension of the Variational Autoencoder (VAE) framework. Using a novel partition of the latent space and in tandem with a recently proposed regularization scheme, the paper provides a rich set of computational analyses analyzing social behavior data of mice with results that represent the state-of-the-art in this subfield. The strength of evidence is <bold>convincing</bold>, with the methodology being well documented and the results being reproducible, although some additional quantifications would have been helpful to fully gauge the circumstances where the approach would be most effectively applied.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88602.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this manuscript, the authors present a valuable new method to represent animal behavior from video data using a variational autoencoder framework that disentangles individual-specific and background variance from variables that can be more reliably compared across individuals. They achieve this aim through the use of a novel Cauchy-Schwatz (C-S) regularization term in their loss function that leads to latents that model continuously varying features in the images. The authors present a variety of validations for the method, including testing across sessions and individuals for a head-fixed task. They also show how the methods could be used for behavioral decoding from neural data, quantifying social behavior in mice, demonstrating the applicability of the method outside of head-fixed environments and for different measurement modalities. While some areas of confusion and questions about the validation exist, this is an overall strong paper and an important contribution to this field.</p>
<p>Strengths:</p>
<p>- The use of the C-S regularizer is novel approach that has potential for wide use across experimental paradigms and model organisms</p>
<p>
- The extent of the validations performed was solid, although perhaps not as convincing in a couple of cases as might be ideal</p>
<p>
- The GitHub code demo worked well, and the code appears to be accessible and well-written</p>
<p>Weaknesses:</p>
<p>- Some of the validation figures were a bit unclear in their presentation, making it difficult to assess exactly what had been tested</p>
<p>
- It is possible that I missed this, but the authors didn't really provide a sense of how to pick a particular distribution to match using the CS term for a specific paradigm/modality and how the choice affects the results</p>
<p>
- While the authors' statements about individual training vs. transfer learning accuracy and efficiency in Figure 6 are technically true, the effect size is rather small ( a few percent at most in each case), thus I don't know how much of a big deal I would want to make out of these results</p>
<p>
- In general, I would have liked to have seen the Discussion section speak more to the choices and limitations inherent in applying the method. How does the choice of prior/metaparameters/architecture/etc affect the results? In what situations would this method to fail? What are the next advances that are necessary for the field to progress?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88602.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper presents a valuable contribution to ongoing methods for understanding and modeling structure via latent variable models for neural and behavioral data. Building on the PS-VAE model of Whiteway et al. (2021), which posited a division of latent variables into unsupervised (i.e., useful for reconstruction) and supervised (useful for predicting selected labeled features) variables, the authors propose an additional set of &quot;constrained subspace&quot; latent variables that are regularized toward a prespecified prior via a Cauchy-Schwarz divergence previously proposed.</p>
<p>The authors contend that the added CS latents aid in capturing both patterns of covariance across the data and individual-specific features that are of particular benefit in multi-animal experiments, all without requiring additional labels. They substantiate these claims with a series of computational experiments demonstrating that their CS-VAE outperforms the PS-VAE in several tasks, particularly that of capturing differences between individuals, consistency in behavioral phenotyping, and predicting correlations with neural data.</p>
<p>Strengths of the present work include an extensive and rigorous set of validation experiments that will be of interest to those analyzing behavioral video. Weaknesses include a lack of discussion of key theoretical ideas motivating the design of the model, including the choice of a Cauchy-Schwarz divergence, the specific form of the prior, and arguments for sorts of information the CS latents might capture and why. In addition, the model makes use of a moderate number of key hyperparameters whose effect on training outcomes are not extensively analyzed. As a result, the model may be difficult for less experienced users to apply to their own data. Finally, as with many similar VAE approaches, the lack of a ground truth against which to validate means that much of evidence provided for the model is necessarily subjective, and its appeal lies in the degree to which the discovered latent spaces appear interpretable in particular applications.</p>
<p>In all, this work is a valuable contribution that is likely to have appeal to those interested in applying latent space methods, particularly to multi-animal video data.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88602.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>As naturalistic neuroscience becomes increasingly popular, the importance of new computational tools that facilitate the study of animals behaving in minimally constrained environments grows. Yi et al convincingly demonstrate the usefulness of their new method on data from neuroethological studies involving multiple animals, including those with social interactions. Briefly, their method improves upon prior semi-supervised machine learning methods in that extracted latent variables can be more cleanly separated into those representing the behavior of individual subjects and those representing social interactions between subjects. Such an improvement is broadly useful for downstream analysis tasks in multi-subject or social neuroethological studies.</p>
<p>Strengths:</p>
<p>
The authors tackle an important problem encountered in behavior analyses in an emerging subfield of neuroscience, naturalistic social neuroscience. They make a case for doing so using semi-unsupervised methods, a toolbox which balances competing scientific needs for building models using large neural-behavioral datasets and for model explainability. The paper is well written, with well-designed figures and relevant analyses that make for an enjoyable reading experience.</p>
<p>The authors provide a remarkable variety of examples that make a convincing case for the utility of their method when used by itself or in conjunction with other data analysis techniques commonly used in modern neuroscience (behavioral motif extraction, neural decoding, etc.). The examples show not just that the extracted latents are more disentangled, but also that the improvement in disentangling has positive effects in downstream analysis tasks.</p>
<p>Weaknesses:</p>
<p>
While the paper does a great job of applying the method to real world data, the components of the method itself are not as thoroughly investigated. For example, the contribution of the novel Cauchy-Schwarz regularization technique has not been systematically investigated. This could be done either by sharing additional data where hyperparameters control the contribution of the regularizer, or cite relevant papers where such an analysis have been carried out. It would also be valuable to understand what other regularization techniques might potentially have been applicable here.</p>
<p>The authors conclude from their empirical investigations that the specific prior distribution does not matter to the regularization process. This seems reasonable given that the neural network can learn a complex and arbitrary transformation of the data during training. It would be helpful if the authors could cite prior work where this type of prior distribution does matter and how their approach is different from such prior work. If there is a visualization/explainability related motivation for choosing one prior distribution over another, this could be clarified.</p>
</body>
</sub-article>
</article>