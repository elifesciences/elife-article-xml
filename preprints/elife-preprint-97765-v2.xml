<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97765</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97765</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97765.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Precision-based causal inference modulates audiovisual temporal recalibration</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0182-3952</contrib-id>
<name>
<surname>Li</surname>
<given-names>Luhe</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>luhe.li@nyu.edu</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1890-1977</contrib-id>
<name>
<surname>Hong</surname>
<given-names>Fangfang</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Badde</surname>
<given-names>Stephanie</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Landy</surname>
<given-names>Michael S</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Psychology, New York University</institution></institution-wrap>, <city>New York</city>, <country>United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>Department of Psychology, University of Pennsylvania</institution></institution-wrap>, <city>Philadelphia</city>, <country>United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05wvpxv85</institution-id><institution>Department of Psychology, Tufts University</institution></institution-wrap>, <city>Medford</city>, <country>United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap>, <city>New York</city>, <country>United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing Interest Statement: The authors have declared no competing interest.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-06-24">
<day>24</day>
<month>06</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-01-15">
<day>15</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97765</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-15">
<day>15</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-11">
<day>11</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.08.584189"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-06-24">
<day>24</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97765.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.97765.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.97765.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.97765.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.97765.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Li et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Li et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97765-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Cross-modal temporal recalibration guarantees stable temporal perception across ever-changing environments. Yet, the mechanisms of cross-modal temporal recalibration remain unknown. Here, we conducted an experiment to measure how participants’ temporal perception was affected by exposure to audiovisual stimuli with consistent temporal delays. Consistent with previous findings, recalibration effects plateaued with increasing audiovisual asynchrony and varied by which modality led during the exposure phase. We compared six observer models that differed in how they update the audiovisual temporal bias during the exposure phase and whether they assume modality-specific or modality-independent precision of arrival latency. The causal-inference observer shifts the audiovisual temporal bias to compensate for perceived asynchrony, which is inferred by considering two causal scenarios: when the audiovisual stimuli have a common cause or separate causes. The asynchrony-contingent observer updates the bias to achieve simultaneity of auditory and visual measurements, modulating the update rate by the likelihood of the audiovisual stimuli originating from a simultaneous event. In the asynchrony-correction model, the observer first assesses whether the sensory measurement is asynchronous; if so, she adjusts the bias proportionally to the magnitude of the measured asynchrony. Each model was paired with either modality-specific or modality-independent precision of arrival latency. A Bayesian model comparison revealed that both the causal-inference process and modality-specific precision in arrival latency are required to capture the nonlinearity and asymmetry observed in audiovisual temporal recalibration. Our findings support the hypothesis that audiovisual temporal recalibration relies on the same causal-inference processes that govern cross-modal perception.</p>
</abstract>

<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Main text and supplements are updated to include the new model comparison.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Perception is not rigid but rather can adapt to the environment. In a multimodal environment, misalignment across the senses can occur because signals in different modalities may arrive with different physical and neural delays in the relevant brain areas (<xref ref-type="bibr" rid="c12">Fain, 2019</xref>; <xref ref-type="bibr" rid="c36">Pöppel, 1988</xref>; <xref ref-type="bibr" rid="c48">Spence &amp; Squire, 2003</xref>). Perceptual misalignment can also arise from changes in the perceptual system relative to the environment, such as when wearing a virtual reality headset or adapting to hearing aids. Cross-modal temporal recalibration serves as a critical mechanism to maintain perceptual synchrony despite changes in the perceptual systems and the environment (reviewed in <xref ref-type="bibr" rid="c26">King, 2005</xref>; <xref ref-type="bibr" rid="c56">Vroomen and Keetels, 2010</xref>). This phenomenon is exemplified in audiovisual temporal recalibration, where consistent exposure to audiovisual stimulus-onset asynchrony (SOA) shifts the point of subjective simultaneity between auditory and visual stimuli; as a result, stimuli perceived as temporally discrepant at first are gradually perceived as more synchronous (<xref ref-type="bibr" rid="c11">Di Luca et al., 2009</xref>; <xref ref-type="bibr" rid="c13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c16">Hanson et al., 2008</xref>; <xref ref-type="bibr" rid="c18">Harrar &amp; Harris, 2008</xref>; <xref ref-type="bibr" rid="c20">Heron et al., 2007</xref>; <xref ref-type="bibr" rid="c25">Keetels &amp; Vroomen, 2007</xref>; <xref ref-type="bibr" rid="c34">Navarra et al., 2005</xref>; <xref ref-type="bibr" rid="c37">Roach et al., 2011</xref>; <xref ref-type="bibr" rid="c50">Tanaka et al., 2011</xref>; <xref ref-type="bibr" rid="c53">Vatakis et al., 2007</xref>, <xref ref-type="bibr" rid="c54">2008</xref>; <xref ref-type="bibr" rid="c55">Vroomen &amp; de Gelder, 2004</xref>; <xref ref-type="bibr" rid="c56">Vroomen &amp; Keetels, 2010</xref>).</p>
<p>However, the mechanisms of cross-modal temporal recalibration remain unknown. The current models of audiovisual temporal recalibration either did not specify the recalibration process (<xref ref-type="bibr" rid="c11">Di Luca et al., 2009</xref>; <xref ref-type="bibr" rid="c33">Navarra et al., 2009</xref>; <xref ref-type="bibr" rid="c62">Yarrow et al., 2015</xref>), or cannot fully capture the characteristics of recalibration effects (<xref ref-type="bibr" rid="c37">Roach et al., 2011</xref>; <xref ref-type="bibr" rid="c45">Sato &amp; Aihara, 2011</xref>; <xref ref-type="bibr" rid="c62">Yarrow et al., 2015</xref>). Specifically, audiovisual temporal recalibration shows two distinct characteristics: the amount of recalibration is nonlinear and asymmetric as a function of the SOA participants are adapted to (adapter SOA). The amount of recalibration is not proportional to the adapter SOA, but instead plateaus at an SOA of approximately 100–300 ms (<xref ref-type="bibr" rid="c13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c55">Vroomen &amp; de Gelder, 2004</xref>). Recalibration can also be asymmetrical: the magnitude of recalibration differs when the visual stimulus leads during the exposure phase compared to when the auditory stimulus leads (<xref ref-type="bibr" rid="c13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c35">O’Donohue et al., 2022</xref>; <xref ref-type="bibr" rid="c51">Van der Burg et al., 2013</xref>). These observations can provide insights into the mechanisms of cross-modal temporal recalibration.</p>
<p>Here, we propose a causal-inference model to explain the mechanism of audiovisual temporal recalibration. Causal inference is the process in which the observer determines whether multisensory signals originate from a common source and should be integrated or kept separate (<xref ref-type="bibr" rid="c46">Sato et al., 2007</xref>; <xref ref-type="bibr" rid="c47">Shams &amp; Beierholm, 2010</xref>; <xref ref-type="bibr" rid="c58">Wei &amp; Körding, 2009</xref>). Bayesian models based on causal inference have been proposed to explain multisensory integration effects (<xref ref-type="bibr" rid="c27">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="c46">Sato et al., 2007</xref>), and these models have been empirically validated in studies of spatial audiovisual and visual-tactile integration (<xref ref-type="bibr" rid="c6">Badde, Navarro, &amp; Landy, 2020</xref>; <xref ref-type="bibr" rid="c7">Beierholm et al., 2009</xref>; <xref ref-type="bibr" rid="c40">Rohe &amp; Noppeney, 2015</xref>; <xref ref-type="bibr" rid="c59">Wozny et al., 2010</xref>). In the temporal domain, some studies have successfully used causal inference to model the integration of cross-modal relative timing, accurately predicting simultaneity judgments in audiovisual speech (<xref ref-type="bibr" rid="c30">Magnotti et al., 2013</xref>) and more complex scenarios involving one auditory and two visual stimuli (<xref ref-type="bibr" rid="c44">Sato, 2021</xref>). In the context of cross-modal recalibration, causal inference is expected to play a role based on the intuition that recalibration should be reduced when the multisensory signals are not perceived as causally related (<xref ref-type="bibr" rid="c13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c24">Hsiao et al., 2022</xref>; <xref ref-type="bibr" rid="c55">Vroomen &amp; de Gelder, 2004</xref>). Supporting this, causal-inference models successfully predicted cross-modal spatial recalibration of visual-auditory (<xref ref-type="bibr" rid="c22">Hong, 2023</xref>; <xref ref-type="bibr" rid="c23">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="c46">Sato et al., 2007</xref>) and visuo-tactile (<xref ref-type="bibr" rid="c6">Badde, Navarro, &amp; Landy, 2020</xref>) signals. Building on this framework, here we propose a causal-inference model for cross-modal temporal recalibration that derives the multisensory percept based on inferences about the shared origin of the signals and updates the cross-modal temporal biases such that subsequent measurements are shifted toward the percept.</p>
<p>The first aim of this study is to test whether performing causal inference is necessary to explain the nonlinearity of audiovisual temporal recalibration across different adapter SOAs. To this aim, we compared the causal-inference model with two alternatives: an asynchrony-contingent model and an asynchrony-correction model. The asynchrony-contingent model scales the amount of recalibration by the likelihood that the sensory measurement of SOA was caused by a synchronous audiovisual stimulus pair. The model predicts a nonlinear recalibration effect across adapter SOAs without requiring observers to perform full Bayesian inference. The asynchrony-correction model assumes that recalibration only occurs when an asynchronous onset of the cross-modal stimuli is registered, followed by the update of the cross-modal temporal bias to compensate for this SOA measurement. This account is based on the intuitive rationale that repeated measurements of asynchrony can prompt the perceptual system to restore coherence. In contrast, this model predicts minimal recalibration when the adapter SOA falls within the range of measured asynchronies that can arise with simultaneously presented stimuli due to sensory noise. This model serves as the baseline for model comparison.</p>
<p>The second aim was to examine factors that had the potential to drive the asymmetry of recalibration across visual-leading and auditory-leading adapter SOAs. It has been suggested that the asymmetry may be explained by physical and neural latency differences between signals (<xref ref-type="bibr" rid="c35">O’Donohue et al., 2022</xref>; <xref ref-type="bibr" rid="c51">Van der Burg et al., 2013</xref>). These latency differences can vary significantly based on the physical distance between the stimulus and the sensors, as well as the neural transmission time required for the signal to reach the relevant sensory region (<xref ref-type="bibr" rid="c6">Badde, Navarro, &amp; Landy, 2020</xref>; <xref ref-type="bibr" rid="c21">Hirsh &amp; Sherrick, 1961</xref>; <xref ref-type="bibr" rid="c26">King, 2005</xref>). While these latency differences can explain the audiovisual temporal bias observed in most humans, they would affect recalibration to different adapter SOAs equally, making it unlikely for any asymmetry to arise. In contrast to latency differences, sensory uncertainty has been shown to affect the degree of cross-modal recalibration in a complex fashion (<xref ref-type="bibr" rid="c6">Badde, Navarro, &amp; Landy, 2020</xref>; <xref ref-type="bibr" rid="c23">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="c52">van Beers et al., 2002</xref>). We hypothesized that the difference across modalities in the variability of the arrival times, the time it takes visual and auditory signals to arrive in the relevant brain areas, plays a critical role in the asymmetry of cross-modal temporal recalibration.</p>
<p>To examine the mechanism underlying audiovisual temporal recalibration, we manipulated the adapter SOA cross sessions, introducing asynchronies up to 0.7 s of either auditory or visual lead. Before and after the exposure phase in each session, we measured participants’ perception of audiovisual relative timing using a ternary temporal-order-judgement (TOJ) task. To preview the empirical results, we confirmed the nonlinearity of the recalibration effect: recalibration magnitude increased linearly for short adapter SOAs, but then reached an asymptote or even decreased with increasing adapter SOAs. Furthermore, participants showed idiosyncratic asymmetries of the recalibration effect across modalities; for most participants, the amount of recalibration was larger when the auditory stimulus led than when it lagged, but the opposite was found for other participants. To scrutinize the factors that might drive the nonlinearity and asymmetry of temporal recalibration, we fitted six models to the data. These models based the amount of recalibration either on perceptual causal-inference processes, a heuristic evaluation of the common cause of the audiovisual stimuli, or a fixed criterion for the need to correct asynchrony. For each of these three models we implemented either modality-specific or modality-independent precision of the arrival times. The model comparison revealed that the assumptions of Bayesian causal inference combined with modality-specific precision are essential to accurately capture the nonlinearity and idiosyncratic asymmetry of temporal recalibration.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Behavioral results</title>
<p>We adopted a classical three-phase recalibration paradigm in which participants completed a pre-test, an exposure phase, and a post-test in each session. In pre- and post-tests, we measured participants’ perception of audiovisual relative timing using a ternary TOJ task: participants reported the perceived order (“visual first,” “auditory first,” or “simultaneous”) of audiovisual stimulus pairs with varying SOA (range: from -0.5 to 0.5 s with 15 levels; <xref rid="fig1" ref-type="fig">Figure 1A</xref>). In the exposure phase, we induced temporal recalibration by having participants perform a control task, the oddball-detection task. Specifically, participants were exposed to a series of audiovisual stimuli with a consistent SOA (250 trials; <xref rid="fig1" ref-type="fig">Figure 1B</xref>). To ensure that participants were attentive to the stimuli, we inserted oddball stimuli with greater intensity in either one or both modalities (5% of the total trials independently sampled for each modality). Participants were instructed to press a key corresponding to the auditory, visual, or both oddballs whenever an oddball stimulus appeared. The high <italic>d</italic>′ of oddball-detection performance (auditory <italic>d</italic>′ = 3.34±0.54, visual <italic>d</italic>′ = 2.44±0.72) indicates that participants paid attention to both modalities. The post-test was almost identical to the pre-test, except that before every temporal-order-judgment trial, there were three top-up oddball-detection trials to maintain the recalibration effect. In total, participants completed nine sessions on separate days. The adapter SOA (range: -0.7 to 0.7 s) was fixed within a session, but varied randomly across sessions and participants.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
    <caption><title>Task timing.</title>
    <p>(A) Temporal-order-judgment task administered in the pre- and post-tests. In each trial, participants made a temporal-order judgment in response to an audiovisual stimulus pair with a varying stimulus-onset asynchrony (SOA). Negative values: auditory lead; positive values: visual lead. The contrast of the visual stimulus has been increased for this illustration. (B) Oddball-detection task performed in the exposure phase and top-up trials during the post-exposure test phase. Participants were repeatedly presented with an audiovisual stimulus pair with a SOA that was fixed within each session but varied across sessions. Occasionally, the intensity of either one or both of the stimuli was increased. Participants were instructed to press a key corresponding to the auditory, visual, or both oddballs whenever an oddball stimulus appeared.</p></caption>
<graphic xlink:href="584189v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We compared the temporal-order judgments between the pre- and post-tests to examine the amount of audiovisual temporal recalibration induced by the audiovisual stimuli during the exposure phase. Specifically, we fitted the data from the pre- and post-tests jointly assuming different points of subjective simultaneity (PSS) between the two tests while assuming the same shape for the psychometric functions that is determined by the relative arrival-latencies, their precision, and fixed response criteria (<xref rid="fig2" ref-type="fig">Figure 2A</xref>; see Supplement Section 1 for the formalization of the atheoretical model and an alternative model assuming a shift in the response criteria due to recalibration). The PSS is the physical SOA that corresponds to the maximum probability of reporting simultaneity (<xref ref-type="bibr" rid="c49">Sternberg &amp; Knoll, 1973</xref>). The amount of audiovisual temporal recalibration was defined as the difference between the two PSS’s. At the group level, we observed a nonlinear pattern of recalibration as a function of the adapter SOA: the amount of recalibration in the direction of the adapter SOA first increased but then plateaued with increasing magnitude of the adapter SOA, the SOA of the pairs presented during the exposure phase (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Additionally, we observed an asymmetry in the amount of recalibration between auditory-leading and visual-leading adapter SOAs, with auditory-leading adapter SOAs inducing a greater amount of recalibration (<xref rid="fig2" ref-type="fig">Figure 2B</xref>; see Supplement Figure S2A for individual participants’ data). To quantify this asymmetry for each participant, we calculated an asymmetry index, defined as the sum of the recalibration effects across all adapter SOAs (zero: no evidence for asymmetry; positive values: greater recalibration given visual-lead adapters; negative: greater recalibration given auditory-lead adapters). For each participant, we bootstrapped the temporal-order judgments to obtain a 95% confidence interval for the asymmetry index. Eight out of nine participants showed an asymmetry index significantly different from zero, with the majority showing greater recalibration for auditory-leading adapter SOAs, suggesting a general asymmetry in recalibration (Supplement Figure S2B).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
    <caption><title>Behavioral results.</title>
    <p>(A) The probability of reporting that the auditory stimulus came first (blue), the two arrived at the same time (green), or the visual stimulus came first (red) as a function of SOA for a representative participant in a single session. The adapter SOA was -0.3 s for this session. Curves: best-fitting psychometric functions estimated jointly using the data from the pre-test (dashed) and post-test (solid). Shaded areas: 95% bootstrapped confidence intervals. (B) Mean recalibration effects averaged across all participants as a function of adapter SOA. The recalibration effects are defined as the shifts in the point of subjective simultaneity (PSS) from the pre-to the post-test, where the PSS is the physical SOA at which the probability of reporting simultaneity is maximized. Error bars: ±SEM.</p></caption>
<graphic xlink:href="584189v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Modeling results</title>
<p>In the following sections, we describe our models for cross-modal temporal recalibration by first laying out the general assumptions of these models, and then elaborating on the differences between them. Then, we compare the models’ ability to capture the observed data.</p>
<sec id="s2b1">
<label>2.2.1</label>
<title>General model assumptions</title>
<p>We formulated six process models of cross-modal temporal recalibration (<xref rid="fig3" ref-type="fig">Figure 3</xref>). These models share several assumptions about audiovisual temporal perception and recalibration that we selected based on a comparison of atheoretical, descriptive models of our data (Supplement Section 1). First, when an auditory and a visual signal are presented, the corresponding neural signals arrive in the relevant brain areas with a variable latency due to internal and external noise. We assume arrival times for the two modalities are independent and that the arrival latencies are exponentially distributed (<xref ref-type="bibr" rid="c14">García-Pérez &amp; Alcalá-Quintana, 2012</xref>) (<xref rid="fig3" ref-type="fig">Figure 3A</xref>, left panel). Moreover, we assume a constant offset between auditory and visual arrival times, reflecting an audiovisual temporal bias. A simple derivation shows that the resulting measurement of SOA has a double-exponential distribution (<xref rid="fig3" ref-type="fig">Figure 3A</xref>, right panel; see derivation in Supplement Section 3). The probability density function peaks at a SOA that is the physical SOA of the stimuli plus the participant’s audiovisual temporal bias. The slopes of the measurement distribution reflect the precision of the arrival times; the steeper the slope, the more precise the measured latency. When the precision differs between modalities, the measurement distribution of the SOA between the auditory and visual stimuli is asymmetrical (<xref rid="fig3" ref-type="fig">Figure 3B</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
    <caption><title>Illustration of the six observer models of cross-modal temporal recalibration.</title>
    <p>(A) Left: Arrival-latency distributions for auditory (blue) and visual (red) sensory signals. When the precision of arrival latency is modality-independent, these two exponential distributions have identical shape. Right: The resulting symmetrical double-exponential measurement distribution of the SOA of the stimuli. (B) When the precision of the arrival latencies is modality-dependent, the arrival-latency distributions for auditory and visual signals have different shapes, and the resulting measurement distribution of the SOA is asymmetrical. (C) Bias update rules and predicted recalibration effects for the three contrasted recalibration models: The causal-inference model updates the audiovisual bias based on the difference between the estimated and measured SOA. The asynchrony-contingent model updates the audiovisual bias by a proportion of the measured SOA and modulates the update rate by the likelihood that the measured sensory signals originated from a simultaneous audiovisual pair. The asynchrony-correction model adjusts the audiovisual bias by a proportion of the measured SOA when this measurement exceeds fixed critera for simultaneity.</p></caption>
<graphic xlink:href="584189v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Second, these models define temporal recalibration as the accumulation of updates to the audiovisual temporal bias after each encounter with an SOA. The accumulated shift in the audiovisual bias at the end of the exposure phase is then carried over to the post-test phase and persists throughout. Lastly, the bias is assumed to be reset to the same initial value in the pre-test across all nine sessions, reflecting the stability of the audiovisual temporal bias over time (<xref ref-type="bibr" rid="c15">Grabot &amp; van Wassenhove, 2017</xref>).</p>
</sec>
<sec id="s2b2">
<label>2.2.2</label>
<title>Models of cross-modal temporal recalibration</title>
<p>The six models we tested differed in the mechanism governing the updates of the audiovisual bias during the exposure phase as well as the modality-specificity of the precision of arrival times.</p>
<p>We formulated a temporal variant of the spatial Bayesian causal-inference model of recalibration (<xref ref-type="bibr" rid="c6">Badde, Navarro, &amp; Landy, 2020</xref>; <xref ref-type="bibr" rid="c22">Hong, 2023</xref>; <xref ref-type="bibr" rid="c23">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="c46">Sato et al., 2007</xref>) to describe the recalibration of the relative timing between cross-modal stimuli (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, left panel). In this model, when an observer is presented with an audiovisual stimulus pair during the exposure phase, they compute two intermediate estimates of the SOA between the stimuli, one for the common-cause scenario and the other for the separate-cause scenario. In the common-cause scenario, the estimated SOA of the stimuli is smaller than the measured SOA as it is combined with a prior distribution over SOA that reflects simultaneity. In the separate-causes scenario, the estimated SOA is approximately equal to the measured SOA. The two estimates are then averaged with each one weighted by the posterior probability of the corresponding causal scenario. The audiovisual bias is then updated to reduce the difference between the measured SOA and the combined estimate of the SOA. In other words, causal inference regulates the recalibration process by shifting the measured SOA to more closely match the percept, which in turn is computed based on the inferred causal structure. The asynchrony-contingent model assumes that the observer estimates the likelihood that the sensory signals originated from a simultaneous audiovisual pair and updates the audiovisual bias by a proportion of measured SOA scaled by this likelihood (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, middle panel). There is a key distinction between the likelihood of simultaneity and the likelihood of a common cause. The likelihood of a common cause considers the prior distribution of SOAs when signals originate from the same source, including nonzero probabilities for SOAs ≠ 0. In contrast, the likelihood of simultaneity exclusively considers the case when SOA = 0. Additionally, we assume that asynchrony-contingent observer computes the likelihood of simultaneity based on the knowledge of the double-exponential measurement distribution, instead of assuming a Gaussian measurement distribution as was done previously (<xref ref-type="bibr" rid="c31">Maij et al., 2009</xref>). The update rate of the audiovisual bias is proportional to this likelihood. For a stimulus pair with a large SOA, the average likelihood of the stimuli being physically simultaneous decreases, leading to reduced recalibration effects compared to stimulus pairs with smaller SOAs. Thus, this asynchrony-contingent model is capable of replicating the nonlinearity of recalibration across adapter SOAs without requiring the observer to perform full Bayesian inference.</p>
<p>The asynchrony-correction model assumes that the observer first compares the sensory measurement of SOA to their criteria for audiovisual simultaneity to decide whether to recalibrate in a given trial. If the measured SOA falls within the range perceived as simultaneous according to the fixed criteria, the observer might attribute a non-zero measurement of SOA to sensory noise and omit recalibration. On the other hand, if the measured SOA exceeds this range, the observer perceives the stimuli as asynchronous, and shifts the audiovisual bias by a proportion of the measurement of SOA (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, right panel). This model serves as a direct contrast to the causal-inference model, as it predicts an opposite pattern: a nonlinear but monotonic increase in temporal recalibration, with minimal recalibration when the measured SOA falls within the simultaneity range and increasing recalibration as the measured SOA moves further outside of this range.</p>
<p>We additionally assumed either modality-specific or modality-independent precision of the arrival times. Each choice suggests a different origin of the variability. Either the variability of the arrival times is limited by neural-latency noise in each sensory channel (<xref ref-type="bibr" rid="c61">Yarrow et al., 2022</xref>) and thus is modality-specific or the variability of arrival times results from the variability in a central timing mechanism (<xref ref-type="bibr" rid="c21">Hirsh &amp; Sherrick, 1961</xref>) and is thus modality-independent.</p>
</sec>
<sec id="s2b3">
<label>2.2.3</label>
<title>Model fitting and model comparison</title>
<p>We fitted six models to each participant’s data. Each model was constrained jointly by the temporal-order judgments from the pre- and post-tests of all nine sessions. To quantify model performance, we calculated model evidence, i.e., the likelihood of each model given the data marginalized over all possible parameters, which revealed that the causal-inference model had the strongest model evidence at the group level and best fit the data of most participants, followed by the asynchrony-contingent model and then the asynchrony-correction model. To quantify the differences between model performance, we performed a Bayesian model comparison by computing the Bayes factor for each model relative to the worst-performing model, the asynchrony-correction model with modality-independent arrival-latency precision (<xref rid="fig4" ref-type="fig">Figure 4A</xref>, see Supplement Figure S3 for individual-level model comparison). Within each of these three model categories, the version incorporating modality-specific precision consistently outperformed the modality-independent version.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
    <caption><title>Model comparison and predictions.</title>
    <p>(A) Model comparison based on model evidence. Each bar represents the group-averaged log Bayes Factor of each model relative to the asynchrony-correction, modality-independent-precision model, which had the weakest model evidence. (B) Empirical data (points) and model predictions (lines and shaded regions) for the recalibration effect as a function of adapter SOA.</p></caption>
<graphic xlink:href="584189v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b4">
<label>2.2.4</label>
<title>Model prediction</title>
<p>To inspect the quality of the model fit, for every model, we used the best-fitting parameter estimates for each participant to predict the group-average recalibration effect as a function of adapter SOA (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). The nonlinearity of audiovisual temporal recalibration across adapter SOAs was captured by both the asynchrony-contingent and causal-inference models. Nonetheless, the causal-inference model outperformed the asynchrony-contingent model by accurately predicting a non-zero average recalibration effect at adapter SOAs of 0.7 s and -0.7 s, where the asynchrony-contingent model predicted no recalibration. Additionally, incorporating modality-specific precision enabled both the asynchrony-contingent and causal-inference models to more accurately predict increased recalibration when the adapter SOA was auditory-leading. Overall, the model that relies on causal inference during the exposure phase and assumes modality-specific precision of arrival times most accurately captured both the nonlinearity and asymmetry of the recalibration effect. This model could also account for individual participants’ idiosyncratic asymmetry in temporal recalibration to auditory- and visual-leading adapter SOAs (see Supplement Figure S4 for predictions of individual participants’ recalibration effects of all models; see Figure S6 for predictions of individual participants’ TOJ responses using the causal-inference models with modality-specific precision).</p>
</sec>
<sec id="s2b5">
<label>2.2.5</label>
<title>Model simulation</title>
<p>Simulations with the causal-inference model revealed which factors of the modeled recalibration process determine the degree of nonlinearity and asymmetry of cross-modal temporal recalibration to different adapter SOAs. The prior belief that the auditory and visual stimuli share a common cause plays a crucial role in adjudicating the relative influence of the two causal scenarios (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). When the observer has a prior belief that audiovisual stimuli always originate from the same source, they recalibrate by a proportion of the perceived SOA no matter how large the measured SOA is, mirroring the behavior of the asynchrony-correction model when its criteria for simultaneity are such that no stimuli are treated as simultaneous. On the other hand, when the observer believes that the audiovisual stimuli always have separate causes, they treat the audiovisual stimuli as independent of each other and do not recalibrate. Estimates of the common-cause prior for our participants fall between the two extreme beliefs, resulting in the nonlinear pattern of recalibration that lies between the extremes of no recalibration and the proportional recalibration effects as a function of the adapter SOA (see Supplement Section 6.1 for parameter estimates for individual participants).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
    <caption><title>Simulation of temporal recalibration using the causal-inference model.</title>
    <p>(A) The influence of the observer’s prior assumption of a common cause: the stronger the prior, the larger the recalibration effects. (B) The influence of latency noise: recalibration effects increase with decreasing sensory precision (i.e., increasing latency noise captured by the exponential time constant) of both modalities. (C) The influence of auditory/visual latency noise: recalibration effects are asymmetric between auditory-leading and visual-leading adapter SOAs due to differences in the precision of auditory and visual arrival latencies. Left panel: Increasing auditory latency precision (i.e., reducing auditory latency noise) reduces recalibration in response to visual-leading adapter SOAs. Right panel: Increasing visual precision (i.e., reducing visual latency noise) reduces recalibration in response to auditory-leading adapter SOAs.</p></caption>
<graphic xlink:href="584189v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Simulations also identified key model elements of the causal-inference model that predict a non-zero recalibration effect even at large SOAs, a feature that distinguishes the causal-inference from the asynchrony-contingent model. This non-zero recalibration effect for large adapter SOAs can be replicated by either assuming a strong prior for a common cause (<xref rid="fig5" ref-type="fig">Figure 5A</xref>) or by assuming low sensory precision of arrival times (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). Both relationships are intuitive: observers with a stronger prior belief in a common cause and ideal observers with lower sensory precision are more likely to assign a higher posterior probability to the common-cause scenario, leading to greater recalibration. A decrease of the spread of the prior distribution over SOA conditioned on a common cause increases the recalibration magnitude, but only over a small range of SOAs for which there is a higher probability of the common-cause scenario (Supplement Figure S8A), and thus cannot account for non-zero recalibration for large SOAs.</p>
<p>Differences in arrival-time precision between audition and vision result in an asymmetry of audiovisual temporal recalibration across adapter SOAs (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). The amount of recalibration is attenuated when the modality with the higher precision lags the less precise one during the exposure phase. When the more recent stimulus component in a cross-modal pair is more precise, the perceptual system is more likely to attribute the asynchrony to separate causes and thus recalibrate less. In addition, the fixed audiovisual bias does not affect asymmetry, but shifts the recalibration function laterally and determines the adapter SOA for which no recalibration occurs (Supplement Figure S8B).</p>
</sec>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>This study scrutinized the mechanism underlying audiovisual temporal recalibration. We measured the effects of exposure to audiovisual stimulus pairs with a constant temporal offset (adapter SOA) on audiovisual temporal-order perception across a wide range of adaptor SOAs. Recalibration effects changed nonlinearly with the magnitude of adapter SOAs and were asymmetric across auditory-leading and visual-leading adapter SOAs. We then compared the predictions of different observer models for the amount of recalibration as a function of adapter SOA. A Bayesian causal-inference model with modality-specific precision of the arrival latencies fit the observed data best. These findings suggest that human observers rely on causal-inference-based percepts to recalibrate cross-modal temporal perception. These results align closely with studies that have demonstrated the role of causal inference in audiovisual (<xref ref-type="bibr" rid="c23">Hong et al., 2021</xref>) and visual-tactile spatial recalibration (<xref ref-type="bibr" rid="c6">Badde, Navarro, &amp; Landy, 2020</xref>). Our results are also consistent with previous recalibration models that assumed a strong relation between perception and recalibration (<xref ref-type="bibr" rid="c44">Sato, 2021</xref>; <xref ref-type="bibr" rid="c46">Sato et al., 2007</xref>). Hence, we suggest that the same mechanisms underly cross-modal perception and recalibration across different sensory features.</p>
<p>The observed recalibration results could not be predicted by the asynchrony-contingent model that employed a heuristic approximation of the causal-inference process. Even though this model was capable of predicting a nonlinear relationship between the recalibration effect and the adapter SOA, it failed to capture a non-zero recalibration effect at large adapter SOAs shown by several of our participants. The reason for that is that this model uses the likelihood of a synchronous audiovisual stimulus pair given the measured SOA to modulate the update rate of audiovisual bias, which will be very small on average for large SOAs. Therefore, the model predicts little to no recalibration at large adaptor SOAs. In contrast, the causal-inference model can capture the non-zero recalibration effect because the common-cause scenario always influences the amount of recalibration even when the adapter SOA is too large to be perceived as synchronous. Simulation (<xref rid="fig5" ref-type="fig">Figure 5A, B</xref>) shows that a strong prior belief in a common cause or less precision of arrival times can result in non-zero recalibration effects following exposure to clearly asynchronous stimulus pairs. Notably, even though it might at first seem counter-intuitive that cross-modal temporal recalibration can be elicited by clearly asynchronous streams of sensory information, many of us have experienced this effect during laggy, long video conferences.</p>
<p>The asynchrony-correction model assumes that observers recalibrate to restore temporal synchrony whenever the SOA measurement indicates a temporal discrepancy, but this model predicts recalibration effects across adapter SOAs that are contrary to our observations. This suggests that cross-modal temporal recalibration is not merely triggered by an asynchronous sensory measurement of SOA and an attempt to correct it. In contrast, the causal-inference model accurately captured the plateau of the recalibration effects as adapter SOA increased, because the probability that the auditory and visual stimuli have separate causes also increased. This resulted in a smaller discrepancy between the sensory measurement and the final percept of the SOA, leading to less recalibration.</p>
<p>We found that most of our participants exhibited larger recalibration effects in response to exposure to audiovisual stimuli with a consistent auditory lead compared to exposure to a visual lead. This result is consistent with a previous study that reported greater cumulative recalibration in response to audiovisual stimuli with an auditory-lead at the group level (<xref ref-type="bibr" rid="c35">O’Donohue et al., 2022</xref>). Our simulation results further suggested that this asymmetry in recalibration effects might be due to higher precision of auditory compared to visual arrival latencies. A few participants displayed the opposite pattern: stronger recalibration effects following exposure to visual-leading audiovisual stimuli. This is not surprising, as causal-inference models often reveal substantial individual differences in sensory noise (<xref ref-type="bibr" rid="c23">Hong et al., 2021</xref>; <xref ref-type="bibr" rid="c30">Magnotti et al., 2013</xref>). A recent EEG study further provided neural correlates for individual sensory noise by identifying correlations between neural-latency noise and behavioral sensory noise measured from simultaneity-judgment tasks for audiovisual, visuo-tactile, and audio-tactile pairs (<xref ref-type="bibr" rid="c61">Yarrow et al., 2022</xref>). Therefore, our model explains how individual differences in precision of arrival latency could contribute to the asymmetry in cross-modal temporal recalibration observed in previous studies. For example, <xref ref-type="bibr" rid="c13">Fujisaki et al. (2004)</xref> found a slightly larger recalibration in response to audiovisual stimuli with a visual lead compared to an auditory lead, while their pilot results with the same design but a wider range of adapter SOAs showed the opposite pattern.</p>
<p>In order to incorporate causal inference in our recalibration models, we modeled recalibration as a shift of audiovisual bias. Building on previous latency-shift models (<xref ref-type="bibr" rid="c11">Di Luca et al., 2009</xref>; <xref ref-type="bibr" rid="c33">Navarra et al., 2009</xref>), we specified a mechanism for how the audiovisual bias is updated during the exposure to an audiovisual SOA. Our model is not mutually exclusive with other models that implement recalibration as a shift of simultaneity criteria (Yarrow, Jahn, et al., 2011; <xref ref-type="bibr" rid="c62">Yarrow et al., 2015</xref>), or a change of sensitivity to discriminate SOA (<xref ref-type="bibr" rid="c43">Roseboom et al., 2015</xref>). A possible implementation of recalibration at the circuity level is given by models assuming that audiovisual offsets are encoded by populations of neurons tuned to different SOAs. In these models, recalibration is the consequence of selective gain reduction of neurons tuned to SOAs similar to the adapter SOA (<xref ref-type="bibr" rid="c9">Cai et al., 2012</xref>; <xref ref-type="bibr" rid="c37">Roach et al., 2011</xref>; <xref ref-type="bibr" rid="c62">Yarrow et al., 2015</xref>). Simulations show that this model can predict nonlinear recalibration effects as a function of adapter SOA depending on the number of neurons and the range of preferred SOAs (Supplementary Section S8). However, to capture the asymmetric recalibration effects depending on which modality leads, one needs to incorporate inhomogenous neuronal selectivity, i.e., unequal tuning curves, for auditory-leading and visual-leading SOAs.</p>
<p>Causal inference may effectively function as a credit-assignment mechanism to enhance perceptual accuracy during recalibration. In sensorimotor adaptation, humans correct motor errors that are more likely attributed to their own motor system rather than to the environment (<xref ref-type="bibr" rid="c8">Berniker &amp; Kording, 2008</xref>; <xref ref-type="bibr" rid="c58">Wei &amp; Körding, 2009</xref>). In visuomotor adaptation, substantial temporal recalibration occurs in response to exposure to movement-leading SOAs but less so to visual-leading SOAs (<xref ref-type="bibr" rid="c38">Rohde &amp; Ernst, 2012</xref>; <xref ref-type="bibr" rid="c39">Rohde et al., 2014</xref>), because only movement-leading SOAs can be interpreted as causally linked sensory feedback from a preceding movement.</p>
<p>Causal-inference-based recalibration can further solve the conundrum that humans, despite our ability for cross-modal temporal recalibration, show persistent temporal biases (<xref ref-type="bibr" rid="c15">Grabot &amp; van Wassenhove, 2017</xref>). These audiovisual and visual-tactile temporal biases appear to be shaped by early sensory experience (Badde, Ley, et al., 2020) and seem to be resistant to recalibration. The persistence of these biases contradicts recalibration models that reduce the measured cross-modal asynchrony. Instead, our causal-inference-based models of recalibration include an assumption that recalibration eliminates the discrepancy between measured and inferred asynchrony, both of which are influenced by cross-modal biases.</p>
<p>Previous studies have probed the role of causal inference for temporal recalibration and perception by experimentally varying task-irrelevant cues to a shared origin of the cross-modal stimuli, with mixed results. Earlier studies found no significant change in temporal recalibration when altering the sound presentation method (headphones versus a speaker) or switching the presentation ear (<xref ref-type="bibr" rid="c13">Fujisaki et al., 2004</xref>), nor did recalibration effects vary with the spatial alignment of the audiovisual stimulus pair (<xref ref-type="bibr" rid="c25">Keetels &amp; Vroomen, 2007</xref>). However, subsequent studies provide evidence that spatial grouping influences temporal recalibration, with the PSS shifting toward the temporal relationship suggested by spatially co-located stimuli (<xref ref-type="bibr" rid="c19">Heron et al., 2012</xref>; <xref ref-type="bibr" rid="c63">Yarrow, Roseboom, &amp; Arnold, 2011</xref>). Others found that spatial cues (<xref ref-type="bibr" rid="c19">Heron et al., 2012</xref>; <xref ref-type="bibr" rid="c64">Yuan et al., 2012</xref>) and featural content cues (<xref ref-type="bibr" rid="c41">Roseboom &amp; Arnold, 2011</xref>; <xref ref-type="bibr" rid="c42">Roseboom et al., 2013</xref>; <xref ref-type="bibr" rid="c64">Yuan et al., 2012</xref>) are both determinants of cross-modal temporal recalibration. The feature content cues can be natural stimuli, such as male or female audiovisual speech, or simple stimuli, such as high-pitch sounds paired with vertically oriented Gabor patches. Recent studies on audiovisual integration have extended causal-inference models to account for both the spatial position and temporal discrepancy of audiovisual signals (<xref ref-type="bibr" rid="c22">Hong, 2023</xref>; <xref ref-type="bibr" rid="c32">McGovern et al., 2016</xref>). These studies suggest that both temporal and spatial information are taken into account for causal inference. In contrast, perceived conflicts in task-irrelevant features of visual-haptic stimuli do not influence the integration of task-relevant features, suggesting that causal inference is feature-specific rather than pertaining to whole objects (<xref ref-type="bibr" rid="c4">Badde et al., 2023</xref>).</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Conclusion</title>
<p>In sum, we found that both causal inference and modality-specific precision are essential for accurately modeling audiovisual temporal recalibration. Although cross-modal temporal recalibration is typically viewed as an early-stage, low-level perceptual process, our findings indicate that it is closely connected to higher cognitive functions.</p>
</sec>
<sec id="s5">
<label>5</label>
<title>Methods</title>
<sec id="s5a">
<label>5.1</label>
<title>Participants</title>
<p>Ten students from New York University (three males; age: 24.4 ± 1.77; all right-handed) participated in the experiment. They all reported normal or corrected-to-normal vision. All participants provided informed written consent before the experiment and received $15/hr as monetary compensation. The study was conducted in accordance with the guidelines laid down in the Declaration of Helsinki and approved by the New York University institutional review board. One out of ten participants was identified as an outlier and therefore excluded from further data analysis (Supplement Figure S9).</p>
</sec>
<sec id="s5b">
<label>5.2</label>
<title>Apparatus and stimuli</title>
<p>Participants completed the experiments in a dark and semi sound-attenuated room. They were seated 1 m from an acoustically transparent, white screen (1.36 × 1.02 m, 68 × 52° visual angle) and placed their head on a chin rest. An LCD projector (Hitachi CP-X3010N, 1024 × 768 pixels, 60 Hz) was mounted above and behind participants to project visual stimuli on the screen. The visual and auditory stimulus durations were 33.33 ms. The visual stimulus was a high-contrast (36.1 cd<italic>/</italic>m<sup>2</sup>) Gaussian blob (SD: 3.6°) on a gray background (10.2 cd<italic>/</italic>m<sup>2</sup>) projected onto the screen. The auditory stimulus was a 500 Hz beep (50 dB SPL) without a temporal window due to its short duration, which was played by a loudspeaker located behind the center of the screen. Some visual and auditory stimuli were of higher intensity, the parameters of these stimuli were determined individually (see Intensity-discrimination task). We adjusted the timing of audiovisual stimulus presentation and verified the timing using an oscilloscope (PICOSCOPE 2204A).</p>
</sec>
<sec id="s5c">
<label>5.3</label>
<title>Procedure</title>
<p>The experiment consisted of nine sessions, which took place on nine separate days. In each session, participants completed a pre-test, an exposure, and a post-test phase in sequence. The adapter SOA was fixed within a session, but varied across sessions (±700, ±300, ±200, ±100, 0 ms). The order of the adapter SOA was randomized across participants, with sessions separated by at least one day. The intensities of the oddball stimuli were determined prior to the experiment for each participant using an intensity-discrimination task to equate the difficulty of detecting oddball stimuli between participants and across modalities.</p>
<sec id="s5c1">
<label>5.3.1</label>
<title>Pre-test phase</title>
<p>Participants completed a ternary TOJ task during the pre-test phase. Each trial started a fixation cross (0.1–0.2 s, uniform distribution; <xref rid="fig1" ref-type="fig">Fig. 1A</xref>), followed by a blank screen (0.4–0.6 s, uniform distribution). Then, an auditory and a visual stimulus (0.033 s) were presented with a variable SOA. There were a total of 15 possible test SOAs (±0.5 s and from -0.3 to 0.3 s in steps of 0.05 s), with positive values representing visual lead and negative values representing auditory lead. Following stimulus presentation there was another blank screen (0.4–0.6 s, uniform distribution), and then a response probe appeared on the screen. Participants indicated by button press whether the auditory stimulus occurred before or after the visual stimulus, or the two were simultaneous. There was no time limit for the response, and response feedback was not provided. The inter-trial interval (ITI) was 0.2–0.4 s (uniform distribution). Each test SOA was presented 20 times in pseudo-randomized order, resulting in 300 trials in total, divided into five blocks. Participants usually took around 15 minutes to finish the pre-test phase.</p>
</sec>
<sec id="s5c2">
<label>5.3.2</label>
<title>Exposure phase</title>
<p>Participants completed an oddball-detection task during the exposure phase. In each trial, participants were presented with an audiovisual stimulus pair with a fixed SOA (adapter SOA). In 10% of trials, the intensity of either the visual or the auditory component (or both) was greater than in the other trials. Participants were instructed to press the corresponding button as soon as possible to indicate whether there was an auditory oddball, a visual oddball, or both stimuli were oddballs. The task timing (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>) was almost identical to the ternary TOJ task, except that there was a response time limit of 1.4 s. Prior to the exposure phase, participants practiced the task for as long as needed to familiarize themselves with the task. During this practice, they were presented with bimodal stimuli with the same adapter SOA used in the exposure phase. There were a total of 250 trials, divided into five blocks. At the end of each block, we presented a performance summary with the hit rate and false alarm rate of each modality. Participants usually took 15 minutes to complete the exposure phase.</p>
</sec>
<sec id="s5c3">
<label>5.3.3</label>
<title>Post-test phase</title>
<p>Participants completed the ternary TOJ task as well as the oddball-detection task during the post-test phase. Specifically, each temporal-order judgment was preceded by three top-up (oddball-detection) trials. The adapter SOA in the top-up trials was the same as that in the exposure phase to prevent dissipation of temporal recalibration (<xref ref-type="bibr" rid="c28">Machulla et al., 2012</xref>). Both visual and auditory <italic>d</italic>′ remained consistent from the exposure to post-test phases, indicating similar performance in the top-up trials to performance during the exposure phase (Supplement Figure S10). To facilitate task switching, the ITI between the last top-up trial and the following TOJ trial was longer (with the additional time jittered around 1 s). Additionally, the fixation cross was displayed in red to signal the start of a TOJ trial. As in the pre-test phase, there were 300 TOJ trials (15 test SOAs <italic>×</italic> 20 repetitions) with the addition of 900 top-up trials, grouped into six blocks. At the end of each block, we provided a summary of the oddball-detection performance. Participants usually took around 1 hour to complete the post-test phase.</p>
</sec>
<sec id="s5c4">
<label>5.3.4</label>
<title>Intensity-discrimination task</title>
<p>This task was conducted to estimate the just-noticeable-difference (JND) in intensity for a standard visual stimulus with a luminance of 36.1 cd<italic>/</italic>m<sup>2</sup> and a standard auditory stimulus with a volume of 40 dB SPL. The task was two-interval, forced choice. The trial started with a fixation (0.1–0.2 s) and a blank screen (0.4–0.6 s). Participants were presented with a standard stimulus (0.033 s) in one randomly selected interval and a comparison stimulus (0.033 s) in the other interval, temporally separated by an inter-stimulus interval (0.6–0.8 s). They indicated which interval contained the brighter/louder stimulus without time constraint. Seven test stimulus levels (luminance range: 5%–195% relative to the standard visual stimulus intensity; volume range: 50%–150% relative to the standard auditory stimulus’ amplitude) were repeated 20 times, resulting in 140 trials for each task. We fit a cumulative Gaussian distribution function to these data and defined the oddball as an auditory or visual stimulus with an intensity judged as more intense than the standard 90% of the time. A higher probability than the standard JND of 75% was selected because the pilot studies showed that the harder oddball detection task became too demanding during the one-hour post-test.</p>
</sec>
</sec>
<sec id="s5d">
<label>5.4</label>
<title>Modeling</title>
<p>In this section, we first outline general assumptions, shared across all candidate models, regarding sensory noise, measurements, and bias. Then, we formalize three process models of recalibration that differ in the implementation of recalibration. In each recalibration model, we also provide a formalization of the ternary TOJ task administered in the pre- and the post-test phases, data from which were used to constrain the model parameters. Finally, we describe how the models were fit to the data.</p>
<sec id="s5d1">
<label>5.4.1</label>
<title>General modal assumptions regarding sensory noise, measurements and bias</title>
<p>When an audiovisual stimulus pair with a SOA, <italic>s</italic> = <italic>t</italic><sub><italic>A</italic></sub> <italic>− t</italic><sub><italic>V</italic></sub>, is presented, it triggers auditory and visual signals that are registered in the relevant region of cortex where audiovisual temporal-order comparisons are made. This leads to two internal measurements of the arrival time for each signal in an observer’s brain. These arrival times are subject to noise and thus vary across presentations of the same physical stimulus pair. As in previous work (<xref ref-type="bibr" rid="c14">García-Pérez &amp; Alcalá-Quintana, 2012</xref>), we model the probability distribution of the arrival time as shifted exponential distributions (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). The arrival time of the auditory signal relative to onset <italic>t</italic><sub><italic>A</italic></sub> is the sum of the fixed delay of internal signal, <italic>β</italic><sub><italic>A</italic></sub>, and an additional random delay that is exponentially distributed with time constant <italic>τ</italic><sub><italic>A</italic></sub>; analogous for the visual latency (with delay <italic>β</italic><sub><italic>V</italic></sub> and time constant <italic>τ</italic><sub><italic>V</italic></sub>).</p>
<p>The measured SOA of the audiovisual stimulus pair is modeled as the difference of the arrival times of the two stimuli. Thus, the sensory measurement of SOA, <italic>m</italic>, reflects the sum of three components: the physical SOA, <italic>s</italic>; a fixed latency that is the difference between the auditory and visual fixed delay, <italic>β</italic><sub>pre</sub> = <italic>β</italic><sub><italic>A</italic></sub> <italic>− β</italic><sub><italic>V</italic></sub>; and the difference between two exponentially distributed random delays. A negative value of <italic>β</italic><sub>pre</sub> indicates faster auditory processing. We assume that the audiovisual fixed latency corresponds to the observer’s default audiovisual temporal bias (Badde, Ley, et al., 2020; <xref ref-type="bibr" rid="c15">Grabot &amp; van Wassenhove, 2017</xref>). Thus, we assume that after leaving the experimental room, the default bias is restored and thus consistent across pre-tests.</p>
<p>We model the recalibration process as a shift of the audiovisual temporal bias at the end of every exposure trial <italic>i, β</italic><sub><italic>i</italic></sub> = <italic>β</italic><sub>pre</sub> +Δ<sub><italic>β,i</italic></sub>, where <italic>β</italic><sub><italic>i</italic></sub> is the current audiovisual bias, and Δ<sub><italic>β,i</italic></sub> is the cumulative shift of audiovisual temporal bias. After the 250 exposure trials the updated biases can be expresses as <italic>β</italic><sub>post</sub> = <italic>β</italic><sub>pre</sub> +Δ<sub><italic>β</italic>,250</sub>. We also assume that the amounts of auditory and visual latency noise, <italic>τ</italic><sub><italic>A</italic></sub> and <italic>τ</italic><sub><italic>V</italic></sub>, remain constant across phases and sessions.</p>
<p>Given that both latency distributions are shifted exponential distributions, the probability density function of the sensory measurements of SOA, <italic>m</italic>, given physical SOA, <italic>s</italic>, is a double-exponential function (see derivation in Supplement Section 3; <xref rid="fig6" ref-type="fig">Figure 6A</xref>):
<disp-formula id="eqn1">
<graphic xlink:href="584189v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
    <caption><title>Simulating responses of the TOJ task with a causal-inference perceptual process.</title>
    <p>(A) An example probability density for the measurement of a zero SOA. (B) The probability density of estimates resulting from a zero-SOA stimulus based on simulation using the causal-inference process. The symmetrical criteria around zero partition the distribution of estimated SOA into three regions, coded by different colors. The area under each segment of the estimate distribution corresponds to the probabilities of the three possible intended responses for a zero SOA. (C) The simulated psychometric function computed by repeatedly calculating the probabilities of the three response types across all test SOAs.</p></caption>
<graphic xlink:href="584189v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The probability density function of measured SOA peaks at the physical SOA of the stimuli plus the participant’s audiovisual temporal bias, <italic>s</italic><sub><italic>i</italic></sub> + <italic>β</italic><sub><italic>i</italic></sub>. The left and right spread of this measurement distribution depends on the amount of the latency noise for the visual, <italic>τ</italic><sub><italic>V</italic></sub>, and auditory, <italic>τ</italic><sub><italic>A</italic></sub>, signals. In models with modality-independent arrival-time precision, <italic>τ</italic><sub><italic>A</italic></sub> = <italic>τ</italic><sub><italic>V</italic></sub> and the measurement distribution is symmetrical. This symmetrical measurement distribution is often approximated by a Gaussian distribution to fit TOJ responses in previous temporal-recalibration studies (<xref ref-type="bibr" rid="c11">Di Luca et al., 2009</xref>; <xref ref-type="bibr" rid="c13">Fujisaki et al., 2004</xref>; <xref ref-type="bibr" rid="c17">Harrar &amp; Harris, 2005</xref>; <xref ref-type="bibr" rid="c25">Keetels &amp; Vroomen, 2007</xref>; <xref ref-type="bibr" rid="c34">Navarra et al., 2005</xref>; <xref ref-type="bibr" rid="c50">Tanaka et al., 2011</xref>; <xref ref-type="bibr" rid="c53">Vatakis et al., 2007</xref>, <xref ref-type="bibr" rid="c54">2008</xref>; <xref ref-type="bibr" rid="c57">Vroomen et al., 2004</xref>). Note that we assume the observer has perfect knowledge of the visual and auditory latency noise. Thus, the density of the measurement distribution corresponds to the likelihood function during the inference process when the observer only has the noisy measurement, <italic>m</italic>, and needs to infer the physical SOA, <italic>s</italic>.</p>
</sec>
<sec id="s5d2">
<label>5.4.2</label>
<title>The causal-inference model</title>
<sec id="s5d2a">
<title>Formalization of recalibration in the exposure phase</title>
<p>The causal-inference model assumes that, at the end of every exposure trial <italic>i</italic>, a discrepancy between the measured SOA, <italic>m</italic><sub><italic>i</italic></sub>, and the final estimate of the stimulus SOA, <italic>ŝ</italic><sub><italic>i</italic></sub>, signals the need for recalibration. The cumulative shift of audiovisual temporal bias Δ<sub><italic>β,i</italic></sub> after exposure trial <italic>i</italic> is,
<disp-formula id="eqn2">
<graphic xlink:href="584189v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>α</italic> is the learning rate.</p>
<p>The ideal observer infers intermediate location estimates for two causal scenarios: the auditory and visual stimuli can arise from a single cause (<italic>C</italic> = 1) or two independent causes (<italic>C</italic> = 2). The posterior distribution of the SOA, <italic>s</italic>, conditioned on each causal scenario is computed by multiplying the likelihood function (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>) with the corresponding prior over SOA. In the case of a common cause (<italic>C</italic> = 1), the prior distribution of the SOA between sound and light is a Gaussian distribution (<xref ref-type="bibr" rid="c30">Magnotti et al., 2013</xref>; <xref ref-type="bibr" rid="c32">McGovern et al., 2016</xref>), <inline-formula><inline-graphic xlink:href="584189v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. To maintain consistency with previous studies, we used an unbiased prior which assigns the highest probability to a physically synchronous stimulus pair <italic>s</italic> = 0. Similarly, the prior distribution conditioned on separate causes (<italic>C</italic> = 2) is also a Gaussian distribution, <inline-formula><inline-graphic xlink:href="584189v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, with a much larger spread compared to the common-cause scenario. The intermediate estimates <italic>ŝ</italic><sub><italic>C</italic>=1</sub> conditioned on the common-cause scenario and <italic>ŝ</italic><sub><italic>C</italic>=2</sub> conditioned on separate-cause scenario are the maximum-a-posteriori estimates of conditional posteriors, which are approximated numerically as there is no closed-form solution.</p>
<p>The final estimate of the stimulus SOA, <italic>ŝ</italic>, depends on the posterior probability of each causal scenario. According to Bayes Rule, the posterior probability that an audiovisual stimulus pair with the measured SOA, <italic>m</italic>, shares a common cause is
<disp-formula id="eqn3">
<graphic xlink:href="584189v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The likelihood of a common source/separate sources for a fixed SOA measurement was approximated by numerically integrating the scenario-specific protoposterior (i.e., the unnormalized posterior),
<disp-formula id="eqn4">
<graphic xlink:href="584189v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The posterior probability of a common cause additionally depends on the observer’s prior belief of a common cause for auditory and visual stimuli, <italic>P</italic> (<italic>C</italic> = 1) = <italic>p</italic><sub>common</sub>.</p>
<p>The final estimate of SOA was derived by model averaging, i.e., the average of the scenario-specific SOA estimates, <italic>ŝ</italic><sub><italic>C</italic>=1</sub> and <italic>ŝ</italic><sub><italic>C</italic>=2</sub> each weighted by the posterior probability of the corresponding causal scenario,
<disp-formula id="eqn5">
<graphic xlink:href="584189v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s5d2b">
<title>Formalization of the ternary TOJ task with a causal-inference perceptual process</title>
<p>In the ternary TOJ task administered in the pre- and post-test phases, the observer is presented with an audiovisual stimulus pair and has to decide whether the auditory stimulus was presented first, the visual stimulus was presented first, or both of them were presented at the same time. The observer makes this perceptual judgment by comparing the final estimate of the SOA, <italic>ŝ</italic>, to two internal criteria (<xref ref-type="bibr" rid="c10">Cary et al., 2024</xref>; <xref ref-type="bibr" rid="c14">García-Pérez &amp; Alcalá-Quintana, 2012</xref>). We assume that the observer has a symmetric pair of criteria, ±<italic>c</italic>, centered on the stimulus SOA corresponding to perceptual simultaneity (<italic>ŝ</italic> = 0). In addition, the observer may lapse or make an error when responding by a lapse rate, <italic>λ</italic>. The probabilities of reporting visual lead, Ψ<sub><italic>V</italic></sub>, auditory lead, Ψ<sub><italic>A</italic></sub>, or that the two stimuli were simultaneous, Ψ<sub><italic>S</italic></sub>, are thus
<disp-formula id="eqn6">
<graphic xlink:href="584189v2_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The probability distribution of causal-inference-based SOA estimates <italic>P</italic> (<italic>ŝ</italic>|<italic>s</italic>) has no closed form distribution function and thus was approximated using simulations, resulting in <inline-formula><inline-graphic xlink:href="584189v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. <xref rid="fig6" ref-type="fig">Figure 6</xref> illustrates the process of simulating the psychometric functions, using a zero test SOA as an example. First, we sampled 10,000 SOA measurements from the double-exponential probability distribution corresponding to the test SOA of zero (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). Second, for each sampled measurement, we simulated the process by which the observer carries out causal inference and by doing so produced an estimate of the stimulus SOA, while keeping the causal-inference model parameters fixed. This process resulted in a Monte-Carlo approximation of the probability density distribution of the causal-inference-based SOA estimates (<xref rid="fig6" ref-type="fig">Figure 6B</xref>). Third, we calculated the probability of the three types of responses (<xref ref-type="disp-formula" rid="eqn6">Eq. 6</xref>) for this specific test SOA. This process was repeated for each test SOA to generate three psychometric functions (<xref rid="fig6" ref-type="fig">Figure 6C</xref>).</p>
</sec>
</sec>
<sec id="s5d3">
<label>5.4.3</label>
<title>The asynchrony-contingent model</title>
<p>In the asynchrony-contingent model, the observer measures the audiovisual SOA, <italic>s</italic>, by comparing the arrival latency of the auditory and visual signals. The observer uses the likelihood that the audiovisual stimuli occurred simultaneously <italic>P</italic> (<italic>m</italic>|<italic>SOA</italic> = 0) to update the temporal bias during recalibration, instead of performing causal inference. We again assume that the observer has perfect knowledge about the variability and fixed delays of the arrival times and thus assume the likelihood corresponds to the measurement distribution (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>). The observer uses this probability of simultaneity to scale the update rate of the audiovisual bias,
<disp-formula id="eqn7">
<graphic xlink:href="584189v2_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We assume the observer’s estimate of the stimulus SOA, <italic>ŝ</italic>, is identical to the measured SOA, <italic>m</italic>. Thus, from the experimenter’s perspective, the probability of the three different responses in the TOJ task can be obtained by replacing the SOA estimate, <italic>ŝ</italic>, with the SOA measurement, <italic>m</italic>, in <xref ref-type="disp-formula" rid="eqn6">Eq. 6</xref>). As we know the probability distribution of <italic>m</italic>, the psychometric functions have a closed form (<xref ref-type="bibr" rid="c14">García-Pérez &amp; Alcalá-Quintana, 2012</xref>).</p>
</sec>
<sec id="s5d4">
<label>5.4.4</label>
<title>The asynchrony-correction model</title>
<p>In the asynchrony-correction model, the observer begins by evaluating if the sensory measurement of SOA, <italic>m</italic>, falls outside the criterion range for reporting that the two stimuli were presented simultaneously ±<italic>c</italic>. If the measurement does exceed this criterion, the observer adjusts the audiovisual bias by shifting it against the measurement, i.e., shifting it so that the measured SOA of a pair would be closer to zero and is more likely to perceived as simultaneous. This adjustment is proportional to the sensory measurement of the SOA, <italic>m</italic>, at a fixed rate determined by the learning rate <italic>α</italic>. The update rule of the audiovisual bias in trial <italic>i</italic> is thus
<disp-formula id="eqn8">
<graphic xlink:href="584189v2_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The derivation of the psychometric functions is identical to the asynchrony-contingent model.</p>
</sec>
<sec id="s5d5">
<label>5.4.5</label>
<title>Model fitting</title>
<sec id="s5d5a">
<title>Model log-likelihood</title>
<p>The model was fitted by optimizing the lower bound on the marginal log-likelihood. We fit the model to the ternary TOJ data collected during the pre- and post-test phases of all sessions together. We did not collect temporal-order judgments in the exposure phase. But, to model the post-test data, we need to estimate the distribution of shifts of audiovisual bias resulting from the exposure phase (Δ<sub><italic>β</italic>,250</sub>). We do this using Monte Carlo simulation of the 250 exposure trials to estimate the probability distribution of the cumulative shifts.</p>
<p>The set of model parameters Θ is listed in <xref rid="tbl1" ref-type="table">Table 1</xref>. There are <italic>J</italic> sessions, each including <italic>K</italic> trials in the pre-test phase and <italic>K</italic> trials in the post-test phase. We denote the full dataset of pre-test data as <italic>X</italic><sub>pre</sub> and for the post-test data as <italic>X</italic><sub>post</sub>. We fit the pre- and post-test data jointly by summing their log-likelihood, log <italic>p</italic>(<italic>X</italic>|<italic>M</italic>, Θ) = log <italic>p</italic>(<italic>X</italic><sub>pre</sub>|<italic>M</italic>, Θ) + log <italic>p</italic>(<italic>X</italic><sub>post</sub>|<italic>M</italic>, Θ).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
    <caption><title>Model parameters.</title>
    <p>Check marks signify that the parameter is used for determining the likelihood of the data from the temporal-order judgment task in the pre- and post-test phase and/or for the Monte Carlo simulation of recalibration in the exposure phase.</p></caption>
<graphic xlink:href="584189v2_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>In a given trial, the observer responded either auditory-first (A), visual-first (V), or simultaneous (S). We denote a single response using indicator variables that are equal to 1 if that was the response in that trial and 0 otherwise. These variables for trial <italic>k</italic> in session <italic>j</italic> are <inline-formula><inline-graphic xlink:href="584189v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="584189v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the pre-test trials, and <inline-formula><inline-graphic xlink:href="584189v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, etc., for the post-test trials.</p>
<p>The log-likelihood of all pre-test responses <italic>X</italic><sub>pre</sub> given the model parameters is
<disp-formula id="eqn9">
<graphic xlink:href="584189v2_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The psychometric functions for the pre-test (e.g., Ψ<sub><italic>A</italic>,pre</sub>) are defined in <xref ref-type="disp-formula" rid="eqn6">Eq. 6</xref>, and are the same across all sessions as we assumed that the audiovisual bias <italic>β</italic><sub>pre</sub> was the same before recalibration in every session.</p>
<p>The log-likelihood of responses in the post-test depends on the audiovisual bias after recalibration <italic>β</italic><sub>post,<italic>j</italic></sub> = <italic>β</italic><sub>pre</sub> + Δ<sub><italic>β</italic>,250,<italic>j</italic></sub> for session <italic>j</italic>. To determine the log-likelihood of the post-test data requires us to integrate out the unknown value of the cumulative shift Δ<sub><italic>β</italic>,250,<italic>j</italic></sub>. We approximated this integral in two steps based on our previous work (<xref ref-type="bibr" rid="c23">Hong et al., 2021</xref>). First, we simulated the 250 exposure trials 1000 times for a given set of parameters Θ and session <italic>j</italic>. This resulted in 1,000 values of Δ<sub><italic>β</italic>,250,<italic>j</italic></sub>. The distribution of these values was well fit by a Gaussian whose parameters were determined by the empirical mean and standard deviation of the sample distribution, resulting in the distribution <inline-formula><inline-graphic xlink:href="584189v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Second, we approximated the integral of the log-likelihood of the data over possible values of Δ<sub><italic>β</italic>,250,<italic>j</italic></sub> by numerical integration. We discretized the approximated distribution <inline-formula><inline-graphic xlink:href="584189v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> into 100 equally spaced bins centered on values Δ<sub><italic>β</italic>,250,<italic>j</italic></sub>(<italic>n</italic>) (<italic>n</italic> = 1, · · ·, 100). The range of the bins was triple the range of the values from the Monte Carlo sample, so that the lower bound was <inline-formula><inline-graphic xlink:href="584189v2_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the upper bound was <inline-formula><inline-graphic xlink:href="584189v2_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>The log-likelihood of the post-test data was approximated as
<disp-formula id="eqn10">
<graphic xlink:href="584189v2_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="eqn11">
<graphic xlink:href="584189v2_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The psychometric functions in the post-test (e.g., Ψ<sub><italic>A</italic>,post,<italic>jn</italic></sub>) differed across sessions and bins because the simulated audiovisual bias after the exposure phase <italic>β</italic><sub>post,<italic>j</italic></sub> depends on the adapter SOA fixed in session <italic>j</italic> and the simulation bin <italic>n</italic>.</p>
</sec>
<sec id="s5d5b">
<title>Parameter estimation and model comparison</title>
<p>We approximated the lower bounds to the model evidence (i.e., the marginal likelihood) of each model for each participant’s data using Variational Bayesian Monte Carlo (<xref ref-type="bibr" rid="c1">Acerbi, 2018</xref>, <xref ref-type="bibr" rid="c2">2020</xref>). We set the prior distribution of parameters based on the results of maximum likelihood estimation using Bayesian Adaptive Direct Search to ensure that the parameter ranges were plausible (<xref ref-type="bibr" rid="c3">Acerbi &amp; Ma, 2017</xref>). We repeated each search 20 times with a different and random starting point to address the possibility of reporting a local minimum. For each model, the fit with the maximum lower bounds of the model evidence across the repeated searches was chosen for the maximum model evidence and best parameter estimates.</p>
<p>We then conducted a Bayesian model comparison based on model evidence. The model with the strongest evidence was considered the best-fitting model (<xref ref-type="bibr" rid="c29">MacKay, 2003</xref>). To quantify the support of model selection, we computed the Bayes factor, the ratio of the model evidence between each model and the asynchrony-correction, modality-independent-precision model, which had the weakest model evidence. To compare any two models, one can simply calculate the difference in their log Bayes factors as both are relative to the same weakest model.</p>
</sec>
<sec id="s5d5c">
<title>Model recovery and parameter recovery</title>
<p>We conducted a model-recovery analysis for the six models and confirmed that they are identifiable (Supplement Section 11). In addition, we considered an alternative causal-inference model in which the bias update is proportional to the posterior probability of a common cause, instead of driven by the percept. A separate model recovery analysis on variations of the causal-inference model was unable to distinguish between them (Supplement Section 12). For the causal-inference, modality-specific-precision model, we also carried out a parameter recovery analysis and confirmed that all the parameters are recoverable (Supplement Section 13).</p>
</sec>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s6a" sec-type="data-availability">
<title>Data and code availability</title>
<p>All data and code are available via the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/8s7qv/">https://osf.io/8s7qv/</ext-link>).</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We thank the NYU High-Performance Computing (NYU HPC) for providing computational resources and support. We thank the anonymous reviewers’ advice that helped us improve the manuscript.</p>
</ack>
<sec id="d1e1766" sec-type="additional-information">
<title>Additional information</title>
<sec id="s7">
<title>Funding</title>
<p>This research was funded by NIH EY08266.</p>
</sec>
</sec>
<sec id="suppd1e1766" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1757">
<label>Supplementary Materials</label>
<media xlink:href="supplements/584189_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Acerbi</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Variational Bayesian Monte Carlo</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Acerbi</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Variational Bayesian Monte Carlo with noisy likelihoods</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>33</volume>, <fpage>8211</fpage>–<lpage>8222</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Acerbi</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Ma</surname>, <given-names>W. J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Practical bayesian optimization for model fitting with bayesian adaptive direct search</article-title>. <conf-name>Proceedings of the 31st International Conference on Neural Information Processing Systems</conf-name>, <fpage>1834</fpage>–<lpage>1844</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name>, &amp; <string-name><surname>Adams</surname>, <given-names>W. J.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Multisensory causal inference is feature-specific, not object-based</article-title>. <source>Philos. Trans. R. Soc. Lond. B Biol. Sci</source>., <volume>378</volume> (<issue>1886</issue>), <fpage>20220345</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ley</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rajendran</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Shareef</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kekunnaya</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Röder</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Sensory experience during early sensitive periods shapes cross-modal temporal biases</article-title>. <source>Elife</source>, <volume>9</volume>, <elocation-id>e61238</elocation-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Navarro</surname>, <given-names>K. T.</given-names></string-name>, &amp; <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Modality-specific attention attenuates visual-tactile integration and recalibration effects by reducing prior expectations of a common source for vision and touch</article-title>. <source>Cognition</source>, <volume>197</volume>, <fpage>104170</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beierholm</surname>, <given-names>U. R.</given-names></string-name>, <string-name><surname>Quartz</surname>, <given-names>S. R.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Bayesian priors are encoded independently from likelihoods in human multisensory perception</article-title>. <source>J. Vis</source>., <volume>9</volume> (<issue>5</issue>), <fpage>23</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berniker</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kording</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Estimating the sources of motor errors for adaptation and generalization</article-title>. <source>Nat. Neurosci</source>., <volume>11</volume> (<issue>12</issue>), <fpage>1454</fpage>–<lpage>1461</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Stetson</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Eagleman</surname>, <given-names>D. M.</given-names></string-name></person-group> (<year>2012</year>). <article-title>A neural model for temporal order judgments and their active recalibration: A common mechanism for space and time?</article-title> <source>Front. Psychol</source>., <volume>3</volume>, <fpage>470</fpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cary</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lahdesmaki</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Audiovisual simultaneity windows reflect temporal sensory uncertainty</article-title>. <source>Psychon. Bull. Rev</source>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Luca</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Machulla</surname>, <given-names>T.-K.</given-names></string-name>, &amp; <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Recalibration of multisensory simultaneity: Cross-modal transfer coincides with a change in perceptual latency</article-title>. <source>J. Vis</source>., <volume>9</volume> (<issue>12</issue>), <fpage>7</fpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fain</surname>, <given-names>G. L.</given-names></string-name></person-group> (<year>2019</year>). <source>Sensory transduction</source> (<edition>2nd</edition> ed.). <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fujisaki</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Shimojo</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kashino</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Nishida</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Recalibration of audio-visual simultaneity</article-title>. <source>Nat. Neurosci</source>., <volume>7</volume> (<issue>7</issue>), <fpage>773</fpage>–<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>García-Pérez</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Alcalá-Quintana</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2012</year>). <article-title>On the discrepant results in synchrony judgment and temporal-order judgment tasks: A quantitative model</article-title>. <source>Psychon. Bull. Rev</source>., <volume>19</volume> (<issue>5</issue>), <fpage>820</fpage>–<lpage>846</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grabot</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>van Wassenhove</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Time order as psychological bias</article-title>. <source>Psychol. Sci</source>., <volume>28</volume> (<issue>5</issue>), <fpage>670</fpage>–<lpage>678</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hanson</surname>, <given-names>J. V. M.</given-names></string-name>, <string-name><surname>Heron</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Whitaker</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Recalibration of perceived time across sensory modalities</article-title>. <source>Exp. Brain Res</source>., <volume>185</volume> (<issue>2</issue>), <fpage>347</fpage>–<lpage>352</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harrar</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Harris</surname>, <given-names>L. R.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Simultaneity constancy: Detecting events with touch and vision</article-title>. <source>Exp. Brain Res</source>., <volume>166</volume> (<issue>3-4</issue>), <fpage>465</fpage>–<lpage>473</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harrar</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Harris</surname>, <given-names>L. R.</given-names></string-name></person-group> (<year>2008</year>). <article-title>The effect of exposure to asynchronous audio, visual, and tactile stimulus combinations on the perception of simultaneity</article-title>. <source>Exp. Brain Res</source>., <volume>186</volume> (<issue>4</issue>), <fpage>517</fpage>–<lpage>524</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heron</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Roach</surname>, <given-names>N. W.</given-names></string-name>, <string-name><surname>Hanson</surname>, <given-names>J. V. M.</given-names></string-name>, <string-name><surname>McGraw</surname>, <given-names>P. V.</given-names></string-name>, &amp; <string-name><surname>Whitaker</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Audiovisual time perception is spatially specific</article-title>. <source>Exp. Brain Res</source>., <volume>218</volume> (<issue>3</issue>), <fpage>477</fpage>–<lpage>485</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heron</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Whitaker</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>McGraw</surname>, <given-names>P. V.</given-names></string-name>, &amp; <string-name><surname>Horoshenkov</surname>, <given-names>K. V.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Adaptation minimizes distance-related audiovisual delays</article-title>. <source>J. Vis</source>., <volume>7</volume> (<issue>13</issue>), <fpage>5</fpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hirsh</surname>, <given-names>I. J.</given-names></string-name>, &amp; <string-name><surname>Sherrick</surname>, <given-names>C. E.</given-names>, <suffix>Jr</suffix></string-name></person-group>. (<year>1961</year>). <article-title>Perceived order in different sense modalities</article-title>. <source>J. Exp. Psychol</source>., <volume>62</volume> (<issue>5</issue>), <fpage>423</fpage>–<lpage>432</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hong</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2023</year>, <month>May</month>). <source>The role of causal inference in multisensory integration and recalibration [</source><publisher-name>Doctoral dissertation</publisher-name>, <publisher-loc>New York University]</publisher-loc>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hong</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Badde</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Landy</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Causal inference regulates audiovisual spatial recalibration via its influence on audiovisual perception</article-title>. <source>PLoS Comput. Biol</source>., <volume>17</volume> (<issue>11</issue>), <fpage>e1008877</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hsiao</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lee-Miller</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Block</surname>, <given-names>H. J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Conscious awareness of a visuo-proprioceptive mismatch: Effect on cross-sensory recalibration</article-title>. <source>Front. Neurosci</source>., <volume>16</volume>, <fpage>958513</fpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keetels</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2007</year>). <article-title>No effect of auditory-visual spatial disparity on temporal recalibration</article-title>. <source>Exp. Brain Res</source>., <volume>182</volume> (<issue>4</issue>), <fpage>559</fpage>–<lpage>565</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>King</surname>, <given-names>A. J.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Multisensory integration: Strategies for synchronization</article-title>. <source>Curr. Biol</source>., <volume>15</volume> (<issue>9</issue>), <fpage>R339</fpage>–<lpage>41</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Körding</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Beierholm</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Quartz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Causal inference in multisensory perception</article-title>. <source>PLoS One</source>, <volume>2</volume> (<issue>9</issue>), <fpage>e943</fpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Machulla</surname>, <given-names>T.-K.</given-names></string-name>, <string-name><surname>Di Luca</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Froehlich</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Multisensory simultaneity recalibration: Storage of the aftereffect in the absence of counterevidence</article-title>. <source>Exp. Brain Res</source>., <volume>217</volume> (<issue>1</issue>), <fpage>89</fpage>–<lpage>97</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>MacKay</surname>, <given-names>D. J. C.</given-names></string-name></person-group> (<year>2003</year>). <source>Information theory, inference and learning algorithms</source>. <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Magnotti</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>W. J.</given-names></string-name>, &amp; <string-name><surname>Beauchamp</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Causal inference of asynchronous audiovisual speech</article-title>. <source>Front. Psychol</source>., <volume>4</volume>, <fpage>798</fpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maij</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Brenner</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Smeets</surname>, <given-names>J. B. J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Temporal information can influence spatial localization</article-title>. <source>J. Neurophysiol</source>., <volume>102</volume> (<issue>1</issue>), <fpage>490</fpage>–<lpage>495</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGovern</surname>, <given-names>D. P.</given-names></string-name>, <string-name><surname>Roudaia</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Newell</surname>, <given-names>F. N.</given-names></string-name>, &amp; <string-name><surname>Roach</surname>, <given-names>N. W.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Perceptual learning shapes multisensory causal inference via two distinct mechanisms</article-title>. <source>Sci. Rep</source>., <volume>6</volume>, <fpage>24673</fpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Navarra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hartcher-O’Brien</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Piazza</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Adaptation to audiovisual asynchrony modulates the speeded detection of sound</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>., <volume>106</volume> (<issue>23</issue>), <fpage>9169</fpage>–<lpage>9173</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Navarra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Vatakis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zampini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Soto-Faraco</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Humphreys</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Exposure to asynchronous audiovisual speech extends the temporal window for audiovisual integration</article-title>. <source>Brain Res. Cogn. Brain Res</source>., <volume>25</volume> (<issue>2</issue>), <fpage>499</fpage>–<lpage>507</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Donohue</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lacherez</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Yamamoto</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Musical training refines audio-visual integration but does not influence temporal recalibration</article-title>. <source>Sci. Rep</source>., <volume>12</volume>, <fpage>15292</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Pöppel</surname>, <given-names>E.</given-names></string-name></person-group> (<year>1988</year>). <source>Mindworks: Time and conscious experience</source>. <publisher-name>Harcourt Brace Jovanovich</publisher-name>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roach</surname>, <given-names>N. W.</given-names></string-name>, <string-name><surname>Heron</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Whitaker</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>McGraw</surname>, <given-names>P. V.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Asynchrony adaptation reveals neural population code for audio-visual timing</article-title>. <source>Proc. Biol. Sci</source>., <volume>278</volume> (<issue>1710</issue>), <fpage>1314</fpage>–<lpage>1322</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rohde</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name></person-group> (<year>2012</year>). <article-title>To lead and to lag - forward and backward recalibration of perceived visuo-motor simultaneity</article-title>. <source>Front. Psychol</source>., <volume>3</volume>, <fpage>599</fpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rohde</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Greiner</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Asymmetries in visuomotor recalibration of time perception: Does causal binding distort the window of integration?</article-title> <source>Acta Psychol</source>., <volume>147</volume>, <fpage>127</fpage>–<lpage>135</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rohe</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Sensory reliability shapes perceptual inference via two mechanisms</article-title>. <source>J. Vis</source>., <volume>15</volume> (<issue>5</issue>), <fpage>22</fpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roseboom</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Arnold</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Twice upon a time: Multiple concurrent temporal recalibrations of audiovisual speech</article-title>. <source>Psychol. Sci</source>., <volume>22</volume> (<issue>7</issue>), <fpage>872</fpage>–<lpage>877</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roseboom</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Kawabe</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Nishida</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Audio-Visual temporal recalibration can be constrained by content cues regardless of spatial overlap</article-title>. <source>Front. Psychol</source>., <volume>4</volume>, <fpage>189</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roseboom</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Linares</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Nishida</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Sensory adaptation for timing perception</article-title>. <source>Proc. Biol. Sci</source>., <volume>282</volume> (<issue>1805</issue>), <fpage>20142833</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sato</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Comparing bayesian models for simultaneity judgement with different causal assumptions</article-title>. <source>J. Math. Psychol</source>., <volume>102</volume> (<issue>102521</issue>), <fpage>102521</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sato</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Aihara</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2011</year>). <article-title>A bayesian model of sensory adaptation</article-title>. <source>PLoS One</source>, <volume>6</volume> (<issue>4</issue>), <fpage>e19377</fpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sato</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Toyoizumi</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Aihara</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Bayesian inference explains perception of unity and ventriloquism aftereffect: Identification of common sources of audiovisual stimuli</article-title>. <source>Neural Comput</source>., <volume>19</volume> (<issue>12</issue>), <fpage>3335</fpage>–<lpage>3355</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Beierholm</surname>, <given-names>U. R.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Causal inference in perception</article-title>. <source>Trends Cogn. Sci</source>., <volume>14</volume> (<issue>9</issue>), <fpage>425</fpage>–<lpage>432</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Squire</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Multisensory integration: Maintaining the perception of synchrony</article-title>. <source>Curr. Biol</source>., <volume>13</volume> (<issue>13</issue>), <fpage>R519</fpage>–<lpage>21</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sternberg</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Knoll</surname>, <given-names>R. L.</given-names></string-name></person-group> (<year>1973</year>). <source>The perception of temporal order: Fundamental issues and a general model</source>. <publisher-name>Academic Press</publisher-name></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanaka</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Asakawa</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Imai</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2011</year>). <article-title>The change in perceptual synchrony between auditory and visual speech after exposure to asynchronous speech</article-title>. <source>Neuroreport</source>, <volume>22</volume> (<issue>14</issue>), <fpage>684</fpage>–<lpage>688</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van der Burg</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Cass</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Rapid recalibration to audiovisual asynchrony</article-title>. <source>J. Neurosci</source>., <volume>33</volume> (<issue>37</issue>), <fpage>14633</fpage>–<lpage>14637</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Beers</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Haggard</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2002</year>). <article-title>When feeling is more important than seeing in sensorimotor adaptation</article-title>. <source>Curr. Biol</source>., <volume>12</volume> (<issue>10</issue>), <fpage>834</fpage>–<lpage>837</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vatakis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Navarra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Soto-Faraco</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Temporal recalibration during asynchronous audiovisual speech perception</article-title>. <source>Exp. Brain Res</source>., <volume>181</volume> (<issue>1</issue>), <fpage>173</fpage>–<lpage>181</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vatakis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Navarra</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Soto-Faraco</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Spence</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Audiovisual temporal adaptation of speech: Temporal order versus simultaneity judgments</article-title>. <source>Exp. Brain Res</source>., <volume>185</volume> (<issue>3</issue>), <fpage>521</fpage>–<lpage>529</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>de Gelder</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Temporal ventriloquism: Sound modulates the flash-lag effect</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>., <volume>30</volume> (<issue>3</issue>), <fpage>513</fpage>–<lpage>518</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Keetels</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Perception of intersensory synchrony: A tutorial review</article-title>. <source>Atten. Percept. Psychophys</source>., <volume>72</volume> (<issue>4</issue>), <fpage>871</fpage>–<lpage>884</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Keetels</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>de Gelder</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Bertelson</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Recalibration of temporal order perception by exposure to audio-visual asynchrony</article-title>. <source>Brain Res. Cogn. Brain Res</source>., <volume>22</volume> (<issue>1</issue>), <fpage>32</fpage>–<lpage>35</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Körding</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Relevance of error: What drives motor adaptation?</article-title> <source>J. Neurophysiol</source>., <volume>101</volume> (<issue>2</issue>), <fpage>655</fpage>–<lpage>664</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wozny</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Beierholm</surname>, <given-names>U. R.</given-names></string-name>, &amp; <string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Probability matching as a computational strategy used in perception</article-title>. <source>PLoS Comput. Biol</source>., <volume>6</volume> (<issue>8</issue>), <fpage>e1000871</fpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarrow</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jahn</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Durant</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Arnold</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Shifts of criteria or neural timing? the assumptions underlying timing perception studies</article-title>. <source>Conscious. Cogn</source>., <volume>20</volume> (<issue>4</issue>), <fpage>1518</fpage>–<lpage>1531</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarrow</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kohl</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Segasby</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kaur Bansal</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Rowe</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Arnold</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Neural-latency noise places limits on human sensitivity to the timing of events</article-title>. <source>Cognition</source>, <volume>222</volume>, <fpage>105012</fpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarrow</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Minaei</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Arnold</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>2015</year>). <article-title>A model-based comparison of three theories of audiovisual temporal recalibration</article-title>. <source>Cogn. Psychol</source>., <volume>83</volume>, <fpage>54</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarrow</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Roseboom</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Arnold</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Spatial grouping resolves ambiguity to drive temporal recalibration</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform</source>., <volume>37</volume> (<issue>5</issue>), <fpage>1657</fpage>–<lpage>1661</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuan</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bi</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Yin</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Huang</surname>, <given-names>X.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Audiovisual temporal recalibration: Space-based versus context-based</article-title>. <source>Perception</source>, <volume>41</volume> (<issue>10</issue>), <fpage>1218</fpage>–<lpage>1233</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this <bold>valuable</bold> study, Li et al., set out to understand the mechanisms of audiovisual temporal recalibration - the brain's ability to adjust to the latency differences that emerge due to different (distance-dependent) transduction latencies of auditory and visual signals - through psychophysical measurements and modeling. The analysis and specification of a formal model for this process provide <bold>convincing</bold> evidence to supports a role for causal inference in recalibration.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study asks whether the phenomenon of crossmodal temporal recalibration, i.e. the adjustment of time perception by consistent temporal mismatches across the senses, can be explained by the concept of multisensory causal inference. In particular they ask whether the explanation offered by causal inference better explains temporal recalibration better than a model assuming that crossmodal stimuli are always integrated, regardless of how discrepant they are.</p>
<p>The study is motivated by previous work in the spatial domain, where it has been shown consistently across studies that the use of crossmodal spatial information is explained by the concept of multisensory causal inference. It is also motivated by the observation that the behavioral data showcasing temporal recalibration feature nonlinearities that, by their nature, cannot be explained by a fixed integration model (sometimes also called mandatory fusion).</p>
<p>To probe this the authors implemented a sophisticated experiment that probed temporal recalibration in several sessions. They then fit the data using the two classes of candidate models and rely model criteria to provide evidence for their conclusion. The study is sophisticated, conceptually and technically state-of-the-art and theoretically grounded. The data clearly support the authors conclusions.</p>
<p>I find the conceptual advance somewhat limited. First, by design the fixed integration model cannot explain data with a nonlinear dependency on multisensory discrepancy, as already explained in many studies on spatial multisensory perception. Hence, it is not surprising that the causal inference model better fits the data. Second, and again similar to studies on spatial paradigms, the causal inference model fails to predict the behavioral data for large discrepancies. The model predictions in Figure 5 show the (expected) vanishing recalibration for large delta, while the behavioral data don't' decay to zero. Either the range of tested SOAs is too small to show that both the model and data converge to the same vanishing effect at large SOAs, or the model's formula is not the best for explaining the data. Again, the studies using spatial paradigms have the same problem, but in my view this poses the most interesting question here.</p>
<p>In my view there is nothing generally wrong with the study, it does extend the 'known' to another type of paradigm. However, it covers little new ground on the conceptual side.</p>
<p>
On that note, the small sample size of n=10 is likely not an issue, but still it is on the very low end for this type of study.</p>
<p>Comments on revision:</p>
<p>The revision has addressed most of these points and makes for a much stronger contribution. The issue of sample size remains.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Li et al.'s goal is to understand the mechanisms of audiovisual temporal recalibration. This is an interesting challenge that the brain readily solves in order to compensate for real-world latency differences in the time of arrival of audio/visual signals. To do this they perform a 3-phase recalibration experiment on 9 observers that involves a temporal order judgment (TOJ) pretest and posttest (in which observers are required to judge whether an auditory and visual stimulus were coincident, auditory leading or visual leading) and a conditioning phase in which participants are exposed to a sequence of AV stimuli with a particular temporal disparity. Participants are required to monitor both streams of information for infrequent oddballs, before being tested again in the TOJ, although this time there are 3 conditioning trials for every 1 TOJ trial. Like many previous studies, they demonstrate that conditioning stimuli shift the point of subjective simultaneity (pss) in the direction of the exposure sequence.</p>
<p>These shifts are modest - maxing out at around -50 ms for auditory leading sequences and slightly less than that for visual leading sequences. Similar effects are observed even for the longest offsets where it seems unlikely listeners would perceive the stimuli as synchronous (and therefore under a causal inference model you might intuitively expect no recalibration, and indeed simulations in Figure 5 seem to predict exactly that which isn't what most of their human observers did). Overall I think their data contribute evidence that a causal inference step is likely included within the process of recalibration.</p>
<p>Strengths:</p>
<p>The manuscript performs comprehensive testing over 9 days and 100s of trials and accompanies this with mathematical models to explain the data. The paper is reasonably clearly written and the data appear to support the conclusions.</p>
<p>Comments on revision:</p>
<p>In the revised manuscript the authors incorporate an alternative model (the asynchrony contingent model), and demonstrate that the causal inference model still out performs this. They provide additional analysis with Bayes factors to perform model comparisons, and provide significant individual subject data in the supplementary materials. Overall they have addressed most of the key points that my original review raised, including a demonstration of the conditions under which recalibration effects do not delay to zero over long delays. The number of subjects remains rather low, but at least we can now appreciate the heterogeneity within them. I still have some reservations about the magnitude of the conceptual advance that this study makes.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Li et al. describe an audiovisual temporal recalibration experiment in which participants perform baseline sessions of ternary order judgments about audiovisual stimulus pairs with various stimulus-onset asynchronies (SOAs). These are followed by adaptation at several adapting SOAs (each on a different day), followed by post-adaptation sessions to assess changes in psychometric functions. The key novelty is the formal specification and application/fit of a causal-inference model for the perception of relative timing, providing simulated predictions for the complete set of psychometric functions both pre and post adaptation.</p>
<p>Strengths:</p>
<p>(1) Formal models are preferable to vague theoretical statements about a process, and prior to this work, certain accounts of temporal recalibration (specifically those that do not rely on a population code) had only qualitative theoretical statements to explain how/why the magnitude of recalibration changes non-linearly with the stimulus-onset asynchrony of the adaptor.</p>
<p>
(2) The experiment is appropriate, the methods are well described, and the average model prediction is a good match to the average data (Figure 4). Conclusions are supported by the data and modelling.</p>
<p>
(3) The work should be impactful. There seems a good chance that this will become the go-to modelling framework for those exploring non population-code accounts of temporal recalibration (or comparing them with population-code accounts).</p>
<p>
(4) Key issues for the generality of the model, such as recalibration asymmetries reported by other authors that are inconsistent with those reported here, are thoughtfully discussed.</p>
<p>Weaknesses:</p>
<p>(1) Models are not compared using a gold-standard measure such as leave-one-out cross validation. However, this is legitimate given lengthy model fitting times, and a sensible approximation is presented.</p>
<p>
(2) The model misses in a systematic way for the psychometric functions of some participants/conditions. In addition to misses relating to occasional failures to estimate the magnitude of recalibration, some of the misses are because all functions are only permitted to shift in central tendency (whereas some participants show changes better characterized at one or both decision criteria). Given the fact that the modelling in general embraces individual differences, it might have been worth allowing different kinds of change for different participants. However, this is not really critical for the central concern (changes in the magnitude of recalibration for different adaptors) and there is a limit to how much can be done along these lines without making the model too flexible to test.</p>
<p>
(3) As a minor point, the model relies on simulation, which may limit its take-up/application by others in the field (although open access code will be provided).</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97765.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Luhe</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0182-3952</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Hong</surname>
<given-names>Fangfang</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1890-1977</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Badde</surname>
<given-names>Stephanie</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Landy</surname>
<given-names>Michael S</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>This study asks whether the phenomenon of crossmodal temporal recalibration, i.e. the adjustment of time perception by consistent temporal mismatches across the senses, can be explained by the concept of multisensory causal inference. In particular, they ask whether the explanation offered by causal inference better explains temporal recalibration better than a model assuming that crossmodal stimuli are always integrated, regardless of how discrepant they are.</p>
<p>The study is motivated by previous work in the spatial domain, where it has been shown consistently across studies that the use of crossmodal spatial information is explained by the concept of multisensory causal inference. It is also motivated by the observation that the behavioral data showcasing temporal recalibration feature nonlinearities that, by their nature, cannot be explained by a fixed integration model (sometimes also called mandatory fusion).</p>
<p>To probe this the authors implemented a sophisticated experiment that probed temporal recalibration in several sessions. They then fit the data using the two classes of candidate models and rely on model criteria to provide evidence for their conclusion. The study is sophisticated, conceptually and technically state-of-the-art, and theoretically grounded. The data clearly support the authors’ conclusions.</p>
<p>I find the conceptual advance somewhat limited. First, by design, the fixed integration model cannot explain data with a nonlinear dependency on multisensory discrepancy, as already explained in many studies on spatial multisensory perception. Hence, it is not surprising that the causal inference model better fits the data.</p>
</disp-quote>
<p>We have addressed this comment by including an asynchrony-contingent model, which is capable of predicting the nonlinearity of recalibration effects by employing a heuristic approximation of the causal-inference process (Fig. 3). We also updated the previous competitor model with a more reasonable asynchrony-correction model as the baseline of model comparison, which assumes recalibration aims to restore synchrony whenever the sensory measurement of SOA indicates an asynchrony. The causal-inference model outperformed both models, as indicated by model evidence (Fig. 4A). Furthermore, model predictions show that the causal-inference model more accurately captures recalibration at large SOAs at both the group (Fig. 4B) and the individual levels (Fig. S4).</p>
<disp-quote content-type="editor-comment">
<p>Second, and again similar to studies on spatial paradigms, the causal inference model fails to predict the behavioral data for large discrepancies. The model predictions in Figure 5 show the (expected) vanishing recalibration for large delta, while the behavioral data don’t decay to zero. Either the range of tested SOAs is too small to show that both the model and data converge to the same vanishing effect at large SOAs, or the model's formula is not the best for explaining the data. Again, the studies using spatial paradigms have the same problem, but in my view, this poses the most interesting question here.</p>
</disp-quote>
<p>We included an additional simulation (Fig. 5B) to show that the causal-inference model can predict non-zero recalibration for long adapter SOAs, especially in observers with a high common-cause prior and low sensory precision. This ability to predict a non-zero recalibration effect even at large SOA, such as 0.7 s, is one key feature of the causal-inference model that distinguishes it from the asynchrony-contingent model.</p>
<disp-quote content-type="editor-comment">
<p>In my view there is nothing generally wrong with the study, it does extend the 'known' to another type of paradigm. However, it covers little new ground on the conceptual side.</p>
<p>On that note, the small sample size of n=10 is likely not an issue, but still, it is on the very low end for this type of study.</p>
</disp-quote>
<p>This study used a within-subject design, which included 3 phases each repeated in 9 sessions, totaling 13.5 hours per participant. This extensive data collection allows us to better constrain the model for each participant. Our conclusions are based on the different models’ ability to fit individual data.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>Li et al.’s goal is to understand the mechanisms of audiovisual temporal recalibration. This is an interesting challenge that the brain readily solves in order to compensate for real-world latency differences in the time of arrival of audio/visual signals. To do this they perform a 3-phase recalibration experiment on 9 observers that involves a temporal order judgment (TOJ) pretest and posttest (in which observers are required to judge whether an auditory and visual stimulus were coincident, auditory leading or visual leading) and a conditioning phase in which participants are exposed to a sequence of AV stimuli with a particular temporal disparity. Participants are required to monitor both streams of information for infrequent oddballs, before being tested again in the TOJ, although this time there are 3 conditioning trials for every 1 TOJ trial. Like many previous studies, they demonstrate that conditioning stimuli shift the point of subjective simultaneity (pss) in the direction of the exposure sequence.</p>
<p>These shifts are modest - maxing out at around -50 ms for auditory leading sequences and slightly less than that for visual leading sequences. Similar effects are observed even for the longest offsets where it seems unlikely listeners would perceive the stimuli as synchronous (and therefore under a causal inference model you might intuitively expect no recalibration, and indeed simulations in Figure 5 seem to predict exactly that which isn't what most of their human observers did). Overall I think their data contribute evidence that a causal inference step is likely included within the process of recalibration.</p>
<p>Strengths:</p>
<p>The manuscript performs comprehensive testing over 9 days and 100s of trials and accompanies this with mathematical models to explain the data. The paper is reasonably clearly written and the data appear to support the conclusions.</p>
<p>Weaknesses:</p>
<p>While I believe the data contribute evidence that a causal inference step is likely included within the process of recalibration, this to my mind is not a mechanism but might be seen more as a logical checkpoint to determine whether whatever underlying neuronal mechanism actually instantiates the recalibration should be triggered.</p>
</disp-quote>
<p>We have addressed this comment by replacing the fixed-update model with an asynchrony-correction model, which assumes that the system first evaluates whether the measurement of SOA is asynchronous, thus indicating a need for recalibration (Fig. 3). If it does, it shifts the audiovisual bias by a proportion of the measured SOA. We additionally included an asynchrony-contingent model, which is capable of replicating the nonlinearity of recalibration effects by a heuristic approximation of the causal-inference process.</p>
<p>Model comparisons indicate that the causal-inference model of temporal recalibration outperforms both alternative models (Fig. 4A). Furthermore, the model predictions demonstrate that the causal-inference model more accurately captures recalibration at large SOAs at both the group level (Fig. 4B) and individual level (Fig. S4).</p>
<disp-quote content-type="editor-comment">
<p>The authors’ causal inference model strongly predicts that there should be no recalibration for stimuli at 0.7 ms offset, yet only 3/9 participants appear to show this effect. They note that a significant difference in their design and that of others is the inclusion of longer lags, which are unlikely to originate from the same source, but don’t offer any explanation for this key difference between their data and the predictions of a causal inference model.</p>
</disp-quote>
<p>We added further simulations to show that the causal-inference model can predict non-zero recalibration also for longer adapter SOAs, especially in observers with a large common-cause prior (Fig. 5A) and low sensory precision (Fig. 5B). This ability to predict a non-zero recalibration effect even at longer adapter SOAs, such as 0.7 s, is a key feature of the causal-inference model that distinguishes it from the asynchrony-contingent model.</p>
<disp-quote content-type="editor-comment">
<p>I’m also not completely convinced that the causal inference model isn’t ‘best’ simply because it has sufficient free parameters to capture the noise in the data. The tested models do not (I think) have equivalent complexity - the causal inference model fits best, but has more parameters with which to fit the data. Moreover, while it fits ‘best’, is it a good model? Figure S6 is useful in this regard but is not completely clear - are the red dots the actual data or the causal inference prediction? This suggests that it does fit the data very well, but is this based on predicting held-out data, or is it just that by having more parameters it can better capture the noise? Similarly, S7 is a potentially useful figure but it's not clear what is data and what are model predictions (what are the differences between each row for each participant; are they two different models or pre-test post-test or data and model prediction?!).</p>
<p>I'm not an expert on the implementation of such models but my reading of the supplemental methods is that the model is fit using all the data rather than fit and tested on held-out data. This seems problematic.</p>
<p>We recognize the risk of overfitting with the causal-inference model. We now rely on Bayesian model comparisons, which use model evidence for model selection. This method automatically incorporates a penalty for model complexity through the marginalization over the parameter space (MacKay, 2003).</p>
<p>Our design is not suitable for cross-validation because the model-fitting process is computationally intensive and time-consuming. Each fit of the causal-inference model takes approximately 30 hours, and multiple fits with different initial starting points are required to rule out that the parameter estimates correspond to local minima.</p>
</disp-quote>
<p>I would have liked to have seen more individual participant data (which is currently in the supplemental materials, albeit in a not very clear manner as discussed above).</p>
<disp-quote content-type="editor-comment">
<p>We have revised Supplementary Figures S4-S6 to show additional model predictions of the recalibration effect for individual participants, and participants’ temporal-order judgments are now shown in Supplement Figure S7. These figures confirm the better performance of the causal-inference model.</p>
</disp-quote>
<p>The way that S3 is described in the text (line 141) makes it sound like everyone was in the same direction, however, it is clear that 2 /9 listeners show the opposite pattern, and 2 have confidence intervals close to zero (albeit on the -ve side).</p>
<p>We have revised the text to clarify that the asymmetry occurs in both directions and is idiosyncratic (lines 168-171). We summarized the distribution of the individual asymmetries of the recalibration effect across visual-leading and auditory-leading adapter SOAs in Supplementary Figure S2.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>Li et al. describe an audiovisual temporal recalibration experiment in which participants perform baseline sessions of ternary order judgments about audiovisual stimulus pairs with various stimulus-onset asynchronies (SOAs). These are followed by adaptation at several adapting SOAs (each on a different day), followed by post-adaptation sessions to assess changes in psychometric functions. The key novelty is the formal specification and application/fit of a causal-inference model for the perception of relative timing, providing simulated predictions for the complete set of psychometric functions both pre and post-adaptation.</p>
<p>Strengths:</p>
<p>(1) Formal models are preferable to vague theoretical statements about a process, and prior to this work, certain accounts of temporal recalibration (specifically those that do not rely on a population code) had only qualitative theoretical statements to explain how/why the magnitude of recalibration changes non-linearly with the stimulus-onset asynchrony of the adapter.</p>
<p>(2) The experiment is appropriate, the methods are well described, and the average model prediction is a fairly good match to the average data (Figure 4). Conclusions may be overstated slightly, but seem to be essentially supported by the data and modelling.</p>
<p>(3) The work should be impactful. There seems a good chance that this will become the go-to modelling framework for those exploring non-population-code accounts of temporal recalibration (or comparing them with population-code accounts).</p>
<p>(4) A key issue for the generality of the model, specifically in terms of recalibration asymmetries reported by other authors that are inconsistent with those reported here, is properly acknowledged in the discussion.</p>
<p>Weaknesses:</p>
<p>(1) The evidence for the model comes in two forms. First, two trends in the data (non-linearity and asymmetry) are illustrated, and the model is shown to be capable of delivering patterns like these. Second, the model is compared, via AIC, to three other models. However, the main comparison models are clearly not going to fit the data very well, so the fact that the new model fits better does not seem all that compelling. I would suggest that the authors consider a comparison with the atheoretical model they use to first illustrate the data (in Figure 2). This model fits all sessions but with complete freedom to move the bias around (whereas the new model constrains the way bias changes via a principled account). The atheoretical model will obviously fit better, but will have many more free parameters, so a comparison via AIC/BIC or similar should be informative</p>
</disp-quote>
<p>In the revised manuscript, we switched from AIC to Bayesian model selection, which approximates and compares model evidence. This method incorporates a strong penalty for model complexity through marginalization over the parameter space (MacKay, 2003).</p>
<p>We have addressed this comment by updating the former competitor model into a more reasonable version that induces recalibration only for some measured SOAs and by including another (asynchrony-contingent) model that is capable of predicting the nonlinearity and asymmetry of recalibration (Fig. 3) while heuristically approximating the causal inference computations. The causal-inference model outperformed the asynchrony-contingent model, as indicated by model evidence (Fig. 4A). Furthermore, model predictions show that the causal-inference model more accurately captures recalibration at large SOAs at both the group (Fig. 4B) and the individual level (Fig. S4).</p>
<disp-quote content-type="editor-comment">
<p>(2) It does not appear that some key comparisons have been subjected to appropriate inferential statistical tests. Specifically, lines 196-207 - presumably this is the mean (and SD or SE) change in AIC between models across the group of 9 observers. So are these differences actually significant, for example via t-test?</p>
</disp-quote>
<p>We statistically compared the models using Bayes factors (Fig. 4A). The model evidence for each model was approximated using Variational Bayesian Monte Carlo. Bayes factors provided strong evidence in support of the causal-inference model relative to the other models.</p>
<disp-quote content-type="editor-comment">
<p>(3) The manuscript tends to gloss over the population-code account of temporal recalibration, which can already provide a quantitative account of how the magnitude of recalibration varies with adapter SOA. This could be better acknowledged, and the features a population code may struggle with (asymmetry?) are considered.</p>
</disp-quote>
<p>We simulated a population-code model to examine its prediction of the recalibration effect for different adapter SOAs (lines 380–388, Supplement Section 8). The population-code model can predict the nonlinearity of recalibration, i.e., a decreasing recalibration effect as the adapter SOA increases. However, to capture the asymmetry of recalibration effects across auditory-leading and visual-leading adapter stimuli, we would need to assume that the auditory-leading and visual-leading SOAs are represented by neural populations with unequal tuning curves.</p>
<disp-quote content-type="editor-comment">
<p>(4) The engagement with relevant past literature seems a little thin. Firstly, papers that have applied causal inference modeling to judgments of relative timing are overlooked (see references below). There should be greater clarity regarding how the modelling here builds on or differs from these previous papers (most obviously in terms of additionally modelling the recalibration process, but other details may vary too). Secondly, there is no discussion of previous findings like that in Fujisaki et al.’s seminal work on recalibration, where the spatial overlap of the audio and visual events didn’t seem to matter (although admittedly this was an N = 2 control experiment). This kind of finding would seem relevant to a causal inference account.</p>
<p>References:</p>
<p>Magnotti JF, Ma WJ and Beauchamp MS (2013) Causal inference of asynchronous audiovisual speech. Front. Psychol. 4:798. doi: 10.3389/fpsyg.2013.00798</p>
<p>Sato, Y. (2021). Comparing Bayesian models for simultaneity judgement with different causal assumptions. J. Math. Psychol., 102, 102521.</p>
</disp-quote>
<p>We have revised the Introduction and Discussion to better situate our study within the existing literature. Specifically, we have incorporated the suggested references (lines 66–69) and provided clearer distinctions on how our modeling approach builds on or differs from previous work on causal-inference models, particularly in terms of modeling the recalibration process (lines 75–79). Additionally, we have discussed findings that might contradict the assumptions of the causal-inference model (lines 405–424).</p>
<disp-quote content-type="editor-comment">
<p>(5) As a minor point, the model relies on simulation, which may limit its take-up/application by others in the field.</p>
</disp-quote>
<p>Upon acceptance, we will publicly share the code for all models (simulation and parameter fitting) to enable researchers to adapt and apply these models to their own data.</p>
<disp-quote content-type="editor-comment">
<p>(6) There is little in the way of reassurance regarding the model’s identifiability and recoverability. The authors might for example consider some parameter recovery simulations or similar.</p>
</disp-quote>
<p>We conducted a model recovery for each of the six models described in the main text and confirmed that the asynchrony-contingent and causal-inference models are identifiable (Supplement Section 11). Simulations of the asynchrony-correction model were sometimes best fit by causal-inference models, because the latter behaves similarly when the prior of a common cause is set to one.</p>
<p>We also conducted a parameter recovery for the winning model, the causal-inference model with modality-specific precision (Supplement Section 13).</p>
<p>Key parameters, including audiovisual bias  , amount of auditory latency noise  , amount of visual latency noise  , criterion, lapse rate  showed satisfactory recovery performance. The less accurate recovery of  is likely due to a tradeoff with learning rate  .</p>
<disp-quote content-type="editor-comment">
<p>(7) I don't recall any statements about open science and the availability of code and data.</p>
</disp-quote>
<p>Upon acceptance of the manuscript, all code (simulation and parameter fitting) and data will be made available on OSF and publicly available.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewing Editor (Recommendations For The Authors):</bold></p>
<p>In addition to the comments below, we would like to offer the following summary based on the discussion between reviewers:</p>
<p>The major shortcoming of the work is that there should ideally be a bit more evidence to support the model, over and above a demonstration that it captures important trends and beats an account that was already known to be wrong. We suggest you:</p>
<p>(1) Revise the figure legends (Figure 5 and Figure 6E).</p>
</disp-quote>
<p>We revised all figures and figure legends.</p>
<disp-quote content-type="editor-comment">
<p>(2) Additionally report model differences in terms of BIC (which will favour the preferred model less under the current analysis);</p>
</disp-quote>
<p>We now base the model comparison on Bayesian model selection, which approximates and compares model evidence. This method incorporates a strong penalty for model complexity through marginalization over the parameter space (MacKay, 2003).</p>
<disp-quote content-type="editor-comment">
<p>(3) Move to instead fitting the models multiple times in order to get leave-one-out estimates of best-fitting loglikelihood for each left-out data point (and then sum those for the comparison metric).</p>
</disp-quote>
<p>Unfortunately, our design is not suitable for cross-validation methods because the model-fitting process is computationally intensive and time-consuming. Each fit of the causal-inference model takes approximately 30 hours, and multiple fits with different initial starting points are required to rule out local minima.</p>
<disp-quote content-type="editor-comment">
<p>(4) Offering a comparison with a more convincing model (for example an atheoretical fit with free parameters for all adapters, e.g. as suggested by Reviewer 3.</p>
</disp-quote>
<p>We updated the previous competitor model and included an asynchrony-contingent model, which is capable of predicting the nonlinearity of recalibration (Fig. 3). The causal-inference model still outperformed the asynchrony-contingent model (Fig. 4A). Furthermore, model predictions show that only the causal-inference model captures non-zero recalibration effects for long adapter SOAs at both the group level (Fig. 4B) and individual level (Figure S4).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>A larger sample size would be better.</p>
</disp-quote>
<p>This study used a within-subject design, which included 9 sessions, totaling 13.5 hours per participant. This extensive data collection allows us to better constrain the model for each participant. Our conclusions are based on the different models’ ability to fit individual data rather than on group statistics.</p>
<disp-quote content-type="editor-comment">
<p>It would be good to better put the study in the context of spatial ventriloquism, where similar model comparisons have been done over the last ten years and there is a large body of work to connect to.</p>
</disp-quote>
<p>We now discuss our model in relation to models of cross-modal spatial recalibration in the Introduction (lines 70–78) and Discussion (lines 324–330).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Previous authors (e.g. Yarrow et al.,) have described latency shift and criterion change models as providing a good fit of experimental data. Did the authors attempt a criterion shift model in addition to a shift model?</p>
</disp-quote>
<p>We have considered criterion-shift variants of our atheoretical recalibration models in Supplement Section 1. To summarize the results, we varied two model assumptions: 1) the use of either a Gaussian or an exponential measurement distribution, and 2) recalibration being implemented either as a shift of bias or a criterion. We fit each model variant separately to the ternary TOJ responses of all sessions. Bayesian model comparisons indicated that the bias-shift model with exponential measurement distributions best captured the data of most participants.</p>
<disp-quote content-type="editor-comment">
<p>Figure 4B - I'm not convinced that the modality-independent uncertainty is anything but a straw man. Models not allowed to be asymmetric do not show asymmetry? (the asymmetry index is irrelevant in the fixed update model as I understand it so it is not surprising the model is identical?).</p>
</disp-quote>
<p>We included the assumption that temporal uncertainty might be modality-independent for several reasons. First, there is evidence suggesting that a central mechanism governs the precision of temporal-order judgments (Hirsh &amp; Sherrick, 1961), indicating that precision is primarily limited by a central mechanism rather than the sensory channels themselves. Second, from a modeling perspective, it was necessary to test whether an audio-visual temporal bias alone, i.e., assuming modality-independent uncertainty, could introduce asymmetry across adapter SOAs. Additionally, most previous studies implicitly assumed symmetric likelihoods, i.e., modality-independent latency noise, by fitting cumulative Gaussians to the psychometric curves derived from 2AFC-TOJ tasks (Di Luca et al., 2009; Fujisaki et al., 2004; Harrar &amp; Harris, 2005; Keetels &amp; Vroomen, 2007; Navarra et al., 2005; Tanaka et al., 2011; Vatakis et al., 2007, 2008; Vroomen et al., 2004).</p>
<disp-quote content-type="editor-comment">
<p>Why does a zero SOA adapter shift the pss towards auditory leading? Is this a consequence of the previous day’s conditioning - it’s not clear from the methods whether all listeners had the same SOA conditioning sequence across days.</p>
</disp-quote>
<p>The auditory-leading recalibration effect for an adapter SOA of zero has been consistently reported in previous studies (e.g., Fujisaki et al., 2004; Vroomen et al., 2004). This effect symbolizes the asymmetry in recalibration. This asymmetry can be explained by differences across modalities in the noisiness of the latencies (Figure 5C) in combination with audiovisual temporal bias (Figure S8).</p>
<p>We added details about the order of testing to the Methods section (lines 456–457).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>Abstract</p>
<p>“Our results indicate that human observers employ causal-inference-based percepts to recalibrate cross-modal temporal perception” Your results indicate this is plausible. However, this statement (basically repeated at the end of the intro and again in the discussion) is - in my opinion - too strong.</p>
</disp-quote>
<p>We have revised the statement as suggested.</p>
<disp-quote content-type="editor-comment">
<p>Intro and later</p>
<p>Within the wider literature on relative timing perception, the temporal order judgement (TOJ) task refers to a task with just two response options. Tasks with three response options, as employed here, are typically referred to as ternary judgments. I would suggest language consistent with the existing literature (or if not, the contrast to standard usage could be clarified).</p>
<p>Ref: Ulrich, R. (1987). Threshold models of temporal-order judgments evaluated by a ternary response task. Percept. Psychophys., 42, 224-239.</p>
</disp-quote>
<p>We revised the term for the task as suggested throughout the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Results, 2.2.2</p>
<p>“However, temporal precision might not be due to the variability of arrival latency.” Indeed, although there is some recent evidence that it might be.</p>
<p>Ref: Yarrow, K., Kohl, C, Segasby, T., Kaur Bansal, R., Rowe, P., &amp; Arnold, D.H. Neural-latency noise places limits on human sensitivity to the timing of events. Cognition, 222, 105012 (2022).</p>
</disp-quote>
<p>We included the reference as suggested (lines 245–248).</p>
<disp-quote content-type="editor-comment">
<p>Methods, 4.3.</p>
<p>Should there be some information here about the order of adaptation sessions (e.g. random for each observer)?</p>
</disp-quote>
<p>We added details about the order of testing to the Methods section (lines 456–457).</p>
<disp-quote content-type="editor-comment">
<p>Supplemental material section 1.</p>
<p>Here, you test whether the changes resulting from recalibration look more like a shift of the entire psychometric function or an expansion of the psychometric function on one side (most straightforwardly compatible with a change of one decision criterion). Fine, but the way you have done this is odd, because you have introduced a further difference in the models (Gaussian vs. exponential latency noise) so that you cannot actually conclude that the trend towards a win for the bias-shift model is simply down to the bias vs. criterion difference. It could just as easily be down to the different shapes of psychometric functions that the two models can predict (with the exponential noise model permitting asymmetry in slopes). There seems to be no reason that this comparison cannot be made entirely within the exponential noise framework (by a very simple reparameterization that focuses on the two boundaries rather than the midpoint and extent of the decision window). Then, you would be focusing entirely on the question of interest. It would also equate model parameters, removing any reliance on asymptotic assumptions being met for AIC.</p>
</disp-quote>
<p>We revised our exploration of atheoretical recalibration models. To summarize the results, we varied two model assumptions: 1) the use of either a Gaussian or an exponential measurement distribution, and 2) recalibration being implemented either as a shift of the cross-modal temporal bias or as a shift of the criterion. We fit each model separately to the ternary TOJ responses of all sessions. Bayesian model comparisons indicated that the bias-shift model with exponential measurement distributions best described the data of most participants.</p>
<p>References</p>
<p>Di Luca, M., Machulla, T.-K., &amp; Ernst, M. O. (2009). Recalibration of multisensory simultaneity:</p>
<p>cross-modal transfer coincides with a change in perceptual latency. Journal of Vision, 9(12), Article 7.</p>
<p>Fujisaki, W., Shimojo, S., Kashino, M., &amp; Nishida, S. ’ya. (2004). Recalibration of audiovisual simultaneity. Nature Neuroscience, 7(7), 773–778.</p>
<p>Harrar, V., &amp; Harris, L. R. (2005). Simultaneity constancy: detecting events with touch and vision. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 166(3-4), 465–473.</p>
<p>Hirsh, I. J., &amp; Sherrick, C. E., Jr. (1961). Perceived order in different sense modalities. Journal of Experimental Psychology, 62(5), 423–432.</p>
<p>Keetels, M., &amp; Vroomen, J. (2007). No effect of auditory-visual spatial disparity on temporal recalibration. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 182(4), 559–565.</p>
<p>MacKay, D. J. (2003). Information theory, inference and learning algorithms.<ext-link ext-link-type="uri" xlink:href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=201b835c3f3a3626ca07b">https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=201b835c3f3a3626ca07b</ext-link> e68cc28cf7d286bf8d5</p>
<p>Navarra, J., Vatakis, A., Zampini, M., Soto-Faraco, S., Humphreys, W., &amp; Spence, C. (2005). Exposure to asynchronous audiovisual speech extends the temporal window for audiovisual integration. Brain Research. Cognitive Brain Research, 25(2), 499–507.</p>
<p>Tanaka, A., Asakawa, K., &amp; Imai, H. (2011). The change in perceptual synchrony between auditory and visual speech after exposure to asynchronous speech. Neuroreport, 22(14), 684–688.</p>
<p>Vatakis, A., Navarra, J., Soto-Faraco, S., &amp; Spence, C. (2007). Temporal recalibration during asynchronous audiovisual speech perception. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 181(1), 173–181.</p>
<p>Vatakis, A., Navarra, J., Soto-Faraco, S., &amp; Spence, C. (2008). Audiovisual temporal adaptation of speech: temporal order versus simultaneity judgments. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 185(3), 521–529.</p>
<p>Vroomen, J., Keetels, M., de Gelder, B., &amp; Bertelson, P. (2004). Recalibration of temporal order perception by exposure to audio-visual asynchrony. Brain Research. Cognitive Brain Research, 22(1), 32–35.</p>
</body>
</sub-article>
</article>