<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99140</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99140</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99140.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Switching perspective: Comparing ground-level and bird’s-eye views for bumblebees navigating dense environments</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3139-3981</contrib-id>
<name>
<surname>Sonntag</surname>
<given-names>Annkathrin</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<email>a.sonntag@uni-bielefeld.de</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1029-8846</contrib-id>
<name>
<surname>Sauzet</surname>
<given-names>Odile</given-names>
</name>
<xref ref-type="aff" rid="a2">b</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2463-2040</contrib-id>
<name>
<surname>Lihoreau</surname>
<given-names>Mathieu</given-names>
</name>
<xref ref-type="aff" rid="a3">c</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9336-4270</contrib-id>
<name>
<surname>Egelhaaf</surname>
<given-names>Martin</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0889-4550</contrib-id>
<name>
<surname>Bertrand</surname>
<given-names>Olivier</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<aff id="a1"><label>a</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02hpadn98</institution-id><institution>Neurobiology, Faculty of Biology, Bielefeld University</institution></institution-wrap>, <city>Bielefeld</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>b</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02hpadn98</institution-id><institution>School of Public Health and Department of Business Administration and Economics, Bielefeld University</institution></institution-wrap>, <city>Bielefeld</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>c</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0111s2360</institution-id><institution>Research Center on Animal Cognition (CRCA), Center for Integrative Biology (CBI); CNRS, University Toulouse</institution></institution-wrap>, <city>Toulouse</city>, <country country="FR">France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Desplan</surname>
<given-names>Claude</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>New York University</institution>
</institution-wrap>
<city>New York</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-24">
<day>24</day>
<month>07</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-04-17">
<day>17</day>
<month>04</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99140</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-08">
<day>08</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-05-09">
<day>09</day>
<month>05</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.21.572344"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-07-24">
<day>24</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99140.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99140.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99140.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99140.1.sa0">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.99140.1.sa3">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Sonntag et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Sonntag et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99140-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Animals navigating in three dimensions encounter different perspectives of their world, often transitioning from bird’s eye views at higher altitudes to ground views closer to the ground. How they integrate this information to pinpoint a goal location is virtually unknown. Here we tested the ability of bumblebees to use both types of views when homing in a dense environment in the vicinity of their inconspicuous nest entrance. Our combined modelling and experimental approach examined various views for localising a goal in dense settings. Whereas, bird’s-eye views performed best in simulations of current nest-centered snapshot homing models, behavioural experiments revealed that bumblebees predominantly relied on ground views when pinpointing nest entrances in dense environments. These findings reveal the limitations of snapshot-homing models and suggest that bumblebees use a combination of navigational tools to successfully find their way home in dense environments. This is not only relevant for understanding bee navigation, but also for other animals and humans navigating in 3D as well as the development of technologies inspired by natural systems, such as autonomous flying robots.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We changed the wording &quot;clutter&quot; to &quot;dense environment&quot; and &quot;frog's eye view&quot; to &quot;ground-level view&quot;. Figure 2 was revised, the discussion section &quot;Hypothetical mechanisms for homing in dense environment&quot; was revised. We added a simulation of the arena wall in the images of the homing models.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Numerous animals, spanning diverse taxa, navigate within a three-dimensional world where changes in altitude are common. Birds, fish, mammals and insects change their flight altitude, climbing height, or swimming depth from near the ground to great heights above or depths below the water surface during foraging, nesting, or in search for shelter [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c17">17</xref>]. Also humans experience that the landscapes and views of the environment change with increasing altitude during activities like hiking, climbing, or aeroplane travel. This prompted us to investigate whether animals that have evolved to solve three-dimensional challenges are also adapted to efficiently use bird’s-eye perspectives for navigational purposes. Over the past decades, researchers have delved into the navigational strategies employed by insects, commonly referred to as their navigational toolkit [<xref ref-type="bibr" rid="c48">48</xref>]. This toolkit primarily comprises a compass and an odometer, which can be synergistically employed as a path integrator—enabling the integration of distance and direction travelled. Additionally, insects utilise landmark guidance and exploration behaviour in their navigation. Taken all together, the navigational toolkit has been studied by analysing the insects’ walking or flight paths mostly in two dimensions (e.g. [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c45">45</xref>]).</p>
<p>Accordingly, the visual mechanisms that have been considered are primarily based on the information that can be seen from close to the ground (ground view) [<xref ref-type="bibr" rid="c52">52</xref>]. However, when flying insects exit their nest for the first time and are not yet familiar with their surroundings, they increase the distance to the nest during loops, arcs and spirals and their flight altitude during so-called learning flights [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c49">49</xref>]. They may therefore learn and use visual sceneries at different altitudes. In particular, visual scenery may drastically change when insects change their flight altitude in dense environments with uniformly distributed objects such as grasslands, flower meadows, or forests with similar plants. Flying insects use views from above the dense environment, i.e., bird’ s-eye views, to recognize ground-level landmarks and locations for large-scale navigation [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c31">31</xref>]. Such bird’s-eye views might not only be relevant at high altitudes and navigation on a large spatial scale, but also on smaller spatial scales and altitudes in the vicinity of the often inconspicuous nest hole during local homing. While several studies have focused on large spatial scales using radar tracking (e.g. [<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c49">49</xref>]), there is limited understanding of the mechanisms behind local homing on a small scale, especially in dense environments.</p>
<p>Using information obtained at different heights might be especially helpful for underground nesting species, such as bumblebees, whose nest entrance is often inconspicuously located within the undergrowth. In such dense environments, bumblebees need to change their flight altitude both when learning the surroundings and when homing. Bird’s eye views might be helpful for guiding homing behaviour in the near-range of the hidden nest hole by providing a kind of overview. In contrast, ground views might help pinpointing the nest entrance. Computational models suggest such views of the visual scenery from within dense environments is sufficient to explain the returning journey of ants [<xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c32">32</xref>] and bees [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c52">52</xref>] under a variety of experimental conditions. These models rely on visual memories of the environments acquired during previous journeys or learning flights at the nest site. To navigate visually, the modelled insect compares its current view with the memorised ones and steers toward the most familiar memories (e.g. active scanning [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c1">1</xref>], rotation-invariant comparison [<xref ref-type="bibr" rid="c43">43</xref>], while oscillating by left and right turns [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>]). This class of models can simulate route-following behaviour within dense environments, even at different flight altitudes [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c8">8</xref>]. Despite all these results at the model level, how insects use views at different altitudes to navigate has never been studied experimentally. Because this is a particular challenge for bumblebees in finding their nest hole, we investigated this in a combined experimental and model analysis.</p>
<p>We addressed the question of whether bumblebees, <italic>Bombus terrestris</italic>, learn views at different altitudes and if they can return home by just using either ground or bird’s eye views. To obtain predictions for the experimental results, we first investigated the navigation of modelled bees guided by standard models (multi-snapshot model), which are frequently used in the literature on homing [<xref ref-type="bibr" rid="c53">53</xref>]. We then performed behavioural experiments to challenge bees in the same environment. The analysis was based on homing experiments in object-dense laboratory environments, where the flight altitude during the learning and homing flights was systematically constrained by manipulating the environment. We related our results to the predictions of snapshot models [<xref ref-type="bibr" rid="c54">54</xref>, <xref ref-type="bibr" rid="c52">52</xref>] comparing the performance of bird’s and ground view snapshots in the same dense environment like the behavioural experiments.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Snapshot models perform best with bird’s eye views</title>
<p>To guide our hypotheses about which views bees should prioritise during their homing flight, we compared predictions of homing models based on either bird’s eye views or ground views. In previous studies on the mechanism of visual homing, models could replicate the insects’ homing behaviour at least in spatially relatively simple situations [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c26">26</xref>]. These models assume that insects learn one or multiple views, so-called <italic>snapshots</italic>, at or around their goal location, like their nest. During their return, they compare their current views with the stored snapshot(s). Therefore we tested the homing performance in a dense environment of rotational image difference models based on two parameters: brightness values of the pixels [<xref ref-type="bibr" rid="c54">54</xref>, <xref ref-type="bibr" rid="c13">13</xref>] or contrast-weighed-nearness values encoding the depth and contrast of the environment based on optic flow information [<xref ref-type="bibr" rid="c14">14</xref>] (for details see Methods). The environment for the model simulations was the same as that used in the experimental analysis (<xref rid="fig2" ref-type="fig">Fig. 2 A&amp;B</xref>). It consisted of an inconspicuous goal on the floor, i.e. the nest hole, surrounded by multiple, similarly-looking objects creating a dense, artificial meadow around the nest. Thus, we investigated the model performance of homing in a dense environment and carried out an image comparison of the snapshots taken either above the dense environment or within it.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Brightness-based homing model at four altitude layers in the environment (0.02 m, 0.15 m, 0.32 m, and 0.45 m) for the area around the artificial meadow (dense environment). The rows show different parameters for the memorised snapshots (eight positions taken either outside or inside the dense environment and either above the dense environment, bird’s eye view, or close to the ground, ground views). <bold>A&amp;B:</bold> Examples of panoramic snapshots in A from the bird’s eye view outside the dense environment and in B ground views inside the dense environment. The axes of the panoramic images refer to the azimuthal directions (x-axis) and to the elevational directions from the simulated bee’s point of view (y-axis, dorsal meaning upwards, ventral downwards, equatorial towards the horizon). <bold>C:</bold> Rendered layers of the environment for a comparison of the current view of the simulated bee. The layers are at 0.02 m(orange), 0.15 m (blue), 0.32 m(green) and 0.45 m (red) heights. <bold>D&amp;E:</bold> The first column shows were the snapshots were taken in relation to the nest position (nest position in black, objects in red and snapshot positions indicated by coloured arrows). The other two columns show the comparison of memorised snapshot for two layers of the environment (0.02 m and 0.45 m as shown in C). The heatmaps show the image similarity between the current view at the position in the arena and the memorised snapshots taken around the nest (blue = very similar, white = very different). Additionally, white lines and arrows present the vector field from which the homing potential is derived. Red circles indicate the positions of the objects and the white dot indicates the nest position. The background colour of each column indicates the height of the current views that the snapshots are compared to. <bold>D:</bold> Memorised bird’s eye view snapshots taken outside (distance to the nest = 0.55 m) and above the dense environment (height = 0.45 m) can guide the model at the highest altitude (red background) to the nest but fails to do so at the three lower altitudes. <bold>E:</bold> Memorised ground view snapshots taken outside (distance to the nest = 0.55 m) the dense environment and close to the floor (height = 0.02 m) can only guide the model towards the center.</p></caption>
<graphic xlink:href="572344v4_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p><bold>A:</bold> We trained two groups of bees in a cylindrical flight arena with cylindrical densely distributed objects (‘artificial meadow’) around the nest entrance. The first group, B<sup>+</sup>G<sup>+</sup>, was trained with a ceiling height of the arena twice the height of the objects providing space to fly above the objects. They might have memorised both a ground view (G<sup>+</sup>) as well as a bird’s eye view (B<sup>+</sup>) when leaving the nest. The second group, B<italic><sup>−</sup></italic>G<sup>+</sup>, was trained with an arena height restricted to the height of the objects, allowing the bees to only use ground views B<italic><sup>−</sup></italic>G<sup>+</sup>. Both groups were tested to return home in three test conditions: HighCeiling (BG), Covered (B) and LowCeilng (G). In test BG the dense environment was shifted from the training position to another position in the arena to exclude the use of potential external cues; the bees could use both, a ground view and a bird’s eye view, during return. In test B a partial ceiling above and a transparent wall was placed around the objects preventing the bees from entering the artificial dense environment during return. In test G the ceiling was lowered to the top of the objects allowing the bees to use only ground views during return. <bold>B:</bold> 3D view of the setup with the hive and the foraging chamber.</p></caption>
<graphic xlink:href="572344v4_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Eight snapshots around the nest position were compared to panoramic images either based on brightness values or contrast-weighted nearness of the rendered environment at four altitudes. These altitudes were chosen to provide information related to ecologically relevant heights for the bees during their return home (see paragraph ”Flight altitude changes with training views”). These were either close to the ground (0.02 m) resembling walking or flying close the floor, half the height of the objects (0.15 m), at the maximum height of the objects (0.32 m) or above the objects (0.45 m). We hypothesised that the simulated bee could memorise panoramic snapshots either at high altitude (bird’s eye views) or close to the floor (ground views). Further, these snapshots were taken either close to the nest (0.1 m) or just outside the dense environment (0.55 m) (<xref rid="fig1" ref-type="fig">Fig.1</xref> positions of snapshots). Previous behavioural studies showed that views close to the goal are acquired during learning walks or flight [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c44">44</xref>] and modelling studies could emphasise the successful use during homing [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c14">14</xref>]. Views just at the border of the dense environment display the change from within the dense environment to a clearance outside the dense environment. Those abrupt changes of the environment have been shown to trigger reorientation by convoluted flight patterns [<xref ref-type="bibr" rid="c9">9</xref>].</p>
<p>The comparison of simulated homing either with memorised snapshots taken from the bird’s eye or the ground views, revealed the best performance, i.e. highest image similarity and homing vectors pointing towards the nest, with bird’s eye view snapshots outside the dense environment (brightness model: <xref rid="fig1" ref-type="fig">Fig. 1</xref> and Fig. S9, contrast-weighted nearness model: Fig. S13). Bird’s eye view snapshots led the simulated bee very close to the nest (brightness model: <xref rid="fig1" ref-type="fig">Fig. 1 D&amp;E</xref> and Fig. S9 - S10, contrast-weighted nearness model: Fig. S13 - S14) while ground view snapshots inside the dense environment could only lead into the dense environment but not to the nest (brightness model: <xref rid="fig1" ref-type="fig">Fig. 1 F&amp;G</xref> and Fig. S11 - S12, contrast-weighted nearness model: Fig. S15 and S16). Snapshots from above the dense environment showed the best homing performance when compared to images above the dense environment. Based on these results we made qualitative predictions on the bumblebees homing behaviour, assuming they are guided by homing mechanisms akin to the one tested in simulation. First, this suggests bumblebees would return to their environment by starting their approach to the goal by flying above the dense environment. Second, bees that could not acquire views above the dense environment, would not be able to locate their nest entrance.</p>
</sec>
<sec id="s2b">
<title>Ground views are sufficient for bees’ homing in a dense environment</title>
<p>Having shown that snapshot models perform best with bird’s eye views, we tested whether homing bees employed this strategy accordingly. Based on the model results, we hypothesised that bees should show the best homing performance when they can learn bird’s eye views and perform worse when only having access to ground views during learning. We first needed to assess their ability to return by using only visual cues provided by the dense environment. Two groups of bees were trained to find their way in a cylindrical flight arena (with a diameter of 1.5 m and a height of 0.8 m) from the nest entrance to the foraging entrance and back (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). The group B<sup>+</sup>G<sup>+</sup> was trained with unrestricted access to bird’s eye views (B<sup>+</sup>) above the dense environment and ground views (G<sup>+</sup>) within the dense environment. The group B<italic><sup>−</sup></italic>G<sup>+</sup> only had access to ground views within the dense environment during the training period. To test if the bees associated the dense environment with their nest location and did not use non-intentional visual cues, e.g., outside the flight arena (even though we tried to avoid such cues), the dense environment was shifted in the tests to another position in the arena, creating two possible nest locations: the true nest location in an external reference system as during the training condition and a visual nest location relative to the dense environment (see Materials and Methods). Further, the ceiling of the arena was either placed at the height of the dense environment, allowing only ground views (G), or high above the dense environment (BG), allowing the bees to get both: ground and bird’s eye views.</p>
<p>In the tests BG and G where the bees had physical access to the dense environment, they searched for their nest within the dense environment. This search was in the vicinity of the visual nest entrance, though it was not always precisely centred at the nest entrance; instead, it showed some spatial spread around this location. A comparison of the time spent at the two possible nest entrance locations showed that the bees were able to find back to the visual nest in the dense environment even when the dense environment was shifted relative to an external reference frame (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). Furthermore, we obtained a higher search percentage at the visual nest in the dense environment for the BG and G tests as well as for both groups trained to either B<sup>+</sup>G<sup>+</sup> or B<italic><sup>−</sup></italic>G<sup>+</sup> (<xref rid="fig3" ref-type="fig">Fig. 3</xref> and Fig. S23, statistical results in SI Table 1). Most time was spent within the arena close to the visual nest location. Still, the spatial distribution of search locations also shows high search percentages in two to three other areas near the nest location (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). These other search areas can be explained by locations similar-looking to the visual nest location between the objects. These location were also similar to those where the simulations revealed a rather broad area with a high image similarity (<xref rid="fig1" ref-type="fig">Fig. 1</xref>, Fig. S13 - 14 for the contrast-weighted nearness model). Both groups, B<sup>+</sup>G<sup>+</sup> and B<italic><sup>−</sup></italic>G<sup>+</sup>, showed similar search distribution during G and BG tests, indicating that the ceiling height does not change the bees’ search distributions (t-test results with Bonferroni-correction: SI Table 1). In conclusion, when the bees could fly between the objects in the BG and G test, they were able to relocate the nest location within the dense environment by using only ground views.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Examples of return flights and bees’ search for their nest in a dense environment (N = 26). <bold>A&amp;B:</bold>Exemplary flight trajectories in 3D (left column) and a top view in 2D (right column) from the group B<sup>+</sup>G<sup>+</sup> in the BG condition (<bold>A</bold>), and from the group B<italic><sup>−</sup></italic>G<sup>+</sup> in the B condition (<bold>B</bold>). The colour indicates the time, blue the start and red the end of the flight. The objects are depicted by red cylinders in the 3D plot and as red circles in the 2D plot. The black dots in the 2D plot shows the visual nest position within the dense environment. <bold>A:</bold> The bee searches for the nest within the dense environment at a low flight altitude. <bold>B:</bold> The bee is mainly trying to enter the covered dense environment from the side. <bold>C:</bold> Spatial search distribution represented by hexagonal binning of the percentage of visits of all bees (relative to each bee’s total flight time) in the BG condition from group B<sup>+</sup>G<sup>+</sup>. Orange circles indicate the true and the visual nest position. Black circles indicate the object positions of the dense environment. <bold>D:</bold> Percentage searching for the two groups B<sup>+</sup>G<sup>+</sup> (filled boxes, N = 26) and B<italic><sup>−</sup></italic>G<sup>+</sup> (hatched boxes, N = 26) in the tests BF, G and B relative to the total flight time. The search percentage at the true nest is given in blue and at the visual nest in green. For all tested conditions and both groups, the bees searched more at the visual nest within the dense environment than at the true nest location (refer to SI Table 1 for statistical tests).</p></caption>
<graphic xlink:href="572344v4_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2c">
<title>Bird’s eye views are not sufficient for bees to return</title>
<p>Based on the results of the simulations (<xref rid="fig1" ref-type="fig">Fig. 1</xref>), we hypothesised that bees could pinpoint their nest position by using only bird’s eye views. In addition, we observed that during the first outbound flights, bees quickly increased their altitude to fly above objects surrounding the nest entrance (Fig. S22). Therefore, we restricted the bees’ access to the dense environment during their return flight using a transparent cover around it, so they did not have access to the dense environment from the side but only from above (see test B, <xref rid="fig2" ref-type="fig">Fig. 2 A</xref>). In this test, the bees tried to enter the dense environment sideways between the objects as the search distributions show (<xref rid="fig5" ref-type="fig">Fig. 5 B&amp;D</xref>). The bees did not search at the nest location above the dense environment (<xref rid="fig5" ref-type="fig">Fig. 5 B&amp;D</xref>) which they could have done if they would have learned to enter the dense environment from above. We can therefore reject the hypothesis that bees use bird’s eye view to return home in a dense environment. Rather ground views seem to be sufficient for homing. Nevertheless, an interesting aspect with respect to finding the nest hole is revealed by the entry positions into the dense environment. Entry points from all three tests (B, BG and G for both groups) from the top and the side into the dense environment supported the finding that most bees tried to cross the boundary to the dense environment from the side while only very few tried to enter the dense environment from the top (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). The entry points from the side concentrate mainly on three locations indicating that the bees learned similar entrance points to the dense environment (<xref rid="fig4" ref-type="fig">Fig. 4 A</xref>). Taken together, these results show that the bees learned certain entry points of the dense environment. This learning could have taken place during learning flight, if the bees exited the dense environment at these locations predominantly, or learning taking place following return trials, or both.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Entry points of the bees of both groups (N = 52) in all three tests (total entry points = 149) to the dense environment, from the side (<bold>A</bold>, circular histogram in grey) and from the top (<bold>B</bold>, scatter plot in blue) of the dense environment. The direction of the direct path from the arena entrance to the nest is given by the green triangle. The kernel density estimation (KDE) of the entries from the side of the dense environment is shown as a black, dashed line in <bold>A</bold>. The radial axes represents the normalized magnitude of the KDE.</p></caption>
<graphic xlink:href="572344v4_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Probability density distribution of the flight altitude and the search distribution for the B condition of the groups B<italic><sup>−</sup></italic>G<sup>+</sup> (<bold>A&amp;B</bold>) and B<sup>+</sup>G<sup>+</sup> <bold>C&amp;D</bold>). A flight altitude of 0 is at the floor, of 300mm is at the height of the objects and 600mm at the ceiling. <bold>A</bold>: The group B<italic><sup>−</sup></italic>G<sup>+</sup> constrained to a low ceiling during training shows two peaks for a low altitude and for a high altitude. <bold>C</bold>: The group B<sup>+</sup>G<sup>+</sup> shows a broader distribution, with three peaks either half the height of the objects, just above the height of the objects, or close below the ceiling. <bold>B&amp;D</bold>: The search distributions reveal that the bees tried to enter the dense environment between the objects.</p></caption>
<graphic xlink:href="572344v4_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2d">
<title>Flight altitude changes with training views</title>
<p>We investigated the influence of the potentially available views during training on the bees’ altitude during homing by comparing the employed flight altitudes. The flight altitude probability density distributions for the G and BG tests showed for both groups that the bees flew mostly very low searching for the nest entrance (Fig. S24). The distributions, however, looked very different for the test B. The flight altitude distribution of the group B<italic><sup>−</sup></italic>G<sup>+</sup> shows two peaks, one around 130mm and one around 550mm (<xref rid="fig5" ref-type="fig">Fig. 5 A</xref>). These peaks indicate that the bees either flew just below half the object height, probably trying to enter the covered dense environment from the side or they flew close to the ceiling of the arena. The flights close to the ceiling might reflect exploratory behaviour in a section of the flight arena the bees could not experience before as they were trained to a ceiling constrained to the height of the objects. The behaviour of flying at a low altitude, trying to enter the dense environment from the side, is fitting to the peaks of search time around the covered dense environment (<xref rid="fig2" ref-type="fig">Fig. 2 A&amp;B</xref>). The group B<sup>+</sup>G<sup>+</sup> showed a broader probability density distribution indicating three shallower peaks, one at an altitude around 170mm, which refers to half the height of the objects, a second peak around 370mm, referring to just above the objects and a third peak just below the ceiling where the bees might have explored the boundaries of the arena. This indicates that the bees trained with B<sup>+</sup>G<sup>+</sup> seemed to have learned to fly above the objects and used this information when the cover blocked the entrance to the dense environment. The group B<italic><sup>−</sup></italic>G<sup>+</sup>, constrained to fly only up to the top of the objects but not above, seemed to be unable to search for another way to enter the dense environment from above. Although the bees exposed to B<sup>+</sup>G<sup>+</sup> during training, did not search above the nest position, they seemed to have learned the height of the objects which the altitude-constrained group, B<italic><sup>−</sup></italic>G<sup>+</sup>, could not learn because the bees flew more at the height of the objects. Overall, there is a clear difference in the flight altitude distributions between the two groups and between the tests. The group trained with B<sup>+</sup>G<sup>+</sup> flew more at the height of the objects when the dense environment was covered, while the group trained with a B<italic><sup>−</sup></italic>G<sup>+</sup> seemed to just explore the newly available space at a very high altitude with flying little at the height of the objects. This indicates that the flight altitude during learning plays a role in what altitude the bees fly during the return flights, and that this could influence what aspects are learned.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We investigated how bumblebees find home in a dense environment using views at different altitudes. The results of simulations of nest-centered snapshot-based homing models [<xref ref-type="bibr" rid="c53">53</xref>] suggested the best homing performance with bird’s eye views from above the dense constellation of objects. Surprisingly, bees performed equally well whether or not they could experience bird’s eye views during training or just ground views, i.e. views taken close to the ground. Even bees trained with both ground and bird’s eye views predominantly did not use bird’s eye views, because they usually flew into the dense environment from the side and not from above. Thus, instead of solely relying on snapshots, we hypothesise a multi-step process for homing in a dense environment relying on a variety of tools from the navigational toolkit supported by the bees’ behaviour.</p>
<sec id="s3a">
<title>Working range of snapshot models is limited inside a dense environment</title>
<p>Snapshot matching is widely assumed to be one of the most prominent navigational strategies of central-place foraging insects [<xref ref-type="bibr" rid="c52">52</xref>]. In the simulations of this study, we found that bird’s eye view snapshots led to the best homing performance, while ground view snapshots could only find the dense environment but not the nest position within the dense environment. Additionally, the model steered only to the nest when the current views were from above the objects (altitudes 0.32 m and 0.45 m). A previous study showed that snapshots taken at higher altitudes result in larger catchment areas than snapshots taken close to the ground [<xref ref-type="bibr" rid="c33">33</xref>]. Aligned with this study, we found that image difference functions pointed towards the location of the objects surrounding the nest when the images were taken above the objects. However, within the dense set of objects surrounding the nest, the model did not perform well in pinpointing the nest position (<xref rid="fig1" ref-type="fig">Fig. 1D&amp;E</xref>). Furthermore, in a particular form of snapshot model, i.e. skyline snapshots, occluding objects lead to smaller catchment areas [<xref ref-type="bibr" rid="c34">34</xref>]. Our model simulations support both conclusions, as the bird’s eye views from above the dense environment were less occluded. Behavioural studies also confirm the advantage of higher altitude snapshots as it was found that honeybees and bumblebees may use ground features of the environment while navigating on a large spatial scale (i.e. of hundreds of metres) [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. A modelling and robotic approach of Stankiewicz and Webb [<xref ref-type="bibr" rid="c42">42</xref>] could also confirm these findings. Overall, our model analysis could show that snapshot models are not able to find home with views within a dense environment but only with views from above it.</p>
</sec>
<sec id="s3b">
<title>Bees’ homing performance in a dense environment</title>
<p>We found that the behavioural performance of bumblebees differed quite substantially from our snapshot-based model predictions, when we tested bees either trained with bird’s and ground views or trained only with ground views. Both groups of bees performed equally well in finding the nest position between the densely distributed objects. This finding suggests that even when bees had the opportunity to fly above the objects of the dense environment during training, they relied on means other than bird’s eye views to find their goal. From a flight control perspective, changing between flying up and down could be energetically more demanding than between left and right. Large-scale studies (e.g. [<xref ref-type="bibr" rid="c31">31</xref>]) state that bees fly more in a plane than up/down. Moreover, from a flight control perspective, it might be difficult for the bees to approach the dense environment from above and then descend to the nest between the objects. This hypothesis is supported by a study showing that bees prefer to avoid flying above obstacles at short distances when they have the choice to fly around [<xref ref-type="bibr" rid="c46">46</xref>].</p>
</sec>
<sec id="s3c">
<title>Hypothetical mechanisms of homing in dense environments</title>
<p>We observed during the first outbound flights of four recorded bees that bees flew above the clutter (for a systematic analysis of the outbound flights see [<xref ref-type="bibr" rid="c41">41</xref>]). This behaviour suggests that bees may acquire nest-centered snapshots during these flights, allowing them to return home, as supported by our model simulations (<xref rid="fig1" ref-type="fig">Fig. 1</xref>) based on an extensively tested model [<xref ref-type="bibr" rid="c54">54</xref>]. However, we observed that bees typically return to their nest by flying below object height and navigating within the dense environment. Since we did not record the bees’ full visual history—being beyond the study’s scope and only becoming relevant due to inconsistencies between modeling and behavioral results—we can only hypothesize a possible mechanism by which experienced bees return to their nest based on known navigation mechanisms. These hypotheses are grounded in known navigation mechanisms such as snapshot-based route following and usage of a home vector.</p>
<p>In snapshot-based route following models, the animal compares its current view with stored snapshots of a route[<xref ref-type="bibr" rid="c2">2</xref>]. For successful navigation, the animal must collect sufficient visual memories along the route. Indeed, as the animal distances itself from the memorized view, it may be led astray. In other terms, each memorised snapshot allows the animal to follow the route within a given area. The set of memorised snapshots forms a catchment area, defined similar to a funnel around the route [<xref ref-type="bibr" rid="c2">2</xref>]. The challenge to build such a memory is at least twofold. First, the catchment area of a single snapshot decreases with increasing clutter [<xref ref-type="bibr" rid="c34">34</xref>]. This would require a bee to memorised many snapshots (the neural capacity of bees has been estimated with model simulation to 100,000 snapshots [<xref ref-type="bibr" rid="c1">1</xref>]). Second, the snapshots need to be either acquired along the route [<xref ref-type="bibr" rid="c2">2</xref>] or associated to attractive (resp. repulsive) weights [<xref ref-type="bibr" rid="c26">26</xref>]. Due to the high number of objects in clutter, it might be challenging to build a set of route-aligned snapshots.</p>
<p>The B<sup>+</sup>G<sup>+</sup> bees (those exposed to both ground and bird’s eye views) might overcome these challenges by utilizing a two-step strategy. During their initial outbound flights, they could use snapshots taken from above the clutter, which require fewer images to be effective. Then, through a trial-and-error approach, the bees could attempt to return from within the dense environment, resorting to the higher-altitude snapshots if they failed. This strategy would be effective for B<sup>+</sup>G<sup>+</sup> bees because during training they had access to both the cluttered and open regions. In contrast, the B<italic><sup>−</sup></italic>G<sup>+</sup> bees, which were only exposed to ground views, also managed to find their way back, indicating they might have relied on a different navigational mechanism.</p>
<p>Since the route-following model does not fully explain the behaviour of the bees, we propose an alternative: visual memories could trigger a reloading of a home vector, as described by Webb [<xref ref-type="bibr" rid="c47">47</xref>]. This hypothesis suggests that bees can reload this vector when stimulated by visual cues. In our study, the memorised home vector, obtained by mechanisms like path integration or others, would guide a bee to the nest based on its initial position (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, black arrow). In the training setup, the bee’s home vector and visual cues, such as the densely distributed objects around the nest, were congruent (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, green arrow). In the test setup, the visual nest hole was displaced, but the bees likely relied on the dense environment as a salient visual cue for homing when entering the flight arena from the foraging chamber. Bees may use this dense object constellation to associate it with their nest, which could then be found by a relatively straight path (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, green arrow), supported by the high concentration of entry points near the dense environment (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). These preferred entry positions used by bees suggest that they memorised specific locations to enter the dense environment, possibly triggered by visual cues like snapshots at the boundary (<xref rid="fig6" ref-type="fig">Fig. 6</xref>, yellow arrow). These visual memories might help refine the bee’s vector memory and guide them along the environment’s border until they find a familiar entry point. Once inside, bees could use the distance between the nest and the wall, along with their memorised direction, to locate the nest 6, blue arrow). This multi-faceted navigation strategy, which integrates visual memories, environmental landmarks, and vector-based navigation, underscores the complexity and flexibility of bees’ homing behaviour in dense environments and may serve as a working hypothesis for future studies.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Schematic of a hypothetical homing mechanisms based on multiple navigational strategies. A bee entering the arena (grey circle) could have memorised a home vector (black arrow) pointing towards the true nest location at the training position in the dense environment (black dot within the light red circle). However, the visual scene changed drastically in the test. Hence, a vector memory might point coarsely to the dense environment (light green arrow) triggered by the prominent visual cue of the dense environment shifted to the test position (dark red circle). While some bees might use this vector providing the shortest path to the nest, other bees could rely on visual memories for entering the clutter. They will search at the border of the dense environment (curved, yellow arrow) for a previously experienced entry position to the dense environment (visual memory, framed in yellow). This position could reload a refined dense environment vector pointing to the more precise nest position within the dense environment (blue arrow).</p></caption>
<graphic xlink:href="572344v4_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3d">
<title>Outlook</title>
<p>Our study on 3D homing of bees underscores the limitations inherent in current snapshot models, revealing their inability to provide precise positional estimates within dense environments, especially when compared to the navigational abilities of bees. Notably, bees that had the opportunity to learn during training bird’s-eye views demonstrated adaptability in spatially constrained situations, although this strategy was not employed explicitly for searching nests above the dense environment. Having shown that bumblebees are capable of finding their nest hole in very challenging, dense environments, we investigated the influence of the environmental structure of the nest surroundings on the learning flights of bumblebees in a separate study [<xref ref-type="bibr" rid="c41">41</xref>]. Future research is needed to extend our findings on a larger scale and explore the development of 3D snapshot models that account for altitude variations. Furthermore, these insights extend beyond bees and may have implications for other species, like birds with altitude fluctuations during foraging or nesting [<xref ref-type="bibr" rid="c4">4</xref>]. The use of bird’s-eye views in dense forests, akin to our findings with bees, prompts consideration of similar behaviours in other flying animals, but also for walking animals, such as ants such navigating varied vegetation heights [<xref ref-type="bibr" rid="c18">18</xref>]. Switching views might considerably affect an animal’s ability to solve spatial problems, as shown in humans [<xref ref-type="bibr" rid="c22">22</xref>]. Understanding how best to combine this information for navigation will benefit the development of autonomously flying robots—for instance, helping them navigate in dense environments.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>View-based homing models</title>
<p>We rendered an environment with 40 randomly placed red cylinders, creating an artificial meadow (dense environment), surrounding the nest entrance placed on a floor with a black and white pattern with a 1/f spatial frequency distribution (a distribution observed in nature [<xref ref-type="bibr" rid="c40">40</xref>]) in a graphic software (Blender, 2.82a). A panoramic image was taken along a grid with 0.5 cm steps within the dense environment and 1 cm steps in the remaing part of the arena. The arena was 1.5 m in diameter like for the following behavioural experiments. For the rendering we focused only on relevant cues like the objects and the patterned floor. We did not render the nest hole which was visually covered during the behavioural experiments. In addition, the arena wall was also not rendered as it did not contain any information for bees other than for flight control. Further simulations with a rendered arena wall led to worse results because the agent was mainly led to the center of the arena (Fig. S17, Fig. S18-21). Therefore, we focused our analysis on the simulation with a rendered arena wall. The grid-spacing was used at four altitudes: 2 cm for walking or hovering bees, 15 cm for bees flying half of the object height, 32 cm for bees flying just above the objects and 45 cm for bees flying very high to compare how the calculated similarity changes regarding the altitude. We expected qualitatively similar results for analyses with different volumes or catchment areas. In the next step, eight images around the nest were taken and stored as memorised snapshots, and the current views were compared (examples in <xref rid="fig7" ref-type="fig">Fig. 7</xref>-<xref rid="fig8" ref-type="fig">8</xref>).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Examples of the memorised snapshots based on brightness values for inside and outside the dense environment as well as from the bird’s and ground view perspective. The axes of the panoramic images refer to the azimuthal directions (x-axis) and to the elevational directions from the simulated bee’s point of view (y-axis, dorsal meaning upwards, ventral downwards, equatorial towards the horizon).</p></caption>
<graphic xlink:href="572344v4_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><p>Examples of the memorised snapshots based on contrast-weighted nearness for inside and outside the dense environment as well as from the bird’s and ground view perspective. The axes of the panoramic images refer to the azimuthal directions (x-axis) and to the elevational directions from the simulated bee’s point of view (y-axis, dorsal meaning upwards, ventral downwards, equatorial towards the horizon).</p></caption>
<graphic xlink:href="572344v4_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><p>Brightness-based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye view snapshots taken outside the dense environment (distance = 0.55 m, height = 0.45 m). <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B-C:</bold> Heatmaps of full environment (<bold>B</bold>) and only the dense area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees with bird’s eye view snapshots outside the dense environment to the nest position when the images are compared to images at an altitude of 0.45 m.</p></caption>
<graphic xlink:href="572344v4_fig9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.</label>
<caption><p>Brightness-based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye view snapshots taken inside the dense environment (distance = 0.1 m, height = 0.45 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B:</bold> Heatmaps of full environment (<bold>B</bold>) and only the dense area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees, with bird’s eye view snapshots inside the dense environment, only coarsely to the dense environment but not to the nest when the images are compared to images at an altitude of 0.45 m.</p></caption>
<graphic xlink:href="572344v4_fig10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig11" position="float" orientation="portrait" fig-type="figure">
<label>Figure 11.</label>
<caption><p>Brightness-based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with ground view snapshots taken inside the dense environment (distance = 0.1 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B:</bold> Heatmaps of full environment (<bold>B</bold>) and only the dense area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees, with ground view snapshots inside the dense environment, only to the center of the dense environment (altitude of 0.32 m) or shifted away from the nest (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v4_fig11.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig12" position="float" orientation="portrait" fig-type="figure">
<label>Figure 12.</label>
<caption><p>Brightness-based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with ground view snapshots taken outside the dense environment (distance = 0.55 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these image, where the local minima are shifted according the nest bearing. <bold>B:</bold> Heatmaps of full environment (<bold>B</bold>) and only the dense area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees, with ground view snapshots outside the dense environment, only to the center of the dense environment (altitude of 0.32 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v4_fig12.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig13" position="float" orientation="portrait" fig-type="figure">
<label>Figure 13.</label>
<caption><p>Contrast-weighted nearness based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye snapshots taken outside the dense environment (distance = 0.55 m, height = 0.45 m). <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image similarity as based on [<xref ref-type="bibr" rid="c14">14</xref>] is the maximum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local maxima for each of these image, where the local maxima are shifted according the nest bearing. <bold>B:</bold> Heatmaps of full environment (<bold>B</bold>) and only the dense area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image similarity between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees, with bird’s eye view snapshots outside the dense environment, only to the center of the dense environment (altitude of 0.32 m) or shifted away from the nest (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v4_fig13.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig14" position="float" orientation="portrait" fig-type="figure">
<label>Figure 14.</label>
<caption><p>Contrast-weighted nearness based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye view snapshots taken inside the dense environment (distance = 0.1 m, height = 0.45 m). <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image similarity as based on [<xref ref-type="bibr" rid="c14">14</xref>] is the maximum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local maxima for each of these image, where the local maxima are shifted according the nest bearing. <bold>B-D:</bold> Heatmaps of full environment (<bold>B</bold>) and only the dense area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image similarity between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees with bird’s eye view snapshots inside the dense environment only to the center of the dense environment (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v4_fig14.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig15" position="float" orientation="portrait" fig-type="figure">
<label>Figure 15.</label>
<caption><p>Contrast-weighted nearness based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with ground view snapshots taken inside the dense environment (distance = 0.1 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image similarity as based on [<xref ref-type="bibr" rid="c14">14</xref>] is the maximum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local maxima for each of these images, where the local maxima are shifted according the nest bearing. <bold>B-D:</bold> Heatmaps of full environment (<bold>B</bold>) and only the dense area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image similarity between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees with ground view snapshots inside the dense environment only to the center of the dense environment (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v4_fig15.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig16" position="float" orientation="portrait" fig-type="figure">
<label>Figure 16.</label>
<caption><p>Contrast-weighted nearness based homing model at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with ground view snapshots taken outside the dense environment (distance = 0.55 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image similarity as based on [<xref ref-type="bibr" rid="c14">14</xref>] is the maximum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local maxima for each of these images, where the local maxima are shifted according the nest bearing. <bold>B-D:</bold> Heatmaps of full environment (<bold>B</bold>) and only the dense area (<bold>C</bold>, as shown in <bold>D</bold>) of the image similarity. The colour indicates the image similarity between the view at the position in the arena and the snapshots taken around the nest. The model leads the simulated bees with ground view snapshots outside the dense environment only to the center of the dense environment (altitude of 0.32 m) or slightly shifted away from center (altitude of 0.45 m) but not to the nest.</p></caption>
<graphic xlink:href="572344v4_fig16.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig17" position="float" orientation="portrait" fig-type="figure">
<label>Figure 17.</label>
<caption><p>Brightness based homing model with the simulated wall of the cylinder at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye view (<bold>D&amp;E</bold>) and ground view (<bold>F&amp;G</bold>) snapshots taken outside (<bold>D&amp;F</bold>) and inside (<bold>E&amp;G</bold>) the dense environment. <bold>A&amp;B:</bold> Examples of panoramic snapshots in A from the bird’s eye view outside the dense environment and in B ground views inside the dense environment. The axes of the panoramic images refer to the azimuthal directions (x-axis) and to the elevational directions from the simulated bee’s point of view (y-axis, dorsal meaning upwards, ventral downwards, equatorial towards the horizon). <bold>C:</bold> Rendered layers of the environment for a comparison of the current view of the simulated bee. The layers are at 0.02 m(orange), 0.15 m (blue), 0.32 m(green) and 0.45 m (red) heights. <bold>D-G:</bold> The first column shows were the snapshots were taken in relation to the nest position (nest position in black, objects in red and snapshot positions indicated by coloured arrows). The other four columns show the comparison of memorised snapshot the four layers of the environment (as shown in <bold>C</bold>). The heatmaps show the image similarity between the current view at the position in the arena and the memorised snapshots taken around the nest (blue = very similar, white = very different). Additionally, white lines and arrows present the vector field from which the homing potential is derived. Red circles indicate the positions of the objects and the white dot indicates the nest position. The background colour of each column indicates the height of the current views that the snapshots are compared to.</p></caption>
<graphic xlink:href="572344v4_fig17.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig18" position="float" orientation="portrait" fig-type="figure">
<label>Figure 18.</label>
<caption><p>Brightness-based homing model with the simulated wall of the cylinder at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye view snapshots taken outside the dense environment (distance = 0.55 m, height = 0.45 m). <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B-C:</bold> Heatmaps of the dense area (<bold>B</bold>, as shown in <bold>C</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest.</p></caption>
<graphic xlink:href="572344v4_fig18.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig19" position="float" orientation="portrait" fig-type="figure">
<label>Figure 19.</label>
<caption><p>Brightness-based homing model with the simulated wall of the cylinder at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with bird’s eye view snapshots taken inside the dense environment (distance = 0.1 m, height = 0.45 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B-C:</bold> Heatmaps of the dense area (<bold>B</bold>, as shown in <bold>C</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest.</p></caption>
<graphic xlink:href="572344v4_fig19.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig20" position="float" orientation="portrait" fig-type="figure">
<label>Figure 20.</label>
<caption><p>Brightness-based homing model with the simulated wall of the cylinder at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with ground view snapshots taken inside the dense environment (distance = 0.1 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these images, where the local minima are shifted according the nest bearing. <bold>B-C:</bold> Heatmaps of the dense area (<bold>B</bold>, as shown in <bold>C</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest.</p></caption>
<graphic xlink:href="572344v4_fig20.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig21" position="float" orientation="portrait" fig-type="figure">
<label>Figure 21.</label>
<caption><p>Brightness-based homing model with the simulated wall of the cylinder at four altitudes (0.02 m, 0.15 m, 0.32 m, and 0.45 m) with ground view snapshots taken outside the dense environment (distance = 0.55 m, height = 0.02 m). The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest. <bold>A:</bold> We applied the model to a list of images, these images are examples of the eight memorised snapshots which are aligned in the nest direction and taken around the nest. For the rotational image difference function, we used memory 0 as a reference image, and compared the seven others by rotating them against memory 0. We observe that the image difference function is minimum for the memorised image at a null rotation, as expected. If the other images are not two far from the nest, we may see other local minima for each of these image, where the local minima are shifted according the nest bearing. <bold>B-C:</bold> Heatmaps of the dense area (<bold>B</bold>, as shown in <bold>C</bold>) of the image similarity. The colour indicates the image difference between the view at the position in the arena and the snapshots taken around the nest.</p></caption>
<graphic xlink:href="572344v4_fig21.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig22" position="float" orientation="portrait" fig-type="figure">
<label>Figure 22.</label>
<caption><p>Four exemplary flight trajectories of the first outbound flight of bees in 3D (left subplot) and a top view in 2D (right subplot). The colour indicates the time, blue the start of the and red the end of the flight. The objects are depicted by red cylinders in the 3D plot and as red circles in the 2D plot. The black dots in the 2D plot shows the nest position within the dense environment.</p></caption>
<graphic xlink:href="572344v4_fig22.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig23" position="float" orientation="portrait" fig-type="figure">
<label>Figure 23.</label>
<caption><p>Search distributions for the group B<sup>+</sup>G<sup>+</sup> (left column) in the condition G (condition BG is shown in the main results in <xref rid="fig2" ref-type="fig">Fig.2D</xref>) and for group B<italic><sup>−</sup></italic>G<sup>+</sup> (right column) in the tests BG and G (N = 26 for each plot). The bees of group B<sup>+</sup>G<sup>+</sup> in the condition G searched for the nest inside the dense environment and spent more time at the visual nest (in the dense environment) than at the true nest (as during training). The bees of group B<italic><sup>−</sup></italic>G<sup>+</sup> in the condition G and BG searched for the nest within the dense environment and spent more time at the visual nest than at the true nest.</p></caption>
<graphic xlink:href="572344v4_fig23.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig24" position="float" orientation="portrait" fig-type="figure">
<label>Figure 24.</label>
<caption><p>Flight altitude distributions for the tests BG and G for the groups B<sup>+</sup>G<sup>+</sup> and B<italic><sup>−</sup></italic>G<sup>+</sup> (N = 26 for each plot).</p></caption>
<graphic xlink:href="572344v4_fig24.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig25" position="float" orientation="portrait" fig-type="figure">
<label>Figure 25.</label>
<caption><p>Exemplary flight trajectories in 3D (left column) and a top view in 2D (right column) from the group B<sup>+</sup>G<sup>+</sup> in the B test. The colour indicates the time, blue the start of the and red the end of the flight. The objects are depicted by red cylinders in the 3D plot and as red circles in the 2D plot. The black dots in the 2D plot shows the nest position within the dense environment. The bee is trying to enter the covered dense environment from the side but, eventually, it is increasing its altitude and it flies above the dense environment. However, the bee is not searching for the visual nest above the covered dense environment.</p></caption>
<graphic xlink:href="572344v4_fig25.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We used the method presented in Doussot et al. [<xref ref-type="bibr" rid="c15">15</xref>] to calculate the image differences according to the brightness and the contrast-weighted-nearness method, each with eight snapshots. The image difference was calculated either by the brightness method relying on the brightness value of each pixel in one panoramic image or the contrast-weighted nearness method (see SI Methods) relying on the contrast calculated by the Michelson contrast (ratio of the luminance-amplitude (I<italic><sub>max</sub> −</italic> I<italic><sub>min</sub></italic>)) and the luminance-background (I<italic><sub>max</sub></italic> +I<italic><sub>min</sub></italic>)) weighted by the inverse of the distance [<xref ref-type="bibr" rid="c14">14</xref>]. We wanted to predict the simulated bee’s probable endpoint by analyzing vector fields from different homing models. Convergence points in these vector fields were determined using the Helmholtz-Hodge Decomposition, focusing on the curl-free component, representing the potential ϕ ([<xref ref-type="bibr" rid="c15">15</xref>]). After applying the Helmholtz-Hodge Decomposition to each vector field, we scaled the resulting potential between 0 and 1 for all homing models. With these potentials, we were able to plot heatmaps accordingly to estimate the areas in the arena where view-based agents/ simulated bees would most likely steer towards. These heatmaps were compared to the bees’ search distributions in the arena.</p>
</sec>
<sec id="s4b">
<title>Animal handling</title>
<p>We used four <italic>B. terrestris</italic> hives provided by Koppert B.V., The Netherlands, that were ordered sequentially to test one colony at a time. The bee hives arrived in small boxes and were transferred to acrylic nest boxes under red light (non-visible to bees [<xref ref-type="bibr" rid="c16">16</xref>]) to 30 <italic>×</italic> 30<italic>×</italic> 30 cm. The nest box was placed on the floor and connected to an experimental arena. We covered the nest box with a black cloth to mimic the natural lighting of underground habitats of <italic>B. terrestris</italic> [<xref ref-type="bibr" rid="c20">20</xref>]. The bee colonies were provided with pollen balls <italic>ad libitum</italic> directly within the nest boxes. The pollen balls were made of 50 mL ground, commercial pollen collected by honeybees (W. Seip, Germany), and 10 mL water. The bees reached a foraging chamber via the experimental arena containing gravity feeders. These are small bottles with a plate at the bottom so that the bees can access the sugar solution through small slots in the plate. The feeders were filled with a sweet aqueous solution (30% saccharose, 70% water in volume). Lighting from above was provided in a 12 h/12 h cycle and the temperature was kept constantly at 20<italic><sup>◦</sup></italic>. Throughout the experiments, foraging bees were individually marked using coloured, numbered plastic tags glued with melted resin on their thorax.</p>
</sec>
<sec id="s4c">
<title>Experimental arena</title>
<p>The experimental arena was a cylinder with a diameter of 1.5 m and a height of 80 cm as in [<xref ref-type="bibr" rid="c15">15</xref>]. A red and white pattern, perceived black and white by bumblebees [<xref ref-type="bibr" rid="c16">16</xref>], with a 1/f spatial frequency distribution (a distribution observed in nature [<xref ref-type="bibr" rid="c40">40</xref>]) covered the wall and floor of the cylindrical arena; the bees were provided with enough contrast to allow them to use optical flow. Red light came from 18 neon tubes (36 W Osram + Triconic) filtered by a red acrylic plate (Antiflex ac red 1600 ttv). Bees did not see these lights and perceived the red-white pattern as black and white. An adjustable transparent ceiling prevented the bees from exiting the arena. It allowed lighting from 8 neon tubes (52 W Osram + Triconic) and 8 LEDs (5 W GreenLED) as in [<xref ref-type="bibr" rid="c15">15</xref>], and recording from five high-speed cameras with different viewing angles. As in many previous indoor experiments about bee navigation, UV light was not provided within the arena [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c19">19</xref>]. A foraging bee exiting the hive crossed three small acrylic boxes (inner dimension of 8 <italic>×</italic> 8<italic>×</italic> 8 cm) with closable exits to select the bee to be tested. The bee then walked through a plastic tube with 2.5 cm in diameter and entered the cylindrical arena. This nest exit was surrounded by visual objects. The foraging chamber was reached through a hole at a height of 28 cm above the floor in the arena wall. After foraging the bees exited the foraging chamber and entered the flight arena through a hole in the arena wall at a height of 28 cm above the floor. The nest entrance within the arena was surrounded by visual objects. When it found this nest entrance, the bee walked through a plastic tube with 2.5 cm in diameter and reached the hive by crossing three small acrylic boxes (inner dimension of 8 <italic>×</italic> 8<italic>×</italic> 8 cm) with closable exits which were used during the experiments to block bees from entering the arena. The objects surrounding the nest exit in the arena consisted of 40 randomly placed red cylinders (2 cm in diameter and 30 cm in height), creating an artificial meadow. The size of the dense object constellation was 80 cm in diameter to allow its displacement to a different location in the flight arena. We considered that the object constellation was dense enough to pose a challenge in finding the nest within the cylinder constellations, as we could show in the snapshot-model comparison. For example, if only three cylinders were used, the bee may only search there because these would be the only conspicuous landmarks. Additionally, the configuration had to be sufficiently sparse for the bee to fly through [<xref ref-type="bibr" rid="c38">38</xref>]. We used the object density and object distances of Gonsek et al. [<xref ref-type="bibr" rid="c19">19</xref>] as a reference to find a randomly distributed object configuration. Red-lighting from below was used only during recordings. The cylindrical arena had a door, allowing the experimenter to change the objects within the arena.</p>
</sec>
<sec id="s4d">
<title>Experimental design</title>
<p>We tested two groups of bees in three tests trained with the dense object constellation (dense environment) surrounding the nest entrance. The two groups differed in the training condition. For the group B<sup>+</sup>G<sup>+</sup>, the flight altitude was unconstrained, and they could experience bird’s eye views above the objects. The second group, B<italic><sup>−</sup></italic>G<sup>+</sup>, were restricted during training to a maximum flight altitude of the height of the objects, so the ground views.</p>
<p>We tested 26 bees per group (4 colonies were used, 2 per group, from each colony 13 bees are included in the analysis resulting in a total of 52 individuals). In both groups, the foraging bees travelled between their nest and the foraging chamber. The return flight of each bee was recorded in all tests of a given experiment. Between individual tests, the bees were allowed to forage <italic>ad libitum</italic>. A dense object constellation surrounded the nest, and the area around the nest positions was cleaned with 70% ethanol between the tests to avoid chemical markings.</p>
<p>To test the behaviour of the bees, we locked up to six individually marked bees in the foraging chamber at a time. Each bee participated either in group B<sup>+</sup>G<sup>+</sup> (high ceiling during training, resulting in available ground and bird’s eye views) or B<italic><sup>−</sup></italic>G<sup>+</sup> (low ceiling during training, resulting in only available ground views) and was tested once in all tests of the respective experiment. The order of the tests was pseudo-random only that in the group B<italic><sup>−</sup></italic>G<sup>+</sup> the test BG was always tested last to not let the bees experience bird’s eye views before testing the B test. Before the tests, the cylindrical arena was emptied of bees, the spatial arrangement of objects was shifted, and the nest entrance closed. One bee at a time was allowed to search for its home for three minutes after take-off. After this time, the spatial arrangement of the ceiling height and the dense object constellation was placed back in the training condition, and the nest entrance opened. The bees had up to two minutes to take off. Otherwise, they were captured and released close to the nest. Between tests, the bees could fly <italic>ad libitum</italic> between the nest and foraging chamber under the training conditions.</p>
<p>For the tests, the dense environment was placed at a different location than during the training condition, and thus, it did not surround the true nest entrance leading to the hive. The dense environment indicated the location of a visual nest entrance. The true nest entrance and the visual nest entrance were covered by a piece of paper with the same texture as the arena floor so that they were not discernible by the bees.</p>
<p>For the test BF, no other constraint was added to the general tests to test if the bees associated their nest entrance with the dense environment. The G test consisted of a transparent wall and ceiling on top of the objects, which prevented the bees from entering the dense object constellation. To return to the nest location in the dense object constellation, they were only able to pinpoint the position from above if they were using only bird’s eye views. In the G test, the flight altitude was constrained to the height of the dense environment so that bees could no longer experience bird’s eye views from above the dense environment. Thus, the bees had to use the ground view to return to their nest.</p>
</sec>
<sec id="s4e">
<title>Flight trajectories</title>
<p>Bee trajectories were recorded at 62.5 Hz (16 ms between two consecutive frames) with five synchronised Basler cameras (Basler acA 2040um-NIR) with different viewing angles (similar to [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c35">35</xref>]). One camera was placed on top of the middle of the arena to track the bumblebees’ movements in the plane, and the other four cameras were distributed around the arena to record the position of the bees from different angles to minimise the error during triangulation of the bee’s position in 3D. Before the bees entered the setup, the recording had already started, and the first 60 frames were used to calculate a bee-less image of the arena (background image). During the rest of the recording, only crops, i.e. image sections (40×40 pixels), containing large differences between the background image and the current image (i.e. potentially containing the bee) were saved to the hard drive together with the position of the crop in the image. The recording scripts were written in C++. The image crops were analysed by a custom-written neural network to classify the crops in bees or not a bee. When non-biological speed (speed above 4 m/s [<xref ref-type="bibr" rid="c20">20</xref>]) or implausible positions (outside the arena) were observed, the crops neighbouring these time points were manually reviewed.</p>
<p>The trajectories were analysed in Python (version 3.8.17), notably with OpenCV. A detailed list of packages used is published in the data publication. The time course of the positions of the bees in 3D within the arena is shown for a selection of flights (<xref rid="fig2" ref-type="fig">Fig. 2</xref> and S13). For each of the tests, a distribution of presence in the flight arena was computed by using hexagonal binning of the 2D positions to show the search areas of the bees qualitatively. Bees that collected food in the foraging chamber returned home and searched for their nest entrance. Even when objects are displaced to a novel location [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c29">29</xref>], replaced by smaller, differently coloured, or camouflaged objects [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c14">14</xref>], or moved to create visual conflict between several visual features [<xref ref-type="bibr" rid="c15">15</xref>], bees search for their nest entrance associated to such objects. Therefore, we assumed that bees entering the arena after visiting a foraging chamber will search for their nest location. The returning bees could spend an equal amount of time in any location in the arena or concentrate their search around the physical position of their nest or around visual objects associated with the nest entrance [<xref ref-type="bibr" rid="c15">15</xref>]. In our experiments, the nest entrance was surrounded by cylindrical objects. During the tests, when the objects were shifted to a novel location, a bee guided by the objects might have searched for its nest around the objects. In contrast, they might not have used the objects to navigate and might have searched for their nest at the original location. We, therefore, quantified the time spent around two locations: at the true nest and the nest according to the objects (‘visual nest’). Additionally, the positions of the bees crossing the boundaries of the densely distributed objects for the first time at the side or at the top of the dense environment were visualised a circular histogram (entries from the top) and a scatter plot (entries from the side). These were used to describe where the bees entered the dense area. As the dense environment was shifted to another position in the arena, we can exclude the use of compass, odour or magnetic cues.</p>
</sec>
<sec id="s4f">
<title>Statistical analysis for hypotheses testing</title>
<p>Hypotheses about the time spent in one area compared to another were tested using the dependent t-test for paired samples. Hypotheses involving multiple comparisons were tested using a Bonferroni correction for the significance level. As long as a hypothesis concerned only two areas, no adjustment was made to the significance level. With a sample size of 26 bees, we were able to detect a time difference spent in two areas (e.g. fictive and true nest locations) of 0.25 s assuming a standard deviation of 0.38 s (estimated from [<xref ref-type="bibr" rid="c15">15</xref>]) with a power of 80% at a significance level of 0.05. The analysis was performed with Python using the Scipy library for statistical analyses.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Supporting Information</title>
<sec id="s5a">
<title>Methods</title>
<sec id="s5a1">
<title>Rotational Image Difference Functions</title>
<p>For each position (<italic>x, y</italic>) in the arena an equi-rectangular panoramic image (360deg along the azimuth, 180 deg along the elevation) was acquired. To determine the most familiar direction, each snapshot in the arena were compared to memorised snapshot. This comparison was based on the minimum rotational image difference function (RIDF, <xref rid="eqn1" ref-type="disp-formula">Eq. 1</xref>) for the brightness model. The minimum RIDF is the minimum root mean squared image difference d<italic><sub>x,y</sub></italic> between two views (the current view <italic>I<sub>x,y</sub></italic> and the view at the nest <italic>I<sub>N</sub></italic>) for different azimuthal viewing directions α weighted by <italic>w</italic>(<italic>v</italic>). <italic>w</italic>(<italic>v</italic>) is a sine wave along the y-axis counterbalancing the oversampling of the poles at the transformation of the 3D sphere mimicking the bee’s eye to 2D equirectangular images by giving values of 1 at the equator and 0 at the poles (<xref rid="eqn2" ref-type="disp-formula">Eq. 2</xref>). In the equation below, (<italic>u, v</italic>) corresponds to the viewing direction in the azimuthal direction u, and direction along the elevation v. The images resolution were (<italic>N<sub>u</sub>, N<sub>v</sub></italic>) = (360, 180) pixels.
<disp-formula id="eqn1">
<graphic xlink:href="572344v4_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2">
<graphic xlink:href="572344v4_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>The RIDF <italic>d<sub>si,x,y</sub></italic> is calculated for each snapshot <italic>s<sub>i</sub></italic> in <italic>S</italic> = <italic>s</italic><sub>0</sub>, <italic>s</italic><sub>1</sub>, …, <italic>s<sub>n</sub></italic> around the nest location and the heading direction <italic>h<sub>si,x,y</sub></italic> at each grid location x, y and each snapshot <italic>s<sub>i</sub></italic> is determined by taking the location of the minimum RIDF (<xref rid="eqn3" ref-type="disp-formula">Eq. 3</xref>). To weigh the heading direction the ratio <italic>w<sub>si</sub></italic> was calculated between the minimum RIDF of all snapshots <italic>s<sub>i</sub></italic> in <italic>S<sub>dmin</sub></italic> and the current RIDF <italic>d<sub>si</sub></italic> (<xref rid="eqn5" ref-type="disp-formula">Eq. 5</xref>). The homing vector <italic>HV⃗</italic> results from the weighted circular mean of the different heading directions <italic>h<sub>si</sub></italic> (<xref rid="eqn6" ref-type="disp-formula">Eq. 6</xref>).
<disp-formula id="eqn3">
<graphic xlink:href="572344v4_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<disp-formula id="eqn4">
<graphic xlink:href="572344v4_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="572344v4_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn6">
<graphic xlink:href="572344v4_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<p>The contrast-weighted-nearness model will use the contrast weighted by the depth of the environment to steer an agent home. The contrast was calculated as the ratio of luminance-amplitude (i.e. standard deviation of the luminance) and luminance-background (i.e. average of the luminance) within a 3×3 pixel window of the snapshot image (i.e. Michelson contrast). As described in [14], we used the rotational similarity function between the current view <italic>I<sub>x,y</sub></italic> and the memorised view <italic>I<sub>N</sub></italic> (<xref rid="eqn7" ref-type="disp-formula">Eq. 7</xref>). As for the brightness-based model, the homing vector (<xref rid="eqn6" ref-type="disp-formula">Eq. 6</xref>) was computed by the weighted circular means of each vectors derived from each memorised views (<xref rid="eqn9" ref-type="disp-formula">Eq. 9</xref>).
<disp-formula id="eqn7">
<graphic xlink:href="572344v4_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<disp-formula id="eqn8">
<graphic xlink:href="572344v4_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9">
<graphic xlink:href="572344v4_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Statistical results of t-tests.</title></caption>
<graphic xlink:href="572344v4_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank Vedant Dixit, Maximilian Stahlsmeier, Pia Hippel and Helene Schnellenberg for their help during the data collection. Additionally, we would like to thank Sina Mews for helpful discussions on statistical models. This project was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) and the Agence Nationale de la Recherche (ANR, French National Research Agency). We also acknowledge support for the publication costs by the Open Access Publication Fund of Bielefeld University.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Ardin</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Lagogiannis</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Webb</surname></string-name></person-group>. <article-title>Using an insect mushroom body circuit to encode route memory in complex natural environments</article-title>. <source>PLoS Computational Biology</source>, <volume>12</volume>(<issue>2</issue>), <year>2016</year>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Baddeley</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Husbands</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name></person-group>. <article-title>A model of ant route navigation driven by scene familiarity</article-title>. <source>PLoS Computational Biology</source>, <volume>8</volume>(<issue>1</issue>), <year>2012</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. S.</given-names> <surname>Brebner</surname></string-name>, <string-name><given-names>J. C.</given-names> <surname>Makinson</surname></string-name>, <string-name><given-names>O. K.</given-names> <surname>Bates</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Rossi</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Lim</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Dubois</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Gómez-Moracho</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Lihoreau</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Chittka</surname></string-name>, and <string-name><given-names>J. L.</given-names> <surname>Woodgate</surname></string-name></person-group>. <article-title>Bumble bees strategically use ground level linear features in navigation</article-title>. <source>Animal Behaviour</source>, <volume>179</volume>, <year>2021</year>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Bruderer</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Peter</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Korner-Nievergelt</surname></string-name></person-group>. <article-title>Vertical distribution of bird migration between the Baltic Sea and the Sahara</article-title>. <source>Journal of Ornithology</source>, <volume>159</volume>(<issue>2</issue>), <year>2018</year>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Buehlmann</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name></person-group>. <article-title>Multimodal interactions in insect navigation</article-title>. <source>Animal Cognition</source>, <volume>23</volume>(<issue>6</issue>), <year>2020</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E. A.</given-names> <surname>Capaldi</surname></string-name>, <string-name><given-names>A. D.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Osborne</surname></string-name>, <string-name><given-names>S. M.</given-names> <surname>Farris</surname></string-name>, <string-name><given-names>D. R.</given-names> <surname>Reynolds</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Edwards</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Martin</surname></string-name>, <string-name><given-names>G. E.</given-names> <surname>Robinson</surname></string-name>, <string-name><given-names>G. M.</given-names> <surname>Poppy</surname></string-name>, and <string-name><given-names>J. R.</given-names> <surname>Riley</surname></string-name></person-group>. <article-title>Ontogeny of orientation ¬ight in the honeybee revealed by harmonic radar</article-title>. <source>Nature</source>, <volume>403</volume>, <year>2000</year>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B. A.</given-names> <surname>Cartwright</surname></string-name> and <string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name></person-group>. <article-title>Landmark learning in bees</article-title>. <source>Journal of Comparative Physiology A</source>, <volume>151</volume>(<issue>4</issue>), <year>1983</year>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Cheung</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Collett</surname></string-name>, <string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Dewar</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Dyer</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Narendra</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Stürzl</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Web</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name></person-group>. <article-title>Still no convincing evidence for cognitive map use by honeybees</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>42</issue>), <year>2014</year>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name>, <string-name><given-names>S. N.</given-names> <surname>Fry</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Wehner</surname></string-name></person-group>. <article-title>Sequence learning by honeybees</article-title>. <source>Journal of Comparative Physiology A</source>, <volume>172</volume>(<issue>6</issue>), <year>1993</year>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name></person-group>. <article-title>Insect navigation: Do honeybees learn to follow highways?</article-title> <source>Current Biology</source>, <volume>25</volume>(<issue>6</issue>), <year>2015</year>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name> and <string-name><given-names>N. Hempel</given-names> <surname>de Ibarra</surname></string-name></person-group>. <article-title>An ‘instinct for learning’: the learning flights and walks of bees, wasps and ants from the 1850s to now</article-title>. <source>Journal of Experimental Biology</source>, <volume>226</volume>(<issue>6</issue>), <year>2023</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Degen</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kirbach</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Reiter</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Lehmann</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Norton</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Storms</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Koblofsky</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Winter</surname></string-name>, <string-name><given-names>P. B.</given-names> <surname>Georgieva</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Chamkhi</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Meyer</surname></string-name>, <string-name><given-names>P. K.</given-names> <surname>Singh</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Manz</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Greggers</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Menzel</surname></string-name></person-group>. <article-title>Honeybees learn landscape features during exploratory orientation flights</article-title>. <source>Current Biology</source>, <volume>26</volume>(<issue>20</issue>), <year>2016</year>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. D.</given-names> <surname>Dewar</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name></person-group>. <article-title>What is the relationship between visual environment and the form of ant learning-walks? An in silico investigation of insect navigation</article-title>. <source>Adaptive Behavior</source>, <volume>22</volume>(<issue>3</issue>), <year>2014</year>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names> <surname>Dittmar</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Stuerzl</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Baird</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Boeddeker</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name></person-group>. <article-title>Goal seeking in honeybees: Matching of optic flow snapshots?</article-title> <source>Journal of Experimental Biology</source>, <volume>213</volume>, <year>2010</year>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Doussot</surname></string-name>, <string-name><given-names>O. J. N.</given-names> <surname>Bertrand</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name></person-group>. <article-title>Visually guided homing of bumblebees in ambiguous situations: A behavioural and modelling study</article-title>. <source>PLOS Computational Biology</source>, <volume>16</volume>(<issue>10</issue>), <year>2020</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. G.</given-names> <surname>Dyer</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Chittka</surname></string-name></person-group>. <article-title>Biological significance of distinguishing between similar colours in spectrally variable illumination: Bumblebees (Bombus terrestris) as a case study. <italic>Journal of Comparative Physiology A: Neuroethology, Sensory</italic></article-title>, <source>Neural, and Behavioral Physiology</source>, <volume>190</volume>(<issue>2</issue>), <year>2004</year>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Finkelstein</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Las</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Ulanovsky</surname></string-name></person-group>. <article-title>3-D Maps and Compasses in the Brain</article-title>. <source>Annual Review of Neuroscience</source>, <volume>39</volume>, <year>2016</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. A.</given-names> <surname>Freas</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Narendra</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Cheng</surname></string-name></person-group>. <article-title>The view from the trees: Nocturnal bull ants, Myrmecia midas, use the surrounding panorama while descending from trees</article-title>. <source>Frontiers in Psychology</source>, <volume>9</volume>(<issue>JAN</issue>), <year>2018</year>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Gonsek</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Jeschke</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Rönnau</surname></string-name>, and <string-name><given-names>O. J.</given-names> <surname>Bertrand</surname></string-name></person-group>. <article-title>From Paths to Routes: A Method for Path Classification</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, <volume>14</volume>, <year>2021</year>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Goulson</surname></string-name></person-group>. <source>Bumblebees : behaviour, ecology, and conservation /</source>. <publisher-loc>Oxford biology. Oxford ; New York</publisher-loc> : <publisher-name>Oxford University Press</publisher-name>, <year>2010</year>., 2010.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. M.</given-names> <surname>Grieves</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Jedidi-Ayoub</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Mishchanchuk</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Renaudineau</surname></string-name>, and <string-name><given-names>K. J.</given-names> <surname>Jeffery</surname></string-name></person-group>. <article-title>The place-cell representation of volumetric space in rats</article-title>. <source>Nature Communications</source>, <volume>11</volume>(<issue>1</issue>), <year>2020</year>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Haxhimusa</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Carpenter</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Catrambone</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Foldes</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Stefanov</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Arns</surname></string-name>, and <string-name><given-names>Z.</given-names> <surname>Pizlo</surname></string-name></person-group>. <article-title>2D and 3D Traveling Salesman Problem</article-title>. <source>The Journal of Problem Solving</source>, <volume>3</volume>(<issue>2</issue>), <year>2011</year>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. A.</given-names> <surname>Helms</surname></string-name>, <string-name><given-names>A. P.</given-names> <surname>Godfrey</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Ames</surname></string-name>, and <string-name><given-names>E. S.</given-names> <surname>Bridge</surname></string-name></person-group>. <article-title>Predator foraging altitudes reveal the structure of aerial insect communities</article-title>. <source>Scientific Reports</source>, <volume>6</volume>(<issue>1</issue>), <year>2016</year>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Khuong</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gautrais</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Perna</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Sbäı</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Combe</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Kuntz</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Jost</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Theraulaz</surname></string-name></person-group>. <article-title>Stigmergic construction and topochemical information shape ant nest architecture</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>5</issue>), <year>2016</year>.</mixed-citation></ref>
    <ref id="c25"><label>25.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Kodzhabashev</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name></person-group>, <year>2015</year>. <article-title>Route following without scanning</article-title>. In <conf-name>Biomimetic and Biohybrid Systems. Living Machines 2015</conf-name>.  </mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names> <surname>Le Möel</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name></person-group>. <article-title>Opponent processes in visual memories: A model of attraction and repulsion in navigating insects’ mushroom bodies</article-title>. <source>PLoS Computational Biology</source>, <volume>16</volume>(<issue>2</issue>), <year>2020</year>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Lehrer</surname></string-name></person-group>. <article-title>Why do bees turn back and look?</article-title> <source>Journal of Comparative Physiology A</source>, <volume>172</volume>(<issue>5</issue>), <year>1993</year>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Lehrer</surname></string-name> and <string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name></person-group>. <article-title>Approaching and departing bees learn different cues to the distance of a landmark</article-title>. <source>Journal of Comparative Physiology A</source>, <volume>175</volume>(<issue>2</issue>), <year>1994</year>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="thesis"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Lobecke</surname></string-name></person-group>. <source>Local homing of the bumblebee, Bombus terrestris</source>. PhD thesis, <publisher-name>ielefeld University</publisher-name>, <year>2018</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Lobecke</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Kern</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name></person-group>. <article-title>Taking a goal-centred dynamic snapshot as a possibility for local homing in initially näıve bumblebees</article-title>. <source>Journal of Experimental Biology</source>, <volume>221</volume>(<issue>2</issue>), <year>2018</year>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Menzel</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Tison</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Fischer-Nakai</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Cheeseman</surname></string-name>, <string-name><given-names>M. S.</given-names> <surname>Balbuena</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Landgraf</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Petrasch</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Polster</surname></string-name>, and <string-name><given-names>U.</given-names> <surname>Greggers</surname></string-name></person-group>. <article-title>Guidance of navigating honeybees by learned elongated ground structures</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, <volume>12</volume>:<issue>322</issue>, <year>2019</year>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Möller</surname></string-name></person-group>. <article-title>A model of ant navigation based on visual prediction</article-title>. <source>Journal of Theoretical Biology</source>, <volume>305</volume>, <year>2012</year>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names> <surname>Murray</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name></person-group>. <article-title>Quantifying navigational information: The catchment volumes of panoramic snapshots in outdoor scenes</article-title>. <source>PLoS One</source>, <volume>12</volume>(<issue>10</issue>), <year>2017</year>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. M.</given-names> <surname>Müller</surname></string-name>, <string-name><given-names>O. J. N.</given-names> <surname>Bertrand</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Differt</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name></person-group>. <article-title>The problem of home choice in skyline-based homing</article-title>. <source>PLoS One</source>, <volume>13</volume>(<issue>3</issue>), <year>2018</year>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names> <surname>Odenthal</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Doussot</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Meyer</surname></string-name>, and <string-name><given-names>O. J. N.</given-names> <surname>Bertrand</surname></string-name></person-group>. <article-title>Analysing Head-Thorax Choreography During Free-Flights in Bumblebees</article-title>. <source>Frontiers in Behavioral Neuroscience</source>, <volume>14</volume>, <year>2021</year>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. L.</given-names> <surname>Osborne</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>S. J.</given-names> <surname>Clark</surname></string-name>, <string-name><given-names>D. R.</given-names> <surname>Reynolds</surname></string-name>, <string-name><given-names>M. C.</given-names> <surname>Barron</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Lim</surname></string-name>, and <string-name><given-names>A. M.</given-names> <surname>Reynolds</surname></string-name></person-group>. <article-title>The ontogeny of bumblebee flight trajectories: From Näıve explorers to experienced foragers</article-title>. <source>PLoS One</source>, <volume>8</volume>(<issue>11</issue>), <year>2013</year>.</mixed-citation></ref>
    <ref id="c37"><label>37.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Steadman</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Dewar</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Walker</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name></person-group>. <article-title>Insect-inspired visual navigation for flying robots</article-title>. In <conf-name>Biomimetic and Biohybrid Systems. Living Machines 2016</conf-name>, <year>2016</year>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Ravi</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Siesenop</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Bertrand</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Doussot</surname></string-name>, <string-name><given-names>W. H.</given-names> <surname>Warren</surname></string-name>, <string-name><given-names>S. A.</given-names> <surname>Combes</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name></person-group>. <article-title>Bumblebees perceive the spatial layout of their environment in relation to their body size and form to minimize inflight collisions</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>117</volume>(<issue>49</issue>), <year>2020</year>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names> <surname>Robert</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Frasnelli</surname></string-name>, <string-name><given-names>N. H.</given-names> <surname>De Ibarra</surname></string-name>, and <string-name><given-names>T. S.</given-names> <surname>Collett</surname></string-name></person-group>. <article-title>Variations on a theme: Bumblebee learning flights from the nest and from flowers</article-title>. <source>Journal of Experimental Biology</source>, <volume>221</volume>(<issue>4</issue>), <year>2018</year>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Schwegmann</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Lindemann</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name></person-group>. <article-title>Temporal Statistics of Natural Image Sequences Generated by Movements with Insect Flight Characteristics</article-title>. <source>PLoS One</source>, <volume>9</volume>(<issue>10</issue>), <year>2014</year>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Sonntag</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Lihoreau</surname></string-name>, <string-name><given-names>O. J. N.</given-names> <surname>Bertrand</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Egelhaaf</surname></string-name></person-group>. <article-title>Bumblebees increase their learning flight altitude in dense environments</article-title>. <source>bioRxiv</source>, <year>2024</year>. Pages: 2024.10.14.618154 Section: New Results.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Stankiewicz</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Webb</surname></string-name></person-group>. <article-title>Looking down: a model for visual route following in flying insects</article-title>. <source>Bioinspiration &amp; Biomimetics</source>, <volume>16</volume>(<issue>5</issue>), <year>2021</year>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names> <surname>Stone</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Webb</surname></string-name></person-group>. <article-title>Rotation invariant visual processing for spatial memory in insects</article-title>. <source>Interface Focus</source>, <volume>8</volume>(<issue>4</issue>), <year>2018</year>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.</given-names> <surname>Stürzl</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Boeddeker</surname></string-name>, and <string-name><given-names>J. M.</given-names> <surname>Hemmi</surname></string-name></person-group>. <article-title>How wasps acquire and use views for homing</article-title>. <source>Current Biology</source>, <volume>26</volume>(<issue>4</issue>), <year>2016</year>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>X.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Yue</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name></person-group>. <article-title>A decentralised neural model explaining optimal integration of navigational strategies in insects</article-title>. <source>eLife</source>, <volume>9</volume>, <year>2020</year>.</mixed-citation></ref>
    <ref id="c46"><label>46.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Thoma</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Fisher</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Bertrand</surname></string-name>, and <string-name><given-names>C.</given-names> <surname>Braun</surname></string-name></person-group>. <article-title>Evaluation of possible flight strategies for close object evasion from bumblebee experiments</article-title>. In <conf-name>Biomimetic and Biohybrid Systems. Living Machines 2020</conf-name>, <year>2021</year>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Webb</surname></string-name></person-group>. <article-title>The internal maps of insects</article-title>. <source>Journal of Experimental Biology</source>, <volume>222</volume>, <year>2019</year>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Wehner</surname></string-name></person-group>. <article-title>The architecture of the desert ant’s navigational toolkit (hymenoptera: Formicidae)</article-title>. <source>Myrmecological News</source>, <volume>12</volume>, <year>2009</year>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. L.</given-names> <surname>Woodgate</surname></string-name>, <string-name><given-names>J. C.</given-names> <surname>Makinson</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Lim</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Reynolds</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Chittka</surname></string-name></person-group>. <article-title>Life-long radar tracking of bumblebees</article-title>. <source>PLoS One</source>, <volume>11</volume>(<issue>8</issue>), <year>2016</year>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mangan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Philippides</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Graham</surname></string-name></person-group>. <article-title>Snapshots in ants? New interpretations of paradigmatic experiments</article-title>. <source>Journal of Experimental Biology</source>, <volume>216</volume>(<issue>10</issue>), <year>2013</year>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Wystrach</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Schwarz</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Schultheiss</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Beugnon</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Cheng</surname></string-name></person-group>. <article-title>Views, landmarks, and routes: How do desert ants negotiate an obstacle course?</article-title> <source>Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioral Physiology</source>, <volume>197</volume>, <year>2011</year>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name></person-group>. <article-title>Visual homing: an insect perspective</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>22</volume>(<issue>2</issue>), <year>2012</year>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name></person-group>. <article-title>Visual navigation: properties, acquisition and use of views</article-title>. <source>Journal of Comparative Physiology A</source>, <volume>209</volume>, <year>2022</year>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Zeil</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Hofmann</surname></string-name>, and <string-name><given-names>J. S.</given-names> <surname>Chahl</surname></string-name></person-group>. <article-title>Catchment areas of panoramic snapshots in outdoor scenes</article-title>. <source>Journal of the Optical Society of America A</source>, <volume>20</volume>(<issue>3</issue>), <year>2003</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99140.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this <bold>useful</bold> study, the authors tested the ability of bumblebees to use bird-view and ground-view for homing in cluttered landscapes using modeling and behavioral experiments, claiming that bumblebees rely most on ground-views for homing. However, due to a lack of analysis of the bees' behavior during training and a lack of information as to how the homing behavior of bees develops over time, the evidence supporting their claims is currently <bold>incomplete</bold>. Moreover, there was concern that the experimental environment was not representative of natural scenes, thus limiting the findings of the study.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99140.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In a 1.5m diameter, 0.8m high circular arena bumblebees were accustomed to exit the entrance to their nest on the floor surrounded by an array of identical cylindrical landmarks and to forage in an adjacent compartment which they could reach through an exit tube in the arena wall at a height of 28cm. The movements of one group of bees were restricted to a height of 30cm, the height of the landmark array, while the other group was able to move up to heights of 80cm, thus being able to see the landmark array from above.</p>
<p>During one series of tests, the flights of bees returning from the foraging compartment were recorded as they tried to reach the nest entrance on the floor of the arena with the landmark array shifted to various positions away from the true nest entrance location. The results of these tests showed that the bees searched for the net entrance in the location that was defined by the landmark array.</p>
<p>In a second series of tests, access to the landmark array was prevented from the side, but not from top, by a transparent screen surrounding the landmark array. These tests showed that the bees of both groups rarely entered the array from above, but kept trying to enter it from the side.</p>
<p>The authors express surprise at this result because modelling the navigational information supplied by panoramic snapshots in this arena had indicated that the most robust information to the location of the nest entrance within the landmark array was supplied by views of the array from above, leading to the following strong conclusions:</p>
<p>line 51: &quot;Snapshot models perform best with bird's eye views&quot;;</p>
<p>
line 188: &quot;Overall, our model analysis could show that snapshot models are not able to find home with views within a cluttered environment but only with views from above it.&quot;;</p>
<p>
line 231: &quot;Our study underscores the limitations inherent in snapshot models, revealing their inability to provide precise positional estimates within densely cluttered environments, especially when compared to the navigational abilities of bees using frog's-eye views.&quot;</p>
<p>Strengths:</p>
<p>The experimental set-up allows to record the flight behaviour of bees in great spatial and temporal detail and in principle also to reconstruct the visual information available to the bees throughout the arena.</p>
<p>Modelling: The revised manuscript now presents the results of modelling that includes information potentially available to the bees from the arena wall and in particular from the top edge of the arena.</p>
<p>As I predicted, this increases the width of rotational image difference functions and therefore provides directional guidance over a larger range of misalignments. However, the authors dismiss the modelling results based on such reconstructed views which more realistically describe the information available to the bumblebees, because (line 291ff): 'Further simulations with a rendered arena wall led to worse results because the agent was mainly led to the centre of the arena (Fig. S17, Fig. S18-21)&quot;.</p>
<p>What the modelling in Fig. 17 actually shows is that the agent is led more or less exactly to the 'entry points' to the arena chosen by the real bees (Fig. 4). The authors ignore this and in their rebuttal state that 'We hypothesised that the arena wall and object location created ambiguity'. The problem here is that you don't remove potential 'ambiguity' for real bees by ignoring information they are unlikely to ignore.</p>
<p>Behavioural analysis: The full potential of the set-up was not used to understand how the bees' navigation behaviour develops over time in this arena and what opportunities the bees have had to learn the location of the nest entrance during repeated learning flights and return flights.</p>
<p>Without a detailed analysis of the bees' behaviour during 'training', including learning flights and return flights, it is very hard to follow the authors' conclusions. The behaviour that is observed in the tests may be the result of the bees' extended experience shuttling between the nest and the entry to the foraging arena at 28cm height in the arena wall. For instance, it would have been important to see the return flights of bees following the learning flights shown in Fig. 17.</p>
<p>Basically both groups of bees (constrained to fly below the height of landmarks (F) or throughout the height of the arena (B)) had ample opportunities to learn that the nest entrance lies on the floor of the landmark array. The only reason why B-bees may not have entered the array from above when access from the side was prevented may simply be that bumblebees, because they bumble, find it hard to perform a hovering descent into the array.</p>
<p>The revised manuscript does not address my concerns. The rebuttal states that a detailed analysis of learning and return flights was 'outside the scope of this particular study', that their experimental design 'does not require the entire history of the bee's trajectory to be tested', that 'the entire flight history...will require...effort...conceptually' and that it would be 'difficult to test a hypothesis'.</p>
<p>These responses clarify the frustrating problem with this study: The authors are more concerned with testing hypotheses than with trying to understand how bumblebees learn to cope with a situation which constrains their learning choreography and confronts them with the one fundamental problem view-based homing has: repetitive scene elements.</p>
<p>Homing is an experience-dependent process and to understand what cues the bees used to navigate this set-up requires an analysis of the whole learning process. For instance, it may well be that the B+G+ bees initially did enter the array from above, but subsequently learnt a more efficient route into the array, by simply entering it from the side, followed by 'unguided' searching.</p>
<p>General: The most serious weakness of the set-up is that it is spatially and visually constrained, in particular lacking a distant visual panorama, which under natural conditions is crucial for the range over which rotational image difference functions provide navigational guidance. In addition, the array of identical landmarks is not representative of natural clutter and, because it is visually repetitive, poses unnatural problems for view-based homing algorithms. This is the reason why the functions degrade so quickly from one position to the next (Fig. 9-12) when more distant scene elements are excluded.</p>
<p>In conclusion, I do not feel that I have learnt anything useful from this experiment; it does suggest, however, that to fully appreciate and understand the homing abilities of insects, there is no alternative but to investigate these abilities in the natural conditions in which they have evolved. A nice start would be to build camera-based 3D models of natural bumblebee nest entrance environments and analyse whether there are any particularly unusual challenges for the visual localization of the nest entrance.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99140.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sonntag</surname>
<given-names>Annkathrin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3139-3981</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Sauzet</surname>
<given-names>Odile</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1029-8846</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Lihoreau</surname>
<given-names>Mathieu</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2463-2040</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Egelhaaf</surname>
<given-names>Martin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9336-4270</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bertrand</surname>
<given-names>Olivier</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0889-4550</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1 (Public Review):</bold></p>
<p>Summary:</p>
<p>In this paper, the authors aimed to test the ability of bumblebees to use bird-view and ground-view for homing in cluttered landscapes. Using modelling and behavioural experiments, the authors showed that bumblebees rely most on ground-views for homing.</p>
<p>Strengths:</p>
<p>The behavioural experiments are well-designed, and the statistical analyses are appropriate for the data presented.</p>
<p>Weaknesses:</p>
<p>Views of animals are from a rather small catchment area.</p>
<p>Missing a discussion on why image difference functions were sufficient to explain homing in wasps (Murray and Zeil 2017).</p>
<p>The artificial habitat is not really 'cluttered' since landmarks are quite uniform, making it difficult to infer ecological relevance.</p>
</disp-quote>
<p>Thank you for your thorough evaluation of our study. We aimed to investigate local homing behaviour on a small spatial scale, which is ecologically relevant given that the entrance of bumblebee nests is often inconspicuously hidden within the vegetation. This requires bees to locate their nest hole within a confined area. While many studies have focused on larger spatial scales using radar tracking (e.g. Capaldi et al. 2000; Osborne et al. 2013; Woodgate et al. 2016), there is limited understanding of the mechanisms behind local homing, especially in dense environments as we propose here.</p>
<p>We appreciate your suggestion to include the study by Murray and Zeil (2017) in our discussion. Their research explored the catchment areas of image difference functions on a larger spatial scale with a cubic volume of 5m x 5m x 5m. Aligned with their results, we found that image difference functions pointed towards the location of the objects surrounding the nest when the images were taken above the objects. However, within the clutter, i.e. the dense set of objects surrounding the nest, the model did not perform well in pinpointing the nest position.</p>
<p>See the new discussion at lines 192-197</p>
<p>We agree with your comment about the term &quot;clutter&quot;. Therefore, we referred to our landmark arrangement as a &quot;dense environment&quot; instead. Uniformly distributed objects do indeed occur in nature, as seen in grasslands, flower meadows, or forests populated with similar plants.</p>
<p>See line 20 and we changed the wording throughout the manuscript and figures.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1 (Recommendations):</bold></p>
<p>The manuscript is well written, nicely designed experiments and well illustrated. I have a few comments below.</p>
<p>It would be useful to discuss known data of learning flights in bumblebees, and the height or catchment area of their flights. This will allow the reader to compare your exp design to the natural learning flights.</p>
</disp-quote>
<p>In our study, we first focused on demonstrating the ability to solve a homing task in a dense environment. As we observed the bees returning within the dense environment and not from above it (contrary to the model predictions), we investigated whether they flew above it during their first flights. The bees did indeed fly above, demonstrating their ability to ascend and descend within the constellation of objects (see Supplementary Material Fig. 22).</p>
<p>In nature, the learning flight of bumblebees may cover several decametres, with the loops performed during these flights increasing with flight time (e.g. Osborne et al. 2013; Woodgate et al. 2016). A similar pattern can be observed on a smaller spatial scale (e.g. Philippides et al. 2013). Similar to the loops that extend over time, the bees gradually gain altitude (Lobecke et al., 2018). However, these observations come from studies where few conspicuous objects surround the nest entrance.</p>
<p>Although our study  focussed on the performance in goal finding in cluttered environments, we now also address the issue of learning flights in the discussion, as learning flights are the scaffolding of visual learning. We have already conducted several learning flight experiments to fill the knowledge gap mentioned above. These will allow us in a forthcoming paper to compare learning flights in this environment with the existing literature (Sonntag et al., 2024).</p>
<p>We added a reference to this in the discussion (lines 218-219 and 269-272)</p>
<p>Include bumblebee in the title rather than 'bees'.</p>
<p>We adapted the title accordingly:</p>
<p>“Switching perspective: Comparing ground-level and bird’s-eye views for bumblebees navigating dense environments”</p>
<disp-quote content-type="editor-comment">
<p>I found switching between bird-views and frog-views to explain bee-views slightly tricky to read. Why not use 'ground-views', which you already have in the title?</p>
</disp-quote>
<p>We agree and adapted the wording in the manuscript according to this suggestion.</p>
<disp-quote content-type="editor-comment">
<p>I am not convinced there is evidence here to suggest the bees do not use view-based navigation, because of the following: In L66: unclear what were the views centred around, I assume it is the nest. Is 45cm above the ground the typical height gained by bumblebees during learning flight? The clutter seems to be used more as an obstacle that they are detouring to reach the goal, isn't it?</p>
</disp-quote>
<p>Based on many previous studies, view-based navigation can be assumed to be one of the plausible mechanisms bees use for homing (Cartwright &amp; Collett, 1987; Doussot et al., 2020; Lehrer &amp; Collett, 1994; Philippides et al., 2013; Zeil, 2022). In our tests, when the dense environment was shifted to a different position in the flight arena, almost no bees searched at the real location of the nest entrance but at the fictive new location within the dense environment, indicating that the bees assumed  the nest to be located within the dense environment, and therefore  that vision played a crucial role for homing. We thus never meant that the bees were not using view-based navigation. We clarified this point in the revised manuscript.</p>
<p>See lines 247-248, 250-259, added visual memory to schematic in Fig. 6</p>
<p>In our model simulations, the memorised snapshots were centred around the nest. However, we found that a multi-snapshot model could not explain the behaviour of the bees. This led us to suggest that bees likely employ acombination of multiple mechanisms for navigation.</p>
<p>We refined paragraph about possible alternative homing mechanisms. See lines  218-263</p>
<p>The height of learning flights has not been extensively investigated in previous studies, and typical heights are not well-documented in the literature. However, from our observations of the first outbound flights of bumblebees within the dense environment, we noted that they quickly increased their altitude and then flew above the objects. Since the objects had a height of 0.3 metres, we chose 0.45 metres as a height above the objects for our study.</p>
<p>Furthermore, the nest is positioned within the arrangement of objects, making it a target the bees must actively find rather than detour around.</p>
<disp-quote content-type="editor-comment">
<p>I think a discussion to contrast your findings with Murray and Zeil 2017 will be useful. It was unclear to me whether the flight arena had UV availability, if it didn't, this could be a reason for the difference.</p>
</disp-quote>
<p>We referred to this study in the discussion of the revised paper (see our response to the public review). Lines 192-197</p>
<p>As in most lab studies on local homing, the bees did not have UV light available in the arena. Even without this, they were successful in finding their nest position during the tests. We clarified that in the revised manuscript. See line 334-336</p>
<disp-quote content-type="editor-comment">
<p>Figure 2A, can you add a scale bar?</p>
</disp-quote>
<p>We added a scale bar to the figure showing the dimensions of the arena. See Fig. 2</p>
<disp-quote content-type="editor-comment">
<p>The citation of figure orders is slightly off. We have Figure 5 after Figure 2, without citing Figures 3 and 4. Similarly for a few others.</p>
</disp-quote>
<p>We carefully checked the order of cited figures and adapted them.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2 (Public Review):</bold></p>
<p>Summary:</p>
<p>In a 1.5m diameter, 0.8m high circular arena bumblebees were accustomed to exiting the entrance to their nest on the floor surrounded by an array of identical cylindrical landmarks and to forage in an adjacent compartment which they could reach through an exit tube in the arena wall at a height of 28cm. The movements of one group of bees were restricted to a height of 30cm, the height of the landmark array, while the other group was able to move up to heights of 80cm, thus being able to see the landmark array from above.</p>
<p>During one series of tests, the flights of bees returning from the foraging compartment were recorded as they tried to reach the nest entrance on the floor of the arena with the landmark array shifted to various positions away from the true nest entrance location. The results of these tests showed that the bees searched for the net entrance in the location that was defined by the landmark array.</p>
<p>In a second series of tests, access to the landmark array was prevented from the side, but not from the top, by a transparent screen surrounding the landmark array. These tests showed that the bees of both groups rarely entered the array from above, but kept trying to enter it from the side.</p>
<p>The authors express surprise at this result because modelling the navigational information supplied by panoramic snapshots in this arena had indicated that the most robust information about the location of the nest entrance within the landmark array was supplied by views of the array from above, leading to the following strong conclusions: line 51: &quot;Snapshot models perform best with bird's eye views&quot;; line 188: &quot;Overall, our model analysis could show that snapshot models are not able to find home with views within a cluttered environment but only with views from above it.&quot;; line 231: &quot;Our study underscores the limitations inherent in snapshot models, revealing their inability to provide precise positional estimates within densely cluttered environments, especially when compared to the navigational abilities of bees using frog's-eye views.&quot;</p>
<p>Strengths:</p>
<p>The experimental set-up allows for the recording of flight behaviour in bees, in great spatial and temporal detail. In principle, it also allows for the reconstruction of the visual information available to the bees throughout the arena.</p>
<p>The experimental set-up allows for the recording of flight behaviour in bees, in great spatial and temporal detail. In principle, it also allows for the reconstruction of the visual information available to the bees throughout the arena.</p>
<p>Weaknesses:</p>
<p>Modelling:</p>
<p>Modelling left out information potentially available to the bees from the arena wall and in particular from the top edge of the arena and cues such as cameras outside the arena. For instance, modelled IDF gradients within the landmark array degrade so rapidly in this environment, because distant visual features, which are available to bees, are lacking in the modelling. Modelling furthermore did not consider catchment volumes, but only horizontal slices through these volumes.</p>
</disp-quote>
<p>When we started modelling the bees’ homing based on image-matching, we included the arena wall. However, the model simulations pointed only coarsely towards the dense environment but not toward the nest position. We hypothesised that the arena wall and object location created ambiguity. Doussot et al. (2020) showed that such a model can yield two different homing locations when distant and local cues are independently moved. Therefore, we reduced the complexity of the environment by concentrating on the visual features, which were moved between training and testing (neither the camera nor the wall were moved between training and test). We acknowledge that this information should have been provided to substantiate our reasoning. As such, we included model results with the arena wall in the supplements of the revised paper. See lines 290-293, Figures S17-21</p>
<p>We agree that the catchment volumes would provide quantitatively more detailed information as catchment slices. Nevertheless, since our goal was  to investigate if bees would use ground views or bird's eye views to home in a dense environment, catchment slices, which provide qualitatively similar information as catchment volumes, are sufficient to predict whether ground or bird's-eye views perform better in leading to the nest. Therefore, we did not include further computations of catchment volumes. (ll. 296-297)</p>
<disp-quote content-type="editor-comment">
<p>Behavioural analysis:</p>
<p>The full potential of the set-up was not used to understand how the bees' navigation behaviour develops over time in this arena and what opportunities the bees have had to learn the location of the nest entrance during repeated learning flights and return flights.</p>
<p>Without a detailed analysis of the bees' behaviour during 'training', including learning flights and return flights, it is very hard to follow the authors' conclusions. The behaviour that is observed in the tests may be the result of the bees' extended experience shuttling between the nest and the entry to the foraging arena at 28cm height in the arena wall. For instance, it would have been important to see the return flights of bees following the learning flights shown in Figure 17. Basically, both groups of bees (constrained to fly below the height of landmarks (F) or throughout the height of the arena (B)) had ample opportunities to learn that the nest entrance lies on the floor of the landmark array. The only reason why B-bees may not have entered the array from above when access from the side was prevented, may simply be that bumblebees, because they bumble, find it hard to perform a hovering descent into the array.</p>
</disp-quote>
<p>A prerequisite for studying the learning flight in a given environment is showing that the bees manage to return to their home. Here, our primary goal was to demonstrate this within a dense environment. While we understand that a detailed analysis of the learning and return flights would be valuable, we feel this is outside the scope of this particular study.</p>
<p>Multi-snapshot models have been repeatedly shown to be sufficient to explain the homing behaviour in natural as well as artificial environments(Baddeley et al., 2012; Dittmar et al., 2010; Doussot et al., 2020; Möller, 2012; Wystrach et al., 2011, 2013; Zeil, 2012). A model can not only be used to replicate but also to predict a given outcome and shape the design of experiments. Here, we used the models to shape the experimental design, as it does not require the entire history of the bee's trajectory to be tested and provides interesting insight into homing in diverse environments.</p>
<p>Since we observed behavioural responses different from the one suggested by the models, it becomes interesting to look at the flight history. If we had found an alignment between the model and the behaviour, looking at thehistory would have become much less interesting. Thus our results raise an interest in looking at the entire flight history, which will require not only effort on the recording procedure, but as well conceptually. At the moment the underlying mechanisms of learning during outbound, inbound, exploration, or orientation flight remains evasive and therefore difficult to test a hypothesis. A detailed description of the flight during the entire bee history would enable us to speculate alternative models to the one tested in our study, but would remain limited in testing those.</p>
<p>While we acknowledge that the bees had ample opportunities to learn the location of the nest entrance, we believe that their behaviour of entering the dense environment at a very low altitude cannot be solely explained by extended experience. It is possible that the bees could have also learned to enter at the edge of the objects or above the objects before descending within the dense environment.</p>
<disp-quote content-type="editor-comment">
<p>General:</p>
<p>The most serious weakness of the set-up is that it is spatially and visually constrained, in particular lacking a distant visual panorama, which under natural conditions is crucial for the range over which rotational image difference functions provide navigational guidance. In addition, the array of identical landmarks is not representative of natural clutter and, because it is visually repetitive, poses un-natural problems for view-based homing algorithms. This is the reason why the functions degrade so quickly from one position to the next (Figures 9-12), although it is not clear what these positions are (memory0-memory7).</p>
<p>In conclusion, I do not feel that I have learnt anything useful from this experiment; it does suggest, however, that to fully appreciate and understand the homing abilities of insects, there is no alternative but to investigate these abilities in the natural conditions in which they have evolved.</p>
</disp-quote>
<p>We respectfully disagree with the evaluation that our study does not provide new insights due to the controlled laboratory conditions. Both field and laboratory research are necessary and should complement each other. Dismissing the value of controlled lab experiments would overlook the contributions of previous lab-based research, which has significantly advanced our understanding of animal behaviour. It is only possible to precisely define the visual test environments under laboratory conditions and to identify the role of the components of the environment for the behaviour through targeted variation of them. These results yield precious information to then guide future field-based experiments for validation.</p>
<p>Our laboratory settings are a kind of abstraction of natural situations focusing on those aspects that are at the centre of the research question. Our approach here was based on the knowledge that bumblebees have to find their inconspicuous nest hole in nature, which is difficult to find in often highly dense environments, and ultimately on a spatial scale in the metre range. We first wanted to find out if bumblebees can find their nest hole under the particularly challenging condition that all objects surrounding the nest hole are the same. This was not yet clear. Uniformly distributed objects may, however, also occur in nature, as seen with visually inconspicuous nest entrances of bumblebees in grass meadows, flower meadows, or forests with similar plants. We agree that the term &quot;clutter&quot; is not well-defined in the literature and now refer to the  environment as a &quot;dense environment.&quot;</p>
<p>We changed the wording throughout the manuscript and figures.</p>
<p>Despite the lack of a distant visual panorama, or also UV light, wind, or other confounding factors inherent to field work conditions, the bees successfully located the nest position even when we shifted the dense environment within the flight arena. We used rotational-image difference functions based on snapshots taken around the nest position to predict the bees' behaviour, as this is one of the most widely accepted and computationally most parsimonious assessments of catchment areas in the context of local homing. This approach also proved effective in our more restricted conditions, where the bees still managed to pinpoint their home.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2 (Recommendations):</bold></p>
<p>(1) Clarify what is meant by modelling panoramic images at 1cm intervals (only?) along the x-axis of the arena.</p>
</disp-quote>
<p>The panoramic images were taken along a grid with 0.5cm steps within the dense environment and 1cm steps in the rest of the arena. A previous study (Doussot et al., 2020) showed successful homing of multi-snapshot models in an environment of similar scale with a grid with 2cm steps. Therefore, we think that our scaling is sufficiently fine. We apologise for the missing information in the method section and added it to the revised manuscript. See lines 286-287</p>
<disp-quote content-type="editor-comment">
<p>(2) In Figures 9-12 what are the memory0 to memory7 locations and reference image orientations? Explain clearly which image comparisons generated the rotIDFs shown.</p>
</disp-quote>
<p>Memory 0 to memory 7 are examples of the eight memorised snapshots, which are aligned in the nest direction and taken around the nest. In the rotIDFs shown, we took memory 0 as a reference image, and compared the 7 others by rotating them against memory 0. We clarified that in the revised manuscript.</p>
<p>See revised figure caption in Fig. S9 – 16</p>
<disp-quote content-type="editor-comment">
<p>(3) Figure 9 seems to compare 'bird's-eye', not 'frog's-eye' views.</p>
</disp-quote>
<p>We apologise for that mistake and carefully double-checked the figure caption.</p>
<p>See revised figure caption Fig. S9</p>
<disp-quote content-type="editor-comment">
<p>(4) Why do you need to invoke a PI vector (Figure 6) to explain your results?</p>
</disp-quote>
<p>Since the bees were able to home in the dense environment without entering the object arrangement from above but from the side, image matching alone could not explain the bees’ behaviour. Therefore, we suggest, as an hypothesis for future studies, a combination of mechanisms such as a home vector. Other alternatives, perhaps without requiring a PI vector, may explain the bees’ behaviour, and we will welcome any future contributions from the scientific community.</p>
<p>References</p>
<p>Baddeley, B., Graham, P., Husbands, P., &amp; Philippides, A. (2012). A Model of Ant Route Navigation Driven by Scene Familiarity. PLoS Computational Biology,8(1), e1002336. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002336">https://doi.org/10.1371/journal.pcbi.1002336</ext-link></p>
<p>Capaldi, E. A., Smith, A. D., Osborne, J. L., Farris, S. M., Reynolds, D. R., Edwards, A. S., Martin, A., Robinson, G. E., Poppy, G. M., &amp; Riley, J. R. (2000).</p>
<p>Ontogeny of orientation flight in the honeybee revealed by harmonic radar. Nature, 403. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35000564">https://doi.org/10.1038/35000564</ext-link></p>
<p>Cartwright, B. A., &amp; Collett, T. S. (1987). Landmark maps for honeybees. Biological Cybernetics, 57(1), 85–93. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00318718">https://doi.org/10.1007/BF00318718</ext-link></p>
<p>Dittmar, L., Stürzl, W., Baird, E., Boeddeker, N., &amp; Egelhaaf, M. (2010). Goal seeking in honeybees: Matching of optic flow snapshots? Journal of Experimental Biology, 213(17), 2913–2923. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1242/jeb.043737">https://doi.org/10.1242/jeb.043737</ext-link></p>
<p>Doussot, C., Bertrand, O. J. N., &amp; Egelhaaf, M. (2020). Visually guided homing of bumblebees in ambiguous situations: A behavioural and modelling study. PLoS Computational Biology, 16(10). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1008272">https://doi.org/10.1371/journal.pcbi.1008272</ext-link></p>
<p>Lehrer, M., &amp; Collett, T. S. (1994). Approaching and departing bees learn different cues to the distance of a landmark. Journal of Comparative Physiology A, 175(2), 171–177. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00215113">https://doi.org/10.1007/BF00215113</ext-link></p>
<p>Lobecke, A., Kern, R., &amp; Egelhaaf, M. (2018). Taking a goal-centred dynamic snapshot as a possibility for local homing in initially naïve bumblebees. Journal of Experimental Biology, 221(2), jeb168674. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1242/jeb.168674">https://doi.org/10.1242/jeb.168674</ext-link></p>
<p>Möller, R. (2012). A model of ant navigation based on visual prediction. Journal of Theoretical Biology, 305, 118–130. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jtbi.2012.04.022">https://doi.org/10.1016/j.jtbi.2012.04.022</ext-link></p>
<p>Murray, T., &amp; Zeil, J. (2017). Quantifying navigational information: The catchment volumes of panoramic snapshots in outdoor scenes. PLOS ONE, 12(10), e0187226. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0187226">https://doi.org/10.1371/journal.pone.0187226</ext-link></p>
<p>Osborne, J. L., Smith, A., Clark, S. J., Reynolds, D. R., Barron, M. C., Lim, K. S., &amp; Reynolds, A. M. (2013). The ontogeny of bumblebee flight trajectories: From Naïve explorers to experienced foragers. PLoS ONE, 8(11). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0078681">https://doi.org/10.1371/journal.pone.0078681</ext-link></p>
<p>Philippides, A., de Ibarra, N. H., Riabinina, O., &amp; Collett, T. S. (2013). Bumblebee calligraphy: The design and control of flight motifs in the learning and return flights of Bombus terrestris. Journal of Experimental Biology, 216(6), 1093–1104. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1242/jeb.081455">https://doi.org/10.1242/jeb.081455</ext-link></p>
<p>Sonntag, A., Lihoreau, M., Bertrand, O. J. N., &amp; Egelhaaf, M. (2024). Bumblebees increase their learning flight altitude in dense environments. bioRxiv, 2024.10.14.618154. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2024.10.14.618154">https://doi.org/10.1101/2024.10.14.618154</ext-link></p>
<p>Woodgate, J. L., Makinson, J. C., Lim, K. S., Reynolds, A. M., &amp; Chittka, L. (2016). Life-long radar tracking of bumblebees. PLoS ONE, 11(8). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0160333">https://doi.org/10.1371/journal.pone.0160333</ext-link></p>
<p>Wystrach, A., Mangan, M., Philippides, A., &amp; Graham, P. (2013). Snapshots in ants? New interpretations of paradigmatic experiments. Journal of Experimental Biology, 216(10), 1766–1770. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1242/jeb.082941">https://doi.org/10.1242/jeb.082941</ext-link></p>
<p>Wystrach, A., Schwarz, S., Schultheiss, P., Beugnon, G., &amp; Cheng, K. (2011). Views, landmarks, and routes: How do desert ants negotiate an obstacle course? Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioral Physiology, 197(2), 167–179. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00359-010-0597-2">https://doi.org/10.1007/s00359-010-0597-2</ext-link></p>
<p>Zeil, J. (2012). Visual homing: An insect perspective. Current Opinion in Neurobiology, 22(2), 285–293. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2011.12.008">https://doi.org/10.1016/j.conb.2011.12.008</ext-link></p>
<p>Zeil, J. (2022). Visual navigation: Properties, acquisition and use of views. Journal of Comparative Physiology A. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00359-022-01599-2">https://doi.org/10.1007/s00359-022-01599-2</ext-link></p>
</body>
</sub-article>
</article>