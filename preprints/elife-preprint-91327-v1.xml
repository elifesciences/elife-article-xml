<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">91327</article-id>
<article-id pub-id-type="doi">10.7554/eLife.91327</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91327.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Early parafoveal semantic integration in natural reading</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0062-4326</contrib-id>
<name>
<surname>Pan</surname>
<given-names>Yali</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Frisson</surname>
<given-names>Steven</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Federmeier</surname>
<given-names>Kara D.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8193-8348</contrib-id>
<name>
<surname>Jensen</surname>
<given-names>Ole</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Centre for Human Brain Health, School of Psychology, University of Birmingham</institution>, Birmingham, <country>UK</country></aff>
<aff id="a2"><label>2</label><institution>Department of Psychology, Program in Neuroscience, and the Beckman Institute for Advanced Science and Technology, University of Illinois</institution>, Champaign, Illinois, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
<city>Hangzhou</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Corresponding author. Email: <email>Y.Pan.1@bham.ac.uk</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-11-28">
<day>28</day>
<month>11</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP91327</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-09-03">
<day>03</day>
<month>09</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-09-04">
<day>04</day>
<month>09</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.09.26.509511"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Pan et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Pan et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-91327-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Humans can read and comprehend text rapidly, implying that readers might process multiple words per fixation. However, the extent to which parafoveal words are previewed and integrated into the evolving sentence context remains disputed. We investigated parafoveal processing during natural reading by recording brain activity and eye movements using MEG and an eye tracker while participants silently read one-line sentences. The sentences contained an unpredictable target word that was either congruent or incongruent with the sentence context. To measure parafoveal processing, we flickered the target words at 60 Hz and measured the resulting brain responses (i.e., <italic>Rapid Invisible Frequency Tagging, RIFT</italic>) during fixations on the pre-target words. Our results revealed a significantly weaker tagging response for target words that were incongruent with the sentence context compared to congruent ones, even within 100 ms of fixating the word immediately preceding the target. This reduction in the RIFT response was also found to be predictive of individual reading speed. We conclude that semantic information is not only extracted from the parafovea but can also be integrated with the sentence context before the word is fixated. This early and extensive parafoveal processing supports the rapid word processing required for natural reading. Our study suggests that theoretical frameworks of natural reading should incorporate the concept of deep parafoveal processing.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>The discussion section in the main text has been updated to better explain the results. Figure 1 revised.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Reading is a remarkable human skill that requires rapid processing of written words. We typically fixate each word for only 225-250 ms, but nevertheless manage to encode its visual information, extract its meaning, and integrate it into the larger context, while also doing saccade planning (<xref ref-type="bibr" rid="c62">Rayner, 2009</xref>). To overcome the tight temporal constraints during reading, we preview the next word in the parafovea before moving our eyes to it (<xref ref-type="bibr" rid="c31">Jensen et al., 2021</xref>; <xref ref-type="bibr" rid="c72">Reichle and Reingold, 2013</xref>; <xref ref-type="bibr" rid="c74">Schotter, 2018</xref>). Substantial evidence suggests that parafoveal information can be extracted at various linguistic levels, including orthography (<xref ref-type="bibr" rid="c13">Drieghe et al., 2005</xref>; <xref ref-type="bibr" rid="c28">Inhoff, 1989</xref>; <xref ref-type="bibr" rid="c32">Johnson et al., 2007</xref>; <xref ref-type="bibr" rid="c89">White, 2008</xref>; <xref ref-type="bibr" rid="c90">Williams et al., 2006</xref>), phonology (<xref ref-type="bibr" rid="c3">Ashby et al., 2006</xref>; <xref ref-type="bibr" rid="c2">Ashby and Rayner, 2004</xref>; <xref ref-type="bibr" rid="c9">Chace et al., 2005</xref>; <xref ref-type="bibr" rid="c47">Miellet and Sparrow, 2004</xref>; <xref ref-type="bibr" rid="c61">Pollatsek et al., 1992</xref>; <xref ref-type="bibr" rid="c67">Rayner et al., 1995</xref>), lexicality (<xref ref-type="bibr" rid="c33">Kennedy and Pynte, 2005</xref>; <xref ref-type="bibr" rid="c35">Kliegl et al., 2006</xref>), syntax (<xref ref-type="bibr" rid="c81">Snell et al., 2017</xref>; <xref ref-type="bibr" rid="c88">Wen et al., 2019</xref>) and semantics (<xref ref-type="bibr" rid="c65">Rayner and Schotter, 2014</xref>; <xref ref-type="bibr" rid="c75">Schotter, 2013</xref>; <xref ref-type="bibr" rid="c78">Schotter et al., 2015</xref>; <xref ref-type="bibr" rid="c77">Schotter and Jia, 2016</xref>); for a comprehensive review see (<xref ref-type="bibr" rid="c76">Schotter et al., 2012</xref>). However, for semantics in particular, controversy remains about the extent and type of information extracted from parafoveal processing under various conditions. Moreover, it is unknown when and how the previewed semantic information can be used – i.e., integrated into the evolving sentence context – which is an integral component of the ongoing reading process.</p>
<p>For some time, it was claimed that parafoveal preview was limited to perceptual features of words and did not extend to semantics (<xref ref-type="bibr" rid="c29">Inhoff, 1982</xref>; <xref ref-type="bibr" rid="c30">Inhoff and Rayner, 1980</xref>; <xref ref-type="bibr" rid="c66">Rayner et al., 2014</xref>, <xref ref-type="bibr" rid="c64">1986</xref>). However, eye tracking-based evidence for the extraction of parafoveal semantic information began to emerge from studies that used languages other than English, including Chinese (<xref ref-type="bibr" rid="c85">Tsai et al., 2012</xref>; <xref ref-type="bibr" rid="c93">Yan et al., 2012</xref>, <xref ref-type="bibr" rid="c92">2009</xref>; <xref ref-type="bibr" rid="c98">Zhou et al., 2013</xref>) and German (<xref ref-type="bibr" rid="c26">Hohenstein et al., 2010</xref>; <xref ref-type="bibr" rid="c25">Hohenstein and Klieg, 2014</xref>). Moreover, more recent studies have also found evidence for semantic parafoveal effects in English, at least under some circumstances, such as when the parafoveal word is presented in a highly constraining context (<xref ref-type="bibr" rid="c78">Schotter et al., 2015</xref>) or made visually more salient by capitalizing the initial letter (<xref ref-type="bibr" rid="c65">Rayner and Schotter, 2014</xref>). Taken together, these studies suggest that semantic information can be extracted from the parafovea simultaneously with the processing of the current foveal word.</p>
<p>Complementary evidence showing that semantic information can be extracted parafoveally, even in English, comes from electrophysiological studies. Context-based facilitation of semantic processing can be observed as reductions in the amplitude of the N400 component (<xref ref-type="bibr" rid="c38">Kutas and Hillyard, 1984</xref>, <xref ref-type="bibr" rid="c39">1980</xref>), a negative-going event-related potential (ERP) response peaking at around 400 ms, which has been linked to semantic access (<xref ref-type="bibr" rid="c12">DeLong et al., 2014</xref>; <xref ref-type="bibr" rid="c18">Federmeier, 2022</xref>; <xref ref-type="bibr" rid="c19">Federmeier et al., 2007</xref>; <xref ref-type="bibr" rid="c37">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="c40">Lau et al., 2008</xref>). Basic effects of contextual congruency on the N400 – smaller responses to words that do versus do not fit a sentence context (e.g., to “butter” compared to “socks” after “He spread the warm bread with …”) – are also observed for parafoveally-presented words (<xref ref-type="bibr" rid="c1">Antúnez et al., 2022</xref>; <xref ref-type="bibr" rid="c5">Barber et al., 2013</xref>, <xref ref-type="bibr" rid="c4">2010</xref>; <xref ref-type="bibr" rid="c43">López-Peréz et al., 2016</xref>; <xref ref-type="bibr" rid="c46">Meade et al., 2021</xref>), and, even when all words are congruent, N400 responses to words in parafoveal preview, like those to foveated words, are graded by increasing context-based predictability (<xref ref-type="bibr" rid="c60">Payne et al., 2019</xref>; <xref ref-type="bibr" rid="c59">Payne and Federmeier, 2017</xref>; <xref ref-type="bibr" rid="c83">Stites et al., 2017</xref>). These studies suggest that semantic information is available from words before they are fixated, even if that information does not always have an impact on eye fixation patterns.</p>
<p>Thus, both eye tracking and electrophysiological studies have provided evidence suggesting that semantic information is extracted from words in parafoveal preview. However, most of these studies have been limited to measuring parafoveal preview from fixations to an immediately adjacent word, raising questions about exactly how far in advance semantic information might become available from parafoveal preview. Moreover, important questions remain about the extent to which parafoveally extracted semantic information can be functionally integrated into the building sentence-level representation. Although some ERP studies have found that the semantic information extracted from parafoveal preview is carried forward, affecting semantic processing when that same word is later fixated (<xref ref-type="bibr" rid="c4">Barber et al., 2010</xref>; <xref ref-type="bibr" rid="c60">Payne et al., 2019</xref>; <xref ref-type="bibr" rid="c83">Stites et al., 2017</xref>), other studies have not observed any downstream impact (<xref ref-type="bibr" rid="c5">Barber et al., 2013</xref>; <xref ref-type="bibr" rid="c42">Li et al., 2015</xref>). Furthermore, post-N400 ERP components, linked to more attentionally-demanding processes associated with message-building and revision, do not seem to be elicited during parafoveal preview (<xref ref-type="bibr" rid="c41">Li et al., 2023</xref>; <xref ref-type="bibr" rid="c49">Milligan et al., 2023</xref>; <xref ref-type="bibr" rid="c60">Payne et al., 2019</xref>; <xref ref-type="bibr" rid="c79">Schotter et al., 2023</xref>). Therefore, critical questions remain about the timecourse and mechanisms by which semantic information is extracted and used during reading. Answering those questions requires an approach that allows a more continuous assessment of sensitivity to target word semantics during parafoveal processing across multiple fixations, and, in particular, that can speak to how attention is allocated across words during natural reading. We tackle these core issues using a new technique that combines the use of frequency tagging and the measurement of magnetoencephalography (MEG)-based signals.</p>
<p>Frequency tagging, also known as steady-state visually evoked potentials, involves flickering a visual stimulus at a specific frequency and then measuring the neuronal response associated with processing the stimulus (<xref ref-type="bibr" rid="c55">Norcia et al., 2015</xref>; <xref ref-type="bibr" rid="c87">Vialatte et al., 2010</xref>). It has been widely used to investigate visuospatial attention (<xref ref-type="bibr" rid="c23">Gulbinaite et al., 2019</xref>; <xref ref-type="bibr" rid="c36">Kritzman et al., 2022</xref>; <xref ref-type="bibr" rid="c51">Müller et al., 2003</xref>, <xref ref-type="bibr" rid="c52">1998</xref>; <xref ref-type="bibr" rid="c55">Norcia et al., 2015</xref>; <xref ref-type="bibr" rid="c87">Vialatte et al., 2010</xref>) and has recently been applied to language processing (<xref ref-type="bibr" rid="c6">Beyersmann et al., 2021</xref>; <xref ref-type="bibr" rid="c50">Montani et al., 2019</xref>; <xref ref-type="bibr" rid="c91">Wu et al., 2023</xref>). However, the traditional frequency tagging technique flickers visual stimuli at a low frequency band, usually below 30 Hz, such that the flickering can be visible and may interfere with the ongoing task. To address this limitation, we developed the rapid invisible frequency tagging (RIFT) technique, which involves flickering visual stimuli at a frequency above 60 Hz, making it invisible and non-disruptive to the ongoing task. Responses to RIFT have been shown to increase with the allocation of attention to the stimulus bearing the visual flicker (<xref ref-type="bibr" rid="c8">Brickwedde et al., 2022</xref>; <xref ref-type="bibr" rid="c14">Drijvers et al., 2021</xref>; <xref ref-type="bibr" rid="c15">Duecker et al., 2021</xref>; <xref ref-type="bibr" rid="c21">Ferrante et al., 2023</xref>; <xref ref-type="bibr" rid="c24">Gutteling et al., 2022</xref>; <xref ref-type="bibr" rid="c94">Zhigalov et al., 2021</xref>, <xref ref-type="bibr" rid="c95">2019</xref>; <xref ref-type="bibr" rid="c96">Zhigalov and Jensen, 2022</xref>, <xref ref-type="bibr" rid="c97">2020</xref>). In our previous study, we adapted RIFT to a natural reading task and found temporally-precise evidence for parafoveal processing at the lexical level (<xref ref-type="bibr" rid="c57">Pan et al., 2021</xref>). Thus, RIFT allows us to better understand the temporal dynamics of parafoveal processing during natural reading.</p>
<p>In the current study, RIFT was utilized in a natural reading task to investigate parafoveal semantic integration. We recruited participants (n = 34) to silently read one-line sentences while their eye movements and brain activity were recorded simultaneously by an eye-tracker and MEG. The target word in each sentence was always unpredictable (see Behavioural pre-tests in Methods) but was semantically congruent or incongruent with the preceding sentence context (for the characteristics of words, see <xref rid="tbl1" ref-type="table">Table 1</xref>). The target words were tagged by flickering an underlying patch, whose luminance kept changing in a 60 Hz sinusoid throughout the sentence presentation. The patch was perceived as grey, the same colour as the background, making it invisible. To ensure that the flicker remained invisible across saccades, we applied a Gaussian transparent mask to smooth out sharp luminance changes around the edges (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Parafoveal processing of the target word was indexed by the RIFT responses recorded using MEG during fixations of pre-target words.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Characteristics of pre-target, target, and post-target words</title></caption>
<graphic xlink:href="509511v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>The paradigm and the eye movement metrics.</title>
<p>(<bold>A</bold>) After the presentation of a cross-fixation at the screen centre for 1.2 −1.6 s, a gaze-contingent box appeared near the left edge of the screen. Fixating the box for 0.2 s triggered the full sentence presentation. Participants (n = 34) read 160 one-line sentences silently while brain activity and eye movements were recorded. Each sentence was embedded with one congruent or incongruent target word (see the dashed rectangle; not shown in the actual experiment). The target words could not be predicted based on the sentence context and word level properties of congruent and incongruent targets were balanced by swapping them between two sentence frames. The target words were tagged by changing the luminance of the underlying patch (with a Gaussian mask) in a 60 Hz sinusoid throughout the sentence presentation (depicted as a bright blob, not shown in the actual experiment). Additionally, we included a small disk at the bottom right of the screen that displayed the tagging signal and was recorded by a photodiode throughout each trial. After reading, gazing at the bottom box for 0.2 s triggered the sentence offset. Twelve percent of the sentences were followed by a simple yes-or-no comprehension question. (<bold>B</bold>) The first fixation durations on the pre-target and target words when the target words were incongruent (in blue) or congruent (in orange) with the sentence context. Each dot indicates one participant. ***<italic>p</italic> &lt; .001; n.s., not statistically significant; ITI, inter-trial interval.</p></caption>
<graphic xlink:href="509511v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This paradigm allows us to address three questions. First, we aimed to clarify if and when semantic information is extracted from parafoveal words. Second, we sought to ascertain whether the previewed semantic information is integrated into the sentence context during parafoveal processing. Modulations of pre-target RIFT responses by the contextual congruity of target words would serve as evidence for the extraction and integration of parafoveal semantic information into the sentence context. Third, we explored whether this parafoveal semantic integration has any relationship to reading speed.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>No evidence for semantic parafoveal processing in the eye movement data</title>
<p>Like prior work measuring eye fixations during English reading (<xref ref-type="bibr" rid="c29">Inhoff, 1982</xref>; <xref ref-type="bibr" rid="c30">Inhoff and Rayner, 1980</xref>; <xref ref-type="bibr" rid="c66">Rayner et al., 2014</xref>, <xref ref-type="bibr" rid="c64">1986</xref>), we found no evidence for parafoveal semantic processing in the eye movement data (<xref rid="fig1" ref-type="fig">Figure 1B</xref>, left). A paired t-test comparing first fixation durations on the pre-target word showed no effect of contextual (in)congruity (<italic>t</italic>(33) = .84, <italic>p</italic> = .407, <italic>d</italic> = .14, two-sided). However, first fixation durations on the target word were significantly longer when they were incongruent (versus congruent) with the context (<italic>t</italic>(33) = 5.99, <italic>p</italic> = 9.83×10<sup>-7</sup>, <italic>d</italic> = 1.03, two-sided pairwise <italic>t</italic>-test; <xref rid="fig1" ref-type="fig">Figure 1B</xref>, right). In addition, we found that the contextual congruity of target words affected later eye movement measures (i.e., total gaze duration and the likelihood of refixation after the first pass reading), with additional processing evident when the target words were incongruent with the context compared with when they were congruent (<xref rid="figs1" ref-type="fig">Supplementary Figure 1</xref>).</p>
</sec>
<sec id="s2b">
<title>Parafoveal processing measured by Rapid Invisible Frequency Tagging (RIFT)</title>
<p>First, we performed a selection procedure to identify MEG sensors that responded to RIFT. We measured neural responses to the flickering target words by calculating the coherence between the MEG sensors and the tagging signal measured by a photodiode. A MEG sensor was considered a good tagging response sensor if it showed significantly stronger 60 Hz coherence during the pre-target intervals (with flicker) compared to the baseline intervals before the sentence presentation (without flicker). Both pre-target and baseline intervals were 1-second epochs. We then applied a cluster-based permutation test and identified sensor clusters that showed a robust tagging response (<italic>p</italic>cluster &lt; .01; <xref rid="fig2" ref-type="fig">Figure 2A</xref>). Tagging response sensors were found in 29 out of 34 participants, and all subsequent analyses were based on these tagging response sensors (7.9 ± 4.5 sensors per participant, M ± SD). The sources of these responses were localized to the left visual association cortex (Brodmann area 18; <xref rid="fig2" ref-type="fig">Figure 2B</xref>) using Dynamic Imaging Coherent Sources (DICS) (<xref ref-type="bibr" rid="c22">Gross et al., 2001</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Neural responses to the rapid invisible frequency tagging (RIFT).</title>
<p>(<bold>A</bold>) Topography of the RIFT response sensors over all participants (7.9 ± 4.5 sensors per participant, M ± SD). These sensors showed significantly stronger coherence to the tagging signal during the pre-target interval (with target words flickering in the parafovea) compared with the baseline interval (no flicker). Further analyses only included participants who had a RIFT response (n = 29). (<bold>B</bold>) The source of the RIFT response sensors was localized to the left visual association cortex (MNI coordinates [-9 -97 3] mm, Brodmann area 18). (<bold>C</bold>) The averaged 60 Hz coherence over the RIFT response sensors when participants fixated on words at different positions, where N indicates the target word and N-1 indicates the pre-target word. Error bars indicate the SE over participants (n = 29). The shaded area indicates the RIFT responses when previewing the flickering target words. We compared the RIFT response at each word position with the baseline (the dashed line). ***<italic>p</italic> &lt; .001, **<italic>p</italic> &lt; .01, *<italic>p</italic> &lt; .05, † <italic>p</italic> = .051; n.s., not statistically significant.</p></caption>
<graphic xlink:href="509511v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we characterized the temporal dynamics of attentional allocation to the flickering target word by calculating the 60 Hz coherence during fixations on several words surrounding the target word (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). The resulting RIFT response curve revealed that significant attention was allocated to the target word as far as three words prior, spanning 15.3 ± 2.7 letters (M ± SD), including the spaces between words. This range is consistent with previous estimations of the perceptual span of 12−15 letters during English reading (<xref ref-type="bibr" rid="c45">McConkie and Rayner, 1975</xref>; <xref ref-type="bibr" rid="c62">Rayner, 2009</xref>, <xref ref-type="bibr" rid="c63">1975</xref>; <xref ref-type="bibr" rid="c86">Underwood and McConkie, 1985</xref>), as reported in the eye movement literature. Moreover, the RIFT response curve was left skewed, indicating a higher allocation of attentional resources to the flickering target words before fixating on them. The normal size and left skewness of the perceptual span in our study suggests that RIFT did not influence attention distribution during natural reading. Notably, the strongest RIFT responses were observed during fixations on the pre-target word (i.e., word position N-1, <xref rid="fig2" ref-type="fig">Figure 2C</xref>), highlighting the suitability of RIFT for measuring neuronal activity associated with parafoveal processing during natural reading.</p>
</sec>
<sec id="s2c">
<title>Neural evidence for semantic parafoveal integration</title>
<p>Importantly, evidence for parafoveal semantic integration was found using RIFT (<xref rid="fig3" ref-type="fig">Figure 3</xref>). The pre-target coherence was weaker when the sentence contained a contextually incongruent word, compared to when it was congruent (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). We conducted a pairwise t-test and found a significant effect on the averaged pre-target coherence at 60 Hz (<italic>t</italic>(28) = -2.561, <italic>p</italic> = .016, <italic>d</italic> = .476, two-sided pairwise; <xref rid="fig3" ref-type="fig">Figure 3B</xref>). To avoid any contamination of the parafoveal measure with activity from target fixation, pre-target coherence was averaged over the minimum pre-target fixation duration across both conditions for each participant (97.4 ± 14.1 ms, M ± SD, denoted as a dashed rectangle). Next, we conducted a jackknife-based latency estimation and found that the congruency effect on the 60 Hz pre-target coherence had a significantly later onset when previewing an incongruent (116.0 ± 1.9 ms, M ± SD) compared to a congruent target word (91.4 ± 2.1 ms, M ± SD, denoted as a dashed rectangle; <italic>t</italic>(28) = - 2.172, <italic>p</italic> = .039, two-sided; <xref rid="fig3" ref-type="fig">Figure 3C</xref>). Therefore, both the magnitude and onset latency of the pre-target coherence were modulated by the contextual congruency of the target word, providing neural evidence that semantic information is integrated into the context during parafoveal processing, detectable within 100 ms after readers fixate the pre-target word.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Neural evidence for parafoveal semantic integration.</title>
<p>(<bold>A</bold>) The pre-target coherence spectrum averaged over the RIFT response sensors at the group level (n = 29) when the subsequent target words were incongruent with the sentence context (top panel), congruent with the sentence context (middle panel), and the difference between the two conditions (bottom panel). The horizontal line indicates the tagging frequency at 60 Hz. The two vertical lines indicate the first fixation onset of the pre-target words and the average fixation offset. (<bold>B</bold>) The averaged 60 Hz coherence during the minimum pre-target intervals for each participant (97.4 ± 14.1 ms, M ± SD; denoted as a dashed rectangle) with respect to the incongruent and congruent target words. Each dot indicates one participant, the horizontal lines inside of the violins indicate the mean values. The upright inserted figure shows the pre-target coherence difference over participants with the error bar as SE. (<bold>C</bold>) The onset latency of the pre-target coherence at the group level (n = 29). The onset latency refers to the time when the coherence curve reached its half maximum, denoted by the dotted lines. Zero time-point indicates the first fixation onset of the pre-target words. The shaded area shows SE around the mean value. *<italic>p</italic> &lt; .05.</p></caption>
<graphic xlink:href="509511v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We conducted a similar analysis of the coherence measured when participants fixated the target word and found no significant modulations related to the contextual congruity of that target word, in either the magnitude (<italic>t</italic>(28) = .499, <italic>p</italic> = .622, <italic>d</italic> = .093, two-sided pairwise) or onset latency (<italic>t</italic>(28) = -.280, <italic>p</italic> = .782); <xref rid="fig4" ref-type="fig">Figure 4</xref>) of the RIFT response. Therefore, the parafoveal semantic integration effect observed during the pre-target intervals cannot be explained as a spill-over from the target fixations.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Neural responses to the rapid invisible frequency tagging (RIFT) during the target interval.</title>
<p>(<bold>A</bold>) The target coherence spectrum averaged over the RIFT response sensors at the group level (n = 29) when the target words were incongruent with the sentence context (top panel), or congruent with the sentence context (middle panel); the bottom panel shows the difference between the two conditions. The horizontal line indicates the tagging frequency at 60 Hz. The two vertical lines indicate the first fixation onset of the target words and the averaged fixation offset. (<bold>B</bold>) We averaged the 60 Hz coherence within the minimum target fixation duration over participants (97.6 ± 15.7 ms, M ± SD; denoted as a dashed rectangle). Each dot indicates one participant, the horizontal lines inside of the violins indicate the mean values. The upright inserted figure shows the target coherence difference over participants with the error bar as SE. (<bold>C</bold>) A jackknife-based method was used to calculate the onset latency of the average coherence at the group level. The onset latency refers to the time when the coherence curve reached its half maximum, denoted by the dotted lines. n.s., not statistically significant.</p></caption>
<graphic xlink:href="509511v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Parafoveal semantic integration is related to individual reading speed</title>
<p>The RIFT effects of congruency observed during parafoveal preview of the targets showed that readers tend to allocate less attention to upcoming text when an upcoming word is semantically incongruent compared to when all words are congruent. If readers differ in the extent to which their attention is “repelled” by incongruent words, then we might expect that the magnitude of the RIFT effect would be related to reading speed. Therefore, we conducted a correlation analysis to investigate this relationship (<xref rid="fig5" ref-type="fig">Figure 5</xref>). Individual reading speed was quantified as the number of words read per second from the congruent sentences in the study. We found a positive correlation between the pre-target coherence difference (incongruent - congruent) and individual reading speed (<italic>r</italic>(27) = .503, <italic>p</italic> = .006; Spearman’s correlation). This suggests that readers who show greater shifts in attentional allocation in response to semantic incongruity read more slowly on average.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Individual reading speed is correlated with the magnitude of the RIFT congruency effect.</title>
<p>Reading speed was measured as the number of words read per second in the congruent sentences. The RIFT effect was measured as the coherence difference during the pre-target fixations for sentences containing incongruent and congruent target words. Each dot indicates one participant (n = 29, Spearman correlation). The shaded area represents the 95% CI.</p></caption>
<graphic xlink:href="509511v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In the current natural reading study, we utilized RIFT to probe for evidence that readers are sensitive to the effect of contextual congruity of an upcoming target word during parafoveal processing. We found no significant modulation of fixation durations of pre-target words based on the contextual congruity of the target word (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). However, we observed a significant difference in the amount of covert attention allocated to the target when previewing congruent and incongruent target words (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Specifically, we found lower RIFT responses for parafoveal words that were incongruent compared to congruent with the sentence context. Because the target words were always of low predictability, their semantic congruence could only be appreciated if they had been integrated (to some extent) with the unfolding context. Thus, the RIFT patterns provide compelling neural evidence that semantic information can not only be extracted but also integrated during parafoveal processing.</p>
<p>More specifically, we observed that pre-target coherence was weaker in magnitude (<xref rid="fig3" ref-type="fig">Figure 3B</xref>) and had a later onset latency (<xref rid="fig3" ref-type="fig">Figure 3C</xref>) in response to a contextually incongruent target word compared to a congruent one. Two possible explanations for these findings can be considered. First, the decreased RIFT responses may be due to changes in the pattern of allocation of attention across the text during reading. When reading in English, attention continuously shifts from left to right. If the semantic information previewed in the parafovea cannot be easily integrated into the context, this pattern may be interrupted, leading to delayed and/or reduced allocation of attention to the parafoveal word, possibly because readers shift more attention to the currently fixated word or to previous words to ensure that they have decoded and understood what they have read thus far. On this view, the RIFT finding may reflect a covert “regression” of attention, similar to overt eye-movement regressions that sometimes occur when readers encounter semantically incongruous words (<xref ref-type="bibr" rid="c1">Antúnez et al., 2022</xref>; <xref ref-type="bibr" rid="c7">Braze et al., 2002</xref>; <xref ref-type="bibr" rid="c53">Ni et al., 1998</xref>; <xref ref-type="bibr" rid="c68">Rayner et al., 2004</xref>) (also see <xref rid="figs1" ref-type="fig">Supplementary Figure 1A</xref>). Alternatively, the reduction in RIFT responses could arise if readers shift attentional resources away from the text altogether. Prior work has shown that responses to a visual flicker decrease with more attention being directed to an internal cognitive process (<xref ref-type="bibr" rid="c36">Kritzman et al., 2022</xref>). In reading, if the parafoveal target word is incongruent with the sentence context, more effort may be required to try to integrate the previewed semantic information into the prior context. Consequently, attentional resources may be allocated more to this internal integration process rather than to the external visual processing of the text, including the flickering word as measured by RIFT. On either account, the reduced forward allocation of attention diminishes parafoveal processing, and, in turn, may tend to slow reading speed, as supported by our correlation results (<xref rid="fig5" ref-type="fig">Figure 5</xref>).</p>
<p>Our results also provide information about the timecourse of semantic integration, as we found evidence that readers appreciated the incongruity – and thus must have begun to integrate the semantics of the parafoveal words with their ongoing message-level representation – by as early as within 100 ms after fixating on the pre-target word. The RIFT results further suggest that this early appreciation of the incongruity arises because reading is characterized by parallel processing spanning multiple words. RIFT responses index the amount of (covert) attention allocated to the target word, which likely spans different aspects of word processing over time and word position. We detected a significant amount of attention allocated to the target word even three words in advance (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Given that this initial increase in RIFT occurred when the target just entered the perceptual span (<xref ref-type="bibr" rid="c45">McConkie and Rayner, 1975</xref>; <xref ref-type="bibr" rid="c63">Rayner, 1975</xref>; <xref ref-type="bibr" rid="c86">Underwood and McConkie, 1985</xref>), it is likely that it coincides with the initial extraction of lower-level perceptual information about the target. The emerging sensitivity of the RIFT signal to target plausibility, which was detected by about 100 ms into the fixation on the pre-target word, suggests that it is by this point in processing that readers had accumulated enough semantic information about the target words and done enough integration of that information with the unfolding sentence context to detect the contextual incongruity. Thus, it seems likely that initial processing of word semantics began even before the pre-target fixation and was spread out across fixations to multiple words.</p>
<p>Our findings have significant implications for theories of reading. The occurrence and early onset of semantic integration in parafoveal vision suggests that words are processed in an exceptionally parallel manner, posing a challenge for existing serial processing models (<xref ref-type="bibr" rid="c73">Reichle et al., 2009</xref>, <xref ref-type="bibr" rid="c70">2006</xref>, <xref ref-type="bibr" rid="c71">2003</xref>, <xref ref-type="bibr" rid="c69">1998</xref>). At the same time, it is important to note that the fact that semantic integration begins in parafoveal vision does not mean that it is necessarily completed before a word is fixated. The fact that we observed semantic congruency effects on the fixation durations of the target words (<xref rid="fig1" ref-type="fig">Figure 1B</xref>) suggests that additional processing is required to fully integrate the semantics with overt attention in foveal vision. This also aligns with previous studies that found some ERP responses to semantic violations, including the LPC (Late Positive Component), are elicited only during foveal processing, but not during parafoveal processing (<xref ref-type="bibr" rid="c41">Li et al., 2023</xref>; <xref ref-type="bibr" rid="c49">Milligan et al., 2023</xref>; <xref ref-type="bibr" rid="c60">Payne et al., 2019</xref>; <xref ref-type="bibr" rid="c79">Schotter et al., 2023</xref>).</p>
<p>Thus, RIFT measures complement eye tracking (and other) measures, providing unique information revealing multiple mechanisms at work during natural reading. Our RIFT approach detects that covert attention is allocated to process the target word starting as early as three words before (<xref rid="fig2" ref-type="fig">Figure 2C</xref>) and that these covert processes afford the detection of semantic incongruity (<xref rid="fig3" ref-type="fig">Figure 3B</xref> and <xref rid="fig3" ref-type="fig">Figure 3C</xref>). This covert attention, however, does not manifest in fixation measures (<xref rid="fig1" ref-type="fig">Figure 1B</xref>), suggesting that it does not have a notable impact on saccade planning. Additional processes, which do impact overt eye movement patterns, are then brought to bear when the target words are fixated, resulting in increased fixation durations for incongruous words. At that same point (i.e., the target word), however, the RIFT responses showed a null effect of congruency (<xref rid="fig4" ref-type="fig">Figure 4</xref>); it may be that the RIFT technique is better suited to capturing parafoveal compared to foveal attentional processes, in part because there are more motion-sensitive rod cells in the parafoveal than foveal area. Finally, even after readers move away from fixating the word, attention to the target can persist or be reinstated, as evidenced by patterns of regressions (<xref rid="figs1" ref-type="fig">Supplementary Figure 1A</xref>). Therefore, during natural reading, attention is distributed across multiple words. The highly flexible and distributed allocation of attention allows readers to be parallel processors and thereby read fluently and effectively (<xref ref-type="bibr" rid="c17">Engbert et al., 2005</xref>, <xref ref-type="bibr" rid="c16">2002</xref>; <xref ref-type="bibr" rid="c82">Snell et al., 2018</xref>; <xref ref-type="bibr" rid="c80">Snell and Grainger, 2019</xref>). Our natural reading paradigm, where all words are available on the screen and saccadic eye movements are allowed, makes it possible to capture the extensive parallel processing. Moreover, saccades have been found to coordinate our visual and oculomotor systems, further supporting the parallel processing of multiple words during natural reading (<xref ref-type="bibr" rid="c58">Pan et al., 2023</xref>).</p>
<p>In summary, our findings show that parafoveal processing is not limited to simply extracting word information, such as lexical features, as demonstrated in our previous study (<xref ref-type="bibr" rid="c57">Pan et al., 2021</xref>). Instead, the previewed parafoveal information from a given word can begin to be integrated into the unfolding sentence representation well before that word is fixated. Moreover, the impact of that parafoveal integration further interacts with reading comprehension by shaping the timecourse and distribution of attentional allocation – i.e., by causing readers to move attention away from upcoming words that are semantically incongruous. These results support the idea that words are processed in parallel and that early and extensive parafoveal processing is critical for fluent reading.</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>We recruited 36 native English speakers (24 females, 22.5 ± 2.8 years old, mean ± SD) with normal or corrected-to-normal vision. All participants are right-handed and without any history of neurological problems or a language disorder diagnosis. Two participants were excluded from analysis due to poor eye tracking or falling asleep during the recordings, which left 34 participants (23 females). The study was approved by the University of Birmingham Ethics Committee. The informed consent form was signed by all participants after the nature and possible consequences of the studies were explained. Participants received £15 per hour or course credits as compensation for their participation.</p>
</sec>
<sec id="s4b">
<title>Stimuli</title>
<p>In total participants read 277 sentences, of which 117 sentences were fillers from a published paper (<xref ref-type="bibr" rid="c89">White, 2008</xref>). The filler sentences were all plausible and were included to make sure the incongruent sentences were less than one third of the sentence set. We constructed the remaining 160 sentences with 80 pairs of target words. In all sentences the context was low constraint; i.e., none of the target words could be predicted by the prior context (see Behavioural pre-tests below for details). The target word in each sentence was either incongruent or congruent with the sentence. To focus on semantic integration and avoid any confounds of word-level properties, we embedded each pair of target words in two different sentence frames. By swapping the target words within a pair of sentences, we created four sentences: two congruent ones and two incongruent ones. These were then counterbalanced over participants. In this way, we counterbalanced across lexical characteristics of the target words and characteristics of the sentence frames within each pair. Each participants read one version of the sentence set (A or B). For example, for the target pair <italic>brother/jacket</italic>, one participant read them in the congruent condition in the sentence set version A; while another participant read them in the incongruent condition in version B (see below, targets are in italic type for illustration, but in normal type in the real experiment).
<list list-type="alpha-upper">
<list-item><p>Last night, my lazy <italic>brother</italic> came to the party one minute before it was over.</p>
<p>Lily says this blue <italic>jacket</italic> will be a big fashion trend this fall.</p>
</list-item>
<list-item><p>Last night, my lazy <italic>jacket</italic> came to the party one minute before it was over.</p></list-item>
</list>
</p>
<p>Lily says this blue <italic>brother</italic> will be a big fashion trend this fall</p>
<p>For all sentences, the pre-target words were adjectives, and the target words were nouns (for detailed characteristics of the words please see <xref rid="tbl1" ref-type="table">Table 1</xref>). The word length of pre-target words was from 4 to 8 letters, and for target words was from 4 to 7 letters. The sentences were no longer than 15 words or 85 letters. The target words were embedded somewhere in the middle of each sentence and were never the first three or the last three words in a sentence. Please see Appendix in the Supplementary material for the full list of the sentence sets that were used in the current study.</p>
</sec>
<sec id="s4c">
<title>Behavioural pre-tests</title>
<p>We recruited native English speakers for two behavioural pre-tests of the sentence sets. These participants did not participate in the MEG session.</p>
<sec id="s4c1">
<title>Predictability of target words</title>
<p>We carried out a cloze test to estimate the predictability of the target words and the contextual constraint of the sentences. Participants read sentence fragments consisting of the experimental materials up to but not including the target words. Then participants were asked to write down the first word that came to mind that could continue the sentence (no need to complete the whole sentence). Example:
<list list-type="order">
<list-item><p>Last night, my lazy ________________</p></list-item>
<list-item><p>Lily says this blue ________________</p></list-item>
</list>
The predictability of a word was estimated as the percentage of participants who wrote down exactly this word in the cloze test. A target word with less than 10% predictability was deemed to be not predicted by the sentence context. In addition, sentences for which no word was predicted with 50% or greater probability were low constraint. Twenty participants (6 males, 24.2 ± 2.0 years old, mean ± SD) took part in the first round of pre-test. Eight sentences were replaced with new sentences because the target words were too predictable and/or the sentence was too constraining. We then conducted a second round of the predictability test with 21 new participants (7 males, 25.0 ± 6.0 years old). None of the target words in this final set were predictable (2.3% ± 4.8%, mean ± SD), and all the sentence contexts were low constraint (25.2% ± 11.8%).</p>
</sec>
<sec id="s4c2">
<title>Plausibility of sentences</title>
<p>Two groups of participants were instructed to rate how plausible (or acceptable) each sentence was in the sentence set version A or B separately. Plausibility was rated on a 7-point scale with plausibility increasing from point 1 to 7. Sentences in the experiment were designed to be either highly implausible (the incongruent condition) or highly plausible (the congruent condition). To occupy the full range of the scale, we constructed 70 filler sentences with middle plausibility (e.g., sentence 1 below). In this example, sentences 2 and 3 were the incongruent and congruent sentences from the experiment.</p>
<table-wrap orientation="portrait" position="float">
<graphic xlink:href="509511v2_tbl1a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>For version A we recruited 27 participants (4 males, 22.8 ± 6.1 years old, mean ± SD): The plausibility rating for the incongruent sentences was 2.08 ± 0.79 (mean ± SD); while for the congruent sentences was 6.18 ± 0.56. For sentence set version B we recruited 22 participants (4 males, 21.1 ± 2.3 years old, one invalid dataset due to incomplete responses): The plausibility rating was 1.81 ± 0.41 (mean ± SD) for the sentences in the incongruent condition and 6.15 ± 0.47 for the sentences in the congruent condition. These results showed that in both versions of the sentences set, incongruent sentences were viewed as highly implausible and congruent sentences as highly plausible.</p>
</sec>
</sec>
<sec id="s4d">
<title>Experimental procedure</title>
<p>Participants were seated 145 cm away from the projection screen in a dimly lit magnetically shielded room. The MEG gantry was set at 60 degrees upright and covered the participant’s whole head. We programmed in Psychophysics Toolbox -3(<xref ref-type="bibr" rid="c34">Kleiner et al., 2007</xref>) to present the one-line sentences on a middle-grey screen (RGB [128 128 128]). All words were displayed in black (RGB [0 0 0]) with an equal-spaced Courier New font. The font size was 20 and the font type was bold so that each letter and space occupied 0.316 visual degrees. The visual angle of the whole sentence was no longer than 27 visual degrees in the horizontal direction. The sentence set was divided into 5 blocks, each of which took about 7 minutes. There was a break of at least 1 minute between blocks and participants pressed a button to continue the experiment at any time afterwards. Participants were instructed to read each sentence silently at their own pace and to keep their heads and body as stable as possible during the MEG session. Eye movements were acquired during the whole session. In total, the experiment took no longer than 55 minutes.</p>
<p>Within a trial, there was first a fixation cross presented at the centre of a middle-grey screen for 1.2 – 1.6 s. This was followed by a black square with a radius of 1 degree of visual angle. This square was placed at the vertical centre, 2 degrees of visual angle away from the left edge of the screen. Participants had to gaze at this black ‘starting square’ for at least 0.2 s to trigger the onset of the sentence presentation. Afterwards, the sentence would start from the location of the square (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). The sentence was presented with an ‘ending square’ 5 degrees of visual angle below the screen centre. The ‘ending square’ was the same size as the ‘starting square’ but in grey colour (RGB [64 64 64]). A gaze at this ‘ending square’ for at least 0.1s would end the presentation of the sentence. Then the trial ended with a blank middle-grey screen that lasted for 0.5s. Randomly, 12% of the trials were followed by a statement about the content of the sentence that was just presented, and participants needed to answer “True or “False” by pressing a button. For example, the statement for sentence 2 was “Lily has a prediction about the fashion trend in this fall”, and the correct answer was “True”. The statement for sentence 3 was “Little Jimmy didn’t have a box”, and the correct answer was “False”. All participants read the sentences carefully as shown by the high accuracy of answering (96.3% ± 4.7%, mean ± SD).</p>
</sec>
<sec id="s4e">
<title>Rapid invisible frequency tagging (RIFT)</title>
<sec id="s4e1">
<title>Projection of the sentence stimuli</title>
<p>We projected the sentences from the stimulus computer screen in the experimenter room to the projection screen inside of the MEG room using a PROPixx DLP LED projector (VPixx Technologies Inc., Canada). The refresh rate of the PROPixx projector was up to 1440 Hz, while the refresh rate of the stimulus screen was only 120 Hz (1920 × 1200 pixels resolution). We displayed the sentence repeatedly in four quadrants of the stimulus computer screen. In each quadrant, the words were coded in three colour channels as RGB. The projector then interpreted these 12 colour channels (3 channels × 4 quadrants) as 12 individual grayscale frames, which were projected onto the projection screen in rapid succession. Therefore, the projection screen refreshed at 12 times the rate of the stimulus computer screen.</p>
</sec>
<sec id="s4e2">
<title>Flickering of the target word</title>
<p>We added a square patch underneath the target word to frequency tag the target word. The side length of the square patch was the width of the target word plus the spaces on both sides (2 to 3° visual angle). We flickered the patch by changing its luminance from black to white at a 60 Hz sinusoid (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). To reduce the visibility of the patch edges across saccades, we applied a Gaussian smoothed transparent mask on top of the square patch. The mask was created by a two-dimensional Gaussian function (<xref rid="eqn1" ref-type="disp-formula">Equation 1</xref>):
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="509511v2_eqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula></p>
<p>where, <italic>x</italic> and <italic>y</italic> are the mesh grid coordinates for the flickering patch, and <italic>σ</italic> is the <italic>x</italic> and <italic>y</italic> spread of the mask with <italic>σ</italic> = 0.02 degrees.</p>
<p>On average, the patch was perceived as middle-grey, the same colour as the background screen, which made it invisible to participants. The target word was still black, the same colour as the other words on the screen. To record the tagging signal, we attached a custom-made photodiode (Aalto NeuroImaging Centre, Finland) to the disk at the bottom right corner of the screen. The luminance of the disk varied the same as that of the flickering patch underneath the target word. The photodiode was plugged into the MEG system as an external channel.</p>
</sec>
</sec>
<sec id="s4f">
<title>Data acquisition</title>
<sec id="s4f1">
<title>MEG</title>
<p>Brain data were acquired with a 306-sensor TRIUX Elekta Neuromag system, which consisted of 204 orthogonal planar gradiometers and 102 magnetometers (Elekta, Finland). After participants signed the consent form, we attached four head-position indicator coils (HPI coils) to their heads: two on the left and right mastoid bone, and two on the forehead with at least 3 cm distance in between. Afterwards, we used a Polhemus Fastrack electromagnetic digitizer system (Polhemus Inc, USA) to digitize the locations for three bony fiducial points: the nasion, left and right preauricular points. Then we digitized the four HPI coils. Furthermore, at least 200 extra points were acquired, which were distributed evenly and covered the whole scalp. These points were used later in the source analysis when spatially co-register the MEG head model with individual structural MRI images. The sampling rate of the MEG system was 1,000 Hz. Data were band-pass filtered prior to sampling from 0.1 to 330 Hz to reduce aliasing effects.</p>
</sec>
<sec id="s4f2">
<title>Eye movements</title>
<p>We used an EyeLink 1000 Plus eye-tracker (long-range mount, SR Research Ltd, Canada) to track eye movements throughout the whole MEG session. The eye tracker was placed on a wooden table in front of the projection screen. The centre of the eye tracker was at the middle line of the projection screen, and the top of the eye tracker reached the bottom edge of the screen. The distance between the eye-tracker camera and the centre of the participant’s eyes was 90 cm. We recorded the horizontal and vertical positions as well as the pupil size from the left eye, at a sampling rate of 1,000 Hz. Each session began with a nine-point calibration and validation test. The test was accepted if the eye-tracking error was below 1 visual degree both horizontally and vertically. During the session, we performed a one-point drift checking test every three trials and after the break between blocks. If the drift checking failed or the sentence presentation was unable to be triggered through gazing, a nine-point calibration and validation test was conducted again.</p>
</sec>
<sec id="s4f3">
<title>MRI</title>
<p>After MEG data acquisition, participants were asked to come to the laboratory another day to have an MRI image acquired. We acquired the T1-weighted structural MRI image using a 3-Tesla Siemens PRISMA scanner (TR = 2000 ms, TE = 2.01 ms, TI = 880 ms, flip angle = 8 degrees, FOV = 256×256×208 mm, 1 mm isotropic voxel). For 11 participants who dropped out of the MRI acquisition, the MNI template brain (Montreal, Quebec, Canada) was used instead in the source analysis later.</p>
</sec>
</sec>
<sec id="s4g">
<title>Eye movement data analysis</title>
<p>We extracted the fixation onset events from the EyeLink output file. The EyeLink parsed fixation events based on the online detection of saccade onset using the following parameters: the motion threshold as 0.1 degrees, the velocity threshold as 30 degrees/sec, and the acceleration threshold as 8000 degrees/sec2. These conservative settings were suggested by the EyeLink user manual for reading studies, as they can prevent false saccade reports and reduce the number of micro-saccades and lengthen fixation durations.</p>
<p>Only the fixation that first landed on a given word was selected. The first fixation durations were averaged within the incongruent and congruent conditions for pre-target and target words. Pairwise, two-sided <italic>t</italic>-test were conducted on the first fixation durations of pre-target and target words separately (conducted in R (Team, 2013)). In addition to the early eye movement measure of the first fixation duration, we also conducted <italic>t</italic>-tests for two later eye movement measures. The likelihood of refixation was measured as the proportion of trials on which there was at least one saccade that regressed back to that word. The total gaze duration was the sum of all fixations on a given word, including those fixations during regression or re-reading.</p>
</sec>
<sec id="s4h">
<title>MEG data analyses</title>
<p>The data analyses were performed in MATLAB R2020a (Mathworks Inc, USA) by using the FieldTrip (<xref ref-type="bibr" rid="c56">Oostenveld et al., 2011</xref>) toolbox (version 20200220), following the FLUX MEG analysis pipeline(<xref ref-type="bibr" rid="c20">Ferrante et al., 2022</xref>), and custom-made scripts.</p>
<sec id="s4h1">
<title>Pre-processing</title>
<p>We first band-pass filtered the MEG data from 0.5 to 100 Hz using phase preserving two-pass Butterworth filters. Then, we detrended the data to factor out the slow drifts and the linear trends. Malfunctioning sensors were removed based on inspecting the data quality during online recording (0 to 2 sensors per participant). Afterwards, the data were decomposed into independent components using an independent component analysis (ICA) (<xref ref-type="bibr" rid="c27">Ikeda and Toyama, 2000</xref>). The number of components was the same as the number of good MEG sensors in the dataset (306 or less). We only removed bad components that related to eye blinks, eye movements, and heartbeat by visually inspecting the components (3.4 ± 0.7 components per participant, M ± SD, range from 2 to 5 components).</p>
<p>MEG segments were extracted from -0.5 to 0.5 s intervals aligned with the first fixation onset of the pre-target and target words respectively (see Eye movement data analysis, above, for information on how fixation onsets were defined). Segments with fixation durations shorter than 0.08 s or longer than 1 s were discarded. We also extracted 1 s long baseline segments, which were aligned with the cross-fixation onset before the sentence presentation. We manually inspected all segments to further identify and remove segments that were contaminated by muscle or movement artefacts.</p>
</sec>
<sec id="s4h2">
<title>Coherence calculation</title>
<p>We calculated the coherence between the MEG sensors and the photodiode (i.e., the tagging signal) to quantify the tagging responses. The amplitude of photodiode channel was normalized across each segment. To estimate the coherence spectrum in the frequency domain over time, we filtered the segments using hamming tapered Butterworth bandpass filters (4<sup>th</sup> order, phase preserving, two-pass). The frequency of interest was from 40 to 80 Hz in a step of 2 Hz. For each centre frequency point, the spectral smoothing was ± 5 Hz. For example, the filter frequency range for 60 Hz was from 55 to 65 Hz. We performed a Hilbert transform to obtain the analytic signals for each centre frequency point, which then were used to estimate the coherence (<xref rid="eqn2" ref-type="disp-formula">Equation 2</xref>):
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="509511v2_eqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula></p>
<p>where <italic>n</italic> is the number of trials. For the time point <italic>t</italic> in the trial <italic>j</italic>, <italic>m</italic><sub><italic>x</italic></sub>(<italic>t</italic>) and <italic>m</italic><sub><italic>y</italic></sub>(<italic>t</italic>) are the time-varying magnitude of the analytic signals from a MEG sensor (<italic>x</italic>) and the photodiode (<italic>y</italic>) respectively, Φ<sub><italic>xy</italic></sub>(<italic>t</italic>) is the phase difference as a function of time (for detailed description, please see (<xref ref-type="bibr" rid="c10">Cohen, 2014</xref>).</p>
</sec>
<sec id="s4h3">
<title>Selection for the RIFT response sensors</title>
<p>MEG sensors that showed significantly stronger coherence at 60 Hz during the pre-target segments than the baseline segments were selected as the RIFT response sensors. We used a non-parametric Monte-Carlo method (<xref ref-type="bibr" rid="c44">Maris et al., 2007</xref>) to estimate the statistical significance. The pre-target segments were constructed by pooling the target contextual congruity conditions together. Several previous RIFT studies from our lab observed robust tagging responses from the visual cortex for flicker above 50 Hz (<xref ref-type="bibr" rid="c14">Drijvers et al., 2021</xref>; <xref ref-type="bibr" rid="c15">Duecker et al., 2021</xref>; <xref ref-type="bibr" rid="c95">Zhigalov et al., 2019</xref>; <xref ref-type="bibr" rid="c97">Zhigalov and Jensen, 2020</xref>). Thus, this sensor selection procedure was confined to the MEG sensors in the visual cortex (52 planar sensors). Here, the pre-target segments and baseline segments were treated as two conditions. For each combination of MEG sensor and photodiode channel, coherence at 60 Hz was estimated over trials for the pre-target and baseline conditions separately. Then, we calculated the z-statistic value for the coherence difference between pre-target and baseline using the following equation (for details please see (<xref ref-type="bibr" rid="c44">Maris et al., 2007</xref>)) (<xref rid="eqn3" ref-type="disp-formula">Equation 3</xref>):
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="509511v2_eqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula></p>
<p>where coh<sub>1</sub> and coh<sub>2</sub> denote the coherence value for pre-target and baseline segments, bias<sub>1</sub> and bias<sub>2</sub> is the term used to correct for the bias from trial numbers of the pre-target (<italic>n</italic><sub>1</sub>) and baseline condition (<italic>n</italic><sub>2</sub>). All trials from the pre-target and baseline conditions were used.</p>
<p>After obtaining the z statistic value for the empirical coherence difference, we ran a permutation procedure to estimate the statistical significance of this comparison. We randomly shuffled the trial labels between pre-target and baseline conditions 5,000 times. During each permutation, coherence was computed for both conditions (with shuffled labels), then entered <xref rid="eqn3" ref-type="disp-formula">Equation 3</xref> to obtain a z score for the coherence difference. After all randomizations were performed, the resulting z-values established the null distribution. Since a tagging response sensor was supposed to have stronger coherence during the pre-target segments compared with the baseline segments, the statistical test was right sided. If the z-value of the empirical coherence difference was larger than 99% of z-values in the null distribution, this sensor was selected as the RIFT response sensor (right-sided, <italic>p</italic> = .01). For each participant, the coherence values were averaged over all sensors with significant tagging response to obtain an averaged coherence for further analyses.</p>
</sec>
<sec id="s4h4">
<title>Coherence response curve</title>
<p>We first extracted MEG segments for the words N-4, N-3, N-2, N+1, N+2, and N+3 following the same procedure described in the pre-processing when extracted MEG segments for the pre-target (N-1) and target words (N). All segments were 1 s long, aligned with the first fixation onset to the word. Then, we calculated the coherence at 60 Hz during these segments for participants who have RIFT response sensors (n = 29). Next for each participant, the 60 Hz coherence was first averaged over the RIFT response sensors, then averaged within a time window of [0 0.2] s (the averaged fixation duration for words). We got an averaged 60 Hz coherence for the word at each position. We also got the 60 Hz coherence for the baseline interval averaged over [0 0.2] s, aligned with the cross-fixation onset. Then a pairwise <italic>t</italic> test was performed between the baseline coherence and the coherence at each word position.</p>
</sec>
<sec id="s4h5">
<title>Coherence comparison between conditions</title>
<p>The coherence comparison analyses were only conducted for the participants who had sensors with a reliable tagging response (n = 29). To avoid any bias from trial numbers, an equal number of trials under the different contextual congruity conditions was entered the coherence analysis per participant. We randomly discarded the redundant trials from the condition that had more trials for both the pre-target and target segments.</p>
<p>To compare the pre-target coherence amplitude between conditions, the coherence values at 60 Hz were averaged across the minimum fixation duration of all pre-target words. The time window for averaging was defined for each participant so that the coherence signal from the target fixation was not involved. Similarly, we averaged the 60 Hz coherence for the target segments over the minimum target fixation duration. Then, a two-sided pairwise Student’s <italic>t</italic>-test was performed to estimate the statistical significance of the coherence difference as shown in <xref rid="fig3" ref-type="fig">Figure 3B</xref> and <xref rid="fig4" ref-type="fig">Figure 4B</xref>.</p>
<p>To assess the coherence onset latency difference between conditions, we used a leave-one-out Jackknife-based method (<xref ref-type="bibr" rid="c48">Miller et al., 1998</xref>). We extracted the 60 Hz coherence during the 1 s long pre-target segments for each participant. Then, during each iteration of participants, we randomly chose and left out one participant. For the remaining participants, coherences were calculated for the incongruent and congruent target conditions. Then, the coherence was averaged over the remaining participants to estimate the onset latency for both conditions. Here, the onset latency was defined as the time point when the averaged coherence value reached its half-maximum (coh<sub>min</sub> + (coh<sub>max</sub> − coh<sub>min</sub>)/2). We computed the onset latency difference by subtracting the onset latency for the incongruent target condition from the congruent condition. After all iterations, onset latency differences from all these subsamples were pooled together to estimate a standard error (<italic>SD</italic>) using the following equation (<xref rid="eqn4" ref-type="disp-formula">Equation 4</xref>):
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="509511v2_eqn4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula></p>
<p>where <inline-formula><inline-graphic xlink:href="509511v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the average onset latency difference over all the subsamples, <italic>D</italic><sub>−<italic>i</italic></sub> is the coherence difference obtained from the subsample when participant <italic>i</italic> was left out, <italic>n</italic> is the number of participants. We also computed the onset latency difference from the overall sample set (without leaving any participant out) and divided it by the <italic>SD</italic> to obtain its <italic>t</italic>-value. A standard t table (pairwise, two-tailed) provided the statistical significance for the coherence onset latency difference between the incongruent and congruent target conditions. This procedure was conducted for both the pre-target and target segments as shown in <xref rid="fig3" ref-type="fig">Figure 3C</xref> and <xref rid="fig4" ref-type="fig">Figure 4C</xref>.</p>
</sec>
<sec id="s4h6">
<title>Source analysis for RIFT</title>
<p>We used a beamforming-based approach, Dynamic Imaging Coherent Sources (DICS) (<xref ref-type="bibr" rid="c22">Gross et al., 2001</xref>), to estimate the neural sources that generated the responses to RIFT. The DICS technique was applied to the pre-target segments regardless of the target contextual congruity conditions, with a focus of 60 Hz in the frequency domain. In this source analysis, only participants with robust tagging responses were included (n = 29).</p>
<p>First, we constructed a semi-realistic head model, where spherical harmonic functions were used to fit the brain surface (<xref ref-type="bibr" rid="c54">Nolte, 2003</xref>). We aligned the individual structural MRI image with the head shape that was digitized during the MEG session. This was done by spatially co-registering the three fiducial anatomical markers (nasion, left and right ear canal) and extra points that covered the whole scalp. For participants whose MRI image was unavailable, the MNI template brain was used instead. The aligned MRI image was segmented into a grid, which was used to prepare the single-shell head model.</p>
<p>Next, we constructed the individual source model by inverse-warping a 5 mm spaced regular grid in the MNI template space to each participant’s segmented MRI image. We got the regular grid from the Fieldtrip template folder, which was constructed before doing the source analysis. In this way, the beamformer spatial filters were constructed on the regular grid that mapped to the MNI template space. Even though after this warping procedure grid points in the individual native space were no longer evenly spaced, the homologous grid points across participants were located at the same location in the normalized template space. Thus, the reconstructed sources can be directly averaged across participants on the group level.</p>
<p>Next, the Cross-Spectral Density (CSD) matrix was calculated at 60 Hz for both the pre-target and baseline segments. The CSD matrix was constructed for all possible combinations between the MEG sensors and the photodiode channel. No regularisation was performed to the CSD matrices (lambda = 0).</p>
<p>Finally, a common spatial filter was computed based on the individual single-shell head model, source model, and CSD matrices. This spatial filter was applied to both the pre-target and baseline CSD matrices for calculating the 60 Hz coherence. This was done by normalizing the magnitude of the summed CSD between the MEG sensor and the photodiode channel by their respective power. After the grand average over participants, the relative change for pre-target coherence was estimated as the following formula, (<italic>coh</italic><sub><italic>pretarget</italic></sub> − <italic>coh<sub>baseline</sub></italic>)/<italic>coh<sub>baseline</sub></italic>.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Jonathan L. Winter for providing help with the MEG recordings. The computations described in this paper were performed using the University of Birmingham’s BlueBEAR HPC service, which provides a High Performance Computing service to the University’s research community. See <ext-link ext-link-type="uri" xlink:href="http://www.birmingham.ac.uk/bear">http://www.birmingham.ac.uk/bear</ext-link> for more details.</p>
</ack>
<sec id="s5">
<title>Funding</title>
<p>This study was supported by the following grants to O.J.: the James S. McDonnell Foundation Understanding Human Cognition Collaborative Award (grant number 220020448), Wellcome Trust Investigator Award in Science (grant number 207550), and the BBSRC grant (BB/R018723/1) as well as the Royal Society Wolfson Research Merit Award. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
</sec>
<sec id="s6">
<title>Author contributions</title>
<p>Y.P., S.F., K.D.F., and O.J. devised and designed the experiments, Y.P. made the sentences with assistance from S.F., Y.P. programmed and conducted the experiments, Y.P. carried out the analyses with assistance from O.J. and S.F., Y.P., O.J., K.D.F., and S.F. wrote the paper together.</p>
</sec>
<sec id="s7" sec-type="COI-statement">
<title>Competing interests</title>
<p>Authors declare that they have no competing interests.</p>
</sec>
<sec id="s8">
<title>Data availability</title>
<p>We have deposited the following data in the current study on figshare (<ext-link ext-link-type="uri" xlink:href="https://figshare.com/projects/Semantic/149801">https://figshare.com/projects/Semantic/149801</ext-link>): the epoch data after pre-processing, the raw EyeLink files, the Psychotoolbox data, and the head models after the co-registration of T1 images with the MEG data. Any additional information will be available from the authors upon reasonable request.</p>
</sec>
<sec id="s9">
<title>Code availability</title>
<p>The experiment presentation scripts (Psychtoolbox), statistics scripts (R), scripts and data to generate all figures (Matlab) are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/yalipan666/Semantic">https://github.com/yalipan666/Semantic</ext-link>).</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Antúnez</surname> <given-names>M</given-names></string-name>, <string-name><surname>Milligan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Andrés Hernández-Cabrera</surname> <given-names>J</given-names></string-name>, <string-name><surname>Barber</surname> <given-names>HA</given-names></string-name>, <string-name><surname>Schotter</surname> <given-names>ER.</given-names></string-name> <year>2022</year>. <article-title>Semantic parafoveal processing in natural reading: Insight from fixation-related potentials &amp; eye movements</article-title>. <source>Psychophysiology</source> <volume>59</volume>:<fpage>e13986</fpage>. doi:<pub-id pub-id-type="doi">10.1111/PSYP.13986</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Ashby</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>2004</year>. <article-title>Representing syllable information during silent reading: Evidence from eye movements</article-title>. <source>Lang Cogn Process</source> <volume>19</volume>:<fpage>391</fpage>–<lpage>426</lpage>. doi:<pub-id pub-id-type="doi">10.1080/01690960344000233</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Ashby</surname> <given-names>J</given-names></string-name>, <string-name><surname>Treiman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kessler</surname> <given-names>B</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>2006</year>. <article-title>Vowel processing during silent reading: Evidence from eye movements</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>32</volume>:<fpage>416</fpage>–<lpage>424</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0278-7393.32.2.416</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Barber</surname> <given-names>HA</given-names></string-name>, <string-name><surname>Doñamayor</surname> <given-names>N</given-names></string-name>, <string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Münte</surname> <given-names>T.</given-names></string-name> <year>2010</year>. <article-title>Parafoveal N400 effect during sentence reading</article-title>. <source>Neurosci Lett</source> <volume>479</volume>:<fpage>152</fpage>–<lpage>156</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neulet.2010.05.053</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Barber</surname> <given-names>HA</given-names></string-name>, <string-name><surname>van der Meij</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kutas</surname> <given-names>M.</given-names></string-name> <year>2013</year>. <article-title>An electrophysiological analysis of contextual and temporal constraints on parafoveal word processing</article-title>. <source>Psychophysiology</source> <volume>50</volume>:<fpage>48</fpage>–<lpage>59</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1469-8986.2012.01489.x</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Beyersmann</surname> <given-names>E</given-names></string-name>, <string-name><surname>Montani</surname> <given-names>V</given-names></string-name>, <string-name><surname>Ziegler</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Grainger</surname> <given-names>J</given-names></string-name>, <string-name><surname>Stoianov</surname> <given-names>IP.</given-names></string-name> <year>2021</year>. <article-title>The dynamics of reading complex words: evidence from steady-state visual evoked potentials</article-title>. <source>Sci Reports 2021</source> <volume>111</volume> <fpage>11:1</fpage>–<lpage>14</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41598-021-95292-0</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Braze</surname> <given-names>D</given-names></string-name>, <string-name><surname>Shankweiler</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ni</surname> <given-names>W</given-names></string-name>, <string-name><surname>Palumbo</surname> <given-names>LC.</given-names></string-name> <year>2002</year>. <article-title>Readers’ eye movements distinguish anomalies of form and content</article-title>. <source>J Psycholinguist Res</source> <volume>31</volume>:<fpage>25</fpage>–<lpage>44</lpage>. doi:<pub-id pub-id-type="doi">10.1023/A:1014324220455/METRICS</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Brickwedde</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bezsudnova</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Kowalczyk</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O</given-names></string-name>, <string-name><surname>Zhigalov</surname> <given-names>A.</given-names></string-name> <year>2022</year>. <article-title>Application of rapid invisible frequency tagging for brain computer interfaces</article-title>. <source>J Neurosci Methods</source> <volume>382</volume>:<fpage>109726</fpage>. doi:<pub-id pub-id-type="doi">10.1016/J.JNEUMETH.2022.109726</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Chace</surname> <given-names>KH</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Well</surname> <given-names>AD.</given-names></string-name> <year>2005</year>. <article-title>Eye movements and phonological parafoveal preview: Effects of reading skill</article-title>. <source>Can J Exp Psychol</source> <volume>59</volume>:<fpage>209</fpage>–<lpage>217</lpage>. doi:<pub-id pub-id-type="doi">10.1037/h0087476</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="book"><string-name><surname>Cohen</surname> <given-names>MX.</given-names></string-name> <year>2014</year>. <chapter-title>Analyzing neural time series data: Theory and practice</chapter-title>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Davis</surname> <given-names>CJ.</given-names></string-name> <year>2005</year>. <article-title>N-watch: A program for deriving neighborhood size and other psycholinguistic statistics</article-title>. <source>Behav Res Methods</source> <volume>37</volume>:<fpage>65</fpage>–<lpage>70</lpage>. doi:<pub-id pub-id-type="doi">10.3758/BF03206399</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>DeLong</surname> <given-names>KA</given-names></string-name>, <string-name><surname>Quante</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kutas</surname> <given-names>M.</given-names></string-name> <year>2014</year>. <article-title>Predictability, plausibility, and two late ERP positivities during written sentence comprehension</article-title>. <source>Neuropsychologia</source> <volume>61</volume>:<fpage>150</fpage>–<lpage>162</lpage>. doi:<pub-id pub-id-type="doi">10.1016/J.NEUROPSYCHOLOGIA.2014.06.016</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Drieghe</surname> <given-names>D</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A.</given-names></string-name> <year>2005</year>. <article-title>Eye movements and word skipping during reading revisited</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>31</volume>:<fpage>954</fpage>–<lpage>969</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-1523.31.5.954</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Drijvers</surname> <given-names>L</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O</given-names></string-name>, <string-name><surname>Spaak</surname> <given-names>E.</given-names></string-name> <year>2021</year>. <article-title>Rapid invisible frequency tagging reveals nonlinear integration of auditory and visual information</article-title>. <source>Hum Brain Mapp</source> <volume>42</volume>:<fpage>1138</fpage>–<lpage>1152</lpage>. doi:<pub-id pub-id-type="doi">10.1002/HBM.25282</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Duecker</surname> <given-names>K</given-names></string-name>, <string-name><surname>Gutteling</surname> <given-names>TP</given-names></string-name>, <string-name><surname>Herrmann</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2021</year>. <article-title>No Evidence for Entrainment: Endogenous Gamma Oscillations and Rhythmic Flicker Responses Coexist in Visual Cortex</article-title>. <source>J Neurosci</source> <volume>41</volume>:<fpage>6684</fpage>–<lpage>6698</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3134-20.2021</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Engbert</surname> <given-names>R</given-names></string-name>, <string-name><surname>Longtin</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R.</given-names></string-name> <year>2002</year>. <article-title>A dynamical model of saccade generation in reading based on spatially distributed lexical processing</article-title>. <source>Vision Res</source> <volume>42</volume>:<fpage>621</fpage>–<lpage>636</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0042-6989(01)00301-7</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Engbert</surname> <given-names>R</given-names></string-name>, <string-name><surname>Nuthmann</surname> <given-names>A</given-names></string-name>, <string-name><surname>Richter</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R.</given-names></string-name> <year>2005</year>. <article-title>Swift: A dynamical model of saccade generation during reading</article-title>. <source>Psychol Rev</source> <volume>112</volume>:<fpage>777</fpage>–<lpage>813</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0033-295X.112.4.777</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Federmeier</surname> <given-names>KD.</given-names></string-name> <year>2022</year>. <article-title>Connecting and considering: Electrophysiology provides insights into comprehension</article-title>. <source>Psychophysiology</source> <volume>59</volume>:<fpage>e13940</fpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Federmeier</surname> <given-names>KD</given-names></string-name>, <string-name><surname>Wlotko</surname> <given-names>EW</given-names></string-name>, <string-name><surname>De Ochoa-Dewald</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kutas</surname> <given-names>M.</given-names></string-name> <year>2007</year>. <article-title>Multiple effects of sentential constraint on word processing</article-title>. <source>Brain Res</source> <volume>1146</volume>:<fpage>75</fpage>–<lpage>84</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.brainres.2006.06.101</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Ferrante</surname> <given-names>O</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Minarik</surname> <given-names>T</given-names></string-name>, <string-name><surname>Gorska</surname> <given-names>U</given-names></string-name>, <string-name><surname>Ghafari</surname> <given-names>T</given-names></string-name>, <string-name><surname>Luo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2022</year>. <article-title>FLUX: A pipeline for MEG analysis</article-title>. <source>Neuroimage</source> <volume>253</volume>:<fpage>119047</fpage>. doi:<pub-id pub-id-type="doi">10.1016/J.NEUROIMAGE.2022.119047</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Ferrante</surname> <given-names>O</given-names></string-name>, <string-name><surname>Zhigalov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hickey</surname> <given-names>C</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2023</year>. <article-title>Statistical Learning of Distractor Suppression Down-regulates Pre-Stimulus Neural Excitability in Early Visual Cortex</article-title>. <source>J Neurosci JN-RM</source>-<fpage>1703</fpage>–<lpage>22</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1703-22.2022</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Gross</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kujala</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hämäläinen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Timmermann</surname> <given-names>L</given-names></string-name>, <string-name><surname>Schnitzler</surname> <given-names>A</given-names></string-name>, <string-name><surname>Salmelin</surname> <given-names>R.</given-names></string-name> <year>2001</year>. <article-title>Dynamic imaging of coherent sources: Studying neural interactions in the human brain</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>98</volume>:<fpage>694</fpage>–<lpage>699</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.98.2.694</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Gulbinaite</surname> <given-names>R</given-names></string-name>, <string-name><surname>Roozendaal</surname> <given-names>DHM</given-names></string-name>, <string-name><surname>VanRullen</surname> <given-names>R.</given-names></string-name> <year>2019</year>. <article-title>Attention differentially modulates the amplitude of resonance frequencies in the visual cortex</article-title>. <source>Neuroimage</source> <volume>203</volume>:<fpage>1</fpage>–<lpage>40</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116146</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Gutteling</surname> <given-names>TP</given-names></string-name>, <string-name><surname>Sillekens</surname> <given-names>L</given-names></string-name>, <string-name><surname>Lavie</surname> <given-names>N</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2022</year>. <article-title>Alpha oscillations reflect suppression of distractors with increased perceptual load</article-title>. <source>Prog Neurobiol</source> <volume>214</volume>:<fpage>102285</fpage>. doi:<pub-id pub-id-type="doi">10.1016/J.PNEUROBIO.2022.102285</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Hohenstein</surname> <given-names>S</given-names></string-name>, <string-name><surname>Klieg</surname> <given-names>R.</given-names></string-name> <year>2014</year>. <article-title>Semantic preview benefit during reading</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>40</volume>:<fpage>166</fpage>–<lpage>190</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0033670</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Hohenstein</surname> <given-names>S</given-names></string-name>, <string-name><surname>Laubrock</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R.</given-names></string-name> <year>2010</year>. <article-title>Semantic preview benefit in eye movements during reading: A parafoveal fast-priming study</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>36</volume>:<fpage>1150</fpage>–<lpage>1170</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0020233</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Ikeda</surname> <given-names>S</given-names></string-name>, <string-name><surname>Toyama</surname> <given-names>K.</given-names></string-name> <year>2000</year>. <article-title>Independent component analysis for noisy data - MEG data analysis</article-title>. <source>Neural Networks</source> <volume>13</volume>:<fpage>1063</fpage>–<lpage>1074</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0893-6080(00)00071-X</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Inhoff</surname> <given-names>AW.</given-names></string-name> <year>1989</year>. <article-title>Parafoveal processing of words and saccade computation during eye fixations in reading</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>15</volume>:<fpage>544</fpage>–<lpage>555</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Inhoff</surname> <given-names>AW.</given-names></string-name> <year>1982</year>. <article-title>Parafoveal word perception: A further case against semantic preprocessing</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>8</volume>:<fpage>137</fpage>–<lpage>145</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Inhoff</surname> <given-names>AW</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>1980</year>. <article-title>Parafoveal word perception: A case against semantic preprocessing</article-title>. <source>Percept Psychophys</source> <volume>27</volume>:<fpage>457</fpage>–<lpage>464</lpage>. doi:<pub-id pub-id-type="doi">10.3758/BF03204463</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Jensen</surname> <given-names>O</given-names></string-name>, <string-name><surname>Pan</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Frisson</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>L.</given-names></string-name> <year>2021</year>. <article-title>An oscillatory pipelining mechanism supporting previewing during visual exploration and reading</article-title>. <source>Trends Cogn Sci</source>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Johnson</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Perea</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>2007</year>. <article-title>Transposed-letter effects in reading: Evidence from eye movements and parafoveal preview</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>33</volume>:<fpage>209</fpage>–<lpage>229</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-1523.33.1.209</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Kennedy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pynte</surname> <given-names>J.</given-names></string-name> <year>2005</year>. <article-title>Parafoveal-on-foveal effects in normal reading</article-title>. <source>Vision Res</source> <volume>45</volume>:<fpage>153</fpage>–<lpage>168</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.visres.2004.07.037</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="other"><string-name><surname>Kleiner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Brainard</surname> <given-names>D</given-names></string-name>, <string-name><surname>Pelli</surname> <given-names>D.</given-names></string-name> <year>2007</year>. <article-title>What’s new in Psychtoolbox-3?</article-title> <volume>14</volume>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Kliegl</surname> <given-names>R</given-names></string-name>, <string-name><surname>Nuthmann</surname> <given-names>A</given-names></string-name>, <string-name><surname>Engbert</surname> <given-names>R.</given-names></string-name> <year>2006</year>. <article-title>Tracking the mind during reading: The influence of past, present, and future words on fixation durations</article-title>. <source>J Exp Psychol Gen</source> <volume>135</volume>:<fpage>12</fpage>–<lpage>35</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-3445.135.1.12</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Kritzman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Eidelman-Rothman</surname> <given-names>M</given-names></string-name>, <string-name><surname>Keil</surname> <given-names>A</given-names></string-name>, <string-name><surname>Freche</surname> <given-names>D</given-names></string-name>, <string-name><surname>Sheppes</surname> <given-names>G</given-names></string-name>, <string-name><surname>Levit-Binnun</surname> <given-names>N.</given-names></string-name> <year>2022</year>. <article-title>Steady-state visual evoked potentials differentiate between internally and externally directed attention</article-title>. <source>Neuroimage</source> <volume>254</volume>:<fpage>119133</fpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Federmeier</surname> <given-names>KD.</given-names></string-name> <year>2011</year>. <article-title>Thirty Years and Counting: Finding Meaning in the N400 Component of the Event-Related Brain Potential (ERP)</article-title>. <source>Annu Rev Psychol</source> <volume>62</volume>:<fpage>621</fpage>–<lpage>647</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hillyard</surname> <given-names>SA.</given-names></string-name> <year>1984</year>. <article-title>Brain potentials during reading reflect word expectancy and semantic association</article-title>. <source>Nature</source> <volume>307</volume>:<fpage>161</fpage>–<lpage>163</lpage>. doi:<pub-id pub-id-type="doi">10.1038/307161a0</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hillyard</surname> <given-names>SA.</given-names></string-name> <year>1980</year>. <article-title>Reading senseless sentences: brain potentials reflect semantic incongruity</article-title>. <source>Science (80-)</source> <volume>207</volume>:<fpage>203</fpage>–<lpage>205</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.7350657</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Lau</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Phillips</surname> <given-names>C</given-names></string-name>, <string-name><surname>Poeppel</surname> <given-names>D.</given-names></string-name> <year>2008</year>. <article-title>A cortical network for semantics: (De)constructing the N400</article-title>. <source>Nat Rev Neurosci</source> <volume>9</volume>:<fpage>920</fpage>–<lpage>933</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn2532</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Li</surname> <given-names>C</given-names></string-name>, <string-name><surname>Midgley</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Holcomb</surname> <given-names>PJ.</given-names></string-name> <year>2023</year>. <article-title>ERPs reveal how semantic and syntactic processing unfold across parafoveal and foveal vision during sentence comprehension</article-title>. <source>Lang Cogn Neurosci</source> <volume>38</volume>:<fpage>88</fpage>–<lpage>104</lpage>. doi:<pub-id pub-id-type="doi">10.1080/23273798.2022.2091150</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Li</surname> <given-names>N</given-names></string-name>, <string-name><surname>Niefind</surname> <given-names>F</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sommer</surname> <given-names>W</given-names></string-name>, <string-name><surname>Dimigen</surname> <given-names>O.</given-names></string-name> <year>2015</year>. <article-title>Parafoveal processing in reading Chinese sentences: Evidence from event-related brain potentials</article-title>. <source>Psychophysiology</source> <volume>52</volume>:<fpage>1361</fpage>–<lpage>1374</lpage>. doi:<pub-id pub-id-type="doi">10.1111/PSYP.12502</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>López-Peréz</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Dampuré</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hernández-Cabrera</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Barber</surname> <given-names>HA.</given-names></string-name> <year>2016</year>. <article-title>Semantic parafoveal-on-foveal effects and preview benefits in reading: Evidence from Fixation Related Potentials</article-title>. <source>Brain Lang</source> <volume>162</volume>:<fpage>29</fpage>–<lpage>34</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.bandl.2016.07.009</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Fries</surname> <given-names>P.</given-names></string-name> <year>2007</year>. <article-title>Nonparametric statistical testing of coherence differences</article-title>. <source>J Neurosci Methods</source> <volume>163</volume>:<fpage>161</fpage>–<lpage>175</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.02.011</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>McConkie</surname> <given-names>GW</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>1975</year>. <article-title>The span of the effective stimulus during a fixation in reading</article-title>. <source>Percept Psychophys</source> <volume>17</volume>:<fpage>578</fpage>–<lpage>586</lpage>. doi:<pub-id pub-id-type="doi">10.3758/BF03203972</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Meade</surname> <given-names>G</given-names></string-name>, <string-name><surname>Declerck</surname> <given-names>M</given-names></string-name>, <string-name><surname>Holcomb</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Grainger</surname> <given-names>J.</given-names></string-name> <year>2021</year>. <article-title>Parallel semantic processing in the flankers task: Evidence from the N400</article-title>. <source>Brain Lang</source> <volume>219</volume>:<fpage>104965</fpage>. doi:<pub-id pub-id-type="doi">10.1016/J.BANDL.2021.104965</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Miellet</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sparrow</surname> <given-names>L.</given-names></string-name> <year>2004</year>. <article-title>Phonological codes are assembled before word fixation: Evidence from boundary paradigm in sentence reading</article-title>. <source>Brain Lang</source> <volume>90</volume>:<fpage>299</fpage>–<lpage>310</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0093-934X(03)00442-5</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Miller</surname> <given-names>J</given-names></string-name>, <string-name><surname>Patterson</surname> <given-names>T</given-names></string-name>, <string-name><surname>Ulrich</surname> <given-names>R.</given-names></string-name> <year>1998</year>. <article-title>Jackknife-based method for measuring LRP onset latency differences</article-title>. <source>Psychophysiology</source> <volume>35</volume>:<fpage>99</fpage>–<lpage>115</lpage>. doi:<pub-id pub-id-type="doi">10.1017/S0048577298000857</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Milligan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nestor</surname> <given-names>B</given-names></string-name>, <string-name><surname>Antúnez</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schotter</surname> <given-names>ER.</given-names></string-name> <year>2023</year>. <article-title>Out of Sight, Out of Mind: Foveal Processing is Necessary for Semantic Integration of Words into Sentence Context</article-title>. <source>J Exp Psychol Hum Percept Perform</source>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Montani</surname> <given-names>V</given-names></string-name>, <string-name><surname>Chanoine</surname> <given-names>V</given-names></string-name>, <string-name><surname>Stoianov</surname> <given-names>IP</given-names></string-name>, <string-name><surname>Grainger</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ziegler</surname> <given-names>JC.</given-names></string-name> <year>2019</year>. <article-title>Steady state visual evoked potentials in reading aloud: Effects of lexicality, frequency and orthographic familiarity</article-title>. <source>Brain Lang</source> <volume>192</volume>:<fpage>1</fpage>–<lpage>14</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.bandl.2019.01.004</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Müller</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Malinowski</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gruber</surname> <given-names>T</given-names></string-name>, <string-name><surname>Hillyard</surname> <given-names>SA.</given-names></string-name> <year>2003</year>. <article-title>Sustained division of the attentional spotlight</article-title>. <source>Nature</source> <volume>424</volume>:<fpage>309</fpage>–<lpage>312</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature01812</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Müller</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Picton</surname> <given-names>TW</given-names></string-name>, <string-name><surname>Valdes-Sosa</surname> <given-names>P</given-names></string-name>, <string-name><surname>Riera</surname> <given-names>J</given-names></string-name>, <string-name><surname>Teder-Sälejärvi</surname> <given-names>WA</given-names></string-name>, <string-name><surname>Hillyard</surname> <given-names>SA.</given-names></string-name> <year>1998</year>. <article-title>Effects of spatial selective attention on the steady-state visual evoked potential in the 20–28 Hz range</article-title>. <source>Cogn Brain Res</source> <volume>6</volume>:<fpage>249</fpage>–<lpage>261</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0926-6410(97)00036-0</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Ni</surname> <given-names>W</given-names></string-name>, <string-name><surname>Fodor</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Crain</surname> <given-names>S</given-names></string-name>, <string-name><surname>Shankweiler</surname> <given-names>D.</given-names></string-name> <year>1998</year>. <article-title>Anomaly Detection: Eye Movement Patterns</article-title>. <source>J Psycholinguist Res</source> <volume>27</volume>:<fpage>515</fpage>–<lpage>539</lpage>. doi:<pub-id pub-id-type="doi">10.1023/A:1024996828734/METRICS</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Nolte</surname> <given-names>G.</given-names></string-name> <year>2003</year>. <article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoenchephalography forward calculation in realistic volume conductors</article-title>. <source>Phys Med Biol</source> <volume>48</volume>:<fpage>3637</fpage>–<lpage>3652</lpage>. doi:<pub-id pub-id-type="doi">10.1088/0031-9155/48/22/002</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Norcia</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Gregory Appelbaum</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ales</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Cottereau</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Rossion</surname> <given-names>B.</given-names></string-name> <year>2015</year>. <article-title>The steady-state visual evoked potential in vision research: A review</article-title>. <source>J Vis</source> <volume>15</volume>:<fpage>4</fpage>–<lpage>4</lpage>. doi:<pub-id pub-id-type="doi">10.1167/15.6.4</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fries</surname> <given-names>P</given-names></string-name>, <string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>J-M.</given-names></string-name> <year>2011</year>. <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Comput Intell Neurosci</source> <volume>2011</volume>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Pan</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Frisson</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2021</year>. <article-title>Neural evidence for lexical parafoveal processing</article-title>. <source>Nat Commun</source> <volume>2021</volume> <issue>121</issue> <fpage>12:1</fpage>–<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41467-021-25571-x</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Pan</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Popov</surname> <given-names>T</given-names></string-name>, <string-name><surname>Frisson</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2023</year>. <article-title>Saccades are locked to the phase of alpha oscillations during natural reading</article-title>. <source>PLOS Biol</source> <volume>21</volume>:<fpage>e3001968</fpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Payne</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Federmeier</surname> <given-names>KD.</given-names></string-name> <year>2017</year>. <article-title>Event-related brain potentials reveal age-related changes in parafoveal-foveal integration during sentence processing</article-title>. <source>Neuropsychologia</source> <volume>106</volume>:<fpage>358</fpage>–<lpage>370</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.10.002</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Payne</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Stites</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Federmeier</surname> <given-names>KD.</given-names></string-name> <year>2019</year>. <article-title>Event-related brain potentials reveal how multiple aspects of semantic processing unfold across parafoveal and foveal vision during sentence reading</article-title>. <source>Psychophysiology</source> <volume>56</volume>:<fpage>1</fpage>–<lpage>15</lpage>. doi:<pub-id pub-id-type="doi">10.1111/psyp.13432</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Pollatsek</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lesch</surname> <given-names>M</given-names></string-name>, <string-name><surname>Morris</surname> <given-names>RK</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>1992</year>. <article-title>Phonological Codes Are Used in Integrating Information Across Saccades in Word Identification and Reading</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>18</volume>:<fpage>148</fpage>–<lpage>162</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-1523.18.1.148</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>2009</year>. <article-title>Eye movements and attention in reading, scene perception, and visual search</article-title>. <source>Q J Exp Psychol</source> <volume>62</volume>:<fpage>1457</fpage>–<lpage>1506</lpage>. doi:<pub-id pub-id-type="doi">10.1080/17470210902816461</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>1975</year>. <article-title>The perceptual span and peripheral cues in reading</article-title>. <source>Cogn Psychol</source> <volume>7</volume>:<fpage>65</fpage>–<lpage>81</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Balota</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A.</given-names></string-name> <year>1986</year>. <article-title>Against parafoveal semantic preprocessing during eye fixations in reading</article-title>. <source>Artic Can J Psychol Rev Can Psychol</source> <volume>40</volume>:<fpage>473</fpage>–<lpage>483</lpage>. doi:<pub-id pub-id-type="doi">10.1037/h0080111</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Schotter</surname> <given-names>ER.</given-names></string-name> <year>2014</year>. <article-title>Semantic preview benefit in reading english: The effect of initial letter capitalization</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>40</volume>:<fpage>1617</fpage>–<lpage>1628</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0036763</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Schotter</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Drieghe</surname> <given-names>D.</given-names></string-name> <year>2014</year>. <article-title>Lack of semantic parafoveal preview benefit in reading revisited</article-title>. <source>Psychon Bull Rev</source> <volume>21</volume>:<fpage>1067</fpage>–<lpage>1072</lpage>. doi:<pub-id pub-id-type="doi">10.3758/s13423-014-0582-9</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Lesch</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A.</given-names></string-name> <year>1995</year>. <article-title>Phonological Codes Are Automatically Activated During Reading: Evidence From an Eye Movement Priming Paradigm</article-title>. <source>Psychol Sci</source> <volume>6</volume>:<fpage>26</fpage>–<lpage>32</lpage>. doi:<pub-id pub-id-type="doi">10.1111/J.1467-9280.1995.TB00300.X</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Warren</surname> <given-names>T</given-names></string-name>, <string-name><surname>Juhasz</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Liversedge</surname> <given-names>SP.</given-names></string-name> <year>2004</year>. <article-title>The Effect of Plausibility on Eye Movements in Reading</article-title>. doi:<pub-id pub-id-type="doi">10.1037/0278-7393.30.6.1290</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fisher</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>1998</year>. <article-title>Toward a model of eye movement control in reading</article-title>. <source>Psychol Rev</source> <volume>105</volume>:<fpage>125</fpage>–<lpage>157</lpage>. doi:<pub-id pub-id-type="doi">10.1016/b978-0-444-70113-8.50043-6</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>2006</year>. <article-title>E-Z Reader: A cognitive-control, serial-attention model of eye-movement behavior during reading</article-title>. <source>Cogn Syst Res</source> <volume>7</volume>:<fpage>4</fpage>–<lpage>22</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cogsys.2005.07.002</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A.</given-names></string-name> <year>2003</year>. <article-title>The E-Z reader model of eye-movement control in reading: Comparisons to other models</article-title>. <source>Behav Brain Sci</source> <volume>26</volume>:<fpage>445</fpage>–<lpage>476</lpage>. doi:<pub-id pub-id-type="doi">10.1017/S0140525X03000104</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Reingold</surname> <given-names>EM.</given-names></string-name> <year>2013</year>. <article-title>Neurophysiological constraints on the eye-mind link</article-title>. <source>Front Hum Neurosci</source> <volume>7</volume>:<fpage>361</fpage>. doi:<pub-id pub-id-type="doi">10.3389/fnhum.2013.00361</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><string-name><surname>Reichle</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Warren</surname> <given-names>T</given-names></string-name>, <string-name><surname>McConnell</surname> <given-names>K.</given-names></string-name> <year>2009</year>. <article-title>Using E-Z reader to model the effects of higher level language processing on eye movements during reading</article-title>. <source>Psychon Bull Rev</source>. doi:<pub-id pub-id-type="doi">10.3758/PBR.16.1.1</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><string-name><surname>Schotter</surname> <given-names>ER.</given-names></string-name> <year>2018</year>. <article-title>Reading Ahead by Hedging Our Bets on Seeing the Future: Eye Tracking and Electrophysiology Evidence for Parafoveal Lexical Processing and Saccadic Control by Partial Word Recognition</article-title>. <source>Psychol Learn Motiv - Adv Res Theory</source> <volume>68</volume>:<fpage>263</fpage>–<lpage>298</lpage>. doi:<pub-id pub-id-type="doi">10.1016/BS.PLM.2018.08.011</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><string-name><surname>Schotter</surname> <given-names>ER.</given-names></string-name> <year>2013</year>. <article-title>Synonyms provide semantic preview benefit in English</article-title>. <source>J Mem Lang</source> <volume>69</volume>:<fpage>619</fpage>–<lpage>633</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jml.2013.09.002</pub-id></mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><string-name><surname>Schotter</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Angele</surname> <given-names>B</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>2012</year>. <article-title>Parafoveal processing in reading</article-title>. <source>Attention, Perception, Psychophys</source> <volume>74</volume>:<fpage>5</fpage>–<lpage>35</lpage>. doi:<pub-id pub-id-type="doi">10.3758/s13414-011-0219-2</pub-id></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><string-name><surname>Schotter</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Jia</surname> <given-names>A.</given-names></string-name> <year>2016</year>. <article-title>Semantic and plausibility preview benefit effects in English: Evidence from eye movements</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>42</volume>:<fpage>1839</fpage>–<lpage>1866</lpage>. doi:<pub-id pub-id-type="doi">10.1037/xlm0000281</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><string-name><surname>Schotter</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>M</given-names></string-name>, <string-name><surname>Reiderman</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>2015</year>. <article-title>The effect of contextual constraint on parafoveal processing in reading</article-title>. <source>J Mem Lang</source> <volume>83</volume>:<fpage>118</fpage>–<lpage>139</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jml.2015.04.005</pub-id></mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><string-name><surname>Schotter</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Milligan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Estevez</surname> <given-names>VM.</given-names></string-name> <year>2023</year>. <article-title>Event-related potentials show that parafoveal vision is insufficient for semantic integration</article-title>. <source>Psychophysiology e</source><volume>14246</volume>. doi:<pub-id pub-id-type="doi">10.1111/PSYP.14246</pub-id></mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><string-name><surname>Snell</surname> <given-names>J</given-names></string-name>, <string-name><surname>Grainger</surname> <given-names>J.</given-names></string-name> <year>2019</year>. <article-title>Readers are parallel processors</article-title>. <source>Trends Cogn Sci</source> <volume>23</volume>:<fpage>537</fpage>–<lpage>546</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2019.04.006</pub-id></mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><string-name><surname>Snell</surname> <given-names>J</given-names></string-name>, <string-name><surname>Meeter</surname> <given-names>M</given-names></string-name>, <string-name><surname>Grainger</surname> <given-names>J.</given-names></string-name> <year>2017</year>. <article-title>Evidence for simultaneous syntactic processing of multiple words during reading</article-title>. <source>PLoS One</source> <volume>12</volume>:<fpage>1</fpage>–<lpage>17</lpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0173720</pub-id></mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><string-name><surname>Snell</surname> <given-names>J</given-names></string-name>, <string-name><surname>van Leipsig</surname> <given-names>S</given-names></string-name>, <string-name><surname>Grainger</surname> <given-names>J</given-names></string-name>, <string-name><surname>Meeter</surname> <given-names>M.</given-names></string-name> <year>2018</year>. <article-title>OB1-reader: A model of word recognition and eye movements in text reading</article-title>. <source>Psychol Rev</source> <volume>125</volume>:<fpage>969</fpage>–<lpage>984</lpage>. doi:<pub-id pub-id-type="doi">10.1037/rev0000119</pub-id></mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><string-name><surname>Stites</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Payne</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Federmeier</surname> <given-names>KD.</given-names></string-name> <year>2017</year>. <article-title>Getting ahead of yourself: Parafoveal word expectancy modulates the N400 during sentence reading</article-title>. <source>Cogn Affect Behav Neurosci</source> <volume>17</volume>:<fpage>475</fpage>–<lpage>490</lpage>. doi:<pub-id pub-id-type="doi">10.3758/s13415-016-0492-6</pub-id></mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="other"><collab>Team RC.</collab> <year>2013</year>.<article-title>R: A language and environment for statistical computing</article-title> <volume>201</volume>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><string-name><surname>Tsai</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R</given-names></string-name>, <string-name><surname>Yan</surname> <given-names>M.</given-names></string-name> <year>2012</year>. <article-title>Parafoveal semantic information extraction in traditional Chinese reading</article-title>. <source>Acta Psychol (Amst)</source> <volume>141</volume>:<fpage>17</fpage>–<lpage>23</lpage>. doi:<pub-id pub-id-type="doi">10.1016/J.ACTPSY.2012.06.004</pub-id></mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><string-name><surname>Underwood</surname> <given-names>NR</given-names></string-name>, <string-name><surname>McConkie</surname> <given-names>GW.</given-names></string-name> <year>1985</year>. <article-title>Perceptual Span for Letter Distinctions during Reading</article-title>. <source>Read Res Q</source> <volume>20</volume>:<fpage>153</fpage>. doi:<pub-id pub-id-type="doi">10.2307/747752</pub-id></mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><string-name><surname>Vialatte</surname> <given-names>FB</given-names></string-name>, <string-name><surname>Maurice</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dauwels</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cichocki</surname> <given-names>A.</given-names></string-name> <year>2010</year>. <article-title>Steady-state visually evoked potentials: Focus on essential paradigms and future perspectives</article-title>. <source>Prog Neurobiol</source> <volume>90</volume>:<fpage>418</fpage>–<lpage>438</lpage>. doi:<pub-id pub-id-type="doi">10.1016/J.PNEUROBIO.2009.11.005</pub-id></mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><string-name><surname>Wen</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Snell</surname> <given-names>J</given-names></string-name>, <string-name><surname>Grainger</surname> <given-names>J.</given-names></string-name> <year>2019</year>. <article-title>Parallel, cascaded, interactive processing of words during sentence reading</article-title>. <source>Cognition</source> <volume>189</volume>:<fpage>221</fpage>–<lpage>226</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cognition.2019.04.013</pub-id></mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><string-name><surname>White</surname> <given-names>SJ.</given-names></string-name> <year>2008</year>. <article-title>Eye movement control during reading: Effects of word frequency and orthographic familiarity</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>34</volume>:<fpage>205</fpage>–<lpage>223</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-1523.34.1.205</pub-id></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><string-name><surname>Williams</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Perea</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pollatsek</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K.</given-names></string-name> <year>2006</year>. <article-title>Previewing the neighborhood: The role of orthographic neighbors as parafoveal previews in reading</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>32</volume>:<fpage>1072</fpage>–<lpage>1082</lpage>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><string-name><surname>Wu</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bosker</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Riecke</surname> <given-names>L</given-names></string-name>, <string-name><surname>Nl</surname> <given-names>W.</given-names></string-name> <year>2023</year>. <article-title>Sentential contextual facilitation of auditory word processing builds up during sentence tracking</article-title>. <source>J Cogn Neurosci</source>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><string-name><surname>Yan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Richter</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Shu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R.</given-names></string-name> <year>2009</year>. <article-title>Readers of Chinese extract semantic information from parafoveal words</article-title>. <source>Psychon Bull Rev</source> <volume>16</volume>:<fpage>561</fpage>–<lpage>566</lpage>. doi:<pub-id pub-id-type="doi">10.3758/PBR.16.3.561</pub-id></mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><string-name><surname>Yan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>W</given-names></string-name>, <string-name><surname>Shu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R.</given-names></string-name> <year>2012</year>. <article-title>Lexical and sublexical semantic preview benefits in Chinese reading</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>38</volume>:<fpage>1069</fpage>–<lpage>1075</lpage>. doi:<pub-id pub-id-type="doi">10.1037/A0026935</pub-id></mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><string-name><surname>Zhigalov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Duecker</surname> <given-names>K</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2021</year>. <article-title>The visual cortex produces gamma band echo in response to broadband visual flicker</article-title>. <source>PLOS Comput Biol</source> <volume>17</volume>:<fpage>e1009046</fpage>. doi:<pub-id pub-id-type="doi">10.1371/JOURNAL.PCBI.1009046</pub-id></mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><string-name><surname>Zhigalov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Herring</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Herpers</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bergmann</surname> <given-names>TO</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2019</year>. <article-title>Probing cortical excitability using rapid frequency tagging</article-title>. <source>Neuroimage</source> <volume>195</volume>:<fpage>59</fpage>–<lpage>66</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.056</pub-id></mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="other"><string-name><surname>Zhigalov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2022</year>. <article-title>Travelling waves observed in MEG data can be explained by two discrete sources</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><string-name><surname>Zhigalov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O.</given-names></string-name> <year>2020</year>. <article-title>Alpha oscillations do not implement gain control in early visual cortex but rather gating in parieto-occipital regions</article-title>. <source>Hum Brain Mapp</source> <volume>41</volume>:<fpage>5176</fpage>–<lpage>5186</lpage>. doi:<pub-id pub-id-type="doi">10.1002/hbm.25183</pub-id></mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname> <given-names>W</given-names></string-name>, <string-name><surname>Kliegl</surname> <given-names>R</given-names></string-name>, <string-name><surname>Yan</surname> <given-names>M.</given-names></string-name> <year>2013</year>. <article-title>A validation of parafoveal semantic information extraction in reading Chinese</article-title>. <source>J Res Read</source> <volume>36</volume>:<fpage>S51</fpage>–<lpage>S63</lpage>. doi:<pub-id pub-id-type="doi">10.1111/J.1467-9817.2013.01556.X</pub-id></mixed-citation></ref>
</ref-list>
<sec id="s11">
<title>Supplementary Materials for</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1.</label>
<caption><title>The likelihood of refixation and total gaze duration of eye movement data.</title>
<p>(<bold>A</bold>) The likelihood of refixation into a word was defined as the proportion of trials that have at least one regression from a later part of the sentence back to that word. We found that when the target words were contextually incongruent with the sentence compared with congruent, there was a significantly higher probability of regression into pre-target words (<italic>t</italic>(33) = 7.83, <italic>p</italic> = 5.04 ×10<sup>-9</sup>, <italic>d</italic> = 1.34, two-sided pairwise <italic>t</italic>-test) and target words (<italic>t</italic>(33) = 9.13, <italic>p</italic> = 1.49 ×10<sup>-10</sup>, <italic>d</italic> = 1.57, two-sided pairwise <italic>t</italic>-test). Each dot indicates one participant. (<bold>B</bold>) The total gaze duration was defined as the sum of all fixations on a given word, including those fixations during re-reading. Significantly longer total gaze durations were found for pre-target words (<italic>t</italic>(33) = 5.78, <italic>p</italic> = 1.86 ×10<sup>-6</sup>, <italic>d</italic> = .99, two-sided pairwise <italic>t</italic>-test) and target words (<italic>t</italic>(33) = 10.55, <italic>p</italic> = 4.20 ×10<sup>-12</sup>, <italic>d</italic> = 1.81, two-sided pairwise <italic>t</italic>-test) when the target words were incongruent with the context compared with congruent. ***<italic>p</italic> &lt; .001; n.s., not statistically significant.</p></caption>
<graphic xlink:href="509511v2_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s10">
<title>Appendix</title>
<sec id="s10a">
<title>Experimental sentence set</title>
<sec id="s10a1">
<title>Here we share all 160 sentences embedded with congruent target words</title>
<p>For sentence set version A, we swapped the target words within each pair for sentences 1 to 80 and made them incongruent, while sentences 81 to 160 were kept congruent. For sentence set version B, target words in sentences 1 to 80 were kept the same but target words in sentences 81 to 160 were swapped within each pair to make them incongruent. The sequence of the sentences was shuffled to make sure that no more than 3 sentences in a row were in the same condition. For illustration, the target words are shown in italic type here, but they were in normal type in the experiment. For the 117 filler sentences, please see the Appendix in (<xref ref-type="bibr" rid="c89">White, 2008</xref>).</p>
<list list-type="order">
<list-item><p>Last night, my lazy <italic>brother</italic> came to the party one minute before it was over.</p></list-item>
<list-item><p>Lily says this blue <italic>jacket</italic> will be a big fashion trend this fall.</p></list-item>
<list-item><p>This area has been populated by many <italic>hikers</italic> over the last year.</p></list-item>
<list-item><p>Little Jimmy picked up a box and put some <italic>coins</italic> inside of it.</p></list-item>
<list-item><p>Joey became an avid <italic>student</italic> during his adolescence.</p></list-item>
<list-item><p>He could only afford a cheap <italic>ring</italic> without a diamond for his fiancée.</p></list-item>
<list-item><p>This morning the noisy <italic>kids</italic> played happily in the backyard.</p></list-item>
<list-item><p>My parents had no firm <italic>ideas</italic> about what I should become.</p></list-item>
<list-item><p>The unfortunate pupil lost his beloved <italic>pony</italic> just before his birthday.</p></list-item>
<list-item><p>Experts say that the severe <italic>crisis</italic> will cause oil prices to triple.</p></list-item>
<list-item><p>The construction of this ancient <italic>castle</italic> cost a lot of money.</p></list-item>
<list-item><p>After the meeting, the anxious <italic>worker</italic> sighed in the hallway.</p></list-item>
<list-item><p>Peter’s love for this sporting <italic>match</italic> inspired all his friends.</p></list-item>
<list-item><p>We could see from her torn <italic>collar</italic> that she had been in a fight.</p></list-item>
<list-item><p>With the help of his clever <italic>friend</italic> Jack, he made the first pot of gold.</p></list-item>
<list-item><p>I always like to order a filling <italic>burger</italic> from the local pub.</p></list-item>
<list-item><p>She looked at the tired <italic>fireman</italic> with a satisfied smile.</p></list-item>
<list-item><p>He felt relieved after completing the complex <italic>scan</italic> within an hour.</p></list-item>
<list-item><p>Scientists found a steep <italic>boulder</italic> sitting in the middle of the canyon.</p></list-item>
<list-item><p>Last week his friendly <italic>cousin</italic> passed out for no apparent reason.</p></list-item>
<list-item><p>They asked the selfish <italic>maid</italic> where her huge sums of money came from.</p></list-item>
<list-item><p>It took Tom a month to mend the broken <italic>roof</italic> all by himself.</p></list-item>
<list-item><p>Under stress, the crafty <italic>boss</italic> promised customers a full refund.</p></list-item>
<list-item><p>The cowboys hung the stolen <italic>rifles</italic> high up on the wall.</p></list-item>
<list-item><p>She submitted the crucial <italic>file</italic> that can prove her innocence.</p></list-item>
<list-item><p>According to history books, the proud <italic>queen</italic> never accepted any criticism.</p></list-item>
<list-item><p>Ana complained that the tall <italic>herbs</italic> behind the house had dried up.</p></list-item>
<list-item><p>He failed in his chosen <italic>sport</italic> with hopes of success fading with each effort.</p></list-item>
<list-item><p>She said that the corrupt <italic>company</italic> offered high salaries to young graduates.</p></list-item>
<list-item><p>In the last few years, the thick <italic>bush</italic> died back dramatically.</p></list-item>
<list-item><p>It turned out that the last-minute <italic>trip</italic> lasted for six hours.</p></list-item>
<list-item><p>Linda found that the slender <italic>tree</italic> dead from a pest infestation.</p></list-item>
<list-item><p>Suddenly, the warm <italic>coffee</italic> stained his brand new shirt.</p></list-item>
<list-item><p>Plenty of rain will make the vivid <italic>flower</italic> blossom well.</p></list-item>
<list-item><p>Jack became a humble <italic>chef</italic> specializing in French cuisine.</p></list-item>
<list-item><p>The young man’s shiny <italic>vehicle</italic> vanished slowly out of sight.</p></list-item>
<list-item><p>To the north, the steep <italic>hills</italic> stretched for many miles.</p></list-item>
<list-item><p>Before sleeping, the nervous <italic>colonel</italic> smoked a cigarette.</p></list-item>
<list-item><p>Decades ago, that algae-covered <italic>pond</italic> was enough to irrigate the crops.</p></list-item>
<list-item><p>He saw the smart <italic>player</italic> throw the ball, causing chaos among the opposition.</p></list-item>
<list-item><p>In recent days, the cruel <italic>murder</italic> has scared citizens from going out.</p></list-item>
<list-item><p>Mary told me that the light <italic>cream</italic> was low in fat but hard to whip.</p></list-item>
<list-item><p>News said that the painful <italic>disease</italic> would continue to affect many children.</p></list-item>
<list-item><p>The boy found his lost <italic>ball</italic> under the tree and stopped crying at once.</p></list-item>
<list-item><p>Politicians hated the brief <italic>report</italic> criticizing the government’s incompetence.</p></list-item>
<list-item><p>My favourite gift is the shiny <italic>drone</italic> from my dad last year.</p></list-item>
<list-item><p>Every year, the sandy <italic>shore</italic> attracts thousands of tourists.</p></list-item>
<list-item><p>After taking a deep breath, the junior <italic>officer</italic> entered the room.</p></list-item>
<list-item><p>Last week, the caring <italic>family</italic> rescued a stray dog and kept it as a pet.</p></list-item>
<list-item><p>During the air raid, the spacious public <italic>plaza</italic> happened to be ruined.</p></list-item>
<list-item><p>With his sharp criticism, the young <italic>actor</italic> annoyed his agent as usual.</p></list-item>
<list-item><p>Laura was told that the sudden <italic>storm</italic> delayed the bus for two days.</p></list-item>
<list-item><p>Lily said that the vacant <italic>cottage</italic> belonged to her grandparents.</p></list-item>
<list-item><p>In the small house, a comic <italic>picture</italic> adorned the reception room.</p></list-item>
<list-item><p>Facing the lion, the brave <italic>hunter</italic> showed no fear.</p></list-item>
<list-item><p>Out of repair, the rattling <italic>engine</italic> was about to be scrapped.</p></list-item>
<list-item><p>Jack had to admit that this planned <italic>visit</italic> turned out to be embarrassing.</p></list-item>
<list-item><p>Tom admired the way his devoted <italic>aunt</italic> always volunteers on weekends.</p></list-item>
<list-item><p>Rob felt that the brief <italic>letter</italic> from his wife expressed a hint of sadness.</p></list-item>
<list-item><p>Alone at home, the tired <italic>clerk</italic> cooked a beef patty.</p></list-item>
<list-item><p>After the surgery, Rob’s poor <italic>health</italic> left him barely able to get out of bed.</p></list-item>
<list-item><p>Sam’s train arrived before <italic>dusk</italic> and we were able to give him a ride home.</p></list-item>
<list-item><p>She gave the dog a quick <italic>bath</italic> after they came back from the outside.</p></list-item>
<list-item><p>Michael made a mean <italic>joke</italic> about Boris Johnson’s hair.</p></list-item>
<list-item><p>They didn’t realize the harsh <italic>impact</italic> that their products could have.</p></list-item>
<list-item><p>In the museum, we saw the golden <italic>crown</italic> that belonged to the first king.</p></list-item>
<list-item><p>Bill is a superb <italic>partner</italic> because he is easy to get along with.</p></list-item>
<list-item><p>The explorer made his way through the gloomy <italic>night</italic> with a small torch.</p></list-item>
<list-item><p>The TV show was an obvious <italic>flop</italic> after the actress joined the cast.</p></list-item>
<list-item><p>On rainy days, the careful <italic>lady</italic> reminded herself to go slowly.</p></list-item>
<list-item><p>Jane complained that her white <italic>kitten</italic> hadn’t come home for two days.</p></list-item>
<list-item><p>I guess no one can solve the hard <italic>problem</italic> without outside help.</p></list-item>
<list-item><p>The new event was such a huge <italic>failure</italic> that people kept talking about it.</p></list-item>
<list-item><p>Before committing the crime, the anxious <italic>suspect</italic> drank a lot of alcohol.</p></list-item>
<list-item><p>Toby kept his money in a small <italic>shed</italic> because he lived on a farm.</p></list-item>
<list-item><p>They noticed the young <italic>deer</italic> eating acorns in the forest.</p></list-item>
<list-item><p>Amy wanted some more of the sliced <italic>pear</italic> for afternoon snack.</p></list-item>
<list-item><p>Laura went down to the narrow <italic>canal</italic> to watch the boats.</p></list-item>
<list-item><p>I wondered if the noisy <italic>club</italic> would be a good place for the bachelorette party.</p></list-item>
<list-item><p>Tara always has an acute <italic>pain</italic> in her tooth after eating ice cream.</p></list-item>
<list-item><p>Before the war, the brave <italic>general</italic> assembled an army.</p></list-item>
<list-item><p>She began to slice up a large <italic>potato</italic> for the dinner.</p></list-item>
<list-item><p>The child had a large <italic>face</italic> with big, expressive eyes.</p></list-item>
<list-item><p>Last night, there was a dense <italic>mist</italic> when they left the cinema.</p></list-item>
<list-item><p>Marla enjoyed seeing the chubby <italic>cats</italic> playing with each other.</p></list-item>
<list-item><p>Jim entered the giant <italic>court</italic> to try out for the basketball team.</p></list-item>
<list-item><p>They visited the antique <italic>chapel</italic> before booking their wedding.</p></list-item>
<list-item><p>After the defeat, the crazy <italic>fans</italic> kept cursing and crying.</p></list-item>
<list-item><p>She approached the rusty <italic>gate</italic> before realizing it was locked.</p></list-item>
<list-item><p>Many people like to eat crispy <italic>toast</italic> with their morning coffee at breakfast.</p></list-item>
<list-item><p>They stepped into the messy <italic>garage</italic> that had high wooden shelves.</p></list-item>
<list-item><p>We watched the large hungry <italic>hawk</italic> swoop down to get the poor chicken.</p></list-item>
<list-item><p>Alexandra used a short <italic>hammer</italic> when she created the stone statue.</p></list-item>
<list-item><p>Ruth visited the public <italic>museum</italic> that she had read about all these years.</p></list-item>
<list-item><p>We’d better buy some tasty <italic>chips</italic> before we watch the big game.</p></list-item>
<list-item><p>Historians believe the rousing <italic>speech</italic> heralded the start of the revolution.</p></list-item>
<list-item><p>Under the tree, there is a little <italic>hare</italic> running happily.</p></list-item>
<list-item><p>In the darkness, only the misty <italic>moon</italic> lit up the street.</p></list-item>
<list-item><p>The prince inherited the supreme <italic>power</italic> from the late king.</p></list-item>
<list-item><p>Look over there, a fluffy <italic>sheep</italic> seems to be lost.</p></list-item>
<list-item><p>The man was a young <italic>teacher</italic> who always worked late into the night.</p></list-item>
<list-item><p>As for this scandal, Jo has a clear <italic>opinion</italic> but she won’t say it.</p></list-item>
<list-item><p>They had no idea that the blue <italic>liquid</italic> shrinks all woollen clothes.</p></list-item>
<list-item><p>The report was sent to the honest <italic>justice</italic> three days before the trial.</p></list-item>
<list-item><p>Every night, this tired <italic>captain</italic> drank wine before going to sleep.</p></list-item>
<list-item><p>The shopkeeper said the metal <italic>bottle</italic> would sell well this year.</p></list-item>
<list-item><p>For the locals, the salt <italic>lake</italic> triggered a political issue.</p></list-item>
<list-item><p>Near the small brook, a hungry <italic>animal</italic> hunts quietly for hours.</p></list-item>
<list-item><p>Every night, this deep <italic>secret</italic> makes the pianist toss and turn.</p></list-item>
<list-item><p>In the lab, a young <italic>surgeon</italic> examined the victim’s body.</p></list-item>
<list-item><p>Sadly, the lonely <italic>poet</italic> died before he could finish his last poem.</p></list-item>
<list-item><p>Due to the moist weather, the wheat <italic>flour</italic> became mouldy quickly.</p></list-item>
<list-item><p>Sue’s colleagues say that her warm <italic>heart</italic> makes everyone like her.</p></list-item>
<list-item><p>On the wall, the small green <italic>screen</italic> shows the room temperature precisely.</p></list-item>
<list-item><p>To his surprise, the yummy <italic>dish</italic> was not expensive.</p></list-item>
<list-item><p>Eventually, the greedy <italic>nanny</italic> disclosed all the details about this affair.</p></list-item>
<list-item><p>The sight of the cotton <italic>factory</italic> was something to behold.</p></list-item>
<list-item><p>It was obvious that the weak <italic>patient</italic> was getting weaker day by day.</p></list-item>
<list-item><p>In the past month alone, the gentle <italic>scholar</italic> published five papers.</p></list-item>
<list-item><p>The filthy and rusty <italic>pots</italic> made the food taste terrible.</p></list-item>
<list-item><p>Just after dawn, an armed <italic>ship</italic> approached the pretty lagoon slowly.</p></list-item>
<list-item><p>At last, she found the wool <italic>skirt</italic> hanging in the wardrobe.</p></list-item>
<list-item><p>Villagers said that the newly built <italic>school</italic> was well equipped.</p></list-item>
<list-item><p>In the downtown market, the agitated <italic>crowd</italic> began the parade.</p></list-item>
<list-item><p>Suzy really likes eating <italic>sugar</italic> because she wasn’t allowed to eat it as a kid.</p></list-item>
<list-item><p>Ali said he really enjoyed modern <italic>music</italic> when he was at college.</p></list-item>
<list-item><p>Tina wants a spacious <italic>yard</italic> because she likes to lie on the grass and read.</p></list-item>
<list-item><p>In an open field, the violent <italic>chief</italic> executed prisoners with a gun.</p></list-item>
<list-item><p>After working overtime for a month, the wronged <italic>manager</italic> wanted to jump ship.</p></list-item>
<list-item><p>In the Stone Age, the spiny <italic>grass</italic> prevailed over the land.</p></list-item>
<list-item><p>The sparrow was being chased by some fluffy <italic>hens</italic> under the hot sun.</p></list-item>
<list-item><p>When the ball was scored, they tapped their <italic>cups</italic> to show their joy.</p></list-item>
<list-item><p>They recorded the details of the stolen <italic>cars</italic> carefully on a spreadsheet.</p></list-item>
<list-item><p>The poor boy stood in the snow with bruised <italic>legs</italic> and cried sadly.</p></list-item>
<list-item><p>Little Roy likes to play with the plastic <italic>bricks</italic> at the Lego store.</p></list-item>
<list-item><p>It was said that the honest <italic>lawyer</italic> convened the committee meeting.</p></list-item>
<list-item><p>I learned about the muddy <italic>trail</italic> through a friend on the last hike.</p></list-item>
<list-item><p>Just now, the calm <italic>jury</italic> delivered a guilty verdict in this notorious case.</p></list-item>
<list-item><p>The holiday was neglected by this busy <italic>parent</italic> but her son was used to it.</p></list-item>
<list-item><p>This widely spread <italic>story</italic> reflected the distortion of human nature.</p></list-item>
<list-item><p>We are meeting at the newly built <italic>airport</italic> tonight for our trip to Europe.</p></list-item>
<list-item><p>Every day before leaving work, the tall <italic>editor</italic> cleans her desk.</p></list-item>
<list-item><p>Nobody knew when the excited <italic>puppy</italic> urinated on the floor.</p></list-item>
<list-item><p>Everyone knows that entire <italic>area</italic> has restricted access.</p></list-item>
<list-item><p>The man’s cunning <italic>excuse</italic> relieved him of the fine.</p></list-item>
<list-item><p>Roy repaired the broken <italic>truck</italic> over the weekend.</p></list-item>
<list-item><p>Ana was glad that the gentle <italic>nurse</italic> said her little boy was out of danger.</p></list-item>
<list-item><p>In the company, the annual <italic>meeting</italic> marks the end of a year’s hard work.</p></list-item>
<list-item><p>He carefully placed the sharp <italic>sword</italic> down after the fight.</p></list-item>
<list-item><p>She found an empty <italic>desk</italic> where she could put her computer.</p></list-item>
<list-item><p>They danced a slow <italic>tango</italic> together after dinner.</p></list-item>
<list-item><p>Steph noticed a torn <italic>note</italic> and looked for the other half.</p></list-item>
<list-item><p>Mindy’s dog has a strange <italic>smell</italic> and likes to bark a lot.</p></list-item>
<list-item><p>Patty likes to cut some pink <italic>tape</italic> to decorate her notebooks.</p></list-item>
<list-item><p>We could hear the angry <italic>priest</italic> shouting at the little girl.</p></list-item>
<list-item><p>David was happy to receive a nice <italic>card</italic> from his daughter at Christmas.</p></list-item>
<list-item><p>She always meets the same happy <italic>couple</italic> when she walks in the park.</p></list-item>
<list-item><p>The stray dog lives in a hidden <italic>hole</italic> that protects it from the cold weather.</p></list-item>
<list-item><p>Jack failed to submit his concise <italic>paper</italic> before the deadline.</p></list-item>
<list-item><p>I have heard that the young <italic>baker</italic> makes the best baguettes in town.</p></list-item>
</list>
</sec>
</sec>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c99"><mixed-citation publication-type="journal"><string-name><surname>White</surname> <given-names>SJ.</given-names></string-name> <year>2008</year>. <article-title>Eye movement control during reading: Effects of word frequency and orthographic familiarity</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>34</volume>:<fpage>205</fpage>–<lpage>223</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-1523.34.1.205</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91327.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
<city>Hangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study provides an <bold>important</bold> contribution to understanding how parafoveal words are neurally processed. The study employs a state-of-the-art frequency tagging paradigm to study the MEG response to words during natural reading. It provides <bold>solid</bold> evidence that semantic information of parafoveal words can be extracted.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91327.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors' primary research question revolves around the inquiry of &quot;how far in advance semantic information might become available from parafoveal preview.&quot; In contrast to prior studies, the current research seeks to achieve a breakthrough in terms of timing by employing innovative technology. They mention in the manuscript that &quot;most of these studies have been limited to measuring parafoveal preview from fixations to an immediately adjacent word... We tackle these core issues using a new technique that combines the use of frequency tagging and the measurement of magnetoencephalography (MEG)-based signals.&quot; However, the argumentation for how this new technology constitutes a breakthrough is not sufficiently substantiated. Specifically, there are two aspects that require further clarification. Firstly, the authors should clarify the importance of investigating the timing of semantic integration in their research question. They need to justify why previous studies focusing on the preview effect during fixations to an immediately adjacent word cannot address their specific inquiry about &quot;how far in advance semantic information might become available from parafoveal preview,&quot; which requires examining parafoveal processing (POF). Secondly, in terms of the research methodology, the authors should provide a more comprehensive explanation of the advantages offered by MEG technology in the observation of the timing of semantic integration compared to the techniques employed in prior research. Indeed, the authors have overlooked some rather significant studies in this area. For instance, the research conducted by Antúnez, Milligan, Hernández-Cabrera, Barber, &amp; Schotter in 2022 addresses the same research question mentioned in the current study and employs a similar experimental design. Importantly, they utilize a natural reading paradigm with synchronized ERP and eye-tracking recordings. Collectively, these studies, along with the series of prior research studies employing ERP techniques and RSVP paradigms discussed by the authors in their manuscript, provide ample evidence that semantic information becomes available and integrated from words before fixation occurs. Therefore, the authors should provide a more comprehensive citation of relevant research and delve deeper into explaining the potential contributions of their chosen technology to this field.</p>
<p>Further, the authors emphasize semantic integration in their observed results but overlook the intricate relationship between access, priming, and integration. This assertion appears overly confident. Despite using low-constraint sentences and low-predicted targets (lines 439-441), differences between congruent and incongruent conditions may be influenced by word-level factors. For instance, in the first coherent sentence, such as &quot;Last night, my lazy brother came to the party one minute before it was over&quot; (line 1049), replacing the keyword &quot;brother&quot; with an incongruent word could create an incoherent sentence, possibly due to semantic violation, relation mismatch with &quot;lazy,&quot; or prediction error related to animate objects. A similar consideration applies to the second example sentence, &quot;Lily says this blue jacket will be a big fashion trend this fall&quot; (line 1050), where the effect might result from a discrepancy between &quot;blue&quot; and an incongruent word. However, the authors do not provide incongruent sentences to substantiate their claims. I recommend that the authors discuss alternative explanations and potentially control for confounding factors before asserting that their results unequivocally reflect semantic integration. My intention is not to dispute the semantic integration interpretation but to stress the necessity for stronger evidence to support this assertion.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.91327.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This MEG study used co-registered eye-tracking and Rapid Invisible Frequency Tagging (RIFT) to track the effects of semantic parafoveal preview during natural sentence reading. Unpredictable target words could either be congruent or incongruent with sentence context. This modulated the RIFT response already while participants were fixating on the preceding word. This indicates that the semantic congruency of the upcoming word modulates visual attention demands already in parafoveal preview.</p>
<p>
The quest for semantic parafoveal preview in natural reading has attracted a lot of attention in recent years, especially with the development of co-registered EEG and MEG. Evidence from dynamic neuroimaging methods using innovative paradigms as in this study is important for this debate.</p>
<p>Major points:</p>
<p>1. The authors frame their study in terms of &quot;congruency with sentence context&quot;. However, it is the congruency between adjective-noun pairs that determines congruency (e.g. &quot;blue brother&quot; vs &quot;blue jacket&quot;, and examples p. 16 and appendix). This is confirmed by Suppl Figure 1, which shows a significantly larger likelihood of refixations to the pre-target word for incongruent sentences, probably because the pre-target word is most diagnostic for the congruency of the target word. The authors discuss some possibilities as to why there is variability in parafoveal preview effects in the literature. It is more likely to see effects for this simple and local congruency, rather than congruency that requires an integration and comprehension of the full sentence. I'm not sure whether the authors really needed to present their stimuli in a full-sentence context to obtain these effects. This should be explicitly discussed and also mentioned in the introduction (or even the abstract).</p>
<p>2. The authors used MEG and provided a source estimate for the tagging response (Figure 2), which unsurprisingly is in the visual cortex. The most important results are presented at the sensor level. This does not add information about the brain sources of the congruency effect, as the RIFT response probably reflects top-down effects on visual attention etc. Was it necessary to use MEG? Would EEG have produced the same results? In terms of sensitivity, EEG is better than MEG as it is more sensitive to radial and deeper sources. This should be mentioned in the discussion and/or methods section.</p>
<p>3. The earliest semantic preview effects occurred around 100ms after fixating the pre-target word (discussed around l. 323). This means that at this stage the brain must have processed the pre-target and the target word and integrated their meanings (at some level). Even in the single-word literature, semantic effects at 100 ms are provocatively early. Even studies that tried to determine the earliest semantic effects arrived at around 200 ms (e.g. (<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3382728/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3382728/</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org/record/2013-17451-002">https://psycnet.apa.org/record/2013-17451-002</ext-link>). The present results need to be discussed in a bit more detail in the context of the visual word recognition literature.</p>
<p>4. As in previous EEG/MEG studies, the authors found a neural but no behavioural preview effect. As before, this raises the question of whether the observed effect is really &quot;critical&quot; for sentence comprehension. The authors provide a correlation analysis with reading speed, but this does not allow causal conclusions: Some people may simply read slowly and therefore pay more attention and get a larger preview response. Some readers may hurry and therefore not pay attention and not get a preview response. In order to address this, one would have to control for reading speed and show an effect of RIFT response on comprehension performance (or vice versa, with a task that is not close to ceiling performance). The last sentence of the discussion is currently not justified by the results.</p>
<p>5. L. 577f.: ICA components were selected by visual inspection. I would strongly recommend including EOG in future recordings when the control of eye movements is critical.</p>
<p>6. The authors mention &quot;saccade planning&quot; a few times. I would suggest looking at the SWIFT model of eye movement control, which is less mechanistic than the dominant EZ-Reader model (<ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org/record/2005-13637-003">https://psycnet.apa.org/record/2005-13637-003</ext-link>). It may be useful for the framing of the study and interpretation of the results (e.g. second paragraph of discussion).</p>
</body>
</sub-article>
</article>