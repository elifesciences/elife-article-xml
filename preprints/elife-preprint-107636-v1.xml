<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107636</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107636</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107636.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>The representation of facial emotion expands from sensory to prefrontal cortex with development</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8115-8621</contrib-id>
<name>
<surname>Fan</surname>
<given-names>Xiaoxu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tripathi</surname>
<given-names>Abhishek</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1624-8767</contrib-id>
<name>
<surname>Bijanki</surname>
<given-names>Kelly R</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>bijanki@bcm.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Baylor College of Medicine</institution></institution-wrap>, <city>Houston</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap>, <city>Houston</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Hu</surname>
<given-names>Xiaoqing</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Hong Kong</institution>
</institution-wrap>
<city>Hong Kong</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-08-07">
<day>07</day>
<month>08</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107636</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-05-23">
<day>23</day>
<month>05</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-05-23">
<day>23</day>
<month>05</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.05.23.655726"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Fan et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Fan et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107636-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Facial expression recognition develops rapidly during infancy and improves from childhood to adulthood. As a critical component of social communication, this skill enables individuals to interpret others’ emotions and intentions. However, the brain mechanisms driving the development of this skill remain largely unclear due to the difficulty of obtaining data with both high spatial and temporal resolution from young children. By analyzing intracranial EEG data collected from childhood (5-10 years old) and post-childhood groups (13-55 years old), we find differential involvement of high-level brain areas in processing facial expression information. For the post-childhood group, both the posterior superior temporal cortex (pSTC) and the dorsolateral prefrontal cortex (DLPFC) encode facial emotion features from a high-dimensional, continuous space. However, in children, the facial expression information is only significantly represented in the pSTC, not in the DLPFC. Further, the encoding of complex emotions in pSTC is shown to increase with age. Taken together, these data suggest that young children rely more on low-level sensory areas than on the prefrontal cortex for facial emotion processing, leading us to hypothesize that top-down modulation from prefrontal cortex to pSTC gradually matures during development to enable a full understanding of facial emotions, especially complex emotions which need social and life experience to comprehend.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Understanding others’ emotional states through their facial expressions is an important aspect of effective social interactions throughout the lifespan. Behavioral data suggest that facial emotion processing emerges very early in life<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup>, as infants just months old can distinguish happy and sad faces from surprised faces<sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c5">5</xref></sup>. However, children’s emotion recognition is substantially less accurate than adults, and this ability prominently improves across childhood and adolescence<sup><xref ref-type="bibr" rid="c6">6</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>. Although extensive research in cognitive and affective neuroscience has assessed developmental changes using behavioral and non-invasive neuroimaging approaches, our understanding of brain development related to facial expression perception remains limited.</p><p>One influential perspective on the development of face recognition is that it depends on the maturation of face-selective brain regions, including the fusiform face area (FFA), occipital face area (OFA), and posterior superior temporal sulcus (pSTS)<sup><xref ref-type="bibr" rid="c12">12</xref></sup>. Supporting this view, Gomez, et al. found evidence for microstructural proliferation in the fusiform gyrus during childhood, suggesting that improvements in face recognition are a product of an interplay between structural and functional changes in the cortex<sup><xref ref-type="bibr" rid="c13">13</xref></sup>. Additionally, monkeys raised without exposure to faces fail to develop normal face-selective patches, suggesting that face experience is necessary for the development of the face-processing network<sup><xref ref-type="bibr" rid="c14">14</xref></sup>. It is likely that the gradual maturation of pSTS and FFA, two early sensory areas involved in the processing of facial expressions<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup>, contributes to the improved facial expression recognition over development. Yet, few studies have investigated the development of neural representation of emotional facial expressions in FFA and pSTS from early childhood to adulthood in human.</p>
<p>Besides the visual processing of facial configurations, understanding the emotional meaning of faces requires the awareness and interpretation of the emotional state of the other person, which is significantly shaped by life experience<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup>. Thus, some researchers have proposed that the maturation of emotional information processing is related to the progressive increase in functional activity in the prefrontal cortex <sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. With development, greater engagement of the prefrontal cortex may facilitate top-down modulation of activity in more primitive subcortical and limbic regions, such as the amygdala<sup><xref ref-type="bibr" rid="c21">21</xref>–<xref ref-type="bibr" rid="c23">23</xref></sup>. Despite these theoretical advances, the functional changes in the prefrontal cortex during the perceptual processing of emotional facial expressions over development remains largely unknown.</p>
<p>Here, we analyze intracranial EEG (iEEG) data collected from childhood (5-10 years old) and post-childhood groups (13-55 years-old) while participants were watching a short audiovisual film. In our results, children’s dorsolateral prefrontal cortex (DLPFC) shows minimal involvement in processing facial expression, unlike the post-childhood group. In contrast, for both children and post-childhood individuals, facial expression information is encoded in the pSTC, a brain region that contributes to the perceptual processing of facial expressions. Furthermore, the encoding of complex emotions in the pSTC increases with age. These neuroimaging data imply that social and emotional experiences shape the prefrontal cortex’s involvement in processing the emotional meaning of faces throughout development, probably through top-down modulation of early sensory areas.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Using AI and encoding models to study the neural representation of facial expression</title>
<p>In this study, we analyzed intracranial EEG (iEEG) data collected from a large group of human neurosurgical patients while they watched a short audiovisual film at the University Medical Center Utrecht<sup><xref ref-type="bibr" rid="c24">24</xref></sup>. The movie consisted of 13 interleaved blocks of videos accompanied by speech or music, 30 seconds each (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). To characterize the neural representation of facial expression in the prefrontal cortex and low-level sensory areas across development, we analyzed iEEG data from 9 children (5-10 years old) and 31 post-childhood individuals (13-55 years old) who have electrode coverage in DLPFC, pSTC or both. First, Hume AI facial expression models were used to continuously extract facial emotion features from the movie (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Then, we tested how well encoding models constructed from the 48 facial emotion features (e.g., fear, joy) predict cortical high-frequency band (HFB) activity (110-140 Hz) induced by the presented movie (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). The model performance was quantified as the correlation between the predicted and actual HFB activities, which is also called prediction accuracy.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Task design and analysis methods.</title>
<p>(<bold>A</bold>) Movie structure. A 6.5-minute short film was created by editing fragments from <italic>Pippi on the Run</italic> into a coherent narrative. The movie consisted of 13 interleaved blocks of videos accompanied by speech or music. (<bold>B</bold>) Data analysis schematic. Standard analysis pipeline for extracting emotion features from the movie and constructing encoding model to predict iEEG responses while participants watching the short film.</p></caption>
<graphic xlink:href="655726v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<title>Differential representation of facial expression in children’s DLPFC</title>
<p>Using the analysis approach described above, we examined how facial emotion information is represented by DLPFC (<xref rid="fig2" ref-type="fig">Figure 2A</xref>) while watching videos accompanied by speech (i.e. speech condition) in childhood and post-childhood groups. The prediction accuracy of the encoding model was significantly greater than zero in the post-childhood group (<xref rid="fig2" ref-type="fig">Figure 2B</xref>, <italic>P</italic>=0.0096, two-tailed permutation test), suggesting that the neural responses in DLPFC were dynamically modulated by the facial emotion features from the movie. However, facial emotion features were not encoded in children’s DLPFC (<xref rid="fig2" ref-type="fig">Figure 2B</xref>, <italic>P</italic>=0.825, two-tailed permutation test). Moreover, the prediction accuracy in children’s DLPFC was significantly lower than in the post-childhood group (<italic>P</italic>=0.0114, two-tailed permutation test). These findings show that the DLPFC dynamically encodes facial expression information in post-childhood individuals but not in young children.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Prediction performance of encoding models in DLPFC.</title>
<p>(<bold>A</bold>) Spatial distribution of electrodes in DLPFC. Electrodes in all participants from each group are projected onto MNI space and shown on the average brain. Red shaded areas indicate middle frontal cortex provided by the FreeSurfer Desikan-Killiany atlas25. Electrodes outside DLPFC are not shown. (<bold>B</bold>) The average prediction accuracy across participants for speech condition. The performance of encoding model is measured as Pearson correlation coefficient (r) between measured and predicted brain activities. (<bold>C</bold>) Prediction accuracy difference between speech condition and music condition for each group. Error bars are standard error of the mean. *<italic>P</italic>&lt;0.05; **<italic>P</italic>&lt;0.01.</p></caption>
<graphic xlink:href="655726v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To further understand the functional development of children’s DLPFC, we compared the effect of human voice on the representation of facial expression in DLPFC between the two groups. The effect of human voice was quantified as difference in prediction accuracy between the speech and music conditions. Our results showed that human voice influences facial expression representation in the DLPFC differently across development (<xref rid="fig2" ref-type="fig">Figure 2C</xref>, <italic>P</italic>=0.0034, two-tailed permutation test). The presence of human voice enhances facial expression representation in the DLPFC of post-childhood individuals but impairs it in children.</p>
<p>Taken together, there are significant developmental changes in DLPFC’s involvement in facial expression perception.</p>
</sec>
<sec id="s2c">
<title>The neural representation of facial expression in young children’s pSTC</title>
<p>After identifying developmental differences in the involvement of high-level brain areas in processing facial expression, we next examined the neural representation of facial expression in children’s early sensory areas. As an area in the core face network, posterior superior temporal sulcus (pSTS) has been associated with early stages of facial expression processing stream<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>. Although previous studies suggested that the development of facial recognition depends on the maturation of face-selective brain regions<sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref></sup>, it is still unclear how facial expression information is encoded in children’s pSTS. Here, we examined the performance of the facial expression encoding model in a rare sample of two children (S19:8-year-old and S39: 5-year-old) with electrode coverage in pSTC (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). In both cases, the encoding model significantly predicts the HFB neural signals in the pSTC under the speech condition (<xref rid="fig3" ref-type="fig">Figure 3B</xref>, S19<sub>speech</sub>: <italic>P</italic>=0.0014, r=0.1951; S39<sub>speech</sub>: <italic>P</italic>=0.0183, r=0.15). The prediction accuracy is reduced when human voice is absent from the video (S19<sub>music</sub>: <italic>P</italic>=0.0313, r=0.1674; S39<sub>music</sub>: <italic>P</italic>=0.3688, r=0.0574). Similarly, group-level results showed that the model performance is significantly greater than zero in the pSTC of post-childhood individuals (N=25, <xref rid="fig3" ref-type="fig">Figure 3C</xref> and <xref ref-type="fig" rid="fig3">3D</xref>, <italic>P</italic>=0.003, two-tailed permutation test) and this neural representation of facial expression information is significantly reduced when human voice is absent (paired-t-test, t<sub>24</sub>=2.897,<italic>P</italic>=0.0079). These results provide evidence that children’s sensory areas encode facial emotion features from a high-dimensional, continuous space in a manner similar to that of post-childhood individuals.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Prediction performance of encoding models in pSTC.</title>
<p>(<bold>A</bold>) The electrode distribution of two children (s19 and s39). Electrodes in pSTC are green. (<bold>B</bold>) Prediction accuracy of encoding models in the two children. (<bold>C</bold>) Spatial distribution of recording contacts in post-childhood participants’ pSTC. The pSTC electrodes identified in individual space are projected onto MNI space and shown on the average brain. Contacts other than pSTC are not shown. Blue shaded areas indicate superior temporal cortex provided by the FreeSurfer Desikan-Killiany atlas<sup><xref ref-type="bibr" rid="c25">25</xref></sup>. (<bold>D</bold>) Average prediction accuracy across post-childhood participants. Error bars are standard error of the mean. **<italic>P</italic>&lt;0.01.</p></caption>
<graphic xlink:href="655726v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>The complexity of facial expression encoding in the pSTC increases across development</title>
<p>To understand how facial expression representation in pSTC changes across development, we examined the feature weights of the facial expression encoding models in all participants with significant prediction accuracy (10 post-childhood individuals and 2 children). The weight for each feature represents its relative contribution to predicting the neural response. First, we calculated the encoding weights for complex emotions (averaging guilt, embarrassment, pride, and envy, which were selected as the most representative complex emotions based on previous studies<sup><xref ref-type="bibr" rid="c28">28</xref>–<xref ref-type="bibr" rid="c30">30</xref></sup>) and basic emotions (averaging joy, sadness, fear, anger, disgust, and surprise). Then, we calculated their correlations with age separately. Our results showed that the encoding weight of complex emotion was significantly positively correlated with age (r<sub>12</sub>=0.8512,<italic>P</italic>=0.004, <xref rid="fig4" ref-type="fig">Figure 4A</xref> left). No significant correlation between encoding weight of basic emotion and age was observed (r<sub>12</sub>=0.3913,<italic>P</italic>=0.2085, <xref rid="fig4" ref-type="fig">Figure 4A</xref> right). In addition, we computed Pearson correlations between each individual feature weight and age, ranking the r values from largest to smallest (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). The highest correlations were found for embarrassment, guilt, pride, interest, and envy—emotions that are all considered complex emotion. Among them, the weights for embarrassment, guilt, pride, and interest showed significant positive correlations with age (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, embarrassment: r=0.7666, <italic>P</italic>=0.0036; pride: r=0.6773, <italic>P</italic>=0.0155; guilt: r=0.6421, <italic>P</italic>=0.0244, interest: r=0.6377, <italic>P</italic>=0.0257, uncorrected for multiple comparisons), suggesting that the encoding of these complex emotions in pSTC increases with age. Thus, our results suggest that as development progresses, the pSTC becomes increasingly engaged in encoding complex emotions which requires representing others’ mental states and emerges later in development <sup><xref ref-type="bibr" rid="c31">31</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup>.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Correlation between encoding weights and age.</title>
<p>(<bold>A</bold>) Left: Correlation between averaged encoding weights of five complex emotions and age. Right: Correlation between averaged encoding weights of six basic emotions and age. (<bold>B</bold>) Pearson correlation coefficient between encoding weights of 48 facial expression features and age. The results are ranked from largest to smallest. Significant correlations noted with *(<italic>P</italic>&lt;0.05, uncorrected) or **(<italic>P</italic>&lt;0.01, uncorrected). (<bold>C</bold>) Correlation between encoding weights of embarrassment, pride, guilt, interest and age (N=12).</p></caption>
<graphic xlink:href="655726v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Methods</title>
<p>In this study, iEEG data from an open multimodal iEEG-fMRI dataset were analyzed<sup><xref ref-type="bibr" rid="c24">24</xref></sup>.</p>
<sec id="s3a">
<title>Participants and electrode distribution</title>
<p>Due to the research purposes of the current study, only participants who had at least four electrode contacts in either DLPFC or pSTC were included in the data analysis (<xref rid="tbl1" ref-type="table">Table 1</xref> and <xref rid="tbl2" ref-type="table">Table 2</xref>). Nine children (5-10 years old, 5 females) and thirty-one post-childhood individuals (13-55 years old, 18 females) are included in the present study. In the childhood group, eight participants had enough electrodes implanted in the DLPFC, and two had enough electrodes implanted in the pSTC. In the post-childhood group, thirteen participants had enough electrodes implanted in the DLPFC, and twenty-five had enough electrodes implanted in the pSTC.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Demographic information of childhood group.</title></caption>
<graphic xlink:href="655726v1_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Demographic information of post-childhood group.</title></caption>
<graphic xlink:href="655726v1_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s3b">
<title>Experimental procedures</title>
<p>A 6.5-minute short film was crafted by editing fragments from <italic>Pippi on the Run</italic> into a coherent narrative. The film is structured into 13 interleaved 30-second blocks of either speech or music, with seven blocks featuring background music only and six blocks retaining the original dialogue and voice from the video. Patients were asked to watch the movie while the intracranial EEG signals were recorded. No fixation cross was displayed in the middle of the screen or elsewhere. The movie was presented using the Presentation software (Neurobehavioral Systems, Berkeley, CA) and the sound was synchronized with the neural recordings. More data acquisition details can be found in Berezutskaya et al.’s article<sup><xref ref-type="bibr" rid="c24">24</xref></sup>.</p>
</sec>
<sec id="s3c">
<title>iEEG data processing</title>
<p>Electrode contacts and epochs contaminated with excessive artifacts and epileptiform activity were removed from data analysis by visual inspection. Raw data were filtered with a 50-Hz notch filter and re-referenced to the common average reference. For each electrode contact in each patient, the preprocessed data were band-pass filtered (110– 140 Hz, 4th-order Butterworth). The Hilbert transform was then applied to extract the analytic amplitude. Each event (block) was extracted in the 0 to 30 s time window around its onset. The fifth music block was excluded, as there were no faces presented on screen. Subsequently, the data were down-sampled to 400 Hz and square-root transformed. Finally, the data were normalized by z-scoring with respect to baseline periods (−0.2 to 0 s before stimulus onset).</p>
</sec>
<sec id="s3d">
<title>Contact Location and Regions of Interest</title>
<p>We identified electrode contacts in STC in individual brains using individual anatomical landmarks (i.e., gyri and sulci). Superior temporal sulci and lateral sulci were used as boundaries. A coronal plane including the posterior tip of the hippocampus served as an anterior/posterior boundary. To identify electrode contacts in DLPFC, we projected the electrode contact positions provided by the open dataset onto Montreal Neurological Institute-152 template brain (MNI) space, using FreeSurfer. DLPFC was defined based on the following sets of HCP-MMP1<sup><xref ref-type="bibr" rid="c34">34</xref></sup> labels on both left and right hemispheres: 9-46d, 46, a9-46v, and p9-46v.</p>
</sec>
<sec id="s3e">
<title>Emotion feature extraction</title>
<p>Hume AI (“<ext-link ext-link-type="uri" xlink:href="http://www.hume.ai/">http://www.hume.ai/</ext-link>”) was used to extract the facial emotion features from the video. When multiple faces appeared in the movie, the maximum score of the facial expression features across all faces was used for each emotion category. All the time courses of facial emotion features were resampled to 2Hz. No facial emotion features were extracted for the fifth music block due to the absence of faces. The full list of the 48 facial emotion features is shown in <xref rid="fig4" ref-type="fig">Figure 4B</xref>.</p>
</sec>
<sec id="s3f">
<title>Encoding model fitting</title>
<p>To model iEEG responses to emotion, we used a linear regression approach with 48 facial emotion features extracted by Hume AI. Time-lagged versions of each feature (with 0, 0.5 and 1-second delays) were used in the model fitting. For each participant, high-frequency broadband (HFB) responses from all electrode contacts within each area were concatenated. To match the temporal resolution of the emotion feature time course, the HFB responses were binned into 500 ms windows. We then modeled the processed HFB response for each participant, each brain area, and each condition (speech vs music) using ridge regression. The optimal regularization parameter was assessed using 5-fold cross-validation, with the 20 different regularization parameters (log spaced between 10 and 10000). To keep the scale of the weights consistent, a single best overall value of the regularization coefficient was used for all areas in both the speech and music conditions in all patients. We used cross-validation iterator to fit the model and test it on held-out data. The model performance was evaluated by calculating Pearson correlation coefficients between measured and predicted HFB response of individual brain areas. The mean prediction accuracy (r value) of the encoding model with 5-fold cross-validation was then calculated.</p>
<p>Non-parametric permutation tests were used to test whether the encoding model performance was significantly &gt; 0 and whether there was a significant difference between groups. Specifically, we shufled facial emotion feature data in time, and then we conducted the standard data analysis steps (described above) using the shufled facial emotion features. This shufle procedure was repeated 5000 times to generate a null distribution, and p-values were calculated as the proportion of results from shufled data more extreme than the observed real value. A two-sided paired t-test was used to examine differences in encoding accuracy between speech and music conditions in post-childhood group (<xref rid="fig3" ref-type="fig">Figure 3D</xref>).</p>
</sec>
<sec id="s3g">
<title>Weight analysis</title>
<p>To examine the correlation between encoding model weights and age, we obtained 48 encoding model weights from all folds of cross-validation for all participants whose pSTC significantly encoded facial expression (i.e. the p-value of prediction accuracy is less than 0.05). Thus, 10 post-childhood individuals and 2 children were involved in the weight analysis. The weight for each feature represents its relative contribution to predicting the neural response. A higher weight indicates that the corresponding feature has a stronger influence on neural activity, meaning that variations in this feature more significantly impact the predicted response. We used the absolute value of weights and therefore did not discriminate whether facial emotion features were mapped to an increase or decrease in the HFB response.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>The current study examines functional changes in both low-level and high-level brain areas across development to provide valuable insights into the neural mechanisms underlying the maturation of facial expression perception. Based on our findings, we propose that young children rely primarily on early sensory areas rather than the prefrontal cortex for facial emotion processing. As development progresses, the prefrontal cortex becomes increasingly involved, perhaps serving to modulate responses in early sensory areas based on emotional context and enabling them to process complex emotions. This developmental progression ultimately enables the full comprehension of facial emotions in adulthood.</p>
<p>Behavioral results suggest that infants as young as only 7–8 months can categorize some emotions <sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c5">5</xref></sup>. However, sensitivity to facial expressions in young children does not mean that they can understand the meaning of that affective state. For example, Kaneshige and Haryu (2014)<sup><xref ref-type="bibr" rid="c35">35</xref></sup> found that although 4-month-old infants could discriminate facial configurations of anger and happiness, they responded positively to both, suggesting that at this stage, they may lack knowledge of the affective meaning behind these expressions. This underscores the idea that additional processes need to be developed for children to fully grasp the emotional content conveyed by facial expressions. Although the neural mechanism behind this development is still unclear, a reasonable perspective is that it requires both visual processing of facial features and emotion-related processing for the awareness of the emotional state of the other person<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. Indeed, growing evidence suggests that the prefrontal cortex plays an important role in integrating prior knowledge with incoming sensory information, allowing interpretation of the current situation in light of past emotional experience<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup>.</p>
<p>In the current study, we observed differential representation of facial expressions in the DLPFC between children and post-childhood individuals. First, in post-childhood individuals, neural activity in the DLPFC encodes high-dimensional facial expression information, whereas this encoding is absent in children. Second, while human voice enhances the representation of facial expressions in the DLPFC of post-childhood individuals, it instead reduces this representation in children. These results suggest that the DLPFC undergoes developmental changes in how it processes facial expressions. The absence of high-dimensional facial expression encoding in children implies that the DLPFC may not yet be fully engaged in emotional interpretation at an early age. Additionally, the opposite effects of human voice on facial expression representation indicate that multimodal integration of social cues develops over time. In post-childhood individuals, voices may enhance emotional processing by providing congruent information<sup><xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c41">41</xref></sup>, whereas in children, the presence of voice might interfere with or redirect attentional resources away from facial expression processing <sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>.</p>
<p>There have been few neuroimaging studies directly examining the functional role of young children’s DLPFC in facial emotion perception. Some evidence suggest that the prefrontal cortex continues to develop until adulthood to achieve its mature function in emotion perception<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup>, and for some emotion categories, this development may extend across the lifespan. For example, prefrontal cortex activation during viewing fearful faces increases with age <sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>. As there were not enough participants for us to calculate correlation between encoding model performance in DLPFC and age, it is still unclear whether the representation of facial expression in DLPFC increase linearly with age. One possibility is that the representation of facial expressions in the DLPFC gradually increases with age until it reaches an adult-like level. This would suggest a continuous developmental trajectory, where incremental improvements in neural processing accumulate over time. Another possibility is that development follows a more nonlinear pattern, showing improvement with prominent changes at specific ages. Interestingly, research has shown that performance on matching emotional expressions improves steadily over development, with notable gains in accuracy occurring between 9 and 10 years and again between 13 and 14 years, after which performance reaches adult-like levels <sup><xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<p>Although there are only two children in our sample with enough electrodes in pSTC, our results clearly showed that facial expression is encoded in each child’s pSTC. Moreover, the prediction accuracy of the encoding model in the two children was comparable to or higher than the average level in the post-childhood group. In the 5-year-old child (S19) who had electrode coverage in both DLPFC and pSTC, facial expressions were represented in the pSTC but not in the DLPFC. This rare and fortunate sampling allows us to rule out the possibility that the low prediction accuracy of the facial expression encoding model in the DLPFC is due to the reduced engagement in the movie-watching task for children. Consistent with our findings, previous studies have shown that the fusiform and superior temporal gyri are involved in emotion-specific processing in 10-year-old children<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. Meanwhile, some other researchers found that responses to facial expression in the amygdala and posterior fusiform gyri decreased as people got older<sup><xref ref-type="bibr" rid="c20">20</xref></sup>, but the use of frontal regions increased with age<sup><xref ref-type="bibr" rid="c44">44</xref></sup>. Therefore, we propose that early sensory areas like the fusiform and superior temporal gyri play a key role in facial expression processing in children, but their contribution may shift with age as frontal regions become more involved. Consistent with this perspective, our results revealed that the encoding weights for complex emotions in pSTC increased with age, suggesting a developmental trajectory in the neural representation of complex emotions in pSTC. This finding aligns with previous behavioral studies showing that social complex emotion recognition does not fully mature until young adulthood<sup><xref ref-type="bibr" rid="c31">31</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup>. In fact, our results suggests that the representation of complex facial expressions in pSTC continues to develop over the lifespan. As for the correlation between basic emotion encoding and age, the lack of a significant effect in our study does not necessarily indicate an absence of developmental change but may instead be due to the limited sample size.</p>
<p>In summary, our study provides novel insights into the neural mechanisms underlying the development of facial expression processing. As with any study, several limitations should be acknowledged. First, most electrode coverage in our study was in the left hemisphere, potentially limiting our understanding of lateralization effects. Second, while our results provide insights into the role of DLPFC during development, we were unable to examine other prefrontal regions, such as the orbitofrontal cortex (OFC) and anterior cingulate cortex (ACC), to examine their unique contributions to emotion processing. Lastly, due to sample size constraints, we were unable to divide participants into more granular developmental stages, such as early childhood, adolescence, and adulthood, which could provide a more detailed characterization of the neural mechanisms underlying the development of facial expression processing. Future studies using non-invasive methods, with more age-diverse samples will be essential for refining our understanding of how facial emotion processing develops across the lifespan.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We would like to thank Dr. Julia Berezutskaya for providing the audiovisual film that was used for iEEG data collection. This work was supported by fundings from United States National Institutes of Health (R01-MH127006).</p>
</ack>
<sec id="d1e860" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author contributions</title>
<p>X.F. and A.T. performed the data analysis. X.F. and K.R.B wrote the paper.</p>
</sec>
</sec>
<ref-list>
<title>Reference</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barrera</surname>, <given-names>M.E.</given-names></string-name>, and <string-name><surname>Maurer</surname>, <given-names>D.</given-names></string-name></person-group> (<year>1981</year>). <article-title>The perception of facial expressions by the three-month-old</article-title>. <source>Child Dev</source>. <volume>52</volume>, <fpage>203</fpage>–<lpage>206</lpage>. <pub-id pub-id-type="doi">10.2307/1129231</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walker-Andrews</surname>, <given-names>A.S.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Infants’ perception of expressive behaviors: differentiation of multimodal information</article-title>. <source>Psychol. Bull</source>. <volume>121</volume>, <fpage>437</fpage>–<lpage>456</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.121.3.437</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caron</surname>, <given-names>R.F.</given-names></string-name>, <string-name><surname>Caron</surname>, <given-names>A.J.</given-names></string-name>, and <string-name><surname>Myers</surname>, <given-names>R.S.</given-names></string-name></person-group> (<year>1982</year>). <article-title>Abstraction of invariant face expressions in infancy</article-title>. <source>Child Dev</source>. <volume>53</volume>, <fpage>1008</fpage>–<lpage>1015</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nelson</surname>, <given-names>C.A.</given-names></string-name>, <string-name><surname>Morse</surname>, <given-names>P.A.</given-names></string-name>, and <string-name><surname>Leavin</surname>, <given-names>L.A.</given-names></string-name></person-group> (<year>1979</year>). <article-title>Recognition of facial expressions by seven-month-old infants</article-title>. <source>Child Dev</source>. <volume>50</volume>, <fpage>1239</fpage>–<lpage>1242</lpage>. <pub-id pub-id-type="doi">10.2307/1129358</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nelson</surname>, <given-names>C.A.</given-names></string-name>, and <string-name><surname>Dolgin</surname>, <given-names>K.G.</given-names></string-name></person-group> (<year>1985</year>). <article-title>The generalized discrimination of facial expressions by seven-month-old infants</article-title>. <source>Child Dev</source>. <volume>56</volume>, <fpage>58</fpage>–<lpage>61</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kolb</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Taylor</surname>, <given-names>L.</given-names></string-name></person-group> (<year>1992</year>). <article-title>Developmental changes in the recognition and comprehension of facial expression: implications for frontal lobe function</article-title>. <source>Brain Cogn</source>. <volume>20</volume>, <fpage>74</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1016/0278-2626(92)90062-q</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Romani-Sponchiado</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Maia</surname>, <given-names>C.P.</given-names></string-name>, <string-name><surname>Torres</surname>, <given-names>C.N.</given-names></string-name>, <string-name><surname>Tavares</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Arteche</surname>, <given-names>A.X.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Emotional face expressions recognition in childhood: developmental markers, age and sex effect</article-title>. <source>Cogn. Process</source>. <volume>23</volume>, <fpage>467</fpage>–<lpage>477</lpage>. <pub-id pub-id-type="doi">10.1007/s10339-022-01086-1</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rodger</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Vizioli</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ouyang</surname>, <given-names>X.</given-names></string-name>, and <string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Mapping the development of facial expression recognition</article-title>. <source>Dev. Sci</source>. <volume>18</volume>, <fpage>926</fpage>–<lpage>939</lpage>. <pub-id pub-id-type="doi">10.1111/desc.12281</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thomas</surname>, <given-names>L.A.</given-names></string-name>, <string-name><surname>De Bellis</surname>, <given-names>M.D.</given-names></string-name>, <string-name><surname>Graham</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>LaBar</surname>, <given-names>K.S.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Development of emotional facial recognition in late childhood and adolescence</article-title>. <source>Dev. Sci</source>. <volume>10</volume>, <fpage>547</fpage>–<lpage>558</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-7687.2007.00614.x</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnston</surname>, <given-names>P.J.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bajic</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sercombe</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Michie</surname>, <given-names>P.T.</given-names></string-name>, and <string-name><surname>Karayanidis</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Facial emotion and identity processing development in 5-to 15-year-old children</article-title>. <source>Front. Psychol</source>. <volume>2</volume>, <fpage>26</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2011.00026</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawrence</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Campbell</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Skuse</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Age, gender, and puberty influence the development of facial emotion recognition</article-title>. <source>Front. Psychol</source>. <volume>6</volume>, <fpage>761</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2015.00761</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2015</year>). <article-title>A revised neural framework for face processing</article-title>. <source>Annu. Rev. Vis. Sci</source>. <volume>1</volume>, <fpage>393</fpage>–<lpage>416</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035518</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gomez</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Barnen</surname>, <given-names>M.A.</given-names></string-name>, <string-name><surname>Natu</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Mezer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Palomero-Gallagher</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>K.S.</given-names></string-name>, <string-name><surname>Amunts</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zilles</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Microstructural proliferation in human cortex is coupled with the development of face processing</article-title>. <source>Science</source> <volume>355</volume>, <fpage>68</fpage>–<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1126/science.aag0311</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arcaro</surname>, <given-names>M.J.</given-names></string-name>, <string-name><surname>Schade</surname>, <given-names>P.F.</given-names></string-name>, <string-name><surname>Vincent</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Ponce</surname>, <given-names>C.R.</given-names></string-name>, and <string-name><surname>Livingstone</surname>, <given-names>M.S.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Seeing faces is necessary for face-domain formation</article-title>. <source>Nat. Neurosci</source>. <volume>20</volume>, <fpage>1404</fpage>–<lpage>1412</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4635</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernstein</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Two neural pathways of face processing: A critical evaluation of current models</article-title>. <source>Neurosci Biobehav Rev</source>. <volume>55</volume>, <fpage>536</fpage>–<lpage>546</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.06.010</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engell</surname>, <given-names>A.D.</given-names></string-name>, and <string-name><surname>Haxby</surname>, <given-names>J.V.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Facial expression and gaze-direction in human superior temporal sulcus</article-title>. <source>Neuropsychologia</source> <volume>45</volume>, <fpage>3234</fpage>–<lpage>3241</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.06.022</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>M.R.</given-names></string-name>, <string-name><surname>Barbosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>de Haan</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Ferreira-Santos</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Understanding the development of face and emotion processing under a predictive processing framework</article-title>. <source>Dev. Psychol</source>. <volume>55</volume>, <fpage>1868</fpage>–<lpage>1881</lpage>. <pub-id pub-id-type="doi">10.1037/dev0000706</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yurgelun-Todd</surname>, <given-names>D.A.</given-names></string-name>, and <string-name><surname>Killgore</surname>, <given-names>W.D.S.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Fear-related activity in the prefrontal cortex increases with age during adolescence: a preliminary fMRI study</article-title>. <source>Neurosci Len</source>. <volume>406</volume>, <fpage>194</fpage>–<lpage>199</lpage>. <pub-id pub-id-type="doi">10.1016/j.neulet.2006.07.046</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Williams</surname>, <given-names>L.M.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>K.J.</given-names></string-name>, <string-name><surname>Palmer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Liddell</surname>, <given-names>B.J.</given-names></string-name>, <string-name><surname>Kemp</surname>, <given-names>A.H.</given-names></string-name>, <string-name><surname>Olivieri</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Peduto</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Gordon</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2006</year>). <article-title>The mellow years?: neural basis of improving emotional stability over age</article-title>. <source>J. Neurosci</source>. <volume>26</volume>, <fpage>6422</fpage>–<lpage>6430</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0022-06.2006</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tessitore</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hariri</surname>, <given-names>A.R.</given-names></string-name>, <string-name><surname>Fera</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>W.G.</given-names></string-name>, <string-name><surname>Das</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Weinberger</surname>, <given-names>D.R.</given-names></string-name>, and <string-name><surname>Manay</surname>, <given-names>V.S.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Functional changes in the activity of brain regions underlying emotion processing in the elderly</article-title>. <source>Psychiatry Res</source>. <volume>139</volume>, <fpage>9</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1016/j.pscychresns.2005.02.009</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yurgelun-Todd</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Emotional and cognitive changes during adolescence</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>17</volume>, <fpage>251</fpage>–<lpage>257</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2007.03.009</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hariri</surname>, <given-names>A.R.</given-names></string-name>, <string-name><surname>Bookheimer</surname>, <given-names>S.Y.</given-names></string-name>, and <string-name><surname>Mazziona</surname>, <given-names>J.C.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Modulating emotional responses: effects of a neocortical network on the limbic system</article-title>. <source>Neuroreport</source> <volume>11</volume>, <fpage>43</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200001170-00009</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hariri</surname>, <given-names>A.R.</given-names></string-name>, <string-name><surname>Manay</surname>, <given-names>V.S.</given-names></string-name>, <string-name><surname>Tessitore</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fera</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Weinberger</surname>, <given-names>D.R.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Neocortical modulation of the amygdala response to fearful stimuli</article-title>. <source>Biol. Psychiatry</source> <volume>53</volume>, <fpage>494</fpage>–<lpage>501</lpage>. <pub-id pub-id-type="doi">10.1016/s0006-3223(02)01786-9</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berezutskaya</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Vansteensel</surname>, <given-names>M.J.</given-names></string-name>, <string-name><surname>Aarnoutse</surname>, <given-names>E.J.</given-names></string-name>, <string-name><surname>Freudenburg</surname>, <given-names>Z.V.</given-names></string-name>, <string-name><surname>Piantoni</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Branco</surname>, <given-names>M.P.</given-names></string-name>, and <string-name><surname>Ramsey</surname>, <given-names>N.F.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film</article-title>. <source>Sci. Data</source> <volume>9</volume>, <fpage>91</fpage>. <pub-id pub-id-type="doi">10.1038/s41597-022-01173-0</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desikan</surname>, <given-names>R.S.</given-names></string-name>, <string-name><surname>Ségonne</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Quinn</surname>, <given-names>B.T.</given-names></string-name>, <string-name><surname>Dickerson</surname>, <given-names>B.C.</given-names></string-name>, <string-name><surname>Blacker</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Buckner</surname>, <given-names>R.L.</given-names></string-name>, <string-name><surname>Dale</surname>, <given-names>A.M.</given-names></string-name>, <string-name><surname>Maguire</surname>, <given-names>R.P.</given-names></string-name>, <string-name><surname>Hyman</surname>, <given-names>B.T.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2006</year>). <article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title>. <source>Neuroimage</source> <volume>31</volume>, <fpage>968</fpage>–<lpage>980</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flack</surname>, <given-names>T.R.</given-names></string-name>, <string-name><surname>Andrews</surname>, <given-names>T.J.</given-names></string-name>, <string-name><surname>Hymers</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Al-Mosaiwi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Marsden</surname>, <given-names>S.P.</given-names></string-name>, <string-name><surname>Strachan</surname>, <given-names>J.W.A.</given-names></string-name>, <string-name><surname>Trakulpipat</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Young</surname>, <given-names>A.W.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Responses in the right posterior superior temporal sulcus show a feature-based response to facial expression</article-title>. <source>Cortex</source> <volume>69</volume>, <fpage>14</fpage>–<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.002</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Shao</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>He</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>The bonom-up and top-down processing of faces in the human occipitotemporal cortex</article-title>. <source>eLife</source> <volume>9</volume>. <pub-id pub-id-type="doi">10.7554/eLife.48764</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alba-Ferrara</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hausmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mitchell</surname>, <given-names>R.L.</given-names></string-name>, and <string-name><surname>Weis</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2011</year>). <article-title>The neural correlates of emotional prosody comprehension: disentangling simple from complex emotion</article-title>. <source>PLoS ONE</source> <volume>6</volume>, <fpage>e28701</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0028701</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burnen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bird</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Blakemore</surname>, <given-names>S.-J.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Pubertal development of the understanding of social emotions: Implications for education</article-title>. <source>Learn Individ Differ</source>. <volume>21</volume>, <fpage>681</fpage>–<lpage>689</lpage>. <pub-id pub-id-type="doi">10.1016/j.lindif.2010.05.007</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russell</surname>, <given-names>J.A.</given-names></string-name>, and <string-name><surname>Paris</surname>, <given-names>F.A.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Do Children acquire Concepts for Complex Emotions Abruptly?</article-title> <source>Int. J. Behav. Dev</source>. <volume>17</volume>, <fpage>349</fpage>–<lpage>365</lpage>. <pub-id pub-id-type="doi">10.1177/016502549401700207</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garcia</surname>, <given-names>N.V.</given-names></string-name>, and <string-name><surname>Scherf</surname>, <given-names>K.S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Emerging sensitivity to socially complex expressions: A unique role for adolescence? Child Dev</article-title>. <source>Perspect</source>. <volume>9</volume>, <fpage>84</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1111/cdep.12114</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burnen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bird</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Moll</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Frith</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Blakemore</surname>, <given-names>S.-J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Development during adolescence of the neural processing of social emotion</article-title>. <source>J. Cogn. Neurosci</source>. <volume>21</volume>, <fpage>1736</fpage>–<lpage>1750</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2009.21121</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mona-Mena</surname>, <given-names>N.V.</given-names></string-name>, and <string-name><surname>Scherf</surname>, <given-names>K.S.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Pubertal development shapes perception of complex facial expressions</article-title>. <source>Dev. Sci</source>. <volume>20</volume>. <pub-id pub-id-type="doi">10.1111/desc.12451</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M.F.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T.S.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>E.C.</given-names></string-name>, <string-name><surname>Hacker</surname>, <given-names>C.D.</given-names></string-name>, <string-name><surname>Harwell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C.F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2016</year>). <article-title>A multi-modal parcellation of human cerebral cortex</article-title>. <source>Nature</source> <volume>536</volume>, <fpage>171</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1038/nature18933</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaneshige</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Haryu</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Categorization and understanding of facial expressions in 4-month-old infants</article-title>. <source>Jpn Psychol Res</source> <volume>57</volume>, <fpage>135</fpage>–<lpage>142</lpage>. <pub-id pub-id-type="doi">10.1111/jpr.12075</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pollak</surname>, <given-names>S.D.</given-names></string-name>, <string-name><surname>Messner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kistler</surname>, <given-names>D.J.</given-names></string-name>, and <string-name><surname>Cohn</surname>, <given-names>J.F.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Development of perceptual expertise in emotion recognition</article-title>. <source>Cognition</source> <volume>110</volume>, <fpage>242</fpage>–<lpage>247</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2008.10.010</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruba</surname>, <given-names>A.L.</given-names></string-name>, and <string-name><surname>Pollak</surname>, <given-names>S.D.</given-names></string-name></person-group> (<year>2020</year>). <article-title>The development of emotion reasoning in infancy and early childhood</article-title>. <source>Annu. Rev. Dev. Psychol</source>. <volume>2</volume>, <fpage>503</fpage>–<lpage>531</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-devpsych-060320-102556</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bany</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Taylor</surname>, <given-names>M.J.</given-names></string-name></person-group> (<year>2006</year>). <article-title>The development of emotional face processing during childhood</article-title>. <source>Dev. Sci</source>. <volume>9</volume>, <fpage>207</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-7687.2006.00480.x</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klasen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y.-H.</given-names></string-name>, and <string-name><surname>Mathiak</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Multisensory emotions: perception, combination and underlying neural processes</article-title>. <source>Rev. Neurosci</source>. <volume>23</volume>, <fpage>381</fpage>–<lpage>392</lpage>. <pub-id pub-id-type="doi">10.1515/revneuro-2012-0040</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collignon</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Girard</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gosselin</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Roy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Saint-Amour</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lassonde</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Lepore</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Audio-visual integration of emotion expression</article-title>. <source>Brain Res</source>. <volume>1242</volume>, <fpage>126</fpage>–<lpage>135</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2008.04.023</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kreifelts</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ethofer</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Grodd</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Erb</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Wildgruber</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Audiovisual integration of emotional signals in voice and face: an event-related fMRI study</article-title>. <source>Neuroimage</source> <volume>37</volume>, <fpage>1445</fpage>–<lpage>1456</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.06.020</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weissman</surname>, <given-names>D.H.</given-names></string-name>, <string-name><surname>Warner</surname>, <given-names>L.M.</given-names></string-name>, and <string-name><surname>Woldorff</surname>, <given-names>M.G.</given-names></string-name></person-group> (<year>2004</year>). <article-title>The neural mechanisms for minimizing cross-modal distraction</article-title>. <source>J. Neurosci</source>. <volume>24</volume>, <fpage>10941</fpage>–<lpage>10949</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3669-04.2004</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jacob</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Brück</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Domin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lotze</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Wildgruber</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2014</year>). <article-title>I can’t keep your face and voice out of my head: neural correlates of an anentional bias toward nonverbal emotional cues</article-title>. <source>Cereb. Cortex</source> <volume>24</volume>, <fpage>1460</fpage>–<lpage>1473</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhs417</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gunning-Dixon</surname>, <given-names>F.M.</given-names></string-name>, <string-name><surname>Gur</surname>, <given-names>R.C.</given-names></string-name>, <string-name><surname>Perkins</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Turetsky</surname>, <given-names>B.I.</given-names></string-name>, <string-name><surname>Chan</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Loughead</surname>, <given-names>J.W.</given-names></string-name>, <string-name><surname>Alsop</surname>, <given-names>D.C.</given-names></string-name>, <string-name><surname>Maldjian</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2003</year>). <article-title>Age-related differences in brain activation during emotional face processing</article-title>. <source>Neurobiol Aging</source> <volume>24</volume>, <fpage>285</fpage>–<lpage>295</lpage>. <pub-id pub-id-type="doi">10.1016/s0197-4580(02)00099-4</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Monk</surname>, <given-names>C.S.</given-names></string-name>, <string-name><surname>McClure</surname>, <given-names>E.B.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>E.E.</given-names></string-name>, <string-name><surname>Zarahn</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bilder</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Leibenlug</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Charney</surname>, <given-names>D.S.</given-names></string-name>, <string-name><surname>Ernst</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Pine</surname>, <given-names>D.S.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Adolescent immaturity in anention-related brain engagement to emotional facial expressions</article-title>. <source>Neuroimage</source> <volume>20</volume>, <fpage>420</fpage>–<lpage>428</lpage>. <pub-id pub-id-type="doi">10.1016/s1053-8119(03)00355-0</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lobaugh</surname>, <given-names>N.J.</given-names></string-name>, <string-name><surname>Gibson</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Taylor</surname>, <given-names>M.J.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Children recruit distinct neural systems for implicit emotional face processing</article-title>. <source>Neuroreport</source> <volume>17</volume>, <fpage>215</fpage>–<lpage>219</lpage>. <pub-id pub-id-type="doi">10.1097/01.wnr.0000198946.00445.2f</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107636.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Hu</surname>
<given-names>Xiaoqing</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Hong Kong</institution>
</institution-wrap>
<city>Hong Kong</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study examines a <bold>valuable</bold> question regarding the developmental trajectory of neural mechanisms supporting facial expression processing. Leveraging a rare intracranial EEG (iEEG) dataset including both children and adults, the authors reported that facial expression recognition mainly engaged the posterior superior temporal cortex (pSTC) among children, while both pSTC and the prefrontal cortex were engaged among adults. However, the sample size is relatively small, with analyses appearing <bold>incomplete</bold> to fully support the primary claims.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107636.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigates how the brain processes facial expressions across development by analyzing intracranial EEG (iEEG) data from children (ages 5-10) and post-childhood individuals (ages 13-55). The researchers used a short film containing emotional facial expressions and applied AI-based models to decode brain responses to facial emotions. They found that in children, facial emotion information is represented primarily in the posterior superior temporal cortex (pSTC) - a sensory processing area - but not in the dorsolateral prefrontal cortex (DLPFC), which is involved in higher-level social cognition. In contrast, post-childhood individuals showed emotion encoding in both regions. Importantly, the complexity of emotions encoded in the pSTC increased with age, particularly for socially nuanced emotions like embarrassment, guilt, and pride. The authors claim that these findings suggest that emotion recognition matures through increasing involvement of the prefrontal cortex, supporting a developmental trajectory where top-down modulation enhances understanding of complex emotions as children grow older.</p>
<p>Strengths:</p>
<p>(1) The inclusion of pediatric iEEG makes this study uniquely positioned to offer high-resolution temporal and spatial insights into neural development compared to non-invasive approaches, e.g., fMRI, scalp EEG, etc.</p>
<p>(2) Using a naturalistic film paradigm enhances ecological validity compared to static image tasks often used in emotion studies.</p>
<p>(3) The idea of using state-of-the-art AI models to extract facial emotion features allows for high-dimensional and dynamic emotion labeling in real time.</p>
<p>Weaknesses:</p>
<p>The study has notable limitations that constrain the generalizability and depth of its conclusions. The sample size was very small, with only nine children included and just two having sufficient electrode coverage in the posterior superior temporal cortex (pSTC), which weakens the reliability and statistical power of the findings, especially for analyses involving age. Electrode coverage was also uneven across brain regions, with not all participants having electrodes in both the dorsolateral prefrontal cortex (DLPFC) and pSTC, and most coverage limited to the left hemisphere-hindering within-subject comparisons and limiting insights into lateralization. The developmental differences observed were based on cross-sectional comparisons rather than longitudinal data, reducing the ability to draw causal conclusions about developmental trajectories. Moreover, the analysis focused narrowly on DLPFC, neglecting other relevant prefrontal areas such as the orbitofrontal cortex (OFC) and anterior cingulate cortex (ACC), which play key roles in emotion and social processing. Although the use of a naturalistic film stimulus enhances ecological validity, it comes at the cost of experimental control, with no behavioral confirmation of the emotions perceived by participants and uncertain model validity for complex emotional expressions in children. A non-facial music block that could have served as a control was available but not analyzed. Generalizability is further limited by the fact that all participants were neurosurgical patients, potentially with neurological conditions such as epilepsy that may influence brain responses. Additionally, the high temporal resolution of intracranial EEG was not fully utilized, as data were downsampled and averaged in 500-ms windows. Finally, the absence of behavioral measures or eye-tracking data makes it difficult to directly link neural activity to emotional understanding or determine which facial features participants attended to.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107636.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this paper, Fan et al. aim to characterize how neural representations of facial emotions evolve from childhood to adulthood. Using intracranial EEG recordings from participants aged 5 to 55, the authors assess the encoding of emotional content in high-level cortical regions. They report that while both the posterior superior temporal cortex (pSTC) and dorsolateral prefrontal cortex (DLPFC) are involved in representing facial emotions in older individuals, only the pSTC shows significant encoding in children. Moreover, the encoding of complex emotions in the pSTC appears to strengthen with age. These findings lead the authors to suggest that young children rely more on low-level sensory areas and propose a developmental shift from reliance on lower-level sensory areas in early childhood to increased top-down modulation by the prefrontal cortex as individuals mature.</p>
<p>Strengths:</p>
<p>(1) Rare and valuable dataset: The use of intracranial EEG recordings in a developmental sample is highly unusual and provides a unique opportunity to investigate neural dynamics with both high spatial and temporal resolution.</p>
<p>(2) Developmentally relevant design: The broad age range and cross-sectional design are well-suited to explore age-related changes in neural representations.</p>
<p>(3) Ecological validity: The use of naturalistic stimuli (movie clips) increases the ecological relevance of the findings.</p>
<p>(4) Feature-based analysis: The authors employ AI-based tools to extract emotion-related features from naturalistic stimuli, which enables a data-driven approach to decoding neural representations of emotional content. This method allows for a more fine-grained analysis of emotion processing beyond traditional categorical labels.</p>
<p>Weaknesses:</p>
<p>(1) The emotional stimuli included facial expressions embedded in speech or music, making it difficult to isolate neural responses to facial emotion per se from those related to speech content or music-induced emotion.</p>
<p>(2) While the authors leveraged Hume AI to extract facial expression features from the video stimuli, they did not provide any validation of the tool's accuracy or reliability in the context of their dataset. It remains unclear how well the AI-derived emotion ratings align with human perception, particularly given the complexity and variability of naturalistic stimuli. Without such validation, it is difficult to assess the interpretability and robustness of the decoding results based on these features.</p>
<p>(3) Only two children had relevant pSTC coverage, severely limiting the reliability and generalizability of results.</p>
<p>(4) The rationale for focusing exclusively on high-frequency activity for decoding emotion representations is not provided, nor are results from other frequency bands explored.</p>
<p>(5) The hypothesis of developmental emergence of top-down prefrontal modulation is not directly tested. No connectivity or co-activation analyses are reported, and the number of participants with simultaneous coverage of pSTC and DLPFC is not specified.</p>
<p>(6) The &quot;post-childhood&quot; group spans ages 13-55, conflating adolescence, young adulthood, and middle age. Developmental conclusions would benefit from finer age stratification.</p>
<p>(7) The so-called &quot;complex emotions&quot; (e.g., embarrassment, pride, guilt, interest) used in the study often require contextual information, such as speech or narrative cues, for accurate interpretation, and are not typically discernible from facial expressions alone. As such, the observed age-related increase in neural encoding of these emotions may reflect not solely the maturation of facial emotion perception, but rather the development of integrative processing that combines facial, linguistic, and contextual cues. This raises the possibility that the reported effects are driven in part by language comprehension or broader social-cognitive integration, rather than by changes in facial expression processing per se.</p>
</body>
</sub-article>
</article>