<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">93959</article-id>
<article-id pub-id-type="doi">10.7554/eLife.93959</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93959.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The asymmetric transfers of visual perceptual learning determined by the stability of geometrical invariants</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-4429-8443</contrib-id>
<name>
<surname>Yang</surname>
<given-names>Yan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhuo</surname>
<given-names>Yan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5909-3884</contrib-id>
<name>
<surname>Zuo</surname>
<given-names>Zhentao</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>zuozt@ibp.ac.cn</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-1005-5106</contrib-id>
<name>
<surname>Zhuo</surname>
<given-names>Tiangang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>tgzhou@ibp.ac.cn</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Lin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>State Key Laboratory of Brain and Cognitive Science, Institute of Biophysics, Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution>Hefei Comprehensive National Science Center, Institute of Artificial Intelligence</institution>, <city>Hefei</city>, <country country="CN">China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>University of Chinese Academy of Sciences, Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-02-28">
<day>28</day>
<month>02</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-03-04">
<day>04</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP93959</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-11-28">
<day>28</day>
<month>11</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-03">
<day>03</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.01.02.573923"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-02-28">
<day>28</day>
<month>02</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93959.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.93959.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.93959.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.93959.1.sa0">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.93959.1.sa3">Author Response</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Yang et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Yang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-93959-v2.pdf"/>
<abstract>
<title>Abstract</title><p>We quickly and accurately recognize the dynamic world by extracting invariances from highly variable scenes, a process can be continuously optimized through visual perceptual learning (VPL). While it is widely accepted that the visual system prioritizes the perception of more stable invariants, the influence of the structural stability of invariants on VPL remains largely unknown. In this study, we designed three geometrical invariants with varying levels of stability for VPL: projective (e.g., collinearity), affine (e.g., parallelism), and Euclidean (e.g., orientation) invariants, following the Klein’s Erlangen program. We found that learning to discriminate low-stability invariant transferred asymmetrically to those with higher stability, and that training on high-stability invariants enabled location transfer. To explore learning-associated plasticity in the visual hierarchy, we trained deep neural networks (DNNs) to model this learning procedure. We reproduced the asymmetric transfer between different invariants in DNN simulations and found that the distribution and time course of plasticity in DNNs suggested a neural mechanism similar to the reverse hierarchical theory (RHT), yet distinct in that invariant stability—not task difficulty or precision—emerged as the key determinant of learning and generalization. We propose that VPL for different invariants follows the Klein hierarchy of geometries, beginning with the extraction of high-stability invariants in higher-level visual areas, then recruiting lower-level areas for the further optimization needed to discriminate less stable invariants.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Long-term learning curves (Figure 6-figure supplement 2) and a new experiment (Experiment 3) added; Abstract, Introduction, Discussion revised to clarify the conceptual foundation of our study and strengthen the theoretical interpretation of our results.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>In order to adapt to a continually evolving and changing environment, the organism must identify information that remains consistent and stable in dynamic changes (<xref ref-type="bibr" rid="c24">Gold and Stocker, 2017</xref>). Through the process of visual perceptual learning (VPL), the visual system acquires an increased ability to extract meaningful and structured information from the environment to guide decisions and actions adaptively (<xref ref-type="bibr" rid="c2">Adolph and Kretch, 2015</xref>; <xref ref-type="bibr" rid="c20">Gibson and Pick, 2000</xref>; <xref ref-type="bibr" rid="c18">Gibson, 1969</xref>; <xref ref-type="bibr" rid="c25">Gold and Watanabe, 2010</xref>). The “information” mentioned above can be conceptualized as invariance preserved under transformation in perception (<xref ref-type="bibr" rid="c23">Gibson, 1979</xref>).</p>
<p>It is widely accepted that different kinds of invariant properties hold distinct ecological significance and possess different levels of utility in perception (<xref ref-type="bibr" rid="c6">Buccella, 2021</xref>). A fairly large set of experimental findings across various paradigms have converged at the conclusion that the detectability and perceptual salience of an object’s attributes are systematically related to their structural stability under change—in a manner similar to Klein hierarchy of geometries; in particular, observers are more sensitive to geometrical properties with higher structural stability (<xref ref-type="bibr" rid="c11">Chen, 2005</xref>, <xref ref-type="bibr" rid="c10">1985</xref>, <xref ref-type="bibr" rid="c9">1982</xref>; <xref ref-type="bibr" rid="c50">Todd et al., 2014</xref>, <xref ref-type="bibr" rid="c51">1998</xref>). According to Klein’s Erlangen Program (<xref ref-type="bibr" rid="c31">Klein, 1893</xref>), a geometrical property is considered as an invariant preserved over a corresponding shape-changing transformation, the more general a transformation group, the more fundamental and stable the geometrical invariants over this transformation group. Within this hierarchy, structures that remain invariant under all projective transformations (projective geometry, e.g., maintaining collinearity) exhibit the highest stability. Less stable geometries arise by imposing additional constraints on the transformation group; for instance, affine geometry adds constraints to the projective group (e.g., maintaining parallelism), and Euclidean geometry introduces further constraints (e.g., maintaining length or orientation). Thus, the hierarchy of geometrical invariants is nested: projective transformations encompass both affine and Euclidean transformations, while affine transformations encompass Euclidean transformations (<xref rid="fig1" ref-type="fig">Figure 1</xref>). Despite this foundational understanding, it remains unclear whether the learning of invariants with different levels of stability follows a predictable pattern—a question central to our research. To address this, we must first grasp the characteristics of VPL.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Nested relationship of the Klein hierarchy of geometries.</title>
<p>Stratify geometrical invariants in ascending order of stability: Euclidean geometry, affine geometry, and projective geometry. The pairs of shapes in each circle differ in corresponding geometries.</p></caption>
<graphic xlink:href="573923v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>According to Gibson’s differentiation view (<xref ref-type="bibr" rid="c22">Gibson and Gibson, 1955</xref>), perceptual learning is a process of “differentiating previously vague impressions” and whereby perceptual information becomes increasingly specific to the stimuli in the world. They proposed that learning through differentiation involves discovery and selectively processing the information most relevant to a task, including identifying “higher-order invariants” that govern some classification and filter out irrelevant information.</p>
<p>Traditionally, a hallmark of VPL has been its specificity for the basic attributes of the trained stimulus and task (<xref ref-type="bibr" rid="c14">Crist et al., 1997</xref>; <xref ref-type="bibr" rid="c17">Fiorentini and Berardi, 1981</xref>; <xref ref-type="bibr" rid="c27">Hua et al., 2010</xref>). Recent studies have challenged the specificity of learned improvements and demonstrated transfer effects between stimuli (<xref ref-type="bibr" rid="c36">Liu and Weinshall, 2000</xref>; <xref ref-type="bibr" rid="c48">Sowden et al., 2002</xref>; <xref ref-type="bibr" rid="c56">Zhang et al., 2010</xref>), location (<xref ref-type="bibr" rid="c29">Hung and Seitz, 2014</xref>) and substantially different tasks (<xref ref-type="bibr" rid="c40">McGovern et al., 2012</xref>; <xref ref-type="bibr" rid="c49">Szpiro and Carrasco, 2015</xref>). To be of practical utility, the generalization of learning effects should be a research focus, and understanding the determinants of specificity and transfer remains one of the large outstanding questions in field. Ahissar and Hochstein uncovered the relationship between task difficulty and transfer effects (<xref ref-type="bibr" rid="c3">Ahissar and Hochstein, 1997</xref>), leading to the formulation of the reverse hierarchy theory (RHT) which suggests that VPL is a top-down process that originates from the top of the cortical hierarchy and gradually progresses downstream to recruit the most informative neurons to encode the stimulus (<xref ref-type="bibr" rid="c4">Ahissar and Hochstein, 2004</xref>). In this framework, the level at which learning occurs is related to the difficulty of the task: easier tasks are learned at higher-level visual areas and show more transfer, while harder tasks engage lower levels and are more specific. There is also evidence demonstrating greater specificity in finer precision tasks, suggesting that the precision of stimuli may be crucial in driving specificity (<xref ref-type="bibr" rid="c30">Jeter et al., 2009</xref>; <xref ref-type="bibr" rid="c36">Liu and Weinshall, 2000</xref>; <xref ref-type="bibr" rid="c29">Hung and Seitz, 2014</xref>). Another study (<xref ref-type="bibr" rid="c37">Manenti et al., 2023</xref>) introduced variability in a task-irrelevant feature during the training, finding that variability enables generalization of learning to new stimuli and locations, irrespective of the required precision of task. Furthermore, training with high-salience stimuli leads to plasticity in higher visual areas, thereby reducing specificity for spatial position and stimulus features (<xref ref-type="bibr" rid="c33">Kourtzi et al., 2005</xref>).</p>
<p>The research described in the present article concerns the very nature of form perception, trying to explore whether VPL of geometrical invariants with various stability also exhibit hierarchical relationships and what a priori rule defines the mode of learning and generalization. Our research focuses on the VPL of discriminations based on differences in geometrical invariants with different levels of structural stability: (1) projective property that is invariant over all projective transformations, such as whether a contour is straight or curved (collinearity); (2) affine property that is invariant under affine transformations, such as whether a pair of contours is parallel or nonparallel (parallelism); (3) Euclidean property that is only invariant under Euclidean transformation, such as the relative orientations of line segments (orientation). We conducted three psychophysics experiments assessing how the structural stability of geometrical properties affect the learning effect, meanwhile investigating the transfer effect between different geometrical invariants or between different locations within each invariant. We modeled the learning processes using a deep neural network (DNN) that recapitulates several known VPL phenomena and thus provides a promising testbed to study learning-related plasticity in the primate visual hierarchy (<xref ref-type="bibr" rid="c53">Wenliang and Seitz, 2018</xref>). The learning-induced changes in the DNN model yield important predictions to the underlying neural substrate. We then interpret the results based on the Klein hierarchy of geometries and RHT.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Asymmetric transfer effect: The learning effect consistently transferred from low-stability to high-stability invariants</title>
<p>The paradigm of “configural superiority effects” with reaction time measures Forty-four right-handed healthy subjects participated in Experiment 1. We randomly assigned subjects into three groups trained with one invariant discrimination task: the collinearity (colli.) training group (n = 15), the parallelism (para.) training group (n = 15) and the orientation (ori.) training group (n = 14). The paradigm of “configural superiority effects” (<xref ref-type="bibr" rid="c11">Chen, 2005</xref>; <xref ref-type="bibr" rid="c45">Pomerantz et al., 1977</xref>) (faster and more accurate detection/discrimination of a composite display than of any of its constituent parts) was adapted to measure the short-term perceptual learning effects of different levels of invariants. As illustrated in <xref rid="fig2" ref-type="fig">Figure 2A</xref>, subjects performed the odd-quadrant discrimination task in which they were asked to report which quadrant differs from the other three as fast as possible on the premise of accuracy. During the test phases before and after training (Pre-test and Post-test, <xref rid="fig2" ref-type="fig">Figure 2B</xref>), subjects performed the three invariant discrimination tasks (colli., para., ori.) at three blocks respectively, the response times (RTs) were recorded to measure the learning effects. To distinguish VPL from programmed learning due to motor learning, a color discrimination task, served as baseline training, were performed before the main experiment (<xref rid="fig2" ref-type="fig">Figure 2B</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>(A) Examples of stimulus arrays designed to measure the perceptual learning effects of the geometries at different levels of structural stability in an odd quadrant task. They represent discriminations based on (a) a difference in collinearity, a kind of projective property, (b) a difference in parallelism, a kind of affine property, and (c) a difference in angle orientation, a kind of Euclidean property. (B) Procedure of Experiment 1.</p>
</caption>
<graphic xlink:href="573923v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2-s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2—figure supplement 1.</label>
<caption><title>The configural superiority effect, adapted from (<xref ref-type="bibr" rid="c45">Pomerantz et al., 1977</xref>; <xref ref-type="bibr" rid="c44">Pomerantz and Portillo, 2011</xref>).</title>
<p>In the odd-quadrant discrimination task, observers were required to locate the odd stimulus in an array of four figures as quickly as possible. The left panel shows the <italic>base display</italic> where the odd quadrant differs from the rest in line slope. The center panel shows the <italic>context display</italic> with four identical quadrants. The <italic>composite display</italic> is shown in the right panel, which is simply the superposition of the base and context displays. Mean correct reaction times (RTs) and error rates are shown. Note that the reaction times and accuracies for the <italic>composite display</italic> were better than the <italic>base display</italic>.</p></caption>
<graphic xlink:href="573923v2_fig2-s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>First of all, to investigate if there was any speed-accuracy trade-off, one-way repeated measures analysis of variance (ANOVA) with task as within-subject factors was conducted on the accuracies collected in the Pre-test phase. A significant main effect of the task was found (F(2, 129) = 7.977, p = 0.0005, <inline-formula><inline-graphic xlink:href="573923v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). We performed further post-hoc analysis carrying out paired t-test with FDR correction to examine the differences of accuracies between each pair of tasks. As a result, the accuracies of the collinearity task were significantly higher than that in the parallelism task (t(43) = 5.443, p &lt; 0.0001, Hedge’s g = 0.917), and that in the orientation task (t(43) = 4.351, p = 0.0001, Hedge’s g = 0.574). There was no difference in accuracy between the parallelism and orientation task (t(43) = 1.535, p = 0.132, Hedge’s g = 0.214). Then the same analysis was applied to the RTs in the three tasks prior to training. A significant main effect of the task was found in ANOVA (F(2, 129) = 59.557, p &lt; 0.0001, <inline-formula><inline-graphic xlink:href="573923v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). As shown from the post-hoc test, the RTs of collinearity task were significantly faster than that in the parallelism task (t(43) = 13.374, p &lt; 0.0001, Hedge’s g = 1.945), and that in the orientation task (t(43) = 13.333, p &lt; 0.0001, Hedge’s g = 2.295). The RT of the parallelism task was faster than that of orientation task (t(43) = 4.179, p &lt; 0.0001, Hedge’s g = 0.416). Taken together, the collinearity task has the highest accuracy as well as the faster RT among the three tasks, showing no speed-accuracy trade-off in Experiment 1. What’s more, the results before training were in line with the prediction from the Klein hierarchy of geometries, which suggest that the more stable invariants possessed higher detectability, resulting in better task performance. The statistical results of the accuracies in Post-test didn’t differ from that in Pre-test (<xref rid="fig3-s1" ref-type="fig">Figure 3—figure Supplement 1</xref>). In the following analysis, only correct trials were used.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Results of Experiment 1, RTs of each discrimination tasks measured at Pre-test and Post-test were compared by one-tailed, paired t-test.</title><p>(A) Results from the group trained on the collinearity task (n=15). Performances of the collinearity task were improved after training (p = 0.0008). (B) Results from the group trained on the parallelism task (n=15). Performances of the collinearity (p = 0.008) and parallelism (p &lt; 0.0001) task were improved after training. (C) Results from the group trained on the orientation task (n=14). Performances of the collinearity (p = 0.0007), parallelism (p = 0.002) and orientation task (p = 0.0002) were improved after training. (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05). Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig3-s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3—figure supplement 1.</label>
<caption><title>Accuracies for the three discrimination task measured at Pre-test and Post-test.</title><p>Accuracy was defined as the average percentage correct per block. At Pre-test, the accuracies of the collinearity task were significantly higher than that in the parallelism (p &lt; 0.0001) and orientation task (p = 0.0001). At Post-test, the accuracies of the collinearity task were still significantly higher than the other two tasks (p &lt; 0.0001 for the parallelism task, p = 0.0001 for the orientation task). Statistical significance was calculated by paired t-test with FDR correction. (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05). Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v2_fig3-s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig3-s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3—figure supplement 2.</label>
<caption><title>The learning indexes of the three geometrical invariants in Experiment 1.</title><p>Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v2_fig3-s2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The second analysis conducted was to assess whether there were learning effects of the trained tasks and transfer effects to the untrained tasks. This was assessed by examining the RTs of Pre-test and Post-test for each geometrical property discrimination task. One-tailed, paired t-test was performed to do this. For the collinearity training group, significant learning effect was found (t(14) = 3.911, p = 0.0008, Cohen’s d = 0.457), but there was no transfer to the other two untrained task (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). The parallelism training group show significant learning effect (t(14) = 5.169, p &lt; 0.0001, Cohen’s d = 1.095), and also show substantial improvement in the collinearity task (t(14) = 2.753, p = 0.008, Cohen’s d = 0.609) (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Moreover, performances of all three task were improved after training on orientation discrimination task: the collinearity task (t(13) = 4.033, p = 0.0007, Cohen’s d = 0.800), the parallelism task (t(13) = 3.482, p = 0.002, Cohen’s d = 0.631), and the orientation task (t(13) = 4.693, p = 0.0002, Cohen’s d = 1.048) (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). This particular pattern of transfer is interesting given the hierarchical relationship of the three different stimulus configurations. For instance, the performance improvement obtained on the parallelism task transferred to the collinearity task which is more stable, whereas not transferred to the orientation task which is less stable. Similarity, learned improvement in orientation discrimination transferred to more stable tasks, with RTs of both collinearity and parallelism tasks showing significant improvements. However, training on collinearity discrimination which is the most stable among the three tasks exhibited task specificity. These findings indicate that perceptual improvements derived from training on a relatively low-stability form invariant can transfer to those invariants with higher stability, but not vice versa.</p>
<p>Finally, we assessed whether learning effect differ in different form invariants. To this end, we computed the “learning index” (LI) (<xref ref-type="bibr" rid="c43">Petrov et al., 2011</xref>), which quantifies learning relative to the baseline performance. ANOVA analysis did not find a significant difference in the learning indices among the three tasks (F(2, 41) = 2.246, p = 0.119, <inline-formula><inline-graphic xlink:href="573923v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) (<xref rid="fig3-s2" ref-type="fig">Figure 3—figure Supplement 2</xref>).</p>
<p>What needs to be cautious is that, Experiment 1 with RT measures has limitations that make it difficult to truly compare the time required for processing different invariants. Specifically, our interest lies in understanding how learning affects the process of extracting geometrical invariants. However, in order to make a response, participants also need to locate the differing quadrant. The strength of the grouping effect of the shapes among the four quadrants can affect the speed of the localization process (<xref ref-type="bibr" rid="c41">Orsten-Hooge et al., 2011</xref>). Additionally, the strength of the grouping effect may vary under different conditions, leading to differences in reaction times that may reflect differences in the extraction time of geometrical invariants as well as the strength of the group effect among the quadrants.</p>
<p>The paradigm of “configural superiority effects” with reaction time measures To overcome the shortcomings of the RT measures, VPL is indexed by the improvements in thresholds of discrimination tasks after training in Experiment 2. We employed the adaptive staircase procedure QUEST (<xref ref-type="bibr" rid="c52">Watson and Pelli, 1983</xref>) to assess the thresholds. QUEST is a kind of Bayesian adaptive methods which typically produces estimates of psychometric function parameters and converges to a threshold more quickly than conventional staircase procedures. Forty-five healthy subjects participated in Experiment 2, and they were randomly assigned into three groups: the collinearity training group (n = 15), the parallelism training group (n = 15) and the orientation training group (n = 15). On each trial, subjects are required to determine whether a “target” is present among two simultaneously displayed stimuli (first-order choice, detection task), and if it is present, further specify its position (second-order choice, two-alternative forced choice task) (<xref rid="fig4" ref-type="fig">Figure 4</xref>, <xref rid="fig5" ref-type="fig">Figure 5</xref> and <xref rid="fig4-s1" ref-type="fig">Figure 4—figure Supplement 1</xref>).The procedure of Experiment 2 is similar to that of Experiment 1 except for no involvement of the baseline training. For each block, a QUEST staircase was used to adaptively adjust the angle separation (<italic>θ</italic>) of discrimination task within all trials but the catch trials, and provided an estimate of each subject’s 50% correct discrimination threshold.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Examples of the layout of a stimulus frame with two stimuli presented in diagonally opposite quadrants.</title><p>The top row demonstrates trials with a “target” (surrounded by orange dashed box), and the bottom row demonstrates the catch trials without “target”. The blue dashed lines represent the “base” orientation for each stimulus, and <italic>θ</italic> is the angle separation of the discrimination task. (A) Stimulus examples of the collinearity (colli.) task, the upper example shows a “target” (a line with one bend along its length) located at the lower right quadrant. Two straight lines were presented on a catch trial. (B) Stimulus examples of the parallelism (para.) task, the upper example shows a “target” (a pair of unparallel lines) located at the lower right quadrant. Two pairs of parallel lines were presented on a catch trial. (C) Stimulus examples of the orientation (ori.) task, the upper example shows a “target” (the more clockwise line) located at the upper right quadrant. Two lines with identical orientation were presented on a catch trial.</p></caption>
<graphic xlink:href="573923v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4-s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4—figure supplement 1.</label>
<caption><title>Examples of stimuli in Experiment 2.</title><p>Sample stimuli in the collinearity (left), parallelism (middle) and orientation (right) discrimination task. The blue dashed lines represent the “base” orientation for each stimulus, and <italic>θ</italic> is the angle separation of the discrimination task.</p></caption>
<graphic xlink:href="573923v2_fig4-s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Schematic descriptions of a trial in Experiment 2.</title><p>A trial included three intervals: a 500-ms pre-stimulus epoch, a 150-ms stimulus epoch, and the response epoch. In the stimulus epoch, two stimuli were presented at two diagonally opposite quadrants. Subjects maintained fixation in a green fixation point until the stimulus disappear, and indicated if there was a “target” by pressing the corresponding key (“J” for Yes, “F” for No). They should further report the position of the “target” if they pressed the J key. Negative feedback tone was presented at the end of a trial if incorrect response was given.</p></caption>
<graphic xlink:href="573923v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>One-way repeated measures ANOVA and post-hoc t-test were conducted on the thresholds collected from all three training groups prior to training. A significant main effect of the task was found in ANOVA analysis (F(2, 132) = 13.598, p &lt; 0.0001, <inline-formula><inline-graphic xlink:href="573923v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). As revealed by post-hoc tests, the initial performances of the three tasks were consistent with the relative stability of the invariant they involved, the discrimination threshold of the collinearity task was significantly lower than that of the parallelism task (t(44) = 3.247, p = 0.002, Hedge’s g = 0.595), and that of the orientation task (t(44) = 4.225, p = 0.0001, Hedge’s g = 0.871). The threshold of the parallelism task was lower than that of the orientation task (t(44) = 3.267, p = 0.002, Hedge’s g = 0.676). Moreover, the accuracies of the three tasks in Pre-test were also submitted to a one-way repeated measures ANOVA and no significant effect was found (F(2, 132) = 0.046, p = 0.955, <inline-formula><inline-graphic xlink:href="573923v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula>), suggesting no difference in difficulty among the three tasks before training.</p>
<p>One-tailed, paired t-test was performed to compare the threshold in Pre-test and Post-test. After orientation discrimination training, the orientation discrimination threshold at Post-test was significantly lower than that at Pre-test (t(14) = 2.443, p = 0.014, Cohen’s d = 0.898), and the same applied to the collinearity discrimination task, t(14) = 2.740, p = 0.008, Cohen’s d = 0.752, and the parallelism discrimination task, t(14) = 1.949, p = 0.036, Cohen’s d = 0.654 (<xref rid="fig6" ref-type="fig">Figure 6C</xref>). After parallelism discrimination training, significant improvements were found in the collinearity discrimination task (t(14) = 2.775, p = 0.007, Cohen’s d = 1.013) and the parallelism discrimination task (t(14) = 3.259, p = 0.003, Cohen’s d = 1.192) (<xref rid="fig6" ref-type="fig">Figure 6B</xref>). And collinearity discrimination training only produced an improvement on its own performance (t(14) = 2.128, p = 0.026, Cohen’s d = 0.759, <xref rid="fig6" ref-type="fig">Figure 6A</xref>). In summary, the pattern of generalization is identical to what was found in Experiment 1, where training of low-stability invariants optimized the perception of high-stability invariants but not vice versa. Just the same as in Experiment 1, no differences were found between the LIs of the three tasks in Experiment 2 (F(2, 42) = 2.905, p = 0.066, <inline-formula><inline-graphic xlink:href="573923v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <xref rid="fig6-s1" ref-type="fig">Figure 6—figure Supplement 1</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Results of Experiment 2, Thresholds of each discrimination task measured at Pre-test and Post-test were compared by one-tailed, paired t-test.</title><p>(A) Results from the group trained on the collinearity task (n = 15). Performances of the collinearity task were improved after training (p = 0.026). (B) Results from the group trained on the parallelism task (n = 15). Performances of the collinearity (p = 0.007) and parallelism (p = 0.003) task were improved after training. (C) Results from the group trained on the orientation task (n = 15). Performances of the collinearity (p = 0.008), parallelism (p = 0.036) and orientation task (p &lt; 0.0001) were improved after training. (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05). Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig6-s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6—figure supplement 1.</label>
<caption><title>The learning indexes of the three geometrical invariants in Experiment 2.</title><p>Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v2_fig6-s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig6-s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6—figure supplement 2.</label>
<caption><title>Bootstrap estimated learning curves across the three training tasks (n=7 for each task, the training lasted for 5 daily sessions, with each session consisting of 8 blocks).</title><p>(A) Threshold data across blocks were fitted with a power function (<italic>y</italic> = <italic>ax</italic><sup><italic>b</italic></sup>, where <italic>y</italic> is the block threshold) to estimate learning rate for each task. Subjects were randomly resampled with replacement 1,000 times, and parameters were fitted to each resample. Smooth curves were plotted based on the mean of the estimated parameters for each task. (B) Learning curves and performance across blocks for three training groups. Open circles indicate mean threshold per block.</p></caption>
<graphic xlink:href="573923v2_fig6-s2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The employment of short-term perceptual learning paradigms has made it difficult to accurately track the temporal learning processes (<xref ref-type="bibr" rid="c55">Yang et al., 2022</xref>).It is also possible that the lack of observed differences in learning effects between tasks could be due to insufficient learning in some tasks, considering the possibility that different training tasks may involve different short-term and longterm learning processes (<xref ref-type="bibr" rid="c1">Aberg et al., 2009</xref>; <xref ref-type="bibr" rid="c39">Mascetti et al., 2013</xref>). Given the considerations above, we recruited three additional groups of subjects (n=7 for each group) for long-term training consisting of five consecutive daily sessions. We computed bootstrapped estimates of the learning curve for each task because of the small sample size (<xref rid="fig6-s2" ref-type="fig">Figure 6—figure Supplement 2</xref>). Power parameter (the ‘<italic>b</italic>’ parameter for <italic>y</italic> = <italic>ax</italic><sup><italic>b</italic></sup>) estimates for the groups trained on colli., para. and ori. Were −0.076 (95% CI [−0.159, 0.008]), −0.136 (95% CI [−0.229, −0.034]), and −0.223 (95% CI [−0.372, −0.156]), respectively, suggesting that learning rates (equivalent to magnitudes of learning) are higher for less stable invariants.</p>
<p>Previous studies claimed that transfer of VPL is controlled by the difficulty or precision of the training task (<xref ref-type="bibr" rid="c3">Ahissar and Hochstein, 1997</xref>; <xref ref-type="bibr" rid="c30">Jeter et al., 2009</xref>; <xref ref-type="bibr" rid="c53">Wenliang and Seitz, 2018</xref>). In this experiment, task difficulty is related to the accuracy, and task precision which is related to the angle separation can be indexed by the threshold. As stated above, prior to training, there was not significant difference among the difficulties (accuracies) of the three tasks, and tasks with higher stability had lower threshold values, resulting in higher precision during training. We showed that the relative stability of invariants determined the transfer effects between tasks even when task difficulty was held constant between tasks, and the particular transfer pattern found in our study (learning from tasks with lower stability and lower precision transferred to the tasks with higher stability and precision) is contrary to the precision-dependent explanation for the generalization of VPL which proposed that training on higher precision can improve performance on lower precision tasks but the reverse is not true (<xref ref-type="bibr" rid="c30">Jeter et al., 2009</xref>; <xref ref-type="bibr" rid="c53">Wenliang and Seitz, 2018</xref>).</p>
</sec>
<sec id="s2b">
<title>Location generalization within each invariant</title>
<p>Having revealed the asymmetric transfer of learning between geometrical invariants with varying levels of stability, we then consider the extent to which learning transfers across locations within each invariant condition in Experiment 3. Experiment 2 was adapted for testing this withininvariant transfer effect. Thirty-eight healthy subjects participated in Experiment 3, and they were randomly assigned into three groups: the collinearity training group (n = 13), the parallelism training group (n = 12) and the orientation training group (n = 13). Each group of subjects performed their assigned task on fixed location during the training phase, and then were tested on the same task with stimuli presented at new location (the transfer test). Specifically, if the phase of training used the upper-left/lower-right diagonal, then the transfer test used the upper-right/lower-left diagonal, and vice versa. The presentation diagonal was randomly assigned to subjects for initial training and switched to the opposite diagonal for the transfer test.</p>
<p>We assessed learning (first vs. last trained block) and transfer effects (first trained block vs. transfer test) using one-tailed, paired t-test. All three groups demonstrated statistically significant increases in discrimination performance after training (colli.: t(12) = 7.821, p &lt; 0.0001, Cohen’s d = 2.032; para.: t(11) = 7.688, p &lt; 0.0001, Cohen’s d = 1.893; ori.: t(12) = 6.153, p &lt; 0.0001, Cohen’s d = 1.931), along with significant transfer effects to untrained location (colli.: t(11) = 5.324, p = 0.0001, Cohen’s d = 1.372; para.: t(11) = 3.704, p = 0.002, Cohen’s d = 0.809; ori.: t(12) = 3.641, p = 0.002, Cohen’s d = 0.948) (<xref rid="fig7" ref-type="fig">Figure 7A</xref>).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Results of Experiment 3.</title><p>(A) Thresholds of each discrimination task measured at the first trained block, the last trained block, and the transfer test. (B) The specificity indexes of the three geometrical invariants. Two-tailed, two-sample t-test were conducted to compare the SIs between each pair of the three tasks. Significant difference was only found between collinearity and parallelism task (p = 0.020). Error bars denote 1 SEM across subjects.</p></caption>
<graphic xlink:href="573923v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To further assess the location transfer effect of VPL for the three groups, we computed the “specificity index” (SI) (<xref ref-type="bibr" rid="c3">Ahissar and Hochstein, 1997</xref>), which estimates the portion of the initial learning that is transferred to untrained conditions (<xref rid="fig7" ref-type="fig">Figure 7B</xref>). Positive SI values indicate that learning is specific, whereas SI values smaller than or equal to 0 indicate generalization. We used two-tailed, one-sample t-test for comparisons against 0 or 1. For the collinearity training group, the SIs were not significantly above 0 (t(12) = 2.042, p = 0.064, Cohen’s d = 0.566) but significantly below 1 (t(12) = −5.069, p = 0.0003, Cohen’s d = 1.406), demonstrating generalization to new location. Partial location transfer was found in the parallelism and orientation task, for which the SIs were significantly above 0 (para.: t(11) = 9.324, p &lt; 0.0001, Cohen’s d = 2.692; ori.: t(12) = 4.526, p = 0.001, Cohen’s d = 1.180) and significantly below 1 (para.: t(11) = −4.114, p = 0.002, Cohen’s d = 1.188; ori.: t(12) = −2.789, p = 0.016, Cohen’s d = 0.774). Furthermore, significant difference in SIs was observed only between the collinearity and parallelism training groups (two-tailed, two-sample t-test: t(18.112) = −2.556, p = 0.020, Cohen’s d = 0.998), indicating that the collinearity task exhibits greater location generalization compared to the parallelism task.</p>
<p>The location transfer observed in the orientation task is somewhat surprising, given that VPL of orientation discrimination is typically reported to be highly specific to the trained retinal location (<xref ref-type="bibr" rid="c16">Fiorentini and Berardi, 1980</xref>; <xref ref-type="bibr" rid="c46">Schoups et al., 1995</xref>; <xref ref-type="bibr" rid="c47">Shiu and Pashler, 1992</xref>). Although previous studies have observed location transfer in orientation discrimination with low precision (large angle separations), our study’s angle separation thresholds were closer to those associated with location specificity (<xref ref-type="bibr" rid="c30">Jeter et al., 2009</xref>; <xref ref-type="bibr" rid="c37">Manenti et al., 2023</xref>). We therefore speculate that the observed location transfer in the orientation task may result from our unique experimental design: in the orientation task, subjects needed to compare pairs of lines presented in different quadrants to identify the “target” (<xref rid="fig4" ref-type="fig">Figure 4</xref>), whereas in the collinearity and parallelism tasks, subjects could make decisions based on stimuli within a single quadrant. This requirement for spatial integration in the orientation task likely enhanced its location generalization.</p>
<p>Overall, the greater location transfer observed in the collinearity task compared to the parallelism task, along with the location specificity in orientation discrimination reported in previous VPL studies, suggests that training with more stable invariants reduces spatial specificity. This finding also implies that plasticity associated with more stable invariants likely occurs in higher-order visual cortex areas, where neurons have larger receptive fields. As in Experiment 2, neither task difficulty nor stimulus precision can account for the location generalization observed in our study.</p>
</sec>
<sec id="s2c">
<title>Deep neural network simulations of learning and transfer effects</title>
<sec id="s2c1">
<title>Behavioral results</title>
<p>In Experiment 4, we repeated Experiment 2 using a DNN to model VPL (<xref ref-type="bibr" rid="c53">Wenliang and Seitz, 2018</xref>), aiming to determine whether training the DNN with invariants yields comparable behavioral improvements to those observed in humans and to gain insight into the underlying neural mechanisms. This network, which is derived from the general AlexNet architecture, fulfilled predictions of existing theories (e.g. the RHT) regarding specificity and plasticity and reproduced findings of tuning changes in neurons of the primate visual areas (<xref ref-type="bibr" rid="c37">Manenti et al., 2023</xref>; <xref ref-type="bibr" rid="c53">Wenliang and Seitz, 2018</xref>). Three networks were trained on the three tasks (colli., para., ori.) respectively, repeated in 12 conditions with varying stimulus parameters.</p>
<p>The performance trajectories of the networks trained with collinearity, parallelism, and orientation discrimination were averaged across the 12 stimulus conditions, and are shown in <xref rid="fig8" ref-type="fig">Figure 8A</xref> respectively. Each network was also tested on the two untrained tasks in the same stimulus condition during the training phase, and the transfer accuracies are presented in <xref rid="fig8" ref-type="fig">Figure 8A</xref> as well. As shown from the final accuracies (the numbers located at the end of each curve), the networks trained on ori. showed greatest transfer effects to the untrained tasks, and the networks trained on colli. showed worst transfer effects.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Performance of the model when trained under different discrimination tasks.</title><p>(A) Accuracy trajectories against training iterations from the models trained on collinearity (left), parallelism (middle), and orientation task (right), with the error bar representing 1 SEM. <italic>t</italic><sub>95</sub> is the iteration where the fully plastic network reached 95% accuracy, depicted by green dashed lines. The numbers located at the end of each curve are the final accuracies of the last iteration. (B) The learning speed which was indexed by <italic>t</italic><sub>95</sub> of the three tasks. The learning speed of the collinearity task was faster than the parallelism (p = 0.018) and orientation task (p &lt; 0.0001). The learning speed of the parallelism task was faster than the orientation task (p &lt; 0.0001). Statistical significance was calculated by paired t-test with FDR correction. (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05). Error bars denote 1 SEM across subjects. (C) Final mean accuracies when the network was trained and tested on all combinations of tasks</p></caption>
<graphic xlink:href="573923v2_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig8-s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8—figure supplement 1.</label>
<caption><title>Stimulus examples in Experiment 3.</title><p>Examples of the pairs of stimulus images for the three discrimination tasks in Experiment 3. The examples here are selected from the stimulus condition with the following parameters: angle separation (10° for colli. &amp; para. and 20° for ori.), distance for para. (40 pixels), location of gap for colli. (the front one-third).</p></caption>
<graphic xlink:href="573923v2_fig8-s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We then investigate the speeds of learning of the three tasks, we calculated <italic>t</italic><sub>95</sub> (<xref ref-type="bibr" rid="c53">Wenliang and Seitz, 2018</xref>), the iteration where the fully plastic network reached 95% accuracy, for each task. We found a significant main effect of the training task on <italic>t</italic><sub>95</sub> using the one-way repeated measures ANOVA (F(2, 33) = 144.636, p &lt; 0.0001, <inline-formula><inline-graphic xlink:href="573923v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). Further post-hoc analysis with paired t-test showed that the performance of collinearity discrimination started to asymptote earlier than the parallelism (t(11) = 2.567, p = 0.018, Cohen’s d = 1.012) and the orientation task (t(11) = 13.172, p &lt; 0.0001, Cohen’s d = 5.192). The orientation discrimination had slowest speed of learning, it reached saturation slower than the parallelism task (t(11) = 11.626, p &lt; 0.0001, Cohen’s d = 4.583) (<xref rid="fig8" ref-type="fig">Figure 8B</xref>).</p>
<p><xref rid="fig8" ref-type="fig">Figure 8C</xref> shows the final learning and transfer performance on all combinations of training and test task. A linear regression on the final accuracies showed a significant positive main effect of the stability of test task on the performance (<italic>β</italic> = 0.055, t(105) = 2.467, p = 0.015, <italic>R</italic><sup>2</sup> = 0.043), shown as increasing color gradient from top to bottom. We also found that the stability of training task had a significant negative effect (<italic>β</italic> = −0.057, t(105) = −2.591, p = 0.011, <italic>R</italic><sup>2</sup> = 0.048), shown as decreasing color gradient from right to left. Overall, consistent with the results in the two psychophysical experiments, these results suggested that transfer is more pronounced from less stable geometrical invariants to more stable invariants than vice versa, shown as higher accuracy on lower-right quadrants compared with top-left quadrants.</p>
</sec>
<sec id="s2c2">
<title>Distribution of learning across layers</title>
<p>To demonstrate the distribution of learning over different levels of hierarchy, we next examined the time course of learning across the layers, the weight changes for each layer were shown in <xref rid="fig9" ref-type="fig">Figure 9A</xref>. Overall, training on lower-stability geometrical invariants produced greater overall changes. Due to this mismatch of weight initialization, we focus on layers 1–5 with weights initialized from the pre-trained AlexNet.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Layer change under different training tasks.</title><p>(A) Layer change trajectories during learning. (B) Iteration at which the rate of change peaked (PSI) in layers 1-5. (C) Final layer change in layers 1-5. The error bar representing 1 SEM.</p></caption>
<graphic xlink:href="573923v2_fig9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To characterize learning across layers, we studied when and how much each layer changed during training. First, to quantify when significant learning happened in each layer, we estimated the iteration at which the gradient of a trajectory reached its peak (peak speed iteration, PSI; shown in <xref rid="fig9" ref-type="fig">Figure 9B</xref>). As the result of a linear regression analysis, in layers 1–5, we observed significant negative main effects of the stability of training task (<italic>β</italic> = −27.315, t(176) = −6.623, p &lt; 0.0001, <italic>R</italic><sup>2</sup> = 0.072), layer number (<italic>β</italic> = −17.327, t(176) = −6.450, p &lt; 0.0001, <italic>R</italic><sup>2</sup> = 0.065) and a positive interaction of the two on PSI (<italic>β</italic> = 6.579, t(176) = 5.290, p &lt; 0.0001, <italic>R</italic><sup>2</sup> = 0.115), suggesting that layer change started to asymptote later for lower layers and less stable invariants. For individual tasks, a linear regression analysis showed a significant negative effect of layer number on PSI only in the least stable task, that is the orientation discrimination task (<italic>β</italic> = −12.833, t(58) = −4.470, p &lt; 0.0001, <italic>R</italic><sup>2</sup> = 0.243). Therefore, for the discrimination of the least stable invariants, the order of change across layers is consistent with the RHT prediction that higher visual areas change before lower ones (<xref ref-type="bibr" rid="c4">Ahissar and Hochstein, 2004</xref>, <xref ref-type="bibr" rid="c3">1997</xref>).</p>
<p>The final layer changes at the end of training for the networks trained on the three tasks are shown in <xref rid="fig9" ref-type="fig">Figure 9C</xref> respectively. A linear regression analysis on the final changes in layers 1–5 revealed significant negative main effects of the stability of training task (<italic>β</italic> = −0.037, t(176) = −16.409, p &lt; 0.0001, <italic>R</italic><sup>2</sup> = 0.689), layer number (<italic>β</italic> = −0.011, t(176) = −7.655, p &lt; 0.001, <italic>R</italic><sup>2</sup> = 0.0001) and a positive interaction of the two (<italic>β</italic> = 0.005, t(176) = 7.329, p &lt; 0.0001, <italic>R</italic><sup>2</sup> = 0.075). However, for individual tasks, a significant negative linear effect of layer number was only found in the orientation discrimination task (<italic>β</italic> = −0.008, t(58) = −12.763, p &lt; 0.0001, <italic>R</italic><sup>2</sup> = 0.733). On the contrary, significant positive effect of layer number was found in the parallelism (<italic>β</italic> = 0.003, t(58) = 3.701, p = 0.0005, <italic>R</italic><sup>2</sup> = 0.177) and collinearity task (<italic>β</italic> = 0.002, t(58) = 2.538, p = 0.014, <italic>R</italic><sup>2</sup> = 0.084). We then calculated the centroid, which is equal to the weighted average of the layer numbers using the corresponding mean final change as the weight. The centroids for the networks trained on colli., para. and ori. are 3.11, 3.14 and 2.77, respectively. These centroids together with the results from linear regression collectively indicate that the least stable task induces more change lower in the hierarchy whereas the two more stable tasks induce change higher in the hierarchy.</p>
<p>Taken together, these findings demonstrate that training with lower-stability invariants delayed the onset of asymptotic learning while resulting in greater overall changes across DNN layers by the end of training. For tasks involving more stable invariants, such as collinearity and parallelism, learning-induced changes occurred predominantly in higher layers. In contrast, the least stable tasks (orientation) primarily involved changes in lower layers.</p>
<p>The distribution of learning across DNN layers suggests that the behavioral VPL effects observed in both humans and DNNs can be understood within the framework of RHT. Specifically, VPL of more stable invariants likely occurs in higher-order visual areas, facilitating greater location generalization, as observed in Experiment 3. In contrast, VPL of less stable invariants requires further optimization where lower-order areas are involved. This results in a greater amount of learning for both humans (<xref rid="fig6-s2" ref-type="fig">Figure 6—figure Supplement 2</xref>) and DNNs (greater final layer change) while producing slower but more substantial changes in lower layers (<xref rid="fig9" ref-type="fig">Figure 9B, C</xref>). More importantly, RHT also predicts a unidirectional transfer, where higher levels of the cortical hierarchy benefit from modifications of neural responses at lower levels (<xref ref-type="bibr" rid="c4">Ahissar and Hochstein, 2004</xref>, <xref ref-type="bibr" rid="c3">1997</xref>; <xref ref-type="bibr" rid="c40">McGovern et al., 2012</xref>). This is consistent with the asymmetric transfer from low- to high-stability invariants observed in our study.</p>
<p>Wenliang and Seitz proposed that high-precision training transfers more broadly to untrained and coarse discriminations than low-precision training (<xref ref-type="bibr" rid="c53">Wenliang and Seitz, 2018</xref>). However, in each stimulus condition of Experiment 4, the angle separations (precisions) in the collinearity and parallelism task were always the same, and were half of the angle separations in the orientation task. So, the pattern of transfer and layer change cannot be explained based on the relative precision of the training and test tasks that was suggested by Wenliang and Seitz. Rather, the relative stability of invariants involved in the tasks provides consistent and reasonable explanation for the asymmetric transfers found in our study.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we investigated perceptual learning of the three geometrical invariants in the Klein hierarchy, uncovering the asymmetric transfer across invariants and the location transfer effect within each invariant, both determined by structural stability. Specifically, learning effects of low-stability invariants transferred to those with higher stability, but not vice versa, with a similar pattern observed in DNNs. Additionally, location transfer effects were more pronounced for more stable invariants. These behavioral findings, alongside observed weight changes in DNNs, can be interpreted through a combination of the Klein’s Erlangen Program and RHT.</p>
<p>From the perspective of the Klein hierarchy of geometries, the asymmetric transfer can be expected given the nested relationship of the three geometrical invariants (<xref ref-type="bibr" rid="c32">Klein, 1941</xref>). On one hand, changes in a high-stability invariant include changes in less stable invariants, allowing improved discrimination abilities from training on low-stability invariants to assist in the discrimination of more stable ones. On the other hand, high-stability invariants are represented holistically during learning, making it difficult to encode their individual components in isolation. When performing a form discrimination task, the learners only need to extract the most stable invariants without necessarily extracting the embedded, less stable ones. This results in the suppression of less stable invariants when embedded within more stable configurations, limiting the generalization of learning from high-stability to low-stability invariants. Furthermore, the location generalization within each invariant and the distribution of learning across DNN layers suggest an underlying mechanism within the RHT framework. Specifically, training on high-stability invariants showed greater generalization to new locations (Experiment 3) and induced more weight changes higher in the DNN hierarchy (Experiment 4), indicating involvement of higher-level cortical areas with large receptive fields. And the order of weight changes across DNN layers during orientation task training aligns with the RHT prediction that higher visual areas change before lower ones (<xref ref-type="bibr" rid="c4">Ahissar and Hochstein, 2004</xref>; <xref ref-type="bibr" rid="c26">Hochstein and Ahissar, 2002</xref>)</p>
<p>Taken together, we speculate that the VPL of high-stability invariants occurs earlier and relies more on higher-level cortical areas, while learning of low-stability invariants requires further optimalization and relies more on lower-level cortical areas with higher resolution for finer discriminations. Due to the feedforward anatomical hierarchy of the visual system (<xref ref-type="bibr" rid="c38">Markov et al., 2013</xref>), the modifications of lower areas caused by training on low-stability invariants will also affect higher-level visual areas, thus influencing the discrimination of high-stability invariants and resulting in transfer effects from low-to-high-stability invariants.</p>
<p>As stated in Introduction, there are several factors (difficulty, precision, variability, salience) which had been proven to determine the locus of plasticity and thus the generalization of learning. Specifically, low-difficulty, low-precision, high-salience, high-variability tasks are learned on the basis of neurons in the higher-order visual cortex, with involvement of more invariant neurons. Our demonstration that training on higher-stability invariants will lead to involvement of higher order visual areas is consistent with these findings in some degree, given that high-stability invariants possess both high variability (underwent more general shape-changing transformation according to definition) and high perceptual salience. However, it should be noted that difficulty and precision fail to explain the pattern of generalization within or across invariants observed in Experiment 2, 3 and 4 (as demonstrated in Results). Hence, the structural stability of form invariants needs to be considered as a more essential and determinant factor underlying the generalization of VPL, at least in our research.</p>
<p>Asymmetric transfer of VPL has also been reported in previous literature. For instance, Huang et al. found an asymmetric transfer of learning from motion discrimination to detection, explaining this phenomenon as a result of only task-relevant stimulus information being learned (<xref ref-type="bibr" rid="c28">Huang et al., 2007</xref>)—an explanation that aligns with our findings. According to the Klein hierarchy of geometries, the discrimination of high-stability invariant was a task-relevant prerequisite for discriminating low-stability invariant. For example, discriminations of collinearity and parallelism were implicitly required for the orientation task and thus benefited from orientation training. Conversely, during training on collinearity and parallelism, the specific orientations of the lines, while present but task-irrelevant, were not learned. A similar principle can be applied to explain the transfer from parallelism to collinearity. Additionally, previous evidence has demonstrated that precise discrimination learning could transfer asymmetrically to coarse discriminations (<xref ref-type="bibr" rid="c29">Hung and Seitz, 2014</xref>; <xref ref-type="bibr" rid="c30">Jeter et al., 2009</xref>). However, precision could not explain the asymmetric transfer observed in our study, as elucidated in Experiment 2 and 4. Moreover, several studies have revealed the asymmetric transfer from clear to noisy display (<xref ref-type="bibr" rid="c8">Chang et al., 2014</xref>, <xref ref-type="bibr" rid="c7">2013</xref>; <xref ref-type="bibr" rid="c12">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="c15">Dosher and Lu, 2005</xref>), where VPL of a visual feature embedded in zero external noise, such as orientation, motion direction, or binocular disparity, transferred to the same feature in high external noise. Evidence from transcranial magnetic stimulation (TMS) studies in humans and physiological studies in monkeys suggests that this generalization results from functional reweighting, where training reweights the causal contributions of cortical areas involved in processing clear and noisy features (<xref ref-type="bibr" rid="c8">Chang et al., 2014</xref>; <xref ref-type="bibr" rid="c12">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="c13">Chowdhury and Deangelis, 2008</xref>; <xref ref-type="bibr" rid="c35">Liu and Pack, 2017</xref>). Specifically, training with clear stimuli containing informative local cues enhances reliance on lower-level cortical areas, where the optimized representation of these stimuli makes them more resistant to noise. This leads to a greater role of lower-level areas in perceptual decisions for noisy stimuli and supports the transfer of learning to such conditions. A similar reweighting mechanism may underlie the asymmetric transfer observed in our study, a possibility that future research should explore further.</p>
<p>According to the results from long-term training regimes in Experiment 2, invariants with lower-stability demonstrated greater learning effects and faster learning rates (<xref rid="fig6-s2" ref-type="fig">Figure 6—figure supplement 2</xref>). The greater magnitude of learning and final layer change in DNNs (<xref rid="fig9" ref-type="fig">Figure 9A, C</xref>) for lower-stability invariants consistently suggest that less stable invariants exhibit greater plasticity. However, behavioral findings on learning rates contrast with DNN simulations, where performance of higher-stability invariants reached an asymptote earlier (<xref rid="fig8" ref-type="fig">Figure 8B</xref>). This inconsistency may be due to initial performance differences between human participants and DNNs. Previous research has shown that learning rate and magnitude of learning are negatively correlated with initial performance (<xref ref-type="bibr" rid="c54">Yang et al., 2020</xref>). In biological visual systems, more stable invariants are easier detected (<xref ref-type="bibr" rid="c11">Chen, 2005</xref>, <xref ref-type="bibr" rid="c10">1985</xref>, <xref ref-type="bibr" rid="c9">1982</xref>; <xref ref-type="bibr" rid="c50">Todd et al., 2014</xref>, <xref ref-type="bibr" rid="c51">1998</xref>) and demonstrate better initial performance (as demonstrated in the psychophysics experiments of this study), leading to lower learning rates and learning effects. In contrast, DNNs typically begin training with initial accuracies near chance level (50%) for all three invariants. We speculate that the learning process in DNNs may model not only VPL but also aspects of evolution or development, where more stable invariants with greater ecological significance mature earlier and thus undergo less improvement through VPL.</p>
<p>In conclusion, our findings on the learning and transfer effects of geometrical invariants align well with the Klein hierarchy of geometries. The results on location generalization and DNN simulations suggest that VPL of more stable invariants occurs earlier in higher-level visual areas, followed by VPL of less stable invariants in lower-level areas, consistent with the predictions of RHT. In light of Gibson’s theory of perceptual learning, we infer that the Klein hierarchy of geometrics belong to what Gibson referred to as “structure” and “invariant” (<xref ref-type="bibr" rid="c2">Adolph and Kretch, 2015</xref>; <xref ref-type="bibr" rid="c19">Gibson, 1970</xref>). More importantly, invariants with higher structural stability parallel Gibson’s “higher-order invariants” (<xref ref-type="bibr" rid="c21">Gibson, 1971</xref>), which are extracted earlier in both perception and perceptual learning processes. Perceptual learning is a process of differentiation, beginning with the extraction of global, more stable invariants, and progressively involving local, less stable invariants to meet various task demands. Since the extraction of high-stability invariants is implicitly required in this process and can benefit from refined local information, the perceptual system which becomes more differentiated also develops an enhanced capacity for distinguishing high-stability invariants, leading to the asymmetric transfers observed in our research.</p>
<p>Future research should employ various neuroimaging and neurophysiological techniques to elucidate the neural mechanisms underlying VPL of geometrical invariants with varying stability and validate our theoretical framework. Such investigations could advance our understanding of object recognition, conscious perception and perceptual development.</p>
</sec>
<sec id="s4">
<title>Methods and materials</title>
<sec id="s4a">
<title>Participants and apparatus</title>
<p>A total of 148 right-handed healthy subjects participated in this study: 44 in Experiment 1 (24 female, mean age 23.70 ± 3.46 years), 45 in Experiment 2 (26 female, mean age 23.02 ± 3.40 years), 21 in Experiment 2—long-term VPL (10 female, mean age 23.14 ± 1.85 years), 38 in Experiment 3 (20 female, mean age 24.39 ± 2.57 years). All subjects were naïve to the experiment with normal or corrected-to-normal vision. Subjects provided written informed consent, and were paid to compensate for their time. Sample size was determined based on power calculations following a pilot study showing significant learning effect of the collinearity task for effect size of Cohen’s d = 0.728 at 80% power. In each experiment, subjects were randomly assigned to one of three training groups (colli., para., and ori. training group). The study was approved by the ethics committee of the Institute of Biophysics at the Chinese Academy of Sciences, Beijing.</p>
<p>Stimuli were displayed on a 24-inch computer monitor (AOC VG248) with a resolution of 1920 × 1080 pixels and a refresh rate of 100 Hz. The experiment was programmed and run in MATLAB (The Mathwork corp, Orien, USA) with the Psychotoolbox-3 extensions (<xref ref-type="bibr" rid="c5">Brainard, 1997</xref>; <xref ref-type="bibr" rid="c42">Pelli, 1997</xref>). Subjects were stabilized using a chin and head rest with visual distance of 70 cm in a dim ambient light room.</p>
</sec>
<sec id="s4b">
<title>Stimuli and tasks for psychophysics experiments</title>
<p>In Experiment 1, the stimulus arrays were designed to measure the learning effects of different geometrical invariants using the “configural superiority effects” (CSEs) paradigm (<xref ref-type="bibr" rid="c45">Pomerantz et al., 1977</xref>; <xref ref-type="bibr" rid="c44">Pomerantz and Portillo, 2011</xref>). CSEs refer to the findings that global configural relations between simple components rather than the components themselves may play a basic role in visual processing and were originally revealed by an odd-quadrant discrimination task, as illustrated in <xref rid="fig2-s1" ref-type="fig">Figure 2—figure supplement 1</xref>. This paradigm was also adapted to measure the relative salience of different levels of invariants (<xref ref-type="bibr" rid="c11">Chen, 2005</xref>). <xref rid="fig2" ref-type="fig">Figure 2A</xref> represents discrimination tasks based on difference in different geometrical properties. <xref rid="fig2" ref-type="fig">Figure 2Aa</xref> represents a discrimination task based on a difference in collinearity, which is a kind of projective property. Here, three quadrants have lines with one bend along their length, whereas the odd quadrant has straight lines. <xref rid="fig2" ref-type="fig">Figure 2Ab</xref> represents a discrimination task based on a difference in parallelism, which is a type of affine property. The orientation of the line segments in the odd quadrant differed from that of the other quadrants. In <xref rid="fig2" ref-type="fig">Figure 2Ac</xref>, the exact same component line segments were present in all quadrants; however, the odd quadrant contained arrows with different orientation, a type of Euclidean property. The stimuli are composed of white line segments with luminance of 38.83 cd/<italic>m</italic><sup>2</sup> presented on black background with luminance of 0.13 cd/<italic>m</italic><sup>2</sup>. The stimulus array is consisted of four quadrants (visual angle 2.6° × 2.6° for each quadrant) and presented in the center region (visual angle, 6° × 6°). The subjects need to identify which quadrant is different from the others. Either of the two states of an invariant may serve as a target. For example, in the Euclidean invariant condition (<xref rid="fig2" ref-type="fig">Figure 2Ac</xref>), both upward and downward arrows could be the target. A green central fixation point (RGB [0,130,0], 0.15°) was presented throughout the entire block. Subjects were instructed to perform the actual task while maintaining central fixation. Each trial began immediately after the Space key was pressed. The stimulus array was presented until the subject indicated the position of the odd quadrant (“target”) via a manual button press as fast as possible on the premise of accuracy. The response time (RT) in each trial was calculated from the onset of the stimulus array. A negative feedback tone was given if the response was wrong.</p>
<p>The sample stimuli and the layout of a stimulus frame in Experiment 2 are illustrated in <xref rid="fig4" ref-type="fig">Figure 4</xref> and <xref rid="fig4-s1" ref-type="fig">Figure 4—figure supplement 1</xref>, respectively. Individual stimulus could occur in one of four quadrants, approximately 250 arc min of visual angle from fixation (<italic>R</italic>); two stimuli were presented at two diagonally opposite quadrants on each trial. The presentation diagonal was randomly selected with equal probability. The “target” referred to the pair of non-collinear lines (in other words, a line with one bend along its length) for the colli. task, the pair of unparallel lines for the para. task, and the more clockwise line for the ori. task. The trials not involved presentation of a “target” were set as catch trials (<xref rid="fig4" ref-type="fig">Figure 4</xref>). All stimuli are white (38.83 cd/<italic>m</italic><sup>2</sup>) presented on black background (0.13 cd/<italic>m</italic><sup>2</sup>). Each stimulus is composed of a group of line(s): a pair of lines which are collinear or non-collinear in colli. task, a pair of lines which are parallel or unparallel in para. task, and a single long line in ori. Task (<xref rid="fig4-s1" ref-type="fig">Figure 4—figure supplement 1</xref>). The stimuli for the three tasks were made up of exactly the same line-segments. Thus, line-segments as well as all local features based on these line-segments, such as luminous flux, and spatial frequency components, were well controlled. The length of line (<italic>l</italic>) in colli. and para. task is 80 arc min, which is half of the length of line ori. task. The width of line is 2 arc min. The distance (<italic>d</italic>) between the pair of lines in the parallelism task is 40 arc min. The “base” orientation (the dashed line in <xref rid="fig4" ref-type="fig">Figure 4</xref> and <xref rid="fig4-s1" ref-type="fig">Figure 4—figure supplement 1</xref>) was randomly selected from 0-180°. For colli. and para., the “base” orientation for each stimulus on a stimulus frame was selected independently.</p>
<p>Schematic description of a trial in Experiment 2 is shown in <xref rid="fig5" ref-type="fig">Figure 5</xref>, to make the first-order choice, subjects were instructed to press the J key if they thought there was a “target” as defined by each task, and they should press the F key if the contrary is the case (that is, there was not a bent line for the colli. task, there was not a pair of unparallel lines for the para. task, and the two lines were oriented in the same direction for the ori. task). A second-order choice needed to be made if subjects have pressed the J key in the first-order choice: they should select which one was the “target” and report its position by pressing the corresponding key (the F, J, V, and N keys correspond to the upper left, upper right, lower left, and lower right quadrants, respectively). The response of a trial was regarded as correct only if both choices were correct. This type of experimental design with two-stage choice options together with relatively higher proportion of catch trials (one third of all trials) wound help reduce false alarms and response bias.</p>
<p>The stimuli and tasks in Experiment 3 are identical to those in Experiment 2, with the sole difference being that the presentation diagonal was fixed during the same phase (either the training or testing phase).</p>
</sec>
<sec id="s4c">
<title>Procedure for psychophysics experiments</title>
<p>The overall procedure of Experiment 1 is show in <xref rid="fig2" ref-type="fig">Figure 2B</xref>. The main VPL procedure consisted of three phases: pre-training test (Pre-test), discrimination training (Training), and post-training test (Post-test). During the testing phases, the three form invariant discrimination tasks were performed counterbalance across subjects at three blocks, respectively. During the training phase, all subjects were required to finish 10 blocks of the training task which is determined by their group. Each block contained 40 trials. Before all of the phases, subjects practiced 5 trials per task to make sure that they fully understood the tasks.</p>
<p>The procedure of Experiment 2 is similar to that of Experiment 1 except for no involvement of the baseline training. During each block, subject’s threshold was measured for each of the three tasks using a QUEST staircase of 40 trials augmented with 20 catch trials, leading to overall 60 trials per block. During the testing phases, the tests for the three tasks were counterbalanced across subjects. The training phase contained 8 blocks of the training task corresponding with the group of subjects. Each subject practiced one block per task with large angle difference to make sure that they fully understood the tasks.</p>
<p>Experiment 3 consisted of only two phases: discrimination training (8 or 9 blocks) and a transfer test (one block) on the same task, with stimuli presented at untrained retinal locations.</p>
</sec>
<sec id="s4d">
<title>Deep neural network simulations</title>
<p>The deep learning model used in this paper was adopted from Wenliang and Seitz (<xref ref-type="bibr" rid="c53">Wenliang and Seitz, 2018</xref>). The model was implemented in PyTorch (version 2.0.0) and consists of two parallel streams, each encompassing the first five convolutional layers of AlexNet (<xref ref-type="bibr" rid="c34">Krizhevsky et al., 2017</xref>) plus one fully connected layer which gives out a single scalar value. The network performed a two-interval two-alternative forced choice (2I-2AFC) task. One stream accepted one standard stimulus and the other stream accepted one comparison stimulus. The comparison stimulus was then compared to the standard stimulus. After the fully connected layers, the outputs of the two parallel streams – two scalar values – were entered to a sigmoid layer to give out one binary value indicating which stimulus was noncolinear, unparallel, or more clockwise, equivalent to the “target” in Experiment 2. Weights at each layer are shared between the two streams so that the representations of the two images are generated by the same parameters.</p>
<p>For the stimulus images, 12 equally spaced “base” orientations were chosen (0-165°, in steps of 15°), and the “base” orientation of the standard and comparison was chosen independently except for the orientation discrimination task. We trained the network on all combinations of the 3 parameters: (1) angle separation between standard and comparison —— ① 5° for colli. &amp; para. and 10° for ori., ② 10° for colli. &amp; para. and 20° for ori., ③ 20° for colli. &amp; para. and 40° for ori.; (2) distance between the pair of lines for para. —— ① 30 pixels, ② 40 pixels; (3) the location of the gap on line for colli. —— ① the midpoint; ② the front one-third. So there were overall 12 (3 × 2 × 2) stimulus conditions. It should be noted that the angle separations in the orientation task were always twice the angle separations in the other two tasks in each condition, this proportion was based on the initial threshold observed in the behavioral experiment. Here are the other stimulus parameters: length of line in para. (100 pixels); length of line in colli. and ori. (200 pixels); width of line (3 pixels); radius of gap for colli. (5 pixels). Each stimulus was centered on an 8-bit 227 × 227 pixel image with black background.</p>
<p>Three networks were trained on the three tasks (colli., para., ori.) respectively, repeated in 12 stimulus conditions. Each network was trained for 1000 iterations of 60-image batches with batch size of 20 pairs, and meanwhile was tested on the other two untrained tasks. Learning and transfer performances were measured at 30 approximately logarithmically spaced iterations from 1 to 1000. We used the same feature maps and kernel size as the original paper. Network weights were initialized such that the last fully connected layer was initialized by zero and weights in the five convolutional layers were copied from an AlexNet trained on object recognition (downloaded from <ext-link ext-link-type="uri" xlink:href="http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel">http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel</ext-link>) to mimic a (pretrained) adult brain. Training parameters were set as follow: learning rate = 0.0001, momentum = 0.9. The cross-entropy loss function was used as an objective function and optimized via stochastic gradient descent.</p>
</sec>
<sec id="s4e">
<title>Data analysis</title>
<p>All data analyses were carried out in Matlab (The Mathwork corp, Orien, USA) and Python (Python Software Foundation, v3.9.13). No data were excluded.</p>
<p>Human behavioral data were analyzed using analysis of variance (ANOVA), post-hoc tests, one-sample t-tests and paired t-tests. For ANOVAs and post-hoc tests, we computed <inline-formula><inline-graphic xlink:href="573923v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and Hedge’s g as effect sizes. For one-sample and paired t-tests, we computed Cohen’s d as effect size. To quantify learning effect, we computed the Learning Index (LI) (<xref ref-type="bibr" rid="c43">Petrov et al., 2011</xref>) as follows:
<disp-formula id="eqn1">
<graphic xlink:href="573923v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>performance</italic> refers to the RT or threshold obtained at the testing phases.</p>
<p>To quantify location transfer in Experiment 3, we computed the Specificity Index (SI) (<xref ref-type="bibr" rid="c3">Ahissar and Hochstein, 1997</xref>) as follows:
<disp-formula id="eqn2">
<graphic xlink:href="573923v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
In DNN experiment, Ordinary Least Squares (OLS) method of linear regression was implemented to analyze the transfer effects between different tasks. The following equation describes the model’s specification:
<disp-formula id="eqn3">
<graphic xlink:href="573923v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>y</italic> represents the dependent variable (final accuracy), <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub> represent the two features (training and test task) respectively. <italic>β</italic><sub>0</sub> is the intercept, <italic>β</italic><sub>1</sub>, <italic>β</italic><sub>2</sub> are coefficients of the linear model, and <italic>∈</italic> is the error term (unexplained by the model).</p>
<p>To estimate the learning effects in layers of DNN, the differences in weights before and after training at each layer were measured. Specifically, for a particular layer with <italic>N</italic> total connections to its lower layer, we denote the original <italic>N</italic>-dimensional weight vector trained on object classification as <italic>w</italic> (<italic>N</italic> and <italic>w</italic> are specified in AlexNet), the change in this vector after perceptual learning as <italic>δw</italic>, and define the layer change as follows:
<disp-formula id="eqn4">
<graphic xlink:href="573923v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>i</italic> indexes each element in the weight vector. Under this measure, scaling the weight vector by a constant gives the same change regardless of dimensionality, reducing the effect of unequal weight dimensionalities on the magnitude of weight change. For the weights in the final read-out layer that were initialized with zeros, the denominator in <xref rid="eqn1" ref-type="disp-formula">Equation 1</xref> was set to <italic>N</italic>, effectively measuring the average change per connection in this layer. Due to the convolutional nature of the layers 1–5, <italic>d</italic><sub><italic>rel</italic></sub> is equal to the change in filters that are shared across location in those layers. When comparing weight change across layers, we focus on the first five layers unless otherwise stated.</p>
<p>OLS method of linear regression with interaction terms was implemented to analyze the learning effects across layers. The following equation describes the model’s specification:
<disp-formula id="eqn5">
<graphic xlink:href="573923v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>y</italic> represents the dependent variable (PSI or final layer change), <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub> represent the two features (training task and layer number) respectively. <italic>β</italic><sub>0</sub> is the intercept, <italic>β</italic><sub>1</sub>, <italic>β</italic><sub>2</sub> and <italic>β</italic><sub>3</sub> are coefficients of the linear model, and <italic>∈</italic> is the error term (unexplained by the model).</p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Code availability</title>
<p>The deep neural network is available from the original authors on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/kevin-w-li/DNN_for_VPL">https://github.com/kevin-w-li/DNN_for_VPL</ext-link>).</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by the Ministry of Science and Technology of China grant and the Young Elite Scientists Sponsorship Program by CAST.</p>
</ack>
<sec id="d1e1741" sec-type="additional-information">
<title>Additional information</title>
<sec id="s7">
<title>Funding</title>
<p>Ministry of Science and Technology of China grant (2020AAA0105601), Tiangang Zhou</p>
<p>Young Elite Scientists Sponsorship Program by CAST (2017QNRC001), Zhentao Zuo</p>
<p>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</p>
</sec>
<sec id="s8">
<title>Author contributions</title>
<p>Yan Yang, Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review &amp; editing; Zhentao Zuo, Tiangang Zhou, Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration, Writing – review &amp; editing; Yan Zhuo, Conceptualization, Resources, Supervision, Funding acquisition; Lin Chen, Conceptualization, Resources, Funding acquisition, Methodology.</p>
</sec>
<sec id="s9" sec-type="ethics-statement">
<title>Ethics</title>
<p>Human subjects: Informed consent, and consent to publish was obtained from each observer before testing. The study was approved by the ethics committee of the Institute of Biophysics at the Chinese Academy of Sciences, Beijin (reference number:2017-IRB-004).</p>
</sec>
</sec>
<sec id="suppd1e1741" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1711">
<label>Supplemental Data 1.</label>
<media xlink:href="supplements/573923_file02.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1718">
<label>Supplemental Data 2.</label>
<media xlink:href="supplements/573923_file03.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1725">
<label>Supplemental Data 3.</label>
<media xlink:href="supplements/573923_file04.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1732">
<label>Supplemental Data 4.</label>
<media xlink:href="supplements/573923_file05.xlsx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aberg</surname> <given-names>KC</given-names></string-name>, <string-name><surname>Tartaglia</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Herzog</surname> <given-names>MH</given-names></string-name></person-group>. <article-title>Perceptual learning with Chevrons requires a minimal number of trials, transfers to untrained directions, but does not require sleep</article-title>. <source>Vision Research</source>. <year>2009</year>; <volume>49</volume>:<fpage>2087</fpage>–<lpage>2094</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.visres.2009.05.020</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adolph</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Kretch</surname> <given-names>KS</given-names></string-name></person-group>. <article-title>Gibson’s Theory of Perceptual Learning</article-title>. <source>International Encyclopedia of the Social &amp; Behavioral Sciences</source>. <year>2015</year> 12; <volume>10</volume>:<fpage>127</fpage>–<lpage>134</lpage>. doi: <pub-id pub-id-type="doi">10.1016/B978-0-08-097086-8.23096-1</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahissar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hochstein</surname> <given-names>S</given-names></string-name></person-group>. <article-title>Task difficulty and the specificity of perceptual learning</article-title>. <source>Nature</source>. <year>1997</year> 5; <volume>387</volume>(<issue>6631</issue>):<fpage>401</fpage>–<lpage>406</lpage>. doi: <pub-id pub-id-type="doi">10.1038/387401a0</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahissar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hochstein</surname> <given-names>S</given-names></string-name></person-group>. <article-title>The reverse hierarchy theory of visual perceptual learning</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2004</year> 10; <volume>8</volume>(<issue>10</issue>):<fpage>457</fpage>–<lpage>464</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2004.08.011</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname> <given-names>DH.</given-names></string-name></person-group> <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>. <year>1997</year>; <volume>10</volume>(<issue>4</issue>):<fpage>433</fpage>–<lpage>436</lpage>. doi: <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id>, pMID: <pub-id pub-id-type="pmid">9176952</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buccella</surname> <given-names>A</given-names></string-name></person-group>. <article-title>The problem of perceptual invariance</article-title>. <source>Synthese</source>. <year>2021</year> 12; <volume>199</volume>(<issue>5-6</issue>):<fpage>13883</fpage>–<lpage>13905</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s11229-021-03402-2</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kourtzi</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Welchman</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Mechanisms for Extracting a Signal from Noise as Revealed through the Specificity and Generality of Task Training</article-title>. <source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source>. <year>2013</year> 07; <volume>33</volume>:<fpage>10962</fpage>–<lpage>71</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0101-13.2013</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Mevorach</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kourtzi</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Welchman</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Training Transfers the Limits on Perception from Parietal to Ventral Cortex</article-title>. <source>Current biology : CB</source>. <year>2014</year> 10; <volume>24</volume>:<fpage>2445</fpage>–<lpage>2450</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2014.08.058</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>L</given-names></string-name></person-group>. <article-title>Topological Structure in Visual Perception</article-title>. <source>Science</source>. <year>1982</year> 11; <volume>218</volume>(<issue>4573</issue>):<fpage>699</fpage>–<lpage>700</lpage>. doi: <pub-id pub-id-type="doi">10.1126/science.7134969</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>L</given-names></string-name></person-group>. <article-title>Topological structure in the perception of apparent motion</article-title>. <source>Perception</source>. <year>1985</year>; <volume>14</volume>(<issue>2</issue>):<fpage>197</fpage>–<lpage>208</lpage>. doi: <pub-id pub-id-type="doi">10.1068/p140197</pub-id>, pMID: <pub-id pub-id-type="pmid">4069950</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>L</given-names></string-name></person-group>. <article-title>The topological approach to perceptual organization</article-title>. <source>Visual Cognition</source>. <year>2005</year> 5; <volume>12</volume>(<issue>4</issue>):<fpage>553</fpage>–<lpage>637</lpage>. doi: <pub-id pub-id-type="doi">10.1080/13506280444000256</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname> <given-names>N</given-names></string-name>, <string-name><surname>Cai</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>T</given-names></string-name>, <string-name><surname>Thompson</surname> <given-names>B</given-names></string-name>, <string-name><surname>Fang</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Perceptual learning modifies the functional specializations of visual cortical areas</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2016</year> 04; <volume>113</volume>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1524160113</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chowdhury</surname> <given-names>S</given-names></string-name>, <string-name><surname>Deangelis</surname> <given-names>G</given-names></string-name></person-group>. <article-title>Fine Discrimination Training Alters the Causal Contribution of Macaque Area MT to Depth Perception</article-title>. <source>Neuron</source>. <year>2008</year> 11; <volume>60</volume>:<fpage>367</fpage>–<lpage>77</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2008.08.023</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crist</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Kapadia</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Westheimer</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gilbert</surname> <given-names>CD</given-names></string-name></person-group>. <article-title>Perceptual Learning of Spatial Localization: Specificity for Orientation, Position, and Context</article-title>. <source>Journal of Neurophysiology</source>. <year>1997</year> 12; <volume>78</volume>(<issue>6</issue>):<fpage>2889</fpage>–<lpage>2894</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.1997.78.6.2889</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dosher</surname> <given-names>B</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>ZL</given-names></string-name></person-group>. <article-title>Perceptual learning in clear displays optimizes perceptual expertise: Learning the limiting process</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2005</year> 05; <volume>102</volume>:<fpage>5286</fpage>–<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.0500492102</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiorentini</surname> <given-names>A</given-names></string-name>, <string-name><surname>Berardi</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Perceptual learning specific for orientation and spatial frequency</article-title>. <source>Nature</source>. <year>1980</year> 10; <volume>287</volume>:<fpage>43</fpage>–<lpage>4</lpage>. doi: <pub-id pub-id-type="doi">10.1038/287043a0</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiorentini</surname> <given-names>A</given-names></string-name>, <string-name><surname>Berardi</surname> <given-names>N</given-names></string-name></person-group>. <article-title>Learning in grating waveform discrimination: Specificity for orientation and spatial frequency</article-title>. <source>Vision Research</source>. <year>1981</year> 1; <volume>21</volume>(<issue>7</issue>):<fpage>1149</fpage>–<lpage>1158</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0042-6989(81)90017-1</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gibson</surname> <given-names>EJ.</given-names></string-name></person-group> <source>Principles of perceptual learning and development</source>. <publisher-loc>East Norwalk, CT, US</publisher-loc>: <publisher-name>Appleton-Century-Crofts</publisher-name>; <year>1969</year>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gibson</surname> <given-names>EJ</given-names></string-name></person-group>. <article-title>The development of perception as an adaptive process</article-title>. <source>American scientist</source>. <year>1970</year>; <volume>58</volume>(<issue>1</issue>):<fpage>98</fpage>–<lpage>107</lpage>. PMID: <pub-id pub-id-type="pmid">5001578</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gibson</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Pick</surname> <given-names>A</given-names></string-name></person-group>. <source>An Ecological Approach to Perceptual Learning and Development</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2000</year>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gibson</surname> <given-names>EJ</given-names></string-name></person-group>. <article-title>Perceptual learning and the theory of word perception</article-title>. <source>Cognitive Psychology</source>. <year>1971</year>; <volume>2</volume>(<issue>4</issue>):<fpage>351</fpage>–<lpage>368</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0010-0285(71)90020-X</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gibson</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Gibson</surname> <given-names>EJ</given-names></string-name></person-group>. <article-title>Perceptual learning: Differentiation or enrichment?</article-title> <source>Psychological Review</source>. <year>1955</year>; <volume>62</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>41</lpage>. doi: <pub-id pub-id-type="doi">10.1037/h0048826</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gibson</surname> <given-names>JJ.</given-names></string-name></person-group> <source>The ecological approach to visual perception</source>. <publisher-loc>Boston, MA, US</publisher-loc>: <publisher-name>Houghton, Mifflin and Company</publisher-name>; <year>1979</year>. Page: <fpage>xiv</fpage>,<lpage>332</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Stocker</surname> <given-names>AA</given-names></string-name></person-group>. <article-title>Visual Decision-Making in an Uncertain and Dynamic World</article-title>. <source>Annual Review of Vision Science</source>. <year>2017</year> <volume>9</volume>; <issue>3</issue>:<fpage>227</fpage>–<lpage>250</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114511</pub-id>, pMID: <pub-id pub-id-type="pmid">28715956</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Watanabe</surname> <given-names>T</given-names></string-name></person-group>. <article-title>Perceptual learning</article-title>. <source>Current biology : CB</source>. <year>2010</year> 1; <volume>20</volume>(<issue>2</issue>). doi: <pub-id pub-id-type="doi">10.1016/j.cub.2009.10.066</pub-id>, pMID: <pub-id pub-id-type="pmid">20129034</pub-id> PMCID: <pub-id pub-id-type="pmcid">PMC3821996</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hochstein</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ahissar</surname> <given-names>M</given-names></string-name></person-group>. <article-title>View from the Top Hierarchies and Reverse Hierarchies in the Visual System</article-title>. <source>Neuron</source>. <year>2002</year>; <volume>36</volume>:<fpage>791</fpage>–<lpage>804</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0896-6273(02)01091-7</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hua</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bao</surname> <given-names>P</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>ZL</given-names></string-name></person-group>. <article-title>Perceptual learning improves contrast sensitivity of V1 neurons in cats</article-title>. <source>Current biology: CB</source>. <year>2010</year> 5; <volume>20</volume>(<issue>10</issue>):<fpage>887</fpage>–<lpage>894</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2010.03.066</pub-id>, pMID: <pub-id pub-id-type="pmid">20451388</pub-id> PMCID: <pub-id pub-id-type="pmcid">PMC2877770</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tjan</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zhou</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Z</given-names></string-name></person-group>. <article-title>Motion perceptual learning: When only task-relevant information is learned</article-title>. <source>Journal of vision</source>. <year>2007</year> 02; <volume>7</volume>:<fpage>14.1</fpage>–<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.1167/7.10.14</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hung</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Seitz</surname> <given-names>AR</given-names></string-name></person-group>. <article-title>Prolonged training at threshold promotes robust retinotopic specificity in perceptual learning</article-title>. <source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source>. <year>2014</year> 6; <volume>34</volume>(<issue>25</issue>):<fpage>8423</fpage>–<lpage>8431</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0745-14.2014</pub-id>, pMID: <pub-id pub-id-type="pmid">24948798</pub-id> PMCID: <pub-id pub-id-type="pmcid">PMC4061387</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jeter</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Dosher</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Petrov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>ZL</given-names></string-name></person-group>. <article-title>Task precision at transfer determines specificity of perceptual learning</article-title>. <source>Journal of Vision</source>. <year>2009</year> 3; <volume>9</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>1</lpage>. doi: <pub-id pub-id-type="doi">10.1167/9.3.1</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname> <given-names>F</given-names></string-name></person-group>. <article-title>Vergleichende Betrachtungen über neuere geometrische Forschungen</article-title>. <source>Mathematische Annalen</source>. <year>1893</year> 3; <volume>43</volume>(<issue>1</issue>):<fpage>63</fpage>–<lpage>100</lpage>. doi: <pub-id pub-id-type="doi">10.1007/BF01446615</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Klein</surname> <given-names>F</given-names></string-name></person-group>. <source>Elementary Mathematics from an Advanced Standpoint: Geometry</source>. <publisher-loc>Washington, D.C.</publisher-loc>: <publisher-name>National Mathematics Magazine</publisher-name>; <year>1941</year>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kourtzi</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Betts</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sarkheil</surname> <given-names>P</given-names></string-name>, <string-name><surname>Welchman</surname> <given-names>A</given-names></string-name></person-group>. <article-title>Distributed Neural Plasticity for Shape Learning in the Human Visual Cortex</article-title>. <source>PLoS biology</source>. <year>2005</year> 08; <volume>3</volume>:<fpage>e204</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.0030204</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>GE</given-names></string-name></person-group>. <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>Communications of the ACM</source>. <year>2017</year> 5; <volume>60</volume>(<issue>6</issue>):<fpage>84</fpage>–<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1145/3065386</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Pack</surname> <given-names>C</given-names></string-name></person-group>. <article-title>The Contribution of Area MT to Visual Motion Perception Depends on Training</article-title>. <source>Neuron</source>. <year>2017</year> 07; <volume>95</volume>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.024</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Weinshall</surname> <given-names>D</given-names></string-name></person-group>. <article-title>Mechanisms of generalization in perceptual learning</article-title>. <source>Vision Research</source>. <year>2000</year> 1; <volume>40</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>109</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0042-6989(99)00140-6</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manenti</surname> <given-names>GL</given-names></string-name>, <string-name><surname>Dizaji</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Schwiedrzik</surname> <given-names>CM</given-names></string-name></person-group>. <article-title>Variability in training unlocks generalization in visual perceptual learning through invariant representations</article-title>. <source>Current Biology</source>. <year>2023</year> <volume>1</volume>; p. <fpage>S0960982223000118</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2023.01.011</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markov</surname> <given-names>NT</given-names></string-name>, <string-name><surname>Ercsey-Ravasz</surname> <given-names>M</given-names></string-name>, <string-name><surname>Essen</surname> <given-names>DCV</given-names></string-name>, <string-name><surname>Knoblauch</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toroczkai</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Kennedy</surname> <given-names>H</given-names></string-name></person-group>. <article-title>Cortical High-Density Counterstream Architectures</article-title>. <source>Science</source>. <year>2013</year>; <volume>342</volume>. doi: <pub-id pub-id-type="doi">10.1126/science.123840</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mascetti</surname> <given-names>L</given-names></string-name>, <string-name><surname>Muto</surname> <given-names>V</given-names></string-name>, <string-name><surname>Matarazzo</surname> <given-names>L</given-names></string-name>, <string-name><surname>Foret</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ziegler</surname> <given-names>E</given-names></string-name>, <string-name><surname>Albouy</surname> <given-names>G</given-names></string-name>, <string-name><surname>Sterpenich</surname> <given-names>V</given-names></string-name>, <string-name><surname>Schmidt</surname> <given-names>C</given-names></string-name>, <string-name><surname>Degueldre</surname> <given-names>C</given-names></string-name>, <string-name><surname>Leclercq</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Phillips</surname> <given-names>C</given-names></string-name>, <string-name><surname>Luxen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vandewalle</surname> <given-names>G</given-names></string-name>, <string-name><surname>Vogels</surname> <given-names>R</given-names></string-name>, <string-name><surname>Maquet</surname> <given-names>P</given-names></string-name>, <string-name><surname>Balteau</surname> <given-names>E</given-names></string-name></person-group>. <article-title>The Impact of Visual Perceptual Learning on Sleep and Local Slow-Wave Initiation</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>; <volume>33</volume>:<fpage>3323</fpage> –<lpage>3331</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0763-12.2013</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGovern</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Webb</surname> <given-names>BS</given-names></string-name>, <string-name><surname>Peirce</surname> <given-names>JW</given-names></string-name></person-group>. <article-title>Transfer of perceptual learning between different visual tasks</article-title>. <source>Journal of Vision</source>. <year>2012</year> 10; <volume>12</volume>(<issue>11</issue>):<fpage>4</fpage>. doi: <pub-id pub-id-type="doi">10.1167/12.11.4</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Orsten-Hooge</surname> <given-names>K</given-names></string-name>, <string-name><surname>Portillo</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pomerantz</surname> <given-names>J</given-names></string-name></person-group>. <article-title>False Pop Out in Visual Search</article-title>. <source>Journal of Vision</source>. <year>2011</year> 9; <volume>11</volume>:<fpage>1314</fpage>–<lpage>1314</lpage>. doi: <pub-id pub-id-type="doi">10.1167/11.11.1314</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name></person-group>. <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spatial Vision</source>. <year>1997</year>; <volume>10</volume>(<issue>4</issue>):<fpage>437</fpage>–<lpage>442</lpage>. doi: <pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id>, pMID: <pub-id pub-id-type="pmid">9176953</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petrov</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Van Horn</surname> <given-names>NM</given-names></string-name>, <string-name><surname>Ratcliff</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Dissociable perceptual-learning mechanisms revealed by diffusion-model analysis</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2011</year> 6; <volume>18</volume>(<issue>3</issue>):<fpage>490</fpage>–<lpage>497</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13423-011-0079-8</pub-id>, pMID: <pub-id pub-id-type="pmid">21394547</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pomerantz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Portillo</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Grouping and Emergent Features in Vision: Toward a Theory of Basic Gestalts</article-title>. <source>Journal of experimental psychology Human perception and performance</source>. <year>2011</year> 07; <volume>37</volume>:<fpage>1331</fpage>–<lpage>49</lpage>. doi: <pub-id pub-id-type="doi">10.1037/a0024330</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pomerantz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Sager</surname> <given-names>L</given-names></string-name>, <string-name><surname>Stoever</surname> <given-names>R</given-names></string-name></person-group>. <article-title>Perception of wholes and of their component parts: Some configural superiority effects</article-title>. <source>Journal of experimental psychology Human perception and performance</source>. <year>1977</year> 08; <volume>3</volume>:<fpage>422</fpage>–<lpage>35</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0096-1523.3.3.422</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schoups</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Vogels</surname> <given-names>R</given-names></string-name>, <string-name><surname>Orban</surname> <given-names>G</given-names></string-name></person-group>. <article-title>Human perceptual learning in identifying the oblique orientation: Retinotopy, orientation specificity and monocularity</article-title>. <source>The Journal of physiology</source>. <year>1995</year> 04; <volume>483</volume> (<issue>Pt 3</issue>):<fpage>797</fpage>–<lpage>810</lpage>. doi: <pub-id pub-id-type="doi">10.1113/jphysiol.1995.sp020623</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shiu</surname> <given-names>LP</given-names></string-name>, <string-name><surname>Pashler</surname> <given-names>H</given-names></string-name></person-group>. <article-title>Improvement in line orientation discrimination is retinally local but dependent on cognitive set</article-title>. <source>Perception &amp; psychophysics</source>. <year>1992</year> <volume>12</volume>; <issue>52</issue>:<fpage>582</fpage>–<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.3758/BF03206720</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sowden</surname> <given-names>PT</given-names></string-name>, <string-name><surname>Rose</surname> <given-names>D</given-names></string-name>, <string-name><surname>Davies</surname> <given-names>IRL</given-names></string-name></person-group>. <article-title>Perceptual learning of luminance contrast detection: specific for spatial frequency and retinal location but not orientation</article-title>. <source>Vision Research</source>. <year>2002</year> 5; <volume>42</volume>(<issue>10</issue>):<fpage>1249</fpage>–<lpage>1258</lpage>. doi: <pub-id pub-id-type="doi">10.1016/s0042-6989(02)00019-6</pub-id>, pMID: <pub-id pub-id-type="pmid">12044757</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szpiro</surname> <given-names>SFA</given-names></string-name>, <string-name><surname>Carrasco</surname> <given-names>M</given-names></string-name></person-group>. <article-title>Exogenous Attention Enables Perceptual Learning</article-title>. <source>Psychological Science</source>. <year>2015</year>; <volume>26</volume>(<issue>12</issue>):<fpage>1854</fpage>–<lpage>1862</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797615598976</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todd</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Weismantel</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kallie</surname> <given-names>CS</given-names></string-name></person-group>. <article-title>On the relative detectability of configural properties</article-title>. <source>Journal of Vision</source>. <year>2014</year> 1; <volume>14</volume>(<issue>1</issue>):<fpage>18</fpage>–<lpage>18</lpage>. doi: <pub-id pub-id-type="doi">10.1167/14.1.18</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todd</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Norman</surname> <given-names>JF</given-names></string-name></person-group>. <article-title>On the Relative Salience of Euclidean, Affine, and Topological Structure for 3-D Form Discrimination</article-title>. <source>Perception</source>. <year>1998</year> 3; <volume>27</volume>(<issue>3</issue>):<fpage>273</fpage>–<lpage>282</lpage>. doi: <pub-id pub-id-type="doi">10.1068/p270273</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watson</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name></person-group>. <article-title>Quest: A Bayesian adaptive psychometric method</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1983</year> 3; <volume>33</volume>(<issue>2</issue>):<fpage>113</fpage>–<lpage>120</lpage>. doi: <pub-id pub-id-type="doi">10.3758/BF03202828</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wenliang</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Seitz</surname> <given-names>AR</given-names></string-name></person-group>. <article-title>Deep Neural Networks for Modeling Visual Perceptual Learning</article-title>. <source>The Journal of Neuroscience</source>. <year>2018</year> 7; <volume>38</volume>(<issue>27</issue>):<fpage>6028</fpage>–<lpage>6044</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1620-17.2018</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fangfang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Xi</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>P</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>ZL</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>CB</given-names></string-name></person-group>. <article-title>General learning ability in perceptual learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2020</year> 07; <volume>117</volume>:<fpage>202002903</fpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.2002903117</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Yan</surname> <given-names>F</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Fan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>YF</given-names></string-name>, <string-name><surname>Jiang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Xi</surname> <given-names>J</given-names></string-name>, <string-name><surname>lei Zhao</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>ZL</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>CB.</given-names></string-name></person-group> <article-title>Identifying Long- and Short-Term Processes in Perceptual Learning</article-title>. <source>Psychological Science</source>. <year>2022</year>; <volume>33</volume>:<fpage>830</fpage> –<lpage>843</lpage>. doi: <pub-id pub-id-type="doi">10.1177/09567976211056620</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname> <given-names>JY</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>GL</given-names></string-name>, <string-name><surname>Xiao</surname> <given-names>LQ</given-names></string-name>, <string-name><surname>Klein</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Levi</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>C</given-names></string-name></person-group>. <article-title>Rule-Based Learning Explains Visual Perceptual Learning and Its Specificity and Transfer</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year> 9; <volume>30</volume>(<issue>37</issue>):<fpage>12323</fpage>–<lpage>12328</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0704-10.2010</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93959.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> and unique study proposes a framework to understand and predict generalization in visual perceptual learning in humans based on form invariants. Using behavioral experiments in humans and by training deep networks, the authors offer evidence that the presence of stable invariants in a task leads to faster learning. However, this interpretation is promising but counter-intuitive and <bold>incomplete</bold>, since there could be possible other confounds such as differing attentional demands that lead to differing patterns of generalization. It can be strengthened through additional experiments and by rejecting alternate explanations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93959.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The strengths of this paper are clear: The authors are asking a novel question about geometric representation that would be relevant to a broad audience. Their question has a clear grounding in pre-existing mathematical concepts, that have been only minimally explored in cognitive science. Moreover, the data themselves are quite striking, such that my only concern would be that the data seem almost too perfect. It is hard to know what to make of that, however. From one perspective, this is even more reason the results should be published. Yet I am of the (perhaps unorthodox) opinion that reviewers should voice these gut reactions, even if it does not influence the evaluation otherwise. I have a few additional comments:</p>
<p>(1) The authors have now explained their theoretical position in a much more thorough and accessible way. I applaud them for that.</p>
<p>(2) Although I continue to believe that the manipulation in Experiment 1 is imperfect, I am convinced by the authors that the subsequent evidence is more convincing, and thus that the merit of this work lies mostly in those data.</p>
<p>If these results are robust, I believe the authors have discovered something of great value. While this paper stops short of providing definitive evidence in support of the Erlangen program (just as most work in vision science has stopped short of providing definitive evidence in support of its favored view), the data are sufficiently novel and provocative that these theories are worth entertaining further.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.93959.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Yang</surname>
<given-names>Yan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-4429-8443</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Zhuo</surname>
<given-names>Yan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zuo</surname>
<given-names>Zhentao</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5909-3884</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Zhuo</surname>
<given-names>Tiangang</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-1005-5106</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Lin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>eLife Assessment</bold></p>
<p>This important study proposes a framework to understand and predict generalization in visual perceptual learning in humans based on form invariants. Using behavioral experiments in humans and by training deep networks, the authors offer evidence that the presence of stable invariants in a task leads to faster learning. However, this interpretation is promising but incomplete. It can be strengthened through clearer theoretical justification, additional experiments, and by rejecting alternate explanations.</p>
</disp-quote>
<p>We sincerely thank the editors and reviewers for their thoughtful feedback and constructive comments on our study. We have taken significant steps to address the points raised, particularly the concern regarding the incomplete interpretation of our findings.</p>
<p>In response to Reviewer #1, we have included long-term learning curves from the human experiments to provide a clearer demonstration of the differences in learning rates across invariants, and have incorporated a new experiment to investigate location generalization within each invariant stability level. These new findings have shifted the focus of our interpretation from learning rates to the generalization patterns both within and across invariants, which, alongside the observed weight changes across DNN layers, support our proposed framework based on the Klein hierarchy of geometries and the Reverse Hierarchy Theory (RHT).</p>
<p>We have also worked to clarify the conceptual foundation of our study and strengthen the theoretical interpretation of our results in light of the concerns raised by Reviewers #1 and #2. We have further expanded the discussion linking our findings to previous work on VPL generalization, and addressed alternative explanations raised by Reviewers #1.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>Visual Perceptual Learning (VPL) results in varying degrees of generalization to tasks or stimuli not seen during training. The question of which stimulus or task features predict whether learning will transfer to a different perceptual task has long been central in the field of perceptual learning, with numerous theories proposed to address it. This paper introduces a novel framework for understanding generalization in VPL, focusing on the form invariants of the training stimulus. Contrary to a previously proposed theory that task difficulty predicts the extent of generalization - suggesting that more challenging tasks yield less transfer to other tasks or stimuli - this paper offers an alternative perspective. It introduces the concept of task invariants and investigates how the structural stability of these invariants affects VPL and its generalization. The study finds that tasks with high-stability invariants are learned more quickly. However, training with low-stability invariants leads to greater generalization to tasks with higher stability, but not the reverse. This indicates that, at least based on the experiments in this paper, an easier training task results in less generalization, challenging previous theories that focus on task difficulty (or precision). Instead, this paper posits that the structural stability of stimulus or task invariants is the key factor in explaining VPL generalization across different tasks</p>
<p>Strengths:</p>
<p>- The paper effectively demonstrates that the difficulty of a perceptual task does not necessarily correlate with its learning generalization to other tasks, challenging previous theories in the field of Visual Perceptual Learning. Instead, it proposes a significant and novel approach, suggesting that the form invariants of training stimuli are more reliable predictors of learning generalization. The results consistently bolster this theory, underlining the role of invariant stability in forecasting the extent of VPL generalization across different tasks.</p>
<p>- The experiments conducted in the study are thoughtfully designed and provide robust support for the central claim about the significance of form invariants in VPL generalization.</p>
<p>Weaknesses:</p>
<p>- The paper assumes a considerable familiarity with the Erlangen program and the definitions of invariants and their structural stability, potentially alienating readers who are not versed in these concepts. This assumption may hinder the understanding of the paper's theoretical rationale and the selection of stimuli for the experiments, particularly for those unfamiliar with the Erlangen program's application in psychophysics. A brief introduction to these key concepts would greatly enhance the paper's accessibility. The justification for the chosen stimuli and the design of the three experiments could be more thoroughly articulated.</p>
</disp-quote>
<p>We appreciate your feedback regarding the accessibility of our paper, particularly concerning the Erlangen Program and its associated concepts. We have revised the manuscript to include a more detailed introduction to Klein’s Erlangen Program in the second paragraph of Introduction section. It provides clear descriptions and illustrative examples for the three invariants within the Klein hierarchy of geometries, as well as the nested relationships among them (see revised Figure 1). We believe this addition will enhance the accessibility of the theoretical framework for readers who may not be familiar with these concepts.</p>
<p>In the revised manuscript, we have also expanded the descriptions of the stimuli and experimental design for psychophysics experiments. These additions aim to clarify the rationale behind our choices, ensuring that readers can fully understand the connection between our theoretical framework and experimental approach.</p>
<disp-quote content-type="editor-comment">
<p>- The paper does not clearly articulate how its proposed theory can be integrated with existing observations in the field of VPL. While it acknowledges previous theories on VPL generalization, the paper falls short in explaining how its framework might apply to classical tasks and stimuli that have been widely used in the VPL literature, such as orientation or motion discrimination with Gabors, vernier acuity, etc. It also does not provide insight into the application of this framework to more naturalistic tasks or stimuli. If the stability of invariants is a key factor in predicting a task's generalization potential, the paper should elucidate how to define the stability of new stimuli or tasks. This issue ties back to the earlier mentioned weakness: namely, the absence of a clear explanation of the Erlangen program and its relevant concepts.</p>
</disp-quote>
<p>We thank you for highlighting the necessary to integrate our proposed framework with existing observations in VPL research.</p>
<p>Prior VPL studies have not concurrently examined multiple geometrical invariants with varying stability levels, making direct comparisons challenging. However, we have identified tasks from the literature that align with specific invariants. For example, orientation discrimination with Gabors (e.g., Dosher &amp; Lu, 2005) and texture discrimination task (e.g., Wang et al., 2016) involve Euclidean invariants, and circle versus square discrimination (e.g., Kraft et al., 2010) involves affine invariants. On the other hand, our framework does not apply to studies using stimuli that are unrelated to geometric transformations, such as motion discrimination with Gabors or random dots, depth discrimination, vernier acuity, spatial frequency discrimination, contrast detection or discrimination.</p>
<p>By focusing on geometrical properties of stimuli, our work addresses a gap in the field and introduces a novel approach to studying VPL through the lens of invariant extraction, echoing Gibson’s ecological approach to perceptual learning.</p>
<p>In the revised manuscript, we have added a clearer explanation of Klein’s Erlangen Program, including the definition of geometrical invariants and their stability (the second paragraph in Introduction section). Additionally, we have expanded the Discussion section to draw more explicit comparisons between our results and previous studies on VPL generalization, highlighting both similarities and differences, as well as potential shared mechanisms.</p>
<disp-quote content-type="editor-comment">
<p>- The paper does not convincingly establish the necessity of its introduced concept of invariant stability for interpreting the presented data. For instance, consider an alternative explanation: performing in the collinearity task requires orientation invariance. Therefore, it's straightforward that learning the collinearity task doesn't aid in performing the other two tasks (parallelism and orientation), which do require orientation estimation. Interestingly, orientation invariance is more characteristic of higher visual areas, which, consistent with the Reverse Hierarchy Theory, are engaged more rapidly in learning compared to lower visual areas. This simpler explanation, grounded in established concepts of VPL and the tuning properties of neurons across the visual cortex, can account for the observed effects, at least in one scenario. This approach has previously been used/proposed to explain VPL generalization, as seen in (Chowdhury and DeAngelis, Neuron, 2008), (Liu and Pack, Neuron, 2017), and (Bakhtiari et al., JoV, 2020). The question then is: how does the concept of invariant stability provide additional insights beyond this simpler explanation?</p>
</disp-quote>
<p>We appreciate your thoughtful alternative explanation. While this explanation accounts for why learning the collinearity task does not transfer to the orientation task—which requires orientation estimation—it does not explain why learning the collinearity task fails to transfer to the parallelism task, which requires orientation invariance rather than orientation estimation. Instead, the asymmetric transfer observed in our study could be perfectly explained by incorporating the framework of the Klein hierarchy of geometries.</p>
<p>According to the Klein hierarchy, invariants with higher stability are more perceptually salient and detectable, and they are nested hierarchically, with higher-stability invariants encompassing lower-stability invariants (as clarified in the revised Introduction). In our invariant discrimination tasks, participants need only extract and utilize the most stable invariant to differentiate stimuli, optimizing their ability to discriminate that invariant while leaving the less stable invariants unoptimized.</p>
<p>For example:</p>
<list list-type="bullet">
<list-item><p>In the collinearity task, participants extract the most stable invariant, collinearity, to perform the task. Although the stimuli also contain differences in parallelism and orientation, these lower-stability invariants are not utilized or optimized during the task.</p>
</list-item>
<list-item><p>In the parallelism task, participants optimize their sensitivity to parallelism, the highest-stability invariant available in this task, while orientation, a lower-stability invariant, remains irrelevant and unoptimized.</p>
</list-item>
<list-item><p>In the orientation task, participants can only rely on differences in orientation to complete the task. Thus, the least stable invariant, orientation, is extracted and optimized.</p>
</list-item>
</list>
<p>This hierarchical process explains why training on a higher-stability invariant (e.g., collinearity) does not transfer to tasks involving lower-stability invariants (e.g., parallelism or orientation). Conversely, tasks involving lower-stability invariants (e.g., orientation) can aid in tasks requiring higher-stability invariants, as these higher-stability invariants inherently encompass the lower ones, resulting in a low-to-high-stability transfer effect.</p>
<p>This unique perspective underscores the importance of invariant stability in understanding generalization in VPL, complementing and extending existing theories such as the Reverse Hierarchy Theory. To help the reader understand our proposed theory, we revised the Introduction and Discussion section.</p>
<disp-quote content-type="editor-comment">
<p>- While the paper discusses the transfer of learning between tasks with varying levels of invariant stability, the mechanism of this transfer within each invariant condition remains unclear. A more detailed analysis would involve keeping the invariant's stability constant while altering a feature of the stimulus in the test condition. For example, in the VPL literature, one of the primary methods for testing generalization is examining transfer to a new stimulus location. The paper does not address the expected outcomes of location transfer in relation to the stability of the invariant. Moreover, in the affine and Euclidean conditions one could maintain consistent orientations for the distractors and targets during training, then switch them in the testing phase to assess transfer within the same level of invariant structural stability.</p>
</disp-quote>
<p>We thank you for this good suggestion. Using one of the primary methods for test generalization, we performed a new psychophysics experiment to specifically examine how VPL generalizes to a new test location within a single invariant stability level (see Experiment 3 in the revised manuscript). The results show that the collinearity task exhibits greater location generalization compared to the parallelism task. This finding suggests the involvement of higher-order visual areas during high-stability invariant training, aligning with our theoretical framework based on the Reverse Hierarchy Theory (RHT). We attribute the unexpected location generalization observed in the orientation task to an additional requirement for spatial integration in its specific experimental design (as explained in the revised Results section “Location generalization within each invariant”). Moreover, based on previous VPL studies that have reported location specificity in orientation discrimination (Fiorentini and Berardi, 1980; Schoups et al., 1995; Shiu and Pashler, 1992), along with the substantial weight changes observed in lower layers of DNNs trained on the orientation task (Figure 9B, C), we infer that under a more controlled experimental design—such as the two-interval, two-alternative forced choice (2I2AFC) task employed in DNN simulations, where spatial integration is not required for any of the three invariants—the plasticity for orientation tasks would more likely occur in lower-order areas.</p>
<p>In the revised manuscript, we have discussed how these findings, together with the observed asymmetric transfer across invariants and the distribution of learning across DNN layers, collectively reveal the neural mechanisms underlying VPL of geometrical invariants.</p>
<disp-quote content-type="editor-comment">
<p>- In the section detailing the modeling experiment using deep neural networks (DNN), the takeaway was unclear. While it was interesting to observe that the DNN exhibited a generalization pattern across conditions similar to that seen in the human experiments, the claim made in the abstract and introduction that the model provides a 'mechanistic' explanation for the phenomenon seems overstated. The pattern of weight changes across layers, as depicted in Figure 7, does not conclusively explain the observed variability in generalizations. Furthermore, the substantial weight change observed in the first two layers during the orientation discrimination task is somewhat counterintuitive. Given that neurons in early layers typically have smaller receptive fields and narrower tunings, one would expect this to result in less transfer, not more.</p>
</disp-quote>
<p>We appreciate your suggestion regarding the clarity of DNN modeling. While the DNN employed in our study recapitulates several known behavioral and physiological VPL effects (Manenti et al., 2023; Wenliang and Seitz, 2018), we acknowledge that the claim in the abstract and introduction suggesting the model provides a ‘mechanistic’ explanation for the phenomenon may have been overstated. The DNN serves primarily as a tool to generate important predictions about the underlying neural substrates and provides a promising testbed for investigating learning-related plasticity in the visual hierarchy.</p>
<p>In the revised manuscript, we have made significant improvements in explaining the weight change across DNN layers and its implication for understanding “when” and “where” learning occurs in the visual hierarchy. Specifically, in the Results (&quot;Distribution of learning across layers&quot;) and Discussion sections, we have provided a more explicit explanation of the weight change across layers, emphasizing its implications for understanding the observed variability in generalizations and the underlying neural mechanisms.</p>
<p>Regarding the substantial weight change observed in the first two layers during the orientation discrimination task, we interpret this as evidence that VPL of this least stable invariant relies more on the plasticity of lower-level brain areas, which may explain the poorer generalization performance to new locations or features observed in the previous literature (Fiorentini and Berardi, 1980; Schoups et al., 1995; Shiu and Pashler, 1992). However, this does not imply that learning effects of this least stable invariant cannot transfer to more stable invariants. From the perspective of Klein’s Erlangen program, the extraction of more stable invariants is implicitly required when processing less stable ones, which leads to their automatic learning. Additionally, within the framework of the Reverse Hierarchy Theory (RHT), plasticity in lower-level visual areas affects higher-level areas that receive the same low-level input, due to the feedforward anatomical hierarchy of the visual system (Ahissar and Hochstein, 2004, 1997; Markov et al., 2013; McGovern et al., 2012). Therefore, the improved signal from lower-level plasticity resulted from training on less stable invariants can enhance higher-level representations of more stable invariants, facilitating the transfer effect from low- to high-stability invariants.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>The strengths of this paper are clear: The authors are asking a novel question about geometric representation that would be relevant to a broad audience. Their question has a clear grounding in pre-existing mathematical concepts, that, to my knowledge, have been only minimally explored in cognitive science. Moreover, the data themselves are quite striking, such that my only concern would be that the data seem almost *too* clean. It is hard to know what to make of that, however. From one perspective, this is even more reason the results should be publicly available. Yet I am of the (perhaps unorthodox) opinion that reviewers should voice these gut reactions, even if it does not influence the evaluation otherwise. Below I offer some more concrete comments:</p>
<p>(1) The justification for the designs is not well explained. The authors simply tell the audience in a single sentence that they test projective, affine, and Euclidean geometry. But despite my familiarity with these terms -- familiarity that many readers may not have -- I still had to pause for a very long time to make sense of how these considerations led to the stimuli that were created. I think the authors must, for a point that is so central to the paper, thoroughly explain exactly why the stimuli were designed the way that they were and how these designs map onto the theoretical constructs being tested.</p>
</disp-quote>
<p>We thank you for reminding us to better justify our experimental designs. In response, we have provided a detailed introduction to Klein’s Erlangen Program, describing projective, affine, and Euclidean geometries, their associated invariants, and the hierarchical relationships among them (see revised Introduction and Figure 1).</p>
<p>All experiments in our study employed stimuli with varying structural stability (collinearity, parallelism, orientation, see revised Figure 2, 4), enabling us to investigate the impact of invariant stability on visual perceptual learning. Experiment 1 was adapted from paradigms studying the &quot;configural superiority effect,&quot; commonly used to assess the salience of geometric invariants. This paradigm was chosen to align with and build upon related research, thereby enhancing comparability across studies. To address the limitations of Experiment 1 (as detailed in our Results section), Experiments 2, 3, and 4 employed a 2AFC (two-alternative forced choice)-like paradigm, which is more common in visual perceptual learning research. Additionally, we have expanded descriptions of our stimuli and designs. aiming to ensure clarity and accessibility for all readers.</p>
<disp-quote content-type="editor-comment">
<p>(2) I wondered if the design in Experiment 1 was flawed in one small but critical way. The goal of the parallelism stimuli, I gathered, was to have a set of items that is not parallel to the other set of items. But in doing that, isn't the manipulation effectively the same as the manipulation in the orientation stimuli? Both functionally involve just rotating one set by a fixed amount. (Note: This does not seem to be a problem in Experiment 2, in which the conditions are more clearly delineated.)</p>
</disp-quote>
<p>We appreciate your insightful observation regarding the design of Experiment 1 and the potential similarity between the manipulations of the parallelism and orientation stimuli.</p>
<p>The parallelism and orientation stimuli in Experiment 1 were originally introduced by Olson and Attneave (1970) to support line-based models of shape coding and were later adapted by Chen (1986) to measure the relative salience of different geometric properties. In the parallelism stimuli, the odd quadrant differs from the others in line slope, while in the orientation stimuli, the odd quadrant contains identical line segments but differs in the direction pointed by their angles. The faster detection of the odd quadrant in the parallelism stimuli compared to the orientation stimuli has traditionally been interpreted as evidence supporting line-based models of shape coding. However, as Chen (1986, 2005) proposed, the concept of invariants over transformations offers a different interpretation: in the parallelism stimuli, the fact that line segments share the same slope essentially implies that they are parallel, and the discrimination may be actually based on parallelism. This reinterpretation suggests that the superior performance with parallelism stimuli reflects the relative perceptual salience of parallelism (an affine invariant property) compared to the orientation of angles (a Euclidean invariant property).</p>
<p>In the collinearity and orientation tasks, the odd quadrant and the other quadrants differ in their corresponding geometries, such as being collinear versus non-collinear. However, in the parallelism task, participants could rely either on the non-parallel relationship between the odd quadrant and the other quadrants or on the difference in line slope to complete the task, which can be seen as effectively similar to the manipulation in the orientation stimuli, as you pointed out. Nonetheless, this set of stimuli and the associated paradigm have been used in prior studies to address questions about Klein’s hierarchy of geometries (Chen, 2005; Wang et al., 2007; Meng et al., 2019). Given its historical significance and the importance of ensuring comparability with previous research, we adopted this set of stimuli despite its imperfections. Other limitations of this paradigm are discussed in the Results section (“The paradigm of ‘configural superiority effects’ with reaction time measures”), and optimized experimental designs were implemented in Experiment 2, 3, and 4 to produce more reliable results.</p>
<disp-quote content-type="editor-comment">
<p>(3) I wondered if the results would hold up for stimuli that were more diverse. It seems that a determined experimenter could easily design an &quot;adversarial&quot; version of these experiments for which the results would be unlikely to replicate. For instance: In the orientation group in Experiment 1, what if the odd-one-out was rotated 90 degrees instead of 180 degrees? Intuitively, it seems like this trial type would now be much easier, and the pattern observed here would not hold up. If it did hold up, that would provide stronger support for the authors' theory.</p>
<p>It is not enough, in my opinion, to simply have some confirmatory evidence of this theory. One would have to have thoroughly tested many possible ways that theory could fail. I'm unsure that enough has been done here to convince me that these ideas would hold up across a more diverse set of stimuli.</p>
</disp-quote>
<p>Thanks for your nice suggestion to validate our results using more diverse stimuli. However, the limitations of Experiment 1 make it less suitable for rigorous testing of diverse or &quot;adversarial&quot; stimuli. In addition to the limitation discussed in response to (2), another issue is that participants may rely on grouping effects among shapes in the quadrants, rather than solely extracting the geometrical invariants that are the focus of our study. As a result, the reaction times measured in this paradigm may not exclusively reflect the extraction time of geometrical invariants but could also be influenced by these grouping effects.</p>
<p>Therefore, we have shifted our focus to the improved design used in Experiment 2 to provide stronger evidence for our theory. Building on this more robust design, we have extended our investigations to study location generalization (revised Experiment 3) and long-term learning effects (revised Figure 6—figure supplement 2). These enhancements allow us to provide stronger evidence for our theory while addressing potential confounds present in Experiment 1.</p>
<p>While we did not explicitly test the 90-degree rotation scenario in Experiment 1, future studies could employ more diverse set of stimuli within the Experiment 2 framework to better understand the limits and applicability of our theoretical predictions. We appreciate this suggestion, as it offers a valuable direction for further research.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Major comments:</p>
<p>- A concise introduction to the Erlangen program, geometric invariants, and their structural stability would greatly enhance the paper. This would not only clarify these concepts for readers unfamiliar with them but also provide a more intuitive explanation for the choice of tasks and stimuli used in the study.</p>
<p>- I recommend adding a section that discusses how this new framework aligns with previous observations in VPL, especially those involving more classical stimuli like Gabors, random dot kinematograms, etc. This would help in contextualizing the framework within the broader spectrum of VPL research.</p>
<p>- Exploring how each level of invariant stability transfers within itself would be an intriguing addition. Previous theories often consider transfer within a condition. For instance, in an orientation discrimination task, a challenging training condition might transfer less to a new stimulus test location (e.g., a different visual quadrant). Applying a similar approach to examine how VPL generalizes to a new test location within a single invariant stability level could provide insightful contrasts between the proposed theory and existing ones. This would be particularly relevant in the context of Experiment 2, which could be adapted for such a test.</p>
<p>- I suggest including some example learning curves from the human experiment for a more clear demonstration of the differences in the learning rates across conditions. Easier conditions are expected to be learned faster (i.e. plateau faster to a higher accuracy level). The learning speed is reported for the DNN but not for the human subjects.</p>
<p>- In the modeling section, it would be beneficial to focus on offering an explanation for the observed generalization as a function of the stability of the invariants. As it stands, the neural network model primarily demonstrates that DNNs replicate the same generalization pattern observed in human experiments. While this finding is indeed interesting, the model currently falls short of providing deeper insights or explanations. A more detailed analysis of how the DNN model contributes to our understanding of the relationship between invariant stability and generalization would significantly enhance this section of the paper.</p>
<p>Minor comments:</p>
<p>- Line 46: &quot;it is remains&quot; --&gt; &quot;it remains&quot;</p>
<p>- Larger font sizes for the vertical axis in Figure 6B would be helpful.</p>
</disp-quote>
<p>We thank your detailed and constructive comments, which have significantly helped us improve the clarity and rigor of our manuscript. Below, we provide a response to each point raised.</p>
<disp-quote content-type="editor-comment">
<p>Major Comments</p>
<p>(1) A concise introduction to the Erlangen program, geometric invariants, and their structural stability:</p>
</disp-quote>
<p>We appreciate your suggestion to provide a clearer introduction to these foundational concepts. In the revised manuscript, we have added a dedicated section in the Introduction that offers a concise explanation of Klein’s Erlangen Program, including the concept of geometric invariants and their structural stability. This addition aims to make the theoretical framework more accessible to readers unfamiliar with these concepts and to better justify the choice of tasks and stimuli used in the study.</p>
<disp-quote content-type="editor-comment">
<p>(2) Contextualizing the framework within the broader spectrum of VPL research:</p>
</disp-quote>
<p>We have expanded the Discussion section to better integrate our framework with previous VPL studies that reported generalization, including those using classical stimuli such as Gabors (Dosher and Lu, 2005; Hung and Seitz, 2014; Jeter et al., 2009; Liu and Pack, 2017; Manenti et al., 2023) and random dot kinematograms (Chang et al., 2013; Chen et al., 2016; Huang et al., 2007; Liu and Pack, 2017). In particular, we now discuss the similarities and differences between our findings and these earlier studies, exploring potential shared mechanisms underlying VPL generalization across different types of stimuli. These additions aim to contextualize our framework within the broader field of VPL research and highlight its relevance to existing literature.</p>
<disp-quote content-type="editor-comment">
<p>(3) Exploring transfer within each invariant stability level:</p>
</disp-quote>
<p>In response to this insightful suggestion, we have added a new psychophysics experiment in the revised manuscript (Experiment 3) to examine how VPL generalizes to a new test location within the same invariant stability level. This experiment provides an opportunity to further explore the neural substrates underlying VPL of geometrical invariants, offering a contrast to existing theories and strengthening the connection between our framework and location generalization findings in the VPL literature.</p>
<disp-quote content-type="editor-comment">
<p>(4) Including example learning curves from the human experiments:</p>
</disp-quote>
<p>We appreciate your suggestion to include learning curves for human subjects. In the revised manuscript, we have added learning curves of long-term VPL (see revised Figure 6—figure supplement 2) to track the temporal learning processes across invariant conditions. Interestingly, and in contrast to the results reported in the DNN simulations, these curves show that less stable invariants are learned faster and exhibit greater magnitudes of learning. We interpret this discrepancy as a result of differences in initial performance levels between humans and DNNs, as discussed in the revised Discussion section.</p>
<disp-quote content-type="editor-comment">
<p>(5) Offering a deeper explanation of the DNN model's findings:</p>
</disp-quote>
<p>We acknowledge your concern that the modeling section primarily demonstrates that DNNs replicate human generalization patterns without offering deeper mechanistic insights. To address this, we have expanded the Results and Discussion sections to more explicitly interpret the weight change patterns observed across DNN layers in relation to invariant stability and generalization. We discuss how the model contributes to understanding the observed generalization within and across invariants with different stability, focusing on the neural network's role in generating predictions about the neural mechanisms underlying these effects.</p>
<disp-quote content-type="editor-comment">
<p>Minor Comments</p>
<p>(1) Line 46: Correction of “it is remains” to “it remains”:</p>
</disp-quote>
<p>We have corrected this typo in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(2) Vertical axis font size in Figure 6B:</p>
</disp-quote>
<p>We have increased the font size of the vertical axis labels in revised Figure 8B for improved readability.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>(1) There are many details throughout the paper that are confusing, such as the caption for Figure 4, which does not appear to correspond to what is shown (and is perhaps a copy-paste of the caption for Experiment 1?). Similarly, I wasn't sure about many methodological details, like: How participants made their second response in Experiment 2? It says somewhere that they pressed the corresponding key to indicate which one was the target, but I didn't see anything explaining what that meant. Also, I couldn't tell if the items in the figures were representative of all trials; the stimuli were described minimally in the paper.</p>
<p>(2) The language in the paper felt slightly off at times, in minor but noticeable ways. Consider the abstract. The word &quot;could&quot; in the first sentence is confusing, and, more generally, that first sentence is actually quite vague (i.e., it just states something that would appear to be true of any perceptual system). In the following sentence, I wasn't sure what was meant by &quot;prior to be perceived in the visual system&quot;. Though I was able to discern what the authors were intending to say most times, I was required to &quot;read between the lines&quot; a bit. This is not to fault the authors. But these issues need to be addressed, I think.</p>
</disp-quote>
<p>(1) We sincerely apologize for the oversight regarding the caption for (original) Figure 4, and thank you for pointing out this error. In the revised manuscript, we have corrected the caption for Figure 4 (revised Figure 5) and ensured it accurately describes the content of the figure. Additionally, we have strengthened the descriptions of the stimuli and tasks in both the <italic>Materials and Methods</italic> section and the captions for (revised) Figures 4 and 5 to provide a clearer and more comprehensive explanation of Experiment 2. These revisions aim to help readers fully understand the experimental design and methodology.</p>
<p>(2) We appreciate your feedback regarding the clarity and precision of the language in the manuscript. We acknowledge that some expressions, particularly in the abstract, were unclear or imprecise. In the revised manuscript, we have rewritten the abstract to improve clarity and ensure that the statements are concise and accurately convey our intended meaning. Additionally, we have thoroughly reviewed the entire manuscript to address any other instances of ambiguous language, aiming to eliminate the need for readers to &quot;read between the lines.&quot; We are grateful for your suggestions, which have helped us enhance the overall readability of the paper.</p>
</body>
</sub-article>
</article>