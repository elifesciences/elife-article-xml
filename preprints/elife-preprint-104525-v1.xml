<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">104525</article-id>
<article-id pub-id-type="doi">10.7554/eLife.104525</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104525.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Comparative fMRI reveals differences in the functional organization of the visual cortex for animacy perception in dogs and humans</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-5946-1999</contrib-id>
<name>
<surname>Farkas</surname>
<given-names>Eszter Borbála</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">§</xref>
<email>eszter.borbala.farkas@ttk.elte.hu</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Hernández-Pérez</surname>
<given-names>Raúl</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">§</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cuaya</surname>
<given-names>Laura Veronica</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rojas-Hortelano</surname>
<given-names>Eduardo</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gácsi</surname>
<given-names>Márta</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Andics</surname>
<given-names>Attila</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
    <aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01jsq2704</institution-id><institution>Neuroethology of Communication Lab, Department of Ethology, Institute of Biology, Eötvös Loránd University</institution></institution-wrap>, <city>Budapest</city>, <country country="HU">Hungary</country></aff>
    <aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01jsq2704</institution-id><institution>MTA-ELTE ‘Lendület’ Neuroethology of Communication Research Group, Eötvös Loránd University</institution></institution-wrap>, <city>Budapest</city>, <country country="HU">Hungary</country></aff>
    <aff id="a3"><label>3</label><institution>ELTE NAP Canine Brain Research Group</institution>, <city>Budapest</city>, <country country="HU">Hungary</country></aff>
    <aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03prydq77</institution-id><institution>Social, Cognitive and Affective Neuroscience Unit, Department of Cognition, Emotion, and Methods in Psychology, Faculty of Psychology, University of Vienna</institution></institution-wrap>, <city>Vienna</city>, <country country="AT">Austria</country></aff>
    <aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tmp8f25</institution-id><institution>Facultad de Medicina, Universidad Nacional Autónoma de México</institution></institution-wrap>, <city>Santiago de Querétaro</city>, <country country="MX">Mexico</country></aff>
    <aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01jsq2704</institution-id><institution>Department of Ethology, Institute of Biology, Eötvös Loránd University</institution></institution-wrap>, <city>Budapest</city>, <country country="HU">Hungary</country></aff>
    <aff id="a7"><label>7</label><institution>HUN-REN–ELTE Comparative Ethology Research Group</institution>, <city>Budapest</city>, <country country="HU">Hungary</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Jbabdi</surname>
<given-names>Saad</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes><fn id="n1" fn-type="equal"><label>§</label><p>These Authors contributed equally</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-03-11">
<day>11</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP104525</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-11-12">
<day>12</day>
<month>11</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-11-17">
<day>17</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.11.12.623268"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Farkas et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Farkas et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-104525-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>The animate-inanimate category distinction is one of the general organizing principles in the primate high-level visual cortex. Much less is known about the visual cortical representations of animacy in non-primate mammals with a different evolutionary trajectory of visual capacities. To compare the functional organization underlying animacy perception of a non-primate to a primate species, here we performed an fMRI study in dogs and humans, investigating how animacy structures neural responses in the visual cortex of the two species. Univariate analyses identified animate-sensitive bilateral occipital and temporal regions, non-overlapping with early visual areas, in both species. Multivariate tests confirmed the categorical representations of animate stimuli in these regions. Regions sensitive to different animate stimulus classes (dog, human, cat) overlapped less in dog than in human brains. Together, these findings reveal that the importance of animate-inanimate distinction is reflected in the organization of higher-level visual cortex, also beyond primates. But a key species difference, that neural representations for animate stimuli are less concentrated in dogs than in humans suggests that certain underlying organizing principles that support the visual perception of animacy in primates may not play a similarly important role in other mammals.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>General principles that govern the organization of the high-level visual cortex as well as several functional specializations were described in visually oriented, highly social primate species (<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c5">5</xref>). The animate-inanimate category distinction is one of the general organizing principles (<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>), resulting in widely distributed animacy representations in the human high-level visual cortices. These representations show various diagnostic features of animacy, including mid-level features such as curvature, face-like and body-like features, and features reflecting capacity for movement and agency (<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c10">10</xref>). The animate-inanimate distinction is also represented in homologous cortical areas of non-human primates (<xref ref-type="bibr" rid="c11">11</xref>).</p>
<p>Much less is known about the functional organization of non-primary visual areas of non-primate mammals, for which vision is secondary to other sensory modalities, with its significance not having increased as dramatically during evolution as for certain primates (<xref ref-type="bibr" rid="c3">3</xref>). For non-primates that are known to use vision for social interaction processing, behavioral evidence suggests that animacy cues are relevant. Cats and dogs distinguish biological motion from other forms of motions (<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>) and other animacy cues such as self-propulsion and speed changes can lead to an orienting response in dogs (<xref ref-type="bibr" rid="c14">14</xref>). When viewing natural scenes, dogs focus their gaze on animate entities, especially on their heads, more than on inanimate entities (<xref ref-type="bibr" rid="c15">15</xref>). Although recent neuroscientific works suggest a handful of high-level visual specializations in those non-primate species that primarily use vision for social interaction processing (<xref ref-type="bibr" rid="c3">3</xref>), data on how the representations of animate and inanimate entities are organized in non-primate visual cortices remains scarce.</p>
<p>Neuroimaging studies of non-primates directly comparing the visual processing of animate and inanimate entities have so far only been conducted with dogs. Dogs that live as companions constitute a special case for comparison to humans, even with their lower visual acuity and different color vision (<xref ref-type="bibr" rid="c16">16</xref>): dogs’ visual social environment is shared with humans, and they are the only non-human species that can be tested in an awake, unrestrained state (<xref ref-type="bibr" rid="c17">17</xref>). Three recent fMRI studies compared dog’ and humans’ high-level visual functions related to different aspects of animacy but remained inconclusive. Phillips and colleagues (<xref ref-type="bibr" rid="c18">18</xref>) found that a classifier distinguishing representations of dynamic animate and inanimate objects in humans failed to make the same distinction in dogs. In contrast, using static images, Boch and colleagues (<xref ref-type="bibr" rid="c19">19</xref>) identified body sensitivity as a factor accounting for animacy organization in the dog visual cortex. Finally, looking for representations of another diagnostic feature of animacy, faces, Bunford, Hernández-Pérez and colleagues (<xref ref-type="bibr" rid="c20">20</xref>) found areas exhibiting sensitivity for dog and human faces in humans but not in dogs. Together, these studies indicated that, similarly to the human brain, the dog brain exhibits sensitivity for certain aspects of animacy, but its extent and similarity to animacy-sensitivity in humans is still unclear.</p>
<p>To better understand the similarities and differences in how animacy structures higher-level visual perception in evolutionarily distant mammal species, we conducted a directly comparative dog-human fMRI study, in which participants viewed short videos of moving animate (dog, human, cat) and inanimate (car) entities. We selected stimuli to be therefore relevant to both species, and dynamic to involve motion-based features of animacy. To test animacy-sensitivity, we measured and compared neural response patterns elicited by these different stimulus classes. We predicted that if the animate-inanimate distinction is one of the general organizing principles of the dog visual cortex as it is in humans, we will find stronger responses to animate stimuli, and distinct response patterns to animate and inanimate stimuli, in non-primary visually responsive areas in the dog brain. Furthermore, we expected that if specific animate stimulus classes share some animacy features that are central for the functional organization of the human but not of the dog visual cortex – perhaps because dogs’ visual-social perception is far less detailed than humans’ –, then the neural responses for dog, human and cat stimuli would be less similar to each other and therefore overlap less in dogs than in humans.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Participants</title>
<p>15 family dogs (average (range) age of 7.1 (3-12.6); 7 spayed females, 1 intact-, 7 neutered males), and 13 humans (average age (range) of 31.5 (21-42), 7 females, 6 males) participated in the experiment. All dogs (7 Border Collies, 2 Golden Retrievers, 1 Australian Shepherd, 1 Labradoodle, and 4 mixed breeds) were mesocephalic and were trained to remain still inside the scanner (<xref ref-type="bibr" rid="c21">21</xref>). All human participants had normal or corrected to normal visual acuity and participated voluntarily. Dog owners and humans were recruited from the participant pool of the Department of Ethology at Eötvös Loránd University in Budapest, Hungary. All procedures were in accordance with relevant guidelines and regulations. Procedures involving dogs were approved by the Food Chain Safety and Animal Health Directorate Government Office, Hungary. Procedures involving humans were approved by the Committee of Scientific and Research Ethics (ETT-TUKEB), Hungary. Dog owners and human participants signed an informed consent.</p>
</sec>
<sec id="s2b">
<title>Experimental Design</title>
<p>The stimuli consisted of natural videos of dogs, humans, cats, and cars, some showing the whole body or object, some only parts, through close-up or wider shots (<xref rid="fig1" ref-type="fig">Figure 1</xref> and Supplementary Video S1). The dog (D), human (H), and cat (C) videos constituted the animate (A) stimuli. We selected these animate stimulus classes to have an identical stimulus set for dogs and humans that involves familiar species to maximize the attention of dog subjects: the conspecific and two heterospecific species. Cars were selected as inanimate (iA) stimuli that move, vary in color and shape and all dog participants had ample experience with them. A reason to use a restricted number of stimulus classes was ensuring enough repetitions of each condition to elicit robust neural response in dogs where representations of categories are not established. All the videos were downloaded from YouTube (available under a Creative Commons License), cut to match the length required and resized to a final resolution of 1024 × 576 pixels. The videos displayed various situations and backgrounds using natural colors to minimize the possibility of systematic differences in visual properties across conditions and maximize ecological validity. To balance the differences in visual properties, brightness, contrast, hue, saturation, and motion for every video was calculated and videos were selected so that there were no significant differences in these properties between conditions in any of the runs (one-way ANOVA, ps &gt; 0.05). To account for other low-level visual properties, captured by Gabor filters, the within- and between-category correlation of the outputs of the HMAX model’s C1 units (<xref ref-type="bibr" rid="c22">22</xref>) were compared and no differences were found for any of the categories (two-sample t test, ps &gt; 0.05).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
    <caption><title>Experimental paradigm.</title>
    <p>Still images from two sample video stimuli representing each of the three animate (yellow) conditions: dog (blue), human (pink), cat (green) and the inanimate condition: car (grey) and the design of a single run. Each 310 s long run was composed of 12 videos (3.5-5.5 s, mean 4.5 s) of each of the four conditions followed by an interstimulus interval (0.7-2.5 s, mean 1.7 s). Stimulus design was identical for dog and human subjects. See also Video S1.</p></caption>
<graphic xlink:href="623268v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We used an event-related design (<xref rid="fig1" ref-type="fig">Figure 1</xref>). Each run contained different stimuli, 12 exemplars from each of the four conditions. The length of the videos varied between 3.5 and 5.5 s (mean 4.5 s). Each video was preceded by a grey screen with variable duration of 0.7 - 2.5 s (mean 1.7 s). The length and variability of stimulus duration and interstimulus interval were chosen to minimize predictability but maximize signal quality (<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>). Runs ended with an additional 10 s of grey screen and lasted 310 s. The order of the videos within a run was randomized for every participant separately. Thirteen dogs completed six runs and two dogs completed only three, all humans completed six runs. Dogs were tested in two runs per session with rest periods of at least 20 min between sessions and a maximum of four runs per day. The dogs’ eyes were observed through an MR-compatible camera during the run to confirm they were awake during scanning. Four runs were discarded and repeated due to the dog falling asleep. Humans were tested in one or two days and in one session with 3 to 6 runs per day depending on the availability of the scanner and the participant.</p>
<p>Videos were presented using an MR compatible NordicNeuroLab LCD Monitor and controlled using Psychophysics Toolbox. Dogs viewed the screen directly, while humans viewed it through a mirror. The screen was set up at 155 cm from the head of the participants; the videos covered the entire height (49.8 cm) and width (88.6 cm) of the screen.</p>
</sec>
<sec id="s2c">
<title>fMRI Data Acquisition and Preprocessing</title>
<p>Data acquisition was performed at the Medical Imaging Centre at the Semmelweis University on a Philips 3T Ingenia scanner. For dogs, a Philips dStream Pediatric Torso 8ch coil and for humans, a Philips dStream Head 32ch coil was used. Functional data were collected using a gradient-echo-planar imaging (EPI) sequence (dogs and humans: TR=2500 ms, TE=20 ms, flip angle=90°, 2.5 mm-thick slices with 0.5 mm gap; dogs: field of view: 200×150×120 mm, acquisition matrix 80×58; 40 axial slices; humans: field of view: 240×240×122 mm, acquisition matrix 96×94; 41 axial slices). 124 volumes were acquired in each run. Anatomical data was collected using a T1-weighted 3D TFE sequence, with 1×1×1 mm resolution (dogs: at a separate session; humans: the end of the last functional imaging session). We acquired 180 slices covering the whole brain for anatomical localization.</p>
<p>Image preprocessing was conducted with FSL version 4.19 (Jenkinson et al., 2012). Statistical analysis of MRI data was performed using FEAT (FMRI Expert Analysis Tool) Version 6.00, part of FSL (FMRIB’s Software Library, <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl">www.fmrib.ox.ac.uk/fsl</ext-link>), time-series statistical analysis using FILM (<xref ref-type="bibr" rid="c25">25</xref>), and representational similarity analysis using PyMVPA software package (<xref ref-type="bibr" rid="c26">26</xref>) and MATLAB (2018b) custom code.</p>
<p>Volumes were motion-corrected and filtered using a 128 s high-pass filter. Scan-to-scan movement was calculated using framewise displacement (FD) (<xref ref-type="bibr" rid="c27">27</xref>), all the volumes where FD exceeded 1 mm were excluded (dogs 0.17%; humans 0%). The mean FD across subjects and runs was 0.07 mm for dogs and 0.05 mm for humans. Dog and human images were skull-stripped.</p>
<p>For dogs, we calculated a mean functional image that comprised all the volumes of all the runs for a given subject. This mean functional image was then manually transformed into a dog anatomic template (<xref ref-type="bibr" rid="c28">28</xref>) using 12 degree-of-freedom affine transformation, and the transformation matrix was used to transform all the volumes into the template space. Human images were first registered to the anatomical scan using a 7 degree-of-freedom linear transformation, then to the Montreal Neurologic Institute MNI template using 12 degree-of-freedom affine transformation. Next, images were smoothed (Gaussian kernel, FWHM 5 mm) for both dogs and humans.</p>
</sec>
<sec id="s2d">
<title>fMRI Statistical Analysis</title>
<sec id="s2d1">
<title>RSA Comparing Neural and an EVC Model</title>
<p>To map neural responses to low-level visual properties, we assessed, using RSA, where the representational geometry of the stimuli in the dog and human brains was similar to how an early visual cortex (EVC) model represent them.</p>
<p>First, representational dissimilarity matrices (DSM) of the stimulus set were calculated to characterize the pairwise dissimilarity of the response patterns elicited by the stimuli (Kriegeskorte, 2009). For this, each stimulus event was modelled convolving its times series with the canonical hemodynamic response function and the resulting, normalized β values were used to describe every voxel by how dissimilar a sphere of voxels (r=2 voxels for dogs and r=3 voxels for humans) around it represents each pair of the stimuli. Dissimilarity was measured as the correlation distance (1 - Pearson correlation). DSMs were calculated this way for each run of every participant and then were averaged across runs and participants. In the case of dogs, this process was repeated for the whole brain. In the case of humans, we restricted the analysis to the occipital and temporal lobes (delineated based on MNI atlas) where the visual areas are, to reduce the computation load.</p>
<p>Second, the stimuli were presented to the computational model HMAX (<xref ref-type="bibr" rid="c22">22</xref>). In its first and second layers, the HMAX model simulates the selective tuning of neurons in primary visual cortex (V1) and has been shown to match with the activity patterns of the EVC of the primate brain (<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>). Specifically, we used the second layer of the model (referred here as EVC model), which corresponds to V1 complex cells. This layer yields a response vector of a V1 unit by performing a 2-dimensional convolution of a Gabor filter with an image. The algorithm was applied to each frame of each video using a set of Gabor filters with four different orientations (−45°, 0°, 45°, 90°) and 17 sizes (7 to 39 pixels in 2-pixel steps) per orientation, modeling 68 V1 units for each frame of a video. To obtain a unidimensional response vector for every video stimulus, principal component analyses were performed within the response vectors of every V1 unit across all frames and for all stimuli. The first principal components were used in the following step, because those contained the most explained variance for the given video. Next, the DSM of response vectors was calculated to assess the representational geometry expected based on our stimuli’s low-level visual features.</p>
<p>Third, to compare the brain representation and the model, correlation maps were created, in which each voxel was assigned the Pearson correlation between neural DSM and model DSM. To account for the spatial dependency of the searchlight approach (<xref ref-type="bibr" rid="c31">31</xref>) and to reduce the number of assumptions on the maps, we used a random permutation method (<xref ref-type="bibr" rid="c32">32</xref>) for group-level analysis and statistical inference.</p>
<p>The stimulus labels were randomly swapped, a random DSM map and then the correlation map between the random DSM map and the DSM of the model were created. This process was repeated 1,000,000 times. Next, the maps were averaged to calculate the correlations expected by chance and their standard deviations. These parameters were then used to transform the original correlation maps to Z-score maps. The maps were then filtered to maintain only the voxels with a Z-score &gt; 3.1 (equivalent to p &lt; 0.001). The random correlation maps were also filtered at Z-score &gt; 3.1. Using these maps, the cluster size distribution expected by chance was calculated and this distribution was used to filter out the clusters with a size smaller than the size expected by chance (p &lt; 0.05).</p>
</sec>
<sec id="s2d2">
<title>General Linear Model (GLM)</title>
<p>To identify regions that are sensitive to our conditions, univariate analysis with the four conditions (D, H, C, iA) as regressors were conducted using the FSL’s implementation of the General Linear Model.</p>
<p>The first-level individual models included the movement parameters as regressors of no interest. The regressors were convolved with the canonical hemodynamic response function, then using all six runs, condition effects were estimated for each participant. At the group level, a random-effects analysis was performed for each species.</p>
<p>For further analyses, the visually responsive regions of the dog and human cortex were identified by comparing all conditions to the implicit baseline. To determine the brain regions sensitive to animate stimuli, the videos from all three animate conditions (D, H, C) were grouped and contrasted with the inanimate condition (A&gt;iA contrast). To further characterize the animacy-sensitivity of the dog and human brains, each of the three animate conditions was contrasted with the inanimate condition separately (D&gt;iA, H&gt;iA, C&gt;iA, specific contrasts).</p>
<p>For all contrasts, the resulting Z-statistic (Gaussianised T/F) images were thresholded at Z &gt; 3.1 and the threshold for cluster-correction was p = 0.05 (Worsley, 2001).</p>
<sec id="s2d2a">
<title>Response profiles</title>
<p>To further characterize the A&gt;iA contrast results, for every subject and for each group level-derived peak, first the parameter estimates (β weights) for each condition in a sphere (radius = 3 voxels) centered around the peak was assessed using FSL’s Featquery tool. Second, these parameter estimates were compared on the group level, using t tests.</p>
</sec>
<sec id="s2d2b">
<title>Overlap calculation</title>
<p>To determine and compare across species the extent to which specific contrast maps overlap, first, the visually response cortex was determined in both species by contrasting all conditions to baseline using a cluster threshold of Z&gt;3.1 and a corrected cluster significance threshold of p=0.05. Then, the number of suprathreshold voxels were assessed for each specific contrast as well as the number of voxels in the visually responsive cortex in dogs and in humans. Then, the percentage of those visually responsive voxels that were suprathreshold in (<xref ref-type="bibr" rid="c1">1</xref>) all three specific contrasts (3-overlap) or (<xref ref-type="bibr" rid="c2">2</xref>) two of the three specific contrasts (2-overlap), were calculated, and the percentages of both 3-overlapping and 2-overlapping voxels were compared between dogs and humans, using t tests. <italic>Species-preference</italic>. We also assessed the proportions of animate-sensitive voxels that responded strongest to conspecific stimuli or to any of the heterospecific stimulus classes. For that, each animate condition (D, H, C) was contrasted with the implicit baseline, and the average Z-scores per contrast were calculated across all dog and across all human participants in every animate-sensitive voxel. The condition from the contrast with the highest average Z-score was assigned as the preferred species for each voxel. To determine whether the percentage of voxels differed above chance (0.33) for the three conditions, permutation test was performed by randomly swapping the labels of the stimuli, calculating the preference for each voxel and their percentages 1,000,000 times.</p>
</sec>
</sec>
<sec id="s2d3">
<title>Category- and Class-boundary Effect Tests</title>
<p>To identify regions with greater representational dissimilarity for between-category (i.e., animate-inanimate) than within-category (i.e., animate-animate) stimulus pairs (category boundary effect test as described in Kriegeskorte et al., 2008), we calculated DSMs per run, per participant for both between-category and within-category pairs across the brain (for dogs: whole brain, for humans: temporal and occipital cortices), using a searchlight approach (sphere with r=2 voxels for dogs, and r=3 voxels for humans). DSMs were then averaged across runs and participants, and a difference map was calculated. To determine whether the difference was significant at the voxel level, we repeated the process 1,000,000 times but randomly swapping stimulus labels, thus generating 1,000,000 permuted difference maps. We transformed each voxel of the original difference map to Z-score by comparing it with the corresponding voxel in the permuted difference maps, generating a Z-score map. To determine cluster size threshold, we repeated the same process for the permuted difference maps, thus generating 1,000,000 permuted Z-score maps. All Z-score maps were thresholded at Z &gt; 3.1. We calculated the cluster size distribution in the permuted Z-score maps and used it to determine the cluster size expected by chance in the original Z-score map. All clusters with a size smaller than expected by chance (p&lt;0.05) were excluded. Category-boundary tests were performed in a similar way for between-class (e.g., dog-car) vs. within-class (e.g., dog-dog) stimulus pairs.</p>
<sec id="s2d3a">
<title>Overlap calculation</title>
<p>To determine and compare across species the extent to which class-boundary tests overlap, first, the results were binarized. Then, the percentage of the voxels active in (<xref ref-type="bibr" rid="c1">1</xref>) all three classes (3-overlap) or (<xref ref-type="bibr" rid="c2">2</xref>) two of the three classes (2-overlap), were calculated, and the percentages of both 3-overlapping and 2-overlapping voxels were compared between dogs and humans, using t tests.</p>
</sec>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>RSA Comparing Neural and an EVC Model</title>
<p>For results in dogs and humans, see <xref rid="fig2" ref-type="fig">Figure 2</xref> and Supplementary Table S1. We found a significant correlation with the early visual cortex model in clusters in the occipital cortex of dogs and humans. In dogs, the cluster in the right hemisphere was centered in the marginal gyrus (MG) and the cluster in the left hemisphere was centered in the splenial gyrus (SpG). In humans, the cluster extended bilaterally, with main peaks in the bilateral calcarine fissure (CAL).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
    <caption><title>RSA comparing neural and EVC model representations.</title>
    <p>A, DSM for the EVC model. B, Results for correlation with the EVC model in dogs (n = 15) and humans (n = 13) superimposed on a template brain (for dogs: Czeibert et al., 2019; for humans: MNI template brain), using a cluster threshold of Z&gt;3.1 and a corrected cluster significance threshold of p=0.05 in a searchlight analysis (sphere radius = 3 voxels for dogs and 3 voxels for humans). L=left; R=right; EMG=ectomarginal gyrus; SpG=splenial gyrus; mSSG=mid suprasylvian gyrus; MG=marginal gyrus; CAL=calcarine fissure and surrounding cortex.</p></caption>
<graphic xlink:href="623268v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3b">
<title>GLM</title>
<p>For visual responsiveness (all stimuli &gt; implicit baseline) results in dogs and humans, see Supplementary Table S2. For GLM results for each contrast (A&gt;iA, D&gt;iA, H&gt;iA, C&gt;iA) in dogs and humans, see <xref rid="fig3" ref-type="fig">Figure 3</xref> and Supplementary Table S3. In both species, these contrasts revealed clusters that extended mainly through the temporal and occipital lobes. Specifically, in dogs, for A&gt;iA, we found bilateral clusters in the mid suprasylvian gyrus (mSSG), extending caudally in the left hemisphere, and bilateral clusters in the ectomarginal gyrus (EMG); for D&gt;iA, left SpG extending to mSSG, and a right cluster in the suprasylvian gyrus (SSG); including mid and caudal portions; for H&gt;iA, left clusters including the mid ectosylvian gyrus (mESG), the mSSG, and the SpG, and a right cluster including mid and caudal portions of the SSG; and for C&gt;iA, a left cluster in the caudal part of the SSG. In humans, for all four contrasts, we found clusters bilaterally, involving portions of the inferior temporal gyrus (ITG), middle temporal gyrus (MTG), inferior occipital gyrus (IOG) and the fusiform gyrus (FFG), typically extending more to the temporal lobe in the right hemisphere. Activity response profiles for GLM-derived A&gt;iA peaks in dogs and humans are shown in <xref rid="fig3" ref-type="fig">Figure 3B</xref>.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
    <caption><title>GLM results in dogs and humans.</title>
    <p>A, Dog and human contrast maps superimposed on a template brain, using a cluster threshold of Z&gt;3.1 and a corrected cluster significance threshold of p=0.05. Upper row: A &gt; iA (yellow), lower rows: D &gt; iA (blue), H &gt; iA (pink), C &gt; iA (green). B, Response profiles. Bar graphs represent parameter estimates (beta weights) in select GLM-derived peaks (sphere radius = 3 voxels) for each condition. C, Activity proportions and overlaps. Horizontal bars (and percentages) represent the proportions of suprathreshold voxels per condition, relative to the total voxel number, within the visually responsive cortex; vertical overlaps represent activity overlaps. D, Overlap calculation results. The percentage of suprathreshold voxels within the visually responsive cortex for which at least two conditions overlap and in which all three overlap. E, Species-preference. Proportion of animate-preferring voxels for which the strongest response was found for the given condition. L=left; R=right; OG = occipital gyrus; mSSG = mid suprasylvian gyrus; cSSG = caudal suprasylvian gyrus; cSG = sylvian gyrus; SpG = splenial gyrus; mESG = mid ectosylvian gyrus; cCG = caudal composite gyrus; FFG = fusiform gyrus; HC=hippocampus; IOG = inferior occipital gyrus; ITG=inferior temporal gyrus; MTG = middle temporal gyrus. *: p&lt;0.01, +: p=0.062. Error bars represent SE.</p></caption>
<graphic xlink:href="623268v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<sec id="s3b1">
<title>Overlap calculation</title>
<p>Comparing the proportion of overlapping activities within the visually responsive cortex, we found that in the dog brain there was a significantly lower proportion of voxels than in the human brain in which all three (D, H, C) contrast maps overlapped at z=3.1 (dogs: 2%, humans: 31%) (t(14.781) = 12.044, p &lt; 0.01). The proportion of overlapping voxels between at least two categories out of the three was also lower in the dog brain compared to that of the human brain (dogs: 11%, humans: 51%) (t(25.925) = 11.236, p &lt; 0.01) (<xref rid="fig3" ref-type="fig">Figure 3D</xref>).</p>
<p><italic>Species-preference</italic>. Analyses of the extent to which voxels in the animate-inanimate contrast responded stronger to dog or human or cat stimuli indicated that in dogs, the majority of animate-preferring voxels (87%) responded strongest to conspecific (dog) stimuli (likelihood of obtaining the observed proportions by chance, using permutation testing: p = 0.004). In humans, the majority of animate-preferring voxels (67%) responded strongest to conspecific (human) stimuli (permutation testing confirmed a marginally significant effect, p = 0.062) (<xref rid="fig3" ref-type="fig">Figure 3E</xref>).</p>
</sec>
</sec>
<sec id="s3c">
<title>Category- and Class-boundary Effect Test</title>
<p>For category- and class-level boundary test results in dogs and humans, see <xref rid="fig4" ref-type="fig">Figure 4</xref> and Supplementary Table S4. We found temporal and occipital clusters, located similarly to those identified by univariate animacy-sensitivity contrasts. Specifically, in dogs, for A&gt;iA, we found bilateral mSSG/mESG clusters; for D&gt;iA, a left cluster centered in the caudal suprasylvian gyrus (cSSG) and an mSSG-centered right cluster; for H&gt;iA, left clusters in cSSG and caudal ectosylvian gyrus (cESG), and right clusters in EMG and rostral sylvian gyrus (rSG); and for C&gt;iA, a left cluster in mESG, and a right SpG cluster, both extending caudally. In humans, for all contrasts, we found large clusters in both hemispheres, extending from MTG to ITG and IOG, and ventrally to FFG, with similar peaks across contrasts.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
    <caption><title>Category boundary test results in dogs and humans.</title>
    <p>A, Dog and human results superimposed on a template brain using a cluster threshold of Z&gt;3.1 and a corrected cluster significance threshold of p=0.05. B, Activity proportions and overlaps. Horizontal bars (and percentages) represent the proportions of suprathreshold voxels per condition, relative to the total voxel number, within the visually responsive cortex; vertical overlaps represent activity overlaps. C, Overlap calculation results. The percentage of suprathreshold voxels within the visually responsive cortex for which at least two conditions overlap and in which all three overlap. L=left. *: p&lt;0.01. Error bars represent SE.</p></caption>
<graphic xlink:href="623268v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<sec id="s3c1">
<title>Overlap calculation</title>
<p>Proportion of overlapping voxels within the visually responsive cortex were significantly lower in dogs than in humans, both in case all three (D, H, C) similarity maps overlapped (dogs: 1%, humans: 26%) and in case at least two of the three similarity maps overlapped (dogs: 14%, humans: 47%), (t(12.217) = 8.518, p &lt; 0.01 and t(20.981) = 9.405, p &lt; 0.01) (<xref rid="fig4" ref-type="fig">Figure 4C</xref>).</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>This study investigated how animacy structures neural responses in the visually responsive cortex of dogs and humans. We found both similarities and differences across the two species. Univariate analyses identified bilateral occipital and temporal regions in both species responding stronger to animate than inanimate stimuli. These animate-sensitive regions are distinct from functionally determined early visual areas (i.e., that represented the stimuli similarly to an EVC model). Most animate-sensitive peaks preferred all animate stimulus classes over inanimate stimuli, but more animate-sensitive voxels responded stronger to conspecific than to any of the heterospecific stimuli in dogs, revealing conspecific-preference. A similar trend was observed in humans. The areas sensitive to the specific class of animate stimuli, dog, human and cat, overlapped less in dog than in human brains. Multivariate tests revealed regions in both species in which the representational geometry of stimulus pairs within animate and within inanimate categories were more similar than that of stimulus pairs crossing the animate-inanimate boundary. Such boundary effects were identified in both species for dog, human and cat stimuli as well, and these overlapped less in dogs than in humans. The regions exhibiting these categorical representations for animate stimulus classes largely overlapped with univariate animacy-sensitive clusters.</p>
<p>The similarities between dog and human neural response patterns, namely animacy-sensitivity and category boundary effects in visual cortical regions, demonstrate that animacy is an organizing principle in the visual perception of both species, and the representations of animate entities exhibit categorical structure. Categorical representation has already been shown across visual domains in multiple primate species. Going beyond these previous results, the present work suggests that animacy and the subordinate categorical feature of the input may determine the location and the structure of the neural response not only in primates’, but at least in some non-primate mammals’ visual perception too. We also note, however, that as animacy is multidimensional in nature (<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c10">10</xref>), dogs’ and humans’ animacy sensitivity, as reported here, do not necessarily originate from the representation of the same animacy dimensions. Nevertheless, the present findings provide evidence that dogs’ visual cortex contains functionally organized areas, as primates’ does.</p>
<p>The animacy-sensitive regions reported here correspond to but extend beyond those reported previously (<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c19">19</xref>). Specifically, for dogs, we found that animacy sensitivity characterizes 22% of the visually responsive cortex, including regions bilaterally in the EMG and mSSG. Using static stimuli, Boch et al. (2023) found similarly located, although less extensive regions as sensitive for animate (specifically, body and face) stimuli. Various factors may have contributed to this difference between studies, including the use of dynamic stimuli in the present study, as these stimuli also carried motion-related animacy cues (<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c34">34</xref>), but also scanning-technical factors. Prior results on dog neural responses to (human) faces vs. objects, potentially also driven by animacy sensitivity, also suggested the involvement of bilateral temporal regions (<xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>). For humans, animacy sensitivity characterized 20,7% of the visually responsive cortex and included extended regions in the higher-level visual cortex. Whereas previous works often focused on specific aspects of animacy (such as face, body, humanness, agency, biological motion), our highly natural stimuli carried multiple aspects of animacy at the same time, and this could have resulted in the finding of more extensive regions here (<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c11">11</xref>).</p>
<p>Beyond similarities, differences in animacy representations were also found between dogs and humans. The main species difference, that the detected neural activity evoked by animate stimulus classes overlapped less in dogs than in humans, may reflect how evolution drove mammalian brain organization for visual perception on a global level. Adaptive visual-social behavior and underlying neural representations shaped by relevant behavioral goals differ substantially between primates and (even visually oriented) non-primate mammals. The greater emphasis on visual social communication in primates, their more nuanced social interactions and within-face movements necessitate the efficient processing of bodily detail-encoding social cues and signals beyond body postures more than in non-primates. Indeed, in non-primates the relative role of perception and interpretation of whole-body actions is arguably greater. Therefore, the behavioral relevance of our stimuli showing moving dogs, humans and cats may have differed for human and dog subjects. Dogs’ focus during the visual perception of animate stimuli may be on bodily actions while humans’ rather on faces and gestures. This notion is supported by previous canine neuroimaging work: On one hand, these studies found no convincing evidence for the existence of face-sensitive areas in dogs, and body sensitivity, weak and restricted to small regions, has been reported for this species in a single study so far. On the other hand, Phillips et al. (<xref ref-type="bibr" rid="c18">18</xref>) found that representations of different actions of animate entities were separable in the dog brain, but the representations of animate and inanimate objects were not. Therefore, the pattern observed in the present study suggests that animacy sensitivity in the dog brain does not primarily stem from the common presence of face and body across all animate stimulus classes, but rather from the representation of biological motion. And it is possible that the representations of different biological motions in dogs are less generalized across the different animate stimulus classes than face and body representations in humans. These findings are in line with the proposal that dogs’ high-level visual cortex may contain distinct agent-responsive areas, and also with the specific sensitivities that have been described in these areas (Boch et al., 2024). Future research should assess the validity of this potential explanation behind the species differences in animacy perception.</p>
<p>The finding on conspecificity-preference in dogs, that the majority of animate-sensitive voxels responded maximally to conspecific stimuli, and a similar tendency in humans, corroborate and extend previous results. Recently, Bunford, Hernández-Pérez et al. (2020) identified conspecific-preferring clusters in the visually responsive cortex in both species. In that paper, in humans, conspecific-preference was restricted to face-sensitive regions. The present findings, using more naturalistic and more variable videos, support these previous results and suggest that conspecific-preference may be characteristic for animate-sensitive visual regions, at least in dogs. Conspecific-preference, therefore, may be present at different stages of visual processing of faces and animacy. This suggests that the bias for processing conspecific stimuli does not reflect a functional specialization but rather a more generalized preference, perhaps driven by attention or motivational relevance (<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c38">38</xref>).</p>
<p>This work has a few potential limitations. First, inanimate stimuli consisted of cars only, and animate stimuli included only a narrow but highly relevant subset of all animate entities that dogs and humans may encounter, which selections, for reasons detailed in the Methods may also have affected the generalizability of the animacy sensitivity results. Potential differences in the level of relevance between animate and inanimate conditions may have also been present: for example, human stimuli may have been more relevant than cars to dogs. The similarity of the cortical locations of our animacy sensitivity results to previous findings in both dogs (<xref ref-type="bibr" rid="c19">19</xref>) and humans (<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>), however, suggests that it was indeed the presence of animacy in dog, human and cat stimuli (and its lack in car stimuli) that drove the measured differences in neural activity. Furthermore, although our stimuli did not involve a wide variety of animate classes, they, being videos, carried not only static but also action-related diagnostic features of animacy, in a highly natural manner.</p>
<p>Another potential limitation of any directly comparative visual study of dogs and humans may stem from their different visual capacities: dogs perceive less details of visual stimuli presented on a monitor than humans, due to their different color vision, lower critical flicker-fusion threshold, lower depth perception, and lower visual acuity [missing references]. As a consequence, dogs may have seen our stimuli as more similar than humans have, and this may have affected neural response patterns. While we cannot exclude such effects, we do not think that visual capacity differences between dogs and humans confounded the key species difference presented here, that is the smaller overlap of condition-specific clusters for dogs. On the one hand, it can be assumed that visual capacities influenced the perception of animate and inanimate stimuli similarly. On the other hand, the less detailed visual perception of dogs may lead to less differentiated and thus more overlapping representations of the conditions, relative to those in humans, but not to less overlapping ones, as was the case here.</p>
<p>In conclusion, the findings presented here suggest that the importance of the animate-inanimate distinction may be reflected in the organization not only of primate, but more generally of mammalian higher-level visual cortex. The key species difference, that neural representations for animate stimuli cluster less in dogs than in humans demonstrates that different animate stimulus classes may not form a unified category in dogs and suggests that the dog brain may lack those functional specializations that drive the unified response in humans. To understand the principles that organize the visual perception of animate objects in non-primate mammals, other factors need to be considered than those underlying the animacy organization of the primate visual cortex.</p>
</sec>
</body>
<back>
<sec id="s6">
<title>Supplementary information</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Supplementary Table S1</label>
<graphic xlink:href="623268v1_tblS1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Supplementary Table S2</label>
<graphic xlink:href="623268v1_tblS2.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="623268v1_tblS2a.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Supplementary Table S3</label>
<graphic xlink:href="623268v1_tblS3.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="623268v1_tblS3a.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tblS4" orientation="portrait" position="float">
<label>Supplementary Table S4</label>
<graphic xlink:href="623268v1_tblS4.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="623268v1_tblS4a.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="623268v1_tblS4b.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="623268v1_tblS4c.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="623268v1_tblS4d.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<sec id="s6a">
<title>Video legend</title>
<p>Supplementary Video S1. fMRI stimuli.Video shows sample stimuli representing each of four conditions: dog, human, cat, car.</p>
<p>Video available at Open Science Framework: <ext-link ext-link-type="uri" xlink:href="https://osf.io/fzt8c/?view_only=74a9c24f13dd45078e59ad2907ec807e">https://osf.io/fzt8c/?view_only=74a9c24f13dd45078e59ad2907ec807e</ext-link></p>
</sec>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This project was funded by the Hungarian Academy of Sciences [MTA Lendület (Momentum) Programme (LP2017-13/2017), National Brain Programme 3.0 (NAP2022-I-3/2022]; the Eötvös Loránd University; the European Research Council [European Union’s Horizon 2020 research and innovation program (grant number 950159)], the HUN-REN – ELTE Comparative Ethology Research Group (01 031). R.H.-P. and L.V.C. were supported by the Mexican Council of Science and Technology (CONACYT, 407590 and 409258, respectively). R.H.-P. was also supported by the Austrian Science Fund (FWF) (10.55776/ESP602).</p>
<p>We thank all dog and human participants and dogs’ owners for their participation and contribution to these data.</p>
</ack>
<sec id="d1e823" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author Contributions</title>
<p>R.H.-P., L.V.C., E.B.F. and A.A. performed conceptualization. R.H.-P., L.V.C. performed investigation. R.H.-P. and E.R.-H. performed formal analysis. R.H.-P., E.B.F and A.A. performed visualization. E.B.F., R.H.-P. and A.A. performed writing – original draft. E.B.F., R.H.-P., M.G. and A.A. performed writing – review &amp; editing.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Bao</surname></string-name>, <string-name><given-names>L.</given-names> <surname>She</surname></string-name>, <string-name><given-names>M.</given-names> <surname>McGill</surname></string-name>, <string-name><given-names>D. Y.</given-names> <surname>Tsao</surname></string-name></person-group>, <article-title>A map of object space in primate inferotemporal cortex</article-title>. <source>Nature</source> <volume>583</volume>, <fpage>103</fpage>–<lpage>108</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Bracci</surname></string-name>, <string-name><given-names>H. P.</given-names> <surname>Op De Beeck</surname></string-name></person-group>, <article-title>Understanding Human Object Vision : A Picture is Worth a Thousand Representations</article-title>. <source>Annu Rev Psychol</source> <fpage>1</fpage>–<lpage>33</lpage> (<year>2023</year>). <pub-id pub-id-type="doi">10.1146/annurev-psych-032720-041031</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>D. A.</given-names> <surname>Leopold</surname></string-name>, <string-name><given-names>J. F.</given-names> <surname>Mitchell</surname></string-name>, <string-name><given-names>W. A.</given-names> <surname>Freiwald</surname></string-name></person-group>, <source>Evolved Mechanisms of High-Level Visual Perception in Primates</source> (<publisher-name>Elsevier Ltd</publisher-name>, <year>2020</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P. E.</given-names> <surname>Downing</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Shuman</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kanwisher</surname></string-name></person-group>, <article-title>A Cortical Area Selective for Visual Processing of the Human Body</article-title> <source>Science</source>. <volume>293</volume>, <fpage>2470</fpage>–<lpage>3</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Kanwisher</surname></string-name>, <string-name><given-names>J.</given-names> <surname>McDermott</surname></string-name>, <string-name><given-names>M. M.</given-names> <surname>Chun</surname></string-name></person-group>, <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>. <source>J Neurosci</source> <volume>17</volume>, <fpage>4302</fpage>–<lpage>11</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names> <surname>Sha</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>The animacy continuum in the human ventral vision pathway</article-title>. <source>J Cogn Neurosci</source> <volume>27</volume>, <fpage>665</fpage>–<lpage>678</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Grill-Spector</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Weiner</surname></string-name></person-group>, <article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title>. <source>Nat Rev Neurosci</source> <volume>15</volume>, <fpage>536</fpage>–<lpage>548</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. B.</given-names> <surname>Ritchie</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>Untangling the animacy organization of occipitotemporal cortex</article-title>. <source>Journal of Neuroscience</source> <volume>41</volume>, <fpage>7103</fpage>–<lpage>7119</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. M.</given-names> <surname>Jozwik</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>Disentangling five dimensions of animacy in human brain and behaviour</article-title>. <source>Commun Biol</source> <volume>5</volume> (<year>2022</year>).</mixed-citation></ref>
    <ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. K.</given-names> <surname>Robinson</surname></string-name>, <string-name><given-names>G. L.</given-names> <surname>Quek</surname></string-name>, <string-name><given-names>T. A.</given-names> <surname>Carlson</surname></string-name></person-group>, <article-title>Visual Representations: Insights from Neural Decoding</article-title>. <source>Annual Review of Vision Science</source> (<year>2023</year>). <pub-id pub-id-type="doi">10.1146/annurev-vision-100120-025301</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Kriegeskorte</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title>. <source>Neuron</source> <volume>60</volume>, <fpage>1126</fpage>–<lpage>1141</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Kovács</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>The effect of oxytocin on biological motion perception in dogs (Canis familiaris)</article-title>. <source>Anim Cogn</source> <volume>19</volume>, <fpage>513</fpage>–<lpage>522</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Abdai</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Ferdinandy</surname></string-name>, <string-name><given-names>C. B.</given-names> <surname>Terencio</surname></string-name>, <string-name><given-names>Á.</given-names> <surname>Pogány</surname></string-name>, <string-name><given-names>Á.</given-names> <surname>Miklósi</surname></string-name></person-group>, <article-title>Perception of animacy in dogs and humans</article-title>. <source>Biol Lett</source> <volume>13</volume> (<year>2017</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. J.</given-names> <surname>Völter</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Huber</surname></string-name></person-group>, <article-title>Pupil size changes reveal dogs’ sensitivity to motion cues</article-title>. <source>iScience</source> <volume>25</volume> (<year>2022</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names> <surname>Törnqvist</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Somppi</surname></string-name>, <string-name><given-names>M. V.</given-names> <surname>Kujala</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Vainio</surname></string-name></person-group>, <article-title>Observing animals and humans: Dogs target their gaze to the biological information in natural scenes</article-title>. <source>PeerJ</source> <volume>8</volume>:<fpage>e10341</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. E.</given-names> <surname>Byosiere</surname></string-name>, <string-name><given-names>P. A.</given-names> <surname>Chouinard</surname></string-name>, <string-name><given-names>T. J.</given-names> <surname>Howell</surname></string-name>, <string-name><given-names>P. C.</given-names> <surname>Bennett</surname></string-name></person-group>, <article-title>What do dogs (Canis familiaris) see? A review of vision in dogs and implications for cognition research</article-title>. <source>Psychon Bull Rev</source> <fpage>1</fpage>–<lpage>16</lpage> (<year>2017</year>). <pub-id pub-id-type="doi">10.3758/s13423-017-1404-7</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Bunford</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Andics</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kis</surname></string-name>, <string-name><given-names>Á.</given-names> <surname>Miklósi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Gácsi</surname></string-name></person-group>, <article-title>Canis familiaris As a Model for Non-Invasive Comparative Neuroscience</article-title>. <source>Trends Neurosci</source> <volume>40</volume>, <fpage>438</fpage>–<lpage>452</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E. M.</given-names> <surname>Phillips</surname></string-name>, <string-name><given-names>K. D.</given-names> <surname>Gillette</surname></string-name>, <string-name><given-names>D. D.</given-names> <surname>Dilks</surname></string-name>, <string-name><given-names>G. S.</given-names> <surname>Berns</surname></string-name></person-group>, <article-title>Through a Dog’s Eyes: fMRI Decoding of Naturalistic Videos from Dog Cortex</article-title>. <source>J. Vis. Exp</source>. <volume>187</volume>, <fpage>1</fpage>–<lpage>18</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Boch</surname></string-name>, <string-name><given-names>I. C.</given-names> <surname>Wagner</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Karl</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Huber</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Lamm</surname></string-name></person-group>, <article-title>Functionally analogous body- and animacy-responsive areas are present in the dog (Canis familiaris) and human occipito-temporal lobe</article-title>. <source>Commun Biol</source> <volume>6</volume> (<year>2023</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Bunford</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>Comparative brain imaging reveals analogous and divergent patterns of species and face sensitivity in humans and dogs</article-title>. <source>Journal of Neuroscience</source> <volume>40</volume>, <fpage>8396</fpage>–<lpage>8408</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Andics</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Gácsi</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Faragó</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kis</surname></string-name>, <string-name><given-names>Á.</given-names> <surname>Miklósi</surname></string-name></person-group>, <article-title>Voice-sensitive regions in the dog and human brain are revealed by comparative fMRI</article-title>. <source>Current Biology</source> <volume>24</volume>, <fpage>574</fpage>–<lpage>578</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names> <surname>Serre</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Wolf</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Bileschi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Riesenhuber</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Poggio</surname></string-name></person-group>, <article-title>Robust Object Recognition with Cortex-Like Mechanisms</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source> <volume>29</volume>., <fpage>411</fpage>–<lpage>426</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>A. M.</given-names> <surname>Dale</surname></string-name></person-group>, <source>Optimal experimental design for event-related fMRI in Human Brain Mapping</source>, (<publisher-name>Wiley-Liss Inc</publisher-name>., <year>1999</year>), pp. <fpage>109</fpage>–<lpage>114</lpage>.</mixed-citation></ref>
    <ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. A.</given-names> <surname>Burock</surname></string-name>, <string-name><given-names>R. L.</given-names> <surname>Buckner</surname></string-name>, <string-name><given-names>M. G.</given-names> <surname>Woldorff</surname></string-name>, <string-name><given-names>B. R.</given-names> <surname>Rosen</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Dale</surname></string-name></person-group>, <article-title>Randomized event-related experimental designs allow for extremely rapid presentation rates using functional MRI</article-title> <source>Neuroreport</source> (<year>1998</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. W.</given-names> <surname>Woolrich</surname></string-name>, <string-name><given-names>B. D.</given-names> <surname>Ripley</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Brady</surname></string-name>, <string-name><given-names>S. M.</given-names> <surname>Smith</surname></string-name></person-group>, <article-title>Temporal autocorrelation in univariate linear modeling of FMRI data</article-title>. <source>Neuroimage</source> <volume>14</volume>, <fpage>1370</fpage>–<lpage>1386</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Hanke</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>PyMVPA: A python toolbox for multivariate pattern analysis of fMRI data</article-title>. <source>Neuroinformatics</source> <volume>7</volume>, <fpage>37</fpage>–<lpage>53</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. D.</given-names> <surname>Power</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title>. <source>Neuroimage</source> <volume>84</volume>, <fpage>320</fpage>–<lpage>341</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Czeibert</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Andics</surname></string-name>, <string-name><given-names>Ö.</given-names> <surname>Petneházy</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Kubinyi</surname></string-name></person-group>, <article-title>A detailed canine brain label map for neuroimaging analysis</article-title>. <source>Biol Futur</source> in press (<year>2019</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Clarke</surname></string-name>, <string-name><given-names>L. K.</given-names> <surname>Tyler</surname></string-name></person-group>, <article-title>Object-specific semantic coding in human perirhinal cortex</article-title>. <source>Journal of Neuroscience</source> <volume>34</volume>, <fpage>4766</fpage>–<lpage>4775</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. C.</given-names> <surname>Connolly</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>The Representation of Biological Classes in the Human Brain</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>, <fpage>2608</fpage>–<lpage>2618</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Kriegeskorte</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Goebel</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Bandettini</surname></string-name></person-group>, <article-title>Information-based functional brain mapping</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>103</volume>, <fpage>3863</fpage>–<lpage>3868</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Stelzer</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Turner</surname></string-name></person-group>, <article-title>Statistical inference and multiple testing correction in classification-based multi-voxel pattern analysis (MVPA): Random permutations and cluster size control</article-title>. <source>Neuroimage</source> <volume>65</volume>, <fpage>69</fpage>–<lpage>82</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Pitcher</surname></string-name>, <string-name><given-names>L. G.</given-names> <surname>Ungerleider</surname></string-name></person-group>, <article-title>Evidence for a Third Visual Pathway Specialized for Social Perception</article-title>. <source>Trends Cogn Sci</source> [Preprint] (<year>2021</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Robert</surname></string-name>, <string-name><given-names>L. G.</given-names> <surname>Ungerleider</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Vaziri-Pashkam</surname></string-name></person-group>, <article-title>Disentangling Object Category Representations Driven by Dynamic and Static Visual Input</article-title>. <source>Journal of Neuroscience</source> <volume>43</volume>, <fpage>621</fpage>–<lpage>634</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. D.</given-names> <surname>Dilks</surname></string-name>, <etal>et al.</etal></person-group>, <article-title>Awake fMRI reveals a specialized region in dog temporal cortex for face processing</article-title>. <source>PeerJ</source> <volume>3</volume>, <fpage>e1115</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L. V</given-names> <surname>Cuaya</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Hernández-pérez</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Concha</surname></string-name></person-group>, <article-title>Our Faces in the Dog ‘ s Brain : Functional Imaging Reveals Temporal Cortex Activation during Perception of Human Faces</article-title>. <source>PLoS One</source> <fpage>1</fpage>–<lpage>13</lpage> (<year>2016</year>). <pub-id pub-id-type="doi">10.1371/journal.pone.0149431</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. N.</given-names> <surname>Kay</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Bonnen</surname></string-name>, <string-name><given-names>R. N.</given-names> <surname>Denison</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Acaro</surname></string-name>, <string-name><given-names>D. L.</given-names> <surname>Barack</surname></string-name></person-group>, <article-title>Tasks and their role in visual neuroscience</article-title>. <source>Neuron</source> <fpage>0</fpage>–<lpage>7</lpage> (<year>2023</year>). <pub-id pub-id-type="doi">10.1016/j.neuron.2023.03.022</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Summerfield</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Egner</surname></string-name></person-group>, <article-title>Expectation (and attention) in visual cognition</article-title>. <source>Trends Cogn Sci</source> [Preprint] (<year>2009</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104525.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Jbabdi</surname>
<given-names>Saad</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>useful</bold> study presents a comparative investigation of category selectivity in dogs and humans. The study compares brain representations of animate and inanimate objects, replicating and extending previous reports in this nascent field of dog FMRI. The methods and results seem to lack sufficient detail, appropriate controls, or statistical evidence, so at this stage of the review process, the strength of evidence is deemed <bold>incomplete</bold>.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104525.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>Farkas and colleagues conducted a comparative neuroimaging study with domestic dogs and humans to explore whether social perception in both species is underpinned by an analogous distinction between animate and inanimate entities an established functional organizing principle in the primate and human brain. Presenting domestic dogs and humans with clips of three animate classes (dogs, humans, cats) and one inanimate control (cars), the authors also set out to compare how dogs and humans perceive their own vs other species. Both research questions have been previously studied in dogs, but the authors used novel dynamic stimuli and added animate and inanimate classes, which have not been investigated before (i.e., cats and cars). Combining univariate and multivariate analysis approaches, they identified functionally analogous areas in the dog and human occipito-temporal cortex involved in the perception of animate entities, largely replicating previous observations. This further emphasizes a potentially shared functional organizing principle of social perception in the two species. The authors also describe between-species divergencies in the perception of the different animate classes, arguing for a less generalized perception of animate entities in dogs, but this conclusion is not convincingly supported by the applied analyses and reported findings.</p>
<p>Strengths</p>
<p>Domestic dogs represent a compelling model species to study the neural bases of social perception and potentially shared functional organizing principles with humans and primates. The field of comparative neuroimaging with dogs is still young, with a growing but still small number of studies, and the present study exemplifies the reproducibility of previous research. Using dynamic instead of static stimuli and adding new stimuli classes, Farkas and colleagues successfully replicated and expanded previous findings, adding to the growing body of evidence that social perception is underpinned by a shared functional organizing principle in the dog and human occipito-temporal cortex.</p>
<p>Weaknesses</p>
<p>The study design is imbalanced, with only one category of inanimate objects vs. three animate entities. Moreover, based on the example videos, it appears that the animate stimuli also differed in the complexity of the content from the car stimuli, with often multiple agents interacting or performing goal-directed actions. Moreover, while dogs are familiar with cars, they are definitely of lower relevance and interest to them than the animate stimuli. Thus, to a certain extent, the results might also reflect differences in attention towards/salience of the stimuli.</p>
<p>The methods section and rationale behind the chosen approaches were often difficult to follow and lacked a lot of information, which makes it difficult to judge the evidence and the drawn conclusions, and it weakens the potential for reproducibility of this work. For example, for many preprocessing and analysis steps, parameters were missing or descriptions of the tools used, no information on anatomical masks and atlas used in humans was provided, and it is often not clear if the authors are referring to the univariate or multivariate analysis.</p>
<p>In regard to the chosen approaches and rationale, the authors generally binarize a lot of rich information. Instead of directly testing potential differences in the neural representations of the different animate entities, they binarize dissimilarity maps for, e.g. animate entity &gt; inanimate cars and then calculate the overlap between the maps. The comparison of the overlap of these three maps between species is also problematic, considering that the human RSA was constricted to the occipital and temporal cortex (there is now information on how they defined it) vs. whole-brain in dogs. Considering that the stimuli do differ based on low-level visual properties (just not significantly within a run), the RSA would also allow the authors to directly test if some of the (dis)similarities might be driven by low-level visual features like they, e.g. did with the early visual cortex model. I do think RSA is generally an excellent choice to investigate the neural representation of animate (and inanimate) stimuli, but the authors should apply it more appropriately and use its full potential.</p>
<p>The authors localized some of the &quot;animate areas&quot; also with the early visual cortex model (e.g. ectomarginal gyrus, mid suprasylvian); in humans, it only included the known early visual cortex - what does this mean for the animate areas in dogs?</p>
<p>The results section also lacks information and statistical evidence; for example, for the univariate region-of-interest (ROI) analysis (called response profiles) comparing activation strength towards each stimulus type, it is not reported if comparisons were significant or not, but the authors state they conducted t-tests. The authors describe that they created spheres on all peaks reported for the contrast animate &gt; inanimate, but they only report results for the mid suprasylvian and occipital gyrus (e.g. caudal suprasylvian gyrus is missing). Furthermore, considering that the ROIs were chosen based on the contrast animate &gt; inanimate stimuli, activation strength should only be compared between animate entities (i.e., dogs, humans, cats), while cars should not be reported (as this would be double dipping, after selecting voxels showing lower activation for that category). The descriptive data in Figure 3B (pending statistical evidence) suggests there were no strong differences in activation for the three species in dog and human animate areas. Thus, the ROI analysis appears to contradict findings from the binary analysis approach to investigate species preference, but the authors only discuss the results of the latter in support of their narrative for conspecific preference in dogs and do not discuss research from other labs investigating own-species preference.</p>
<p>The authors also unnecessarily exaggerate novelty claims. Animate vs inanimate and own vs other species perceptions have both been investigated before in dogs (and humans), so any claims in that direction seem unsubstantiated - and also not needed, as novelty itself is not a sign of quality; what is novel, and a sign of theoretical advance besides the novelty, are as said the conceptual extension and replication of previous work.</p>
<p>Overall, more analyses and appropriate tests are needed to support the conclusions drawn by the authors, as well as a more comprehensive discussion of all findings.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104525.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript reports an fMRI study looking at whether there is animacy organization in a non-primate, mammal, the domestic dog, that is similar to that observed in humans and non-human primates (NHPs). A simple experiment was carried out with four kinds of stimulus videos (dogs, humans, cats, and cars), and univariate contrasts and RSA searchlight analysis was performed. Previous studies have looked at this question or closely associated questions (e.g. whether there is face selectivity in dogs). The import of the present study is that it looks at multiple types of animate objects, dogs, humans, and cats, and tests whether there was overlapping/similar topography (or magnitude) of responses when these stimuli were compared to the inanimate reference class of cars. The main finding was of some selectivity for animacy though this was primarily driven by the dog stimuli, which did overlap with the other animate stimulus types, but far less so than in humans.</p>
<p>Strengths:</p>
<p>I believe that this is an interesting study in so far as it builds on other recent work looking at category-selectivity in the domestic dog. Given the limited number of such studies, I think it is a natural step to consider a number of different animate stimuli and look at their overlap. While some of the results were not wholly surprising (e.g. dog brains respond more selectively for dogs than humans or cats), that does not take away from their novelty, such as it is. The findings of this study are useful as a point of comparison with other recent work on the organization of high-level visual function in the brain of the domestic dog.</p>
<p>Weaknesses:</p>
<p>(1) One challenge for all studies like this is a lack of clarity when we say there is organization for &quot;animacy&quot; in the human and NHP brains. The challenge is by no means unique to the present study, but I do think it brings up two more specific topics.</p>
<p>First, one property associated with animate things is &quot;capable of self-movement&quot;. While cognitively we know that cars require a driver, and are otherwise inanimate, can we really assume that dogs think of cars in the same way? After all, just think of some dogs that chase cars. If dogs represent moving cars as another kind of self-moving thing, then it is not clear we can say from this study that we have a contrast between animate vs inanimate. This would not mean that there are no real differences in neural organization being found. It was unclear whether all or some of the car videos showed them moving. But if many/most do, then I think this is a concern.</p>
<p>Second, there is quite a lot of potential complexity in the human case that is worth considering when interpreting the results of this study. In the human case, some evidence suggests that animacy may be more of a continuum (Sha et al. 2015), which may reflect taxonomy (Connolly et al. 2012, 2016). However moving videos seem to be dominated more by signals relevant to threat or predation relative to taxonomy (Nastase et al. 2017). Some evidence suggests that this purported taxonomic organization might be driven by gradation in representing faces and bodies of animals based on their relative similarity to humans (Ritchie et al. 2021). Also, it may be that animacy organization reflects a number of (partially correlated) dimensions (Thorat et al. 2019, Jozwik et al. 2022). One may wonder whether the regions of (partial) overlap in animate responses in the dog brain might have some of these properties as well (or not).</p>
<p>(2) It is stated that previous studies provide evidence that the dog brain shows selectivity to &quot;certain aspects of animacy&quot;. One of these already looked at selectivity for dog and human faces and bodies and identified similar regions of activity (Boch et al. 2023). An earlier study by Dilks et al. (2015), not cited in the present work (as far as I can tell), also used dynamic stimuli and did not suffer from the above limitations in choosing inanimate stimuli (e.g. using toy and scene objects for inanimate stimuli). But it only included human faces as the dynamic animate stimulus. So, as far as stimulus design, it seems the import of the present study is that it included a *third* animate stimulus (cats) and that the stimuli were dynamic.</p>
<p>(3) I am concerned that the univariate results, especially those depicted in Figure 3B, include double dipping (Kriegesorte et al. 2009). The analysis uses the response peak for the A &gt; iA contrast to then look at the magnitude of the D, H, C vs iA contrasts. This means the same data is being used for feature selection and then to estimate the responses. So, the estimates are going to be inflated. For example, the high magnitudes for the three animate stimuli above the inanimate stimuli are going to inherently be inflated by this analysis and cannot be taken at face value. I have the same concern with the selectivity preference results in Figure 3E.</p>
<p>I think the authors have two options here. Either they drop these analyses entirely (so that the total set of analyses really mirrors those in Figure 4), or they modify them to address this concern. I think this could be done in one of two ways. One would be to do a within-subject standard split-half analysis and use one-half of the data for feature selection and the other for magnitude estimation. The other would be to do a between-subject design of some kind, like using one subject for magnitude estimation based on an ROI defined using the data for the other subjects.</p>
<p>(4) There are two concerns with how the overlap analyses were carried out. First, as typically carried out to look at overlap in humans, the proportion is of overlapping results of the contrasts of interest, e.g, for face and body selectivity overlap (Schwarlose et al. 2006), hand and tool overlap (Bracci et al. 2012), or more recently, tool and food overlap (Ritchie et al. 2024). There are a number of ways of then calculating the overlap, with their own strengths and weaknesses (see Tarr et al. 2007). Of these, I think the Jaccard index is the most intuitive, which is just the intersection of two sets as a proportion of their union. So, for example, the N of overlapping D &gt; iA and H &gt; iA active voxels is divided by the total number of unique active voxels for the two contrasts. Such an overlap analysis is more standard and interpretable relative to previous findings. I would strongly encourage the authors to carry out such an analysis or use a similar metric of overlap, in place of what they have currently performed (to the extent the analysis makes sense to me).</p>
<p>Second, the results summarized in Figure 3A suggest multiple distinct regions of animacy selectivity. Other studies have also identified similar networks of regions (e.g. Boch et al. 2023). These regions may serve different functions, but the overlap analysis does not tell us whether there is overlap in some of these portions of the cortex and not in others. The overlap is only looked at in a very general sense. There may be more overlap locally in some portions of the cortex and not in others.</p>
<p>(5) Two comments about the RSA analyses. First, I am not quite sure why the authors used HMAX rather than layers of a standardly trained ImageNet deep convolutional neural network. This strikes me also as a missed opportunity since many labs have looked at whether later layers of DNNs trained on object categorization show similar dissimilarity structures as category-selective regions in humans and NHPs. In so far as cross-species comparisons are the motivation here, it would be genuinely interesting to see what would happen if one did a correlation searchlight with the dog brain and layers of a DNN, a la Cichy et al. (2016).</p>
<p>Second, from the text is hard to tell what the models for the class- and category-boundary effects were. Are there RDMs that can be depicted here? I am very familiar with RSA searchlight and I found the description of the methods to be rather opaque. The same point about overlap earlier regarding the univariate results also applies to the RSA results. Also, this is again a reason to potentially compare DNN RDMs to both the categorical models and the brains of both species.</p>
<p>(6) There has been emphasis of late on the role of face and body selective regions and social cognition (Pitcher and Ungerleider, 2021, Puce, 2024), and also on whether these regions are more specialized for representing whole bodies/persons (Hu et al. 2020, Taubert, et al. 2022). It may be that the supposed animacy organization is more about how we socialize and interact with other organisms than anything about animacy as such (see again the earlier comments about animacy, taxonomy, and threat/predation). The result, of a great deal of selectivity for dogs, some for humans, and little for cats, seems to readily make sense if we assume it is driven by the social value of the three animate objects that are presented. This might be something worth reflecting on in relation to the present findings.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104525.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Farkas</surname>
<given-names>Eszter Borbála</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-5946-1999</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Hernández-Pérez</surname>
<given-names>Raúl</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cuaya</surname>
<given-names>Laura Veronica</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rojas-Hortelano</surname>
<given-names>Eduardo</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gácsi</surname>
<given-names>Márta</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Andics</surname>
<given-names>Attila</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary</p>
<p>Farkas and colleagues conducted a comparative neuroimaging study with domestic dogs and humans to explore whether social perception in both species is underpinned by an analogous distinction between animate and inanimate entities an established functional organizing principle in the primate and human brain. Presenting domestic dogs and humans with clips of three animate classes (dogs, humans, cats) and one inanimate control (cars), the authors also set out to compare how dogs and humans perceive their own vs other species. Both research questions have been previously studied in dogs, but the authors used novel dynamic stimuli and added animate and inanimate classes, which have not been investigated before (i.e., cats and cars). Combining univariate and multivariate analysis approaches, they identified functionally analogous areas in the dog and human occipitotemporal cortex involved in the perception of animate entities, largely replicating previous observations. This further emphasizes a potentially shared functional organizing principle of social perception in the two species. The authors also describe between- species divergencies in the perception of the different animate classes, arguing for a less generalized perception of animate entities in dogs, but this conclusion is not convincingly supported by the applied analyses and reported findings.</p>
<p>Strengths</p>
<p>Domestic dogs represent a compelling model species to study the neural bases of social perception and potentially shared functional organizing principles with humans and primates. The field of comparative neuroimaging with dogs is still young, with a growing but still small number of studies, and the present study exemplifies the reproducibility of previous research. Using dynamic instead of static stimuli and adding new stimuli classes, Farkas and colleagues successfully replicated and expanded previous findings, adding to the growing body of evidence that social perception is underpinned by a shared functional organizing principle in the dog and human occipito-temporal cortex.</p>
<p>Weaknesses</p>
<p>The study design is imbalanced, with only one category of inanimate objects vs. three animate entities. Moreover, based on the example videos, it appears that the animate stimuli also differed in the complexity of the content from the car stimuli, with often multiple agents interacting or performing goal-directed actions. Moreover, while dogs are familiar with cars, they are definitely of lower relevance and interest to them than the animate stimuli. Thus, to a certain extent, the results might also reflect differences in attention towards/salience of the stimuli.</p>
</disp-quote>
<p>We agree with the Reviewer and were aware that using only one class of inanimate objects but three classes of animate entities, along with the differences in complexity and relevance between the animate and the inanimate stimuli potentially elicited more attention to the inanimate condition and may have thus introduced a confound. We are revising the related limitation in the discussion to acknowledge this and to emphasize why we believe these differences do not compromise our main findings.</p>
<disp-quote content-type="editor-comment">
<p>The methods section and rationale behind the chosen approaches were often difficult to follow and lacked a lot of information, which makes it difficult to judge the evidence and the drawn conclusions, and it weakens the potential for reproducibility of this work. For example, for many preprocessing and analysis steps, parameters were missing or descriptions of the tools used, no information on anatomical masks and atlas used in humans was provided, and it is often not clear if the authors are referring to the univariate or multivariate analysis.</p>
</disp-quote>
<p>We acknowledge the concerns regarding the clarity and completeness of the methods section and are significantly revising the descriptions of the methods. Of note, in humans, the Harvard-Oxford Cortical Structural Atlas (Frazier et al., 2005; Makris et al., 2006; Desikan et al., 2006; Goldstein et al., 2007), implemented within the FSL software package, was used for anatomical masks, while the Automated Anatomical Labeling atlas (Tzourio-Mazoyer et al., 2002) was used for assigning labels.</p>
<disp-quote content-type="editor-comment">
<p>In regard to the chosen approaches and rationale, the authors generally binarize a lot of rich information. Instead of directly testing potential differences in the neural representations of the different animate entities, they binarize dissimilarity maps for, e.g. animate entity &gt; inanimate cars and then calculate the overlap between the maps.</p>
</disp-quote>
<p>We thank the Reviewer for these comments and ideas. We also appreciate the second Reviewer for their related concerns and suggestions about the overlap calculation. Since the neural processing of different animate entities in the dog brain is largely unexplored, in some of our analyses we aimed to provide a straightforward and directly comparable characterization of animacy perception in the two species. We believe that a measure of how overlapping the neural representations of different animate classes are in the dog vs. the human visual cortex is a simple but meaningful and insightful characterization of how animacy perception is structured in the two species, despite the lack of spatial detail. Our decision to use binarization was based on these considerations. In response to this Reviewer’s request for providing richer information, in our revised manuscript, we will present more details and additional non-binarized calculations. Specifically, we are going to use nonbinarized data to present the response profiles of a broad, anatomically defined set of regions that have been related in other works to visual functions, to thus show where there is significant difference and overlap between the neural responses for the three animate classes in each species.</p>
<disp-quote content-type="editor-comment">
<p>The comparison of the overlap of these three maps between species is also problematic, considering that the human RSA was constricted to the occipital and temporal cortex (there is now information on how they defined it) vs. whole-brain in dogs.</p>
</disp-quote>
<p>We thank this Reviewer for raising yet another relevant point about overlap calculation. We note that the overlap calculation for univariate results used the visually responsive cortex in both dogs and humans. The decision to restrict the multivariate analysis to the occipital and temporal lobes in humans, where the visual areas are, was to reduce computational load. Since RSA in dogs yielded significant voxels almost exclusively in the occipital and temporal cortices, we believe this decision did not introduce major bias in our results. This concern will also be discussed in our revised submission.</p>
<p>Of note, in the category- and class-boundary test, as for the other multivariate tests, the occipital and temporal cortex of humans was delineated based on the MNI atlas.</p>
<disp-quote content-type="editor-comment">
<p>Considering that the stimuli do differ based on low-level visual properties (just not significantly within a run), the RSA would also allow the authors to directly test if some of the (dis)similarities might be driven by low-level visual features like they, e.g. did with the early visual cortex model. I do think RSA is generally an excellent choice to investigate the neural representation of animate (and inanimate) stimuli, but the authors should apply it more appropriately and use its full potential.</p>
</disp-quote>
<p>We thank the Reviewer for this suggestion. While this study did not aim to investigate the correlation between low-level visual features and animacy, the data is available, and the suggested analysis can be conducted in the future. This issue will also be discussed in our revised submission.</p>
<disp-quote content-type="editor-comment">
<p>The authors localized some of the &quot;animate areas&quot; also with the early visual cortex model (e.g. ectomarginal gyrus, mid suprasylvian); in humans, it only included the known early visual cortex - what does this mean for the animate areas in dogs?</p>
</disp-quote>
<p>We thank the Reviewer for raising this point. Although the labels are the same, both EMG and mSSG are relatively large gyri, and the clusters revealed by each of the two analyses hardly overlap, with peak coordinates more than 12 mm apart for R EMG, and in different hemispheres for mSSG (but more than 11 mm apart even if projected on the same hemisphere). We will detail the differences and the overlaps in the revised submission.</p>
<disp-quote content-type="editor-comment">
<p>The results section also lacks information and statistical evidence; for example, for the univariate region-of-interest (ROI) analysis (called response profiles) comparing activation strength towards each stimulus type, it is not reported if comparisons were significant or not, but the authors state they conducted t-tests. The authors describe that they created spheres on all peaks reported for the contrast animate &gt; inanimate, but they only report results for the mid suprasylvian and occipital gyrus (e.g. caudal suprasylvian gyrus is missing).</p>
</disp-quote>
<p>We thank this Reviewer for catching these errors. The missing statistics will be provided in the revised manuscript. Also, we mistakenly named the peak in caudal suprasylvian gyrus occipital gyrus on the figure depicting the response profiles. This will also be corrected.</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, considering that the ROIs were chosen based on the contrast animate &gt; inanimate stimuli, activation strength should only be compared between animate entities (i.e., dogs, humans, cats), while cars should not be reported (as this would be double dipping, after selecting voxels showing lower activation for that category).</p>
</disp-quote>
<p>We thank both Reviewers for raising this relevant point about potential double dipping. The aim of this analysis was to describe the relationship between the neural response elicited by the three animate stimulus classes, to show that the animacy-sensitive peaks are not the results of the standalone greater response to a single animate class. We conducted t-tests only to assess significant difference between these three animate conditions and no stats were performed or reported for any animate class vs. inanimate comparisons in these ROIs. In addition to providing the missing t-tests (comparing animate classes), we will present response profiles and corresponding statistics for a broad set of additional, independent ROIs, defined either anatomically or functionally by other studies in the revised version.</p>
<disp-quote content-type="editor-comment">
<p>The descriptive data in Figure 3B (pending statistical evidence) suggests there were no strong differences in activation for the three species in dog and human animate areas. Thus, the ROI analysis appears to contradict findings from the binary analysis approach to investigate species preference, but the authors only discuss the results of the latter in support of their narrative for conspecific preference in dogs and do not discuss research from other labs investigating own-species preference.</p>
</disp-quote>
<p>Studying conspecific-preference was not the primary aim of this study. We only used our data to characterize the animate-sensitive regions from this aspect. The species-preference test provides an overall characterization of the entire animate-sensitive region, revealing a higher number of voxels with a maximal response to conspecific than other stimuli in dogs (and a similar tendency in humans), confirming previous evidence on neural conspecific preference in visual areas in both species. The response profiles presented so far describe only the ROIs around the main animate-sensitive peaks and, as the Reviewer points out, in most cases reveal no significant conspecific bias. We believe there is no contradiction here: the entire animate-sensitive region may weakly but still be conspecific-preferring, whereas the main animate-sensitive peaks are not; the centers of conspecific preference may be located elsewhere in the visual cortex and may be supported by mechanisms other than animacy-sensitivity. In the revised manuscript, we will elaborate more on this. Additionally, in response to other comments, and for a better and more coherent characterization of species preference (and animacy sensitivity) across the visual cortex, we will present response profiles for other, independently defined regions and explore conspecific-sensitivity in those additional regions as well. Furthermore, we will discuss related own-species preference literature in greater detail.</p>
<disp-quote content-type="editor-comment">
<p>The authors also unnecessarily exaggerate novelty claims. Animate vs inanimate and own vs other species perceptions have both been investigated before in dogs (and humans), so any claims in that direction seem unsubstantiated - and also not needed, as novelty itself is not a sign of quality; what is novel, and a sign of theoretical advance besides the novelty, are as said the conceptual extension and replication of previous work.</p>
</disp-quote>
<p>We agree with this Reviewer regarding novelty claims in general, and we confirm that we had no intention to overstate the uniqueness of our results. We also did not mean to imply that this work would be the first one on animacy perception in dogs, which it obviously is not. But we understand that we could have been more explicit presenting our work as a conceptual extension and replication of previous works, and we are revising the wording of the discussion from this aspect.</p>
<disp-quote content-type="editor-comment">
<p>Overall, more analyses and appropriate tests are needed to support the conclusions drawn by the authors, as well as a more comprehensive discussion of all findings.</p>
</disp-quote>
<p>We are thankful for all comments. We will revise the methods section to provide sufficient detail and ensure replicability; conduct additional analyses as detailed above; and provide a more comprehensive discussion of all findings.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>The manuscript reports an fMRI study looking at whether there is animacy organization in a non-primate, mammal, the domestic dog, that is similar to that observed in humans and non-human primates (NHPs). A simple experiment was carried out with four kinds of stimulus videos (dogs, humans, cats, and cars), and univariate contrasts and RSA searchlight analysis was performed. Previous studies have looked at this question or closely associated questions (e.g. whether there is face selectivity in dogs). The import of the present study is that it looks at multiple types of animate objects, dogs, humans, and cats, and tests whether there was overlapping/similar topography (or magnitude) of responses when these stimuli were compared to the inanimate reference class of cars. The main finding was of some selectivity for animacy though this was primarily driven by the dog stimuli, which did overlap with the other animate stimulus types, but far less so than in humans.</p>
<p>Strengths:</p>
<p>I believe that this is an interesting study in so far as it builds on other recent work looking at category-selectivity in the domestic dog. Given the limited number of such studies, I think it is a natural step to consider a number of different animate stimuli and look at their overlap. While some of the results were not wholly surprising (e.g. dog brains respond more selectively for dogs than humans or cats), that does not take away from their novelty, such as it is. The findings of this study are useful as a point of comparison with other recent work on the organization of high-level visual function in the brain of the domestic dog.</p>
<p>Weaknesses:</p>
<p>(1) One challenge for all studies like this is a lack of clarity when we say there is organization for &quot;animacy&quot; in the human and NHP brains. The challenge is by no means unique to the present study, but I do think it brings up two more specific topics.</p>
<p>First, one property associated with animate things is &quot;capable of self-movement&quot;. While cognitively we know that cars require a driver, and are otherwise inanimate, can we really assume that dogs think of cars in the same way? After all, just think of some dogs that chase cars. If dogs represent moving cars as another kind of selfmoving thing, then it is not clear we can say from this study that we have a contrast between animate vs inanimate. This would not mean that there are no real differences in neural organization being found.</p>
<p>It was unclear whether all or some of the car videos showed them moving. But if many/most do, then I think this is a concern.</p>
</disp-quote>
<p>We thank this Reviewer for raising this relevant point about the potential animacy of cars for dogs and its implication for our results. Of note, two-thirds of our car stimuli showed a car moving (slow, accelerating, or fast). We acknowledge that these stimuli contained motionbased animacy cues, and in this regard, there was no clear difference between our animate and inanimate conditions, and possibly between some of the representations they elicited. However, our animate and inanimate stimuli differed in other key factors accounting for animacy organization, such as visual features including the presence of faces, bodies, body parts, postures, and certain aspects of biological motion. So we believe that this limitation does not compromise our main conclusions. We will elaborate on this point further in the revised discussion, also considering how dogs’ differential behavioral responses to cars and animate entities may provide additional insights in this regard.</p>
<disp-quote content-type="editor-comment">
<p>Second, there is quite a lot of potential complexity in the human case that is worth considering when interpreting the results of this study. In the human case, some evidence suggests that animacy may be more of a continuum (Sha et al. 2015), which may reflect taxonomy (Connolly et al. 2012, 2016). However moving videos seem to be dominated more by signals relevant to threat or predation relative to taxonomy (Nastase et al. 2017). Some evidence suggests that this purported taxonomic organization might be driven by gradation in representing faces and bodies of animals based on their relative similarity to humans (Ritchie et al. 2021). Also, it may be that animacy organization reflects a number of (partially correlated) dimensions (Thorat et al. 2019, Jozwik et al. 2022). One may wonder whether the regions of (partial) overlap in animate responses in the dog brain might have some of these properties as well (or not).</p>
</disp-quote>
<p>We agree that it would be interesting to dissect which animacy-related factor(s) contribute to the observed animacy sensitivity in different regions, and although this was not the original aim of the study, we agree that we could have made better use of the variation in our stimuli to discuss this aspect. Specifically, some animacy features are shared by all three animate stimulus classes, namely the presence of biological motions, faces, and bodies. In contrast, animate classes differed in some other aspects, for example in how dogs perceived dogs, humans, and cats as social agents and in their potential behavioral goals towards them. It can therefore be argued that regions with two- and especially three-way overlapping activations are more probably involved in processing biological motion, face and body aspects, and non-overlapping ones the social agency- and behavioural goal-related aspects. In line with this, the shared animacy features are indeed ones that have been reported to be central in human animacy representation and that may have made the overlaps in human brain responses greater. We will provide a more detailed discussion of the results from this viewpoint in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(2) It is stated that previous studies provide evidence that the dog brain shows selectivity to &quot;certain aspects of animacy&quot;. One of these already looked at selectivity for dog and human faces and bodies and identified similar regions of activity (Boch et al. 2023). An earlier study by Dilks et al. (2015), not cited in the present work (as far as I can tell), also used dynamic stimuli and did not suffer from the above limitations in choosing inanimate stimuli (e.g. using toy and scene objects for inanimate stimuli). But it only included human faces as the dynamic animate stimulus. So, as far as stimulus design, it seems the import of the present study is that it included a *third* animate stimulus (cats) and that the stimuli were dynamic.</p>
</disp-quote>
<p>We agree with this Reviewer that the findings of Dilks et al. (2015) are relevant to our study and have therefore cited them. However, the citation itself was imprecise and will be corrected in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(3) I am concerned that the univariate results, especially those depicted in Figure 3B, include double dipping (Kriegesorte et al. 2009). The analysis uses the response peak for the A &gt; iA contrast to then look at the magnitude of the D, H, C vs iA contrasts. This means the same data is being used for feature selection and then to estimate the responses. So, the estimates are going to be inflated. For example, the high magnitudes for the three animate stimuli above the inanimate stimuli are going to inherently be inflated by this analysis and cannot be taken at face value. I have the same concern with the selectivity preference results in Figure 3E.</p>
<p>I think the authors have two options here. Either they drop these analyses entirely (so that the total set of analyses really mirrors those in Figure 4), or they modify them to address this concern. I think this could be done in one of two ways. One would be to do a within- subject standard split-half analysis and use one-half of the data for feature selection and the other for magnitude estimation. The other would be to do a between-subject design of some kind, like using one subject for magnitude estimation based on an ROI defined using the data for the other subjects.</p>
</disp-quote>
<p>We thank both Reviewers again for raising this important point about potential double dipping. We also thank this Reviewer for specific suggestions for split-half analyses – we agree that, had our original analyses involved double dipping, such a modification would be necessary. But, as we explained in our response above, this was not the case. Indeed, whereas we do visualize all four conditions in Fig. 3B, we only conducted t-tests to assess differences between the three animate conditions (the corresponding stats have been missing from the original manuscript but will be added during revision). So, importantly, we did not evaluate the magnitude of the D, H, C vs iA contrasts in any of the ROIs defined by animate-sensitive peaks; therefore, we believe that these analyses do not involve double dipping. This holds for the species preference results in Fig. 3E as well. We will clarify this in the revised manuscript. Of note, in response to a request by the other reviewer and to provide richer information about the univariate results, we will also provide response profiles and corresponding stats for a broad set of additional ROIs, defined either anatomically or functionally by other studies (e.g., Boch et al., 2023).</p>
<disp-quote content-type="editor-comment">
<p>(4) There are two concerns with how the overlap analyses were carried out. First, as typically carried out to look at overlap in humans, the proportion is of overlapping results of the contrasts of interest, e.g, for face and body selectivity overlap (Schwarlose et al. 2006), hand and tool overlap (Bracci et al. 2012), or more recently, tool and food overlap (Ritchie et al. 2024). There are a number of ways of then calculating the overlap, with their own strengths and weaknesses (see Tarr et al. 2007). Of these, I think the Jaccard index is the most intuitive, which is just the intersection of two sets as a proportion of their union. So, for example, the N of overlapping D &gt; iA and H &gt; iA active voxels is divided by the total number of unique active voxels for the two contrasts. Such an overlap analysis is more standard and interpretable relative to previous findings. I would strongly encourage the authors to carry out such an analysis or use a similar metric of overlap, in place of what they have currently performed (to the extent the analysis makes sense to me).</p>
</disp-quote>
<p>We agree with this Reviewer that the Jaccard index is an intuitive and straightforward overlap measure. Importantly, for our overlap calculations we already use this measure (and a very similar one) – but we acknowledge that this was not clear from the original description. Specifically, for the multivariate overlap test, we used the Jaccard index exactly as described by this Reviewer. For the univariate overlap test, we use a very similar measure, with the only difference that there, to reference the search space, the intersection of specific animate-inanimate contrasts was divided by the total voxel number of animate-sensitive areas (which is highly similar to the union of the specific animate-inanimate contrasts). In the revised submission we will provide a more detailed explanation of the overlap calculations, making it explicit that we used the Jaccard index (and a variant of it).</p>
<disp-quote content-type="editor-comment">
<p>Second, the results summarized in Figure 3A suggest multiple distinct regions of animacy selectivity. Other studies have also identified similar networks of regions (e.g. Boch et al. 2023). These regions may serve different functions, but the overlap analysis does not tell us whether there is overlap in some of these portions of the cortex and not in others. The overlap is only looked at in a very general sense. There may be more overlap locally in some portions of the cortex and not in others.</p>
</disp-quote>
<p>We thank this Reviewer for this comment, we agree that adding spatial specificity to these results will improve the manuscript. Therefore, during revision, we will assess the anatomical distribution of the overlap results, making use of a broad set of ROIs potentially relevant for animacy perception, defined either anatomically or functionally by other studies (e.g., Boch et al., 2023 for dogs).</p>
<disp-quote content-type="editor-comment">
<p>(5) Two comments about the RSA analyses. First, I am not quite sure why the authors used HMAX rather than layers of a standardly trained ImageNet deep convolutional neural network. This strikes me also as a missed opportunity since many labs have looked at whether later layers of DNNs trained on object categorization show similar dissimilarity structures as category-selective regions in humans and NHPs. In so far as cross-species comparisons are the motivation here, it would be genuinely interesting to see what would happen if one did a correlation searchlight with the dog brain and layers of a DNN, a la Cichy et al. (2016).</p>
</disp-quote>
<p>We thank the Reviewer for this comment and suggestion. At the start of the project, HMAX was the most feasible model to implement given our time and expertise constrains. Additionally, the biologically motivated HMAX was also an appropriate choice, as it simulates the selective tuning of neurons in the primary visual cortex (V1) of primates, which is considered homologous with V1 in carnivores (Boch et al., 2024).</p>
<p>Although we agree that using DNNs have recently been extensively and successfully used to explore object representations and could provide valuable additional insights for dogs’ visual perception as well, we believe that adding a large set of additional analyses would stretch the frames of this manuscript, disproportionately shifting its focus from our original research question. Also, our experiment, designed with a different, more specific aim in mind, did not provide a large enough stimulus variety of animate stimuli for a general comparison of the cortical hierarchy underlying object representations in dog and human brains and thus our data are not an optimal starting point for such extensive explorations. Having said that, we are thankful for this Reviewer for the idea and will consider using a DNN to uncover dog’ visual cortical hierarchy in future studies with a better suited stimulus set. Furthermore, in accordance with eLife’s data-sharing policies, we will make the current dataset publicly available so further hypothesis and models can be tested.</p>
<disp-quote content-type="editor-comment">
<p>Second, from the text is hard to tell what the models for the class- and categoryboundary effects were. Are there RDMs that can be depicted here? I am very familiar with RSA searchlight and I found the description of the methods to be rather opaque. The same point about overlap earlier regarding the univariate results also applies to the RSA results. Also, this is again a reason to potentially compare DNN RDMs to both the categorical models and the brains of both species.</p>
</disp-quote>
<p>In the revised manuscript we will provide a more detailed explanation of the methods used to determine class- and category-boundary effects. In short, the analysis we performed here followed Kriegeskorte et al. (2008), and the searchlight test looked for regions in which between-class/category differences were greater than within-class/category differences. We will also include RDMs. Additionally, we will provide anatomical details for the overlap results for RSA, just as for the univariate results, using the same independently defined broad set of ROIs, defined either anatomically or functionally by other studies (e.g., Boch et al., 2023 for dogs).</p>
<disp-quote content-type="editor-comment">
<p>(6) There has been emphasis of late on the role of face and body selective regions and social cognition (Pitcher and Ungerleider, 2021, Puce, 2024), and also on whether these regions are more specialized for representing whole bodies/persons (Hu et al. 2020, Taubert, et al. 2022). It may be that the supposed animacy organization is more about how we socialize and interact with other organisms than anything about animacy as such (see again the earlier comments about animacy, taxonomy, and threat/predation). The result, of a great deal of selectivity for dogs, some for humans, and little for cats, seems to readily make sense if we assume it is driven by the social value of the three animate objects that are presented. This might be something worth reflecting on in relation to the present findings.</p>
</disp-quote>
<p>We thank the Reviewer for this suggestion. The original manuscript already discussed how motion-related animacy cues involved in social cognition may explain that animacysensitive regions reported in our study extend beyond those reported previously and also the role of biological motion in the observed across-species differences. This discussion of the role of visual diagnostic features and features that involved in perceiving social agents will be extended in the revised discussion, also in response to the first comment of this Reviewer, to reflect on how social cognition-related animacy cues may have affected our results in dogs.</p>
</body>
</sub-article>
</article>