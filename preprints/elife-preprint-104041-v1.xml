<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">104041</article-id>
<article-id pub-id-type="doi">10.7554/eLife.104041</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104041.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Preparatory attentional templates in prefrontal and sensory cortex encode target-associated information</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0687-2398</contrib-id>
<name>
<surname>Zhou</surname>
<given-names>Zhiheng</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>zhou@sicnu.edu.cn</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5663-9637</contrib-id>
<name>
<surname>Geng</surname>
<given-names>Joy J</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>jgeng@ucdavis.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043dxc061</institution-id><institution>College of Psychology, Sichuan Normal University</institution></institution-wrap>, <city>Chengdu</city>, <country>China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rrcem69</institution-id><institution>Center for Mind and Brain, University of California, Davis</institution></institution-wrap>, <city>Davis</city>, <country>United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rrcem69</institution-id><institution>Department of Psychology, University of California, Davis</institution></institution-wrap>, <city>Davis</city>, <country>United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Moore</surname>
<given-names>Tirin</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University, Howard Hughes Medical Institute</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing Interest Statement: The authors have declared no competing interest.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-01-20">
<day>20</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP104041</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-10-25">
<day>25</day>
<month>10</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-10-29">
<day>29</day>
<month>10</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.01.601634"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Zhou &amp; Geng</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Zhou &amp; Geng</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-104041-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Visual search relies on the ability to use information about the target in working memory to guide attention and make target-match decisions. The representation of target features is referred to as the “attentional” or “target” template and is thought to be encoded within an IFJ-visual cortical network (<xref ref-type="bibr" rid="c6">Baldauf &amp; Desimone, 2014</xref>; <xref ref-type="bibr" rid="c11">Bichot et al., 2015b</xref>). The contents of the template typically contain veridical target information that is used to modulate sensory processing in preparation for guiding attention during search. However, many behavioral studies have shown that target-associated information is used to guide attention, especially when target discrimination is difficult (<xref ref-type="bibr" rid="c7">Battistoni et al., 2017</xref>; <xref ref-type="bibr" rid="c34">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="c92">Peelen et al., 2024</xref>; <xref ref-type="bibr" rid="c117">Vo et al., 2019</xref>; <xref ref-type="bibr" rid="c125">Yu et al., 2023</xref>; <xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>). Thus, while target-associated information is known to impact search performance, its presence within the IFJ-visual attentional network during the preparatory period has never been demonstrated. Here, we use fMRI and multivariate pattern analysis, to test if attentional guidance by target-associated information is explicitly represented in the preparatory period before search begins, either in conjunction with the target or even in place of it. Participants were first trained on four face-scene category pairings after which they completed a cued visual search task for the same faces. Each trial began with a face cue, followed by a delay period, and then a search display with two lateralized faces superimposed on scene images. The critical results showed that while face information could be decoded in the fusiform face area (FFA), superior parietal lobule (SPL), and dorsolateral prefrontal cortex (dLPFC), during the cue period, face information could not be decoded in any brain regions during the delay period. In contrast, the associated scene was decoded only in ventrolateral prefrotnal cortex (vLPFC) curing the cue period but most importantly, in the inferior frontal junction (IFJ) and the parahippocampal place area (PPA) during the delay period. Our results are a novel demonstration that target-associated information from memory can supplant veridical target information in the brain’s “target template” in anticipation of difficult visual search.</p>
</abstract>

<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>New text in the introduction, results, and discussion were added to clarify the novelty of the findings.
Fix the correct grants order.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>We engage in visual search repeatedly throughout the day whenever we look for something, such as a friend in a café, a phone on the table, or ingredients at a store. When visual search is efficient, behavior feels frictionless, but failures result in frustration or even the derailment of behavioral goals. The canonical mechanism supporting visual search is thought to involve a network of frontal regions that maintain target information in a working memory “template” that is used to adjust gain in sensory neurons that encode matching features (<xref ref-type="bibr" rid="c25">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="c35">Robert Desimone &amp; John Duncan, 1995</xref>; <xref ref-type="bibr" rid="c69">Liu et al., 2007</xref>; <xref ref-type="bibr" rid="c86">O’Connor et al., 2002</xref>; <xref ref-type="bibr" rid="c87">O’Craven et al., 1999</xref>; <xref ref-type="bibr" rid="c97">Reynolds &amp; Heeger, 2009</xref>; <xref ref-type="bibr" rid="c101">Serences &amp; Boynton, 2007</xref>; <xref ref-type="bibr" rid="c107">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c110">Sylvester et al., 2007</xref>; <xref ref-type="bibr" rid="c113">Treue &amp; Trujillo, 1999</xref>). Preparatory changes in sensory gain lead to increases in baseline activity that enhance subsequent stimulus-evoked responses by target-matching stimuli (<xref ref-type="bibr" rid="c15">Boettcher et al., 2020</xref>; <xref ref-type="bibr" rid="c25">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="c36">R. Desimone &amp; J. Duncan, 1995</xref>; <xref ref-type="bibr" rid="c46">Gayet &amp; Peelen, 2022</xref>; <xref ref-type="bibr" rid="c86">O’Connor et al., 2002</xref>; <xref ref-type="bibr" rid="c93a">Peelen &amp; Kastner, 2011</xref>; <xref ref-type="bibr" rid="c101">Serences &amp; Boynton, 2007</xref>; <xref ref-type="bibr" rid="c107">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c110">Sylvester et al., 2007</xref>; <xref ref-type="bibr" rid="c116">van Loon et al., 2018</xref>; <xref ref-type="bibr" rid="c121">Witkowski &amp; Geng, 2023</xref>). In this way, information in the target template is thought to dictate sensory processing priorities that impact the ability to perceive and act on behaviorally relevant objects. Despite its importance for behavior, the bulk of work on target templates in the brain has focused on target veridical features (e.g., the color “red” or small round shapes when looking for an apple). Few studies have examined how non-target information might also be held in the template and used as a proxy for the target. The current study tests if objects that are statistically associated with the target are ever prioritized in the target template in conjunction with, or even instead of, the actual target in anticipation of visual search (<xref ref-type="bibr" rid="c13">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="c54">Helbing et al., 2022</xref>; <xref ref-type="bibr" rid="c74">Mack &amp; Eckstein, 2011</xref>; <xref ref-type="bibr" rid="c125">Yu et al., 2023</xref>; <xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>).</p>
<p>Information in the target template is considered a source of goal-directed sensory-motor control (<xref ref-type="bibr" rid="c81">Nee, 2021</xref>). Recently, the posterior end of the inferior frontal sulcus and its junction with the precentral sulcus has been identified as holding object and feature-based information consistent with the definition of target template (<xref ref-type="bibr" rid="c6">Baldauf &amp; Desimone, 2014</xref>; <xref ref-type="bibr" rid="c8">Bedini et al., 2023</xref>; <xref ref-type="bibr" rid="c10">Bichot et al., 2015a</xref>; <xref ref-type="bibr" rid="c49">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c89">O’Reilly, 2010</xref>; <xref ref-type="bibr" rid="c104">Soyuhos &amp; Baldauf, 2023</xref>; <xref ref-type="bibr" rid="c121">Witkowski &amp; Geng, 2023</xref>; <xref ref-type="bibr" rid="c126">Zanto et al., 2010</xref>). These regions include a posterior-to-anterior cortical organization labeled as premotor eye field (PEF), posterior inferior frontal junction (IFJp), and anterior inferior frontal junction (IFJa) in the multi-modal parcellation atlas (<xref ref-type="bibr" rid="c8">Bedini et al., 2023</xref>; <xref ref-type="bibr" rid="c48">Glasser et al., 2016</xref>). For example, <xref ref-type="bibr" rid="c6">Baldauf and Desimone (2014)</xref> found that attention to superimposed faces versus houses resulted in induced gamma synchrony between the IFJ and fusiform face area (FFA) or parahippocampal place area (PPA), respectively; this was coupled with selective sensory enhancements in stimulus processing. A likely homologue of this region has also been identified in non-human primates in the ventral prearcuate region (VPA) of prefrontal cortex (<xref ref-type="bibr" rid="c10">Bichot et al., 2015a</xref>; <xref ref-type="bibr" rid="c12">Bichot et al., 2019</xref>). Consistent with its role as a source for feature-based attention, inactivation of VPA resulted in poorer visual search performance and reduced V4 visual responses to the target. These results provide evidence for a causal link between the target representation in IFJ and sensory biases in V4 that facilitate visual search behavior. Together the data suggest that regions encompassed by IFJ maintain target information and provide feedback signals to visual neurons encoding those same features. The role IFJ plays for feature-based attention appears to be analogous to that of the frontal eye fields (FEF) for spatial attention (<xref ref-type="bibr" rid="c10">Bichot et al., 2015a</xref>; <xref ref-type="bibr" rid="c32">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="c77">Moore et al., 2003</xref>; <xref ref-type="bibr" rid="c99">Ruff et al., 2006</xref>; <xref ref-type="bibr" rid="c112">Thompson et al., 2005</xref>).</p>
<p>In addition to IFJ, which appears to encode the source of target information, visual search involves a number of other cognitive control functions that recruit other frontal and parietal regions. In particular, the dorsolateral and ventrolateral prefrontal cortices (dLPFC and vLPFC) are involved in proactive and reactive cognitive control including the maintenance and manipulation of goal-relevant information across stimulus types, and flexibly updating those goal-based representations when unexpected errors are encountered (<xref ref-type="bibr" rid="c4">Badre &amp; Nee, 2018</xref>; <xref ref-type="bibr" rid="c9">Bettencourt &amp; Xu, 2016</xref>; <xref ref-type="bibr" rid="c21">Braver et al., 2009</xref>; <xref ref-type="bibr" rid="c27">Christophel et al., 2018</xref>; <xref ref-type="bibr" rid="c28">Christophel et al., 2017</xref>; <xref ref-type="bibr" rid="c39">Emrich et al., 2013</xref>; <xref ref-type="bibr" rid="c40">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="c44">Finn et al., 2019</xref>; <xref ref-type="bibr" rid="c66">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="c70">Liu et al., 2003</xref>; <xref ref-type="bibr" rid="c71">Long &amp; Kuhl, 2018</xref>; <xref ref-type="bibr" rid="c83">Nee &amp; D’Esposito, 2017</xref>; <xref ref-type="bibr" rid="c103">Soon et al., 2013</xref>; <xref ref-type="bibr" rid="c108">Stokes et al., 2013</xref>). Working memory representations in lateral prefrontal and parietal regions are engaged in cognitive control computations that task non-specific but essential to their functioning (<xref ref-type="bibr" rid="c9">Bettencourt &amp; Xu, 2016</xref>; <xref ref-type="bibr" rid="c64">Kwak &amp; Curtis, 2022</xref>; <xref ref-type="bibr" rid="c71">Long &amp; Kuhl, 2018</xref>; <xref ref-type="bibr" rid="c91">Panichello &amp; Buschman, 2021</xref>). The ability to flexibly update goals is also an important aspect of visual search because finding a target requires an iterative cycle of choosing an object based on the current template to attend to or look at, and then deciding if it is the correct target or not (<xref ref-type="bibr" rid="c2">Alexander &amp; Zelinsky, 2011</xref>; <xref ref-type="bibr" rid="c55">Hout &amp; Goldinger, 2014</xref>; <xref ref-type="bibr" rid="c75">Malcolm &amp; Henderson, 2010</xref>; <xref ref-type="bibr" rid="c124">Yu et al., 2022</xref>; <xref ref-type="bibr" rid="c125">Yu et al., 2023</xref>). When it is the target, search is terminated, but when it is not, attention is guided to a new potential target and the cycle continues. Thus, targets must be maintained in memory over time, but control mechanisms flexibly adjust goals based on sensory outcomes (<xref ref-type="bibr" rid="c5">Badre &amp; Wagner, 2007</xref>; <xref ref-type="bibr" rid="c38">Egner, 2023</xref>; <xref ref-type="bibr" rid="c63">Kurtin et al., 2023</xref>; <xref ref-type="bibr" rid="c95">Poskanzer &amp; Aly, 2023</xref>; <xref ref-type="bibr" rid="c98">Rossi et al., 2009</xref>; <xref ref-type="bibr" rid="c114">Tunnermann et al., 2021</xref>).</p>
<p>In contrast to the relatively straightforward idea that veridical target features are held in working memory and used to bias sensory processing, there is substantial evidence that during naturalistic scene viewing, attention is often guided by objects that are associated with the target but not the target itself (<xref ref-type="bibr" rid="c13">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="c54">Helbing et al., 2022</xref>; <xref ref-type="bibr" rid="c74">Mack &amp; Eckstein, 2011</xref>; <xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>). For example, people are faster to find targets (e.g., sand toys) that are near associated “anchor” objects (e.g., sandbox), because the anchor serves as a large and more easily perceived spatial predictor of the target’s location (<xref ref-type="bibr" rid="c13">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="c54">Helbing et al., 2022</xref>; <xref ref-type="bibr" rid="c115">Turini &amp; Võ, 2022</xref>). Using such proxy information is beneficial because it reduces the space over which the target must be searched for (<xref ref-type="bibr" rid="c13">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="c22">Castelhano &amp; Krzys, 2020</xref>; <xref ref-type="bibr" rid="c52">Hall &amp; Geng, 2024</xref>; <xref ref-type="bibr" rid="c57">Josephs et al., 2016</xref>). A recent study found that target-associated anchor objects implied by a visual scene could be decoded in parietal and visual cortex before the target appeared (<xref ref-type="bibr" rid="c67">Lerebourg et al., 2024</xref>), supporting the notion that objects associated with the target are used to bias sensory processing and guide attention (<xref ref-type="bibr" rid="c46">Gayet &amp; Peelen, 2022</xref>; <xref ref-type="bibr" rid="c92">Peelen et al., 2024</xref>; <xref ref-type="bibr" rid="c125">Yu et al., 2023</xref>). Despite these studies, it remains unknown if target-associated information in memory is encoded in attentional control networks during the preparatory period at all, and if it is, if it occurs along with target information or instead of target information.</p>
<p>In this study, we address this gap in knowledge by asking if associated information in memory can be found in in the IFJ, the frontoparietal attention network, and category-selective sensory cortex during the delay period in anticipation of visual search. Such an outcome would suggest that information associated with the target is prioritized in preparation for attentional guidance, potentially even at the expense of the target itself. We base our paradigm on behavioral work showing that participants can learn new face-scene category associations and use this information to guide target search, but only when target-distractor discrimination is difficult (<xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>). In this previous study, we trained participants on four face-scene associations in advance of a cued-visual search task. During visual search, a single face was cued to indicate the target on each trial. Next, a search display appeared with two lateralized faces superimposed on scenes. The target appeared on its associated scene on 75% of “scene-valid” trials. Different sub-experiments used a variety of scene-invalid trials, including ones in which the associated scene was never present, was co-located with the distractor only, or appeared with both the target and the distractor. Here, we only use one of the “scene-invalid” conditions because the search trials are not of primary interest. The main purpose of the study is to determine if the face cue activates a memory representation of the associated scene during the delay period in attentional control regions that encode the target template, namely IFJ. This outcome would be novel evidence for the hypothesis that the associated scene is represented within the target template during the delay period, in anticipation of guiding attention to a location that is statistically likely to contain the target.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Behavioral tasks</title>
<p>All participants (n = 26) were first trained outside the scanner on four face-scene category associations (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). First, they saw the displays introducing a face with a scene, for example, the text “Emma is a lifeguard and can be found on the beach” presented with an image of a face, “Emma”, superimposed on a beach scene (see Methods). Second, participants were given a match-nonmatch decision task in which they indicated if a specific face-scene pair was a previously learned pair (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). All participants reached a decision accuracy greater than 90%, with mean accuracy = 97.4% ± 2.7% (<xref rid="fig1" ref-type="fig">Figure 1C</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Associative learning task.</title>
<p>(A) The four target face stimuli and their associated scenes. (B) Associative learning task to test memory for face-scene pairs. Participants viewed a series of face-scene pairs and made a judgment about whether the face and scene were matched or not. (C) Memory performance for both match and nonmatch conditions was high suggesting a strong association was formed before the face search task.</p></caption>
<graphic xlink:href="601634v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>During the scan session, participants were given a cued visual search task (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). Each trial began with a <italic>search cue</italic> of the target face for 1-second, followed by an 8-second blank <italic>search delay</italic> period, and then a search display for 250ms. Trials ended with an 8-second intertrial interval to allow the BOLD response to return to baseline. The delay period was the temporal interval of greatest interest. While the cue-evoked stimulus response should contain decodable face information, any scene information during the delay period must reflect memory-evoked representations because no scene information was visually presented with the face cue. Search trials were composed of two face stimuli superimposed on scene images. The target appeared on the associated scene on 75% of trials (scene-valid) and on one of the three other scenes on 25% of trials (scene-invalid). Thus, the scene was probabilistically predictive of the target’s location but not deterministic. The distractor face was a race-gender match, which we previously found made target discrimination difficult enough to make the scene useful for guiding attention (<xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>). After the main search task, participants completed a face/scene 1-back task (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Data from this task was used as a benchmark test set for neural activation patterns trained in response to faces and scenes from the main visual search task.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>(A) Face search task. Each trial started with a 1-second <italic>search cue</italic> indicating the target face for that trial. This was followed by an 8-second blank <italic>search delay</italic> period, and then the search display for 250 ms. Participants pressed a button to indicate the location of the target on the left or right. (B) Illustration of the separate 1-back task used for cross-task classifier testing. Consistent with the main visual search task, the image was presented for 1 second followed by an 8-second <italic>delay</italic> period. (C) Classification scheme. Classifiers were trained on the neural response patterns from the <italic>search cue</italic> face stimulus and <italic>delay</italic> periods from the visual search task; the classifier was tested on face or scene <italic>sample</italic> stimulus and <italic>delay</italic> period from the 1-back task. (D) Visualization of the twelve functional ROIs on the cortical surface of a representative participant. All ROIs were defined in an individual’s native space.</p></caption>
<graphic xlink:href="601634v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2b">
<title>Cue-evoked face information is translated into a preparatory scene template prior to visual search</title>
<p>To test if face and/or scene information was present in the IFJ, attentional control networks, and category-selective visual cortex during the cue and delay periods prior to visual search, we adopted a cross-classification multivariate pattern analysis (MVPA) approach (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Separate classifiers were created for the cue and delay periods, but both classifiers were trained on data from the main search task and tested on data from the separate 1-back task using trials with the same face and scene stimuli. Analyses were focused on twelve regions of interest (ROIs) within the frontoparietal network and the sensory cortex that are known to be involved in representing task structure, target templates for feature-based attention, and category-selective visual processing (<xref rid="fig2" ref-type="fig">Figure 2D</xref>). Frontoparietal ROIs were defined by the HCP-MMP1 (<xref ref-type="bibr" rid="c48">Glasser et al., 2016</xref>) and the 17-network atlases (<xref ref-type="bibr" rid="c100">Schaefer et al., 2018</xref>). Category-selective visual regions for faces (FFA) and scenes (PPA) in the ventral visual pathway were identified for each participant using the independent functional localizer task (<xref ref-type="bibr" rid="c106">Stigliani et al., 2015</xref>).</p>
<p>The cross-classification procedure for the cue stimulus involved training the classifier on activation patterns in response to the <italic>search cue</italic> in the face search task, and testing on activation patterns in response to <italic>face</italic> and <italic>scene</italic> stimulus <italic>samples</italic> seen during an independent 1-back task (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). We expected the <italic>search cue</italic> classifier to successfully decode face stimuli but not necessarily scene stimuli since this time period should primarily reflect the stimulus-evoked sensory processing of the face cue. The <italic>delay</italic> period cross-classification procedure involved training the classifier on patterns of activation during the 8-second <italic>search delay</italic> period after the face <italic>search cue</italic> had disappeared and testing the classifier on activation patterns from the 8-second <italic>delay</italic> following the <italic>face</italic> or <italic>scene</italic> sample stimulus from the 1-back task (<xref rid="fig2" ref-type="fig">Figure 2C</xref>).</p>
<p>Here, we hypothesized that we would see decoding of scene information in the IFJ-PPA network either in conjunction with face information in IFJ, or even in place of it. Critically, the decodable information during the delay period for the <italic>face</italic> could be contaminated by the stimulus-driven activity during the cue. This is because the temporal separation between the face and the delay period is too short for the BOLD response to the face cue to completely resolve. However, this is not a concern for <italic>scene</italic> decoding because the scene is never visually shown during the cue period. Thus, the ability to decode scene information during the delay period has to reflect memory-evoked representations in response to seeing the specific face cue.</p>
<p>Application of this approach to the <italic>search cue</italic> period (<xref rid="fig3" ref-type="fig">Figure 3A</xref>) revealed significant decoding of face information in bilateral FFA (left <italic>p</italic> = 0.0074, uncorrected; right <italic>p</italic> &lt; 0.005). As expected, each of the four face cues could be distinguished from each other in FFA. Faces were also significantly decoded in bilateral SPL (left <italic>p</italic> = 0.018; right <italic>p</italic> &lt; 0.005), right dLPFC (<italic>p</italic> &lt; 0.005), and left IFJp (<italic>p</italic> = 0.012, uncorrected) consistent with the storage of face stimuli in working memory for task-based control. In contrast, scene information was only decoded in bilateral vLPFC (left <italic>p</italic> = 0.0066, uncorrected; right <italic>p</italic> = 0.023); there was no significant decoding of scene information in the scene-selective regions PPA (left <italic>p</italic> = 0.12; right <italic>p</italic> = 0.51). Decoding in vLPFC is important and consistent with the retrieval of associated-scene information from long-term memory in response to the face cue. In addition to the ROI results, we also conducted a whole-brain searchlight using a cluster-corrected threshold of <italic>p</italic>TFCE &lt; .005 to identify additional areas that may have encoded face or scene information during the <italic>search cue</italic> period. This revealed additional significant clusters where the face cue could be decoded in the left dLPFC (<xref rid="fig3" ref-type="fig">Figure 3B</xref> and Table S1). These results indicate that the <italic>search cue</italic> period is dominated by face information in a widespread network of sensory and frontoparietal regions, with limited scene information in vLPFC, related to the cognitive control of memory (<xref ref-type="bibr" rid="c5">Badre &amp; Wagner, 2007</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Decoding of face and scene information during the <italic>search cue</italic> period.</title>
<p>(A) Evidence of face information in <italic>a priori</italic> defined ROIs. Greater than chance-level classification accuracies were found in the dLPFC, SPL, and FFA. Evidence for scene information was found in vLPFC. (B and C) Significant brain regions revealed by a whole-brain searchlight procedure with information about the face cue (B) or associated scene (C). Note that scene information was never shown during the cue period and therefore decoding of scene information reflects memory-evoked responses to the cued face.</p></caption>
<graphic xlink:href="601634v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In contrast to the <italic>search cue</italic> period, face information during the <italic>search delay</italic> period (<xref rid="fig4" ref-type="fig">Figure 4A</xref>) could only be decoded in the left IPS (<italic>p</italic> = 0.019, uncorrected), perhaps reflecting working memory storage of the target face (<xref ref-type="bibr" rid="c9">Bettencourt &amp; Xu, 2016</xref>; <xref ref-type="bibr" rid="c56">Jeong &amp; Xu, 2016</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Decoding of face and scene information during the <italic>search delay</italic> period.</title>
<p>(A) Evidence of face information in <italic>a priori</italic> defined ROIs was only found in the left IPS. However, scene information was decoded in both IFJ and PPA, reflecting memory-evoked target-associated information in a network that encodes the target template. (B and C) Whole-brain searchlight analyses showed no additional brain regions carried significant information about the face (B), but additional scene information was found in the retrosplenial cortex (C).</p></caption>
<graphic xlink:href="601634v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>However, the associated scene was now reliably decoded in the bilateral PPA (left <italic>p</italic> = 0.007; right <italic>p</italic> &lt; 0.005), bilateral PEF (left <italic>p</italic> = 0.03; right <italic>p</italic> &lt; 0.005), and right IFJp (<italic>p</italic> = 0.027, uncorrected). Additional searchlight analyses revealed clusters (<italic>p</italic>TFCE &lt; .05) in bilateral retrosplenial cortex (<xref rid="fig4" ref-type="fig">Figure 4B</xref> and Table S2). The results show robust decoding of scene information in the IFJ-PPA network only during the delay period, suggesting that the face cue was translated into a “guiding template” of the associated scene in anticipation of visual search. Strikingly, the overall pattern shows a double dissociation between face information in both the prefrontal and category-selective visual cortex during the <italic>search cue</italic> period and scene information during the <italic>search delay</italic> period. Importantly, during the <italic>search delay</italic> period, the target face was not represented in the frontal attentional control regions, nor category-selective visual cortex. Whole brain analyses showed that no brain regions showed decoding of both face and scene information. Instead, the associated scene was activated during the delay period in IFJ and category-selective visual cortex, providing strong evidence for the hypothesis that target-associated information is activated from memory in preparation for guiding attention during target search.</p>
</sec>
<sec id="s2c">
<title>Scene-invalid trials require more cognitive control and delay target localization</title>
<p>The decoding results from the <italic>search delay</italic> period suggest that scene information is prioritized over that of the target face in preparation for visual search. If true, then scene-invalid trials should produce a prediction error and require reactive cognitive control to switch the search template back to the target face. The behavioral results support this interpretation: participants were more accurate (<italic>t</italic>(25) = 4.96, <italic>p</italic> &lt; 0.001, <italic>d</italic> = 0.97) and had shorter RTs (<italic>t</italic>(25) = −3.46, <italic>p</italic> = 0.002, <italic>d</italic> = 0.68) on scene-valid compared to scene-invalid trials (<xref rid="fig5" ref-type="fig">Figure 5A</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Behavioral and brain results from the face <italic>search</italic> period.</title>
<p>(A) Behavioral accuracy and RT both showed a scene-validity effect, suggesting scene information was used to guide attention during search. **<italic>p</italic> &lt; .01, ***<italic>p</italic> &lt; .001. Error bars refer to 95% confidence intervals. (B) Whole-brain group-level univariate contrast results showing significantly greater activations for the scene-invalid than scene-valid conditions are illustrated in blue (cold colors), and the reverse contrast in red (hot colors). (C) Contrast betas from the scene-invalid minus scene-valid conditions within each of the <italic>a priori</italic> ROIs, *<italic>p</italic> &lt; .05, **<italic>p</italic> &lt; .01.</p></caption>
<graphic xlink:href="601634v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To test for prediction error and reactive cognitive control, we conducted a univariate analysis contrasting scene-invalid and scene-valid trials. This “validity effect” contrast showed greater activation on scene-invalid trials in bilateral inferior frontal junction (IFJ) extending to the middle frontal gyrus, bilateral IPS, insula, and the anterior cingulate cortex (ACC; <xref rid="fig5" ref-type="fig">Figure 5B</xref>, Figure S1, and Table S3). These regions are similar to those reported in previous studies when cognitive control is needed for task switching, attentional shifting, and resolving cognitive conflict (<xref ref-type="bibr" rid="c32">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="c37">Dombert et al., 2016</xref>; <xref ref-type="bibr" rid="c50">Guerin et al., 2012</xref>; <xref ref-type="bibr" rid="c70">Liu et al., 2003</xref>). In particular, the combined activation of the lateral frontal cortex and ACC is indicative of conflict detection in ACC and the updating of goals in lateral PFC necessary for successful goal adaptation (<xref ref-type="bibr" rid="c38">Egner, 2023</xref>).</p>
<p>Moreover, looking specifically within our <italic>a priori</italic> ROIs, we find significant activation in bilateral IFJp, IFJa, vLPFC, and the left dLPFC and IPS during scene-invalid compared to scene-valid trials (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). Notably, scene-valid trials did not produce significantly greater activation in any of the ROIs, suggesting that scene-invalid trials required cognitive control to reset attentional priorities when expectations that the scene could be used to find the target were violated. Together these results are consistent with the idea that scene-invalid trials required the active reinstatement of the target face into working memory and the target template to make a response. These results are consistent with scene-invalid trials producing a prediction error that required flexible, reactive, cognitive control mechanisms to switch search templates (<xref ref-type="bibr" rid="c29">Cole et al., 2013</xref>), a process that slows responses and reduces accuracy. Finally, It is worth noting that both the univariate contrast maps and ROI analyses revealed a potential distinction between the posterior IFJ subregion, PEF, and the relatively anterior IFJp and IFJa in feature-based attention and cognitive control (<xref ref-type="bibr" rid="c78">Muhle-Karbe et al., 2016</xref>; <xref ref-type="bibr" rid="c104">Soyuhos &amp; Baldauf, 2023</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The canonical mechanism for visual attention involves preparatory changes in sensory gain that increases stimulus-driven responsivity when a matching stimulus appears (<xref ref-type="bibr" rid="c25">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="c32">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="c60">Kastner et al., 1999</xref>; <xref ref-type="bibr" rid="c72">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="c97">Reynolds &amp; Heeger, 2009</xref>; <xref ref-type="bibr" rid="c101">Serences &amp; Boynton, 2007</xref>; <xref ref-type="bibr" rid="c118">Vossel et al., 2014</xref>). This sensory modulation is thought to have its source in a network of frontoparietal regions that encode and hold target information as a “template” for sensory modulation (<xref ref-type="bibr" rid="c25">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="c41">Ester et al., 2016</xref>; <xref ref-type="bibr" rid="c42">Esterman &amp; Yantis, 2009</xref>; <xref ref-type="bibr" rid="c49">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c70">Liu et al., 2003</xref>; <xref ref-type="bibr" rid="c93a">Peelen &amp; Kastner, 2011</xref>; <xref ref-type="bibr" rid="c107">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c116">van Loon et al., 2018</xref>). These studies provide a foundation for understanding how working memory representations that are distributed across frontal and parietal regions are used to bias visual processing towards relevant information for achieving behavioral goals. However, most of the work on this topic has focused on target-veridical features. Much less is known about how these mechanisms support adaptive updates to information used to find the target during active visual search.</p>
<p>This stands in contrast to the considerable evidence showing that preparatory attention is flexible, highly adaptive, and sensitive to changing contexts. Many behavioral studies have shown that scene structure, statistically co-occurring object pairs, and large, stable, predictive “anchor objects” are all used to guide attention and locate smaller objects (<xref ref-type="bibr" rid="c7">Battistoni et al., 2017</xref>; <xref ref-type="bibr" rid="c13">Boettcher et al., 2018</xref>; <xref ref-type="bibr" rid="c22">Castelhano &amp; Krzys, 2020</xref>; <xref ref-type="bibr" rid="c23">Castelhano et al., 2009</xref>; <xref ref-type="bibr" rid="c30">Collegio et al., 2019</xref>; <xref ref-type="bibr" rid="c34">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="c46">Gayet &amp; Peelen, 2022</xref>; <xref ref-type="bibr" rid="c52">Hall &amp; Geng, 2024</xref>; <xref ref-type="bibr" rid="c54">Helbing et al., 2022</xref>; <xref ref-type="bibr" rid="c57">Josephs et al., 2016</xref>; <xref ref-type="bibr" rid="c74">Mack &amp; Eckstein, 2011</xref>; <xref ref-type="bibr" rid="c76">Malcolm &amp; Shomstein, 2015</xref>; <xref ref-type="bibr" rid="c80">Nah &amp; Geng, 2022</xref>; <xref ref-type="bibr" rid="c92">Peelen et al., 2024</xref>; <xref ref-type="bibr" rid="c117">Vo et al., 2019</xref>; <xref ref-type="bibr" rid="c125">Yu et al., 2023</xref>; <xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>). For example, in a previous behavioral study, we showed that when the target is hard to find, scene information is used as a proxy in the target template to guide attention toward the likely target location more efficiently (<xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>).</p>
<p>Despite the vast behavioral evidence for the use of non-target information in target-guided visual search, there has been little evidence so far for the presence of target-associated information in preparatory “target templates” encoded in frontal-parietal-sensory attentional control networks. One exception is a recent study that found preparatory activation in the lateral occipital complex (LOC) for a target-associated anchor object in advance of target detection (<xref ref-type="bibr" rid="c67">Lerebourg et al., 2024</xref>). In this study, participants were asked to find either a book or a bowl located on two visually unique tables within two living room scenes. The scene context predicted which table would hold each object. When participants were given a target cue and shown a scene with the table regions masked, the associated table, but not the target, could be decoded from LOC. This study provided compelling evidence that target-associated information is preferentially represented in advance of object detection. However, because the tables were implied by the scene preview, it is somewhat unclear if decoding of the tables reflected the preparatory reinstatement of a target-associated memory or priming of task-relevant information by the scene. There was also no information about template source regions in frontal cortex, such as IFJ (<xref ref-type="bibr" rid="c6">Baldauf &amp; Desimone, 2014</xref>; <xref ref-type="bibr" rid="c8">Bedini et al., 2023</xref>; <xref ref-type="bibr" rid="c104">Soyuhos &amp; Baldauf, 2023</xref>).</p>
<p>Our study builds on this prior literature by teaching participants novel face-scene associations and then using the faces as cues and targets in a visual search task. Scenes were technically task-irrelevant in the search task and were not shown during the cue period. Thus, the presence of any pre-search preparatory scene information in the brain has to be reinstated from memory based on a previously learned association without direct visual prompting. This is important because any scene information represents memory-evoked response to a specific visually presented face cue. First, however, we confirmed that the identity of the face shown as the cue was decoded in bilateral FFA, right dLPFC, bilateral SPL, and left IFJp using a priori ROIs. Decoding in left dLFPC was found in subsequent searchlight analyses, which revealed symmetric clusters of voxels located bilaterally in the middle frontal gyrus extending to the superior frontal gyrus (<xref rid="fig3" ref-type="fig">Figure 3B</xref>, Table S1). Overall, these results were as expected, and showed that the onset of the face cue was represented in sensory and task-based control regions (<xref ref-type="bibr" rid="c26">Chiu et al., 2011</xref>; <xref ref-type="bibr" rid="c31">Contreras et al., 2013</xref>; <xref ref-type="bibr" rid="c51">Guntupalli et al., 2017</xref>; <xref ref-type="bibr" rid="c56">Jeong &amp; Xu, 2016</xref>; <xref ref-type="bibr" rid="c71">Long &amp; Kuhl, 2018</xref>; <xref ref-type="bibr" rid="c84">Nestor et al., 2011</xref>).</p>
<p>Next, and in contrast to the cue-evoked face representations, the only brain region containing scene category information during the cue period was in vLPFC. In prior studies, scene decoding has been found in vLPFC during perceptual scene categorization tasks (<xref ref-type="bibr" rid="c58">Jung et al., 2018</xref>; <xref ref-type="bibr" rid="c59">Jung &amp; Walther, 2021</xref>; <xref ref-type="bibr" rid="c96">Qin et al., 2011</xref>; <xref ref-type="bibr" rid="c119">Walther et al., 2009</xref>), but more generally, vLPFC is engaged when cognitive control is required to adjudicate task-based memories (<xref ref-type="bibr" rid="c5">Badre &amp; Wagner, 2007</xref>; <xref ref-type="bibr" rid="c38">Egner, 2023</xref>; <xref ref-type="bibr" rid="c68">Levy &amp; Wagner, 2011</xref>; <xref ref-type="bibr" rid="c79">Muhle-Karbe et al., 2018</xref>; <xref ref-type="bibr" rid="c82">Nee &amp; D’Esposito, 2016</xref>; <xref ref-type="bibr" rid="c85">Neubert et al., 2014</xref>; <xref ref-type="bibr" rid="c111">Tamber-Rosenau et al., 2018</xref>). This suggests that the face cue evoked the retrieval of scene category information from memory. The most important question is therefore whether this cue related scene information in vLPFC would be translated into a preparatory search template.</p>
<p>We predicted that, if the associated scene was used as the target template, we should observe successful decoding of scene categories in scene-selective sensory cortex during the delay period reflecting enhancements in sensory gain, and in parietal and lateral prefrontal cortex reflecting scene information in working memory and cognitive control. It is well-documented that template information can be decoded in distributed cortical regions, but that each region likely plays a different computational role (<xref ref-type="bibr" rid="c40">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="c41">Ester et al., 2016</xref>; <xref ref-type="bibr" rid="c49">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c65">Lee &amp; Geng, 2017</xref>; <xref ref-type="bibr" rid="c71">Long &amp; Kuhl, 2018</xref>; <xref ref-type="bibr" rid="c93a">Peelen &amp; Kastner, 2011</xref>; <xref ref-type="bibr" rid="c103">Soon et al., 2013</xref>; <xref ref-type="bibr" rid="c107">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c116">van Loon et al., 2018</xref>; <xref ref-type="bibr" rid="c121">Witkowski &amp; Geng, 2023</xref>). Consistent with this, we the associated scene category was decoded in IFJ regions and PPA during the delay period. Scene information decoded in vLPFC during the cue period appeared to trigger retrieval of the associated scene, which was turned into a scene template in IFJ and PPA in preparation for target search (<xref ref-type="bibr" rid="c5">Badre &amp; Wagner, 2007</xref>; <xref ref-type="bibr" rid="c122">Xu et al., 2022</xref>). Additional searchlight analyses revealed clusters in bilateral retrosplenial cortex that contained scene information (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, Table S2). Interestingly, no face information was decodable from FFA or IFJ during the search delay period, suggesting it was not actively maintained in the target template. However, there was evidence of decoding in IPS, indicating that face information was still held in working memory (<xref ref-type="bibr" rid="c9">Bettencourt &amp; Xu, 2016</xref>; <xref ref-type="bibr" rid="c27">Christophel et al., 2018</xref>; <xref ref-type="bibr" rid="c40">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="c43">Favila et al., 2018</xref>; <xref ref-type="bibr" rid="c49">Gong et al., 2022</xref>; <xref ref-type="bibr" rid="c56">Jeong &amp; Xu, 2016</xref>; <xref ref-type="bibr" rid="c64">Kwak &amp; Curtis, 2022</xref>; <xref ref-type="bibr" rid="c90">Olmos-Solis et al., 2021</xref>; <xref ref-type="bibr" rid="c123">Yu &amp; Postle, 2021</xref>).</p>
<p>The consequences of prioritizing associated scenes over the target face were seen in longer RT and more errors on scene-invalid trials. The univariate results suggested that scene-invalid trials triggered a prediction error in ACC that led to an updating of what information should be used to find the target in vLPFC (including IFJp/IFJa) and IPS. This pattern of data supports the notion that preparatory scene activity in the delay period supplanted the target in the “target” template in service of facilitating behavior; when it was inaccurate, however, reactive cognitive control was necessary to update search adaptively (<xref ref-type="bibr" rid="c17">Botvinick et al., 2004</xref>; <xref ref-type="bibr" rid="c19">Braver, 2012</xref>; <xref ref-type="bibr" rid="c38">Egner, 2023</xref>; <xref ref-type="bibr" rid="c61">Kolling et al., 2016</xref>; <xref ref-type="bibr" rid="c88">O’Reilly et al., 2013</xref>). Our current findings are likely due to the utility of scene information for finding the face. In this task, the target and distractor faces were perceptually similar, but scene information was highly distinctive. If the target face was easy to discriminate, the utility of the scene disappears (<xref ref-type="bibr" rid="c127">Zhou and Geng, 2024</xref>). Under those conditions, we would not expect to see scene information loaded into the target template. Thus, we expect our results to generalize only when target-associated information is easier to discriminate than target information.</p>
<p>In summary, our study builds on a growing body of literature on preparatory attention, focusing specifically on what information is used to generate the most predictive target template. We provide novel evidence that target-associated information can supplant the actual target in the “target” template when the associated information is expected to more easily facilitate behavioral goals. In our case, the cued target face was translated into a template for a probabilistically predictive associated scene in IFJ and sensory cortex in preparation for search. This suggests that when there is uncertainty in target detection and decisions, additional information is dynamically recalled to facilitate performance (<xref ref-type="bibr" rid="c53">Hansen et al., 2012</xref>; <xref ref-type="bibr" rid="c109">Summerfield et al., 2011</xref>; <xref ref-type="bibr" rid="c121">Witkowski &amp; Geng, 2023</xref>). The results are consistent with decades of behavioral work showing the adaptive nature of attention in supporting goal-directed behavior. It goes farther in providing a first demonstration how this mechanism unfolds using target-associated information from memory in frontal-parietal-sensory networks to anticipate visual search.</p>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<sec id="s4a">
<title>Participants</title>
<p>The sample size was chosen based on a power analysis of a previous behavioral study (<xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>) and similar fMRI MVPA studies (<xref ref-type="bibr" rid="c1">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="c116">van Loon et al., 2018</xref>). Using the more conservative estimate based on the behavioral validity effect size (<italic>d</italic>z = .678), the estimated minimum sample size to achieve significant effects (<italic>p</italic> = .05, two-tailed) with a power .9 was N = 25. A final number of twenty-six participants (mean age = 21.1 years, range = 18 – 29 years, females = 14, males = 12) were recruited from the University of California, Davis, and were given a combination of course credit and monetary compensation ($50) for a 2-hour MRI session. All participants were right-handed and native English speakers or had spoken English before the age of 5. None of them reported a history of neurological or psychiatric disorders. In addition, they all had normal or corrected-to-normal vision and passed the color blindness test (<ext-link ext-link-type="uri" xlink:href="https://colormax.org/color-blind-test/">https://colormax.org/color-blind-test/</ext-link>). Two additional participants were removed due to excessive motion during scanning (head movement &gt; 3mm), and a third participant was removed due to falling asleep in the scanner. The study adhered to the ethical principles of the Declaration of Helsinki, and all consent forms and procedures were approved by the Institutional Review Board of the University of California, Davis.</p>
</sec>
<sec id="s4b">
<title>Stimuli</title>
<p>Stimuli for all experiments consisted of 16 faces (<xref rid="fig1" ref-type="fig">Figure 1A</xref> and Figure S2) selected from the Chicago Face Database (CFD, <xref ref-type="bibr" rid="c73">Ma et al., 2015</xref>) and 64 scenes (<xref rid="fig1" ref-type="fig">Figure 1A</xref> and Figure S3) selected from a scene categorization study (<xref ref-type="bibr" rid="c58">Jung et al., 2018</xref>). The ratings for gender and race for all faces were restricted to be larger than 0.9 (based on normalized rating scores ranging between 0 to 1 provided by the CFD; a higher score indicates being more representative of that category). As in our previous study (<xref ref-type="bibr" rid="c127">Zhou &amp; Geng, 2024</xref>), we used four target faces derived from crossings between two genders (woman and man) and races (black and white). An additional three faces were selected from each category to serve as distractors for the study. An oval mask was used to crop all selected faces to reduce the visibility of extra-facial features, e.g., hair. The scene categories were crossed between content (nature and urban) and layout (open and closed). There were 16 exemplar images from each scene category used in the experiment. The resolution of the face images was 150 × 200 pixels (2° × 2.7° visual angles) and the scenes were 800 × 600 pixels (10.7° × 8° visual angles). All stimuli were presented against a gray background (RGB: 128, 128, 128). The low-level luminance and contrast of all face and scene images were controlled with the SHINE_color toolbox (<xref ref-type="bibr" rid="c120">Willenbockel et al., 2010</xref>).</p>
</sec>
<sec id="s4c">
<title>Experimental design</title>
<p><italic>Face-scene associative learning task.</italic> Before the MRI scan session, participants first learned four face-scene category associations (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). During the learning phase, each of the face-scene pairs was shown along with a narrative, for example, “This is Holly. She is a ranger, so you will find her in the forest.” Next, participants were tested for their memory on the newly learned associations in a face-scene pair matching task (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). On each test trial, both a face and scene appeared on the screen, and participants were asked to judge whether the face was associated with the scene by pressing the “m” for a match, or “n” for a nonmatch. Feedback was given with the word “correct” or “incorrect” presented for 0.5 s after each response. The inter-trial interval (ITI) was 0.5 s. Each of the four faces was tested on 18 scene-match trials and 9 scene-nonmatch trials. There was a total of 108 trials presented in a pseudorandomized order.</p>
<p>For the 18 match trials, participants viewed the target face on 18 different scene exemplars from the same category. For the 9 nonmatch trials, participants viewed the target face on 3 different scene images (randomly selected) from the remaining three scene categories.</p>
<p><italic>Face search task.</italic> After the learning task, participants completed the face search task in the first scan session (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). At the beginning of each trial, a face cue was presented for 1 s followed by an 8 s fixation cross. Next, the search display appeared for 0.25 s. The search display was followed by an 8.75 s ITI. The search display was composed of the target face and one randomly selected distractor face. Each face was superimposed on a scene image. The face-scene pairs appeared on the left and right of the screen split by a central fixation cross. The faces were centered in each of the scenes and the distance from the inner edge of the scene to the central fixation was 1° visual angle. The target face was always presented while the distractor face was randomly drawn from the set of three faces that matched the target in both gender and race. Participants indicated the location of the target face by pressing the “1” or “2” key on the MRI-compatible keypad corresponding to the left or right side. The response was limited to within 2 s.</p>
<p>Search trials were divided into scene-valid and scene-invalid conditions. The scene was valid on 75% of trials and invalid on the remaining 25%. On the scene-valid trials, the target face was always superimposed on its associated scene while the distractor face was superimposed on a scene from one of the three unassociated categories. On scene-invalid trials, the scenes that appeared with the target and distractor faces were from two different scene categories, both of which were unassociated with the target face.</p>
<p>All participants completed 8 runs of the face search task. There were 16 trials (12 scene-valid trials and 4 scene-invalid trials) in each run and 4 trials with each of the four target faces. The participants were not informed about the exact probability of the valid to invalid trials in the experiment. The trial order as well as the location of the target face in the search display was counterbalanced and pseudorandomized. Participants completed a full practice run in the scanner at the beginning to familiarize them with the task and the testing environment.</p>
<p><italic>Face and scene 1-back task</italic>. After completing the face search task, a 6-min T1-weighted structural scan was collected. Then, each participant completed a 1-back task in a second scan session with 4 runs (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). On each trial, one stimulus was presented at the center for 1 s, followed by an 8 s fixation. Participants were instructed to make a button press when the current stimulus was the same as the previous one. There were four trials for each target face and scene category. Different exemplars were used for each repetition of the same scene category. An additional 6 trials were designated as the 1-back trial with a repeated stimulus. The trial sequence was counterbalanced and pseudorandomized. Participants completed a full practice run in the scanner before the formal test.</p>
<p><italic>Functional category-selective localizer</italic>. We identified two regions of interest (ROIs), the FFA and the PPA for each individual participant in an independent functional localizer task (<xref ref-type="bibr" rid="c106">Stigliani et al., 2015</xref>). Stimuli were 2-D grayscale images (∼16° × 16° for all stimuli) consisting of faces, houses, corridors, cars, instruments, and phase-scrambled images. Eight images from the same object category were on for 0.4 s and off for 0.1 s in a 4-s mini-block. Participants performed a 1-back memory task and repeats occurred in half of the blocks in each run. Each run consisted of 54 blocks evenly divided by the 6 stimulus categories, and the block order was counterbalanced and pseudo-randomized across runs. Two runs were collected for each participant.</p>
</sec>
<sec id="s4d">
<title>Stimulus apparatus</title>
<p>The <italic>Face-scene associative learning task</italic> was conducted on a 14-inch MacBook Pro laptop with a spatial resolution of 1920 × 1080, and all stimuli were presented using the Testable platform (<ext-link ext-link-type="uri" xlink:href="https://www.testable.org">www.testable.org</ext-link>). The remaining experimental tasks were conducted while the participants were in the scanner. All stimuli for the scanner experiments were generated using Psychotoolbox-3 installed on a Dell desktop PC and displayed on a 24-inch BOLDscreen LCD monitor with a spatial resolution of 1920 × 1200 pixels. Participants viewed stimuli through a mirror attached to the head coil which projected the monitor ∼ 120 cm away outside of the scanner bore.</p>
</sec>
<sec id="s4e">
<title>MRI acquisition and preprocessing</title>
<p>All scans were performed on a 3-Tesla Siemens Skyra scanner with a 32-channel phased-array head coil at University of California, Davis. Functional scans using T2-weighted echoplanar imaging (EPI) sequence with an acceleration factor of 2 were acquired with whole-brain volumes of 48 axial slices of 3 mm thickness (TR/TE 1500/24.6ms, flip angle 75°, base/phase resolution 70/100, 3 × 3 mm<sup>2</sup>, FOV 210 mm). High-resolution structural MPRAGE T1-weighted images (TR/TE 1800/2.97 ms, flip angle 7°, base/phase resolution 256/100, FOV 256 mm, 208 sagittal slices) were acquired and were used for anatomical normalization, co-registration, and cortical surface reconstruction. The whole MRI session was finished within 2 hours.</p>
<p>Functional and structural data were preprocessed using SPM12 (Wellcome Department of Imaging Neuroscience), FreeSurfer (<xref ref-type="bibr" rid="c33">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="c45">Fischl et al., 1999</xref>), and in-house MATLAB (MathWorks) code. The first six initial functional scans of each run were discarded to allow for equilibrium effects. The preprocessing for functional EPI data included slice-time correction and spatial realignment. Using a two-pass procedure, fMRI data from all three scan sessions were aligned to the mean of the EPI images of the first session run. Participants with head motion &gt; 3 mm were excluded. The structural image was coregistered with the mean image. Cortical hemispheric surface reconstruction was performed using the ‘recon-all’ commend in the FreeSurfer. All fMRI analyses were performed in the native individual space without spatial smoothing, except for a 4 mm full-width at half-maximum (FWHM) smoothing was applied to the <italic>functional category-selective localizer</italic> runs.</p>
<p>The segmentation utility in SPM12 was applied to estimate gray and white matter boundary parameters for spatial normalization. Then, the analyzed native space functional data (i.e., univariate beta images and searchlight classification accuracy images) was normalized into the standard 2 × 2 × 2 mm<sup>3</sup> MNI reference space and smoothed with an 8 mm FWHM isotropic kernel for group-based whole-brain analyses.</p>
</sec>
<sec id="s4f">
<title>fMRI general linear model (GLM)</title>
<p>For each scan session, a GLM was generated by convolving the canonical hemodynamic response functions with experimental conditions for each voxel. Six motion realignment parameters in each experimental run were included as nuisance regressors to control for head motion confounds. For the <italic>face search task</italic>, the GLM was based on different time segments of the task corresponding to the face <italic>search cue</italic>, <italic>search delay</italic> period, and search display. The GLM was constructed with 11 regressors for each experimental run. The first four regressors had a duration of 1 s and were used to estimate the four different face search cue identities. The next four regressors were 8 s long and estimated the search delay period following each face. Three additional regressors with a duration of 1 s were used to model the three types of search displays: scene-valid trials, scene-invalid trials, and error response trials.</p>
<p>A separate GLM was used to model the <italic>face and scene 1-back task</italic>. The GLM was constructed with 17 regressors for each experimental condition in each run. There were 8 regressors for each of the four face images and its corresponding delay period, and another 8 regressors for each scene category image and its corresponding delay period. The BOLD responses to the stimulus image and its corresponding delay period were modeled separately, just as it was for the <italic>face search task</italic>. The same model parameters were used so that the two tasks would be compatible for our cross-task decoding procedure. The last regressor with 1 s duration was used to define the 1-back event, i.e., response to a repeated image, which was not a trial type of interest.</p>
<p>A third GLM was used to model the <italic>functional category-selective localizer</italic> in order to identify FFA and PPA in each participant. The GLM was constructed with 6 regressors with a 4 s duration to estimate all object categories (faces, houses, corridors, cars, instruments, and phase-scrambled images) within two runs. Contrast beta images between specific types of stimulus categories were used to identify category-selective regions in the visual cortex at the individual level.</p>
</sec>
<sec id="s4g">
<title>Regions of interest (ROIs) definition</title>
<p>The ROIs of primary interest focused on subregions of the frontal, parietal, and visual cortex that are known to be involved in attentional and cognitive control. As shown in <xref rid="fig2" ref-type="fig">Figure 2D</xref>, A total of twelve ROIs were defined in the native space of each participant through a combination of the Human Connectome Project Multi-Modal Parcellation (HCP-MMP1; <xref ref-type="bibr" rid="c48">Glasser et al., 2016</xref>), the resting-state network (<xref ref-type="bibr" rid="c100">Schaefer et al., 2018</xref>), and our independent category-selective localizer. Each ROI selected from the surface-based HCP-MMP1 atlas and the resting-state network was first transformed from the grouped-based fsaverage surface into an individual surface, and then remapped onto the volumetric native space using the ‘mri_surf2vol’.</p>
<p>The HCP-MMP1 atlas was used to define three ROIs corresponding to the IFJ (<xref ref-type="bibr" rid="c8">Bedini et al., 2023</xref>), located along the posteriorly-to-anteriorly along the inferior frontal sulcus, namely the premotor eyefield (PEF), posterior IFJ (IFJp), anterior IFJ (IFJa). Using the 17-network resting-state network, we extracted the dorsal attention network, the ventral attention network, and the frontoparietal control network ROIs the lateral frontal and parietal cortex. The cortical parcellations in the three networks in the lateral frontal cortex correspond to the frontal eye fields (FEF), ventrolateral prefrontal cortex (vLPFC), and dorsolateral prefrontal cortex (dLPFC); the cortical parcellations in the lateral parietal cortex correspond to the superior parietal lobule (SPL), intraparietal sulcus (IPS), and inferior parietal lobule (IPL). In addition, the early visual cortex (V1) ROI was defined from the visual network located in areas labeled the striate cortex and striate calcarine. The last two visual ROIs, the fusiform face area (FFA) and parahippocampal place area (PPA), were identified from an independent functional category-selective localizer. The preprocessed data of the two functional localizer runs were fitted with a first-level GLM to estimate BOLD responses to each of the stimulus categories in the individual volumetric brain. The FFA was defined by a contiguous cluster of voxels in the fusiform gyrus from the contrast between faces vs. all remaining categories, at <italic>p</italic> &lt; 0.001, uncorrected; the PPA was defined as the contiguous cluster of voxels in the parahippocampal gyrus from the contrast between scenes/corridors vs. all remaining categories, at <italic>p</italic>(FWE) &lt; 0.005. No activation was observed at these thresholds for six participants, thus a more lenient threshold was used to allow sufficient voxels in FFA and PPA to be identified for analyses. The average number of voxels for each ROI is reported in the Supplementary Materials, Table S4. Note that a large portion of IFJp and IFJa from the HCP-MMP1 atlas overlapped with the vLPFC from the 17-network atlas. In addition, PEF from the HCP-MMP1 atlas highly overlapped with the precentral label from the dorsal attention network in the left hemisphere and the ventral attention network in the right using the 17-network atlas (Figure S4).</p>
</sec>
<sec id="s4h">
<title>Univariate whole-brain analysis</title>
<p>Although our primary goal was to identify cortical involvement in holding the search template during the face cue and delay periods, we also examined the scene validity effect during the search display period of the <italic>face search task</italic>. To do this, the normalized and smoothed data for the scene-valid and scene-invalid conditions were entered into a second-level GLM with random effects modeled at the group level. Linear contrasts between the two conditions were used to reveal selective regions with significant BOLD activations to either the scene-valid (valid &gt; invalid) or scene-invalid (invalid &gt; valid) conditions. The resultant t-value maps were corrected for multiple comparisons at the cluster level using the threshold-free cluster enhancement (TFCE; <xref ref-type="bibr" rid="c102">Smith &amp; Nichols, 2009</xref>; <xref ref-type="bibr" rid="c105">Spisák et al., 2019</xref>) implemented in SPM12. The threshold was set at <italic>p</italic>TFCE &lt; 0.005 for clusters with greater than 50 voxels.</p>
</sec>
<sec id="s4i">
<title>Multivariate pattern analysis (MVPA)</title>
<p>The estimated beta parameters from the <italic>face search task</italic> and the <italic>face and scene 1-back task</italic> first-level GLMs were used to decode whether the target face and/or its associated scene was held as the search template during the <italic>search cue</italic> or <italic>search delay</italic> periods. The beta parameters extracted from each of the 12 ROIs were normalized to remove univariate differences between conditions before being submitted to a binary linear SVM classifier implemented in LIBSVM (<xref ref-type="bibr" rid="c24">Chang &amp; Lin, 2011</xref>), with a default cost parameter of 1. <xref rid="fig2" ref-type="fig">Figure 2C</xref> illustrates the single-step cross-classification scheme (<xref ref-type="bibr" rid="c18">Brandman &amp; Peelen, 2017</xref>; <xref ref-type="bibr" rid="c46">Gayet &amp; Peelen, 2022</xref>) adopted in the current study. For the decoding of faces during the search cue period, the classifiers were trained to discriminate between pairs of faces (with a full combination of 6 pairs based on 4 types of face cues) on the <italic>face search task</italic> runs. These classifiers were then tested using <italic>face and scene</italic> information from the 1-back task stimulus sample period. Following the same procedure for the delay period, classifiers were trained to discriminate between pairs of <italic>search delay</italic> period information following each face cue (6 pairs of delay periods); the classifiers were then tested on the delay period following face or scene sample stimuli from the <italic>face and scene 1-back task</italic>. The classification accuracy was estimated by averaging performance from the six binary classifiers for each type of cross-classification.</p>
</sec>
<sec id="s4j">
<title>Statistical inference</title>
<p>Group-level statistical significance testing was established based on a nonparametric permutation method in which data labels were randomized. The null hypothesis classification accuracy distribution was generated by 10,000 iterations of each type of cross-classification for each participant. The group-level null distribution was then calculated by averaging these classification accuracies across participants. The permuted p-value was defined by the proportion of counts in the null distribution that were equal to or higher than the observed real group average classification accuracy, (n + 1)/(10,000 + 1). This p-value was used as the assessment for statistical significance, and all resulting p-values controlled for family-wise error by using Bonferroni correction.</p>
</sec>
<sec id="s4k">
<title>Searchlight analysis</title>
<p>We also investigated the whole-brain multivariate decoding results using the searchlight approach (<xref ref-type="bibr" rid="c62">Kriegeskorte et al., 2006</xref>). At the individual level, the linear SVM decoding accuracy was assigned to the center voxel within a 9 mm radius sphere in volumetric space. This procedure estimates classification accuracies for all voxels within a whole-brain mask. The pair-wise classification schemes were identical to that implemented for the ROI multivariate decoding analysis. The resulting individual participant whole-brain information maps for different classifications were first smoothed using a 4 mm FWHM Gaussian kernel and then normalized and tested for statistical significance against 50% chance decoding using a one-sample t-test (one-tailed). The statistical threshold of all group-level searchlight maps was corrected for multiple comparisons at the cluster level using TFCE.</p>
</sec>
</sec>

</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This research received funding supported by the National Institutes of Health under Grant R01-MH113855 to Joy J. Geng, and the Humanity and Social Science Youth Foundation of the Ministry of Education in China under Grant 24XJC190010 to Zhiheng Zhou.</p>
</ack>
<sec id="d1e1586" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>CRediT authorship contribution statement</title>
<p>Zhiheng Zhou: Conceptualization, Methodology, Investigation, Writing – original draft, Writing – review &amp; editing, Data curation, Formal analysis, Visualization.</p>
<p>Joy J. Geng: Conceptualization, Methodology, Investigation, Writing – original draft, Writing – review &amp; editing, Supervision.</p>
<p>The deidentified data, analysis code, and supplemental materials are available on Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/xw8hm/">https://osf.io/xw8hm/</ext-link>).</p>
</sec>
</sec>
<sec id="suppd1e1586" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1577">
<label>ZhouGeng_elife102_supp</label>
<media xlink:href="supplements/601634_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Albers</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Toni</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Dijkerman</surname>, <given-names>H. C.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex</article-title>. <source>Current Biology</source>, <volume>23</volume>(<issue>15</issue>), <fpage>1427</fpage>–<lpage>1431</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alexander</surname>, <given-names>R. G.</given-names></string-name>, &amp; <string-name><surname>Zelinsky</surname>, <given-names>G. J.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Visual similarity effects in categorical search</article-title>. <source>J Vis</source>, <volume>11</volume>(<issue>8</issue>). <pub-id pub-id-type="doi">10.1167/11.8.9</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badre</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Nee</surname>, <given-names>D. E.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Frontal Cortex and the Hierarchical Control of Behavior</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>22</volume>(<issue>2</issue>), <fpage>170</fpage>–<lpage>188</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2017.11.005</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badre</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Wagner</surname>, <given-names>A. D.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Left ventrolateral prefrontal cortex and the cognitive control of memory</article-title>. <source>Neuropsychologia</source>, <volume>45</volume>(<issue>13</issue>), <fpage>2883</fpage>–<lpage>2901</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.06.015</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldauf</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Neural Mechanisms of Object-Based Attention</article-title>. <source>Science</source>, <volume>344</volume>(<issue>6182</issue>), <fpage>424</fpage>–<lpage>427</lpage>. <pub-id pub-id-type="doi">10.1126/science.1247003</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Battistoni</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Stein</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Preparatory attention in visual cortex</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1396</volume>(<issue>1</issue>), <fpage>92</fpage>–<lpage>107</lpage>. <pub-id pub-id-type="doi">10.1111/nyas.13320</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bedini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Olivetti</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Avesani</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Baldauf</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Accurate localization and coactivation profiles of the frontal eye field and inferior frontal junction: an ALE and MACM fMRI meta-analysis</article-title>. <source>Brain Structure &amp; Function</source>. <pub-id pub-id-type="doi">10.1007/s00429-023-02641-y</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bettencourt</surname>, <given-names>K. C.</given-names></string-name>, &amp; <string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>1</issue>), <fpage>150</fpage>–<lpage>157</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4174</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bichot</surname>, <given-names>N. P.</given-names></string-name>, <string-name><surname>Heard</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>DeGennaro</surname>, <given-names>E. M.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2015a</year>). <article-title>A Source for Feature-Based Attention in the Prefrontal Cortex</article-title>. <source>Neuron</source>, <volume>88</volume>(<issue>4</issue>), <fpage>832</fpage>–<lpage>844</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.10.001</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bichot</surname>, <given-names>N. P.</given-names></string-name>, <string-name><surname>Heard</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>DeGennaro</surname>, <given-names>E. M.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2015b</year>). <article-title>A Source for Feature-Based Attention in the Prefrontal Cortex</article-title>. <source>Neuron</source>, <volume>88</volume>(<issue>4</issue>), <fpage>832</fpage>–<lpage>844</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.10.001</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bichot</surname>, <given-names>N. P.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Ghadooshahy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2019</year>). <article-title>The role of prefrontal cortex in the control of feature attention in area V4</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), <fpage>5727</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-13761-7</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boettcher</surname>, <given-names>S. E. P.</given-names></string-name>, <string-name><surname>Draschkow</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Dienhart</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Vo</surname>, <given-names>M. L.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Anchoring visual search in scenes: Assessing the role of anchor objects on eye movements during visual search</article-title>. <source>Journal of Vision</source>, <volume>18</volume>(<issue>13</issue>), <fpage>11</fpage>. <pub-id pub-id-type="doi">10.1167/18.13.11</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boettcher</surname>, <given-names>S. E. P.</given-names></string-name>, <string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name>, &amp; <string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2020</year>). <article-title>One Thing Leads to Another: Anticipating Visual Object Identity Based on Associative-Memory Templates</article-title>. <source>Journal of Neuroscience</source>, <volume>40</volume>(<issue>20</issue>), <fpage>4010</fpage>–<lpage>4020</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.2751-19.2020</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Botvinick</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>J. D.</given-names></string-name>, &amp; <string-name><surname>Carter</surname>, <given-names>C. S.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Conflict monitoring and anterior cingulate cortex: an update</article-title>. <source>Trends Cogn Sci</source>, <volume>8</volume>(<issue>12</issue>), <fpage>539</fpage>–<lpage>546</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2004.10.003</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brandman</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Peelen</surname>, <given-names>X. V.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Interaction between Scene and Object Processing Revealed by Human fMRI and MEG Decoding</article-title>. <source>Journal of Neuroscience</source>, <volume>37</volume>(<issue>32</issue>), <fpage>7700</fpage>–<lpage>7710</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.0582-17.2017</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Braver</surname>, <given-names>T. S.</given-names></string-name></person-group> (<year>2012</year>). <article-title>The variable nature of cognitive control: a dual mechanisms framework</article-title>. <source>Trends Cogn Sci</source>, <volume>16</volume>(<issue>2</issue>), <fpage>106</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2011.12.010</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Braver</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Paxton</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Locke</surname>, <given-names>H. S.</given-names></string-name>, &amp; <string-name><surname>Barch</surname>, <given-names>D. M.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Flexible neural mechanisms of cognitive control within human prefrontal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>106</volume>(<issue>18</issue>), <fpage>7351</fpage>–<lpage>7356</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.0808187106</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Castelhano</surname>, <given-names>M. S.</given-names></string-name>, &amp; <string-name><surname>Krzys</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Rethinking Space: A Review of Perception, Attention, and Memory in Scene Processing</article-title>. <source>Annual Review of Vision Science</source>, <volume>6</volume>, <fpage>563</fpage>–<lpage>586</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-121219-081745</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Castelhano</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Mack</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Henderson</surname>, <given-names>J. M.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Viewing task influences eye movement control during active scene perception</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>3</issue>), <fpage>6</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1167/9.3.6</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>C.-C.</given-names></string-name>, &amp; <string-name><surname>Lin</surname>, <given-names>C.-J.</given-names></string-name></person-group> (<year>2011</year>). <article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM transactions on intelligent systems and technology (TIST</source><italic>)</italic>, <volume>2</volume>(<issue>3</issue>), <fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chelazzi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>E. K.</given-names></string-name>, <string-name><surname>Duncan</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1993</year>). <article-title>A neural basis for visual search in inferior temporal cortex</article-title>. <source>Nature</source>, <volume>363</volume>(<issue>6427</issue>), <fpage>345</fpage>–<lpage>347</lpage>. <pub-id pub-id-type="doi">10.1038/363345a0</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chiu</surname>, <given-names>Y. C.</given-names></string-name>, <string-name><surname>Esterman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Rosen</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Yantis</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Decoding task-based attentional modulation during face categorization</article-title>. <source>J Cogn Neurosci</source>, <volume>23</volume>(<issue>5</issue>), <fpage>1198</fpage>–<lpage>1204</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2010.21503</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christophel</surname>, <given-names>T. B.</given-names></string-name>, <string-name><surname>Iamshchinina</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Allefeld</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Haynes</surname>, <given-names>J. D.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Cortical specialization for attended versus unattended working memory</article-title>. <source>Nature Neuroscience</source>, <volume>21</volume>(<issue>4</issue>), <fpage>494</fpage>-+. <pub-id pub-id-type="doi">10.1038/s41593-018-0094-4</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christophel</surname>, <given-names>T. B.</given-names></string-name>, <string-name><surname>Klink</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Spitzer</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Roelfsema</surname>, <given-names>P. R.</given-names></string-name>, &amp; <string-name><surname>Haynes</surname>, <given-names>J. D.</given-names></string-name></person-group> (<year>2017</year>). <article-title>The Distributed Nature of Working Memory</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>21</volume>(<issue>2</issue>), <fpage>111</fpage>–<lpage>124</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2016.12.007</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Reynolds</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Power</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Repovs</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Anticevic</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Braver</surname>, <given-names>T. S.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Multi-task connectivity reveals flexible hubs for adaptive task control</article-title>. <source>Nature Neuroscience</source>, <volume>16</volume>(<issue>9</issue>), <fpage>1348</fpage>–<lpage>1355</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3470</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collegio</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nah</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Scotti</surname>, <given-names>P. S.</given-names></string-name>, &amp; <string-name><surname>Shomstein</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Attention scales according to inferred real-world object size</article-title>. <source>Nature Human Behaviour</source>, <volume>3</volume>(<issue>1</issue>), <fpage>40</fpage>–<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1038/s41562-018-0485-2</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Contreras</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Banaji</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Mitchell</surname>, <given-names>J. P.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Multivoxel patterns in fusiform face area differentiate faces by sex and race</article-title>. <source>PLoS One</source>, <volume>8</volume>(<issue>7</issue>), <fpage>e69684</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0069684</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Shulman</surname>, <given-names>G. L.</given-names></string-name></person-group> (<year>2008</year>). <article-title>The Reorienting System of the Human Brain: From Environment to Theory of Mind</article-title>. <source>Neuron</source>, <volume>58</volume>(<issue>3</issue>), <fpage>306</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2008.04.017</pub-id></mixed-citation></ref>
    <ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Cortical surface-based analysis. I. Segmentation and surface reconstruction</article-title>. <source>Neuroimage</source>, <volume>9</volume>(<issue>2</issue>), <fpage>179</fpage>–<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name>, <string-name><surname>Heilbron</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2018</year>). <article-title>How Do Expectations Shape Perception?</article-title> <source>Trends in Cognitive Sciences</source>, <volume>22</volume>(<issue>9</issue>), <fpage>764</fpage>–<lpage>779</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Duncan</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Neural Mechanisms of Selective Visual Attention</article-title>. <source>Annual Review of Neuroscience</source>, <volume>18</volume>(Volume 18, 1995), <fpage>193</fpage>–<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Duncan</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Neural mechanisms of selective visual attention</article-title>. <source>Annu Rev Neurosci</source>, <volume>18</volume>, <fpage>193</fpage>–<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dombert</surname>, <given-names>P. L.</given-names></string-name>, <string-name><surname>Kuhns</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mengotti</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Fink</surname>, <given-names>G. R.</given-names></string-name>, &amp; <string-name><surname>Vossel</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Functional mechanisms of probabilistic inference in feature- and space-based attentional systems</article-title>. <source>Neuroimage</source>, <volume>142</volume>, <fpage>553</fpage>–<lpage>564</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.010</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Egner</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Principles of cognitive control over task focus and task switching</article-title>. <source>Nature Reviews Psychology</source>, <volume>2</volume>(<issue>11</issue>), <fpage>702</fpage>–<lpage>714</lpage>. <pub-id pub-id-type="doi">10.1038/s44159-023-00234-4</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Emrich</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Riggall</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>LaRocque</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Postle</surname>, <given-names>B. R.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Distributed Patterns of Activity in Sensory Cortex Reflect the Precision of Multiple Items Maintained in Visual Short-Term Memory</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>15</issue>), <fpage>6516</fpage>–<lpage>6523</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.5732-12.2013</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ester</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Sprague</surname>, <given-names>T. C.</given-names></string-name>, &amp; <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Parietal and Frontal Cortex Encode Stimulus-Specific Mnemonic Representations during Visual Working Memory</article-title>. <source>Neuron</source>, <volume>87</volume>(<issue>4</issue>), <fpage>893</fpage>–<lpage>905</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.013</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ester</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Sutterer</surname>, <given-names>D. W.</given-names></string-name>, <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name>, &amp; <string-name><surname>Awh</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Feature-Selective Attentional Modulations in Human Frontoparietal Cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>36</volume>(<issue>31</issue>), <fpage>8188</fpage>–<lpage>8199</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.3935-15.2016</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esterman</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Yantis</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Perceptual Expectation Evokes Category-Selective Cortical Activity</article-title>. <source>Cerebral Cortex</source>, <volume>20</volume>(<issue>5</issue>), <fpage>1245</fpage>–<lpage>1253</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhp188</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Favila</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Samide</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sweigart</surname>, <given-names>S. C.</given-names></string-name>, &amp; <string-name><surname>Kuhl</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Parietal Representations of Stimulus Features Are Amplified during Memory Retrieval and Flexibly Aligned with Top-Down Goals</article-title>. <source>J Neurosci</source>, <volume>38</volume>(<issue>36</issue>), <fpage>7809</fpage>–<lpage>7821</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0564-18.2018</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Huber</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Jangraw</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Molfese</surname>, <given-names>P. J.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Layer-dependent activity in human prefrontal cortex during working memory</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>10</issue>), <fpage>1687</fpage>–<lpage>1695</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-019-0487-z</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name>, &amp; <string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Cortical surface-based analysis. II: Inflation, flattening, and a surface-based coordinate system</article-title>. <source>Neuroimage</source>, <volume>9</volume>(<issue>2</issue>), <fpage>195</fpage>–<lpage>207</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gayet</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Preparatory attention incorporates contextual expectations</article-title>. <source>Current Biology</source>, <volume>32</volume>(<issue>3</issue>), <fpage>687</fpage>-+. <pub-id pub-id-type="doi">10.1016/j.cub.2021.11.062</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Hacker</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Harwell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> (<year>2016</year>). <article-title>A multi-modal parcellation of human cerebral cortex</article-title>. <source>Nature</source>, <volume>536</volume>(<issue>7615</issue>), <fpage>171</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1038/nature18933</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gong</surname>, <given-names>M. Y.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y. L.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T. S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Preparatory attention to visual features primarily relies on non-sensory representation</article-title>. <source>Scientific Reports</source>, <volume>12</volume>(<issue>1</issue>), <fpage>21726</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-022-26104-2</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guerin</surname>, <given-names>Scott A.</given-names></string-name>, <string-name><surname>Robbins</surname>, <given-names>Clifford A.</given-names></string-name>, <string-name><surname>Gilmore</surname>, <given-names>Adrian W.</given-names></string-name>, &amp; <string-name><surname>Schacter</surname>, <given-names>Daniel L.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Interactions between Visual Attention and Episodic Retrieval: Dissociable Contributions of Parietal Regions during Gist-Based False Recognition</article-title>. <source>Neuron</source>, <volume>75</volume>(<issue>6</issue>), <fpage>1122</fpage>–<lpage>1134</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.020</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guntupalli</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Wheeler</surname>, <given-names>K. G.</given-names></string-name>, &amp; <string-name><surname>Gobbini</surname>, <given-names>M. I.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Disentangling the Representation of Identity from Head View Along the Human Face Processing Pathway</article-title>. <source>Cereb Cortex</source>, <volume>27</volume>(<issue>1</issue>), <fpage>46</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhw344</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hall</surname>, <given-names>E. H.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Object-based attention during scene perception elicits boundary contraction in memory</article-title>. <source>Memory &amp; Cognition</source>. <pub-id pub-id-type="doi">10.3758/s13421-024-01540-9</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hansen</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Hillenbrand</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Ungerleider</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Effects of Prior Knowledge on Decisions Made Under Perceptual vs. Categorical Uncertainty [Original Research]</article-title>. <source>Frontiers in Neuroscience</source>, <volume>6</volume>. <pub-id pub-id-type="doi">10.3389/fnins.2012.00163</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Helbing</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Draschkow</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Vo</surname>, <given-names>M. L. H.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Auxiliary Scene-Context Information Provided by Anchor Objects Guides Attention and Locomotion in Natural Search Behavior</article-title>. <source>Psychological Science</source>, <volume>33</volume>(<issue>9</issue>), <fpage>1463</fpage>–<lpage>1476</lpage>. <pub-id pub-id-type="doi">10.1177/09567976221091838</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hout</surname>, <given-names>M. C.</given-names></string-name>, &amp; <string-name><surname>Goldinger</surname>, <given-names>S. D.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Target templates: the precision of mental representations affects attentional guidance and decision-making in visual search. <italic>Attention, Perception</italic></article-title>, <source>&amp; Psychophysics</source>, <volume>77</volume>(<issue>1</issue>), <fpage>128</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-014-0764-6</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jeong</surname>, <given-names>S. K.</given-names></string-name>, &amp; <string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Behaviorally Relevant Abstract Object Identity Representation in the Human Parietal Cortex</article-title>. <source>J Neurosci</source>, <volume>36</volume>(<issue>5</issue>), <fpage>1607</fpage>–<lpage>1619</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1016-15.2016</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Josephs</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Drew</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Wolfe</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Shuffling your way out of change blindness</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>23</volume>(<issue>1</issue>), <fpage>193</fpage>–<lpage>200</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-015-0886-4</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jung</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Larsen</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Walther</surname>, <given-names>D. B.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Modality-Independent Coding of Scene Categories in Prefrontal Cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>38</volume>(<issue>26</issue>), <fpage>5969</fpage>–<lpage>5981</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0272-18.2018</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jung</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Walther</surname>, <given-names>D. B.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Neural Representations in the Prefrontal Cortex Are Task Dependent for Scene Attributes But Not for Scene Categories</article-title>. <source>J Neurosci</source>, <volume>41</volume>(<issue>34</issue>), <fpage>7234</fpage>–<lpage>7245</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2816-20.2021</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pinsk</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>De Weerd</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Increased Activity in Human Visual Cortex during Directed Attention in the Absence of Visual Stimulation</article-title>. <source>Neuron</source>, <volume>22</volume>(<issue>4</issue>), <fpage>751</fpage>–<lpage>761</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(00)80734-5</pub-id></mixed-citation></ref>
    <ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kolling</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wittmann</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Rushworth</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Multiple signals in anterior cingulate cortex</article-title>. <source>Curr Opin Neurobiol</source>(<volume>37</volume>), <fpage>36</fpage>-<lpage>43</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2015.12.007</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Information-based functional brain mapping</article-title>. <source>Proc Natl Acad Sci U S A</source>, <volume>103</volume>(<issue>10</issue>), <fpage>3863</fpage>–<lpage>3868</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kurtin</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Araña-Oiarbide</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lorenz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Violante</surname>, <given-names>I. R.</given-names></string-name>, &amp; <string-name><surname>Hampshire</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Planning ahead: Predictable switching recruits task-active and resting-state networks</article-title>. <source>Human Brain Mapping</source>, <volume>44</volume>(<issue>15</issue>), <fpage>5030</fpage>–<lpage>5046</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.26430</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwak</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Curtis</surname>, <given-names>C. E.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Unveiling the abstract format of mnemonic representations</article-title>.<source>Neuron</source>, <volume>110</volume>(<issue>11</issue>), <fpage>1822</fpage>–<lpage>1828</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.016</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Idiosyncratic Patterns of Representational Similarity in Prefrontal Cortex Predict Attentional Performance</article-title>. <source>J Neurosci</source>, <volume>37</volume>(<issue>5</issue>), <fpage>1257</fpage>–<lpage>1268</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1407-16.2016</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>S. H.</given-names></string-name>, <string-name><surname>Kravitz</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Goal-dependent dissociation of visual and prefrontal cortices during working memory</article-title>. <source>Nature Neuroscience</source>, <volume>16</volume>(<issue>8</issue>), <fpage>997</fpage>–<lpage>U935</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3452</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lerebourg</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name>, &amp; <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Preparatory activity during visual search reflects attention-guiding objects rather than search targets</article-title>. <source>bioRxiv</source></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levy</surname>, <given-names>B. J.</given-names></string-name>, &amp; <string-name><surname>Wagner</surname>, <given-names>A. D.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Cognitive control and right ventrolateral prefrontal cortex: reflexive reorienting, motor inhibition, and action updating</article-title>. <source>Ann N Y Acad Sci</source>, <volume>1224</volume>(<issue>1</issue>), <fpage>40</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1111/j.1749-6632.2011.05958.x</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Larsson</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Carrasco</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Feature-Based Attention Modulates Orientation-Selective Responses in Human Visual Cortex</article-title>. <source>Neuron</source>, <volume>55</volume>(<issue>2</issue>), <fpage>313</fpage>–<lpage>323</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.030</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Slotnick</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name>, &amp; <string-name><surname>Yantis</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Cortical mechanisms of feature-based attentional control</article-title>. <source>Cereb Cortex</source>, <volume>13</volume>(<issue>12</issue>), <fpage>1334</fpage>–<lpage>1343</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhg080</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Long</surname>, <given-names>N. M.</given-names></string-name>, &amp; <string-name><surname>Kuhl</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Bottom-Up and Top-Down Factors Differentially Influence Stimulus Representations Across Large-Scale Attentional Networks</article-title>. <source>J Neurosci</source>, <volume>38</volume>(<issue>10</issue>), <fpage>2495</fpage>–<lpage>2504</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2724-17.2018</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luck</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Chelazzi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Neural Mechanisms of Spatial Selective Attention in Areas V1, V2, and V4 of Macaque Visual Cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>77</volume>(<issue>1</issue>), <fpage>24</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1152/jn.1997.77.1.24</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Correll</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Wittenbrink</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2015</year>). <article-title>The Chicago face database: A free stimulus set of faces and norming data</article-title>. <source>Behavior Research Methods</source>, <volume>47</volume>(<issue>4</issue>), <fpage>1122</fpage>–<lpage>1135</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-014-0532-5</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mack</surname>, <given-names>S. C.</given-names></string-name>, &amp; <string-name><surname>Eckstein</surname>, <given-names>M. P.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Object co-occurrence serves as a contextual cue to guide and facilitate visual search in a natural viewing environment</article-title>. <source>Journal of Vision</source>, <volume>11</volume>(<fpage>9</fpage>). <pub-id pub-id-type="doi">10.1167/11.9.9</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malcolm</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Henderson</surname>, <given-names>J. M.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Combining top-down processes to guide eye movements during real-world scene search</article-title>. <source>J Vis</source>, <volume>10</volume>(<issue>2</issue>), <fpage>4</fpage> 1-11. <pub-id pub-id-type="doi">10.1167/10.2.4</pub-id></mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malcolm</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Shomstein</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Object-based attention in real-world scenes</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>144</volume>(<issue>2</issue>), <fpage>257</fpage>–<lpage>263</lpage>. <pub-id pub-id-type="doi">10.1037/xge0000060</pub-id></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Armstrong</surname>, <given-names>K. M.</given-names></string-name>, &amp; <string-name><surname>Fallah</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Visuomotor Origins of Covert Spatial Attention</article-title>. <source>Neuron</source>, <volume>40</volume>(<issue>4</issue>), <fpage>671</fpage>–<lpage>683</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(03)00716-5</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muhle-Karbe</surname>, <given-names>P. S.</given-names></string-name>, <string-name><surname>Derrfuss</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lynn</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Neubert</surname>, <given-names>F. X.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>P. T.</given-names></string-name>, <string-name><surname>Brass</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Co-Activation-Based Parcellation of the Lateral Prefrontal Cortex Delineates the Inferior Frontal Junction Area</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>5</issue>), <fpage>2225</fpage>–<lpage>2241</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhv073</pub-id></mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muhle-Karbe</surname>, <given-names>P. S.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Egner</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Causal Evidence for Learning-Dependent Frontal Lobe Contributions to Cognitive Control</article-title>. <source>J Neurosci</source>, <volume>38</volume>(<issue>4</issue>), <fpage>962</fpage>–<lpage>973</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1467-17.2017</pub-id></mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nah</surname>, <given-names>J. C.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Thematic object pairs produce stronger and faster grouping than taxonomic pairs</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>48</volume>(<issue>12</issue>), <fpage>1325</fpage>–<lpage>1335</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0001031</pub-id></mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nee</surname>, <given-names>D. E.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Integrative frontal-parietal dynamics supporting cognitive control</article-title>. <source>Elife</source>, <volume>10</volume>, <elocation-id>e57244</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.57244</pub-id></mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nee</surname>, <given-names>D. E.</given-names></string-name>, &amp; <string-name><surname>D’Esposito</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The hierarchical organization of the lateral prefrontal cortex</article-title>. <source>Elife</source>, <volume>5</volume>. <pub-id pub-id-type="doi">10.7554/eLife.12112</pub-id></mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nee</surname>, <given-names>D. E.</given-names></string-name>, &amp; <string-name><surname>D’Esposito</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Causal evidence for lateral prefrontal cortex dynamics supporting cognitive control</article-title>. <source>Elife</source>, <volume>6</volume>, <elocation-id>e28040</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.28040</pub-id></mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nestor</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Plaut</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name><surname>Behrmann</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Unraveling the distributed neural code of facial identity through spatiotemporal pattern analysis</article-title>. <source>Proc Natl Acad Sci U S A</source>, <volume>108</volume>(<issue>24</issue>), <fpage>9998</fpage>–<lpage>10003</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1102433108</pub-id></mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neubert</surname>, <given-names>F.-X.</given-names></string-name>, <string-name><surname>Mars</surname>, <given-names>Rogier B.</given-names></string-name>, <string-name><surname>Thomas</surname>, <given-names>Adam G.</given-names></string-name>, <string-name><surname>Sallet</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Rushworth</surname>, <given-names>Matthew F. S.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Comparison of Human Ventral Frontal Cortex Areas for Cognitive Control and Language with Areas in Monkey Frontal Cortex</article-title>. <source>Neuron</source>, <volume>81</volume>(<issue>3</issue>), <fpage>700</fpage>–<lpage>713</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.012</pub-id></mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Connor</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Fukui</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Pinsk</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Attention modulates responses in the human lateral geniculate nucleus</article-title>. <source>Nature Neuroscience</source>, <volume>5</volume>(<issue>11</issue>), <fpage>1203</fpage>–<lpage>1209</lpage>. <pub-id pub-id-type="doi">10.1038/nn957</pub-id></mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Craven</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Downing</surname>, <given-names>P. E.</given-names></string-name>, &amp; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name></person-group> (<year>1999</year>). <article-title>fMRI evidence for objects as the units of attentional selection</article-title>. <source>Nature</source>, <volume>401</volume>(<issue>6753</issue>), <fpage>584</fpage>–<lpage>587</lpage>. <pub-id pub-id-type="doi">10.1038/44134</pub-id></mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Reilly</surname>, <given-names>J. X.</given-names></string-name>, <string-name><surname>Schuffelgen</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Cuell</surname>, <given-names>S. F.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Mars</surname>, <given-names>R. B.</given-names></string-name>, &amp; <string-name><surname>Rushworth</surname>, <given-names>M. F.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Dissociable effects of surprise and model update in parietal and anterior cingulate cortex</article-title>. <source>Proc Natl Acad Sci U S A</source>, <volume>110</volume>(<issue>38</issue>), <fpage>E3660</fpage>–<lpage>3669</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1305373110</pub-id></mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Reilly</surname>, <given-names>R. C.</given-names></string-name></person-group> (<year>2010</year>). <article-title>The What and How of prefrontal cortical organization</article-title>. <source>Trends in Neurosciences</source>, <volume>33</volume>(<issue>8</issue>), <fpage>355</fpage>–<lpage>361</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2010.05.002</pub-id></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olmos-Solis</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>van Loon</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Olivers</surname>, <given-names>C. N. L.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Content or status: Frontal and posterior cortical representations of object category and upcoming task goals in working memory</article-title>. <source>Cortex</source>, <volume>135</volume>, <fpage>61</fpage>–<lpage>77</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2020.11.011</pub-id></mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panichello</surname>, <given-names>M. F.</given-names></string-name>, &amp; <string-name><surname>Buschman</surname>, <given-names>T. J.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Shared mechanisms underlie the control of working memory and attention</article-title>. <source>Nature</source>, <volume>592</volume>(<issue>7855</issue>), <fpage>601</fpage>–<lpage>605</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-021-03390-w</pub-id></mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Berlot</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Predictive processing of scenes and objects</article-title> <source>Nature Reviews Psychology</source>, <volume>3</volume>(<issue>1</issue>), <fpage>13</fpage>–<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1038/s44159-023-00254-0</pub-id></mixed-citation></ref>
<ref id="c93a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, &amp; <string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2011</year>). <article-title>A neural basis for real-world visual search in human occipitotemporal cortex</article-title>, <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>108</volume>(<issue>29</issue>), <fpage>12125</fpage>–<lpage>12130</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1101042108</pub-id></mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poskanzer</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Aly</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Switching between External and Internal Attention in Hippocampal Networks</article-title>. <source>The Journal of Neuroscience</source>, <volume>43</volume>(<issue>38</issue>), <fpage>6538</fpage>–<lpage>6552</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.0029-23.2023</pub-id></mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qin</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>van Marle</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Hermans</surname>, <given-names>E. J.</given-names></string-name>, &amp; <string-name><surname>Fernandez</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Subjective sense of memory strength and the objective amount of information accurately remembered are related to distinct neural correlates at encoding</article-title>. <source>J Neurosci</source>, <volume>31</volume>(<issue>24</issue>), <fpage>8920</fpage>–<lpage>8927</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2587-10.2011</pub-id></mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reynolds</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>The Normalization Model of Attention</article-title>. <source>Neuron</source>, <volume>61</volume>(<issue>2</issue>), <fpage>168</fpage>–<lpage>185</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id></mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rossi</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Pessoa</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name></person-group> (<year>2009</year>). <article-title>The prefrontal cortex and the executive control of attention</article-title>. <source>Experimental Brain Research</source>, <volume>192</volume>(<issue>3</issue>), <fpage>489</fpage>–<lpage>497</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-008-1642-z</pub-id></mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruff</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Blankenburg</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bjoertomt</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bestmann</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Freeman</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Haynes</surname>, <given-names>J.-D.</given-names></string-name>, <string-name><surname>Rees</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Josephs</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Deichmann</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Driver</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Concurrent TMS-fMRI and Psychophysics Reveal Frontal Influences on Human Retinotopic Visual Cortex</article-title>. <source>Current Biology</source>, <volume>16</volume>(<issue>15</issue>), <fpage>1479</fpage>–<lpage>1488</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2006.06.057</pub-id></mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schaefer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kong</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gordon</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T. O.</given-names></string-name>, <string-name><surname>Zuo</surname>, <given-names>X.-N.</given-names></string-name>, <string-name><surname>Holmes</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name>, &amp; <string-name><surname>Yeo</surname>, <given-names>B. T. T.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Local-Global Parcellation of the Human Cerebral Cortex from Intrinsic Functional Connectivity MRI</article-title>. <source>Cerebral Cortex</source>, <volume>28</volume>(<issue>9</issue>), <fpage>3095</fpage>–<lpage>3114</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhx179</pub-id></mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name>, &amp; <string-name><surname>Boynton</surname>, <given-names>G. M.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Feature-Based Attentional Modulations in the Absence of Direct Visual Stimulation</article-title>. <source>Neuron</source>, <volume>55</volume>(<issue>2</issue>), <fpage>301</fpage>–<lpage>312</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.015</pub-id></mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Threshold-free cluster enhancement: Addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title>. <source>Neuroimage</source>, <volume>44</volume>(<issue>1</issue>), <fpage>83</fpage>–<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id></mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Soon</surname>, <given-names>C. S.</given-names></string-name>, <string-name><surname>Namburi</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Chee</surname>, <given-names>M. W. L.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Preparatory patterns of neural activity predict visual category search speed</article-title>. <source>Neuroimage</source>, <volume>66</volume>, <fpage>215</fpage>–<lpage>222</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.036</pub-id></mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Soyuhos</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Baldauf</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Functional connectivity fingerprints of the frontal eye field and inferior frontal junction suggest spatial versus nonspatial processing in the prefrontal cortex</article-title>. <source>European Journal of Neuroscience</source>, <volume>57</volume>(<issue>7</issue>), <fpage>1114</fpage>–<lpage>1140</lpage>. <pub-id pub-id-type="doi">10.1111/ejn.15936</pub-id></mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spisák</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Spisák</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Zunhammer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bingel</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Kincses</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Probabilistic TFCE: A generalized combination of cluster size and voxel intensity to increase statistical power</article-title>. <source>Neuroimage</source>, <volume>185</volume>, <fpage>12</fpage>–<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.078</pub-id></mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stigliani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name>, &amp; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Temporal Processing Capacity in High-Level Visual Cortex Is Domain Specific</article-title>. <source>J Neurosci</source>, <volume>35</volume>(<issue>36</issue>), <fpage>12412</fpage>–<lpage>12424</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4822-14.2015</pub-id></mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name>, &amp; <string-name><surname>Duncan</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Shape-specific preparatory activity mediates attention to targets in human visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>106</volume>(<issue>46</issue>), <fpage>19569</fpage>–<lpage>19574</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0905306106</pub-id></mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>Mark G.</given-names></string-name>, <string-name><surname>Kusunoki</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sigala</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Nili</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Gaffan</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Duncan</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Dynamic Coding for Cognitive Control in Prefrontal Cortex</article-title>. <source>Neuron</source>, <volume>78</volume>(<issue>2</issue>), <fpage>364</fpage>–<lpage>375</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id></mixed-citation></ref>
<ref id="c109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>Timothy E.</given-names></string-name>, &amp; <string-name><surname>Koechlin</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Perceptual Classification in a Rapidly Changing Environment</article-title>. <source>Neuron</source>, <volume>71</volume>(<issue>4</issue>), <fpage>725</fpage>–<lpage>736</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.022</pub-id></mixed-citation></ref>
<ref id="c110"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sylvester</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Shulman</surname>, <given-names>G. L.</given-names></string-name>, <string-name><surname>Jack</surname>, <given-names>A. I.</given-names></string-name>, &amp; <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Asymmetry of Anticipatory Activity in Visual Cortex Predicts the Locus of Attention and Perception</article-title>. <source>The Journal of Neuroscience</source>, <volume>27</volume>(<issue>52</issue>), <fpage>14424</fpage>–<lpage>14433</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.3759-07.2007</pub-id></mixed-citation></ref>
<ref id="c111"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tamber-Rosenau</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Asplund</surname>, <given-names>C. L.</given-names></string-name>, &amp; <string-name><surname>Marois</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Functional dissociation of the inferior frontal junction from the dorsal attention network in top-down attentional control</article-title>. <source>Journal of Neurophysiology</source>, <volume>120</volume>(<issue>5</issue>), <fpage>2498</fpage>–<lpage>2512</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00506.2018</pub-id></mixed-citation></ref>
<ref id="c112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thompson</surname>, <given-names>K. G.</given-names></string-name>, <string-name><surname>Biscoe</surname>, <given-names>K. L.</given-names></string-name>, &amp; <string-name><surname>Sato</surname>, <given-names>T. R.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Neuronal Basis of Covert Spatial Attention in the Frontal Eye Field</article-title>. <source>The Journal of Neuroscience</source>, <volume>25</volume>(<issue>41</issue>), <fpage>9479</fpage>–<lpage>9487</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.0741-05.2005</pub-id></mixed-citation></ref>
<ref id="c113"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treue</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Trujillo</surname>, <given-names>J. C. M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title>. <source>Nature</source>, <volume>399</volume>(<issue>6736</issue>), <fpage>575</fpage>–<lpage>579</lpage>. <pub-id pub-id-type="doi">10.1038/21176</pub-id></mixed-citation></ref>
<ref id="c114"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tunnermann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chelazzi</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Schubo</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>How Feature Context Alters Attentional Template Switching</article-title>. <source>Journal of Experimental Psychology-Human Perception and Performance</source>, <volume>47</volume>(<issue>11</issue>), <fpage>1431</fpage>–<lpage>1444</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0000951</pub-id></mixed-citation></ref>
<ref id="c115"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turini</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Võ</surname>, <given-names>M. L.-H.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Hierarchical organization of objects in scenes is reflected in mental representations of objects</article-title>. <source>Scientific Reports</source>, <volume>12</volume>(<issue>1</issue>), <fpage>20068</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-022-24505-x</pub-id></mixed-citation></ref>
<ref id="c116"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Loon</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Olmos-Solis</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Fahrenfort</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Olivers</surname>, <given-names>C. N. L.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Current and future goals are represented in opposite patterns in object-selective cortex</article-title>. <source>Elife</source>, <volume>7</volume>. <pub-id pub-id-type="doi">10.7554/eLife.38677</pub-id></mixed-citation></ref>
<ref id="c117"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vo</surname>, <given-names>M. L. H.</given-names></string-name>, <string-name><surname>Boettcher</surname>, <given-names>S. E. P.</given-names></string-name>, &amp; <string-name><surname>Draschkow</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Reading scenes: how scene grammar guides attention and aids perception in real-world environments</article-title>. <source>Current Opinion in Psychology</source>, <volume>29</volume>, <fpage>205</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1016/j.copsyc.2019.03.009</pub-id></mixed-citation></ref>
<ref id="c118"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vossel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Fink</surname>, <given-names>G. R.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Dorsal and Ventral Attention Systems:Distinct Neural Circuits but Collaborative Roles</article-title>. <source>The Neuroscientist</source>, <volume>20</volume>(<issue>2</issue>), <fpage>150</fpage>–<lpage>159</lpage>. <pub-id pub-id-type="doi">10.1177/1073858413494269</pub-id></mixed-citation></ref>
<ref id="c119"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walther</surname>, <given-names>D. B.</given-names></string-name>, <string-name><surname>Caddigan</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Fei-Fei</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Beck</surname>, <given-names>D. M.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Natural Scene Categories Revealed in Distributed Patterns of Activity in the Human Brain</article-title>. <source>Journal of Neuroscience</source>, <volume>29</volume>(<issue>34</issue>), <fpage>10573</fpage>–<lpage>10581</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.0559-09.2009</pub-id></mixed-citation></ref>
<ref id="c120"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willenbockel</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Sadr</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fiset</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Horne</surname>, <given-names>G. O.</given-names></string-name>, <string-name><surname>Gosselin</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Tanaka</surname>, <given-names>J. W.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Controlling low-level image properties: the SHINE toolbox</article-title>. <source>Behavior Research Methods</source>, <volume>42</volume>(<issue>3</issue>), <fpage>671</fpage>–<lpage>684</lpage>. <pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id></mixed-citation></ref>
<ref id="c121"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Witkowski</surname>, <given-names>P. P.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Prefrontal Cortex Codes Representations of Target Identity and Feature Uncertainty</article-title>. <source>The Journal of Neuroscience</source>, <volume>43</volume>(<issue>50</issue>), <fpage>8769</fpage>–<lpage>8776</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.1117-23.2023</pub-id></mixed-citation></ref>
<ref id="c122"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bichot</surname>, <given-names>N. P.</given-names></string-name>, <string-name><surname>Takahashi</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The cortical connectome of primate lateral prefrontal cortex</article-title>. <source>Neuron</source>, <volume>110</volume>(<issue>2</issue>), <fpage>312</fpage>–<lpage>327.e317.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2021.10.018</pub-id></mixed-citation></ref>
<ref id="c123"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>Q.</given-names></string-name>, &amp; <string-name><surname>Postle</surname>, <given-names>B. R.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The Neural Codes Underlying Internally Generated Representations in Visual Working Memory</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>33</volume>(<issue>6</issue>), <fpage>1142</fpage>–<lpage>1157</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01702</pub-id></mixed-citation></ref>
<ref id="c124"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Hanks</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Attentional Guidance and Match Decisions Rely on Different Template Information During Visual Search</article-title>. <source>Psychological Science</source>, <volume>33</volume>(<issue>1</issue>), <fpage>105</fpage>–<lpage>120</lpage>. <pub-id pub-id-type="doi">10.1177/09567976211032225</pub-id></mixed-citation></ref>
<ref id="c125"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Becker</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Boettcher</surname>, <given-names>S. E. P.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Good-enough attentional guidance</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>27</volume>(<issue>4</issue>), <fpage>391</fpage>–<lpage>403</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2023.01.007</pub-id></mixed-citation></ref>
<ref id="c126"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zanto</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Rubens</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Bollinger</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Gazzaley</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Top-down modulation of visual feature processing: The role of the inferior frontal junction</article-title>. <source>Neuroimage</source>, <volume>53</volume>(<issue>2</issue>), <fpage>736</fpage>–<lpage>745</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.012</pub-id></mixed-citation></ref>
<ref id="c127"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Learned associations serve as target proxies during difficult but not easy visual search</article-title>. <source>Cognition</source>, <volume>242</volume>, <fpage>105648</fpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2023.105648</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104041.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>South China Normal University</institution>
</institution-wrap>
<city>Guangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study decoded target-associated information in prefrontal and sensory cortex during the preparatory period of a visual search task, suggesting a memory-driven attentional template. The evidence supporting this claim is <bold>convincing</bold>, based on multivariate pattern analyses of fMRI data. The results will be of interest to psychologists and cognitive neuroscientists.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104041.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>When you search for something, you need to maintain some representation (a &quot;template&quot;) of that target in your mind/brain. Otherwise, how would you know what you were looking for? If your phone is in a shocking pink case, you can guide your attention to pink things based on a target template that includes the attribute 'pink'. That guidance should get you to the phone pretty effectively if it is in view. Most real-world searches are more complicated. If you are looking for the toaster, you will make use of your knowledge of where toasters can be. Thus, if you are asked to find a toaster, you might first activate a template of a kitchen or a kitchen counter. You might worry about pulling up the toaster template only after you are reasonably sure you have restricted your attention to a sensible part of the scene.</p>
<p>Zhou and Geng are looking for evidence of this early stage of guidance by information about the surrounding scene in a search task. They train Os to associate four faces with four places. Then, with Os in the scanner, they show one face - the target for a subsequent search. After an 8 sec delay, they show a search display where the face is placed on the associated scene 75% of the time. Thus, attending to the associated scene is a good idea. The questions of interest are &quot;When can the experimenters decode which face Os saw from fMRI recording?&quot; &quot;When can the experimenters decode the associated scene?&quot; and &quot;Where in the brain can the experimenters see evidence of this decoding? The answer is that the face but not the scene can be read out during the face's initial presentation. The key finding is that the scene can be read out (imperfectly but above chance) during the subsequent delay when Os are looking at just a fixation point. Apparently, seeing the face conjures up the scene in the mind's eye.</p>
<p>This is a solid and believable result. The only issue, for me, is whether it is telling us anything specifically about search. Suppose you trained Os on the face-scene pairing but never did anything connected to the search. If you presented the face, would you not see evidence of recall of the associated scene? Maybe you would see the activation of the scene in different areas and you could identify some areas as search specific. I don't think anything like that was discussed here.</p>
<p>You might also expect this result to be asymmetric. The idea is that the big scene gives the search information about the little face. The face should activate the larger useful scene more than the scene should activate the more incidental face, if the task was reversed. That might be true if the finding is related to a search where the scene context is presumed to be the useful attention guiding stimulus. You might not expect an asymmetry if Os were just learning an association.</p>
<p>It is clear in this study that the face and the scene have been associated and that this can be seen in the fMRI data. It is also clear that a valid scene background speeds the behavioral response in the search task. The linkage between these two results is not entirely clear but perhaps future research will shed more light.</p>
<p>It is also possible that I missed the clear evidence of the search-specific nature of the activation by the scene during the delay period. If so, I apologize and suggest that the point be underlined for readers like me.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104041.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work is one of the best instances of a well-controlled experiment and theoretically impactful findings within the literature on templates guiding attentional selection. I am a fan of the work that comes out of this lab and this particular manuscript is an excellent example as to why that is the case. Here, the authors use fMRI (employing MVPA) to test whether during the preparatory search period, a search template is invoked within the corresponding sensory regions, in the absence of physical stimulation. By associating faces with scenes, a strong association was created between two types of stimuli that recruit very specific neural processing regions - FFA for faces and PPA for scenes. The critical results showed that scene information that was associated with a particular cue could be decoded from PPA during the delay period. This result strongly supports the invoking of a very specific attentional template.</p>
<p>Strengths:</p>
<p>There is so much to be impressed with in this report. The writing of the manuscript is incredibly clear. The experimental design is clever and innovative. The analysis is sophisticated and also innovative. The results are solid and convincing.</p>
<p>Weaknesses:</p>
<p>I only have a few weaknesses to point out.</p>
<p>
This point is not so much of a weakness, but a further test of the hypothesis put forward by the authors. The delay period was long - 8 seconds. It would be interesting to split the delay period into the first 4seconds and the last 4seconds and run the same decoding analyses. The hypothesis here is that semantic associations take time to evolve, and it would be great to show that decoding gets stronger in the second delay period as opposed to the period right after the cue. I don't think this is necessary for publication, but I think it would be a stronger test of the template hypothesis.</p>
<p>
Type in the abstract &quot;curing&quot; vs &quot;during.&quot;</p>
<p>
It is hard to know what to do with significant results in ROIs that are not motivated by specific hypotheses. However, for Figure 3, what are the explanations for ROIs that show significant differences above and beyond the direct hypotheses set out by the authors?</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104041.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript contains a carefully designed fMRI study, using MVPA pattern analysis to investigate which high-level associate cortices contain target-related information to guide visual search. A special focus is hereby on so-called 'target-associated' information, that has previously been shown to help in guiding attention during visual search. For this purpose the author trained their participants and made them learn specific target-associations, in order to then test which brain regions may contain neural representations of those learnt associations. They found that at least some of the associations tested were encoded in prefrontal cortex during the cue and delay period.</p>
<p>The manuscript is very carefully prepared. As far as I can see, the statistical analyses are all sound and the results integrate well with previous findings.</p>
<p>I have no strong objections against the presented results and their interpretation.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.104041.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhou</surname>
<given-names>Zhiheng</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0687-2398</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Geng</surname>
<given-names>Joy J</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5663-9637</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>When you search for something, you need to maintain some representation (a &quot;template&quot;) of that target in your mind/brain. Otherwise, how would you know what you were looking for? If your phone is in a shocking pink case, you can guide your attention to pink things based on a target template that includes the attribute 'pink'. That guidance should get you to the phone pretty effectively if it is in view. Most real-world searches are more complicated. If you are looking for the toaster, you will make use of your knowledge of where toasters can be. Thus, if you are asked to find a toaster, you might first activate a template of a kitchen or a kitchen counter. You might worry about pulling up the toaster template only after you are reasonably sure you have restricted your attention to a sensible part of the scene.</p>
<p>Zhou and Geng are looking for evidence of this early stage of guidance by information about the surrounding scene in a search task. They train Os to associate four faces with four places. Then, with Os in the scanner, they show one face - the target for a subsequent search. After an 8 sec delay, they show a search display where the face is placed on the associated scene 75% of the time. Thus, attending to the associated scene is a good idea. The questions of interest are &quot;When can the experimenters decode which face Os saw from fMRI recording?&quot; &quot;When can the experimenters decode the associated scene?&quot; and &quot;Where in the brain can the experimenters see evidence of this decoding? The answer is that the face but not the scene can be read out during the face's initial presentation. The key finding is that the scene can be read out (imperfectly but above chance) during the subsequent delay when Os are looking at just a fixation point. Apparently, seeing the face conjures up the scene in the mind's eye.</p>
<p>This is a solid and believable result. The only issue, for me, is whether it is telling us anything specifically about search. Suppose you trained Os on the face-scene pairing but never did anything connected to the search. If you presented the face, would you not see evidence of recall of the associated scene? Maybe you would see the activation of the scene in different areas and you could identify some areas as search specific. I don't think anything like that was discussed here.</p>
<p>You might also expect this result to be asymmetric. The idea is that the big scene gives the search information about the little face. The face should activate the larger useful scene more than the scene should activate the more incidental face, if the task was reversed. That might be true if the finding is related to a search where the scene context is presumed to be the useful attention guiding stimulus. You might not expect an asymmetry if Os were just learning an association.</p>
<p>It is clear in this study that the face and the scene have been associated and that this can be seen in the fMRI data. It is also clear that a valid scene background speeds the behavioral response in the search task. The linkage between these two results is not entirely clear but perhaps future research will shed more light.</p>
<p>It is also possible that I missed the clear evidence of the search-specific nature of the activation by the scene during the delay period. If so, I apologize and suggest that the point be underlined for readers like me.</p>
</disp-quote>
<p>We will respond to this question by acknowledging that the reviewer is right in that the delay period activation of the scene is not necessarily search-specific. We will then discuss how this possibility affects the interpretation of our results and what kind of studies would need to be conducted in order to fully establish a causal link between delay period activity and visual search performance. We will also discuss the literature on cued attention and situate our work within the context of these other studies that have used similar task paradigms to infer attentional processes. Finally, we will discuss the interpretation of delay period activity in PPA and IFJ.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>This work is one of the best instances of a well-controlled experiment and theoretically impactful findings within the literature on templates guiding attentional selection. I am a fan of the work that comes out of this lab and this particular manuscript is an excellent example as to why that is the case. Here, the authors use fMRI (employing MVPA) to test whether during the preparatory search period, a search template is invoked within the corresponding sensory regions, in the absence of physical stimulation. By associating faces with scenes, a strong association was created between two types of stimuli that recruit very specific neural processing regions - FFA for faces and PPA for scenes. The critical results showed that scene information that was associated with a particular cue could be decoded from PPA during the delay period. This result strongly supports the invoking of a very specific attentional template.</p>
<p>Strengths:</p>
<p>There is so much to be impressed with in this report. The writing of the manuscript is incredibly clear. The experimental design is clever and innovative. The analysis is sophisticated and also innovative. The results are solid and convincing.</p>
<p>Weaknesses:</p>
<p>I only have a few weaknesses to point out.</p>
<p>This point is not so much of a weakness, but a further test of the hypothesis put forward by the authors. The delay period was long - 8 seconds. It would be interesting to split the delay period into the first 4seconds and the last 4seconds and run the same decoding analyses. The hypothesis here is that semantic associations take time to evolve, and it would be great to show that decoding gets stronger in the second delay period as opposed to the period right after the cue. I don't think this is necessary for publication, but I think it would be a stronger test of the template hypothesis.</p>
</disp-quote>
<p>We will conduct the suggested analysis. Depending on the outcome, we will include it in supplemental materials or the main text.</p>
<disp-quote content-type="editor-comment">
<p>Type in the abstract &quot;curing&quot; vs &quot;during.&quot;</p>
</disp-quote>
<p>We will fix this.</p>
<disp-quote content-type="editor-comment">
<p>It is hard to know what to do with significant results in ROIs that are not motivated by specific hypotheses. However, for Figure 3, what are the explanations for ROIs that show significant differences above and beyond the direct hypotheses set out by the authors?</p>
</disp-quote>
<p>We will address how each of the ROIs wdas selected based on the use of a priori networks as masks with ROIs as sub-parcels. We will explain why specific ROIs were associated with the strongest hypotheses but how the entire networks are relevant and related to existing literatures on attentional control and working memory. This content will be included in the introduction and discussion sections.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>The manuscript contains a carefully designed fMRI study, using MVPA pattern analysis to investigate which high-level associate cortices contain target-related information to guide visual search. A special focus is hereby on so-called 'target-associated' information, that has previously been shown to help in guiding attention during visual search. For this purpose the author trained their participants and made them learn specific target-associations, in order to then test which brain regions may contain neural representations of those learnt associations. They found that at least some of the associations tested were encoded in prefrontal cortex during the cue and delay period.</p>
<p>The manuscript is very carefully prepared. As far as I can see, the statistical analyses are all sound and the results integrate well with previous findings.</p>
<p>I have no strong objections against the presented results and their interpretation.</p>
</disp-quote>
<p>Thank you.</p>
</body>
</sub-article>
</article>