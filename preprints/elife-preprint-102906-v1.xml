<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">102906</article-id>
<article-id pub-id-type="doi">10.7554/eLife.102906</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.102906.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Physics of Living Systems</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Q-Learning to navigate turbulence without a map</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Rando</surname>
<given-names>Marco</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
<email xlink:href="mailto:marco.rando@edu.unige.it">marco.rando@edu.unige.it</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>James</surname>
<given-names>Martin</given-names>
</name>
<xref ref-type="aff" rid="A2">b</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Verri</surname>
<given-names>Alessandro</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rosasco</surname>
<given-names>Lorenzo</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Seminara</surname>
<given-names>Agnese</given-names>
</name>
<xref ref-type="aff" rid="A2">b</xref>
<email xlink:href="mailto:agnese.seminara@unige.it">agnese.seminara@unige.it</email>
</contrib>
<aff id="A1"><label>a</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0107c5v14</institution-id><institution>MalGa - DIBRIS, University of Genova</institution></institution-wrap>, <city>Genoa</city>, <country>Italy</country></aff>
<aff id="A2"><label>b</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0107c5v14</institution-id><institution>MalGa - DICCA, University of Genova</institution></institution-wrap>, <city>Genoa</city>, <country>Italy</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Walczak</surname>
<given-names>Aleksandra M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>CNRS</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<pub-date pub-type="epub">
<day>26</day>
<month>04</month>
<year>2024</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2024-12-16">
<day>16</day>
<month>12</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP102906</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-09-12">
<day>12</day>
<month>09</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-26">
<day>26</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2404.17495"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Rando et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Rando et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-102906-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>We consider the problem of olfactory searches in a turbulent environment. We focus on agents that respond solely to odor stimuli, with no access to spatial perception nor prior information about the odor location. We ask whether navigation strategies to a target can be learned robustly within a sequential decision making framework. We develop a reinforcement learning algorithm using a small set of interpretable olfactory states and train it with realistic turbulent odor cues. By introducing a temporal memory, we demonstrate that two salient features of odor traces, discretized in few olfactory states, are sufficient to learn navigation in a realistic odor plume. Performance is dictated by the sparse nature of turbulent plumes. An optimal memory exists which ignores blanks within the plume and activates a recovery strategy outside the plume. We obtain the best performance by letting agents learn their recovery strategy and show that it is mostly casting cross wind, similar to behavior observed in flying insects. The optimal strategy is robust to substantial changes in the odor plumes, suggesting minor parameter tuning may be sufficient to adapt to different environments.</p>
</abstract>
<kwd-group>
<title>Keywords</title>
<kwd>Navigation</kwd>
<kwd>Reinforcement learning</kwd>
<kwd>Olfaction</kwd>
<kwd>Turbulence</kwd>
<kwd>Memory</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<p>Bacterial cells localize the source of an attractive chemical even if they hold no spatial perception. They respond solely to temporal changes in chemical concentration and the result of their response is that they move toward attractive stimuli by climbing concentration gradients [<xref ref-type="bibr" rid="c1">1</xref>]. Larger organisms also routinely sense chemicals in their environment to localize or escape targets, but cannot follow chemical gradients since turbulence breaks odors into sparse pockets and gradients lose information [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>]. The question of which features of turbulent odor traces are used by animals for navigation is natural, but not well understood. Beyond olfaction, some animals could use also prior spatial information to navigate [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>], but if and how chemosensation and spatial perception are coupled is still not clear.</p>
<p>An algorithmic perspective to olfactory navigation in turbulence can shed light on some of these questions. Without aiming at an exhaustive taxonomy, see e.g. [<xref ref-type="bibr" rid="c11">11</xref>] for a recent review, we recall some approaches relevant to put our contribution in context. One class of methods are biomimetic algorithms, where explicit navigation rules are crafted taking inspiration from animal behavior. An advantage of these methods is interpretability, in the sense that they provide insights into features that effectively achieve turbulent navigation, for example: odor presence/absence [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c5">5</xref>]; odor slope at onset of detection [<xref ref-type="bibr" rid="c15">15</xref>]; number of detections in a given interval of time [<xref ref-type="bibr" rid="c16">16</xref>] and the time of odor onset [<xref ref-type="bibr" rid="c17">17</xref>]. On the flip side, in biomimetic algorithms behaviors are hardwired and typically reactive, not relying on any optimality criterion.</p>
<p>A way to tackle this shortcoming is to cast olfactory navigation within a sequential decision making framework [<xref ref-type="bibr" rid="c18">18</xref>]. In this context, navigation is formalized as a task with a reward for success; by maximizing reward, optimal strategies can be sought to efficiently reach the target. A byproduct is that most algorithmic choices can often be done in a principled way. Within this framework, some approaches make explicit use of spatial information. Bayesian algorithms use a spatial map to guess the target location and use odor to refine this guess or “belief”. A prominent algorithm for olfactory navigation based on the concept of belief is the information-seeking algorithm [<xref ref-type="bibr" rid="c3">3</xref>] akin to exploration heuristics widely used in robotics [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>] (see e.g. [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>]). Using Bayesian sequential decision making and the notion of beliefs, navigation can be formalized as a Partially Observable Markov Decision Process (POMDP) [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>], that can be approximatively solved [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>]. POMDP approaches are appealing since beliefs are a sufficient statistics for the entire history of odor detections. However, they are computationally cumbersome. Further, they leave the question open of whether navigation as sequential decision making can be performed using solely olfactory information.</p>
<p>Recently, two algorithms studied navigation as a response to olfactory input alone [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>]. In [<xref ref-type="bibr" rid="c29">29</xref>] artificial neural networks were shown to learn near optimal strategies, but they were trained on odor cues with limited sparsity, and training with sparse odor cues typical of turbulence remains to be tested. In [<xref ref-type="bibr" rid="c30">30</xref>] an approach based on finite state controllers was proposed. Here, optimization was done using a model-based technique, relying on prior knowledge of the likelihood to detect the odor in space, hence still using spatial information. A different model free optimization could also be considered avoiding spatial information but this latter approach also remains to be tested. More generally, all the above approaches manipulate internally the previous history (memory) of odor detections. In this sense they are less interpretable, since the features of odor traces that drive navigation do not emerge explicitly.</p>
<p>In this paper, we propose a reinforcement learning (RL) approach to navigation in turbulence based on a set of interpretable olfactory features, with no spatial information, and highlight the role played by memory within this context. More precisely, we learn optimal strategies from data by training tabular Q learning [<xref ref-type="bibr" rid="c18">18</xref>] with realistic odor cues obtained from state-of-the-art Direct Numerical Simulations of turbulence. From the odor cues, we define features as moving averages of odor intensity and sparsity: the moving window is the temporal memory and naturally connects to the physics of turbulent odors. States are then obtained discretizing such features. Due to sparsity, agents may detect no odor within the moving window. We show there is an optimal memory minimizing the occurrence of this “void state”. The optimal memory scales with the blank time dictated by turbulence as it emerges from a trade off requiring that: <italic>(i)</italic> short blanks-typical of turbulent plumes-are ignored by responding to detections further in the past, and <italic>(ii)</italic> long blanks promptly trigger a recovery strategy to make contact with the plume again. We leverage these observations to tune the memory adaptively, by setting it equal to the previous blank experienced along an agent’s path. With this choice, the algorithm tests successfully in distinct environments, suggesting that tuning can be made robustly to enable generalization. The agent learns to surge upwind in most non-void states and to recover by casting crosswind in the absence of detections. Optimal agents limit encounters with the void state to a narrow band right at the edge of the plume. This suggests that the temporal odor features we considered effectively predict when the agent is exiting the plume and point to an intimate connection between temporal predictions and spatial navigation.</p>
<sec id="s1">
<title>Significance</title>
<p>Finding mates or food in the presence of turbulence is challenging because odors constantly switch on and off unpredictably. As a result, it is unclear whether animals couple odor to other sources of information, what are the relevant features of odor stimuli and how they change according to the environment. A long history of bioinspired algorithms address this problem by crafting rules for navigation that mimic animal behavior: but can effective navigation be learned from the environment rather than set a priori? To address this question we train a reinforcement learning algorithm with realistic turbulent stimuli. Searchers learn to navigate by trial and error and respond solely to odor, with no further input. All computations are defined explicitly, enhancing interpretability. The upshot is that the algorithm identifies odor features as averages over a temporal scale (memory) dictated by the time between odor detections and thus by physics. There is no need to know physics beforehand, as memory can be adjusted based on experience. This approach naturally complements previous algorithms that use prior information and a map of space to plan navigation, rather than learn it from the environment. To what extent different animals plan vs learn to navigate remains to be understood.</p>
</sec>
<sec id="s2" sec-type="results">
<title>Results</title>
<sec id="s2-1">
<title>Background</title>
<p>Given a source of odor placed in an unknown position of a two-dimensional space, we consider the problem of learning to reach the source, <xref ref-type="fig" rid="fig1">Figure 1A</xref>. We formulate the problem as a discrete Markov Decision Process by discretizing space in tiles, also called “gridworld” in the reinforcement learning literature [<xref ref-type="bibr" rid="c18">18</xref>]. In this problem, an agent is in a given state s which is one of a discrete set of <italic>n</italic> tiles: <italic>s</italic> ∈ <italic>S</italic> := {s<sub>1</sub>, …, s<sub>n</sub>}. At each time step it chooses an action <italic>a</italic> which is a step in any of the coordinate directions <italic>a</italic> ∈ {up,down,left,right}. The goal is to find sequences of actions that lead to the source as fast as possible and is formalized with the notion of policy and reward which we will introduce later. If agents have perfect knowledge of their own location and of the location of the source in space, the problem reduces to finding the shortest path.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><p>Learning a stimulus-response strategy for turbulent navigation. (A) Representation of the search problem with turbulent odor cues obtained from Direct Numerical Simulations of fluid turbulence (grey scale, odor snapshot from the simulations). The discrete position <italic>s</italic> is hidden; the odor concentration <italic>z<sub>T</sub></italic> = <italic>z</italic>(<italic>s</italic>(<italic>t</italic>’), <italic>t</italic>’)|<italic>t</italic> — <italic>T</italic> ≤ <italic>t</italic>’ ≤ <italic>t</italic> is observed along the trajectory <italic>s</italic>(<italic>t</italic>’), where <italic>T</italic> is the sensing memory. (B) Odor traces from direct numerical simulations at different (fixed) points within the plume. Odor is noisy and sparse, information about the source is hidden in the temporal dynamics. (C) Contour maps of olfactory states with nearly infinite memory (<italic>T</italic> = 2598): on average olfactory states map to different locations within the plume and the void state is outside the plume. (D) Performance of stimulus-response strategies obtained during training, averaged over 500 episodes. We train using realistic turbulent data with memory <italic>T</italic> = 20 and backtracking recovery.</p></caption>
<graphic xlink:href="2404.17495v1_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s2-2">
<title>Using time <italic>vs</italic> space to address partial observability</title>
<p>In our problem however, the agent does not know where the source is, hence its position <italic>s</italic> relative to the source, is unknown or “partially observed”. Instead, it can sense odor released by the target. In the language of RL, odor is an “observation” - but does it hold information about the position <italic>s</italic>? The answer is yes: several properties of odor stimuli depend on the distance from the source. However in the presence of turbulence, information lies in the statistics of the odor stimulus. Indeed, when odor is carried by a turbulent flow, it develops into a dramatically stochastic plume, i.e. a complex and convoluted region of space where the fluid is rich in odor molecules. Turbulent plumes break into structures that distort and expand while they travel away from their source and become more and more diluted [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c6">6</xref>], see <xref ref-type="fig" rid="fig1">Figure 1A</xref>. As a consequence, an agent within the plume experiences intermittent odor traces that endlessly switch on (whiff) and off (blank) <xref ref-type="fig" rid="fig1">Figure 1B</xref>. The intensity of odor whiffs and how they are interleaved with blanks depends on distance from release, as dictated by physics [<xref ref-type="bibr" rid="c32">32</xref>]. Thus the upshot of turbulent transport is that the statistical properties of odor traces depend intricately on the position of the agent relative to the source. In other words, information about the state s is hidden within the observed odor traces.</p>
<p>This positional information can be leveraged with a Bayesian approach that relies on guessing <italic>s</italic>, i.e. defining the probability distribution of the position, also called belief. This is the approach that has been more commonly adopted in the literature until now [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>]. Note that because of the complexity of these algorithms, only relatively simple measures of the odor are computationally feasible, e.g. instantaneous presence/absence. Here we take a different model-free and map-free approach. Instead of guessing the current state <italic>s</italic>, we ignore the spatial position and respond directly to the temporal traces of the odor cues. Two other algorithms have been proposed to solve partial observability by responding solely to odor traces with recurrent neural networks [<xref ref-type="bibr" rid="c29">29</xref>] and finite state controllers [<xref ref-type="bibr" rid="c30">30</xref>] that manipulate implicitly the odor traces. Here instead we manipulate odor traces explicitly, by defining memory as a moving window and by crafting a small number of features of odor traces.</p>
</sec>
<sec id="s2-3">
<title>Features of odor cues: definition of discrete olfactory states and sensing memory</title>
<p>To learn a response to odor traces, we first define a finite set of <italic>olfactory states</italic>, <italic>o</italic> ∈ <italic>O</italic>, so that they bear information about the location <italic>s</italic>. Defining the olfactory states is a challenge due to the dramatic fluctuations and irregularity of turbulent odor traces. To construct a fully interpretable low dimensional state space, we aim at a small number of olfactory states that bear robust information about <italic>s</italic>, i.e. for all values of <italic>s</italic>. We previously found that pairing features of sparsity as well as intensity of turbulent odor traces predicts robustly the location of the source for all <italic>s</italic> [<xref ref-type="bibr" rid="c33">33</xref>]. Guided by these results, we use these two features extracted from the temporal history of odor detections to define a small set of olfactory states.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><p>The optimal memory <italic>T</italic>*. (A) Four measures of performance as a function of memory with backtracking recovery (solid line) show that the optimal memory <italic>T</italic>* = 20 maximizes average performance and minimizes standard deviation, except for the normalized time. Top: Averages computed over 10 realizations of test trajectories starting from 43000 initial positions (dash: results with adaptive memory). Bottom: standard deviation of the mean performance metrics for each initial condition (see Materials and Methods). (B) Average number of times agents encounter the void state along their path, <inline-formula id="ID1">
<alternatives>
<mml:math display="inline" id="I1"><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mo>∅</mml:mo></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq1.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>, as a function of memory (top); cumulative average reward <inline-formula id="ID2">
<alternatives>
<mml:math display="inline" id="I2"><mml:mrow><mml:mo>〈</mml:mo><mml:mi>G</mml:mi><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq2.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula> is inversely correlated to <inline-formula id="ID3">
<alternatives>
<mml:math display="inline" id="I3"><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mo>∅</mml:mo></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq3.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula> (bottom), hence the optimal memory minimizes encounters with the void. (C) Colormaps: Probability that agents at different spatial locations are in the void state at any point in time, starting the search form anywhere in the plume and representative trajectory of a successful searcher (green solid line) with memory <italic>T</italic> = 1, <italic>T</italic> = 20, <italic>T</italic> = 50 (left to right). At the optimal memory agents in the void state are concentrated near the edge of the plume. Agents with shorter memories encounter voids throughout the plume; agents with longer memories encounter more voids outside of the plume as they delay recovery. In all panels, shades are ± standard deviation.</p></caption>
<graphic xlink:href="2404.17495v1_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>We proceed to define a function that takes as input the history of odor detections along an agent’s path and returns its current olfactory state. We indicate with <italic>s</italic>(<italic>t</italic>) the (unknown) path of an agent, and with <italic>z</italic>(<italic>s</italic>(<italic>t</italic>), <italic>t</italic>) the observations i.e. odor detections along its path. First, we define a sensing memory <italic>T</italic> and we consider a short excerpt of the history of odor detections <italic>z<sub>T</sub></italic> of duration <italic>T</italic> prior to the current time <italic>t</italic>. Formally, <italic>z<sub>T</sub></italic>(<italic>t</italic>) := {<italic>z</italic>(<italic>s</italic>(<italic>t</italic>’), <italic>t</italic>’) |<italic>t</italic> — <italic>T</italic> ≤ <italic>t</italic>’ ≤ <italic>t</italic>}. Second, we measure the average intensity <italic>c</italic> (moving average of odor intensity over the time window <italic>T</italic> , conditioned to times when odor is above threshold), and intermittency <italic>i</italic> (the fraction of time the odor is above threshold during the sensing window <italic>T</italic>). Both features <italic>c</italic> and <italic>i</italic> are described by continuous, positive real numbers. Third, we define 15 olfactory states by discretizing <italic>i</italic> and <italic>c</italic> in 3 and 5 bins respectively. Intermittency <italic>i</italic> is bounded between 0 and 1 and we discretize it in 3 bins by defining two thresholds (33% and 66%). The average concentration, <italic>c</italic>, is bounded between 0 and the odor concentration at the source, hence prior information on the source is needed to discretize <italic>c</italic> using set thresholds. To avoid relying on prior information, we define thresholds of intensity as percentiles, based on a histogram that is populated online, along each agent’s path (see Materials and Methods). The special case where no odor is detected over <italic>T</italic> deserves attention, hence we include it as an additional state named “void state” and indicate it with <italic>o</italic> ≡ ∅. When <italic>T</italic> is sufficiently long, the resulting olfactory states map to different spatial locations (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, with <italic>T</italic> equal to the simulation time). Hence this definition of olfactory states can potentially mitigate the problem of partial observability using temporal traces, rather than spatial maps. But will these olfactory states with finite memory <italic>T</italic> guide agents to the source?</p>
</sec>
<sec id="s2-4">
<title>Q learning: a map-less and model-free navigation to odor sources</title>
<p>To answer this question, we trained tabular episodic Q learning [<xref ref-type="bibr" rid="c18">18</xref>]. In a nutshell, we use a simulator to place an agent at a random location in space at the beginning of each episode. The agent is not aware of its location in space, but it senses odor provided by the fluid dynamics simulator and thus can compute its olfactory state <italic>0</italic>, based on odor detected along its path in the previous <italic>T</italic> sensing window. It then makes a move according to a set policy of actions a <italic>a</italic> ~ π<sub>0</sub>(<italic>a</italic>|<italic>o</italic>). After the move, the simulator displaces the agent to its new location and relays the agent a negative reward <italic>R</italic> = —η if it is not at the source and a positive reward <italic>R</italic> = 1 if it reaches the source. The goal of RL is to find a policy of actions that maximizes the expected cumulative future reward <inline-formula id="ID4">
<alternatives>
<mml:math display="inline" id="I4"><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mtext>γ</mml:mtext><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq4.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula> where the expectation is over the ensemble of tra jectories and rewards generated by the policy from any initial condition. Because reward is only positive at the source, the optimal policy is the one that reaches the source as fast as possible. To further encourage the agent to reach the source quickly, we introduce a discount factor γ &lt; 1.</p>
<p>Episodes where the agent does not reach the source are ended after <italic>H<sub>max</sub></italic> = 5000 with no positive reward. As it tries actions and receives rewards, the agent learns how good the actions are. This is accomplished by estimating the quality matrix <italic>Q</italic>(<italic>o</italic>, <italic>a</italic>), i.e. the maximum expected cumulative reward conditioned to being in <italic>o</italic> and choosing action a at the present time: <inline-formula id="ID5">
<alternatives>
<mml:math display="inline" id="I5"><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>max</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mtext>γ</mml:mtext><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq5.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>. At each step, the agent improves its policy by choosing more frequently putatively good actions. Once the agent has a good approximation of the quality matrix, the optimal policy corresponds to the simple readout: π*(<italic>a</italic>|<italic>o</italic>) = δ(<italic>a</italic> — <italic>a</italic>*(<italic>o</italic>)) where <italic>a</italic>*(<italic>o</italic>) = arg max<sub>a</sub> <italic>Q</italic>(<italic>o</italic>, <italic>a</italic>), for non-void states <italic>o</italic> ≠ ∅.</p>
<sec id="s2-4-1">
<title>Recovery strategy</title>
<p>To fully describe the behavior of our Q-learning agents, we have to prescribe their policy from the void state <italic>o</italic> ≡ ∅. This is problematic because turbulent plumes are full of holes thus the void state can occur anywhere both within and outside the plume, <xref ref-type="fig" rid="fig1">Figure 1A</xref>. As a consequence, the optimal action <italic>a</italic>*(∅) from the void state is ill defined. We address this issue by using a separate policy called “recovery strategy”. Inspired by path integration as defined in biology [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>], we propose the backtracking strategy consisting in retracing the last <italic>T<sub>a</sub></italic> steps after the agent lost track of the odor. If at the end of backtracking the agent is still in the void state, it activates Brownian motion. Backtracking requires that we introduce memory of the past <italic>T<sub>a</sub></italic> actions. This timescale <italic>T<sub>a</sub></italic> for activating recovery is conceptually distinct from the duration of the sensing memory - however here we set <italic>T<sub>a</sub></italic> = <italic>T</italic> for simplicity.</p>
<p>We find that Q-learning agents successfully learn to navigate to the odor source by responding solely to their olfactory state, with no sense of space nor models of the odor cues. Learning can be quantified by monitoring the cumulative reward which continuously improves with further training episodes (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, left). Improved reward corresponds to agents learning how to reach the source more quickly and reliably with training. Indeed, it is easy to show that the expected cumulative reward <inline-formula id="ID6">
<alternatives>
<mml:math display="inline" id="I6"><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mtext>λτ</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mtext>λτ</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>γ</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq6.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>, where τ is a random variable corresponding to time to reach the source and <italic>γ</italic> = <italic>e</italic><sup>-λΔ<italic>t</italic></sup> is the discount factor, with the time step Δ<italic>t</italic> = 1 (see Materials and Methods). Large rewards arise when <italic>(i)</italic> a large fraction <italic>f</italic><sup>+</sup> of agents successfully reaches the source and <italic>(ii)</italic> the agents reach the source quickly, which maximizes <inline-formula id="ID7">
<alternatives>
<mml:math display="inline" id="I7"><mml:msup><mml:mi>g</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mtext>λτ</mml:mtext></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mtext>success</mml:mtext></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq7.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>. Indeed <italic>G</italic> = <italic>f</italic><sup>+</sup><italic>G</italic><sup>+</sup> + (1 — <italic>f</italic><sup>+</sup>)<italic>G</italic><sup>–</sup>, where <italic>G</italic><sup>+</sup> = <italic>g</italic><sup>+</sup> — <italic>η</italic>(1 — <italic>g</italic><sup>+</sup>)/(1 — γ) and <italic>G</italic><sup>–</sup> = —γ(1 — e<sup>-λ<italic>H</italic><sub>max</sub></sup>)/(1 — γ). <italic>H</italic><sub>max</sub> is the horizon of the agent i.e. the maximum time the agent is allowed to search, and after which the search is considered failed. Note that agents starting closer to the target receive larger rewards purely because of their initial position. To monitor performance independently on the starting location, we introduce the inverse time to reach the source relative to the shortest-path time from the same initial location, which goes from 0 for failing agents to 1 for ideal agents xtmin <inline-formula id="ID8">
<alternatives>
<mml:math display="inline" id="I8"><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq8.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>, independently on their starting location. Note that this is not the quantity that is optimized for. One may specifically target this perfomance metrics, which is agnostic on the duration of an agent’s path, by discounting proportionally to <italic>t</italic>/τ<sub>min</sub>. All four measures of performance plateau to a maximum, suggesting learning has achieved a nearly optimal policy (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Once training is completed, we simulate the tra jectory of test-agents starting from any of the about 43 000 admissible locations within the plume and moving according to the optimal policy. We will recapitulate performance with the cumulative reward <italic>G</italic> averaged over the test-agents and dissect it into speed <italic>g</italic><sup>+</sup>, convergence <italic>f</italic><sup>+</sup> and relative time <inline-formula id="ID9">
<alternatives>
<mml:math display="inline" id="I9"><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq9.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>.</p>
</sec>
</sec>
<sec id="s2-5">
<title>Optimal memory</title>
<p>By repeating training using different values of <italic>T</italic> we find that performance depends on memory and an optimal memory <italic>T</italic>* exists (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Why is there an optimal memory? The shortest memory <italic>T</italic> = 1 corresponds to instantaneous olfactory states: the instantaneous contour maps of the olfactory states are convoluted and the void state is pervasive (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, top). As a consequence, agents often activate recovery even when they are within the plume, the policy almost always leads to the source (<italic>f</italic><sup>+</sup> = 79% ± 13%) but follows lengthy convoluted paths (τ<sub>min</sub>/τ = 0.14 ± 0.05, <xref ref-type="fig" rid="fig2">Figure 2A</xref>, bottom). As memory increases, the olfactory states become smoother and agents encounter less voids (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, center), perform straighter tra jectories (τ<sub>min</sub>/τ = 0.5 ± 0.3) and reach the source reliably (<italic>f</italic><sup>+</sup> = 95% ± 8%), <xref ref-type="fig" rid="fig2">Figure 2C</xref>, bottom. Further increasing memory leads to even less voids within the plume and even smoother olfactory states. However - perhaps surprisingly - performance does not further improve but slightly decreases (at <italic>T</italic> = 50, <italic>f</italic><sup>+</sup> = 94% ± 8% and τ<sub>min</sub>/τ = 0.38 ± 0.36). A long memory is deleterious because it delays recovery from accidentally exiting the plume, thus increases the number of voids <italic>outside</italic> of the plume (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, bottom). Indeed, agents often leave the plume accidentally as they measure their olfactory state <italic>while they move</italic>. They receive no warning, but realize their mistake after <italic>T</italic> steps, when they enter the void state and activate recovery to re-enter the plume. The delay is linear with memory when agents recover by backtracking, but it depends on the recovery strategy (see discussion below and <xref ref-type="fig" rid="fig6">Supplementary Figure 1</xref>).</p>
<p>Thus short memories increase time in void <italic>within</italic> the plume, whereas long memories increase time in void <italic>outside</italic> the plume: the optimal memory minimizes the overall chances to experience the void (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Intuitively, <italic>T</italic>* should match the typical duration <italic>τ<sub>b</sub></italic> of blanks encountered within the plume, so that voids within the plume are effectively ignored without delaying recovery unnecessarily. Consistently, <inline-formula id="ID10">
<alternatives>
<mml:math display="inline" id="I10"><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq10.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula> averaged across all locations and times within the plume is <inline-formula id="ID11">
<alternatives>
<mml:math display="inline" id="I11"><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>9.97</mml:mn><mml:mo>±</mml:mo><mml:mn>41.16</mml:mn></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq11.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>, comparable with the optimal memory <italic>T</italic>* (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p>
</sec>
<sec id="s2-6">
<title>Adaptive memory</title>
<p>There is no way to select the optimal memory <italic>T</italic>* without comparing several agents or relying on prior information on the blank durations. In order to avoid prior information, we venture to define memory adaptively along each agent’s path, using the intuition outlined above. We define a buffer memory <italic>T<sub>b</sub></italic> , and let the agent respond to a sensing window <italic>T</italic> &lt; <italic>T<sub>b</sub></italic>. Ideally we would like to set <inline-formula id="ID12">
<alternatives>
<mml:math display="inline" id="I12"><mml:mi>T</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq12.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>. With this choice, blanks shorter than the average blank are ignored, as they are expected within the plume, whereas blanks longer than average initiate recovery, as they signal that the agent exited the plume. However, agents do not have access to <inline-formula id="ID13">
<alternatives>
<mml:math display="inline" id="I13"><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq13.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula> hence we set <inline-formula id="ID14">
<alternatives>
<mml:math display="inline" id="I14"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>b</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq14.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula> , where <inline-formula id="ID15">
<alternatives>
<mml:math display="inline" id="I15"><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>b</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq15.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula> is the most recent blank experienced by the agent. With this choice, the sensing memory <italic>T</italic> fluctuates considerably along an agent’s path, due to turbulence ([<xref ref-type="bibr" rid="c32">32</xref>] and <xref ref-type="fig" rid="fig3">Figure 3A-B</xref>). Note that blanks are estimated along paths, thus the statistics of <italic>T</italic> only qualitatively matches the Eulerian statistics of <italic>τ<sub>b</sub></italic>. Despite the fluctuations, performance using the adaptive memory nears performance with the optimal memory (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This result confirms our intuition that memory should match the blank time. The advantage of the adaptive memory is that it relies solely on experience, with no prior information whatsoever. This is unlike <italic>T</italic>* which can only be selected using prior information, with no guarantee of generalization to other plumes.</p>
</sec>
<sec id="s2-7">
<title>Learning to recover</title>
<p>So far, our agents combine a learned policy from non-void states to a heuristics from the void state, which we called the recovery strategy. We have considered a biologically-inspired heuristics where searchers make it back to locations within the plume by retracing their path backward. Similar results are obtained for Brownian recovery with a different optimal memory (see Materials and Methods and <xref ref-type="fig" rid="fig6">Supplementary Figure 1</xref>). To further strip the algorithm of heuristics, we ask whether the recovery strategy may be learned, rather than fixed a priori. To this end, we split the void state in many states, labeled with the time elapsed since first entering the void. The counter is reset to 0 whenever the searcher detects the odor. The definition of the 15 non-void states <italic>o</italic><sub>1</sub>, …, <italic>o</italic><sub>15</sub> remains unaltered. Interestingly, with this added degree of freedom, the agent learns an even better recovery strategy as reflected by all our measures of performance <xref ref-type="fig" rid="fig3">Figure 3D</xref>. Note that the learned recovery strategy resembles the casting behavior observed in flying insects [<xref ref-type="bibr" rid="c37">37</xref>], as discussed below.</p>
<sec id="s2-7-1">
<title>Characterization of the optimal policies</title>
<p>To understand how different recoveries affect the agent’s behavior, we characterize the optimal policies obtained using the three recovery strategies. We visualize the probability to encounter each of the 16 olfactory states, or occupancy (circles in <xref ref-type="fig" rid="fig4">Figure 4</xref>), and the spatial distribution of the olfactory states. In the void state, the agent activates the recovery strategy. Recovery from the void state affects non-void olfactory states as well: their occupancy, their spatial distribution, and the action they elicit (<xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig8">Supplementary Figure 3</xref>). This is because the agent computes its olfactory state online, according to its prior history which is affected by encounters with the void state. However, for all recoveries, non-void states are mostly encountered within the plume and largely elicit upwind motion (<xref ref-type="fig" rid="fig4">Figure 4</xref>, top, center). Thus macroscopically, all agents learn to surge upwind when they detect any odor within their memory, and to recover when their memory is empty. This suggests a considerable level of redundancy which may be leveraged to reduce the number of olfactory states, thus the computational cost.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><p>The adaptive memory approximates the duration of the blank dictated by physics and it is an efficient heuristics, especially when coupled with a learned recovery strategy. (A) Colormaps of the Eulerian blank times <italic>τ<sub>b</sub></italic> (top) and the sensing memory <italic>T</italic> (bottom): Left: averages; Right: standard deviations. The sensing memory statistics is computed over all agents that are located at each discrete cell, at any point in time. (B) Probability distribution of <italic>τ<sub>b</sub></italic> across all spatial locations and times (black) and of <italic>T</italic> across all agents at all times (gray). (C) Performance with the adaptive memory nears performance of the optimal fixed memory, here shown for backtracking; similar results apply to the Brownian recovery (<xref ref-type="fig" rid="fig7">Supplementary Figure 2</xref>). (D) Comparison of three recovery strategies with adaptive memory: The learned recovery with adaptive memory outperforms all fixed and adaptive memory agents. In (C) and (D) dark squares mark the mean, and light rectangles mark ± standard deviation. No standard deviation is shown for the <italic>f</italic><sup>+</sup> measure in the learned case as this strategy is deterministic (see Materials and Methods).</p></caption>
<graphic xlink:href="2404.17495v1_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><p>Optimal policies with adaptive memory for different recovery strategies: backtracking (green), Brownian (red) and learned (blue). For each recovery, we show the spatial distribution of the olfactory states (top); the policy (center) and the state occupancy (bottom) for non-void states (left) <italic>vs</italic> the void state π*(<italic>a</italic>|∅)(right). Spatial distribution: probability that an agent at a given position is in any non-void olfactory state (left) or in the void state (right), color-coded from yellow to blue. Policy: actions learned in the non-void states <inline-formula id="ID16">
<alternatives>
<mml:math display="inline" id="I16"><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mo>≠</mml:mo><mml:mo>∅</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mi>π</mml:mi><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq16.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>, weighted on their occupancy <italic>n<sub>o</sub></italic> (left, arrows proportional to the frequency of the corresponding action) and schematic view of recovery policy in the void state (right). State occupancy: fraction of agents that is in any of the 15 non-void states (left) or in the void state (right) at any point in space and time. Occupancy is proportional to the radius of the corresponding circle. The position of the circle identifies the olfactory state (rows and columns indicate the discrete intensity and intermittency respectively). All statistics is computed over 43000 trajectories, starting from any location within the plume.</p></caption>
<graphic xlink:href="2404.17495v1_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>The void state shows the most relevant differences: for both heuristic recoveries, 40% or more of the agents are in the void state and they are spatially spread out. In contrast, in the case of learned recovery, the optimal policy limits occurrence of the void state to 26% of the agents, confined to a narrow band near the edge of the plume. From these locations, the agents quickly recover the plume, explaining the boost in performance discussed above. Note that, exclusively for the learned recovery, the optimal policy is enriched in actions downwind to avoid overshooting the source. Indeed, from positions beyond the source, the learned strategy is unable to recover the plume as it mostly casts sideways, with little to no downwind action. Intuitively, the precise locations where agents move downwind may be crucial to efficiently avoid overshooting. Thus the policy may depend on specific details of the odor plume, consistent with poorer generalization of the learned recovery (discussed next).</p>
</sec>
</sec>
<sec id="s2-8">
<title>Tuning for adaptation to different environments</title>
<p>Finally, we test performance of the trained agents on six environments, characterized by distinct fluid flows and odor plumes (<xref ref-type="fig" rid="fig5">Figure 5</xref> and Materials and Methods). Environment 1 is the native environment, where the agents were originally trained; Environment 2 is obtained by increasing the threshold of detection, which makes the signals considerably more sparse with longer blanks. Environments 3 and 4 are closer to the lower surface of the simulated domain, where the plume is smaller and fluctuates less. Environment 5 is a similar geometry, but obtained for a smaller Reynolds number and a different way to generate turbulence. Finally Environment 6 has an even larger Reynolds number, a longer domain and a smaller source, which creates an even more dramatically sparse signal. We consider agents with adaptive memory and compare the three recovery strategies discussed above - Brownian, backtracking and learned, see <xref ref-type="fig" rid="fig5">Figure 5B</xref>. Comparing performance across environments we find that: <italic>(i)</italic> although performance is degraded when testing in non native environments, all agents with adaptive memory are still extremely likely to find the source; <italic>(ii)</italic> Brownian recovery has lowest performance and generalization across all environments; <italic>(iii)</italic> backtracking provides good performance and generalization; <italic>(iii)</italic> the learned recovery strategy performs best in all environments by all performance metrics. In the most intermittent Environment 6 a striking 91% of agents succeeds in finding the source, with trajectories less the twice as long as the shortest path to the source. The upshot of generalization is that agents may navigate distinct turbulent plumes using a baseline strategy learned in a specific plume. Importantly, even if performance (mildly) degrades, most agents still do reach the source, suggesting that fine-tuning this strategy may enable efficient adaptation to different environments. Further work is needed to establish how much fine-tuning is needed to fully adapt the baseline strategy to different environments.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><p>Generalization to statistically different environments. (A) Snapshots of odor concentration normalizes with concentration at the source, colorcoded from blue (0) to yellow (1) for environment 1 to 6 from top to bottom. Environment 1* is the native environment where all agents are trained. (B) Performance for the three recovery strategies backtracking (green) Brownian (red) and learned (blue), with adaptive memory, trained on the native environment and tested across all environments. Four measures of performance defined in the main text are shown. Dark squares mark the mean, and empty rectangles ± standard deviation. No standard deviation is shown for the <italic>f</italic><sup>+</sup> measure in the learned case as this strategy is deterministic (see Materials and Methods).</p></caption>
<graphic xlink:href="2404.17495v1_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3" sec-type="discussion">
<title>Discussion</title>
<p>In this work, we showed that agents exposed to a turbulent plume learn to associate salient features of the odor time trace - the olfactory state - to an optimal move that guides them to the odor source. The upshot of responding solely to odor is that the agent does not navigate based on <italic>where</italic> it believes the target is and thus needs no map of space nor prior information about the odor plume, which avoids considerable computational burden. On the flip side, in our stimulus-response algorithm, agents need to start from within the plume, however sparse and fragmented. Indeed, far enough from the source, Q-learning agents are mostly in the void state and they can only recover the plume if they have previously detected the odor or are right outside the plume. In contrast, agents using a map of space can navigate from larger distances than are reachable by responding directly to odor cues. Indeed, in the map-based POMDP setting, absence of odor detection is still informative and it enables agents to first find the plume, and then refine the search to localize the target within the plume [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c38">38</xref>].</p>
<p>We show that because turbulent odor plumes constantly switch on and off, navigation must handle both absence and presence of odor stimuli. We address this fundamental issue by alternating between two distinct strategies: <italic>(i)</italic> Prolonged absence of odor prompts entry in the void state and triggers a recovery strategy to make contact with the plume again. We explored two heuristic recoveries and found that back-tracking to where the agent last detected odor is much more efficient than Brownian recovery. An even more efficient recovery can be learned that resembles cross-wind casting and limits the void state to a narrow region right outside of the plume. Casting is a well-studied computational strategy [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c5">5</xref>] also observed in animal behavior, most famously in flying insects [<xref ref-type="bibr" rid="c37">37</xref>]. Intriguingly, cast and surge also emerges in algorithms making use of a model of the odor, whether for Bayesian updates or for policy optimization [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c30">30</xref>]. Whether natural casting behavior is learned, as in Q-learning, or is hard-wired in a model of the odor plume remains a fascinating question for further research. <italic>(ii)</italic> Odor detections prompt entry in non-void olfactory states, which predominantly elicit upwind surge. Blanks shorter than the sensing memory are ignored, i.e. agents do not enact recovery but respond to stimuli experienced prior to the short blank. Further work may optimize these non-void olfactory states by feature engineering, e.g. testing different discretizations to reduce redundancy or screening a large library of features using supervised learning as in [<xref ref-type="bibr" rid="c33">33</xref>] to potentially improve performance. Alternatively, feature engineering may be bypassed altogether by the use of recurrent neural networks (RNNs) as recently proposed in [<xref ref-type="bibr" rid="c29">29</xref>], possibly at the expense of interpretability. A systematic comparison using a common dataset is needed to elucidate how other heuristic and normative model-free algorithms handle odor presence vs odor absence.</p>
<p>To switch between the odor-driven strategy and the recovery strategy, we introduce a timescale <italic>T</italic> , which is an explicit form of temporal memory. <italic>T</italic> delimits a sensing window extending in the recent past, prior to the present time. All odor stimuli experienced within the sensing window affect the current response. By using fixed memories of different duration, we demonstrate that an optimal memory exists and that the optimal memory minimizes the occurrence of the void state. On the one hand, long memories are detrimental because they delay recovery from accidentally exiting the plume. On the other hand, short memories are detrimental because they trigger recovery unnecessarily, i.e. even for blanks typically experienced within the turbulent plume. The optimal memory thus matches the typical duration of the blanks. To avoid using prior information on the statistics of the odor, we propose a simple heuristics setting memory adaptively equal to the most recent blank experienced along the path. The adaptive memory nears optimal performance despite dramatic fluctuations dictated by turbulence. Success of the heuristics suggests that a more accurate estimate of the future blank time may enable an even better adaptive memory; further work is needed to corroborate this idea.</p>
<p>Thus in Q-learning, memory is a temporal window matching odor blanks and distinguishing whether agents are in or out of the plume. The role of memory for olfactory search has been recently discussed in ref. [<xref ref-type="bibr" rid="c30">30</xref>]. In POMDPs, memory is stored in a detailed belief of agent position relative to the source. In finite state controllers, memory denotes an internal state of the agent and was linked to a coarse grained belief of the searcher being within or outside of the plume, similar to our findings. In recurrent neural networks memory is stored in the learned weights. A quantitative relationship between these different forms of memory and their connection to spatial perception remains to be understood.</p>
<p>We conclude by listing a series of experiments to test these ideas in living systems. First, olfactory search in living systems displays memory ([<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c10">10</xref>] and refs. therein). In insects, temporal scales can be measured associated to memory. Indeed, for flying insects loss of contact with a pheromone plume triggers crosswind casting and sometimes even downwind displacement [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c39">39</xref>]. Interestingly, the onset of casting is delayed with respect to loss of contact with the plume, but this delay is not understood [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>]. In walking flies, the timing of previous odor encounters biases navigation [<xref ref-type="bibr" rid="c41">41</xref>]. (How) do these temporal timescales depend on the waiting times between previous detections? Using optogenetics [<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>] or olfactory virtual reality with controlled odor delivery [<xref ref-type="bibr" rid="c46">46</xref>] experiments may measure memory as a function of the full history of odor traces. For insects, one may monitor memory by tracking the onset of cross wind casting with respect to the loss of the plume. More in general, a temporal memory may be defined by monitoring how far back in the past should two odor traces be identical in order to elicit the same repertoire of motor controls.</p>
<p>Second, our algorithm learns a stimulus-response strategy that relies solely on odor cues. The price to pay is that the agent must follow the ups and downs of the odor trace in order to compute averages and recognize blanks. A systematic study may use our algorithm to test the requirements of fidelity of this temporal representation, and how it depends on turbulence. How does turbulence affect the fidelity of odor temporal representation in living systems? Crustaceans provide excellent model system to ask this question, as they are known to use bursting olfactory receptor neurons to encode temporal information from olfactory scenes [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c48">48</xref>]. Temporal information is also encoded in the olfactory bulb of mammals [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. Organisms with chemo-tactile systems like the octopus [<xref ref-type="bibr" rid="c51">51</xref>] may serve as a comparative model, to ask whether touch-chemosensation displays a sloppier temporal response, reflecting that surface-bound stimuli are not intermittent.</p>
<p>Third, our Q-learning algorithm requires the agent to receive olfactory information, thus start near or within the odor plume. In contrast, algorithms making use of a spatial map and prior information on the odor plume may first search for the plume (in conditions of near zero information) and then search the target within the plume [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. Animals are known to use prior information to home into regions of space where the target is more likely to be found; but they can switch to navigation in response to odor (see e.g. [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>]). What triggers the switch from navigation driven by spatial perception to navigation driven by odor? For mice, the need for spatial perception may be tested indirectly by comparing paths in light <italic>vs</italic> dark, noting that neuronal place fields, that mediate spatial perception, are better stabilized by vision than olfaction [<xref ref-type="bibr" rid="c52">52</xref>, <xref ref-type="bibr" rid="c53">53</xref>]. Thus in the light, animals have the ability to implement both map-less and map-based algorithms, whereas in the dark they are expected to more heavily rely on map-less algorithms. To make sure animals start searching for the odor target even before sensing odor, operant conditioning can be deployed so that animals associate an external cue (e.g. a sound) to the beginning of the task.</p>
<p>The reinforcement learning view of olfactory navigation offers an exciting opportunity to probe how living systems interact with the environment to accomplish complex real-world tasks affected by uncertainty. Coupling time varying odor stimuli with spatial perception is an instance of the broader question asking how animals combine prior knowledge regarding the environment with reaction to sensory stimuli. We hope that our work will spark further progress into connecting these broader questions to the physics of fluids.</p>
</sec>
</body>
<back>
<sec id="s4">
<title>Author’s contributions</title>
<p>M.R., A.V., L.R. and A.S. designed research; M.R., M.J., A.V., L.R. and A.S. performed research; M.R., L.R. and A.S. wrote manuscript.</p>
<p>The authors declare no conflict of interest.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This research was supported by grants to A.S. from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 101002724 RIDING), the Air Force Office of Scientific Research under award number FA8655-20-1-7028, and the National Institutes of Health (NIH) under award number R01DC018789. L.R. acknowledges the financial support of the European Research Council (grant SLING 819789). We thank Francesco Viola for support and discussions regarding computational fluid dynamics as well as Antonio Celani, Venkatesh Murthy, Yujia Qi, Francesco Boccardo, Luca Gagliardi, Francesco Marcolli and Arnaud Ruymaekers for comments on the manuscript.</p>
</ack>
<sec id="s5" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="s5-1">
<title>Data description</title>
<p>The data we used to train the agents is a set of 2598 matrices <inline-formula id="ID17">
<alternatives>
<mml:math display="inline" id="I17"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2538</mml:mn></mml:mrow></mml:msubsup></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq17.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>. Every matrix <italic>D<sub>t</sub></italic> ∈ ℝ<sup>1225×280</sup> contains the odor intensity in every position (<italic>i</italic>, <italic>j</italic>) i.e. (<italic>D<sub>t</sub></italic>)<sub><italic>i,j</italic></sub> represents the odor intensity in position (<italic>i</italic>, <italic>j</italic>) at time <italic>t</italic>. The source of odor is in position p20, 142q and, in order to simplify the training, we considered as terminal states every position in a circle centered in the source position and with radius 10 called <italic>source region</italic>. Data are obtained from a direct numerical simulation of the Navier-Stokes equations and the equations of transport of the odor. Environments 1 to 4 are derived from simulations of a channel flow described in ref [<xref ref-type="bibr" rid="c33">33</xref>]; Environment 5 corresponds to an additional simulation described below. We preprocess the data to eliminate simulation noise by setting to zero every entry of these matrices when they are smaller than a <italic>noise level</italic> <italic>n</italic><sub>lvl</sub> := 3 × 10<sup>-6</sup>. Data information are summarized in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p>
<table-wrap id="tbl1" position="float" orientation="portrait">
<label>Table 1</label>
<caption><title>Data information</title></caption>
<alternatives>
<graphic xlink:href="2404.17495v1_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" valign="top">Feature</th>
<th align="left" valign="top">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Shape</td>
<td align="left" valign="top">1225 × 280</td>
</tr>
<tr>
<td align="left" valign="top">Number of time slices</td>
<td align="left" valign="top">2598</td>
</tr>
<tr>
<td align="left" valign="top"><italic>n</italic><sub>thr</sub></td>
<td align="left" valign="top">3 × 10<sup>-6</sup></td>
</tr>
<tr>
<td align="left" valign="top">Source location</td>
<td align="left" valign="top">(20, 142)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In environment 5, the odor is advected by a turbulent open channel flow, with three hemispherical obstacles placed on the ground close to the inlet to generate turbulence. The Navier-Stokes equations (1) and advectiondiffusion equation for odor transport (2) are solved using a central second order finite difference scheme. The convective terms are discretized in time using an explicit Adams - Bashforth method; and the viscous and diffusion terms using an implicit Crank-Nicolson method [<xref ref-type="bibr" rid="c54">54</xref>]. The code is written in Fortran and is GPU parallelized. The channel is divided into 1024 x 256 x 128 grid points along streamwise, spanwise and wall-normal directions respectively. The corresponding average spatial resolutions are Δ<italic>x</italic> = 5η, Δ<italic>y</italic> = 5η, Δ<italic>z</italic> = 4η, where η is the Kolmogorov length scale. Three hemispheres of radius 100η are placed at a distance of 250η from the inlet on the ground, equally spaced along the spanwise direction. The channel is forced using a constant pressure gradient. For the velocity field, we impose a no-slip boundary condition at the ground and on the obstacles (<italic>u</italic> = 0) and a free-slip boundary on top (<italic>u<sub>z</sub></italic> = 0, <italic>∂<sub>z</sub>u<sub>x</sub></italic> = <italic>∂<sub>z</sub>u<sub>y</sub></italic> = 0). The velocity field is periodic along the streamwise and spanwise directions. The bulk Reynolds number is 7800. For the odor field, we impose Dirichlet condition (<italic>c</italic> = 0) at the ground, on the obstacles and inlet, no-flux (<italic>∂<sub>z</sub>c</italic> = 0) on top, and outflow along other directions. Similar to the native environment, we choose the Schmidt number to be 1. The odor source is located downstream of the obstacle and centered at [640η, 640η, 256η] along streamwise, spanwise and wall-normal directions respectively. The odor source has a Gaussian profile with a standard deviation of 8η.</p>
<p>Environment 6 is similar to environment 5 albeit with a higher bulk Reynolds number of 17500. Here, the channel is divided into 2000 × 500 × 200 grid points and has an average spatial resolution of Δ<italic>x</italic> = Δ<italic>y</italic> = Δ<italic>z</italic> = 5.5η. The odor source has a Gaussian profile centered at [825η, 1375η, 550η] with a standard deviation of 3η.</p>
<disp-formula id="FD1">
<alternatives>
<mml:math id="M1" display="block"><mml:mtable><mml:mtr><mml:mtd><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo>⋅</mml:mo><mml:mo>∇</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mi>g</mml:mi><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∇</mml:mo><mml:mo>⋅</mml:mo><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math>
<graphic xlink:href="2404.17495v1_eqn1.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(1)</label>
</disp-formula>
<disp-formula id="FD2">
<alternatives>
<mml:math id="M2" display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo>⋅</mml:mo><mml:mo>∇</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn2.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(2)</label>
</disp-formula>
<p>Olfactory states, Features &amp; Discretization. Each agent stores the odor concentrations detected in the previous <italic>T</italic> time steps in a vector M. We introduce an adaptive sensitivity threshold function sthrP-q defined as
<disp-formula id="FD3">
<alternatives>
<mml:math id="M3" display="block"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn3.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(3)</label>
</disp-formula>
</p>
<p>where <italic>M<sub>i</sub></italic> denotes the <italic>i</italic>-th element of <italic>M</italic> and <italic>C<sub>thr</sub></italic> &gt; 0 is a scaling constant (in our experiments we set it as 0.5). <italic>T</italic> denotes the cardinality of <italic>M</italic>. Given a memory <italic>M</italic>, we can define the filtered memory Δ<sup>M</sup> as the set which contains every element of the memory <italic>M</italic> that is higher than the sensitivity threshold <italic>s</italic><sub>thr</sub> (<italic>M</italic>) i.e.</p>
<disp-formula id="FD4">
<alternatives>
<mml:math id="M4" display="block"><mml:msup><mml:mi>Δ</mml:mi><mml:mi>M</mml:mi></mml:msup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:mi>M</mml:mi><mml:mo>|</mml:mo><mml:mi>z</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn4.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(4)</label>
</disp-formula>
<p>Then at timestep <italic>t</italic>, given the agent memory <italic>M<sub>t</sub></italic> , we define the average intensity <italic>c</italic>(<italic>M<sub>t</sub></italic>) and the intermittency <italic>i</italic>(<italic>M<sub>t</sub></italic>) as:
<disp-formula id="FD5">
<alternatives>
<mml:math id="M5" display="block"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Δ</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Δ</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Δ</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Δ</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Δ</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math>
<graphic xlink:href="2404.17495v1_eqn5.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(5)</label>
</disp-formula>
</p>
<p>Note that the average intensity is defined on the filtered memory Δ<sup><italic>M</italic></sup>, i.e. conditioned to detecting odors above threshold. Since the features defined in (5) returns real numbers, in order to use (tabular) q-learning, we need to discretize them. We denote with <inline-formula id="ID18">
<alternatives>
<mml:math display="inline" id="I18"><mml:mover accent="true"><mml:mi>i</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq18.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula>(<italic>M<sub>t</sub></italic>) the discretized intermittency. This is defined as follow
<disp-formula id="FD6">
<alternatives>
<mml:math id="M6" display="block"><mml:mover accent="true"><mml:mi>i</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.33</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>  </mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.33</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.66</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>  </mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.66</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn6.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(6)</label>
</disp-formula>
</p>
<p>The average intensity is bounded between zero and the maximum concentration of odor at the source. To avoid prior information on the source, we use a more structured procedure to discretize the average intensity online, based on the agent’s experience only. At every timestep <italic>t</italic>, the average intensity <italic>c</italic>(<italic>M<sub>t</sub></italic>) is computed and collected in a dataset <italic>X<sub>t</sub></italic> i.e.</p>
<disp-formula id="FD7">
<alternatives>
<mml:math id="M7" display="block"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn7.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
<p>Then, its discretized value is obtained by the following rule:
<disp-formula id="FD8">
<alternatives>
<mml:math id="M8" display="block"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>25</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>25</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>50</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>50</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>80</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>80</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>99</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>99</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn8.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(7)</label>
</disp-formula>
</p>
<p>where <italic>p</italic>(<italic>X<sub>t</sub></italic>, <italic>n</italic>) denotes the <italic>n</italic>-th percentile of <italic>X<sub>t</sub></italic>. Finally, we can define the feature map <italic>∅<sub>t</sub></italic> as a function of the memory <italic>M<sub>t</sub></italic> and the dataset of average intensities <italic>X<sub>t</sub></italic> at timestep <italic>t</italic>
<disp-formula id="FD9">
<alternatives>
<mml:math id="M9" display="block"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>i</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn9.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>This defines the current olfactory state <italic>s<sub>t</sub></italic> i.e. at timestep <italic>t</italic>, the agent is in the olfactory state <italic>o<sub>t</sub></italic> := <italic>∅<sub>t</sub></italic>(<italic>M<sub>t</sub></italic>, <italic>X<sub>t</sub></italic>). The case where the agent has no odor detections above threshold in its current memory, i.e. |Δ(<italic>M<sub>t</sub></italic>)| = 0 corresponds to an additional state called void state (∅) in the main text.</p>
</sec>
<sec id="s5-2">
<title>Agent Behavior and Policies</title>
<p>Now, we describe how the agent interacts with the environment to solve the navigation problem. At every time step <italic>t</italic> ∈ ℕ, the agent observes an odor point <italic>z<sub>t</sub></italic> and updates its memory including the new observation and removing the oldest i.e. it defines a memory <italic>M<sub>t</sub></italic> with the following rule
<disp-formula id="FD10">
<alternatives>
<mml:math id="M10" display="block"><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn10.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(8)</label>
</disp-formula>
</p>
<p>Then, it updates the dataset of average intensities i.e. <italic>X<sub>t</sub></italic> := <italic>X</italic><sub><italic>t</italic>-1</sub> ∪ {<italic>c</italic>(<italic>M<sub>t</sub></italic>)} and it computes the olfactory state <italic>o<sub>t</sub></italic>. According to <italic>o<sub>t</sub></italic>, the agent chooses an action <italic>a<sub>t</sub></italic> using a policy. As indicated in the main text, actions are the coordinate directions i.e. we define an action set A as follow
<disp-formula id="FD11">
<alternatives>
<mml:math id="M11" display="block"><mml:mi>A</mml:mi><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn11.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where <italic>e<sub>i</sub></italic> denotes the <italic>i</italic>-th canonical base. As explained in the main text, actions are selected using one of two policies according to the current olfactory state <italic>o<sub>t</sub></italic>. More precisely, if the olfactory state <italic>o<sub>t</sub></italic> is not the void state, then the (ε-greedy) Q-learning policy is used. Formally, let <italic>Q</italic> be the Q matrix of the agent and let <italic>o<sub>t</sub></italic> ≠ ∅, then the agent plays the action <italic>a<sub>t</sub></italic> such that
<disp-formula id="FD12">
<alternatives>
<mml:math id="M12" display="block"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>arg</mml:mi><mml:mtext> </mml:mtext><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mo>∼</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mtext> </mml:mtext><mml:mi>ε</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math>
<graphic xlink:href="2404.17495v1_eqn12.tif" mime-subtype="tif" mimetype="image"/></alternatives>
<label>(9)</label>
</disp-formula>
</p>
<p>where, with <italic>a</italic> ~ <italic>u</italic> (A), we indicate an action <italic>a</italic> uniformly sampled from <italic>A</italic>. At test phase, the explorationexploitation parameter ε is set to 0 and, thus, in an olfactory state <italic>o<sub>t</sub></italic> ≠ ∅ the policy is deterministic. While training phase behavior is described in next paragraphs. In the void state <italic>o<sub>t</sub></italic> = ∅, the agent chooses the action <italic>a<sub>t</sub></italic> ∈ <italic>A</italic> according to a separated policy called <italic>recovery strategy</italic>. In our experiments, we defined and compared three different recovery strategies: Brownian, Backtracking and Learned.</p>
</sec>
<sec id="s5-3">
<title>Brownian recovery</title>
<p>It is the simplest strategy we consider, consisting of playing random actions in the void state. Suppose that at time step t, the agent is in the void olfactory state, i.e., <italic>o<sub>t</sub></italic> = ∅, then <italic>a<sub>t</sub></italic> is sampled uniformly from the action set <italic>A</italic>. However, it is important to note for long memories agents start to recover when they are already far from the plume, and hitting the plume by random walk is prohibitively long. To avoid wandering away from the plume, the memory is constrained to be shorter, consistent with the observation that the optimal memory is <italic>T</italic>* = 3 to 5, much shorter than for backtracking. At this memory, several blanks within the plume will cause the agent to recover, hence the lower performance of the Brownian recovery.</p>
</sec>
<sec id="s5-4">
<title>Backtracking Recovery</title>
<p>In order to accelerate recovery from accidentally exiting the plume, we let the agents backtrack to the position where they last detected the odor. To this end, we first enumerate the actions with numbers from one to four. Then we introduce a new memory called <italic>action memory A</italic>. For simplicity, we consider the setting in which |<italic>A</italic>| = |<italic>M</italic>|. At timestep <italic>t</italic> = 0, this memory is initialized as a vector of zeros indicating that the action memory is empty i.e. we define <italic>A</italic><sub>0</sub> ∈ ℕ<sub>|<italic>M</italic>|</sub> such that for every <italic>i</italic> = 1, …, |<italic>A</italic>|
<disp-formula id="FD13">
<alternatives>
<mml:math id="M13" display="block"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:math>
<graphic xlink:href="2404.17495v1_eqn13.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>For every timestep <italic>t</italic> &gt; 0, the agent observes an odor point <italic>z<sub>t</sub></italic> and updates the memory through (8). Moreover, the action memory is updated according to the status of the memory. If the last observation is smaller than the sensitivity threshold i.e. <italic>z<sub>t</sub></italic> &lt; <italic>s<sub>thr</sub></italic>(M<sub>t</sub>), the action previously played <italic>a</italic><sub><italic>t</italic>-1</sub> (represented by a natural number in (1,4) is stored in the action memory i.e. for some Δ &gt; 0, let
<disp-formula id="FD14">
<alternatives>
<mml:math id="M14" display="block"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>Δ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn14.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>Then
<disp-formula id="FD15">
<alternatives>
<mml:math id="M15" display="block"><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>Δ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn15.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>If at time-step <italic>t</italic>, the observation <italic>z<sub>t</sub></italic> is larger than the sensitivity threshold then the action memory is reset i.e. <italic>A<sub>t</sub></italic> ∈ ℕ<sup>|<italic>M</italic>|</sup> with (<italic>A<sub>t</sub></italic>)<italic><sub>i</sub></italic> = 0 for every <italic>i</italic>. If at timestep <italic>t</italic>, the memory is empty i.e. <italic>c</italic>(<italic>M<sub>t</sub></italic>) = 0, then the backtracking procedure is executed: the last non-zero element of the action memory is extracted and the inverse action is played i.e. For some Δ &gt; 0, let
<disp-formula id="FD16">
<alternatives>
<mml:math id="M16" display="block"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>Δ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn16.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>Then, it plays the action <italic>a</italic><sub><italic>t</italic>-2</sub> and updates the action memory as follow
<disp-formula id="FD17">
<alternatives>
<mml:math id="M17" display="block"><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>Δ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn17.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>This procedure is repeated until either an observation larger than the sensitivity threshold is obtained or the action memory becomes empty. In the former case, the action memory is cleared and the action is chosen according to the Q-learning policy ((9)). In the latter case, a random action is played.</p>
<p>Note that this strategy only provides exploration after the backtracking fails to recover detections. Also, if agents start with no detection at time 0, the procedure is equivalent to Brownian motion.</p>
</sec>
<sec id="s5-5">
<title>Learned recovery</title>
<p>In this case recovery policy is learned by splitting the void state in several states labeled by the time since entry in the void state. In our experiments, we split the void state in 30 states. Actions are then learned as in all other non-void states and the optimal action is always chosen with (9).</p>
</sec>
<sec id="s5-6">
<title>Training</title>
<p>An agent start at a random location within the odor plume at time 0. Its memory is initialized with the prior |<italic>M</italic><sub>0</sub>| odor detections at its initial location <italic>M</italic><sub>0</sub> = [<italic>z</italic><sub>-|<italic>M</italic><sub>0</sub>|</sub>, ∆, <italic>z</italic><sub>0</sub>], obtained from the fluid dynamics simulation. The Q-function <italic>Q</italic><sub>0</sub> is initialized with 0.6 for all actions and olfactory states. The first dataset of average intensities contains the first value <italic>X</italic><sub>0</sub> = {<italic>c</italic>(<italic>M</italic><sub>0</sub>)}. At every times step <italic>t</italic> &gt; 0, the agent gets an odor observation <italic>z<sub>t</sub></italic> from its new position and updates its memory including the new observation and removing the oldest and the olfactory state <italic>o<sub>t</sub></italic> is computed (as described in previous paragraphs). The dataset of average intensities is updated: <italic>X<sub>t</sub></italic> = <italic>X</italic><sub><italic>t</italic>-1</sub> ∪ {<italic>c</italic>(<italic>M<sub>t</sub></italic>)}. Exploration exploitation parameter ε<sub><italic>k</italic></sub> is scheduled as follow
<disp-formula id="FD18">
<alternatives>
<mml:math id="M18" display="block"><mml:msub><mml:mi>ε</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mtext>decay</mml:mtext></mml:mrow></mml:msub><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn18.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where, in our experiments, ℕ<sub>init</sub> = 0.99 and <italic>ℕ</italic><sub>decay</sub> = 0.0001. At every episode <italic>k</italic>, the Q-function is updated at every time step <italic>t</italic> as
<disp-formula id="FD19">
<alternatives>
<mml:math id="M19" display="block"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>Q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn19.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where <italic>R<sub>t</sub></italic> is the immediate reward received playing the action <italic>a<sub>t</sub></italic>. <italic>o<sub>t</sub></italic> and <italic>o</italic><sub><italic>t</italic>+1</sub> are the current and the next olfactory states and <italic>α<sub>k</sub></italic> is the learning rate at episode <italic>k</italic>. This is scheduled as
<disp-formula id="FD20">
<alternatives>
<mml:math id="M20" display="block"><mml:msub><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>decay</mml:mtext></mml:mrow></mml:msub><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math>
<graphic xlink:href="2404.17495v1_eqn20.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where, in our experiments, α<sub>init</sub> = 0.25 and α<sub>decay</sub> = 0.001. For the experiments, agents are trained in 100000 episodes and an horizon of 5000 steps. The agent velocity is set to 10 and the discount factor is γ = 0.9999.</p>
</sec>
<sec id="s5-7">
<title>Agents Evaluation</title>
<p>To evaluate the performance of the different agents, we consider four metrics: the cumulative reward <italic>G</italic> (which is the actual quantity that the algorithm optimizes for); normalized time (defined below); the fraction of success <italic>f</italic><sup>+</sup> and the value conditioned on success <italic>g</italic><sup>+</sup>. For a fixed position (<italic>i</italic>,<italic>j</italic>), we denote with τ<sub>min</sub>(<italic>i</italic>,<italic>j</italic>) the minimum number of steps required to reach the source region from (<italic>i</italic>,<italic>j</italic>) i.e. the length of the shortest path.</p>
<p>We define <italic>D</italic><sub>init</sub> the set of points in which the first observation is above the sensitivity threshold (valid points). For each initial position (<italic>i</italic>,<italic>j</italic>) ∈ <italic>D</italic><sub>init</sub>, let τ(<italic>i</italic>,<italic>j</italic>) be the duration of the path obtained by an agent to reach the source. Note that τ(<italic>i</italic>,<italic>j</italic>) is a random variable for the stochastic backtracking and Brownian recoveries, but it is deterministic for the learned strategy that has no random components. For each admissible location (<italic>i</italic>,<italic>j</italic>), we define four performance metrics:
<disp-formula id="FD21">
<alternatives>
<mml:math id="M21" display="block"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mtext>λ</mml:mtext><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mtext>λ</mml:mtext><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>f</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>   </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>success</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>reps</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>g</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>  </mml:mtext><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mtext>λ</mml:mtext><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mtext>success</mml:mtext></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>   </mml:mtext><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
<graphic xlink:href="2404.17495v1_eqn21.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where <italic>n</italic><sub>reps</sub> is the number of test trajectories from each admissible location, and we use <italic>n</italic><sub>reps</sub> = 10. We then compute statistics of the perfomance metrics over the <italic>Dinit</italic> initial positions and report the average <inline-formula id="ID19">
<alternatives>
<mml:math display="inline" id="I19"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mo>·</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<inline-graphic xlink:href="2404.17495v1_ieq19.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</inline-formula> and standard deviation (std). Note that for the learned strategy, τ(<italic>i</italic>,<italic>j</italic>) is deterministic, hence <italic>f</italic><sup>+</sup>(<italic>i</italic>,<italic>j</italic>) is 0 or 1 and therefore we omit its standard deviation.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H C</given-names> <surname>Berg</surname></string-name></person-group>. <article-title>Chemotaxis in bacteria</article-title>. <source>Annual Review of Biophysics and Bioengineering</source>, <volume>4</volume>(<issue>1</issue>):<fpage>119</fpage>—<lpage>136</lpage>, <year>1975</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Murlis</surname></string-name>, <string-name><given-names>J. S.</given-names> <surname>Elkinton</surname></string-name>, and <string-name><given-names>R. T.</given-names> <surname>Carde</surname></string-name></person-group>. <article-title>Odor plumes and how insects use them</article-title>. <source>Annual Review of Entomology</source>, <volume>37</volume>:<fpage>505</fpage>, <year>1992</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Vergassola</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Villermaux</surname></string-name>, and <string-name><given-names>B.I.</given-names> <surname>Shraiman</surname></string-name></person-group>. <article-title>‘Infotaxis’ as a strategy for searching without gradients</article-title>. <source>Nature</source>, <volume>445</volume>:<fpage>406</fpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Shraiman</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Siggia</surname></string-name></person-group>. <article-title>Scalar turbulence</article-title>. <source>Nature</source>, <volume>405</volume>:<fpage>639</fpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E</given-names> <surname>Balkovsky</surname></string-name> and <string-name><surname>Shraiman</surname> <given-names>B. I.</given-names></string-name></person-group> <article-title>Olfactory search at high reynolds number</article-title>. <source>Proc Nat Acad Sci</source>, <volume>99</volume>(<issue>20</issue>):<fpage>12589</fpage>–<lpage>93</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gautam</given-names> <surname>Reddy</surname></string-name>, <string-name><given-names>Venkatesh N.</given-names> <surname>Murthy</surname></string-name>, and <string-name><given-names>Massimo</given-names> <surname>Vergassola</surname></string-name></person-group>. <article-title>Olfactory sensing and navigation in turbulent environments</article-title>. <source>Annual Review of Condensed Matter Physics</source>, <volume>13</volume>(<issue>1</issue>):<fpage>191</fpage>–<lpage>213</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. T.</given-names> <surname>Carde</surname></string-name></person-group>. <article-title>Navigation along windborne Ipumes of pheromone and resource-linked odors</article-title>. <source>Annual Review of Entomology</source>, <volume>66</volume>:<fpage>317</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Schal</surname></string-name></person-group>. <article-title>Intraspecific vertical stratification as a mate-finding mechanism in tropical cockroaches</article-title>. <source>Science</source>, <volume>215</volume>:<fpage>1505</fpage>, <year>1982</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>DH.</given-names> <surname>Gire</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Kapoor</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Arrighi-Allisan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Seminara</surname></string-name>, and <string-name><given-names>VN.</given-names> <surname>Murthy</surname></string-name></person-group>. <article-title>Mice develop efficient strategies for foraging and navigation using complex natural stimuli</article-title>. <source>Curr Biol</source>, <volume>26</volume>:<fpage>1261</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. L.</given-names> <surname>Baker</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Dickinson</surname></string-name>, <string-name><given-names>T. M.</given-names> <surname>Findley</surname></string-name>, <string-name><given-names>D. H.</given-names> <surname>Gire</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Louis</surname></string-name>, <string-name><given-names>M. P.</given-names> <surname>Suver</surname></string-name>, <string-name><given-names>J. V.</given-names> <surname>Verhagen</surname></string-name>, <string-name><given-names>K. I.</given-names> <surname>Nagel</surname></string-name>, and <string-name><given-names>M. C.</given-names> <surname>Smear</surname></string-name></person-group>. <article-title>Algorithms for olfactory search across species</article-title>. <source>Journal of Neuroscience</source>, <volume>38</volume>:<fpage>9383</fpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Celani</surname></string-name> and <string-name><given-names>E.</given-names> <surname>Panizon</surname></string-name></person-group>. <article-title>Olfactory search</article-title>. <comment>In review</comment>, <year>2024</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Baker</surname><given-names>T. C.</given-names></string-name></person-group> <article-title>Upwind flight and casting flight: complementary and tonic systems used for location of sex pheromone sources by male moths</article-title>. <conf-name>Proc. 10<sup>th</sup> Intl Symposium on Olfaction and Taste</conf-name>, <volume>13</volume>:<fpage>18</fpage>, <year>1990</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Kramer</surname></string-name></person-group>. <article-title>A tentative intercausal nexus and its computer model on insect orientation in windborne pheromone plumes</article-title>. In <source>Insect Pher. Res, New Dir</source>.,.:<fpage>232</fpage>, <year>1997</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>J.H.</given-names> <surname>Belanger</surname></string-name> and <string-name><given-names>M.A.</given-names> <surname>Willis</surname></string-name></person-group>. <article-title>Biologically-inspired search algorithms for locating unseen odor sources</article-title>. In, <conf-name>Proc. IEEE Symp. Intell. Control (ISIC ‘98) and IEEE Symp. Comp. Intell. Robot. Autom. (CIRA ‘98)</conf-name>, <fpage>265</fpage>, <year>1988</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J</given-names> <surname>Atema</surname></string-name></person-group>. <article-title>Eddy chemotaxis and odor landscapes: exploration of nature with animal sensors</article-title>. <source>Biol. Bull</source>., <volume>191</volume>:<fpage>129</fpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>BT</given-names> <surname>Michaelis</surname></string-name>, <string-name><given-names>KW</given-names> <surname>Leathers</surname></string-name>, <string-name><given-names>YV</given-names> <surname>Bobkov</surname></string-name>, <string-name><given-names>BW</given-names> <surname>Ache</surname></string-name>, <string-name><given-names>JC</given-names> <surname>Principe</surname></string-name>, <string-name><given-names>R</given-names> <surname>Baharloo</surname></string-name>, <string-name><given-names>IM</given-names> <surname>Park</surname></string-name>, and <string-name><given-names>MA</given-names> <surname>Reidenbach</surname></string-name></person-group>. <article-title>Odor tracking in aquatic organisms: the importance of temporal and spatial intermittency of the turbulent plume</article-title>. <source>Sci. Rep</source>., <volume>10</volume>:<fpage>7961</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mahmut</given-names> <surname>Demir</surname></string-name>, <string-name><given-names>Nirag</given-names> <surname>Kadakia</surname></string-name>, <string-name><given-names>Hope D</given-names> <surname>Anderson</surname></string-name>, <string-name><given-names>Damon A</given-names> <surname>Clark</surname></string-name>, and <string-name><given-names>Thierry</given-names> <surname>Emonet</surname></string-name></person-group>. <article-title>Walking <italic>Drosophila</italic> navigate complex plumes using stochastic decisions biased by the timing of odor encounters</article-title>. <source>eLife</source>, <volume>9</volume>:<elocation-id>e57524</elocation-id>, <year>2020</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Sutton</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Barto</surname></string-name></person-group>. <source>Reinforcement Learning: An Introduction</source>. <publisher-name>MIT Press</publisher-name>, <year>1998</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>A. R.</given-names> <surname>Cassandra</surname></string-name>, <string-name><given-names>L. P.</given-names> <surname>Kaelbling</surname></string-name>, and <string-name><given-names>J. A.</given-names> <surname>Kurien</surname></string-name></person-group>. <article-title>Acting under uncertainty: Discrete bayesian models for mobile-robot navigation</article-title>. <conf-name>Proc IEEE/RSJ Internl Conf Intelligent Robots and Systems. IROS ‘96</conf-name>, <volume>2</volume>:<fpage>963</fpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Steven M</given-names> <surname>LaValle</surname></string-name></person-group>. <source>Planning algorithms</source>. <publisher-name>Cambridge University Press</publisher-name>, <year>2006</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Loisy</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Eloy</surname></string-name></person-group>. <article-title>Searching for a source without gradients, how good is infotaxis and how to beat it</article-title>. <source>Proc. R. Soc. A</source>, <volume>478</volume>:<fpage>20220118</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names> <surname>Ishida</surname></string-name></person-group>. <article-title>Chemical sensing in robotic applications: a review</article-title>. <source>IEEE Sensors, J.</source>, <volume>12</volume>:<fpage>3163</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>V</given-names> <surname>Krishnamurthy</surname></string-name></person-group>. <source>Partially Observed Markov Decision Processes</source>. <publisher-name>Cambridge University Press</publisher-name>, <year>2016</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M</given-names> <surname>Hauskrecht</surname></string-name></person-group>. <article-title>Value-function approximations for partially observable markov decision processes</article-title>. <source>J. Artif. Intell. Res</source>., <volume>13</volume>:<fpage>33</fpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G</given-names> <surname>Shani</surname></string-name>, <string-name><given-names>J</given-names> <surname>Pineau</surname></string-name>, and <string-name><given-names>R</given-names> <surname>Kaplow</surname></string-name></person-group>. <article-title>A survey of point-based pomdp solvers</article-title>. <source>Autonomous Agents and MultiAgent Systems</source>, <volume>27</volume>:<fpage>1</fpage>–<lpage>51</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nicola</given-names> <surname>Rigolli</surname></string-name>, <string-name><given-names>Gautam</given-names> <surname>Reddy</surname></string-name>, <string-name><given-names>Agnese</given-names> <surname>Seminara</surname></string-name>, and <string-name><given-names>Massimo</given-names> <surname>Vergassola</surname></string-name></person-group>. <article-title>Alternation emerges as a multi-modal strategy for turbulent odor navigation</article-title>. <source>eLife</source>, <volume>11</volume>:<elocation-id>e76989</elocation-id>, <month>aug</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. A.</given-names> <surname>Heinonen</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Biferale</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Celani</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Vergassola</surname></string-name></person-group>. <article-title>Optimal policies for bayesian olfactory search in turbulent flows</article-title>. <source>Phys. Rev. E</source>, <volume>107</volume>:<fpage>055105</fpage>, <month>May</month> <year>2023</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aurore</given-names> <surname>Loisy</surname></string-name> and <string-name><given-names>Robin A.</given-names> <surname>Heinonen</surname></string-name></person-group>. <article-title>Deep reinforcement learning for the olfactory search pomdp: a quantitative benchmark</article-title>. <source>Cereb CortexThe European Physical Journal E</source>, <volume>46</volume>:<fpage>17</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Satpreet H.</given-names> <surname>Singh</surname></string-name>, <string-name><given-names>Floris van</given-names> <surname>Breugel</surname></string-name>, <string-name><given-names>Rajesh P. N.</given-names> <surname>Rao</surname></string-name>, and <string-name><given-names>Bingni W.</given-names> <surname>Brunton</surname></string-name></person-group>. <article-title>Emergent behaviour and neural dynamics in artificial agents tracking odour plumes</article-title>. <source>Nature Machine Intelligence</source>, <volume>5</volume>:<fpage>5870</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. V. B.</given-names> <surname>Verano</surname></string-name>, <string-name><given-names>E</given-names> <surname>Panizon</surname></string-name>, and <string-name><given-names>A</given-names> <surname>Celani</surname></string-name></person-group>. <article-title>Olfactory search with finite-state controllers</article-title>. <source>Proc Nat Acad Sci</source>, <volume>120</volume>(<issue>34</issue>):<elocation-id>e2304230120</elocation-id>, <year>2023</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Falkovich</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Gawedzki</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Vergassola</surname></string-name></person-group>. <article-title>Particles and fields in fluid turbulence</article-title>. <source>Rev. Mod. Phys</source>., <volume>73</volume>:<fpage>913</fpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Celani</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Villermaux</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Vergassola</surname></string-name></person-group>. <article-title>Odor landscapes in turbulent environments</article-title>. <source>Phys. Rev. X</source>, <volume>4</volume>:<fpage>041015</fpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nicola</given-names> <surname>Rigolli</surname></string-name>, <string-name><given-names>Nicodemo</given-names> <surname>Magnoli</surname></string-name>, <string-name><given-names>Lorenzo</given-names> <surname>Rosasco</surname></string-name>, and <string-name><given-names>Agnese</given-names> <surname>Seminara</surname></string-name></person-group>. <article-title>Learning to predict target location with turbulent odor plumes</article-title>. <source>eLife</source>, <volume>11</volume>:<elocation-id>e72196</elocation-id>, <month>aug</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ariane S.</given-names> <surname>Etienne</surname></string-name> and <string-name><given-names>Kathryn J.</given-names> <surname>Jeffery</surname></string-name></person-group>. <article-title>Path integration in mammals</article-title>. <source>Hippocampus</source>, <volume>14</volume>(<issue>2</issue>):<fpage>180</fpage>–<lpage>192</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ariane S.</given-names> <surname>Etienne</surname></string-name>, <string-name><given-names>Roland</given-names> <surname>Maurer</surname></string-name>, and <string-name><given-names>Valerie</given-names> <surname>Seguinot</surname></string-name></person-group>. <article-title>Path Integration in Mammals and its Interaction With Visual Landmarks</article-title>. <source>Journal of Experimental Biology</source>, <volume>199</volume>(<issue>1</issue>):<fpage>201</fpage>–<lpage>209</lpage>, <fpage>01</fpage> <year>1996</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Heinze</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Narendra</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Cheung</surname></string-name></person-group>. <article-title>Principles of insect path integration</article-title>. <source>Current Biology</source>, <volume>28</volume>:<fpage>R1043</fpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. T.</given-names> <surname>David</surname></string-name>, <string-name><given-names>J. S.</given-names> <surname>Kennedy</surname></string-name>, and <string-name><given-names>A. R.</given-names> <surname>Ludlow</surname></string-name></person-group>. <article-title>Finding of a sex pheromone source by gypsy moths released in the field</article-title>. <source>Nature</source>, <volume>303</volume>:<fpage>804</fpage>–<lpage>806</lpage>, <year>1983</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Loisy</surname></string-name> and <string-name><given-names>R.A.</given-names> <surname>Heinonen</surname></string-name></person-group>. <article-title>Deep reinforcement learning for the olfactory search pomdp: a quantitative benchmark</article-title>. <source>European Physical Journal E</source>, <volume>46</volume>:<fpage>17</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robin A.</given-names> <surname>Heinonen</surname></string-name> <string-name><given-names>L. P. S.</given-names> <surname>Kuenen</surname></string-name> and <string-name><given-names>R. T.</given-names> <surname>Carde</surname></string-name></person-group>. <article-title>Strategies for recontacting a los pheromone plume: casting and upwind flight in the male gypsy moth</article-title>. <source>Physiological Entomology</source>, <volume>15</volume>:<fpage>317</fpage>, <year>1994</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van</surname> <given-names>Breugel F</given-names></string-name> and <string-name><surname>Dickinson</surname> <given-names>MH</given-names></string-name></person-group>. <article-title>Plume-tracking behavior of flying drosophila emerges from a set of distinct sensory-motor reflexes</article-title>. <source>Curr Biol</source>, <volume>24</volume>:<fpage>274</fpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mahmut</given-names> <surname>Demir</surname></string-name>, <string-name><given-names>Nirag</given-names> <surname>Kadakia</surname></string-name>, <string-name><given-names>Hope D</given-names> <surname>Anderson</surname></string-name>, <string-name><given-names>Damon A</given-names> <surname>Clark</surname></string-name>, and <string-name><given-names>Thierry</given-names> <surname>Emonet</surname></string-name></person-group>. <article-title>Walking <italic>Drosophila</italic> navigate complex plumes using stochastic decisions biased by the timing of odor encounters</article-title>. <source>eLife</source>, <volume>9</volume>:<elocation-id>e57524</elocation-id>, <month>nov</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ruben</given-names> <surname>Gepner</surname></string-name>, <string-name><given-names>Mirna Mihovilovic</given-names> <surname>Skanata</surname></string-name>, <string-name><given-names>Natalie M</given-names> <surname>Bernat</surname></string-name>, <string-name><given-names>Margarita</given-names> <surname>Kaplow</surname></string-name>, and <string-name><given-names>Marc</given-names> <surname>Gershow</surname></string-name></person-group>. <article-title>Computations underlying <italic>Drosophila</italic> phototaxis, odor-taxis, and multi-sensory integration</article-title>. <source>eLife</source>, <volume>4</volume>:<elocation-id>e06229</elocation-id>, <month>may</month> <year>2015</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Luis</given-names> <surname>Hernandez-Nunez</surname></string-name>, <string-name><given-names>Jonas</given-names> <surname>Belina</surname></string-name>, <string-name><given-names>Mason</given-names> <surname>Klein</surname></string-name>, <string-name><given-names>Guangwei</given-names> <surname>Si</surname></string-name>, <string-name><given-names>Lindsey</given-names> <surname>Claus</surname></string-name>, <string-name><given-names>John R</given-names> <surname>Carlson</surname></string-name>, and <string-name><given-names>Aravinthan DT</given-names> <surname>Samuel</surname></string-name></person-group>. <article-title>Reverse-correlation analysis of navigation dynamics in <italic>Drosophila</italic> larva using optogenetics</article-title>. <source>eLife</source>, <volume>4</volume>:<elocation-id>e06225</elocation-id>, <month>may</month> <year>2015</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrew M. M.</given-names> <surname>Matheson</surname></string-name>, <string-name><given-names>Aaron J.</given-names> <surname>Lanz</surname></string-name>, <string-name><given-names>Ashley M.</given-names> <surname>Medina</surname></string-name>, <string-name><given-names>Al M.</given-names> <surname>Licata</surname></string-name>, <string-name><given-names>Timothy A.</given-names> <surname>Currier</surname></string-name>, <string-name><given-names>Mubarak H.</given-names> <surname>Syed</surname></string-name>, and <string-name><given-names>Katherine I.</given-names> <surname>Nagel</surname></string-name></person-group>. <article-title>A neural circuit for wind-guided olfactory navigation</article-title>. <source>Nature Communications</source>, <volume>13</volume>:<fpage>4613</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>S. D. David</given-names> <surname>Stupski</surname></string-name> and <string-name><given-names>F. van</given-names> <surname>Breugel</surname></string-name></person-group>. <article-title>Wind gates search states in free flight</article-title>. <source>bioRxiv</source>, doi.org/<pub-id pub-id-type="doi">10.1101/2023.11.30.569086</pub-id>:<fpage>1</fpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Brad A.</given-names> <surname>Radvansky</surname></string-name> and <string-name><given-names>Daniel A.</given-names> <surname>Dombeck</surname></string-name></person-group>. <article-title>An olfactory virtual reality system for mice</article-title>. <source>Nature Communications</source>, <volume>9</volume>:<fpage>839</fpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.V.</given-names> <surname>Bobkov</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Ache</surname></string-name></person-group>. <article-title>Intrinsically bursting olfactory receptor neurons</article-title>. <source>J. Neurophysiol</source>, <volume>97</volume>:<fpage>1052</fpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.W.</given-names> <surname>Ache</surname></string-name>, <string-name><given-names>A.M.</given-names> <surname>Hein</surname></string-name>, <string-name><given-names>Y.V.</given-names> <surname>Bobkov</surname></string-name>, and <string-name><given-names>J.C.</given-names> <surname>Principe</surname></string-name></person-group>. <article-title>Smelling time: A neural basis for olfactory scene analysis</article-title>. <source>Trends Neurosci</source>., <volume>39</volume>:<fpage>649</fpage>–<lpage>655</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ryan M.</given-names> <surname>Carey</surname></string-name>, <string-name><given-names>Justus V.</given-names> <surname>Verhagen</surname></string-name>, <string-name><given-names>Daniel W.</given-names> <surname>Wesson</surname></string-name>, <string-name><given-names>Nicolas</given-names> <surname>Pirez</surname></string-name>, and <string-name><given-names>Matt</given-names> <surname>Wachowiak</surname></string-name></person-group>. <article-title>Temporal structure of receptor neuron input to the olfactory bulb imaged in behaving rats</article-title>. <source>Journal of Neurophysiology</source>, <volume>101</volume>(<issue>2</issue>):<fpage>1073</fpage>–<lpage>1088</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names> <surname>Ackels</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Erskine</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Dasgupta</surname></string-name>. <etal>et al.</etal></person-group> <article-title>Fast odour dynamics are encoded in the olfactory system and guide behaviour</article-title>. <source>Nature</source>, <volume>593</volume>:<fpage>558</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>CAH</given-names> <surname>Allard</surname></string-name>, <string-name><given-names>G</given-names> <surname>Kang</surname></string-name>, <string-name><given-names>JJ</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>WA</given-names> <surname>ValenciaMontoya</surname></string-name>, <string-name><given-names>RE</given-names> <surname>Hibbs</surname></string-name>, and <string-name><given-names>NW</given-names> <surname>Bellono</surname></string-name></person-group>. <article-title>Structural basis of sensory receptor evolution in octopus</article-title>. <source>Nature</source>, <volume>616</volume>:<fpage>373</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Save</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Nerad</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Poucet</surname></string-name></person-group>. <article-title>Contribution of multiple sensory information to place field stability in hippocampal place cells</article-title>. <source>Hippocampus</source>, <volume>10</volume>:<fpage>64</fpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S</given-names> <surname>Zhang</surname></string-name> and <string-name><surname>Manahan-Vaughan</surname> <given-names>D</given-names></string-name></person-group>. <article-title>Spatial olfactory learning contributes to place field formation in the hippocampus</article-title>. <source>Cereb Cortex</source>, <volume>25</volume>:<fpage>423</fpage>–<lpage>32</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francesco</given-names> <surname>Viola</surname></string-name>, <string-name><given-names>Valentina</given-names> <surname>Meschini</surname></string-name>, and <string-name><given-names>Roberto</given-names> <surname>Verzicco</surname></string-name></person-group>. <article-title>Fluid-structure-electrophysiology interaction (fsei) in the left-heart: a multi-way coupled computational model</article-title>. <source>European Journal of Mechanics-B/Fluids</source>, <volume>79</volume>:<fpage>212</fpage>–<lpage>232</lpage>, <year>2020</year>.</mixed-citation></ref>
</ref-list>
<sec id="s6">
<fig id="fig6" position="float" fig-type="figure">
<label>Supplementary Figure 1</label>
<caption><p>The role of temporal memory with Brownian recovery strategy (same as main <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Total cumulative reward (top left) and standard deviation (top right) as a function of memory showing an optimal memory <italic>T</italic>* = 3 for the Brownian agent. Other measures of performance with their standard deviations show the same optimal memory (bottom). The tradeoff between long and short memories discussed in the main text holds, but here exiting the plume is much more detrimental because regaining position within the plume by Brownian motion is much lenghtier.</p></caption>
<graphic xlink:href="2404.17495v1_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig7" position="float" fig-type="figure">
<label>Supplementary Figure 2</label>
<caption><p>All four measures of performance across all agents with fixed and adaptive memory and with adaptive memory for the three recovery strategies.</p></caption>
<graphic xlink:href="2404.17495v1_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig8" position="float" fig-type="figure">
<label>Supplementary Figure 3</label>
<caption><p>Optimal policies for different recovery strategies and adaptive memory. From left to right: results for backtracking (green), Brownian (red) and learnd (blue) recovery strategies. Top: probability that an agent in a given olfactory state is at a specific spatial location color-coded from yellow to blue. Rows and columns indicate the olfactory state; the void state is in the lower right corner. Arrows indicate the optimal action from that state. Bottom: Circles represent occupancy of each state, olfactory states are arranged as in the top panel. All statistics is computed over 43000 trajectories, starting from any location within the plume.</p></caption>
<graphic xlink:href="2404.17495v1_fig8.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102906.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study uses reinforcement learning to study how turbulent odor stimuli should be processed to yield successful navigation. They find that there is an optimal memory length over which an agent should ignore blanks in the odor to discriminate whether the agent is still inside the plume or outside of it, complementing recent studies using RNNs and finite state controllers to identify optimal strategies for navigating a turbulent plume. While the overall strength of evidence is <bold>convincing</bold>, better justification for using Brownian motion as a recovery strategy and the addition of accompanying code for reproducibility would add to this strength.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102906.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Overall I found the approach taken by the authors to be clear and convincing. It is striking that the conclusions are similar to those obtained in a recent study using a different computational approach (finite state controllers), and lend confidence to the conclusions about the existence of an optimal memory duration. There are a few points or questions that could be addressed in greater detail in a revision:</p>
<p>(1) Discussion of spatial encoding</p>
<p>The manuscript contrasts the approach taken here (reinforcement learning in a grid world) with strategies that involve a &quot;spatial map&quot; such as infotaxis. The authors note that their algorithm contains &quot;no spatial information.&quot; However, I wonder if further degrees of spatial encoding might be delineated to better facilitate comparisons with biological navigation algorithms. For example, the gridworld navigation algorithm seems to have an implicit allocentric representation, since movement can be in one of four allocentric directions (up, down, left, right). I assume this is how the agent learns to move upwind in the absence of an explicit wind direction signal. However, not all biological organisms likely have this allocentric representation. Can the agent learn the strategy without wind direction if it can only go left/right/forward/back/turn (in egocentric coordinates)? In discussing possible algorithms, and the features of this one, it might be helpful to distinguish</p>
<p>
(1) those that rely only on egocentric computations (run and tumble),</p>
<p>
(2) those that rely on a single direction cue such as wind direction,</p>
<p>
(3) those that rely on allocentric representations of direction, and</p>
<p>
(4) those that rely on a full spatial map of the environment.</p>
<p>(2) Recovery strategy on losing the plume</p>
<p>While the approach to encoding odor dynamics seems highly principled and reaches appealingly intuitive conclusions, the approach to modeling the recovery strategy seems to be more ad hoc. Early in the paper, the recovery strategy is defined to be path integration back to the point at which odor was lost, while later in the paper, the authors explore Brownian motion and a learned recovery based on multiple &quot;void&quot; states. Since the learned strategy works best, why not first consider learned strategies, and explore how lack of odor must be encoded or whether there is an optimal division of void states that leads to the best recovery strategies? Also, although the authors state that the learned recovery strategies resemble casting, only minimal data are shown to support this. A deeper statistical analysis of the learned recovery strategies would facilitate comparison to those observed in biology.</p>
<p>(3) Is there a minimal representation of odor for efficient navigation?</p>
<p>The authors suggest (line 280) that the number of olfactory states could potentially be reduced to reduce computational cost. This raises the question of whether there is a maximally efficient representation of odors and blanks sufficient for effective navigation. The authors choose to represent odor by 15 states that allow the agent to discriminate different spatial regimes of the stimulus, and later introduce additional void states that allow the agent to learn a recovery strategy. Can the number of states be reduced or does this lead to loss of performance? Does the optimal number of odor and void states depend on the spatial structure of the turbulence as explored in Figure 5?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102906.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors investigate the problem of olfactory search in turbulent environments using artificial agents trained using tabular Q-learning, a simple and interpretable reinforcement learning (RL) algorithm. The agents are trained solely on odor stimuli, without access to spatial information or prior knowledge about the odor plume's shape. This approach makes the emergent control strategy more biologically plausible for animals navigating exclusively using olfactory signals. The learned strategies show parallels to observed animal behaviors, such as upwind surging and crosswind casting. The approach generalizes well to different environments and effectively handles the intermittency of turbulent odors.</p>
<p>Strengths:</p>
<p>(1) The use of numerical simulations to generate realistic turbulent fluid dynamics sets this paper apart from studies that rely on idealized or static plumes.</p>
<p>(2) A key innovation is the introduction of a small set of interpretable olfactory states based on moving averages of odor intensity and sparsity, coupled with an adaptive temporal memory.</p>
<p>(3) The paper provides a thorough analysis of different recovery strategies when an agent loses the odor trail, offering insights into the trade-offs between various approaches.</p>
<p>(4) The authors provide a comprehensive performance analysis of their algorithm across a range of environments and recovery strategies, demonstrating the versatility of the approach.</p>
<p>(5) Finally, the authors list an interesting set of real-world experiments based on their findings, that might invite interest from experimentalists across multiple species.</p>
<p>Weaknesses:</p>
<p>(1) The inclusion of Brownian motion as a recovery strategy, seems odd since it doesn't closely match natural animal behavior, where circling (e.g. flies) or zigzagging (ants' &quot;sector search&quot;) could have been more realistic.</p>
<p>(2) Using tabular Q-learning is both a strength and a limitation. It's simple and interpretable, making it easier to analyze the learned strategies, but the discrete action space seems somewhat unnatural. In real-world biological systems, actions (like movement) are continuous rather than discrete. Additionally, the ground-frame actions may not map naturally to how animals navigate odor plumes (e.g. insects often navigate based on their own egocentric frame).</p>
<p>(3) The lack of accompanying code is a major drawback since nowadays open access to data and code is becoming a standard in computational research. Given that the turbulent fluid simulation is a key element that differentiates this paper, the absence of simulation and analysis code limits the study's reproducibility.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102906.1.sa3</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Rando</surname>
<given-names>Marco</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>James</surname>
<given-names>Martin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Verri</surname>
<given-names>Alessandro</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rosasco</surname>
<given-names>Lorenzo</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Seminara</surname>
<given-names>Agnese</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the Editor and Reviewers for their work on our manuscript, and are happy to receive their positive comments, as well as their questions and suggestions. We are currently revising the manuscript and are planning to de-emphasize Brownian recovery as a simple yet biologically irrelevant benchmark and include comparisons with other biologically inspired strategies suggested by the reviewers. As for sharing the code and data: we completely agree: dataset 1 is already public and we will share the other dataset as well as the code. In a nutshell, we will be addressing the referee’s suggestions as follows:</p>
<p>(1)   As Referee 1 points out, even if the algorithm does not require a map of space, the agent is still required to tell apart North, East, South and West relative to the wind direction which is implicitly assumed known. We will better clarify the spatial encoding required to implement these strategies.</p>
<p>(2)   Referee 1 remarks that the learned recovery strategy works best and suggests to give it a more prominent role and better characterize it. We agree that what is done in the void state is definitely key and more work is needed to understand it. In the revised manuscript, we are planning to further substantiate the statistics of the learned recovery by repeating training several times and comparing several trajectories. Note that this strategy is much more flexible than the others and could potentially mix aspects of recovery to aspects of exploitation: we defer a more in-depth analysis that disentangles these two aspects elsewhere.</p>
<p>(3)   Referee 1 asks whether an optimal, minimal representation of the olfactory states exists. Q learning defines the olfactory states prior to training and does not allow to systematically optimize odor representation for the task. Given the odor features, we can however discretize them in more or less olfactory states. We expect that decreasing the number of olfactory states provides less positional information and potentially degrades performance, although loss in performance may be overshadowed by noise or by efficient recovery. We are planning to re-train our model with a smaller numer of non-void states and will provide the comparison. The number of void states does not need further testing: we chose 50 void states because it matches the time agents typically remain in the void and indeed achieves very high performance (less than 50 void states results in no convergence and more than 50 introduces states that are rarely visited)</p>
<p>(4)   Both reviewers correctly remark that Brownian motion is not biologically relevant. We will make sure to further clarify that this is a rather simple --but biologically irrelevant-- benchmark. We are planning to include results with both circling and zigzaging as biologically inspired recovery strategies.</p>
<p>(5)   We agree with reviewer 2 that animal locomotion does not look like a series of discrete displacements on a checkerboard. However, to overcome this limitation, one has to first focus on a specific system to define actions in a way that best adheres to a species’ motor controls. Second, these actions are likely continuous, which makes reinforcement learning notoriously more complex. While we agree that more realistic models are definitely needed for a comparison with real systems, this remains outside the scope of the current work.</p>
<p>(6)   We agree with the referees and editor that it is important to publish the code and data alongside with the manuscript. It was already planned and we will make sure to share the links within the revised version of the manuscript.</p>
</body>
</sub-article>
</article>