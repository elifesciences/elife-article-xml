<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">100739</article-id>
<article-id pub-id-type="doi">10.7554/eLife.100739</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100739.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>DeePosit: an AI-based tool for detecting mouse urine and fecal depositions from thermal video clips of behavioral experiments</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-5543-1676</contrib-id>
<name>
<surname>Peles</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>davidpelz@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4176-1124</contrib-id>
<name>
<surname>Netser</surname>
<given-names>Shai</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ray</surname>
<given-names>Natalie</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-4177-5391</contrib-id>
<name>
<surname>Suliman</surname>
<given-names>Taghreed</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7618-0752</contrib-id>
<name>
<surname>Wagner</surname>
<given-names>Shlomo</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f009v59</institution-id><institution>Sagol Department of Neurobiology, Faculty of Natural Sciences, University of Haifa</institution></institution-wrap>, <city>Haifa</city>, <country country="IL">Israel</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Wassum</surname>
<given-names>Kate M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of California, Los Angeles</institution>
</institution-wrap>
<city>Los Angeles</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-09-02">
<day>02</day>
<month>09</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-04-25">
<day>25</day>
<month>04</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP100739</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-06-24">
<day>24</day>
<month>06</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-27">
<day>27</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.24.600419"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-09-02">
<day>02</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100739.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.100739.1.sa4">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100739.1.sa3">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100739.1.sa2">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100739.1.sa1">Reviewer #3 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.100739.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Peles et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Peles et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-100739-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>In many mammals, including rodents, social interactions are often accompanied by active urination (micturition), which is considered a mechanism for spatial scent marking. Urine and fecal deposits contain a variety of chemosensory signals that convey information about the individual’s identity, genetic strain, social rank, and physiological or hormonal state. Furthermore, scent marking has been shown to be influenced by the social context and by the individual’s internal state and experience. Therefore, analyzing scent-marking behavior during social interactions can provide valuable insight into the structure of mammalian social interactions in health and disease. However, conducting such analyses has been hindered by several technical challenges. For example, the widely used void spot assay lacks temporal resolution and is prone to artifacts, such as urine smearing. To solve these issues, recent studies employed thermal imaging for the spatio-temporal analysis of urination activity. However, this method involved manual analysis, which is time-consuming and susceptible to observer bias. Moreover, defecation activity was hardly analyzed by previous studies. In the present study, we integrate thermal imaging with an open-source algorithm based on a transformer-based video classifier for automatic detection and classification of urine and fecal deposits made by male and female mice during various social behavior assays. Our results reveal distinct dynamics of urination and defecation in a test-, strain- and sex-dependent manner, indicating two separate processes of scent marking in mice. We validate this algorithm, termed by us DeePosit, and show that its accuracy is comparable to that of a human annotator and that it is efficient in various setups and conditions. Thus, the method and tools introduced here enable efficient and unbiased automatic spatio-temporal analysis of scent marking behavior in the context of behavioral experiments in small rodents.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>1. We increased the training set size from 39 video clips to 97 video clips and the testing set size from 25 video clips to 60 video clips. The increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 in the current version. The F1 score for urine detection was improved from 0.79 to 0.88.
2. We evaluated the accuracy of the DeePosit algorithm in comparison to a second human annotator and found that the algorithm accuracy is comparable to human-level accuracy.
3. The additional test videos allowed us to test the consistency of the algorithm performance across gender, space, time, and experiment type (SP, SxP, and ESPs). We found consistent levels of performance across all categories (see Figure 3), suggesting that errors made by the algorithm are uniform across conditions, hence should not create any bias in the results.
4. We tested the algorithm performance on a second strain of mice (male C57BL/6) in a different environmental condition (white arena instead of a black one) and found that the algorithm achieves comparable accuracy, even though C57BL/6 mice and white arena were not included in the training set.
5. Analyzing urination and defecation dynamics in an additional strain of mice revealed interesting strain-specific features, as discussed in the revised manuscript.
6. Overall, we found DeePosit accuracy to be stable with no significant bias across stages of the experiment, types of the experiment, gender of the mice, strain of mice, and across experimental conditions.
7. We compared the performance of DeePosit to a classic object detection algorithm: YOLOv8. We trained YOLOv8 both on a single image input (YOLOv8 Gray) and on 3 image inputs representing a sequence of three time points around the deposition event (t): t+0, t+10, and t+30 seconds (YOLOv8 RGB). DeePosit achieved significantly better accuracy over both YOLOv8 alternatives.
8. As for the algorithm parameters, we tested the effect of the main parameter of the preliminary detection (the temperature threshold for the detection of a new blob) and found that a threshold of 1.6C gave the best accuracy and used this parameter for all of the experiments instead of 1.1C which was used in the original manuscript. It's worth mentioning that the performance is quite stable (mean F1 score of 0.88-0.89) for the thresholds between 1.1C and 3C.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>In many mammalian species, including rodents, social interactions are accompanied or followed by events of active urination, also known as micturition or voiding activity <xref ref-type="bibr" rid="c1">Arakawa et al. (2008)</xref>. Multiple studies have demonstrated that urine and fecal deposits comprise many chemosensory social signals that carry information about the individual, such as its species, sex, social rank and identity, as well as its reproductive and health conditions <xref ref-type="bibr" rid="c3">Bigiani et al. (2005)</xref>. These chemosensory signals include various metabolites, as well as many proteins, such as major urinary proteins <xref ref-type="bibr" rid="c4">Brennan (2004)</xref>. Thus, by depositing urine spots and feces in its environment, the individual also deposits social information, which may later be perceived by other individuals and modify their future social interactions with this individual <xref ref-type="bibr" rid="c12">Hurst and Beynon (2004)</xref>. In other words, the use of urine and fecal deposits allows individuals to advertise their availability to possible mates and communicate with other conspecifics. Moreover, in territorial species, urination is used to mark the territory of the individual, thus functioning as a spatio-social scent-marking activity <xref ref-type="bibr" rid="c5">Brennan and Kendrick (2006)</xref>. In rodents, urination was shown to be strongly influenced by the individual’s internal state, social rank, social context, and previous social experience <xref ref-type="bibr" rid="c9">Desjardins et al. (1973)</xref>; <xref ref-type="bibr" rid="c13">Hyun et al. (2021)</xref>. Therefore, monitoring urination activity can provide valuable information on the individual’s social behavior and internal state. Specifically, deficits in urine depositing may reflect atypical social behavior in rodent models of various diseases (see <xref ref-type="bibr" rid="c26">Wöhr et al. (2011)</xref> for example), hence may be used for testing potential treatments in such models.</p>
<p>Urination during a given task is traditionally analyzed via the void spot assay, which uses filter paper placed on the arena floor to analyze, after the end of the experiment, the spatial distribution of urine spots <xref ref-type="bibr" rid="c27">Wolff and Powell (1984)</xref>; <xref ref-type="bibr" rid="c11">Higuchi and Arakawa (2022)</xref>. However, this analysis usually lacks the temporal dimension, is distorted by urine smearing across the arena floor caused by the individual’s movement (see <xref rid="fig2" ref-type="fig">Figure 2d,e</xref>), and is limited in detecting overlapping urine spots. Another caveat is that the filter paper may be torn down by the mouse during the behavioral experiment. Recently, <xref ref-type="bibr" rid="c8">Dalghi et al. (2023)</xref> used a filter paper on the arena floor, UV light, several cameras, and a manual video annotation to analyze urination events. Several other studies <xref ref-type="bibr" rid="c24">Verstegen et al. (2020)</xref>; <xref ref-type="bibr" rid="c19">Miller et al. (2023a)</xref>) used thermal imaging via infrared (IR) camera for such analysis, as urine deposits are emitted while being in body temperature, hence can be seen in the thermal image. However, fecal deposits are also emitted in body temperature, making it difficult to distinguish between feces and small urine spots by thermal imaging alone. Moreover, these studies relied on manual analysis of thermal video clips, which made the analysis process time-consuming and subjected to observer bias. To cope with these limitations, we have developed an open-source computer vision-based software to automatically detect and classify deposited urine and feces from thermal video clips. Our detection and classification algorithm is based on a combination of a heuristic algorithm used for the preliminary detection of bright (warm) blobs in the thermal video clip and a trainable video classifier used to classify the preliminary detections as either urine, feces, or background (BG, i.e., not urine or feces). We demonstrate the efficiency of this tool by analyzing the temporal dynamics of urination and defecation activities in male and female CD1 (ICR) mice while performing three social behavior tests, and further validate the algorithm by testing it with male C57BL/6J mice. We found that urination and defecation activities show distinct dynamics across the various tests in a sex-, strain- and test-dependent manner.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Investigation time across sexes and tests in CD1 mice.</title>
<p>Each of the tests (SP, SxP, and ESPs) is comprised of a 15-minute habituation stage with empty chambers, followed by a 5-minute trial stage in which the stimuli are present in the chambers (<bold>a</bold>). The Setup row shows schematic representations of the arena for the (<bold>b</bold>) SP, (<bold>c</bold>) SxP, and (<bold>d</bold>) ESPs tests, while the Males and Females rows show the mean (±SEM) time dedicated by male (n=36, blue bars) and female (n=35, red bars) mice to investigate each stimulus during the various tests. The two leftmost bars in each panel show the total investigation time, while The two middle bars show the time spent on short (≤6 s) investigation bouts, and the two rightmost bars show the time spent on long (&gt;6 s) investigation bouts.</p></caption>
<graphic xlink:href="600419v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>The experimental setup and analysis method</title>
<p>The experimental setup (<bold>a</bold>) includes a visible light (VIS) camera, an infrared (IR) camera, and a blackbody set to 37°C. VIS (<bold>b</bold>) and IR (<bold>c</bold>) images that were captured at the same moment, a short time after a urine deposition, exemplify that, as the urine is still warm, it appears as highly contrasted blob in the IR image but not in the VIS one. Large urine spots, such as the one shown in (<bold>d</bold>), may be smeared across the arena’s floor (<bold>e</bold>), which is one limitation of the use of filter paper for quantifying urination at the end of the experiment. The preliminary detection algorithm is based on subtracting a background image from each frame in the video (<bold>f</bold>), which allows the detection of hot blobs reflecting the animal itself and urine and feces deposits. The detected blobs are then classified using a transformer-based artificial neural network (<bold>g</bold>), which gets as its input a time series of patches cropped around the detection and provides its classification as an output. Each three patches in that time series are merged into a single RGB image (see methods). In the confusion matrix presenting the accuracy of the full pipeline for test videos (<bold>h</bold>) in CD1 mice, the “Miss” row counts the events that were not detected by the preliminary hot blobs detection and, hence, were not fed to the classifier. The BG (background) column counts the number of automatic detections for which no matching manually tagged event exists in the relevant space and time window. See Methods for more details. The precision, recall, and F1 score for urine detection is 0.90,0.86,0.88 accordingly, and 0.91,0.89,0.90 for feces detection. The mean F1 score: (<italic>F</italic> 1<sub><italic>Urine</italic></sub> + <italic>F</italic> 1<sub><italic>Feces</italic></sub>)/2 is 0.89.</p>
<p><xref rid="fig2_S1" ref-type="fig">Figure 2—figure supplement 1</xref><bold>.Accuracy for small and large detections in CD1 mice</bold>.</p>
<p><xref rid="d1e1894" ref-type="supplementary-material">Figure 2—video 1</xref><bold>Video for the events in the confusion matrix</bold>. Each urine or feces event is shown in a 65×65 pixel window from −11 seconds before the event to +60 seconds afterward (similar to the classifier input). The video shows both the manual annotation and the automatic detection that was matched with it (side by side). Note that there are no automatic detections for “Miss” and no manual annotation for “BG”. The video plays at X3 speed.</p></caption>
<graphic xlink:href="600419v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2">
<title>Methods and Materials</title>
<sec id="s2a">
<title>Animals</title>
<p>Subject animals were adult (12-14 weeks old) male and female wild-type offspring derived from breeding couples of Gtf2i+/Dup with a CD1 (ICR) genetic background mice <xref ref-type="bibr" rid="c18">Mervis et al. (2012)</xref>, bred and grown in the SPF mouse facility of the University of Haifa or C57BL/6 mice purchased from Envigo (Rehovot, Israel). Stimulus animals were adult (12-14 weeks old) male and female CD1 or C57BL/6 mice purchased from Envigo (Rehovot, Israel). All mice were housed in groups of 3-5 in a dark/light 12-hour cycle (lights on at 7 pm), with <italic>ad libitum</italic> food and water under veterinary inspection. Experiments were performed in the dark phase of the dark/light cycle. All experiments were approved by the University of Haifa ethics committee (Reference #: UoH-IL-2301-103-4).</p>
</sec>
<sec id="s2b">
<title>Setup and Video Acquisition</title>
<p>The experimental setup is based on the setup described in <xref ref-type="bibr" rid="c22">Netser et al. (2019)</xref>. Briefly, a black or white Plexiglass box arena (37 cm x 22 cm x 35 cm) was placed in a sound-attenuated chamber. A visible light (VIS) camera (both Flea3 and Grasshopper3 models manufactured by Teledyne FLIR were used, both with a wide-angle lens, rate of 30 frames per second, and USB3 interface) and a long wave infrared (IR) camera (Opgal’s Thermapp MD with 6.8 mm lens, 384×288 pixels at a rate of 8.66 frames per second (FPS)) were placed about 70 cm above the arena’s floor. The IR camera was designed to measure human skin temperature and outputs the apparent temperature for each pixel. Raw pixel values were converted to Celsius degrees using the formula supplied by the manufacturer. We acquired the camera videos using custom-made Python software (code is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit">https://github.com/davidpl2/DeePosit</ext-link> and at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/14754159">https://zenodo.org/records/14754159</ext-link>) that used the manufacturer’s SDK (SDK version: EyeR-op-SDK-x86-64-2.15.915.8688-MD). To improve the accuracy of and reduce possible drifts in the measured temperature, a high-emissivity blackbody (Nightingale BTR-03 blackbody by Santa Barbara Infrared, Inc.) was placed in the camera’s field of view and was set to 37°C. During analysis, the offset between the blackbody apparent temperature and 37°C was subtracted from the image. To improve image quality, we turned on the camera at least 15 min before the beginning of the experiment (this allows the camera’s temperature to get stable). In addition, to reduce pixel non-uniformity, we captured 16 frames of a uniform surface (a piece of cardboard placed in front of the camera) before each test. These images were then averaged, and the average image’s mean was subtracted from it to get a non-uniformity image with zero mean. The non-uniformity image was then subtracted from each image in the video to achieve better pixel uniformity.</p>
</sec>
<sec id="s2c">
<title>Social Behavior tests</title>
<p>We used three distinct social discrimination tests, as previously described in <xref ref-type="bibr" rid="c21">Mohapatra et al. (2024)</xref>. Briefly, all tests consisted of 15 min of habituation, during which the subject mouse got used to the arena with empty triangular chambers (12 cm isosceles, 35 cm height) located at randomly chosen opposite corners. Each triangular chamber had a metal mesh (18 mm x 6 cm; 1 cm x 1 cm holes) at its bottom, through which subject mice could interact with the stimuli. After habituation, the empty chambers were removed and new stimuli-containing chambers were introduced into the arena for the 5-minute trial. In the Social Preference (SP) test, a novel (i.e., unfamiliar to the subject mouse) sex-matched stimulus mouse was placed in one chamber, whereas an object stimulus (a Lego toy) was placed in the opposite chamber. In the Sex Preference (SxP) test, a novel female mouse was placed in one chamber while a novel male was placed in the opposite chamber. In the ESPs test, a novel stressed (restrained in a 50 ml plastic tube for 15 minutes before the test) sex-matched mouse was introduced to one chamber of the arena while a novel naïve mouse was placed in the opposite chamber.</p>
</sec>
<sec id="s2d">
<title>Behavioral Analysis</title>
<p>VIS video clips were analyzed using TrackRodent (<ext-link ext-link-type="uri" xlink:href="https://github.com/shainetser/TrackRodent">https://github.com/shainetser/TrackRodent</ext-link>), as previously described in <xref ref-type="bibr" rid="c23">Netser et al. (2017)</xref></p>
</sec>
<sec id="s2e">
<title>Urine and Feces Detection Algorithm</title>
<p>The detection algorithm consists of two main parts. A preliminary heuristic detection algorithm detects warm blobs. These blobs are then fed into a machine learning-based classifier, which classifies them as either urine, feces, or background (i.e., no detection). The algorithm’s code is available here: <ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit">https://github.com/davidpl2/DeePosit</ext-link> and at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/14754159">https://zenodo.org/records/14754159</ext-link>.</p>
</sec>
<sec id="s2f">
<title>Manual Inputs</title>
<p>A graphical user interface (GUI) was developed in Matlab to support all of the required manual annotations. Each video went through a manual annotation of the arena’s floor, the area of the blackbody, and a specification of the first and last frames of both the habituation and trial periods. These two periods were separated by a ∼30-second period during which the stimuli were introduced to the arena, which was excluded from the analysis. Also, the arena side of each stimulus (for example, the male and female sides in the SxP test) was defined as the half of the arena close to this stimulus’s chamber. To generate the train and test sets, a human annotator manually tagged urine and fecal deposition events in videos of 157 experiments with CD1 mice, of which 97 were used for training and 60 for testing. A single click was used to mark the center of each urine or fecal deposit in the first frame where it was clearly visible. The training set included 751 urine annotations and 637 feces annotations. The test set included 438 urine annotations and 374 feces annotations. Additional details can be found in the software’s manual.</p>
</sec>
<sec id="s2g">
<title>Preliminary Detection of Hot Blobs</title>
<p>Urine and fecal deposits appear as hot (bright) blobs in the first seconds after deposition. After a cool-down period, which takes about 30-60 seconds for feces and small urine spots and up to ∼four minutes for large urine spots, feces and urine appear as dark spots in the thermal image. The preliminary detection relies on these effects (See pseudo-code in Algorithm 1 below). It uses image subtraction to search for hot blobs that appear in the video and cool down later. We generate a background image <italic>B</italic><sub><italic>i</italic></sub> for each frame <italic>F</italic><sub><italic>i</italic></sub> to detect new hot blobs. Subtraction of <italic>B</italic><sub><italic>i</italic></sub> from <italic>F</italic><sub><italic>i</italic></sub> generates an image in which the mouse pixels and new (warm) urine and feces pixels appear bright. We set <italic>B</italic><sub>0</sub> as the per-pixel minimum of the first 20 seconds of video (note that habituation and trial videos are analyzed separately to account for possible minor shifts in the arena’s position). We assume that the mouse is brighter than the arena’s floor and that the mouse moves during the first 20 seconds, so each pixel will get the arena’s floor value at least once during this time.</p>
<p>For <italic>i</italic> &gt; 0 we compute <italic>B</italic><sub><italic>i</italic></sub> as the minimum of images <italic>N</italic><sub><italic>j</italic></sub>, <italic>j</italic> ∈ [<italic>i</italic> − 44, .., <italic>i</italic> − 36] (this roughly matches time range [<italic>i</italic>−5<italic>sec</italic>, .., <italic>i</italic>−4<italic>sec</italic>]) where <italic>N</italic><sub><italic>j</italic></sub> is an image in which the mouse pixels were replaced by the last known values from before the time that the mouse occupied these pixels. We set <italic>N</italic><sub><italic>j</italic>&lt;=0</sub> = <italic>B</italic><sub>0</sub>.</p>
<p>To compute the mouse mask at frame i, <italic>B</italic><sub><italic>i</italic>−1</sub> is subtracted from <italic>F</italic><sub><italic>i</italic></sub>. The subtraction result is dilated by Matlab’s <italic>imdilate</italic> function with a structuring element of a disk of a radius of 2 pixels and then compared against a threshold of 1°C to get a binary mask of the pixels that are warmer than the arena’s floor. Connected regions are then computed using Matlab’s <italic>bwlabel</italic> function and the connected region with the largest intersection with the arena’s floor is considered as the mask of the mouse (denoted <italic>M</italic><sub><italic>i</italic></sub>).</p>
<p><italic>N</italic><sub><italic>i</italic></sub> is then computed by taking <italic>F</italic><sub><italic>i</italic></sub> values for the pixels outside <italic>M</italic><sub><italic>i</italic></sub> and taking the values of <italic>N</italic><sub><italic>i</italic>−1</sub> for the mouse containing pixels: <italic>N</italic><sub><italic>i</italic></sub> = <italic>N</italic><sub><italic>i</italic>−1</sub> * <italic>M</italic><sub><italic>i</italic></sub> +<italic>F</italic><sub><italic>i</italic></sub> * (1−<italic>M</italic><sub><italic>i</italic></sub>) where * denotes pixel-wise multiplication. The difference image <italic>D</italic><sub><italic>i</italic></sub> is computed by: <italic>D</italic><sub><italic>i</italic></sub> = <italic>F</italic><sub><italic>i</italic></sub> − <italic>max</italic>(<italic>T</italic> , <italic>B</italic><sub><italic>i</italic></sub>) where <italic>T</italic> is the arena’s floor median temperature, computed by <italic>T</italic> = <italic>median</italic>(<italic>B</italic><sub><italic>i</italic></sub>(<italic>AF</italic> &amp;¬<italic>M</italic><sub><italic>i</italic></sub>&amp;¬<italic>M</italic><sub><italic>i</italic>−1</sub>)) where <italic>AF</italic> is a mask of the arena’s floor, &amp; is pixel-wise AND operation and ¬ is pixel-wise NOT operations. Using T prevents higher detection sensitivity in darker regions of the arena floor (regions in the arena’s floor that are covered in cooled-down urine appear darker than dry regions of the arena’s floor, see <xref rid="fig2" ref-type="fig">Figure 2e</xref>).</p>
<p>The cooldown rate <italic>CD</italic><sub><italic>i</italic></sub> is computed by taking the per pixel minimum of the frames in the next 40 seconds following <italic>F</italic><sub><italic>i</italic></sub> and subtracting it from <italic>F</italic><sub><italic>i</italic></sub>.</p>
<p>The hot blobs mask <italic>BM</italic><sub><italic>i</italic></sub> is computed by taking the pixels for which <italic>D</italic><sub><italic>i</italic></sub> &gt; Δ<italic>T</italic><sub><italic>Threshold</italic></sub> and not included in <italic>M</italic><sub><italic>i</italic></sub> and <italic>M</italic><sub><italic>i</italic>−1</sub> and for which the <italic>CD</italic><sub><italic>i</italic></sub> &gt; 1.1°C and <italic>CD</italic><sub><italic>i</italic></sub> &gt; 0.5 * <italic>D</italic><sub><italic>i</italic></sub>. We explored several values for Δ<italic>T</italic><sub><italic>Threshold</italic></sub> (see <xref rid="fig3_S2" ref-type="fig">Figure 3—figure Supplement 2</xref>) and chose Δ<italic>T</italic><sub><italic>Threshold</italic></sub> = 1.6°C as the default value for this parameter. We ask for the cooldown to be at least half of the increase in the temperature but not more than that since very large urinations cool down slower and might take more than 40 seconds to cool down fully. We excluded pixels in <italic>M</italic><sub><italic>i</italic>−1</sub> (mouse containing pixels in frame i-1) and not just <italic>M</italic><sub><italic>i</italic></sub> since the IR sensor has a response time that might causes pixels included in <italic>M</italic><sub><italic>i</italic>−1</sub> to be slightly brighter.</p>
<p><italic>BM</italic><sub><italic>i</italic></sub> goes through a morphological close operation using Matlab’s <italic>imclose</italic> function with a structure element of a disk with a radius of 4 pixels. This causes any nearby drops of urine to unify to a single detection. Blobs that overlap pixels outside the arena’s floor or touch the mouse mask are ignored to avoid detection on darker areas of the mouse (mostly the tail), reflections from the arena’s wall, and detections due to a stimulus mouse which sometimes sticks his nose throughout the barrier net of the chamber. Also, blobs with a size &lt; 2 pixels or larger than 900 pixels are ignored (pixel size is roughly 0.02<italic>cm</italic><sup>2</sup>).</p>
<p>Blobs that intersect previously detected blobs are considered to be the same detection if no more than 30 seconds passed from the last frame in which the previous detection was last detected. A unified detection mask is computed each time a detection is associated with a previous detection. This allows reduction of false alarms which might be caused by the smearing of a still-hot urine drop. If no such intersection exists, a new preliminary detection is added to the list of detections. A blob should be detected in at least two frames to be included in the output detections. The selected frame ID for each blob is the frame that contains the maximum intensity for this blob out of all frames in which this blob was detected. The representative coordinates for each detected blob were chosen by taking the pixel with the maximum intensity inside the blob in the selected frame. Usually, the selected frame for each blob is the first frame of the detection (as the detection cools, the maximum intensity is usually in the first detected frame). Still, it might be another frame if the detection was partly occluded by the mouse tail or if a second urine event occurred in the same place during the relevant time frame. The output detections are fed into a classifier, which will be described next.</p>
<p>The detection threshold Δ<italic>T</italic><sub><italic>Threshold</italic></sub> is higher than the mouse detection threshold (1°C) to avoid false defections within the borders of the subject mouse body.</p>
<statement id="alg1">
<label>Algorithm 1</label>
<p>Preliminary Detection of Hot Blobs</p>
<p><fig id="alg1a" position="float" fig-type="figure">
<graphic xlink:href="600419v3_alg1.tif" mime-subtype="tiff" mimetype="image"/>
</fig></p>
</statement>
</sec>
<sec id="s2h">
<title>Classifying Preliminary Detections Using an Artificial Neural Network</title>
<p>Preliminary detections are fed to a trained artificial neural network classifier which classifies them as either: <italic>Urine, Feces</italic> or <italic>Background</italic> (<xref rid="fig2" ref-type="fig">Figure 2g</xref>). We relied on the transformer-based architecture proposed by <xref ref-type="bibr" rid="c6">Carion et al. (2020)</xref>. This architecture was designed for object detection in RGB images. It receives an RGB image as input and outputs a set of bounding boxes around each detected object and the classification of each detection. In brief, this neural network architecture consists of a convolutional neural network (CNN) based on the <italic>ResNet</italic> architecture proposed by <xref ref-type="bibr" rid="c10">He et al. (2016)</xref>, which serves as the backbone and extracts a set of feature vectors from each location in the input image. The feature vectors are attached with a position encoding, which is a second feature vector that describes the spatial location in the input image, associated with the backbone’s feature vector. For each spatial location, the feature vectors from the backbone and the positional encoding are summed and fed into an encoder transformer, which uses an attention mechanism to share information between the feature vectors from various spatial locations. A decoder block is fed with the output of the encoder, and an additional set of vectors is denoted as queries. The decoder uses several layers of self and cross-attention to share information between queries (self-attention) and between the queries and the decoder output (cross-attention). Finally, the encoder outputs a feature vector for each input query. This vector is fed into a feed-forward network (FFN) to compute each query’s bounding box and classification. One of the possible classification outputs for each query is “no object”. We relied on the popular open-source code published by <xref ref-type="bibr" rid="c6">Carion et al. (2020)</xref> and made a few adjustments. Instead of feeding a single RGB image as input, for each detection in <italic>F</italic><sub><italic>i</italic></sub> we used a series of 78 grayscale image patches cropped around the detection pixel (65×65 pixels patch) and representing a time window of about [-11sec .. 60sec] around the detection. For detection in <italic>F</italic><sub><italic>i</italic></sub> we used the frames [<italic>F</italic><sub><italic>i</italic>−12*8</sub>, <italic>F</italic><sub><italic>i</italic>−11*8</sub>, …<italic>F</italic><sub><italic>i</italic>−0*8</sub>, …, <italic>F</italic><sub><italic>i</italic>+65*8</sub>] for classification. We used this relatively large time window to capture the cooldown of the feces and urine, movement of feces (which are frequently moved by the mouse), or smearing of urine. Additionally, this time window allows for capturing the moment of the deposition of the urine or feces, which sometimes occurs a few seconds before the preliminary detection (since the mouse may fully or partly occlude the detection in the first seconds). In case one or more frames in this sequence are not available (Exceeds the time limits of the video), a uniform image with a temperature of 22°C was used instead. Each of the three consequent patches in this set was combined into a single RGB patch and was fed to the backbone. This allows the use of pre-trained backbone weights as well as reduced run-time in comparison to the option of feeding each patch separately to the backbone. Similarly to <xref ref-type="bibr" rid="c6">Carion et al. (2020)</xref>, each of the backbone’s output feature vectors was attached with a positional encoder. However, we adjusted the positional encoding to include additional information on the time of each feature vector (in addition to its spatial location). To do that, we computed time encoding in the same way it was computed by <xref ref-type="bibr" rid="c6">Carion et al. (2020)</xref> for encoding the x or y coordinate and concatenated it to the x,y position encoding vector. To keep the length of the joint position and time encoding the same, we added a fully connected trainable layer that gets the (x,y,t) embedding as input (dim = 128*3=384) and outputs a feature vector with dim=256 which allows using the rest of the neural network and pre-trained weights without additional changes. Lastly, instead of using 100 queries as in <xref ref-type="bibr" rid="c6">Carion et al. (2020)</xref>, we used just a single query to get just the classification of the input set of patches and disabled the computation of a bounding box. Since our training set is relatively small, we used transfer learning and initialized the learnable weights with the weights published by <xref ref-type="bibr" rid="c6">Carion et al. (2020)</xref> (weight file: detr-r50-dc5-f0fb7ef5.pth). We used the dc5 (dilated C5 stage) option proposed by <xref ref-type="bibr" rid="c6">Carion et al. (2020)</xref>, which increases the spatial resolution of the backbone’s output by a factor of 2 as it may be more suitable for classifying small objects, and used ResNet-50 as the backbone. We first trained the classifier using 39 train videos (Each video contains a single experiment and includes both the habituation and trial periods and is of length of roughly 20 minutes). A second round of training used the weights of the first round as initial weights and included an additional 58 training videos (a total of 97 training videos).</p>
<p>Training database generation included extraction of: a. Positive examples of urine and feces that were manually marked. b. forty negative examples (labeled as background) per video in randomly selected positions and time (half during habituation and half during trial) that are not close in space and time to any manual annotation. c. hard negative examples consist of preliminary detected blobs (detected by the heuristic detection algorithm) that are not close in space and time to any manual detection. For both types of negative examples, a negative example in position <italic>x</italic><sub><italic>d</italic></sub> and time <italic>t</italic><sub><italic>d</italic></sub> was considered to be close to a manual detection of position <italic>x</italic><sub><italic>m</italic></sub> in time <italic>t</italic><sub><italic>m</italic></sub> if <italic>distance</italic>(<italic>x</italic><sub><italic>d</italic></sub> , <italic>x</italic><sub><italic>m</italic></sub>) &lt; 25<italic>pixels</italic> and −10<italic>sec</italic> ≤ <italic>t</italic><sub><italic>d</italic></sub> − <italic>t</italic><sub><italic>m</italic></sub> ≤ 30<italic>sec</italic>. For the positive examples, we augmented the data by a time shift of [-3..6] sec, compensating for possible differences between the manual tagging and the preliminary detection time, as well as increasing the training set size. Data augmentation for all examples included a random spatial shift of +-2 pixels, random flip, and rotation of 90, 180, and 270 degrees. Input data was normalized to contain values between [0..255] using linear mapping that mapped 10°C to 0 and 40°C to 255. Values that exceeded 0 or 255 were trimmed. The first training round (39 training videos) was done for 230 epochs with a learning rate of 1e-5 for the backbone and 1e-4 for the rest of the weights and a factor 10 learning rate drop after 200 epochs. The second training round (97 training videos) was done for 50 epochs with a learning rate of 1e-5 for the backbone and 1e-4 for the rest of the weights and a factor 10 learning rate drop after 40 epochs.</p>
</sec>
<sec id="s2i">
<title>Accuracy Measurement</title>
<p>The accuracy of automatic detections was evaluated using the following principles: 1. Manually tagged urine or fecal deposition is considered correctly detected by the algorithm, if an automatic detection with the same label exists at a distance of up to 20 pixels (2.9 cm) and in a time difference of up to 15 seconds. Spatial tolerance is required due to inherent ambiguity in the manual urine tagging process, as different observers often mark large spots or long traces of urine differently (see <xref rid="fig2" ref-type="fig">Figure 2d</xref> for an example of such a trace). Specifically, the detection algorithm might unify adjacent urine spots, tagged as multiple urine depositions by human annotators (see for example <xref rid="d1e1894" ref-type="supplementary-material">Figure 2—video 1</xref> and <xref rid="fig3_S3" ref-type="fig">Figure 3—figure Supplement 3</xref>). Temporal tolerance is required as the mouse body may cover the deposit or be very close to it for a while, thus delaying the time the preliminary detection algorithm detects it. 2. In the case described in 1, all automatic detections in this time and space window that got a correct label by the algorithm as the manual tagging are not counted as false alarms. 3. In contrast, if only automatic detections carrying labels different from the manually tagged deposition exist in the relevant space and time around it, then the closest one will be associated with this manual annotation and will be counted as misclassification (i.e, urine that was classified as feces or BG and feces that was classified as urine or BG), while the others will be counted as false alarms (will be counted in the BG column of the confusion matrix).</p>
</sec>
<sec id="s2j">
<title>Comparison with a second human annotator</title>
<p>The task of detecting and correctly classifying urine and feces in thermal videos is also challenging for a human annotator. To assess the performance of the DeePosit algorithm and compare it to a human annotator, 25 test videos were manually annotated by a second human annotator that marked a polygon surrounding each feces or urine spot. The detections of the DeePosit algorithm and of the second human annotator were compared to the annotation of the first human annotator. See <xref rid="fig3" ref-type="fig">Figure 3f-g</xref>.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Validation of DeePosit accuracy</title>
<p>Accuracy of detecting urine (<bold>a</bold>) and fecal (<bold>b</bold>) deposits by DeePosit, as measured by F1 score across various stages of the experiment. Each “+” or “o” marks the F1 accuracy for a single mouse in a single experiment. No significant difference was found. Similarly, DeePosit accuracy was not significantly affected by the experiment type (<bold>c</bold>), by the sex of the subject mouse (<bold>d</bold>), or by the spatial location of the deposition in the arena (arena’s floor was divided into three equal parts) (<bold>e</bold>). (<bold>a, b, c, e</bold>) are FDR corrected Rank sum tests <xref ref-type="bibr" rid="c2">Benjamini and Hochberg (1995)</xref>. The # at (<bold>b</bold>) stands for FDR corrected p-value of 0.08. Since differentiating small urine and feces in thermal videos can be a challenging task even for humans, we evaluated the accuracy of a second human annotator on 25 test videos of CD1 mice (a subset of the full test set) and reported both the accuracy achieved by DeePosit (<bold>f</bold>) and the second human annotator (<bold>g</bold>) on these test videos. The mean F1 score, (<italic>F</italic> 1<sub><italic>Urine</italic></sub> + <italic>F</italic> 1<sub><italic>Feces</italic></sub>)/2 is 0.86 for the second human annotator and 0.84 for the DeePosit algorithm. To compare our result with another popular object detection approach, we annotated 39 training videos of CD1 mice with bounding boxes to match the YOLOv8 framework. For fairness, we trained both algorithms on the same training set of videos. (<bold>h</bold>) shows the confusion matrix for DeePosit, while (<bold>i</bold>,<bold>j</bold>) Show the confusion matrices achieved using YOLOv8 with a single image as input (YOLOv8 Gray) and with 3 images as input representing time t+0, t+10, t+30 seconds from each event (YOLOv8 RGB). DeePosit accuracy surpasses YOLOv8 results in both cases. YOLOv8 RGB accuracy surpasses YOLOv8 Gray, suggesting that temporal information is helpful in the detection of urine and feces.</p>
<p><xref rid="fig3_S1" ref-type="fig">Figure 3—figure supplement 1</xref><bold>. Accuracy for small and large detections in C57BL/6 mice</bold>.</p>
<p><xref rid="fig3_S2" ref-type="fig">Figure 3—figure supplement 2</xref><bold>. Detection accuracy at various values of</bold> Δ<italic>T</italic><sub><italic>Threshold</italic></sub></p>
<p><xref rid="fig3_S3" ref-type="fig">Figure 3—figure supplement 3</xref><bold>. Examples of detections in test videos</bold>.</p></caption>
<graphic xlink:href="600419v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2k">
<title>Comparison with YOLOv8 Object Detector</title>
<p>We compared our algorithm with a YOLOv8 <xref ref-type="bibr" rid="c14">Jocher et al. (2023)</xref> based algorithm (YOLOv8n architecture). We trained YOLOv8 on 39 thermal video clips that were manually tagged with bounding boxes around each feces or urine spot. An additional 25 videos were annotated with bounding boxes for validation. OpenLabeling annotation tool was used for bounding boxes annotation (<xref ref-type="bibr" rid="c7">Cartucho et al. (2018)</xref>). The training was done for 10,000 epochs with default parameters. Weights were initialized with YOLOv8n.pt pre-trained weight file, which was published by <xref ref-type="bibr" rid="c14">Jocher et al. (2023)</xref>. Output weight file with the best accuracy on the validation videos was chosen. As YOLOv8 expects the pixel values to be between 0 and 255, temperatures between 10°C and 40°C were linearly mapped to values between 0 and 255. As YOLOv8 is designed for 3-channel RGB images, we compared two training approaches. The first approach (termed YOLOv8 gray) used the same thermal image for the R, G, and B channels. The second approach used three thermal images from time t, t+10 seconds, and t+30 seconds, where t is the time of the deposition tagging, and fed them to the YOLOv8 classifier as the R, G, and B channels. This gives the classifier relevant temporal information that might capture the cool-down process, smearing of urine or shift of feces. Training examples included all frames in which a manual detection was labeled. Bounding boxes were annotated around all warm and clearly visible urine or feces in each of these frames (including old urine and feces that are still warm and clearly visible). In addition, 40 randomly selected images (from each training video) with no manual detection in a time period of −60..+10 seconds were added to the training set. During inference, YOLOv8 gray or YOLOv8 RGB was activated on each frame of the thermal video. To prevent the same deposition from being detected many times, overlapping detections with the same label were unified if no more than 30 seconds passed between them. We compared the accuracy achieved by YOLOv8 gray and YOLOv8 RGB with the DeePosit algorithm that was trained on the same 39 training videos. The results are shown in <xref rid="fig3" ref-type="fig">Figure 3h-j</xref>.</p>
</sec>
<sec id="s2l">
<title>Model Evaluation on Mice of a Different Strain (C57BL/6)</title>
<p>To evaluate the usability of our method in a different strain of mice and a different setting, we conducted 10 SP and 10 SxP experiments with C57BL/6 black mice using a white Plexiglass box arena (37 cm x 22 cm x 35 cm). We used the same classifier and the same preliminary detection parameters. Note that the training set does not include C57BL/6 mice videos or videos with white arenas. see <xref rid="fig3_S1" ref-type="fig">Figure 3—figure Supplement 1</xref>, <xref rid="fig4_S1" ref-type="fig">Figure 4—figure Supplement 1</xref>, <xref rid="fig5" ref-type="fig">Figure 5d,e</xref>, <xref rid="fig5_S1" ref-type="fig">Figure 5— figure Supplement 1d,e</xref> for results.</p>
</sec>
<sec id="s2m">
<title>Statistical Analysis</title>
<p>We used a two-sided Wilcoxon rank sum test (Matlab’s <italic>ranksum</italic> function) for all pairwise comparisons. Rank sum p-value equal to or smaller than 0.1, 0.05, 0.01, 0.001 was marked with #, *, **, ***, respectively. In addition, since some of the data is zero-inflated (many mice do not deposit urine or feces in the relevant measured period), we used a two-way chi-square test to compare the distribution of zeros and non-zeros in the male group vs. the female group in <xref rid="fig6" ref-type="fig">Figure 6</xref> and in <xref rid="fig6_S1" ref-type="fig">Figure 6—figure Supplement 1</xref>. The two-way chi-square test was implemented using Matlab (see code in <xref rid="ufig1" ref-type="fig">Listing 1</xref>). P-value equal or smaller than 0.1, 0.05, 0.01, 0.001 was marked with !, +, ++, +++, respectively, and was mentioned to the left side of the ranksum p-value symbol (i.e, the notation +/** means that two-way chi-square test resulted in p-value&lt;=0.05 and the ranksum test resulted in p-value &lt;= 0.01). For the habituation vs. trial comparison (<xref rid="fig5" ref-type="fig">Figure 5a-b</xref> and <xref rid="fig5_S2" ref-type="fig">Figure 5—figure Supplement 2</xref>), and the side preference analysis (<xref rid="fig4_S2" ref-type="fig">Figure 4—figure Supplement 2</xref>), mice with zero urine detections across all periods of the same test were ignored. The same was done for the feces analysis. Lastly, we used Matlab’s <italic>kruskalwallis</italic> function for the Krusukal-Wallis test, which was used to examine the effect of test type (SP, SxP, ESPs) on the dynamics of the urine and feces rate (<xref ref-type="table" rid="tbl1">Table 1</xref>) and area (<xref ref-type="table" rid="tblA1">Appendix 1—table 1</xref>). Additional statistical data for the figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit/tree/main/FigStat/PostRevision">https://github.com/davidpl2/DeePosit/tree/main/FigStat/PostRevision</ext-link>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>The effect of the test (SP, SxP, and ESPs) on urination or defecation events rates.</title>
<p>Kruskal-Wallis test was used to check if the test type affects the rate of urination or defecation events.</p></caption>
<graphic xlink:href="600419v3_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Social Discrimination</title>
<p>Each CD1 subject animal performed three different social discrimination tests, as previously described by <xref ref-type="bibr" rid="c21">Mohapatra et al. (2024)</xref>, on three consecutive days in the order described below. Each test consisted of a 15-minute habituation stage, during which the subject mouse got used to an experimental arena containing empty chambers at randomly chosen opposite corners. After habituation, the empty chambers were replaced with similar chambers containing stimuli for a 5-minute trial stage (<xref rid="fig1" ref-type="fig">Figure 1a</xref>). In the <bold>Social Preference</bold> (SP) test, a novel (i.e., unfamiliar to the subject mouse) sex-matched stimulus mouse was placed in one chamber, while an object stimulus (a Lego toy) was placed in the opposite chamber. In the <bold>Sex Prefence</bold> (SxP) test, a novel female mouse was placed in one chamber while a novel male was placed in the opposite chamber. In the stress version of the <bold>Emotional State Preference</bold> (ESPs) test, a novel stressed (restrained for 15 minutes before the test) mouse was introduced into one chamber while a naïve mouse was placed in the opposite chamber. We first analyzed the time spent by the subject mouse on investigating each stimulus during the three tests (<xref rid="fig1" ref-type="fig">Figure 1</xref>), using the video clips recorded via the visible light (VIS) camera. Both male and female mice showed the behavior expected from CD1 mice, as previously described by us <xref ref-type="bibr" rid="c17">Kopachev et al. (2022)</xref>. Males showed a significantly higher investigation time towards the social stimulus, as compared to the object in the SP test, towards the opposite sex, as compared to the same sex stimulus mouse in the SxP test, and towards the stressed mouse, as compared to the naïve mouse in the ESPs test. Females showed similar behavior, except for the SxP test, where they exhibited no preference for any of the two stimuli. In accordance with our previous study <xref ref-type="bibr" rid="c23">Netser et al. (2017)</xref>, in all cases, the preference towards a given stimulus was reflected only by long (&gt; 6<italic>s</italic>), but not by short (≤ 6<italic>s</italic>) investigation bouts (<xref rid="fig1" ref-type="fig">Figure 1</xref>). Thus, in terms of social behavior, the subject mice behaved as expected.</p>
</sec>
<sec id="s3b">
<title>Urine and Feces Detection</title>
<p>The experimental setup used for the detection of urine and fecal deposits, comprising VIS and IR cameras, as well as a black body, is schematically shown in <xref rid="fig2" ref-type="fig">Figure 2a</xref>. Unlike the VIS camera (<xref rid="fig2" ref-type="fig">Figure 2b</xref>), the IR camera captures the warm urine and feces drops soon after they were deposited (<xref rid="fig2" ref-type="fig">Figure 2c</xref>). This allowed us to overcome several caveats of the void spot assay. For example, we could tolerate smeared urine spots (<xref rid="fig2" ref-type="fig">Figure 2d-e</xref>) and identify the exact time of each urine or fecal deposition event. Using the thermal video clips, we designed a detection algorithm (termed DeePosit) consisting of two main parts: 1) A preliminary heuristic detection algorithm detects warm blobs (<xref rid="fig2" ref-type="fig">Figure 2f</xref>). 2) These blobs are then fed into a machine learning-based classifier (<xref rid="fig2" ref-type="fig">Figure 2g</xref>), which classifies them as urine, feces, or background (that is, without detection) (See <xref rid="d1e1880" ref-type="supplementary-material">Video 1</xref>, <xref rid="d1e1887" ref-type="supplementary-material">Video 2</xref> and <xref rid="d1e1894" ref-type="supplementary-material">Figure 2—video 1</xref>).</p>
<p>For the generation of training and testing data sets, a human annotator manually tagged urination and defecation events in 157 thermal video clips (about 20 minutes each), of which 97 were used for training and 60 for testing. The precision, recall, and F1 score of the DeePosit algorithm for the test video clips are 0.90,0.86,0.88 for urine deposits and 0.91,0.89,0.90 for feces, respectively. The mean F1 score: (<italic>F</italic> 1<sub><italic>Urine</italic></sub> + <italic>F</italic> 1<sub><italic>Feces</italic></sub>)/2 is 0.89 and the confusion matrix is shown in <xref rid="fig2" ref-type="fig">Figure 2h</xref>. Notably, for large urine deposits, the classification precision is higher (0.98), in comparison to small urine precision (0.85), most probably because large urine drops are more distinguishable from fecal deposits, which are always small <xref rid="fig2_S1" ref-type="fig">Figure 2—figure Supplement 1</xref>. See <xref rid="d1e1894" ref-type="supplementary-material">Figure 2—video 1</xref> and <xref rid="fig3_S3" ref-type="fig">Figure 3—figure Supplement 3</xref>, for examples of correct detections, as well as mistakes made by the detection algorithm in the test videos, which are further discussed in the Discussion section.</p>
</sec>
<sec id="s3c">
<title>Detection Stability and Consistency</title>
<p>We tested the algorithm’s accuracy across various stages of the experiment (<xref rid="fig3" ref-type="fig">Figure 3a-b</xref>), the various experiments (<xref rid="fig3" ref-type="fig">Figure 3c</xref>), the two sexes (<xref rid="fig3" ref-type="fig">Figure 3d</xref>) and three equal spatial divisions of the arena (<xref rid="fig3" ref-type="fig">Figure 3e</xref>). We found that the accuracy was stable in all cases, with no significant difference between them. These results suggest that the accuracy level of the algorithm is uniform across all these instances, hence the algorithm’s mistakes should not create a bias that may affect the experimental results.</p>
<p>We further compared the accuracy level of DeePosit with that of a second human annotator, using the first human annotator as a ground truth to both. For that, we used a subset of 25 video clips from the entire test set. The accuracy achieved by DeePosit with this data set was comparable to that of the second human annotator (mean F1 score of 0.84 and 0.86, respectively, <xref rid="fig3" ref-type="fig">Figure 3f-g</xref>). These results demonstrate the partial accuracy of urination and defecation annotation by human observers and show that DeePosit is comparable to a trained observer in tagging urine and fecal depositions.</p>
<p>We also compared the accuracy of DeePosit with the accuracy achieved by a classic object detection algorithm (YOLOv8) <xref ref-type="bibr" rid="c14">Jocher et al. (2023)</xref>. For that, we annotated 39 training videos of CD1 mice with bounding boxes to match the YOLOv8 framework. For fairness, we compared YOLOv8 results with DeePosit algorithm that was trained on the same set of video clips. DeePosit was significantly better (mean F1=0.81) than YOLOv8, regardless whether we used a single image (YOLOv8 Gray, F1=0.58), or a sequence of three images (0, 10 and 30 seconds after each frame, YOLOv8 RGB, F1=0.68) as in input (see <xref rid="fig3" ref-type="fig">Figure 3h-j</xref>). The fact that using a sequence of images (YOLOv8 RGB) gave better results compared to a single one (YOLOv8 Gray) suggests that temporal information is important for the accurate detection and classification of deposition events.</p>
<p>Finally, to test the accuracy of DeePosit across different mouse strains and experimental arenas, we evaluated DeePosit accuracy for SP and SxP tests performed by C57BL/6 black mice (n=10) in a white Plexiglass arena. DeePosit achieved good performance (mean F1=0.81), even though videos with black mice or with white arenas were not included in the training set (see <xref rid="fig3_S1" ref-type="fig">Figure 3—figure Supplement 1</xref>). Thus, DeePosit shows stable accuracy across experimental conditions.</p>
<p>Our code allows changing the main parameters of the algorithm in order to adjust them to the relevant settings. Therefore, we examined the sensitivity of DeePosit to changes in the parameters used by the algorithm. We first examined DeePosit accuracy as a function of the Δ<italic>T</italic><sub><italic>Threshold</italic></sub> parameter of the preliminary heuristic detection. We found that Δ<italic>T</italic><sub><italic>Threshold</italic></sub> =1.6°C gave the best performance in our setting (see <xref rid="fig3_S2" ref-type="fig">Figure 3—figure Supplement 2</xref>), although the accuracy was quite stable (mean F1 score of 0.88-0.89) for values between 1.1°C to 3°C. We also trained the DeePosit classifier with an input time window of [-11..30] seconds instead of [-11..60] seconds and got no difference in the accuracy level (mean F1 score of 0.89 in both cases).</p>
</sec>
<sec id="s3d">
<title>Distinct Dynamics of Urination and Defecation Activities across the various tests</title>
<p><xref rid="fig4" ref-type="fig">Figure 4a,b</xref> shows the raw results of urine and fecal deposit detection by the DeePosit algorithm as a function of time across all three tests, for each male (blue symbols) and female (red symbols) subject mouse. The symbols representing the various deposit types are also labeled (with black dots) according to the arena side of each deposition (relative to the two stimuli). These raw results were further analyzed by computing the average number of urine or fecal deposits, per minute <xref rid="fig4" ref-type="fig">Figure 4c</xref>. The area of the deposits (<italic>cm</italic><sup>2</sup>) is also plotted (<xref rid="fig4" ref-type="fig">Figure 4d</xref>), since urine deposit size might vary significantly between distinct events and conditions <xref ref-type="bibr" rid="c25">Wegner et al. (2018)</xref>. In general, the event rate and deposit area showed similar trends. As for the side preference, females showed a slight tendency to a higher urination rate at the social stimulus side in the SP test, while males showed a tendency to a higher defecation rate at the social stimulus side (see <xref rid="fig4_S2" ref-type="fig">Figure 4—figure Supplement 2</xref>). Importantly, urination and defecation activities showed distinct dynamics from each other: defecation exhibited a single clear peak in an early stage of the habituation, which appeared in all cases. In contrast, urination was characterized by two peaks, which were not visible in the SP test but appeared in the SxP and got even stronger in the ESPs test, thus showing a gradual increase across test days. The first urination peak occurred in males at the early habituation stage, parallel to the peak in defecation, while the second urination peak occurred in both males and females at the beginning of the trial stage, after stimuli insertion into the arena. For statistical analysis of these dynamics, we compared the mean urine and fecal deposition rates between three periods: the beginning of habituation (habituation minutes 1-4), the end of habituation (habituation minutes 11-14), and the trial - after stimuli introduction (trial minutes 1-4) (<xref rid="fig5" ref-type="fig">Figure 5a,b</xref>). The last minute of both the habituation and the trial stages was not included in the analysis since DeePosit uses one minute of video after the deposition as input; hence, the accuracy may be lower in cases where we have less than one minute of video after the deposition. However, including the missing minute of each stage in the analysis yielded similar results (see <xref rid="fig5_S1" ref-type="fig">Figure 5—figure Supplement 1</xref>). For both males and females and across all tests (besides female SP, where only a trend was observed), we found a significantly higher level of fecal deposition at the beginning of habituation than at the habituation end and the trial stage. In contrast, a similar comparison of urination showed that its level was significantly higher during early habituation than at the end of it only for males in the SxP and ESPs tests. A similar elevation in urination was observed during the trial stage, as compared to the habituation end, for both males and females, again specifically during the SxP and ESPs tests. Interestingly, we found an opposite trend for fecal deposits, with a significant decrease in defecation rate during the trial, as compared to the end of habituation, in all the tests for males and in the SxP test for females (<xref rid="fig5" ref-type="fig">Figure 5a,b</xref>). Similar results were found for urine and fecal deposit areas (<xref rid="fig5_S2" ref-type="fig">Figure 5—figure Supplement 2</xref>). Moreover, similar trends were observed when the proportion of mice actively depositing urine or feces during each stage was calculated for each case (<xref rid="fig5" ref-type="fig">Figure 5c</xref>). These data reveal distinct dynamics for urination and defecation activities in a sex- and test-specific manner.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Urine and fecal deposition detection results across tests in CD1 mice.</title>
<p>Each o represents a single detection of urine deposition (<bold>a</bold>), while each + represents a single detection of fecal deposition (<bold>b</bold>). A black dot in the center of a circle or a + sign marks that this detection is on the side of the preferred stimulus, defined as the social stimulus in the SP trial, the female in the SxP trial, and the stressed mouse in the ESPs trial. Short green lines mark the start and end of the habituation stage and the end of the trial stage, while short vertical black lines mark the end of minute 14 of the habituation stage. The vertical black line at time=0 marks the start of the trial stage after stimuli introduction to the arena, while the vertical dashed line marks four minutes after the beginning of the trial. Dynamics plots (right) show mean rate (<bold>c</bold>) and mean area (<bold>d</bold>) per minute for both urine and fecal deposits. Error bars represent standard error.</p>
<p><xref rid="fig4_S1" ref-type="fig">Figure 4—figure supplement 1</xref><bold>. Urine and fecal deposition detection results across tests in C57BL/6 mice</bold>.</p>
<p><xref rid="fig4_S2" ref-type="fig">Figure 4—figure supplement 2</xref><bold>. Urine and fecal deposition side preference</bold>.</p></caption>
<graphic xlink:href="600419v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Comparison between test stages.</title>
<p>Mean rate of urination and defecation events detected during habituation start (minutes 1-4), habituation end (minutes 11-14), and trial (minutes 1-4) stages, for male CD1 mice (<bold>a</bold>), female CD1 mice (<bold>b</bold>) and male C57BL/6 mice (<bold>d</bold>). (<bold>c</bold>,<bold>e</bold>): Percent of active mice (mice with at least one detection) across tests during habituation start, habituation end, and trial stages, for CD1 mice (<bold>c</bold>) and for male C57BL/6 mice (<bold>e</bold>)</p>
<p><xref rid="fig5_S1" ref-type="fig">Figure 5—figure supplement 1</xref><bold>. Comparison of deposition events rate between test stages using 5 minutes periods</bold></p>
<p><xref rid="fig5_S2" ref-type="fig">Figure 5—figure supplement 2</xref><bold>. Comparison of deposition area between test stages using 4 minutes periods</bold>.</p></caption>
<graphic xlink:href="600419v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>When comparing the urination and defecation patterns of CD1 male mice with those observed in C57BL/6 male mice (<xref rid="fig4_S1" ref-type="fig">Figure 4—figure Supplement 1</xref>, <xref rid="fig5" ref-type="fig">Figure 5d</xref>, <xref rid="fig5_S1" ref-type="fig">Figure 5—figure Supplement 1d</xref>), we found distinct characteristics. In contrast to CD1 mice, urination rate of C57BL/6 mice was higher at the beginning of habituation compared to the end of it already in the SP experiment. On the other hand, urination rate of C57BL/6 mice did not increase during the trial as compared to the end of habituation in any of the experiments. Notably, unlike CD1 mice, of C57BL/6 mice did not deposit urine spots smaller than 1 <italic>cm</italic><sup>2</sup>.(compare <xref rid="fig3_S1" ref-type="fig">Figure 3—figure Supplement 1</xref> with <xref rid="fig2_S1" ref-type="fig">Figure 2— figure Supplement 1</xref>). As for the defecation rate of C57BL/6 mice, similarly to CD1 mice, it was higher at the beginning of habituation compared to the end of it. However, unlike the trend in CD1 mice, it was not reduced in the trial stage, as compared to end of habituation. Thus, the distinct dynamics of urination and defecation activities observed using DeePosit, are mouse strain-specific.</p>
</sec>
<sec id="s3e">
<title>Sex-Dependent Differences across the various stages</title>
<p>We used two types of statistical tests to compare between male and female CD1 mice. A two-sided Wilcoxon rank sum test (significance marked by *) was used for all pairwise comparisons. In addition, since some of the data was zero-inflated (many mice did not deposit urine or feces at all during the relevant period), we used a two-way chi-square test (significance marked by +) to compare the distribution of zeros and non-zeros in the male group vs. the female group. A test-dependent significant difference between males and females was found in the early stage of habituation (<xref rid="fig6" ref-type="fig">Figure 6a</xref>). On the first day of experiments (the SP test), males and females showed a low urination rate at the first four minutes of habituation, with no significant difference between them. However, in the next two testing days (SxP and ESPs tests), when the mice were already familiar with the arena, we found a significantly higher rate and area of urine deposition in males compared to females (<xref rid="fig6" ref-type="fig">Figure 6a</xref> and <xref rid="fig6_S1" ref-type="fig">Figure 6—figure Supplement 1a</xref>). As for defecation events, males showed a significantly higher level in this period, in all tests. During the last stage of habituation (minutes 11-14), we found a significant difference between males and females only for the ESPs test, with males showing higher levels of both urination and defecation rate (<xref rid="fig6" ref-type="fig">Figure 6b</xref>) and area (<xref rid="fig6_S1" ref-type="fig">Figure 6—figure Supplement 1b</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Comparison of deposition rates between sexes.</title>
<p>The mean rate of urination and defecation events for males (blue bars) vs. females (red bars) during early (minutes 1-4) and late (minutes 11-14) periods of the habituation stage and during the first minute and minutes 2-4 of the trial stage. A significant difference between the mean rate of urine or fecal depositions (Wilcoxon rank sum test) is marked with * (or # for 0.05&lt;p-value ≤0.1), and a significant difference in the distribution of non-depositing animals (Chi-square test) is marked with + (or ! for 0.05&lt;p-value ≤0.1).</p>
<p><xref rid="fig6_S1" ref-type="fig">Figure 6—figure supplement 1</xref><bold>. Comparison of deposition areas between sexes</bold>.</p></caption>
<graphic xlink:href="600419v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For statistical comparison between males and females during the trial, where an initial peak was observed in some cases (<xref rid="fig4" ref-type="fig">Figure 4c-d</xref>), we divided the trial stage into two periods: the first minute and minutes 2-4, and averaged the results of each period separately. As apparent in <xref rid="fig6" ref-type="fig">Figure 6c-d</xref> and <xref rid="fig6_S1" ref-type="fig">Figure 6—figure Supplement 1c-d</xref>, the urination rate during the first minute of the trial stage showed no sex-dependent difference in the SP test. In contrast, a significantly higher level was observed for males vs. females in the SxP and ESPs tests. No sex-dependent difference in urination rate was observed For trial minutes 2-4, or in defacation rate for any of the trial periods.</p>
</sec>
<sec id="s3f">
<title>Male Urine and Fecal Deposition Rates are Test-Dependent</title>
<p>Since the data so far suggest a dynamic change from the SP (first day) to the SxP (second day) and ESPs (third day) tests specifically for males, we checked the effect of test type (SP, SxP, ESPs) on the dynamics of urination and defecation activities using Kruskal-Wallis test <xref rid="tbl1" ref-type="table">Table 1</xref> and <xref ref-type="table" rid="tblA1">Appendix 1—table 1</xref>. The urination and defecation rates (<xref rid="tbl1" ref-type="table">Table 1</xref>) and deposits areas (<xref ref-type="table" rid="tblA1">Appendix 1—table 1</xref>) of males showed both a significant effect of the test type, with urination showing this effect during early habituation and the first minute of the trial, while defecation showing such effect at early habituation, but not during the trial stage. No significant effect was found for females.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion and Limitations</title>
<p>Here we present a new algorithm and an open-code trainable AI-based computational tool for detecting and classifying urination and defecation events from thermal video clips. This algorithm enables a detailed characterization of the dynamics of urination and defecation activities during social behavior of small rodents. One advantage of this tool is that it is automated, thus allowing a rapid and observer-unbiased analysis of urine and fecal deposition events and areas, with a good temporal and spatial resolution. Specifically, combining our algorithm with an IR camera for thermal imaging of behavioral experiments can replace the void spot test, which usually lacks any temporal resolution and is prone to mistakes caused by urine smearing and filter-paper tearing. Finally, our algorithm facilitates the analysis of defecation activity, which was rather unexplored so far but may contribute to scent marking behavior, as discussed below. Our algorithm uses thermal video clips generated by an IR camera placed above the arena and does not require a camera placed below a clear arena floor, as used by a recent paper (see <xref ref-type="bibr" rid="c16">Keller et al. (2018)</xref> for example). Thus, it can be utilized for analyzing experiments conducted in standard experimental setups, such as those used for the three-chamber test. The computational tool and experimental method presented here may be useful for a detailed characterization of social behavior in mice, including murine models of autism spectrum disorder and other pathological conditions. It may also be used to explore urination and defecation activities in other scientific contexts, unrelated to social behavior. Finally, Our experimental setup is cheap and easy to assemble, and the detection algorithm can run on a standard PC with a GPU card.</p>
<p>Analysis of the errors made by the algorithm in the test data set (see <xref rid="d1e1894" ref-type="supplementary-material">Figure 2—video 1</xref> for video clips of these events) raised several limitations, that might be addressed in future work. Urine or fecal deposits must be fully visible while the deposit is still warm. A close adjacency between the mouse and the deposit might cause the mouse mask to overlap the mask of the deposit, thus preventing its detection. Many of the “miss” events in the test video clips were created by the mouse staying close to the urine or fecal deposits for a long period after their deposition. Few other “miss” events were due to very small urine spots or due to repeated urination in the same position during a very short time period, which resulted in detecting these separate urination events as a single event by the algorithm. A wrong classification of urine as fecal deposition occurred in 2.3% of the urination events. In many of these events, the urination spot was small (and therefore harder to distinguish from a fecal deposition) (See <xref rid="fig3_S3" ref-type="fig">Figure 3—figure Supplement 3c</xref>). Wrong classification of background as feces occurred 21 times in the test set. In most of these events, the mistake was due to feces that were moved by the mouse to a new location while still being warm. Such cases may be mitigated in future work by a tracking algorithm that continuously tracks the location of each fecal deposit. Wrong classification of background as urine occurred 33 times in the test set, with some of these errors caused by smearing of large warm urine spots.</p>
<p>We evaluated the accuracy of the algorithm and found it to be uniform across the various sexes, tests and session stages of the experiments used by us. This suggests that the low level of errors made by the algorithm should not create a bias during biological experiments. Moreover, the algorithm achieved a good and stable accuracy even for C57BL/6 mice examined in a while arena, a condition that was not represented in the training videos. Thus, the algorithm seems to be robust, with a low sensitivity to changing conditions. We also compared the algorithm’s accuracy to the accuracy achieved by a second human annotator on the same dataset and concluded that the algorithm accuracy is comparable to the accuracy of a human annotator while being much faster and unbiased. Finally, the algorithm showed superior performance over classic object detection algorithms, such as YOLOv8, which are based on a single image input. This is most likely due to the transformer-based architecture of our algorithm, which allows it to use the temporal information extracted from the thermal video clips.</p>
<p>Future work might improve DeePosit by extending the training set and including more challenging examples. Notably, comparing a small training set (<xref rid="fig3" ref-type="fig">Figure 3h</xref>) with a larger one (<xref rid="fig2" ref-type="fig">Figure 2h</xref>) shows that the larger training set improved the accuracy of DeePosit. Another way for future improvement in DeePosit accuracy may be by using a trainable detection and segmentation algorithm instead of heuristic preliminary detection. Note that our classifier currently does not get the mask of the preliminary detection as an input, making the classification task harder when there are adjacent deposition events. An end-to-end trainable detection, segmentation, and classification pipeline might address these limitations but will require a much larger training set. Future work might also adapt the algorithm for multi-animal experiments. Such adaptation might require detecting the mask of each of the animals, separating the identity of each of the animals, and associating each deposition with the relevant animal.</p>
<p>We validated our method and algorithm using experimental results from social discrimination tests conducted by male and female CD1 and male C57BL/6 mice. We demonstrated distinct dynamics of urination and defecation activities across the habituation and trial stages, with sex-, test- and strain-dependent differences. Both male and female CD1 mice, as well as male C57BL/6 mice showed higher rates of defecation activity at the early stage of the habituation phase, as compared to later stages <xref rid="fig5" ref-type="fig">Figure 5</xref>. This tendency may reflect a higher level of anxiety at the beginning of the habituation phase, caused by the novel context. Still, it may also serve for scent-marking activity, that labels the arena as a familiar environment. The latter explanation is supported by the fact that the peak in defecation activity was not reduced from the first-day test (SP) to the second and third-day tests (SxP and ESPs), when the subject is expected to be less anxious due to the familiar context. In contrast to defecation, urination activity at the beginning of the habitation phase in CD1 mice was test-dependent. While no peak was observed during the SP test, the first time the animals were exposed to the experimental arena, it was observed in the second test (SxP) and got even stronger in the last test (ESPs). This development was statistically significant in CD1 males but not in females. Since these changes occur during the habituation phase, before the introduction of stimuli to the arena, they cannot reflect the type of test and thus seem to be induced by the order of the experiments. Notably, similar dynamics across experimental days were previously reported using the void spot assay for C57BL/6j mice <xref ref-type="bibr" rid="c15">Keil et al. (2016)</xref>. This suggests that the induction of urination activity by males at the early stage of the habituation phase represents territorial scent-marking activity, which is positively correlated with the higher familiarity experienced by the subject in the arena as the experiments progressed between days. It should be noted that an early peak of urination upon entering an environment was reported by a recent study using a thermal camera for manual analysis of urination activity <xref ref-type="bibr" rid="c20">Miller et al. (2023b)</xref>). A second peak of urination activity was observed at the beginning of the trial period, after stimuli insertion to the arena. This was observed in both male and female CD1 mice, but the test type significantly affected it only in males. In this case, we cannot dissect the effect of test type from the test order, as the urination activity occurred after stimuli insertion and, hence, may be induced by the presence of specific social stimuli. Since the subjects are already habituated to the arena at this stage, the elevated urination activity seems to serve as part of the subjects’ social behavior, most probably as a territorial scent-marking behavior induced by the presence of social stimuli, i.e., competitors. Interestingly, we found several differences in the dynamics of CD1 male mice and C57BL/6 male mice, suggesting that the scent-marking behavior is also strain-specific. Unlike CD1 male mice, C57BL/6 male mice exhibited a peak in urination already at the beginning of the first (SP) habituation, a trend towards higher level of defecation activity in the SP trial stage, and no increase in urination activity during the SP and SxP trial stage, compared to the habituation end. However, several findings were common for both CD1 and C57BL/6 male mice, such as the higher feces rate at the beginning of habituation in comparison to the end of habituation and the higher levels of urination at the beginning of the SxP habituation stage.</p>
<p>We did not observe a consistent spatial distribution of the urine or fecal deposits between the arena sides of the preferred and non-preferred stimuli in CD1 mice. This seems to contradict a recent study <xref ref-type="bibr" rid="c20">Miller et al. (2023b)</xref>), that reported opposite bias towards familiar vs. unfamiliar stimuli in losers vs. winners wild-derived mice following a social contest. This contradiction may be due to the distinct mouse strains or the distinct contexts of social behavior (presentation of a single stimulus animal in comparison to two simultaneously presented animals) used by both studies.</p>
<p>Overall, the novel algorithm and software presented here enable a cost-effective, rapid, and unbiased analysis of urination and defecation activities of behaving mice from thermal video clips. The algorithm is trainable and may be adapted to various behavioral and experimental contexts. Thus, it may pave the way for the integration of this important behavioral aspect in the analysis of small rodents’ social and non-social behaviors, in health and disease.</p>
</sec>

</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We want to thank Yaniv Goldstein, Janet Tabakova, Wjdan Awaisy, and Shorook Amara for their help annotating the videos and Sara Sheikh for drawing the experiment setup illustration. This study was supported by ISF-NSFC joint research program (grant No. 3459/20), the Israel Science Foundation (grants No. 1361/17 and 2220/22), the Ministry of Science, Technology and Space of Israel (Grant No. 3-12068), the Ministry of Health of Israel (grant #3-18380 for EPINEURODEVO), the German Research Foundation (DFG) (GR 3619/16-1 and SH 752/2-1), the Congressionally Directed Medical Research Programs (CDMRP) (grant No. AR210005) and the United States-Israel Binational Science Foundation (grant No. 2019186).</p>
</ack>
<sec id="suppd1e1910" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1880">
<label>Video 1</label><caption><title>IR video of a single ESP trial of a male mouse with an overlay of the automatic detections.</title><p>Automatic detections are overlayed in red for feces, green for urine, and blue for BG. The stressed mouse side of the arena is marked in green, and the object side is marked in red. Counters of the number and area of automatic detections in each side of the arena are written on the top left. The video plays at X8 speed.</p></caption>
<media xlink:href="supplements/600419_file02.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1887">
<label>Video 2</label><caption><title>IR video of a single ESP habituation of a male mouse with an overlay of the automatic detections.</title><p>The video shows the habituation part of the experiment in <xref rid="d1e1880" ref-type="supplementary-material">Video 1</xref>.</p></caption>
<media xlink:href="supplements/600419_file03.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1894">
<label>Figure 2 - Video 1</label>
<media xlink:href="supplements/600419_file04.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1901">
<label>Full Statistics Data for Figures</label>
<media xlink:href="supplements/600419_file05.zip"/>
</supplementary-material>
</sec>
<sec>
<label>External links:</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit">https://github.com/davidpl2/DeePosit</ext-link></p><p><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/14754159">https://zenodo.org/records/14754159</ext-link></p><p><ext-link ext-link-type="uri" xlink:href="https://drive.google.com/drive/folders/13md92rBTyqe1blTBNV1_7ObcudG-Jh1u?usp=drive_link">https://drive.google.com/drive/folders/13md92rBTyqe1blTBNV1_7ObcudG-Jh1u?usp=drive_link</ext-link></p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arakawa</surname> <given-names>H</given-names></string-name>, <string-name><surname>Blanchard</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Arakawa</surname> <given-names>K</given-names></string-name>, <string-name><surname>Dunlap</surname> <given-names>C</given-names></string-name>, <string-name><surname>Blanchard</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>Scent marking behavior as an odorant communication in mice</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>. <year>2008</year>; <volume>32</volume>(<issue>7</issue>):<fpage>1236</fpage>–<lpage>1248</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benjamini</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hochberg</surname> <given-names>Y.</given-names></string-name></person-group> <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal statistical society: series B (Methodological)</source>. <year>1995</year>; <volume>57</volume>(<issue>1</issue>):<fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bigiani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mucignat-Caretta</surname> <given-names>C</given-names></string-name>, <string-name><surname>Montani</surname> <given-names>G</given-names></string-name>, <string-name><surname>Tirindelli</surname> <given-names>R.</given-names></string-name></person-group> In: <source>Pheromone reception in mammals</source> <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>2005</year>. p. <fpage>1</fpage>–<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1007/s10254-004-0038-0</pub-id>, doi: <pub-id pub-id-type="doi">10.1007/s10254-004-0038-0</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brennan</surname> <given-names>PA</given-names></string-name></person-group>. <article-title>The nose knows who’s who: chemosensory individuality and mate recognition in mice</article-title>. <source>Hormones and Behavior</source>. <year>2004</year>; <volume>46</volume>(<issue>3</issue>):<fpage>231</fpage>–<lpage>240</lpage>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0018506X04000972">https://www.sciencedirect.com/science/article/pii/S0018506X04000972</ext-link>, doi: <pub-id pub-id-type="doi">10.1016/j.yhbeh.2004.01.010</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brennan</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Kendrick</surname> <given-names>KM</given-names></string-name></person-group>. <article-title>Mammalian social odours: attraction and individual recognition</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2006</year>; <volume>361</volume>(<issue>1476</issue>):<fpage>2061</fpage>–<lpage>2078</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Carion</surname> <given-names>N</given-names></string-name>, <string-name><surname>Massa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Synnaeve</surname> <given-names>G</given-names></string-name>, <string-name><surname>Usunier</surname> <given-names>N</given-names></string-name>, <string-name><surname>Kirillov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zagoruyko</surname> <given-names>S.</given-names></string-name></person-group> <article-title>End-to-end object detection with transformers</article-title>. In: <conf-name>European conference on computer vision</conf-name> <publisher-name>Springer</publisher-name>; <year>2020</year>. p. <fpage>213</fpage>–<lpage>229</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cartucho</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ventura</surname> <given-names>R</given-names></string-name>, <string-name><surname>Veloso</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots</article-title>. In: <conf-name>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</conf-name>; <year>2018</year>. p. <fpage>2336</fpage>–<lpage>2341</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dalghi</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Montalbetti</surname> <given-names>N</given-names></string-name>, <string-name><surname>Wheeler</surname> <given-names>TB</given-names></string-name>, <string-name><surname>Apodaca</surname> <given-names>G</given-names></string-name>, <string-name><surname>Carattino</surname> <given-names>MD</given-names></string-name></person-group>. <article-title>Real-time void spot assay</article-title>. <source>JoVE (Journal of Visualized Experiments)</source>. <year>2023</year>; (<issue>192</issue>):<fpage>e64621</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desjardins</surname> <given-names>C</given-names></string-name>, <string-name><surname>Maruniak</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bronson</surname> <given-names>F.</given-names></string-name></person-group> <article-title>Social rank in house mice: differentiation revealed by ultraviolet visualization of urinary marking patterns</article-title>. <source>Science</source>. <year>1973</year>; <volume>182</volume>(<issue>4115</issue>):<fpage>939</fpage>–<lpage>941</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Deep residual learning for image recognition</article-title>. In: <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>; <year>2016</year>. p. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Higuchi</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Arakawa</surname> <given-names>H.</given-names></string-name></person-group> <article-title>Contrasting central and systemic effects of arginine-vasopressin on urinary marking behavior as a social signal in male mice</article-title>. <source>Hormones and Behavior</source>. <year>2022</year>; <volume>141</volume>:<fpage>105128</fpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hurst</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Beynon</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>Scent wars: the chemobiology of competitive signalling in mice</article-title>. <source>Bioessays</source>. <year>2004</year>; <volume>26</volume>(<issue>12</issue>):<fpage>1288</fpage>–<lpage>1298</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hyun</surname> <given-names>M</given-names></string-name>, <string-name><surname>Taranda</surname> <given-names>J</given-names></string-name>, <string-name><surname>Radeljic</surname> <given-names>G</given-names></string-name>, <string-name><surname>Miner</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>W</given-names></string-name>, <string-name><surname>Ochandarena</surname> <given-names>N</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>KW</given-names></string-name>, <string-name><surname>Osten</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sabatini</surname> <given-names>BL</given-names></string-name></person-group>. <article-title>Social isolation uncovers a circuit underlying context-dependent territory-covering micturition</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2021</year>; <volume>118</volume>(<issue>1</issue>):<fpage>e2018078118</fpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Jocher</surname> <given-names>G</given-names></string-name>, <string-name><surname>Chaurasia</surname> <given-names>A</given-names></string-name>, <string-name><surname>Qiu</surname> <given-names>J</given-names></string-name></person-group>, <source>Ultralytics YOLOv8</source>; <year>2023</year>. <ext-link ext-link-type="uri" xlink:href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</ext-link>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keil</surname> <given-names>KP</given-names></string-name>, <string-name><surname>Abler</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Altmann</surname> <given-names>HM</given-names></string-name>, <string-name><surname>Bushman</surname> <given-names>W</given-names></string-name>, <string-name><surname>Marker</surname> <given-names>PC</given-names></string-name>, <string-name><surname>Li</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ricke</surname> <given-names>WA</given-names></string-name>, <string-name><surname>Bjorling</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Vezina</surname> <given-names>CM</given-names></string-name></person-group>. <article-title>Influence of animal husbandry practices on void spot assay outcomes in C57BL/6J male mice</article-title>. <source>Neurourology and urodynamics</source>. <year>2016</year>; <volume>35</volume>(<issue>2</issue>):<fpage>192</fpage>–<lpage>198</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keller</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Simpson</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>EHJ</given-names></string-name>, <string-name><surname>Lilascharoen</surname> <given-names>V</given-names></string-name>, <string-name><surname>George</surname> <given-names>O</given-names></string-name>, <string-name><surname>Lim</surname> <given-names>BK</given-names></string-name>, <string-name><surname>Stowers</surname> <given-names>L.</given-names></string-name></person-group> <article-title>Voluntary urination control by brainstem neurons that relax the urethral sphincter</article-title>. <source>Nature neuroscience</source>. <year>2018</year>; <volume>21</volume>(<issue>9</issue>):<fpage>1229</fpage>–<lpage>1238</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kopachev</surname> <given-names>N</given-names></string-name>, <string-name><surname>Netser</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wagner</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Sex-dependent features of social behavior differ between distinct laboratory mouse strains and their mixed offspring</article-title>. <source>Iscience</source>. <year>2022</year>; <volume>25</volume>(<issue>2</issue>).</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mervis</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Dida</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lam</surname> <given-names>E</given-names></string-name>, <string-name><surname>Crawford-Zelli</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Young</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Henderson</surname> <given-names>DR</given-names></string-name>, <string-name><surname>Onay</surname> <given-names>T</given-names></string-name>, <string-name><surname>Morris</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Woodruff-Borden</surname> <given-names>J</given-names></string-name>, <string-name><surname>Yeomans</surname> <given-names>J</given-names></string-name>, <string-name><surname>Osborne</surname> <given-names>LR</given-names></string-name></person-group>. <article-title>Duplication of GTF2I results in separation anxiety in mice and humans</article-title>. <source>American Journal of Human Genetics</source>. <year>2012</year> <month>jun</month>; <volume>90</volume>(<issue>6</issue>):<fpage>1064</fpage>–<lpage>1070</lpage>. <pub-id pub-id-type="pmid">22578324</pub-id>, doi: <pub-id pub-id-type="doi">10.1016/j.ajhg.2012.04.012</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname> <given-names>CH</given-names></string-name>, <string-name><surname>Haxhillari</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hillock</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Reichard</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Sheehan</surname> <given-names>MJ</given-names></string-name></person-group>. <article-title>Scent mark signal investment predicts fight dynamics in house mice</article-title>. <source>Proceedings of the Royal Society B</source>. <year>2023</year>; <volume>290</volume>(<issue>1993</issue>):<fpage>20222489</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname> <given-names>CH</given-names></string-name>, <string-name><surname>Hillock</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Carlson-Clarke</surname> <given-names>B</given-names></string-name>, <string-name><surname>Haxhillari</surname> <given-names>K</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>AY</given-names></string-name>, <string-name><surname>Warden</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Sheehan</surname> <given-names>MJ</given-names></string-name></person-group>. <article-title>Dynamic changes to signal allocation rules in response to variable social environments in house mice</article-title>. <source>Communications Biology</source>. <year>2023</year>; <volume>6</volume>(<issue>1</issue>):<fpage>297</fpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mohapatra</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Peles</surname> <given-names>D</given-names></string-name>, <string-name><surname>Netser</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wagner</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Synchronized LFP rhythmicity in the social brain reflects the context of social encounters</article-title>. <source>Communications Biology</source>. <year>2024</year>; <volume>7</volume>(<issue>1</issue>):<fpage>2</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Netser</surname> <given-names>S</given-names></string-name>, <string-name><surname>Haskal</surname> <given-names>S</given-names></string-name>, <string-name><surname>Magalnik</surname> <given-names>H</given-names></string-name>, <string-name><surname>Bizer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wagner</surname> <given-names>S.</given-names></string-name></person-group> <article-title>A system for tracking the dynamics of social preference behavior in small rodents</article-title>. <source>JoVE (Journal of Visualized Experiments)</source>. <year>2019</year>; (<issue>153</issue>):<fpage>e60336</fpage>. doi: doi:<pub-id pub-id-type="doi">10.3791/60336</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Netser</surname> <given-names>S</given-names></string-name>, <string-name><surname>Haskal</surname> <given-names>S</given-names></string-name>, <string-name><surname>Magalnik</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wagner</surname> <given-names>S.</given-names></string-name></person-group> <article-title>A novel system for tracking social preference dynamics in mice reveals sex-and strain-specific characteristics</article-title>. <source>Molecular autism</source>. <year>2017</year>; <volume>8</volume>:<fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Verstegen</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Tish</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Szczepanik</surname> <given-names>LP</given-names></string-name>, <string-name><surname>Zeidel</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Geerling</surname> <given-names>JC</given-names></string-name></person-group>. <article-title>Micturition video thermography in awake, behaving mice</article-title>. <source>Journal of neuroscience methods</source>. <year>2020</year>; <volume>331</volume>:<fpage>108449</fpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wegner</surname> <given-names>KA</given-names></string-name>, <string-name><surname>Abler</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Oakes</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>GS</given-names></string-name>, <string-name><surname>Ritter</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Hill</surname> <given-names>WG</given-names></string-name>, <string-name><surname>Zwaans</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Lamb</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Bjorling</surname> <given-names>DE</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Void spot assay procedural optimization and software for rapid and objective quantification of rodent voiding function, including overlapping urine spots</article-title>. <source>American Journal of Physiology-Renal Physiology</source>. <year>2018</year>; <volume>315</volume>(<issue>4</issue>):<fpage>F1067</fpage>–<lpage>F1080</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wöhr</surname> <given-names>M</given-names></string-name>, <string-name><surname>Roullet</surname> <given-names>FI</given-names></string-name>, <string-name><surname>Hung</surname> <given-names>AY</given-names></string-name>, <string-name><surname>Sheng</surname> <given-names>M</given-names></string-name>, <string-name><surname>Crawley</surname> <given-names>JN</given-names></string-name></person-group>. <article-title>Communication impairments in mice lacking Shank1: reduced levels of ultrasonic vocalizations and scent marking behavior</article-title>. <source>PloS one</source>. <year>2011</year>; <volume>6</volume>(<issue>6</issue>):<fpage>e20631</fpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname> <given-names>P</given-names></string-name>, <string-name><surname>Powell</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Urine patterns in mice: an analysis of male/female counter-marking</article-title>. <source>Animal behaviour</source>. <year>1984</year>; <volume>32</volume>(<issue>4</issue>):<fpage>1185</fpage>–<lpage>1191</lpage>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<title>Appendix 1</title>
<table-wrap id="tblA1" orientation="portrait" position="float">
<label>Appendix 1—table 1.</label>
<caption><title>The effect of the test on the urine and feces area.</title>
<p>Kruskal-Wallis test was used to check if the test type (SP, SxP, and ESPs) affects the area of urine or feces.</p></caption>
<graphic xlink:href="600419v3_tblA1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="ufig1" position="float" fig-type="figure">
<label>Listing 1</label><caption><title>Code for computing Two Way Chi-Square Test which was used to compare the distribution of active mice (with at least one detection) in males vs females.</title></caption>
<graphic xlink:href="600419v3_ufig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2_S1" position="float" fig-type="figure">
<label>Figure 2—figure supplement 1.</label>
<caption><title>Accuracy for small and large detections in CD1 mice.</title>
<p><bold>(a</bold>,<bold>b</bold>) Confusion matrices on test videos with separation between large and small automatic detections. The threshold for large detections is an area of 1<italic>cm</italic><sup>2</sup> which is 47.3 pixels. Shown percents sum to 1 for each column in (<bold>a</bold>) and each row in (<bold>b</bold>). The <italic>Large Urination</italic> class is correct in 98.2% of the cases in which it was reported by the classifier while <italic>Small Urination</italic> is correct in only 84.5% as shown in (<bold>b</bold>). Most of the confusion between feces and urine spots is for small detections: 2.3% of the Ground Truth (GT) urine events were classified as <italic>Small Feces</italic> while 0% as <italic>Large Feces</italic> as shown in (<bold>a</bold>). Also, 2.4% of the GT feces events were classified as <italic>Small Urine</italic> while 0% as <italic>Large Urine</italic>. No GT feces event was classified as Large BG. While feces are usually small, <italic>Large Feces</italic> detection might occur when two adjacent feces are detected as a single segment or when the detected segment contains both urine and feces.</p></caption>
<graphic xlink:href="600419v3_fig2_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig3_S1" position="float" fig-type="figure">
<label>Figure 3—figure supplement 1.</label>
<caption><title>Accuracy for small and large detections in C57BL/6 mice.</title>
<p>To check the robustness of our method for different strains of mice and experimental conditions, we tested our algorithm on black C57BL/6 male mice and a white arena (the arena is white in visible light but looks dark in long-wave infrared). (<bold>a</bold>) Confusion matrices reflecting the accuracy of DeePosit algorithm on 10 SP and 10 SxP videos that were not included in the training set. The mean F1 for C57BL/6 is 0.81. Interestingly, C57BL/6 mice do not produce small urine spots, and hence, all the “small urine” detections were wrong. Ignoring the small urine detections improves the mean F1 score to 0.86.</p></caption>
<graphic xlink:href="600419v3_fig3_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig3_S2" position="float" fig-type="figure">
<label>Figure 3—figure supplement 2.</label>
<caption><p><bold>Detection accuracy at various values of</bold> Δ<italic>T</italic><sub><italic>Threshold</italic></sub> DeePosit accuracy was measured for several values of the preliminary heuristic detection temperature threshold Δ<italic>T</italic><sub><italic>Threshold</italic></sub>. The best results were achieved with a threshold of Δ<italic>T</italic><sub><italic>Threshold</italic></sub> =1.6°C. However, a good accuracy level (F1 score between 0.88 and 0.89) was observed in all cases between 1.1 to 3.0°C. See Methods for more details.</p></caption>
<graphic xlink:href="600419v3_fig3_S2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig3_S3" position="float" fig-type="figure">
<label>Figure 3—figure supplement 3.</label>
<caption><title>Examples of detections in test videos.</title>
<p><bold>(a</bold>,<bold>b</bold>): Examples of urination and defecation events that were detected and classified correctly. Each pair of columns includes a ground truth detection (to the left) next to the matched automatic detection (to the right), which includes the mask of the detected blob. The overlaid text mentions the video index and the frame index. (<bold>b</bold>): Urination events that were wrongly classified as background. (<bold>c</bold>): Urine depositions that were classified as feces. (<bold>d</bold>): Fecal depositions that were classified as urine.</p></caption>
<graphic xlink:href="600419v3_fig3_S3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4_S1" position="float" fig-type="figure">
<label>Figure 4—figure supplement 1.</label>
<caption><title>Urine and fecal deposition detection results across tests in C57BL/6 mice.</title>
<p>DeePosit detections for 10 SP and 10 SxP tests performed by male C57BL/6 mice, that were not included in the training set are shown in (<bold>a</bold>-<bold>h</bold>) in a similar manner to <xref rid="fig4" ref-type="fig">Figure 4</xref>. We chose to ignore small urine detections (deposition area&lt;1<italic>cm</italic><sup>2</sup>) as we found that C57BL/6 males do not emit small urine depositions.</p></caption>
<graphic xlink:href="600419v3_fig4_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4_S2" position="float" fig-type="figure">
<label>Figure 4—figure supplement 2.</label>
<caption><title>Urine and fecal deposition side preference.</title>
<p>A comparison of the mean ±SEM rate ((<bold>a</bold>) and (<bold>b</bold>)) and area ((<bold>c</bold>) and (<bold>d</bold>)) of urine (two left bars in each panel) and fecal (two right bars in each panel) depositions made by male (blue bars) and female (red bars) subject mice in each side of the arena, for all three tests. Rank sum p-value equal to or smaller than 0.1, 0.05, 0.01, 0.001 was marked with #, *, **, ***, respectively.</p></caption>
<graphic xlink:href="600419v3_fig4_S2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5_S1" position="float" fig-type="figure">
<label>Figure 5—figure supplement 1.</label>
<caption><title>Comparison of deposition events rate between test stages using 5 minutes periods</title>
<p>Mean rate of urination and defecation events during habituation start (minutes 1-5), habituation end (minutes 11-15), and trial (minutes 1-5) stages for male CD1 (<bold>a</bold>), female CD1 (<bold>b</bold>) and male C57BL/6 mice (<bold>d</bold>). (<bold>c</bold>,<bold>e</bold>): Percent of active mice (mice with at least one detection) across tests during habituation start, habituation end, and trial for male and female CD1 mice (<bold>c</bold>) and for male C57BL/6 mice (<bold>e</bold>)</p></caption>
<graphic xlink:href="600419v3_fig5_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5_S2" position="float" fig-type="figure">
<label>Figure 5—figure supplement 2.</label>
<caption><title>Comparison of deposition area between test stages using 4 minutes periods.</title>
<p>mean area ±SEM of urine and fecal depositions per minute during habituation start (minutes 1-4), habituation end (minutes 11-14), and trial (minutes 1-4) stages. Statistical comparisons between the three periods (three pair-wise comparisons) were done separately for urine and fecal depositions. Mice with no urine or feces detection in these periods were ignored from the urine or feces analysis, respectively.</p></caption>
<graphic xlink:href="600419v3_fig5_S2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig6_S1" position="float" fig-type="figure">
<label>Figure 6—figure supplement 1.</label>
<caption><title>Comparison of mean deposition areas between sexes.</title>
<p>The mean area ±SEM of urine and fecal depositions for males (blue bars) vs. females (red bars) during early (minutes 1-4) and late (minutes 11-14) periods of the habituation stage and during the first minute and minutes 2-4 of the trial stage. A significant difference between the mean area of urine or fecal depositions (Wilcoxon rank sum test) is marked with * (or # for 0.05&lt;p-value ≤0.1) and a significant difference in the distribution of non-depositing animals (Chi-square test) is marked with + (or ! for 0.05&lt;p-value ≤0.1).</p></caption>
<graphic xlink:href="600419v3_fig6_S1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100739.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This manuscript presents an <bold>important</bold> machine-learning-based approach to the automated detection of urine and fecal deposits by rodents, key ethological behaviors that have traditionally been very poorly studied. The strength of evidence for the claim is <bold>solid</bold>, showing accuracy near 90% across several contexts. Training and testing for the specific contexts used by other experimenters, however, is probably warranted to make the model most relevant to the data that may be analyzed.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100739.2.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript provides a novel method for the automated detection of scent marks from urine and feces in rodents. Given the importance of scent communication in these animals and their role as model organisms, this is a welcome tool.</p>
<p>Strengths:</p>
<p>The method uses a single video stream to allow for the distinction between urine and feces. It is automated.</p>
<p>Weaknesses:</p>
<p>The accuracy is decent but not perfect and may be too low to detect some effects that are biologically real but subtle (e.g. less than 10% differences). For many assays, however, this tools will be useful.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100739.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peles</surname>
<given-names>David</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-5543-1676</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Netser</surname>
<given-names>Shai</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4176-1124</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Ray</surname>
<given-names>Natalie</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Suliman</surname>
<given-names>Taghreed</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-4177-5391</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Wagner</surname>
<given-names>Shlomo</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7618-0752</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<p>We thank the reviewers for their constructive and helpful comments, which led us to make major changes in the model and manuscript, including adding the results of new experiments and analyses. We believe that the revised manuscript is much better than the previous version and that it addresses all issued raised by the reviewers.</p>
<p>Summary of changes made in the revised manuscript:</p>
<p>(1) We increased the training set size from 39 video clips to 97 video clips and the testing set size from 25 video clips to 60 video clips. The increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 (see Figure 2 and Figure 3) in the current version. Specifically, the F1 score for urine detection was improved from 0.79 to 0.88.</p>
<p>(2) We further evaluated the accuracy of the DeePosit algorithm in comparison to a second human annotator and found that the algorithm accuracy is comparable to human-level accuracy.</p>
<p>(3) The additional test videos allowed us to test the consistency of the algorithm performance across gender, space, time, and experiment type (SP, SxP, and ESPs). We found consistent levels of performance across all categories (see Figure 3), suggesting that errors made by the algorithm are uniform across conditions, hence should not create any bias of the results.</p>
<p>(4) In addition, we tested the algorithm performance on a second strain of mice (male C57BL/6) in a different environmental condition (white arena instead of a black one) and found that the algorithm achieves comparable accuracy, even though C57BL/6 mice and white arena were not included in the training set. Thus, the algorithm seems to be robust and efficient across various experimental conditions.</p>
<p>(5) Analyzing urination and defecation dynamics in an additional strain of mice revealed interesting strain-specific features, as discussed in the revised manuscript.</p>
<p>(6) Overall, we found DeePosit accuracy to be stable with no significant bias across stages of the experiment, types of the experiment, gender of the mice, strain of mice, and across experimental conditions.</p>
<p>(7) We also compared the performance of DeePosit to a classic object detection algorithm: YOLOv8. We trained YOLOv8 both on a single image input (YOLOv8 Gray) and on 3 image inputs representing a sequence of three time points around the ground truth event (t): t+0, t+10, and t+30 seconds (YOLOv8 RGB). DeePosit achieved significantly better accuracy over both YOLOv8 alternatives. YOLOv8 RGB achieved better accuracy than YOLOv8 Gray, suggesting that temporal information is important for this task. It's worth mentioning that while YOLOv8 requires the annotator to draw rectangles surrounding each urine spot or feces as part of the training set, our algorithm training set used just a single click inside each spot, allowing faster generation of training sets.</p>
<p>(8) As for the algorithm parameters, we tested the effect of the main parameter of the preliminary detection (the temperature threshold for the detection of a new blob) and found that a threshold of 1.6°C gave the best accuracy and used this parameter for all of the experiments instead of 1.1°C which was used in the original manuscript. It's worth mentioning that the performance is quite stable (mean F1 score of 0.88-0.89) for the thresholds between 1.1°C and 3°C (Figure 3—Figure Supplement 2).</p>
<p>(9) We also checked if changing the input length of the video clip that is fed to the classifier affects the accuracy by training the classifier with -11..30 seconds video clips (41 seconds in total) instead of -11..60 seconds (71 seconds in total) and found no difference in accuracy.</p>
<p>(10) In the revised paper, we report recall, precision, and F1 scores in the caption of the relevant figures and also supply Excel files with the full statistics for each of the figures.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The manuscript provides a novel method for the automated detection of scent marks from urine and feces in rodents. Given the importance of scent communication in these animals and their role as model organisms, this is a welcome tool.</p>
</disp-quote>
<p>We thank the reviewer for the positive assessment of our tool</p>
<disp-quote content-type="editor-comment">
<p>Strengths:</p>
<p>The method uses a single video stream (thermal video) to allow for the distinction between urine and feces. It is automated.</p>
<p>Weaknesses:</p>
<p>The accuracy level shown is lower than may be practically useful for many studies. The accuracy of urine is 80%.</p>
</disp-quote>
<p>We have trained the model better, using a larger number of video clips. The increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 (see Figure 2 and Figure 3) in the current version. Specifically, the F1 score for urine detection was improved from 0.79 to 0.88.</p>
<disp-quote content-type="editor-comment">
<p>This is understandable given the variability of urine in its deposition, but makes it challenging to know if the data is accurate. If the same kinds of mistakes are maintained across many conditions it may be reasonable to use the software (i.e., if everyone is under/over counted to the same extent). Differences in deposition on the scale of 20% would be challenging to be confident in with the current method, though differences of the magnitude may be of biological interest. Understanding how well the data maintain the same relative ranking of individuals across various timing and spatial deposition metrics may help provide further evidence for the utility of the method.</p>
</disp-quote>
<p>The additional test videos allowed us to test the consistency of the algorithm performance across gender, space, time and experiment type (SP, SxP, and ESP). We found consistent levels of performance across all categories (see Figure 3), suggesting that errors made by the algorithm are uniform across conditions, hence should not create any bias of the results.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors built a tool to extract the timing and location of mouse urine and fecal deposits in their laboratory set up. They indicate that they are happy with the results they achieved in this effort.</p>
</disp-quote>
<p>Yes, we are.</p>
<disp-quote content-type="editor-comment">
<p>The authors note urine is thought to be an important piece of an animal's behavioral repertoire and communication toolkit so methods that make studying these dynamics easier would be impactful.</p>
</disp-quote>
<p>We thank the reviewer for the positive assessment of our work.</p>
<disp-quote content-type="editor-comment">
<p>Strengths:</p>
<p>With the proposed method, the authors are able to detect 79% of the urine that is present and 84% of the feces that is present in a mostly automated way.</p>
<p>Weaknesses:</p>
<p>The method proposed has a large number of design choices across two detection steps that aren't investigated. I.e. do other design choices make the performance better, worse, or the same?</p>
</disp-quote>
<p>We chose to use a heuristic preliminary detection algorithm for the detection of warm blobs, since warm blobs can be robustly detected with heuristic algorithms without the need for a training set. This design selection might allow easier adaptation of our algorithm for different types of arenas. Another advantage of using a heuristic preliminary detection is the easy control of the preliminary detection parameters such as the minimum temperature difference for detecting a blob, size limits of the detected blob, cooldown rate and so on that may help in adopting it to new conditions. As for the classifier, we chose to feed it with a relatively small window surrounding each preliminary detection, and hence it is not affected by the arena’s appearance outside of its region of interest. This should allow lower sensitivity to the arena’s appearance.</p>
<p>As for the algorithm parameters, we tested the effect of the main parameter of the preliminary detection (the temperature threshold for the detection of a new blob) and found that a threshold of 1.6°C gave the best accuracy and used this parameter for all of the experiments instead of 1.1°C which was used in the original manuscript. It's worth mentioning that the performance is quite stable (mean F1 score of 0.88-0.89) for the thresholds between 1.1°C and 3°.</p>
<p>We also checked if changing the input length of the video clip fed to the classifier affects the accuracy by training the classifier with -11..30 seconds video clips (41 seconds in total) instead of -11..60 seconds (71 seconds in total) and found no difference in accuracy.</p>
<p>Overall, the algorithm's accuracy seems to be rather stable across various choices of parameters.</p>
<disp-quote content-type="editor-comment">
<p>Are these choices robust across a range of laboratory environments?</p>
</disp-quote>
<p>We tested the algorithm performance on a second strain of mice (male C57BL/6) in a different environmental condition (white arena instead of a black one) and found that the algorithm achieves comparable accuracy, even though C57BL/6 mice and white arena were not included in the training set. Thus, the algorithm seems to be robust and efficient across various experimental conditions.</p>
<disp-quote content-type="editor-comment">
<p>How much better are the demonstrated results compared to a simple object detection pipeline (i.e. FasterRCNN or YOLO on the raw heat images)?</p>
</disp-quote>
<p>We compared the performance of DeePosit to a classic object detection algorithm: YOLOv8. We trained YOLOv8 both on a single image input (YOLOv8 Gray) and on 3 image inputs representing a sequence of three time points around the ground truth event (t): t+0, t+10, and t+30 seconds (YOLOv8 RGB). DeePosit achieved significantly better accuracy over both YOLOv8 alternatives. YOLOv8 RGB achieved better accuracy than YOLOv8 Gray, suggesting that temporal information is important for this task. It's worth mentioning that while YOLOv8 requires annotator to draw rectangles surrounding each urine spot or feces as part of the training set, our algorithm training set used just a single click inside each spot, allowing faster generation of a training sets.</p>
<disp-quote content-type="editor-comment">
<p>The method is implemented with a mix of MATLAB and Python.</p>
</disp-quote>
<p>That is right.</p>
<disp-quote content-type="editor-comment">
<p>One proposed reason why this method is better than a human annotator is that it &quot;is not biased.&quot; While they may mean it isn't influenced by what the researcher wants to see, the model they present is still statistically biased since each object class has a different recall score. This wasn't investigated. In general, there was little discussion of the quality of the model.</p>
</disp-quote>
<p>We tested the consistency of the algorithm performance across gender, space, time and experiment type (SP, SxP, and ESP). We found consistent levels of performance across all categories (see Figure 3), suggesting that errors made by the algorithm are uniform across conditions, hence should ne create any bias of the results. Specifically, the detection accuracy is similar between urine and feces, hence should not impose a bias between the various object classes.</p>
<disp-quote content-type="editor-comment">
<p>Precision scores were not reported.</p>
</disp-quote>
<p>In the revised paper we report recall, precision, and F1 scores in the caption of the relevant figures and also supply Excel files with the full statistics for each of the figures.</p>
<disp-quote content-type="editor-comment">
<p>Is a recall value of 78.6% good for the types of studies they and others want to carry out? What are the implications of using the resulting data in a study?</p>
</disp-quote>
<p>We have trained the model better, using a larger number of video clips. The increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 (see Figure 2 and Figure 3) in the current version. Specifically, the F1 score for urine detection was improved from 0.79 to 0.88.</p>
<disp-quote content-type="editor-comment">
<p>How do these results compare to the data that would be generated by a &quot;biased human?&quot;</p>
</disp-quote>
<p>We further evaluated the accuracy of the DeePosit algorithm in comparison to a second human annotator and found that the algorithm accuracy is comparable to human-level accuracy (Figure 3).</p>
<disp-quote content-type="editor-comment">
<p>5 out of the 6 figures in the paper relate not to the method but to results from a study whose data was generated from the method. This makes a paper, which, based on the title, is about the method, much longer and more complicated than if it focused on the method.</p>
</disp-quote>
<p>We appreciate the reviewer's comment, but the analysis of this new dataset by DeePosit demonstrates how the algorithm may be used to reveal novel and distinguishable dynamics of urination and defecation activities during social interactions, which were not yet reported.</p>
<disp-quote content-type="editor-comment">
<p>Also, even in the context of the experiments, there is no discussion of the implications of analyzing data that was generated from a method with precision and recall values of only 7080%. Surely this noise has an effect on how to correctly calculate p-values etc. Instead, the authors seem to proceed like the generated data is simply correct.</p>
</disp-quote>
<p>As mentioned above, the increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 (see Figure 2 and Figure 3) in the current version. Specifically, the F1 score for urine detection was improved from 0.79 to 0.88.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors introduce a tool that employs thermal cameras to automatically detect urine and feces deposits in rodents. The detection process involves a heuristic to identify potential thermal regions of interest, followed by a transformer network-based classifier to differentiate between urine, feces, and background noise. The tool's effectiveness is demonstrated through experiments analyzing social preference, stress response, and temporal dynamics of deposits, revealing differences between male and female mice.</p>
<p>Strengths:</p>
<p>The method effectively automates the identification of deposits</p>
<p>The application of the tool in various behavioral tests demonstrates its robustness and versatility.</p>
<p>The results highlight notable differences in behavior between male and female mice</p>
</disp-quote>
<p>We thank the reviewer for the positive assessment of our work.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>The definition of 'start' and 'end' periods for statistical analysis is arbitrary. A robustness check with varying time windows would strengthen the conclusions.</p>
</disp-quote>
<p>In all the statistical tests conducted in the revised manuscript, we have used a time period of 4 minutes for the analysis. We did not used the last minute of each stage for the analysis since the input of DeePosit requires 1 minute of video after the event. Nevertheless, we also conducted the same tests using a 5-minute period and found similar results (Figure 5—Figure Supplement 1).</p>
<disp-quote content-type="editor-comment">
<p>The paper could better address the generalizability of the tool to different experimental setups, environments, and potentially other species.</p>
</disp-quote>
<p>As mentioned above, we tested the algorithm performance on a second strain of mice (male C57BL/6) in a different environmental condition (white arena instead of a black one) and found that the algorithm achieves comparable accuracy, even though C57BL/6 mice and white arena were not included in the training set. Thus, the algorithm seems to be robust and efficient across various experimental conditions.</p>
<disp-quote content-type="editor-comment">
<p>The results are based on tests of individual animals, and there is no discussion of how this method could be generalized to experiments tracking multiple animals simultaneously in the same arena (e.g., pair or collective behavior tests, where multiple animals may deposit urine or feces).</p>
</disp-quote>
<p>At the moment, the algorithm cannot be applied for multiple animals freely moving in the same arena. However, in the revised manuscript we explicitly discussed what is needed for adapting the algorithm to perform such analyses.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p>-  Add a note and/or perform additional calculations to show that the results do not depend on the specific definitions of 'start' and 'end' periods. For instance, vary the time window thresholds and recalculate the statistics using different windows (e.g., 1-5 minutes instead of 1-4 minutes).</p>
</disp-quote>
<p>In all the statistical tests conducted in the revised manuscript, we have used a time period of 4 minutes for the analysis. We did not use the last minute of each stage for the analysis since the input of DeePosit requires 1 minute of video after the event. Nevertheless, we also conducted the same tests using a 5-minute period and found similar results (Figure 5—Figure Supplement 1).</p>
<disp-quote content-type="editor-comment">
<p>- Condense Figures 4, 5, and 6 to simplify the presentation. Focus on demonstrating the effectiveness of the tool rather than detailed experimental outcomes, as the primary contribution of this paper is methodological.</p>
</disp-quote>
<p>We have added to the revised manuscript one technical figure (Figure 3) comparing the accuracy of the algorithm performance across gender, space, time, and experiment type (SP, SxP, and ESP) as well as comparing its performance to a second human annotator and to YOLOv8. One more partially technical figure (Figure 5) compares the results of the algorithm between white ICR mice in the black arena and black C57BL/6 mice in the white arena. Thus, only Figures 4 and 6 show detailed experimental outcomes.</p>
<disp-quote content-type="editor-comment">
<p>- Provide more detail on how the preliminary detection procedure and parameters might need adjustment for different experimental setups or conditions. Discuss potential adaptations for field settings or more complex environments.</p>
</disp-quote>
<p>As for the algorithm parameters, we tested the effect of the main parameter of the preliminary detection (the temperature threshold for the detection of a new blob) and found that a threshold of 1.6°C gave the best accuracy and used this parameter for all of the experiments instead of 1.1°C which was used in the original manuscript. It's worth mentioning that the performance is quite stable (mean F1 score of 0.88-0.89) for the thresholds between 1.1°C and 3°.</p>
<p>We also checked if changing the input length of the video clip that is fed to the classifier affects the accuracy by training the classifier with -11..30 seconds video clips (41 seconds in total) instead of -11..60 seconds (71 seconds in total) and found no difference in accuracy.</p>
<p>Overall, the algorithm's accuracy seems to be rather stable across various choices of parameters.</p>
<disp-quote content-type="editor-comment">
<p>Editor's note:</p>
<p>Should you choose to revise your manuscript, please ensure your manuscript includes full statistical reporting including exact p-values wherever possible alongside the summary statistics (test statistic and df) and 95% confidence intervals. These should be reported for all key questions and not only when the p-value is less than 0.05 in the main manuscript.</p>
</disp-quote>
<p>We have deposited the detailed statistics of each figure in <ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit/tree/main/FigStat/PostRevision">https://github.com/davidpl2/DeePosit/tree/main/FigStat/PostRevision</ext-link></p>
</body>
</sub-article>
</article>