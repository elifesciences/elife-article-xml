<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105081</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105081</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105081.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Movie reconstruction from mouse visual cortex activity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5858-166X</contrib-id>
<name>
<surname>Bauer</surname>
<given-names>Joel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>joel.bauer@ucl.ac.uk</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5526-4578</contrib-id>
<name>
<surname>Margrie</surname>
<given-names>Troy W</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4507-8648</contrib-id>
<name>
<surname>Clopath</surname>
<given-names>Claudia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04kjqkz56</institution-id><institution>Sainsbury Wellcome Centre, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Bioengineering Dept, Imperial College,</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Denison</surname>
<given-names>Rachel</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Boston University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally as last authors</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-03-25">
<day>25</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105081</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-12-02">
<day>02</day>
<month>12</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-11-11">
<day>11</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.19.599691"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Bauer et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Bauer et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105081-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>The ability to reconstruct imagery represented by the brain has the potential to give us an intuitive understanding of what the brain sees. Reconstruction of visual input from human fMRI data has garnered significant attention in recent years. Comparatively less focus has been directed towards vision reconstruction from single-cell recordings, despite its potential to provide a more direct measure of the information represented by the brain. Here, we achieve high-quality reconstructions of videos presented to mice, from the activity of neurons in their visual cortex. Using our method of video optimization via backpropagation through a state-of-the-art dynamic neural encoding model we reliably reconstruct 10-second movies at 30 Hz from two-photon calcium imaging data. We achieve a ≈ 2-fold increase in pixel-by-pixel correlation compared to previous state-of-the-art reconstructions of static images from mouse V1, while also capturing temporal dynamics. We find that critical for high-quality reconstructions are the number of neurons in the dataset and the use of model ensembling. This paves the way for movie reconstruction to be used as a tool to investigate a variety of visual processing phenomena.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>In this version, we have added Gaussian filtering, extended the discussion and methods, added a benchmarking table, and other minor edits.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>One fundamental aim of neuroscience is to eventually gain insight into the ongoing perceptual experience of humans and animals. Reconstruction of visual perception directly from brain activity has the potential to give us a deeper understanding of how the brain represents visual information. Over the past decade, there have been considerable advances in reconstructing images and videos from human brain activity [<xref ref-type="bibr" rid="c27">Nishimoto et al., 2011</xref>, <xref ref-type="bibr" rid="c42">Shen et al., 2019a</xref>,<xref ref-type="bibr" rid="c43">b</xref>, <xref ref-type="bibr" rid="c34">Rakhimberdina et al., 2021</xref>, <xref ref-type="bibr" rid="c37">Ren et al., 2021</xref>, <xref ref-type="bibr" rid="c47">Takagi and Nishimoto, 2023</xref>, <xref ref-type="bibr" rid="c29">Ozcelik and VanRullen, 2023</xref>, <xref ref-type="bibr" rid="c16">Ho et al., 2023</xref>, <xref ref-type="bibr" rid="c41">Scotti et al., 2023</xref>, <xref ref-type="bibr" rid="c7">Chen et al., 2023</xref>, <xref ref-type="bibr" rid="c2">Benchetrit et al., 2023</xref>, <xref ref-type="bibr" rid="c22">Kupershmidt et al., 2022</xref>]. These advances have largely leveraged deep learning techniques to interpret fMRI or MEG recordings, taking advantage of the fact that spatially separated clusters of neurons have distinct visual and semantic response properties [<xref ref-type="bibr" rid="c34">Rakhimberdina et al., 2021</xref>]. Due to the low resolution of fMRI and MEG, relative to single neurons, the most successful models heavily rely on extracting semantic content and use diffusion models to generate semantically similar images and videos. Some approaches combine low-level perceptual (retinotopic) and semantic information in separate modules to achieve even better image similarity [<xref ref-type="bibr" rid="c37">Ren et al., 2021</xref>, <xref ref-type="bibr" rid="c29">Ozcelik and VanRullen, 2023</xref>, <xref ref-type="bibr" rid="c41">Scotti et al., 2023</xref>]. However, the pixel-level similarities are still relatively low. These methods are highly useful in humans, but their focus on semantic content may make them less useful when applied to non-human subjects or when using the reconstructed images to investigate visual processing.</p>
<p>Less attention has been given to image reconstruction from non-human brains. This is surprising given the advantages of large-scale single-cell-resolution recording techniques available in animal models, particularly mice. In the past, reconstructions using linear summation of receptive fields or Gabor filters have shown some success using responses from retinal ganglion cells [<xref ref-type="bibr" rid="c3">Brackbill et al., 2020</xref>], thalamo-cortical neurons in lateral genicular nucleus [<xref ref-type="bibr" rid="c45">Stanley et al., 1999</xref>], and primary visual cortex [<xref ref-type="bibr" rid="c13">Garasto et al., 2019</xref>, <xref ref-type="bibr" rid="c56">Yoshida and Ohki, 2020</xref>]. Recently, deep nonlinear neural networks have been used with promising results to reconstruct static images from retina [<xref ref-type="bibr" rid="c57">Zhang et al., 2020</xref>, <xref ref-type="bibr" rid="c23">Li et al., 2023</xref>] and in particular from monkey V4 extracellular recordings [<xref ref-type="bibr" rid="c23">Li et al., 2023</xref>, <xref ref-type="bibr" rid="c31">Pierzchlewicz et al., 2023</xref>].</p>
<p>Here, we present a method for the reconstruction of 10-second movie clips using two-photon calcium imaging data recorded in mouse V1 [<xref ref-type="bibr" rid="c49">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c50">2024</xref>]. Our method takes advantage of a state-of-the-art (SOTA) dynamic neural encoding model (DNEM) [<xref ref-type="bibr" rid="c1">Baikulov, 2023</xref>] which predicts neuronal activity based on video input as well as behavior (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Our method allows us to successfully reconstruct videos despite the fact that V1 neuronal activity in awake mice is heavily modulated by behavioral factors such as running speed [<xref ref-type="bibr" rid="c26">Niell and Stryker, 2010</xref>] and pupil diameter (correlated with arousal; [<xref ref-type="bibr" rid="c36">Reimer et al., 2014</xref>]). We then quantify the spatio-temporal limits of this reconstruction approach and identify key aspects of our method necessary for optimal performance.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Video reconstruction from neuronal activity in mouse V1 (data provided by the Sensorium 2023 competition; [<xref ref-type="bibr" rid="c49">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c50">2024</xref>]) using a state-of-the-art (SOTA) dynamic neural encoding model (DNEM; [<xref ref-type="bibr" rid="c1">Baikulov, 2023</xref>]). A) SOTA DNEMs predict neuronal activity from mouse primary visual cortex, given a video and behavioural input. B) We use a SOTA DNEM to reconstruct part of the input video given neuronal population activity, using gradient descent to optimize the input. C) Poisson negative log likelihood loss across training steps between ground truth neuronal activity and predicted neuronal activity in response to videos. Left: all 50 videos from 5 mice for one model. Right: average loss across all videos for 7 model instances. D) Spatio-temporal (pixel-by-pixel) correlation between reconstructed video and ground truth video.</p></caption>
<graphic xlink:href="599691v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2">
<label>2</label>
<title>Video reconstruction using state-of-the-art dynamic neural encoding models</title>
<p>We used publicly available data provided by the Sensorium 2023 competition [<xref ref-type="bibr" rid="c49">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c50">2024</xref>]. The data included movies that were presented to mice and the evoked activity of V1 neurons along with pupil position, pupil diameter, and running speed. The neuronal activity was measured using two-photon imaging of GCaMP6s [<xref ref-type="bibr" rid="c5">Chen et al., 2013</xref>] fluorescence from 10 mice, with ≈ 8000 neurons from each mouse. In total, we reconstructed ten 10s natural movies from 5 mice.</p>
<p>We used the winning model of the Sensorium 2023 competition which achieved a score of 0.301 ([<xref ref-type="bibr" rid="c1">Baikulov, 2023</xref>, <xref ref-type="bibr" rid="c50">Turishcheva et al., 2024</xref>] single trial correlation between predicted and ground truth neuronal activity; <xref rid="fig1" ref-type="fig">Figure 1A</xref> and <xref rid="figS1" ref-type="fig">Figure S1A-C</xref>). This state-of-the-art (SOTA) dynamic neural encoding model (DNEM) was composed of three parts: core, cortex and readout. The model takes the video as input with the behavioral data (pupil position, pupil diameter, and running speed) broadcast to four additional channels of the video. The original model weights were not used to avoid reconstructing movies the model was trained on. Instead, we retrained 7 instances of the model using the same training data, which did not include the movies reserved for reconstruction. Beyond this point the weights of the model were frozen, i.e. not influenced by future movie presentations.</p>
<p>To reconstruct the videos presented to mice we iteratively optimized an initially blank input video to the SOTA DNEM until the predicted activity in response to this input matched the ground truth recorded neuronal activity. To achieve this we used an input optimization through gradient descent approach inspired by the optimization of maximally exciting images [<xref ref-type="bibr" rid="c52">Walker et al., 2019</xref>] and the reconstruction of static images from monkey V4 extracellular recordings [<xref ref-type="bibr" rid="c31">Pierzchlewicz et al., 2023</xref>]. The input videos were initialized as uniform grey values and the behavioral parameters (<xref rid="figS1" ref-type="fig">Figure S1A</xref>) were added as additional channels, i.e. these were not reconstructed but given. The neuronal activity in response to the input video was predicted using the SOTA DNEM for a sliding window of 32 frames (1.067 sec) with a stride of 8 frames. We saw slightly better results with a stride of 2 frames but, in our case, this did not warrant the increase in training time. For each window, the difference between the predicted and ground truth responses was calculated and this loss backpropagated to the pixels of the input video to get the gradient of the loss with respect to each pixel. In effect, the input pixels were thus treated as if they were model weights. The gradients for each pixel were then averaged across all windows and the pixels of the input video updated accordingly (See Supplementary Algorithm 1).</p>
<p>The data from the Sensorium competition provided the activity of neurons within a 630 by 630 <italic>µ</italic>m field of view for each mouse, i.e. covering roughly one-fifth of mouse V1. Due to the retinotopic organization of V1 we therefore did not expect to get good reconstructions of the entire video frame. However, gradients still propagated to the full video frame and produced non-sensical results along the periphery of the video frames. Inspired by previous work [<xref ref-type="bibr" rid="c25">Mordvintsev et al., 2018</xref>, <xref ref-type="bibr" rid="c54">Willeke et al., 2023</xref>] we therefore decided to apply a mask during training and evaluation. To generate these masks, we optimized a transparency layer placed at the input to the SOTA DNEM. High values are given to pixels that contribute to the accurate prediction of neuronal activity and represent the collective receptive field of the neural population. This mask was applied during the optimization of the reconstructed movies (training mask: binarized with threshold <italic>α</italic> = 0.5) and applied again to the final reconstruction (evaluation mask: binarized with threshold <italic>α</italic> = 1) (See Supplementary Algorithm 2).</p>
<p>As the loss between predicted (<xref rid="figS1" ref-type="fig">Figure S1D</xref>) and ground truth responses (<xref rid="figS1" ref-type="fig">Figure S1B</xref>) decreased, the similarity between the reconstructed and ground truth input video increased (<xref rid="fig1" ref-type="fig">Figure 1C-D</xref>). We generated 7 separate reconstructions from 7 neural encoding models (trained on the same data) and averaged them. Finally, we applied a 3D Gaussian filter with sigma 0.5 pixels to remove the remaining static noise and applied the evaluation mask. The Gaussian filter was not applied when evaluating spatial or temporal resolution (<xref rid="fig4" ref-type="fig">Figure 4</xref> and <xref rid="figS2" ref-type="fig">Figure S2</xref>).</p>
<sec id="s2a">
<label>2.1</label>
<title>High-quality video reconstruction</title>
<p>As can be seen in <xref rid="fig2" ref-type="fig">Figure 2</xref> and Supplementary Video 1, the reconstructed videos capture much of the spatial and temporal dynamics of the original input video. To evaluate performance of the video reconstructions we correlated either all pixels from all time points between ground truth and reconstructed videos (Pearson’s correlation r = 0.563; to quantify temporal and spatial similarity), or the average correlation between all sets of frames (Pearson’s correlation r = 0.500; to quantify just spatial similarity)(<xref rid="fig2" ref-type="fig">Figure 2B</xref> and <xref rid="figS1" ref-type="fig">Figure S1E</xref>). Importantly, this represents a ≈ 2x improvement over previous static image reconstructions from V1 in awake mice (image correlation 0.23 +/-0.02 s.e.m for awake mice) [<xref ref-type="bibr" rid="c56">Yoshida and Ohki, 2020</xref>] over a similar retinotopic area (≈ 60<sup>°</sup> diameter) while also capturing temporal dynamics (<xref rid="tbl1" ref-type="table">Table 1</xref>).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Benchmarking against previous natural image reconstructions from mouse visual cortex. Correlation r values are given as mean and std across mice (except for [<xref ref-type="bibr" rid="c13">Garasto et al., 2019</xref>], where they are given as mean and std across reconstructed images).</title></caption>
<graphic xlink:href="599691v3_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Reconstruction performance. A) Three reconstructions of 10s videos from different mice (see Supplementary Video 1 for full set: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/pD4AhAHAx9k">YouTube link</ext-link>). Reconstructions have been luminance (mean pixel value across video) and contrast (standard deviation of pixel values across video) matched to ground truth. B) The reconstructed videos have high correlation to ground truth in both spatio-temporal correlation (mean Pearson’s correlation r = 0.563 with 95% CIs 0.534 to 0.593, t-test between ground truth and random video p = 8.66 * 10<sup>−45</sup>, n = 50 videos from 5 mice) and mean frame correlation (mean Pearson’s correlation r = 0.500 with 95% CIs 0.468 to 0.532, t-test between ground truth and random video p = 3.27 * 10<sup>−38</sup>, n = 50 videos from 5 mice).</p></caption>
<graphic xlink:href="599691v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Reconstruction quality, however, was not consistent across movies (<xref rid="fig2" ref-type="fig">Figure 2B</xref>) or constant throughout the 10 second videos (<xref rid="figS1" ref-type="fig">Figure S1E</xref>). We therefore investigated what factors may cause these fluctuations by correlating video motion energy, contrast and luminance, as well as running speed, pupil diameter and eye movement with frame correlation. We found that only video motion energy and contrast correlated with frame correlation, but only to a moderate degree (<xref rid="figS3" ref-type="fig">Supplementary Figure S3</xref>).</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Ensembling</title>
<p>We found that the 7 instances of the SOTA DNEMs by themselves performed similarly in terms of reconstructed video correlation (<xref rid="fig1" ref-type="fig">Figure 1D</xref>), but that this correlation was significantly increased by taking the average across reconstructions from different models (<xref rid="fig3" ref-type="fig">Figure 3</xref>) – A technique known as bagging, and more generally ensembling [<xref ref-type="bibr" rid="c4">Breiman, 1996</xref>]. Individual models produced reconstructions with high-frequency noise in the temporal and spatial domains. We therefore think the increase in performance from ensembling is mostly an effect of averaging out this high-frequency noise.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Model ensembling. Mean video correlation is improved when predictions from multiple models are averaged. Dashed lines are individual animals, solid line is mean. One-way repeated measures ANOVA p = 1.11 * 10<sup>−16</sup>. Bonferroni corrected paired ttest outcomes between consecutive ensemble sizes are all p &lt; 0.001, n = 5 mice.</p></caption>
<graphic xlink:href="599691v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In this paper we averaged over 7 model instances which gave a performance increase of 28.3%, but the largest gain in performance, 13.8%, came from averaging across just 2 models (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Doubling the number of models to 4 increased the performance by another 8.12%. Overall, although ensembling over models trained on separate data splits is a computationally expensive method, it substantially improved reconstruction quality.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Not all spatial and temporal frequencies are reconstructed equally</title>
<p>While the reconstructed videos achieve high correlation to ground truth, it is not entirely clear if the remaining deviations are due to the limitations of the model or arise from the recorded neurons themselves. To assess the resolution limits of our reconstruction process, we assessed the model’s ability to reconstruct synthetic stimuli at varying spatial and temporal resolutions in a noise-free scenario.</p>
<p>To quantify which spatial and temporal frequencies our reconstruction approach is able to capture we used a Gaussian noise stimulus set generated using a Gaussian process (<ext-link ext-link-type="uri" xlink:href="https://github.com/TomGeorge1234/gp_video">https://github.com/TomGeorge1234/gp_video</ext-link>; <xref rid="fig4" ref-type="fig">Figure 4A</xref>). The dataset consisted of 49, 2 second, 36 by 36 pixel videos at 30 Hz, which varied in the spatial and temporal length constants. As we did not have ground truth neuronal activity in response to this stimulus set, we first predicted the neuronal responses given these videos using the ensembled SOTA DNEMs. We then used gradient descent to reconstruct the original input using these predicted neuronal responses as the target. In this way, we generated reconstructions in an ideal case with no biological noise and assuming the SOTA DNEM perfectly predicts neuronal activity (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). This means the video reconstruction quality loss reflects the inefficiency of the reconstruction process itself without the additional loss or transformation of information by processes such as top-down modulation, e.g. predictive coding or selective feature attention (see Discussion). We found that the reconstruction process failed at high spatial frequencies (&lt; 1 pixel, or &lt; 3.4<sup>°</sup> retinotopy) and performed worse at high temporal frequencies (&lt; 1 frame, or &lt; 30 Hz)(<xref rid="fig4" ref-type="fig">Figure 4C</xref> and Supplementary Video 2). We repeated this analysis using full-field high-contrast square gratings drifting in the four cardinal directions and similarly found that high-spatial and temporal frequencies were not reconstructed as well as low-spatial and temporal frequency gratings (<xref rid="figS2" ref-type="fig">Figure S2</xref>).</p>
<p>To test if model ensembling improves Gaussian noise reconstruction quality across all spatial and temporal length constants uniformly, we subtracted the average video correlation across the six model instances from the video correlation of the average video (i.e. ensembled video reconstruction minus unensembled video reconstruction; <xref rid="fig4" ref-type="fig">Figure 4D</xref>). We found that in particular short temporal and spatial length constant stimuli improved in correlation, supporting our hypothesis that ensembling mitigates the high-frequency noise we observed in the reconstruction from individual models.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Neuronal population size</title>
<p>In order to design future <italic>in vivo</italic> experiments to investigate visual processing using our video reconstruction approach, it would be useful to know how reconstruction performance scales with the number of recorded neurons. This is vital for prioritizing experimental parameters such as weighing between sampling density within a similar retinotopic area and retinotopic coverage to maximize both video reconstruction quality and visual coverage. We therefore performed an <italic>in silico</italic> ablation experiment, dropping either 50, 75% or 87.5% of the total recorded population of ≈ 8000 neurons per mouse by setting their activity to 0 (<xref rid="fig5" ref-type="fig">Figure 5</xref>). We found that dropping 50% of the neurons reduced the video correlation by only 9.8% while dropping 75% reduced the performance by 23.8%. We would therefore argue that ≈ 4000-8000 neurons within a 630 by 630 <italic>µ</italic>m area (≈ 10000-20000 neurons/mm<sup>2</sup>) of mouse V1 would prove a blanance when compromising between density and 2D coverage.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Reconstruction of Gaussian noise across the spatial and temporal spectrum using predicted activity. A) Example Gaussian noise stimulus set with evaluation mask for one mouse. Shown is the last frame of a 2 second video. B) Reconstructed Gaussian stimuli with SOTA DNEM predicted neuronal activity as the target (see also Supplementary Video 2: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/pD4AhAHAx9k">YouTube link</ext-link>). C) Pearson’s correlation between ground truth (A) and reconstructed videos (B) across the range of spatial and temporal length constants. For each stimulus type the average correlation across 5 movies reconstructed from the SOTA DNEM of 3 mice is given. D) Ensembling effect for each stimulus. Video correlation for ensembled prediction (average videos from 7 model instances) minus the mean video correlation across the 7 individual model instances.</p></caption>
<graphic xlink:href="599691v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Video reconstruction using fewer neurons (i.e. population ablation) leads to lower reconstruction quality. Dashed lines are individual animals, solid line is mean. One-way repeated measures ANOVA p = 1.58* 10<sup>−12</sup>. Bonferroni corrected paired t-test outcomes between consecutive drops in population size are all p &lt; 0.001, n = 5 mice.</p></caption>
<graphic xlink:href="599691v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<sec id="s3a">
<label>3.1</label>
<title>Stimulus identification vs reconstruction</title>
<p>Stimulus identification, i.e. identifying the most likely stimulus from a constrained set, has been a popular approach for quantifying whether a population of neurons encodes the identity of a particular stimulus [<xref ref-type="bibr" rid="c12">Földiák, 1993</xref>, <xref ref-type="bibr" rid="c20">Kay et al., 2008</xref>]. This approach has, for instance, been used to decode frame identity within a movie [<xref ref-type="bibr" rid="c8">Deitch et al., 2021</xref>, <xref ref-type="bibr" rid="c55">Xia et al., 2021</xref>, <xref ref-type="bibr" rid="c39">Schneider et al., 2023</xref>, <xref ref-type="bibr" rid="c6">Chen et al., 2024</xref>]. Some of these approaches have also been used to reorder the frames of the ground truth movie [<xref ref-type="bibr" rid="c39">Schneider et al., 2023</xref>] based on the decoded frame identity. Importantly, stimulus identification methods are distinct from stimulus reconstruction where the aim is to recreate what the sensory content of a neuronal code is in a way that generalizes to new sensory stimuli [<xref ref-type="bibr" rid="c34">Rakhimberdina et al., 2021</xref>]. This is inherently a more demanding task because the range of possible solutions is much larger. Although stimulus identification is a valuable tool for understanding the information content of a population code, stimulus reconstruction promises a more generalizable approach.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Comparison to other reconstruction methods</title>
<p>There has recently been a growing number of publications in the field of image reconstruction, primarily from fMRI data, and a comprehensive review of all the approaches is outside the scope of this paper. However, we will briefly summarize the most common approaches and how they relate to our own method. In general, image reconstruction methods can be categorized into one of four groups: direct decoding models, encoder-decoder models, invertible encoding models, and encoder model input optimization.</p>
<p>Direct decoders, directly decode the input image/videos from neuronal activity with deep neuronal networks [<xref ref-type="bibr" rid="c42">Shen et al., 2019a</xref>, <xref ref-type="bibr" rid="c57">Zhang et al., 2020</xref>, <xref ref-type="bibr" rid="c23">Li et al., 2023</xref>]. When training direct decoders, the decoders can be pretrained [<xref ref-type="bibr" rid="c37">Ren et al., 2021</xref>] or additional constraints can be added to the loss function to encourage the decoder to produce images that adhere to learned image statistics [<xref ref-type="bibr" rid="c42">Shen et al., 2019a</xref>, <xref ref-type="bibr" rid="c22">Kupershmidt et al., 2022</xref>]. A direct decoder approach has been used for video reconstruction in mice [<xref ref-type="bibr" rid="c6">Chen et al., 2024</xref>], but in that case, the training and test movies were the same, meaning it is unclear if out-of-training set generalization was achieved (a key distinction between sensory reconstruction and stimulus identification, see previous section).</p>
<p>In encoder-decoder models the aim is to combine separately trained brain encoders (brain activity to latent space) and decoders (latent space to image/video). Recently this approach has become particularly popular because it allows the use of SOTA generative image models such as stable diffusion [<xref ref-type="bibr" rid="c38">Rombach et al., 2021</xref>, <xref ref-type="bibr" rid="c47">Takagi and Nishimoto, 2023</xref>, <xref ref-type="bibr" rid="c41">Scotti et al., 2023</xref>, <xref ref-type="bibr" rid="c7">Chen et al., 2023</xref>, <xref ref-type="bibr" rid="c2">Benchetrit et al., 2023</xref>]. The encoder part of the models are first trained to translate brain activity into a latent space that the pretrained generative networks can interpret. Because these latent spaces are often conditioned on semantic information, this lends itself to separate processing of low-level visual and high-level semantic information from brain activity [<xref ref-type="bibr" rid="c41">Scotti et al., 2023</xref>].</p>
<p>Invertible encoding models are encoding models which, once trained to predict neuronal activity, can implicitly be inverted to predict sensory input given brain activity. We would also include those models in this class which first compute the receptive field or preferred stimulus of neurons (or voxels) and reconstruct the input as the weighted sum of the receptive fields by their activity [<xref ref-type="bibr" rid="c45">Stanley et al., 1999</xref>, <xref ref-type="bibr" rid="c48">Thirion et al., 2006</xref>, <xref ref-type="bibr" rid="c13">Garasto et al., 2019</xref>, <xref ref-type="bibr" rid="c3">Brackbill et al., 2020</xref>, <xref ref-type="bibr" rid="c56">Yoshida and Ohki, 2020</xref>, <xref ref-type="bibr" rid="c27">Nishimoto et al., 2011</xref>]. The down-side of this approach is that these encoding models generally under perform in terms of capturing the coding properties of neurons compared to more complex deep neural networks [<xref ref-type="bibr" rid="c54">Willeke et al., 2023</xref>].</p>
<p>Encoder input optimization, also involves first training an encoder which predicts the activity of neurons or voxels given sensory input. Once trained the encoder is fixed and the input to the network is optimized using backpropagation until the predicted activity matches the observed activity [<xref ref-type="bibr" rid="c31">Pierzchlewicz et al., 2023</xref>]. Unlike with invertible encoding models any SOTA neuronal encoding model can be used. But like invertible models, the networks are not specifically trained to reconstruct images so they may be less likely to extrapolate information encoded by the brain by learning general image statistics.</p>
<p>Although outlined here as 4 distinct classes these approaches can be combined. For instance, Encoding input optimization can be combined with image diffusion [<xref ref-type="bibr" rid="c31">Pierzchlewicz et al., 2023</xref>] and in principle invertible models could also be combined in such a way.</p>
<p>We chose to pursue a pure encoder input optimization approach for single cell mouse visual cortex activity for two reasons. First, there have been considerable advances in the performance of neuronal encoding models for dynamic visual stimuli [<xref ref-type="bibr" rid="c44">Sinz et al., 2018</xref>, <xref ref-type="bibr" rid="c53">Wang et al., 2023</xref>, <xref ref-type="bibr" rid="c50">Turishcheva et al., 2024</xref>] and we aimed to take advantage of these developments. Second, the addition of a generative decoder trained to producing high quality images brings with it the risk of extrapolating information based on general image statistics rather than interpreting what the brain is representing. In some cases, the brain may not be encoding coherent images and in those cases we would argue image reconstruction should fail, rather than producing an image when only the semantic information is present.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Key contributions and limitations</title>
<p>We demonstrate high-quality video reconstruction from mouse V1 using SOTA DNEMs to iteratively optimize the input video to match the resulting predicted activity with the recorded neuronal activity. Key to achieving high-quality reconstructions is model ensembling and using a large enough number of recorded neurons over a given retinotopic area.</p>
<p>While we averaged the video reconstructions from several models, an alternative method would be to average the gradients calculated by multiple models at each epoch, as has been done for the generation of maximally exciting images in the past [<xref ref-type="bibr" rid="c52">Walker et al., 2019</xref>]. However, this requires a large amount of GPU memory when using video models and is likely not practical with most hardware limitations. However, there might be situations in which averaging gradients yields better reconstructions. For instance, there may be multiple solutions for the activation pattern of a neural population, e.g. if their responses are translation/phase invariant [<xref ref-type="bibr" rid="c17">Ito et al., 1995</xref>, <xref ref-type="bibr" rid="c46">Tacchetti et al., 2018</xref>]. In such a case, averaging ‘misaligned’ reconstructions from multiple models might degrade overall quality.</p>
<p>The SOTA DNEM we used takes video data at an angular resolution of 3.4 <sup>°</sup>/pixels at the center of the screen which is about 3x worse than the visual acuity of mice (≈ 0.5 cycles/<sup>°</sup> [<xref ref-type="bibr" rid="c33">Prusky and Douglas, 2004</xref>]). As our model can reconstruct Gaussian noise stimuli down to a spatial length constant of 1 pixel, and drifting gratings up to a spatial frequency of 0.071 cycles/<sup>°</sup>, there is still some potential for improving spatial resolution. To close this gap and achieve reconstructions equivalent to the limit of mouse visual acuity, a different dataset and model would likely need to be developed. However, the frame rate of the videos the SOTA DNEM takes as input (30 Hz) is faster than the flicker fusion frequency of mice (14 Hz [<xref ref-type="bibr" rid="c28">Nomura et al., 2019</xref>]) and our tests with Gaussian noise and drifting grating stimuli show that the temporal resolution of reconstruction is close to this expected limit. Future efforts should therefore focus on the spatial resolution of video reconstruction rather than the temporal resolution.</p>
<p>It is, however, unclear how closely the representation of vision by the brain is expected to match the actual input. There are a number of visual processing phenomena that have previously been identified which leads us to suspect that some deviations between video reconstructions and ground truth input are to be expected. One such phenomenon is predictive coding [<xref ref-type="bibr" rid="c35">Rao and Ballard, 1999</xref>, <xref ref-type="bibr" rid="c11">Fiser et al., 2016</xref>]. It is possible that the unexpected parts of visual stimuli are sharper and have higher contrast compared to the expected parts when reconstructed from neuronal activity. Alternatively, perceptual learning is a phenomenon where visual stimulus detection or discriminability is enhanced through prolonged training [<xref ref-type="bibr" rid="c24">Li, 2015</xref>] and is associated with changes in the tuning distribution of neurons in the visual system [<xref ref-type="bibr" rid="c15">Goltstein et al., 2013</xref>, <xref ref-type="bibr" rid="c32">Poort et al., 2015</xref>, <xref ref-type="bibr" rid="c18">Jurjut et al., 2017</xref>, <xref ref-type="bibr" rid="c40">Schumacher et al., 2022</xref>]. Similarly, selective feature attention can modulate the response amplitude of neurons that have a preference for the features that are currently being attended to [<xref ref-type="bibr" rid="c19">Kanamori and Mrsic-Flogel, 2022</xref>]. Visual task engagement and training could therefore alter the accuracy and biases of what features of a video can accurately be reconstructed from the neuronal activity.</p>
<p>Such visual processing phenomena are likely difficult to investigate using existing fMRI-based reconstruction approaches, due to the low spatial and temporal resolution of the data. Additionally, many of these fMRI-based reconstruction approaches rely on the use of pretrained generative diffusion models to achieve more naturalistic and semantically interpretable images [<xref ref-type="bibr" rid="c47">Takagi and Nishimoto, 2023</xref>, <xref ref-type="bibr" rid="c29">Ozcelik and VanRullen, 2023</xref>, <xref ref-type="bibr" rid="c41">Scotti et al., 2023</xref>, <xref ref-type="bibr" rid="c7">Chen et al., 2023</xref>], but very likely at the cost of introducing information that may not be present in the actual neuronal representation. In contrast, our video reconstruction approach using single-cell resolution recordings, without a pretrained generative model, provides a more accurate method to investigate visual processing phenomena such as predictive coding, perceptual learning, and selective feature attention.</p>
<p><bold>In conclusion</bold>, we reconstruct videos presented to mice based on the activity of neurons in the mouse visual cortex, with a ≈ 2-fold improvement in pixel-by-pixel correlation compared to previous static image reconstruction methods. This paves the way to using movie reconstruction as a tool to investigate a variety of visual processing phenomena.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Source data</title>
<p>The data was provided by the Sensorium 2023 competition [<xref ref-type="bibr" rid="c49">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c50">2024</xref>] and downloaded from <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/pollytur/Sensorium2023Data">https://gin.g-node.org/pollytur/Sensorium2023Data</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/pollytur/sensorium_2023_dataset">https://gin.g-node.org/pollytur/sensorium_2023_dataset</ext-link>. The data included grayscale movies presented to the mice at 30 Hz on a 31.8 by 56.5 cm monitor 15 cm from and perpendicular to the left eye. The movies were provided as spatially downsampled versions of the original screen resolution to 36 by 64 pixels, corresponding to an angular resolution of 3.4 <sup>°</sup>/pixel at the center of the screen. The pupil position and diameter were recorded at 20 Hz and the running at 100 Hz. The neuronal activity was measured using two-photon imaging [<xref ref-type="bibr" rid="c9">Denk et al., 1990</xref>] of GCaMP6s [<xref ref-type="bibr" rid="c5">Chen et al., 2013</xref>] fluorescence at 8 Hz, extracted and deconvolved using the CAIMAN pipeline [<xref ref-type="bibr" rid="c14">Giovannucci et al., 2019</xref>]. For each of the 10 mice, the activity of ≈ 8000 neurons was provided. The different data types were resampled to 30 Hz.</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>State-of-the-art dynamic neural encoding model</title>
<p>We used the winning model of the Sensorium 2024 competition, DwiseNeuro [<xref ref-type="bibr" rid="c49">Turishcheva et al., 2023</xref>, <xref ref-type="bibr" rid="c50">2024</xref>]. The code for the SOTA DNEM was downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/lRomul/">https://github.com/lRomul/</ext-link> sensorium. The full model consists of 3 main components: core, cortex, and readout. The core largely consisted of factorized 3D convolution blocks with residual connections, positional encoding [<xref ref-type="bibr" rid="c51">Vaswani et al., 2017</xref>] and SiLU activations [<xref ref-type="bibr" rid="c10">Elfwing et al., 2017</xref>] followed by spatial average pooling. The cortex consisted of three fully connected layers. The readout consisted of a 1D convolution for each mouse with a final Softplus nonlinearity, that gives activity predictions for all neurons of each mouse. The kernel of the input layer had size 16 with a dilation of 2 in the time dimension, so spanned 32 video frames.</p>
<p>The original ensemble of models consisted of 7 model instances trained on a 7-fold cross-validation split of all available Sensorium 2023 competition data (≈ 1 hour of training data and ≈ 8 min of cross-validation data per fold from each mouse). Each model instance was trained on 6 of 7 data folds, with different validation data excluded from training for each model. To allow ensembled reconstructions of videos without test set contamination we instead retrained the models with a shared validation fold, i.e. we retrained the models leaving out the same validation data for all 7 model instances. The only other difference in the training procedure was that we retrained the models using a batch size of 24 instead of 32, this did not change the performance of neuronal response prediction on the withheld data folds (mean validation fold predicted vs ground truth response correlation for original weights: 0.293; and retrained weights: 0.291). We also did not use model distillation, while the original model did (see <ext-link ext-link-type="uri" xlink:href="https://github.com/lRomul/sensorium">https://github.com/lRomul/sensorium</ext-link>).</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Additional visual stimuli</title>
<p>The Gaussian noise stimuli were downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/TomGeorge1234/gp_video">https://github.com/TomGeorge1234/gp_video</ext-link> and spanned a range of 0 to 32 pixels in spatial length constant and 0 to 32 frames in temporal length constant used in the Gaussian process. The drifting grating stimuli were produced using PsychoPy [<xref ref-type="bibr" rid="c30">Peirce et al., 2019</xref>] and ranged from 0.5 to 0.062 cycles/degree and 0.5 to 0 cycles/second, with 2 seconds of movie for each cardinal direction. These ranges were chosen to avoid aliasing effects in the 36 by 64 pixel videos. The highest temporal frequency corresponds to a flicker stimulus.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Mask training</title>
<p>To generate the transparency masks we used an alpha blending approach inspired by [<xref ref-type="bibr" rid="c25">Mordvintsev et al., 2018</xref>, <xref ref-type="bibr" rid="c54">Willeke et al., 2023</xref>]. A transparency layer was placed at the input to the SOTA DNEM. This transparency layer was used to alpha blend the true video <italic>V</italic> with another randomly selected background video <italic>BG</italic> from the data:
<disp-formula id="eqn1">
<graphic xlink:href="599691v3_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>α</italic> is the 2D transparency mask and <italic>V</italic><sub><italic>BG</italic></sub> is the blended input video. This mask was optimized using stochastic gradient descent (for 1000 epochs with learning rate 10) with mean squared error (<italic>MSE</italic>) loss between the true responses <italic>y</italic> and the predicted responses <italic>ŷ</italic> scaled by the average weight of the transparency mask <inline-formula><inline-graphic xlink:href="599691v3_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>:
<disp-formula id="eqn2">
<graphic xlink:href="599691v3_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="599691v3_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>n</italic> is the total number of neurons. The mask was initialized as uniform noise between 0 and 0.05. At each epoch the neuronal activity in response to a randomly selected 32 frame video segment from the training set was predicted and the gradients of the loss (<xref ref-type="disp-formula" rid="eqn3">Equation 3</xref>) with respect to the pixels in the transparency mask <italic>α</italic> were calculated for each video frame. The gradients were normalized by their matrix norm, clipped to between -1 and 1 and averaged across frames. The gradients were smoothed with a 2D Gaussian kernel of <italic>σ</italic> = 5 and subtracted from the transparency mask. The transparency mask was only calculated using one SOTA DNEM instance using its validation fold. See Supplementary Algorithm 2.</p>
<p>The transparency mask was thresholded and binarized at 0.5 for the masked gradients ∇<sub><italic>masked</italic></sub> or 1 for the masked videos for evaluation <italic>V</italic><sub><italic>eval</italic></sub>:
<disp-formula id="eqn4">
<graphic xlink:href="599691v3_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="599691v3_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where ∇ is the gradients of the loss with respect to each pixel in the video and <italic>V</italic> is the reconstructed video before masking. These masks were trained independently for each mouse using one model instance with the original weights of the model <ext-link ext-link-type="uri" xlink:href="https://github.com/lRomul/sensorium">https://github.com/lRomul/sensorium</ext-link>, not the retrained models used in the rest of this paper to reconstruct the videos.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Video reconstruction</title>
<p>To reconstruct the input video we initialized the video as uniform gray values and concatenated the ground truth behavioral parameters. The SOTA DNEM took 32 frames at a time and we shifted this window by 8 frames until all frames of the whole 10s video were covered. For each 32-frame window, the Poisson negative log-likelihood loss between the predicted and true neuronal responses was calculated:
<disp-formula id="eqn6">
<graphic xlink:href="599691v3_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>ŷ</italic> are the predicted responses and <italic>y</italic> are the ground truth responses. The gradients of the loss with respect to each pixel of the input video were calculated for each window of frames and averaged across all windows. The gradients for each pixel were normalized by the matrix norm across all gradients and clipped to between -1 and 1. The gradients were masked (<xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>) and applied to the input video using Adam without second order momentum [<xref ref-type="bibr" rid="c21">Kingma and Ba, 2014</xref>] (<italic>β</italic><sub>1</sub> = 0.9) for 1000 epochs and a learning rate of 1000, with a learning rate warm-up for the first 10 epochs. After each epoch, the video was clipped to between 0 and 255. The optimization was run for 1000 epochs. 7 reconstructions from 7 model instances were averaged, denoised with a 3D Gaussian filter <italic>σ</italic> = 0.5 (unless specified otherwise), and masked with the evaluation mask. See Supplementary Algorithm 1. Optimizing each 10-second video with one model instance for 1000 epochs took ≈ 60 min using a desktop with an RTX4070 GPU.</p>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Reconstruction quality assessment</title>
<p>To evaluate the similarity between reconstructed and ground truth videos, we used the mean Pearson’s correlation between pixels of corresponding frames to evaluate spatial similarity:
<disp-formula id="eqn7">
<graphic xlink:href="599691v3_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>f</italic> is the number of frames, and <italic>x</italic><sub><italic>i</italic></sub> and <inline-formula><inline-graphic xlink:href="599691v3_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the ground truth and reconstructed frames. To evaluate temporal and spatial similarity between ground truth and reconstructed videos we used the Pearson’s correlation between all pixels of the whole movie:
<disp-formula id="eqn8">
<graphic xlink:href="599691v3_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
</sec>
</sec>

</body>
<back>
<sec id="s6">
<title>A Appendix / supplemental material</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplementary Figure S1:</label>
<caption><p>Summary ethogram of SOTA DNEM inputs, output predictions, and video reconstruction over time for three videos from three mice (same as <xref rid="fig2" ref-type="fig">Figure 2A</xref>). A) Top: motion energy of the input video. Bottom: pupil diameter and running speed of the mouse during the video. B) Ground truth neuronal activity. C) Predicted neuronal activity in response to input video and behavioural parameters. D) predicted neuronal activity given reconstructed video and ground truth behaviour as input. E) Frame by frame correlation between reconstructed and ground truth video.</p></caption>
<graphic xlink:href="599691v3_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplementary Figure S2:</label>
<caption><p>Reconstruction of drifting grating stimuli with different spatial and temporal frequencies using predicted activity. A) Example drifting grating stimuli (rightwards moving) masked with the evaluation mask for one mouse. Shown is the 31st frame of a 2 second video. B) Reconstructed drifting grating stimuli with SOTA DNEM predicted neuronal activity as the target (see also Supplementary Video 3: <ext-link ext-link-type="uri" xlink:href="https://youtu.be/pD4AhAHAx9k">YouTube link</ext-link>). C) Pearson’s correlation between ground truth (A) and reconstructed videos (B) across the range of spatial and temporal frequencies. For each stimulus type the average correlation across 4 directions (up, down, left, right) reconstructed from the SOTA DNEM of 1 mouse is given. Interestingly, video correlation at 15 cycles/second (half the video frame rate of 30 Hz) is much higher than 7.5 cycles/second. This is an artifact of using predicted responses rather than true neural responses. The DNEM input layer convolution has dilation 2. The predicted activity is therefore based on every second frame, with the effect that the activity is predicted as the response of 2 static images which are then interleaved.</p></caption>
<graphic xlink:href="599691v3_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplementary Figure S3:</label>
<caption><p>Reconstruction performance correlates with video motion energy and frame contrast but not with behavioral parameters. Pearson’s correlation between mean frame correlation per movie, and 3 movie parameters and 3 behavioral parameters. Linear fit as black line.</p></caption>
<graphic xlink:href="599691v3_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<statement id="alg1">
<label>Algorithm 1</label>
<p>Movie reconstruction</p>
<p><fig position="float" fig-type="figure">
<graphic xlink:href="599691v3_alg1.tif" mime-subtype="tiff" mimetype="image"/>
</fig></p>
</statement>
<statement id="alg2">
<label>Algorithm 2</label>
<p>Mask training</p>
<p><fig position="float" fig-type="figure">
<graphic xlink:href="599691v3_alg2.tif" mime-subtype="tiff" mimetype="image"/>
</fig></p>
</statement>
</sec>
<ack>
<title>Acknowledgments and Disclosure of Funding</title>
<p>We would like to thank Emmanuel Bauer, Sandra Reinert and the anonymous reviewers for useful input and discussions, and Tom George for the Gaussian noise stimulus set. T.W.M. is funded by The Wellcome Trust (219627/Z/19/Z; 214333/Z/18/Z) and Gatsby Charitable Foundation (GAT3755) and J.B. is funded by EMBO (ALTF 415-2024).</p>
</ack>
<sec id="d1e1186" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<label>5</label>
<title>Code</title>
<p>The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Joel-Bauer/movie_reconstruction_code">https://github.com/Joel-Bauer/movie_reconstruction_code</ext-link>.</p>
</sec>
</sec>
<sec id="suppd1e1186" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e1177">
<label>Supplementary Videos</label><caption><p>Supplementary Video 1: Reconstructed natural videos from mouse brain activity. Odd rows are ground truth (GT) movie clips presented to mice. Even rows are the reconstructed movies from the activity of <italic>≈</italic> 8000 V1 neurons. Reconstructed movies are smoothed (<italic>σ</italic> = 0.5 pixels), masked, and contrast (std) and luminance (mean) matched to ground truth movies.</p>
<p>Supplementary Video 2: Gaussian noise stimuli and reconstructions. Odd rows are ground truth (GT) video inputs to the model. Even rows are the reconstructed videos from the predicted neuronal activity for 1 mouse. Reconstructed movies are masked, and contrast (std) and luminance (mean) matched to ground truth videos.</p>
<p>Supplementary Video 3: Drifting grating stimuli and reconstructions. Odd rows are ground truth (GT) video inputs to the model. Even rows are the reconstructed videos from the predicted neuronal activity for 1 mouse. Reconstructed movies are masked, and contrast (std) and luminance (mean) matched to ground truth videos.</p></caption>
<media xlink:href="supplements/599691_file03.mp4"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><given-names>Ruslan</given-names> <surname>Baikulov</surname></string-name></person-group>. <data-title>Solution for Sensorium 2023 Competition (v23.11.22)</data-title>. <source>Zenodo</source>, <year>2023</year>. URL <ext-link ext-link-type="uri" xlink:href="https://github.com/lRomul/sensorium">https://github.com/lRomul/sensorium</ext-link>. <pub-id pub-id-type="doi">10.5281/zenodo.10155151</pub-id>, <month>nov</month> 2023.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Yohann</given-names> <surname>Benchetrit</surname></string-name>, <string-name><given-names>Hubert</given-names> <surname>Banville</surname></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name></person-group>. <article-title>Brain decoding: toward real-time reconstruction of visual perception</article-title>. <source>arXiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2310.19812</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nora</given-names> <surname>Brackbill</surname></string-name>, <string-name><given-names>Colleen</given-names> <surname>Rhoades</surname></string-name>, <string-name><given-names>Alexandra</given-names> <surname>Kling</surname></string-name>, <string-name><given-names>Nishal P</given-names> <surname>Shah</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>Alan M</given-names> <surname>Litke</surname></string-name>, and <string-name><given-names>EJ</given-names> <surname>Chichilnisky</surname></string-name></person-group>. <article-title>Reconstruction of natural images from responses of primate retinal ganglion cells</article-title>. <source>eLife</source>, <volume>9</volume>: <elocation-id>e58516</elocation-id>, <year>2020</year>. doi: <pub-id pub-id-type="doi">10.7554/elife.58516</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Leo</given-names> <surname>Breiman</surname></string-name></person-group>. <article-title>Stacked regressions</article-title>. <source>Machine Learning</source>, <volume>24</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>64</lpage>, <year>1996</year>. ISSN <issn>0885-6125</issn>. doi: <pub-id pub-id-type="doi">10.1007/bf00117832</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Tsai-Wen</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Trevor J.</given-names> <surname>Wardill</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>Stefan R.</given-names> <surname>Pulver</surname></string-name>, <string-name><given-names>Sabine L.</given-names> <surname>Renninger</surname></string-name>, <string-name><given-names>Amy</given-names> <surname>Baohan</surname></string-name>, <string-name><given-names>Eric R.</given-names> <surname>Schreiter</surname></string-name>, <string-name><given-names>Rex A.</given-names> <surname>Kerr</surname></string-name>, <string-name><given-names>Michael B.</given-names> <surname>Orger</surname></string-name>, <string-name><given-names>Vivek</given-names> <surname>Jayaraman</surname></string-name>, <string-name><given-names>Loren L.</given-names> <surname>Looger</surname></string-name>, <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name>, and <string-name><given-names>Douglas S.</given-names> <surname>Kim</surname></string-name></person-group>. <article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title>. <source>Nature</source>, <volume>499</volume>(<issue>7458</issue>):<fpage>295</fpage>–<lpage>300</lpage>, <year>2013</year>. ISSN <issn>0028-0836</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature12354</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ye</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Beech</surname></string-name>, <string-name><given-names>Ziwei</given-names> <surname>Yin</surname></string-name>, <string-name><given-names>Shanshan</given-names> <surname>Jia</surname></string-name>, <string-name><given-names>Jiayi</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Zhaofei</given-names> <surname>Yu</surname></string-name>, and <string-name><given-names>Jian K.</given-names> <surname>Liu</surname></string-name></person-group>. <article-title>Decoding dynamic visual scenes across the brain hierarchy</article-title>. <source>PLOS Computational Biology</source>, <volume>20</volume>(<issue>8</issue>):<fpage>e1012297</fpage>, <year>2024</year>. ISSN <issn>1553-734X</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1012297</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Zijiao</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Jiaxin</given-names> <surname>Qing</surname></string-name>, and <string-name><given-names>Juan Helen</given-names> <surname>Zhou</surname></string-name></person-group>. <article-title>Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity</article-title>. <source>arXiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2305.11675</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel</given-names> <surname>Deitch</surname></string-name>, <string-name><given-names>Alon</given-names> <surname>Rubin</surname></string-name>, and <string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name></person-group>. <article-title>Representational drift in the mouse visual cortex</article-title>. <source>Current Biology</source>, <volume>31</volume>(<issue>19</issue>):<fpage>4327</fpage>–<lpage>4339.e6,</lpage> <year>2021</year>. ISSN <issn>0960-9822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2021.07.062</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Winifried</given-names> <surname>Denk</surname></string-name>, <string-name><given-names>James H.</given-names> <surname>Strickler</surname></string-name>, and <string-name><given-names>Watt W.</given-names> <surname>Webb</surname></string-name></person-group>. <article-title>Two-Photon Laser Scanning Fluorescence Microscopy</article-title>. <source>Science</source>, <volume>248</volume>(<issue>4951</issue>):<fpage>73</fpage>–<lpage>76</lpage>, <year>1990</year>. ISSN <issn>0036-8075</issn>. doi: <pub-id pub-id-type="doi">10.1126/science.2321027</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Stefan</given-names> <surname>Elfwing</surname></string-name>, <string-name><given-names>Eiji</given-names> <surname>Uchibe</surname></string-name>, and <string-name><given-names>Kenji</given-names> <surname>Doya</surname></string-name></person-group>. <article-title>Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</article-title>. <source>arXiv</source>, <year>2017</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.1702.03118</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aris</given-names> <surname>Fiser</surname></string-name>, <string-name><given-names>David</given-names> <surname>Mahringer</surname></string-name>, <string-name><given-names>Hassana K</given-names> <surname>Oyibo</surname></string-name>, <string-name><given-names>Anders V</given-names> <surname>Petersen</surname></string-name>, <string-name><given-names>Marcus</given-names> <surname>Leinweber</surname></string-name>, and <string-name><given-names>Georg B</given-names> <surname>Keller</surname></string-name></person-group>. <article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>12</issue>):<fpage>1658</fpage>–<lpage>1664</lpage>, <year>2016</year>. ISSN <issn>1097-6256</issn>. doi: <pub-id pub-id-type="doi">10.1038/nn.4385</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Peter</given-names> <surname>Földiák</surname></string-name></person-group>. <chapter-title>The ‘Ideal Homunculus’: Statistical Inference from Neural Population Responses</chapter-title>, <source>Computation and Neural Systems</source>. pages <fpage>55</fpage>–<lpage>60</lpage>, <year>1993</year>. doi: <pub-id pub-id-type="doi">10.1007/978-1-4615-3254-5_9</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Stef</given-names> <surname>Garasto</surname></string-name>, <string-name><given-names>Wilten</given-names> <surname>Nicola</surname></string-name>, <string-name><given-names>Anil A.</given-names> <surname>Bharath</surname></string-name>, and <string-name><given-names>Simon R.</given-names> <surname>Schultz</surname></string-name></person-group>. <article-title>Neural Sampling Strategies for Visual Stimulus Reconstruction from Two-photon Imaging of Mouse Primary Visual Cortex</article-title>. <conf-name>2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)</conf-name>, <volume>00</volume>:<fpage>566</fpage>–<lpage>570</lpage>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1109/ner.2019.8716934</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea</given-names> <surname>Giovannucci</surname></string-name>, <string-name><given-names>Johannes</given-names> <surname>Friedrich</surname></string-name>, <string-name><given-names>Pat</given-names> <surname>Gunn</surname></string-name>, <string-name><given-names>Jérémie</given-names> <surname>Kalfon</surname></string-name>, <string-name><given-names>Brandon L</given-names> <surname>Brown</surname></string-name>, <string-name><given-names>Sue Ann</given-names> <surname>Koay</surname></string-name>, <string-name><given-names>Jiannis</given-names> <surname>Taxidis</surname></string-name>, <string-name><given-names>Farzaneh</given-names> <surname>Najafi</surname></string-name>, <string-name><given-names>Jeffrey L</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>Pengcheng</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>Baljit S</given-names> <surname>Khakh</surname></string-name>, <string-name><given-names>David W</given-names> <surname>Tank</surname></string-name>, <string-name><given-names>Dmitri B</given-names> <surname>Chklovskii</surname></string-name>, and <string-name><given-names>Eftychios A</given-names> <surname>Pnevmatikakis</surname></string-name></person-group>. <article-title>CaImAn an open source tool for scalable calcium imaging data analysis</article-title>. <source>eLife</source>, <volume>8</volume>:<elocation-id>e38173</elocation-id>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.7554/elife.38173</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Pieter M.</given-names> <surname>Goltstein</surname></string-name>, <string-name><given-names>Emily B. J.</given-names> <surname>Coffey</surname></string-name>, <string-name><given-names>Pieter R.</given-names> <surname>Roelfsema</surname></string-name>, and <string-name><given-names>Cyriel M. A.</given-names> <surname>Pennartz</surname></string-name></person-group>. <article-title>In Vivo Two-Photon Ca2+ Imaging Reveals Selective Reward Effects on Stimulus-Specific Assemblies in Mouse Visual Cortex</article-title>. <source>The Journal of Neuroscience</source>, <volume>33</volume>(<issue>28</issue>):<fpage>11540</fpage>–<lpage>11555</lpage>, <year>2013</year>. ISSN <issn>0270-6474</issn>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.1341-12.2013</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jun Kai</given-names> <surname>Ho</surname></string-name>, <string-name><given-names>Tomoyasu</given-names> <surname>Horikawa</surname></string-name>, <string-name><given-names>Kei</given-names> <surname>Majima</surname></string-name>, <string-name><given-names>Fan</given-names> <surname>Cheng</surname></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name></person-group>. <article-title>Inter-individual deep image reconstruction via hierarchical neural code conversion</article-title>. <source>NeuroImage</source>, <volume>271</volume>:<fpage>120007</fpage>, <year>2023</year>. ISSN <issn>1053-8119</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120007</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Ito</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Tamura</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Fujita</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Tanaka</surname></string-name></person-group>. <article-title>Size and position invariance of neuronal responses in monkey inferotemporal cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>73</volume>(<issue>1</issue>):<fpage>218</fpage>–<lpage>226</lpage>, <year>1995</year>. ISSN <issn>0022-3077</issn>. doi: <pub-id pub-id-type="doi">10.1152/jn.1995.73.1.218</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ovidiu</given-names> <surname>Jurjut</surname></string-name>, <string-name><given-names>Petya</given-names> <surname>Georgieva</surname></string-name>, <string-name><given-names>Laura</given-names> <surname>Busse</surname></string-name>, and <string-name><given-names>Steffen</given-names> <surname>Katzner</surname></string-name></person-group>. <article-title>Learning Enhances Sensory Processing in Mouse V1 before Improving Behavior</article-title>. <source>The Journal of Neuroscience</source>, <volume>37</volume>(<issue>27</issue>):<fpage>6460</fpage>–<lpage>6474</lpage>, <year>2017</year>. ISSN <issn>0270-6474</issn>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.3485-16.2017</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Takahiro</given-names> <surname>Kanamori</surname></string-name> and <string-name><given-names>Thomas D.</given-names> <surname>Mrsic-Flogel</surname></string-name></person-group>. <article-title>Independent response modulation of visual cortical neurons by attentional and behavioral states</article-title>. <source>Neuron</source>, <volume>110</volume>(<issue>23</issue>):<fpage>3907</fpage>–<lpage>3918</lpage>, <year>2022</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2022.08.028</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kendrick N.</given-names> <surname>Kay</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Naselaris</surname></string-name>, <string-name><given-names>Ryan J.</given-names> <surname>Prenger</surname></string-name>, and <string-name><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name></person-group>. <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source>, <volume>452</volume>(<issue>7185</issue>):<fpage>352</fpage>–<lpage>355</lpage>, <year>2008</year>. ISSN <issn>0028-0836</issn>. doi: <pub-id pub-id-type="doi">10.1038/nature06713</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Diederik P</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>Jimmy</given-names> <surname>Ba</surname></string-name></person-group>. <article-title>Adam: A Method for Stochastic Optimization</article-title>. <source>arXiv</source>, <year>2014</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.1412.6980</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Ganit</given-names> <surname>Kupershmidt</surname></string-name>, <string-name><given-names>Roman</given-names> <surname>Beliy</surname></string-name>, <string-name><given-names>Guy</given-names> <surname>Gaziv</surname></string-name>, and <string-name><given-names>Michal</given-names> <surname>Irani</surname></string-name></person-group>. <article-title>A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity</article-title>. <source>arXiv</source>, <year>2022</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2206.03544</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Wenyi</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Shengjie</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Yufan</given-names> <surname>Liao</surname></string-name>, <string-name><given-names>Rongqi</given-names> <surname>Hong</surname></string-name>, <string-name><given-names>Chenggang</given-names> <surname>He</surname></string-name>, <string-name><given-names>Weiliang</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Chunshan</given-names> <surname>Deng</surname></string-name>, and <string-name><given-names>Xiaojian</given-names> <surname>Li</surname></string-name></person-group>. <article-title>The brain-inspired decoder for natural visual image reconstruction</article-title>. <source>Frontiers in Neuroscience</source>, <volume>17</volume>:<issue>1130606</issue>, <year>2023</year>. ISSN <issn>1662-4548</issn>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2023.1130606</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Wu</given-names> <surname>Li</surname></string-name></person-group>. <article-title>Perceptual Learning: Use-Dependent Cortical Plasticity</article-title>. <source>Annual Review of Vision Science</source>, <volume>2</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>22</lpage>, <year>2015</year>. ISSN <issn>2374-4642</issn>. doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114351</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><given-names>Alexander</given-names> <surname>Mordvintsev</surname></string-name>, <string-name><given-names>Nicola</given-names> <surname>Pezzotti</surname></string-name>, <string-name><given-names>Ludwig</given-names> <surname>Schubert</surname></string-name>, and <string-name><given-names>Chris</given-names> <surname>Olah</surname></string-name></person-group>. <data-title>Differentiable image param-eterizations</data-title>. <source>Distill</source>, <volume>3</volume>(<issue>7</issue>), <month>July</month> <year>2018</year>. ISSN <issn>2476-0757</issn>. doi: <pub-id pub-id-type="doi">10.23915/distill.00012</pub-id>. URL 10.23915/distill.00012.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Cristopher M.</given-names> <surname>Niell</surname></string-name> and <string-name><given-names>Michael P.</given-names> <surname>Stryker</surname></string-name></person-group>. <article-title>Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex</article-title>. <source>Neuron</source>, <volume>65</volume>(<issue>4</issue>):<fpage>472</fpage>–<lpage>479</lpage>, <year>2010</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name>, <string-name><given-names>An T.</given-names> <surname>Vu</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Naselaris</surname></string-name>, <string-name><given-names>Yuval</given-names> <surname>Benjamini</surname></string-name>, <string-name><given-names>Bin</given-names> <surname>Yu</surname></string-name>, and <string-name><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name></person-group>. <article-title>Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies</article-title>. <source>Current Biology</source>, <volume>21</volume>(<issue>19</issue>):<fpage>1641</fpage>–<lpage>1646</lpage>, <year>2011</year>. ISSN <issn>0960-9822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2011.08.031</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yuichiro</given-names> <surname>Nomura</surname></string-name>, <string-name><given-names>Shohei</given-names> <surname>Ikuta</surname></string-name>, <string-name><given-names>Satoshi</given-names> <surname>Yokota</surname></string-name>, <string-name><given-names>Junpei</given-names> <surname>Mita</surname></string-name>, <string-name><given-names>Mami</given-names> <surname>Oikawa</surname></string-name>, <string-name><given-names>Hiroki</given-names> <surname>Matsushima</surname></string-name>, <string-name><given-names>Akira</given-names> <surname>Amano</surname></string-name>, <string-name><given-names>Kazuhiro</given-names> <surname>Shimonomura</surname></string-name>, <string-name><given-names>Yasuhiro</given-names> <surname>Seya</surname></string-name>, and <string-name><given-names>Chieko</given-names> <surname>Koike</surname></string-name></person-group>. <article-title>Evaluation of critical flicker-fusion frequency measurement methods using a touchscreen-based visual temporal discrimination task in the behaving mouse</article-title>. <source>Neuroscience Research</source>, <volume>148</volume>:<fpage>28</fpage>–<lpage>33</lpage>, <year>2019</year>. ISSN <issn>0168-0102</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neures.2018.12.001</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Furkan</given-names> <surname>Ozcelik</surname></string-name> and <string-name><given-names>Rufin</given-names> <surname>VanRullen</surname></string-name></person-group>. <article-title>Natural scene reconstruction from fMRI signals using generative latent diffusion</article-title>. <source>Scientific Reports</source>, <volume>13</volume>(<issue>1</issue>):<fpage>15666</fpage>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1038/s41598-023-42891-8</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jonathan</given-names> <surname>Peirce</surname></string-name>, <string-name><given-names>Jeremy R</given-names> <surname>Gray</surname></string-name>, <string-name><given-names>Sol</given-names> <surname>Simpson</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>MacAskill</surname></string-name>, Richard Höchenberger, <string-name><given-names>Hiroyuki</given-names> <surname>Sogo</surname></string-name>, <string-name><given-names>Erik</given-names> <surname>Kastman</surname></string-name>, and <string-name><given-names>Jonas</given-names> <surname>Kristoffer Lindeløv</surname></string-name></person-group>. <article-title>Psychopy2: Experiments in behavior made easy</article-title>. <source>Behavior research methods</source>, <volume>51</volume>:<fpage>195</fpage>–<lpage>203</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Pawel A.</given-names> <surname>Pierzchlewicz</surname></string-name>, <string-name><given-names>Konstantin F.</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Arne F.</given-names> <surname>Nix</surname></string-name>, <string-name><given-names>Pavithra</given-names> <surname>Elumalai</surname></string-name>, <string-name><given-names>Kelli</given-names> <surname>Restivo</surname></string-name>, <string-name><given-names>Tori</given-names> <surname>Shinn</surname></string-name>, <string-name><given-names>Cate</given-names> <surname>Nealley</surname></string-name>, <string-name><given-names>Gabrielle</given-names> <surname>Rodriguez</surname></string-name>, <string-name><given-names>Saumil</given-names> <surname>Patel</surname></string-name>, <string-name><given-names>Katrin</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name>, and <string-name><given-names>Fabian H.</given-names> <surname>Sinz</surname></string-name></person-group>. <article-title>Energy Guided Diffusion for Generating Neurally Exciting Images</article-title>. <source>bioRxiv</source>, page 2023.05.18.541176, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.05.18.541176</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jasper</given-names> <surname>Poort</surname></string-name>, <string-name><given-names>Adil G.</given-names> <surname>Khan</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>Abdellatif</given-names> <surname>Nemri</surname></string-name>, <string-name><given-names>Ivana</given-names> <surname>Orsolic</surname></string-name>, <string-name><given-names>Julija</given-names> <surname>Krupic</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Bauza</surname></string-name>, <string-name><given-names>Maneesh</given-names> <surname>Sahani</surname></string-name>, <string-name><given-names>Georg B.</given-names> <surname>Keller</surname></string-name>, <string-name><given-names>Thomas D.</given-names> <surname>Mrsic-Flogel</surname></string-name>, and <string-name><given-names>Sonja B.</given-names> <surname>Hofer</surname></string-name></person-group>. <article-title>Learning Enhances Sensory and Multiple Non-sensory Representations in Primary Visual Cortex</article-title>. <source>Neuron</source>, <volume>86</volume>(<issue>6</issue>):<fpage>1478</fpage>–<lpage>1490</lpage>, <year>2015</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.037</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.T.</given-names> <surname>Prusky</surname></string-name> and <string-name><given-names>R.M.</given-names> <surname>Douglas</surname></string-name></person-group>. <article-title>Characterization of mouse cortical spatial vision</article-title>. <source>Vision Research</source>, <volume>44</volume>(<issue>28</issue>): <fpage>3411</fpage>–<lpage>3418</lpage>, <year>2004</year>. ISSN <issn>0042-6989</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.visres.2004.09.001</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Zarina</given-names> <surname>Rakhimberdina</surname></string-name>, <string-name><given-names>Quentin</given-names> <surname>Jodelet</surname></string-name>, <string-name><given-names>Xin</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>Tsuyoshi</given-names> <surname>Murata</surname></string-name></person-group>. <article-title>Natural Image Reconstruction From fMRI Using Deep Learning: A Survey</article-title>. <source>Frontiers in Neuroscience</source>, <volume>15</volume>:<fpage>0795488</fpage>, <year>2021</year>. ISSN <issn>1662-4548</issn>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2021.795488</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rajesh P. N.</given-names> <surname>Rao</surname></string-name> and <string-name><given-names>Dana H.</given-names> <surname>Ballard</surname></string-name></person-group>. <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source>, <volume>2</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>87</lpage>, <year>1999</year>. ISSN <issn>1097-6256</issn>. doi: <pub-id pub-id-type="doi">10.1038/4580</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, <string-name><given-names>Emmanouil</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>Cathryn R.</given-names> <surname>Cadwell</surname></string-name>, <string-name><given-names>Dimitri</given-names> <surname>Yatsenko</surname></string-name>, <string-name><given-names>George H.</given-names> <surname>Denfield</surname></string-name>, and <string-name><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Pupil Fluctuations Track Fast Switching of Cortical States during Quiet Wakefulness</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>2</issue>):<fpage>355</fpage>–<lpage>362</lpage>, <year>2014</year>. ISSN <issn>0896-6273</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.033</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ziqi</given-names> <surname>Ren</surname></string-name>, <string-name><given-names>Jie</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Xuetong</given-names> <surname>Xue</surname></string-name>, <string-name><given-names>Xin</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Fan</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Zhicheng</given-names> <surname>Jiao</surname></string-name>, and <string-name><given-names>Xinbo</given-names> <surname>Gao</surname></string-name></person-group>. <article-title>Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning</article-title>. <source>NeuroImage</source>, <volume>228</volume>: <fpage>117602</fpage>, <year>2021</year>. ISSN <issn>1053-8119</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117602</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Robin</given-names> <surname>Rombach</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Blattmann</surname></string-name>, <string-name><given-names>Dominik</given-names> <surname>Lorenz</surname></string-name>, <string-name><given-names>Patrick</given-names> <surname>Esser</surname></string-name>, and <string-name><given-names>Björn</given-names> <surname>Ommer</surname></string-name></person-group>. <article-title>High-Resolution Image Synthesis with Latent Diffusion Models</article-title>. <source>arXiv</source>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2112.10752</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Steffen</given-names> <surname>Schneider</surname></string-name>, <string-name><given-names>Jin Hwa</given-names> <surname>Lee</surname></string-name>, and <string-name><given-names>Mackenzie Weygandt</given-names> <surname>Mathis</surname></string-name></person-group>. <article-title>Learnable latent embeddings for joint be-havioural and neural analysis</article-title>. <source>Nature</source>, <volume>12</volume>(<issue>1</issue>):<fpage>5170</fpage>, <year>2023</year>. ISSN <issn>0028-0836</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41586-023-06031-6</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Joseph W.</given-names> <surname>Schumacher</surname></string-name>, <string-name><given-names>Matthew K.</given-names> <surname>McCann</surname></string-name>, <string-name><given-names>Katherine J.</given-names> <surname>Maximov</surname></string-name>, and <string-name><given-names>David</given-names> <surname>Fitzpatrick</surname></string-name></person-group>. <article-title>Selective enhance-ment of neural coding in V1 underlies fine-discrimination learning in tree shrew</article-title>. <source>Current Biology</source>, <year>2022</year>. ISSN <issn>0960-9822</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2022.06.009</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Paul S</given-names> <surname>Scotti</surname></string-name>, <string-name><given-names>Atmadeep</given-names> <surname>Banerjee</surname></string-name>, <string-name><given-names>Jimmie</given-names> <surname>Goode</surname></string-name>, <string-name><given-names>Stepan</given-names> <surname>Shabalin</surname></string-name>, <string-name><given-names>Alex</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>Ethan</given-names> <surname>Cohen</surname></string-name>, <string-name><given-names>Aidan J</given-names> <surname>Dempster</surname></string-name>, <string-name><given-names>Nathalie</given-names> <surname>Verlinde</surname></string-name>, <string-name><given-names>Elad</given-names> <surname>Yundler</surname></string-name>, <string-name><given-names>David</given-names> <surname>Weisberg</surname></string-name>, <string-name><given-names>Kenneth A</given-names> <surname>Norman</surname></string-name>, and <string-name><given-names>Tanishq Mathew</given-names> <surname>Abraham</surname></string-name></person-group>. <article-title>Reconstructing the Mind’s Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors</article-title>. <source>arXiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2305.18274</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Guohua</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>Kshitij</given-names> <surname>Dwivedi</surname></string-name>, <string-name><given-names>Kei</given-names> <surname>Majima</surname></string-name>, <string-name><given-names>Tomoyasu</given-names> <surname>Horikawa</surname></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name></person-group>. <article-title>End-to-End Deep Image Reconstruction From Human Brain Activity</article-title>. <source>Frontiers in Computational Neuroscience</source>, <volume>13</volume>:<fpage>21</fpage>, <year>2019a</year>. ISSN <issn>1662-5188</issn>. doi: <pub-id pub-id-type="doi">10.3389/fncom.2019.00021</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Guohua</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>Tomoyasu</given-names> <surname>Horikawa</surname></string-name>, <string-name><given-names>Kei</given-names> <surname>Majima</surname></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name></person-group>. <article-title>Deep image reconstruction from human brain activity</article-title>. <source>PLoS Computational Biology</source>, <volume>15</volume>(<issue>1</issue>):<fpage>e1006633</fpage>, <year>2019b</year>. ISSN <issn>1553-734X</issn>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006633</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Fabian H.</given-names> <surname>Sinz</surname></string-name>, <string-name><given-names>Alexander S.</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>Paul G.</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Edgar Y.</given-names> <surname>Walker</surname></string-name>, <string-name><given-names>Erick</given-names> <surname>Cobos</surname></string-name>, <string-name><given-names>Emmanouil</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>Dimitri</given-names> <surname>Yatsenko</surname></string-name>, <string-name><given-names>Xaq</given-names> <surname>Pitkow</surname></string-name>, <string-name><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, and <string-name><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Stimulus domain transfer in recurrent models for large scale cortical population prediction on video</article-title>. <source>bioRxiv</source>, page <fpage>452672</fpage>, <year>2018</year>. doi: <pub-id pub-id-type="doi">10.1101/452672</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Garrett B</given-names> <surname>Stanley</surname></string-name>, <string-name><given-names>Fei F</given-names> <surname>Li</surname></string-name>, and <string-name><given-names>Yang</given-names> <surname>Dan</surname></string-name></person-group>. <article-title>Reconstruction of Natural Scenes from Ensemble Responses in the Lateral Geniculate Nucleus</article-title>. <source>The Journal of Neuroscience</source>, <volume>19</volume>(<issue>18</issue>):<fpage>8036</fpage>–<lpage>8042</lpage>, <year>1999</year>. ISSN <issn>0270-6474</issn>. doi: <pub-id pub-id-type="doi">10.1523/jneurosci.19-18-08036.1999</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea</given-names> <surname>Tacchetti</surname></string-name>, <string-name><given-names>Leyla</given-names> <surname>Isik</surname></string-name>, and <string-name><given-names>Tomaso A.</given-names> <surname>Poggio</surname></string-name></person-group>. <article-title>Invariant Recognition Shapes Neural Representations of Visual Input</article-title>. <source>Annual Review of Vision Science</source>, <volume>4</volume>(<issue>1</issue>):<fpage>403</fpage>–<lpage>422</lpage>, <year>2018</year>. ISSN <issn>2374-4642</issn>. doi: <pub-id pub-id-type="doi">10.1146/annurev-vision-091517-034103</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Yu</given-names> <surname>Takagi</surname></string-name> and <string-name><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name></person-group>. <article-title>High-resolution image reconstruction with latent diffusion models from human brain activity</article-title>. <source>bioRxiv</source>, page 2022.11.18.517004, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2022.11.18.517004</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Bertrand</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>Edouard</given-names> <surname>Duchesnay</surname></string-name>, <string-name><given-names>Edward</given-names> <surname>Hubbard</surname></string-name>, <string-name><given-names>Jessica</given-names> <surname>Dubois</surname></string-name>, <string-name><given-names>Jean-Baptiste</given-names> <surname>Poline</surname></string-name>, <string-name><given-names>Denis</given-names> <surname>Lebihan</surname></string-name>, and <string-name><given-names>Stanislas</given-names> <surname>Dehaene</surname></string-name></person-group>. <article-title>Inverse retinotopy: Inferring the visual content of images from brain activation patterns</article-title>. <source>NeuroImage</source>, <volume>33</volume>(<issue>4</issue>):<fpage>1104</fpage>–<lpage>1116</lpage>, <year>2006</year>. ISSN <issn>1053-8119</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.06.062</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Polina</given-names> <surname>Turishcheva</surname></string-name>, <string-name><given-names>Paul G</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Laura</given-names> <surname>Hansel</surname></string-name>, <string-name><given-names>Rachel</given-names> <surname>Froebe</surname></string-name>, <string-name><given-names>Kayla</given-names> <surname>Ponder</surname></string-name>, <string-name><given-names>Michaela</given-names> <surname>Vystrcilová</surname></string-name>, <string-name><given-names>Kon-stantin F</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Mohammad</given-names> <surname>Bashiri</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Zhiwei</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Andreas S</given-names> <surname>Tolias</surname></string-name>, <string-name><given-names>Fabian H</given-names> <surname>Sinz</surname></string-name>, and <string-name><given-names>Alexander S</given-names> <surname>Ecker</surname></string-name></person-group>. <article-title>The Dynamic Sensorium competition for predicting large-scale mouse visual cortex activity from videos</article-title>. <source>arXiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.2305.19654</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Polina</given-names> <surname>Turishcheva</surname></string-name>, <string-name><given-names>Paul G</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Michaela</given-names> <surname>Vystrcilová</surname></string-name>, <string-name><given-names>Laura</given-names> <surname>Hansel</surname></string-name>, <string-name><given-names>Rachel</given-names> <surname>Froebe</surname></string-name>, <string-name><given-names>Kayla</given-names> <surname>Ponder</surname></string-name>, <string-name><given-names>Yongrong</given-names> <surname>Qiu</surname></string-name>, <string-name><given-names>Konstantin F</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Mohammad</given-names> <surname>Bashiri</surname></string-name>, <string-name><given-names>Ruslan</given-names> <surname>Baikulov</surname></string-name>, <string-name><given-names>Yu</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>Lei</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>Shan</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>Tiejun</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Bryan M</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Wolf</given-names> <surname>De Wulf</surname></string-name>, <string-name><given-names>Nina</given-names> <surname>Kudryashova</surname></string-name>, <string-name><given-names>Matthias H</given-names> <surname>Hennig</surname></string-name>, <string-name><given-names>Nathalie L</given-names> <surname>Rochefort</surname></string-name>, <string-name><given-names>Arno</given-names> <surname>Onken</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Zhiwei</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Andreas S</given-names> <surname>Tolias</surname></string-name>, <string-name><given-names>Fabian H</given-names> <surname>Sinz</surname></string-name>, and <string-name><given-names>Alexander S</given-names> <surname>Ecker</surname></string-name></person-group>. <article-title>Retrospective for the Dynamic Sensorium Competition for predicting large-scale mouse primary visual cortex activity from videos</article-title>. <source>arXiv</source>, <year>2024</year>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Ashish</given-names> <surname>Vaswani</surname></string-name>, <string-name><given-names>Noam</given-names> <surname>Shazeer</surname></string-name>, <string-name><given-names>Niki</given-names> <surname>Parmar</surname></string-name>, <string-name><given-names>Jakob</given-names> <surname>Uszkoreit</surname></string-name>, <string-name><given-names>Llion</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>Aidan N</given-names> <surname>Gomez</surname></string-name>, <string-name><given-names>Lukasz</given-names> <surname>Kaiser</surname></string-name>, and <string-name><given-names>Illia</given-names> <surname>Polosukhin</surname></string-name></person-group>. <article-title>Attention Is All You Need</article-title>. <source>arXiv</source>, <year>2017</year>. doi: <pub-id pub-id-type="doi">10.48550/arxiv.1706.03762</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Edgar Y.</given-names> <surname>Walker</surname></string-name>, <string-name><given-names>Fabian H.</given-names> <surname>Sinz</surname></string-name>, <string-name><given-names>Erick</given-names> <surname>Cobos</surname></string-name>, <string-name><given-names>Taliah</given-names> <surname>Muhammad</surname></string-name>, <string-name><given-names>Emmanouil</given-names> <surname>Froudarakis</surname></string-name>, <string-name><given-names>Paul G.</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Alexander S.</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, <string-name><given-names>Xaq</given-names> <surname>Pitkow</surname></string-name>, and <string-name><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Inception loops discover what excites neurons most using deep predictive models</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>12</issue>):<fpage>2060</fpage>–<lpage>2065</lpage>, <year>2019</year>. ISSN <issn>1097-6256</issn>. doi: <pub-id pub-id-type="doi">10.1038/s41593-019-0517-x</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Eric Y</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Paul G</given-names> <surname>Fahey</surname></string-name>, <string-name><given-names>Kayla</given-names> <surname>Ponder</surname></string-name>, <string-name><given-names>Zhuokun</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Andersen</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>Taliah</given-names> <surname>Muhammad</surname></string-name>, <string-name><given-names>Saumil</given-names> <surname>Patel</surname></string-name>, <string-name><given-names>Zhiwei</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Dat</given-names> <surname>Tran</surname></string-name>, <string-name><given-names>Jiakun</given-names> <surname>Fu</surname></string-name>, <string-name><given-names>Stelios</given-names> <surname>Papadopoulos</surname></string-name>, <string-name><given-names>Katrin</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>Alexander S</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>Jacob</given-names> <surname>Reimer</surname></string-name>, <string-name><given-names>Xaq</given-names> <surname>Pitkow</surname></string-name>, <string-name><given-names>Fabian H</given-names> <surname>Sinz</surname></string-name>, and <string-name><given-names>Andreas S</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Towards a Foundation Model of the Mouse Visual Cortex</article-title>. <source>bioRxiv</source>, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.03.21.533548</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Konstantin F.</given-names> <surname>Willeke</surname></string-name>, <string-name><given-names>Kelli</given-names> <surname>Restivo</surname></string-name>, <string-name><given-names>Katrin</given-names> <surname>Franke</surname></string-name>, <string-name><given-names>Arne F.</given-names> <surname>Nix</surname></string-name>, <string-name><given-names>Santiago A.</given-names> <surname>Cadena</surname></string-name>, <string-name><given-names>Tori</given-names> <surname>Shinn</surname></string-name>, <string-name><given-names>Cate</given-names> <surname>Nealley</surname></string-name>, <string-name><given-names>Gabrielle</given-names> <surname>Rodriguez</surname></string-name>, <string-name><given-names>Saumil</given-names> <surname>Patel</surname></string-name>, <string-name><given-names>Alexander S.</given-names> <surname>Ecker</surname></string-name>, <string-name><given-names>Fabian H.</given-names> <surname>Sinz</surname></string-name>, and <string-name><given-names>Andreas S.</given-names> <surname>Tolias</surname></string-name></person-group>. <article-title>Deep learning-driven characterization of single cell tuning in primate visual area V4 unveils topological organization</article-title>. <source>bioRxiv</source>, page 2023.05.12.540591, <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.05.12.540591</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ji</given-names> <surname>Xia</surname></string-name>, <string-name><given-names>Tyler D.</given-names> <surname>Marks</surname></string-name>, <string-name><given-names>Michael J.</given-names> <surname>Goard</surname></string-name>, and <string-name><given-names>Ralf</given-names> <surname>Wessel</surname></string-name></person-group>. <article-title>Stable representation of a naturalistic movie emerges from episodic activity with gain variability</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>):<fpage>5170</fpage>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1038/s41467-021-25437-2</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Takashi</given-names> <surname>Yoshida</surname></string-name> and <string-name><given-names>Kenichi</given-names> <surname>Ohki</surname></string-name></person-group>. <article-title>Natural images are reliably represented by sparse and variable populations of neurons in visual cortex</article-title>. <source>Nature Communications</source>, <volume>11</volume>(<issue>1</issue>):<fpage>872</fpage>, <year>2020</year>. doi: <pub-id pub-id-type="doi">10.1038/s41467-020-14645-x</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yichen</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Shanshan</given-names> <surname>Jia</surname></string-name>, <string-name><given-names>Yajing</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Zhaofei</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>Yonghong</given-names> <surname>Tian</surname></string-name>, <string-name><given-names>Siwei</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>Tiejun</given-names> <surname>Huang</surname></string-name>, and <string-name><given-names>Jian K.</given-names> <surname>Liu</surname></string-name></person-group>. <article-title>Reconstruction of natural visual scenes from neural spikes with deep neural networks</article-title>. <source>Neural Networks</source>, <volume>125</volume>:<fpage>19</fpage>–<lpage>30</lpage>, <year>2020</year>. ISSN <issn>0893-6080</issn>. doi: <pub-id pub-id-type="doi">10.1016/j.neunet.2020.01.033</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Denison</surname>
<given-names>Rachel</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Boston University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study uses state-of-the-art neural encoding and video reconstruction methods to achieve a substantial improvement in video reconstruction quality from mouse neural data, providing a <bold>convincing</bold> demonstration of how reconstruction performance can be improved by combining these methods. The findings showed that model ensembling and the number of neurons used for reconstruction were key determinants of reconstruction accuracy, but the theoretical contribution to understanding neural encoding was less clear. The treatment of how image masking improved reconstruction performance was also <bold>incomplete</bold>.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents a method for reconstructing videos from mouse visual cortex neuronal activity using a state-of-the-art dynamic neural encoding model. The authors achieve high-quality reconstructions of 10-second movies at 30 Hz from two-photon calcium imaging data, reporting a 2-fold increase in pixel-by-pixel correlation compared to previous methods. They identify key factors for successful reconstruction including the number of recorded neurons and model ensembling techniques.</p>
<p>Strengths:</p>
<p>(1) A comprehensive technical approach combining state-of-the-art neural encoding models with gradient-based optimization for video reconstruction.</p>
<p>(2) Thorough evaluation of reconstruction quality across different spatial and temporal frequencies using both natural videos and synthetic stimuli.</p>
<p>(3) Detailed analysis of factors affecting reconstruction quality, including population size and model ensembling effects.</p>
<p>(4) Clear methodology presentation with well-documented algorithms and reproducible code.</p>
<p>(5) Potential applications for investigating visual processing phenomena like predictive coding and perceptual learning.</p>
<p>Weaknesses:</p>
<p>The main metric of success (pixel correlation) may not be the most meaningful measure of reconstruction quality:</p>
<p>High correlation may not capture perceptually relevant features.</p>
<p>Different stimuli producing similar neural responses could have low pixel correlations The paper doesn't fully justify why high pixel correlation is a valuable goal</p>
<p>Comparison to previous work (Yoshida et al.) has methodological concerns: Direct comparison of correlation values across different datasets may be misleading; Large differences in the number of recorded neurons (10x more in the current study); Different stimulus types (dynamic vs static) make comparison difficult; No implementation of previous methods on the current dataset or vice versa.</p>
<p>Limited exploration of how the reconstruction method could provide insights into neural coding principles beyond demonstrating technical capability.</p>
<p>The claim that &quot;stimulus reconstruction promises a more generalizable approach&quot; (line 180) is not well supported with concrete examples or evidence.</p>
<p>The paper would benefit from addressing how the method handles cases where different stimuli produce similar neural responses, particularly for high-speed moving stimuli where phase differences might be lost in calcium imaging temporal resolution.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This is an interesting study exploring methods for reconstructing visual stimuli from neural activity in the mouse visual cortex. Specifically, it uses a competition dataset (published in the Dynamic Sensorium benchmark study) and a recent winning model architecture (DNEM, dynamic neural encoding model) to recover visual information stored in ensembles of the mouse visual cortex.</p>
<p>This is a great project - the physiological data were measured at a single-cell resolution, the movies were reasonably naturalistic and representative of the real world, the study did not ignore important correlates such as eye position and pupil diameter, and of course, the reconstruction quality exceeded anything achieved by previous studies. Overall, it is great that teams are working towards exploring image reconstruction. Arguably, reconstruction may serve as an endgame method for examining the information content within neuronal ensembles - an alternative to training interminable numbers of supervised classifiers, as has been done in other studies. Put differently, if a reconstruction recovers a lot of visual features (maybe most of them), then it tells us a lot about what the visual brain is trying to do: to keep as much information as possible about the natural world in which its internal motor circuits may act consequently.</p>
<p>While we enjoyed reading the manuscript, we admit that the overall advance was in the range of those that one finds in a great machine learning conference proceedings paper. More specifically, we found no major technical flaws in the study, only a few potential major confounds (which should be addressable with new analyses), and the manuscript did not make claims that were not supported by its findings, yet the specific conceptual advance and significance seemed modest. Below, we will go through some of the claims, and ask about their potential significance.</p>
<p>(1) The study showed that it could achieve high-quality video reconstructions from mouse visual cortex activity using a neural encoding model (DNEM), recovering 10-second video sequences and approaching a two-fold improvement in pixel-by-pixel correlation over attempts. As a reader, I am left with the question: okay, does this mean that we should all switch to DNEM for our investigations of the mouse visual cortex? What makes this encoding model special? It is introduced as &quot;a winning model of the Sensorium 2023 competition which achieved a score of 0.301... single-trial correlation between predicted and ground truth neuronal activity,&quot; but as someone who does not follow this competition (most eLife readers are not likely to do so, either), I do not know how to gauge my response. Is this impressive? What is the best achievable score, in theory, given data noise? Is the model inspired by the mouse brain in terms of mechanisms or architecture, or was it optimized to win the competition by overfitting it to the nuances of the data set? Of course, I know that as a reader, I am invited to read the references, but the study would stand better on its own if clarified how its findings depended on this model.</p>
<p>(2) Along those lines, two major conclusions were that &quot;critical for high-quality reconstructions are the number of neurons in the dataset and the use of model ensembling.&quot; If true, then these principles should be applicable to networks with different architectures. How well can they do with other network types?</p>
<p>(3) One major claim was that the quality of the reconstructions depended on the number of neurons in the dataset. There were approximately 8000 neurons recorded per mouse. The correlation difference between the reconstruction achieved by 1 neuron and 8000 neurons was ~0.2. Is that a lot or a little? One might hypothesize that ~7,999 additional neurons could contribute more information, but perhaps, those neurons were redundant if their receptive fields were too close together or if they had the same orientation or spatiotemporal tuning. How correlated were these neurons in response to a given movie? Why did so many neurons offer such a limited increase in correlation?</p>
<p>(4) On a related note, the authors address the confound of RF location and extent. The study resorted to the use of a mask on the image during reconstruction, applied during training and evaluation (Line 87). The mask depends on pixels that contribute to the accurate prediction of neuronal activity. The problem for me is that it reads as if the RF/mask estimate was obtained during the very same process of reconstruction optimization, which could be considered a form of double-dipping (see the &quot;Dead salmon&quot; article, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1053-8119(09)71202-9">https://doi.org/10.1016/S1053-8119(09)71202-9</ext-link>). This could inflate the reconstruction estimate. My concern would be ameliorated if the mask was obtained using a held-out set of movies or image presentations; further, the mask should shift with eye position, if it indeed corresponded to the &quot;collective receptive field of the neural population.&quot; Ideally, the team would also provide the characteristics of these putative RFs, such as their weight and spatial distribution, and whether they matched the biological receptive fields of the neurons (if measured independently).</p>
<p>(5) We appreciated the experiments testing the capacity of the reconstruction process, by using synthetic stimuli created under a Gaussian process in a noise-free way. But this further raised questions: what is the theoretical capability for the reconstruction of this processing pipeline, as a whole? Is 0.563 the best that one could achieve given the noisiness and/or neuron count of the Sensorium project? What if the team applied the pipeline to reconstruct the activity of a given artificial neural network's layer (e.g., some ResNet convolutional layer), using hidden units as proxies for neuronal calcium activity?</p>
<p>(6) As the authors mentioned, this reconstruction method provided a more accurate way to investigate how neurons process visual information. However, this method consisted of two parts: one was the state-of-the-art (SOTA) dynamic neural encoding model (DNEM), which predicts neuronal activity from the input video, and the other part reconstructed the video to produce a response similar to the predicted neuronal activity. Therefore, the reconstructed video was related to neuronal activity through an intermediate model (i.e., SOTA DNEM). If one observes a failure in reconstructing certain visual features of the video (for example, high-spatial frequency details), the reader does not know whether this failure was due to a lack of information in the neural code itself or a failure of the neuronal model to capture this information from the neural code (assuming a perfect reconstruction process). Could the authors address this by outlining the limitations of the SOTA DNEM encoding model and disentangling failures in the reconstruction from failures in the encoding model?</p>
<p>(7) The authors mentioned that a key factor in achieving high-quality reconstructions was model assembling. However, this averaging acts as a form of smoothing, which reduces the reconstruction's acuity and may limit the high-frequency content of the videos (as mentioned in the manuscript). This averaging constrains the tool's capacity to assess how visual neurons process the low-frequency content of visual input. Perhaps the authors could elaborate on potential approaches to address this limitation, given the critical importance of high-frequency visual features for our visual perception.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents a method for reconstructing input videos shown to a mouse from the simultaneously recorded visual cortex activity (two-photon calcium imaging data). The publicly available experimental dataset is taken from a recent brain-encoding challenge, and the (publicly available) neural network model that serves to reconstruct the videos is the winning model from that challenge (by distinct authors). The present study applies gradient-based input optimization by backpropagating the brain-encoding error through this selected model (a method that has been proposed in the past, with other datasets). The main contribution of the paper is, therefore, the choice of applying this existing method to this specific dataset with this specific neural network model. The quantitative results appear to go beyond previous attempts at video input reconstruction (although measured with distinct datasets). The conclusions have potential practical interest for the field of brain decoding, and theoretical interest for possible future uses in functional brain exploration.</p>
<p>Strengths:</p>
<p>The authors use a validated optimization method on a recent large-scale dataset, with a state-of-the-art brain encoding model. The use of an ensemble of 7 distinct model instances (trained on distinct subsets of the dataset, with distinct random initializations) significantly improves the reconstructions. The exploration of the relation between reconstruction quality and the number of recorded neurons will be useful to those planning future experiments.</p>
<p>Weaknesses:</p>
<p>The main contribution is methodological, and the methodology combines pre-existing components without any new original components. The movie reconstructions include a learned &quot;transparency mask&quot; to concentrate on the most informative area of the frame; it is not clear how this choice impacts the comparison with prior experiments. Did they all employ this same strategy? If not, shouldn't the quantitative results also be reported without masking, for a fair comparison?</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105081.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bauer</surname>
<given-names>Joel</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5858-166X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Margrie</surname>
<given-names>Troy W</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5526-4578</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Clopath</surname>
<given-names>Claudia</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4507-8648</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the reviewers for their thorough review of our manuscript and their constructive feedback. We will address their comments and concerns in a point-by-point response at a later stage but would like to clarify some minor misunderstanding to not confuse any readers in the meantime.</p>
<p>- In regard to population ablation: When investigating the contribution of population size to reconstruction quality, we used 12.5, 25, 50 or 100% of the recorded neuronal population, which corresponds to ~1000/2000/4000/8000 neurons per animal. We did not produce reconstructions from only 1 neuron.</p>
<p>- In regard to the training of the transparency masks: The transparency masks were not produced using the same movies we reconstructed. We apologize for the lack of clarity on this point in the manuscript. We calculated the masks using an original model instance rather than a retrained instances used in the rest of the paper. Specifically, the masks were calculated using the original model instance ‘fold 1’ and data fold 1, which is it’s validation fold. In contrast, the model instances used in the paper for movie reconstruction were retrained while omitting the same validation fold across all instances (fold 0) and all the reconstructed movies in the paper are from data fold 0.</p>
<p>- In regard to reconstruction based on predicted activity: We always reconstructed the videos based on the true neural responses not the predicted neural response, with the exception of the Gaussian noise and drifting grating stimuli in Figure 4 and Supplementary Figure S2 where no recorded neural activity was available).</p>
</body>
</sub-article>
</article>