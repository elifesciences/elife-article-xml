<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107111</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107111</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107111.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>The distinct role of human PIT in attention control</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0004-2347-293X</contrib-id>
<name>
<surname>Huang</surname>
<given-names>Siyuan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Lan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>He</surname>
<given-names>Sheng</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>hes@ibp.ac.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tyv8576</institution-id><institution>State Key Laboratory of Brain and Cognitive Science, Institute of Biophysics, Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>University of Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Wang</surname>
<given-names>Shuo</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Washington University in St. Louis</institution>
</institution-wrap>
<city>St. Louis</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="con"><p>Author Contributions: S. Huang, L. Wang, and S. He designed research; S. Huang performed research; S. Huang analyzed data; and S. Huang, L. Wang, and S. He wrote the paper.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-06-16">
<day>16</day>
<month>06</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107111</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-04-11">
<day>11</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-03-10">
<day>10</day>
<month>03</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.03.07.641975"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Huang et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Huang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107111-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Attentional distribution depends on both endogenous and exogenous processes, but how they interact in attention allocation remains unclear. The attentional priority map, jointly determined by stimulus saliency and task relevance, provides a framework for investigating their interplay. We propose that the human posterior inferotemporal cortex (hPIT), located near object-processing cortical areas, serves as an attentional priority map. Using fMRI with behavioral tasks, we show that hPIT shows stronger attentional modulation than classical attention regions across motion, color, and shape tasks. hPIT shows lateralized attentional enhancement even in the absence of visual input, with further elevated modulation in the presence of stimuli, indicating its integrated role in priority control. Furthermore, its modulation is invariant to stimulus category but sensitive to attentional demands, and the region is functionally connected to both dorsal and ventral attentional networks. These findings highlight the hPIT as an integrator in attentional control and provide critical insights into the brain’s strategy for optimizing responses to the environment.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>attention</kwd>
<kwd>cognition</kwd>
<kwd>fMRI</kwd>
<kwd>PITd</kwd>
<kwd>priority map</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Attention serves as a pivotal mechanism within the brain, facilitating the allocation of its finite resource towards the most pertinent stimuli in our environment <sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Attention can be systematically categorized according to its origin: bottom-up attention (exogenous), triggered by highly salient stimuli, and top-down attention (endogenous), initiated through voluntary selection <sup><xref ref-type="bibr" rid="c2">2</xref>–<xref ref-type="bibr" rid="c4">4</xref></sup>. The neural networks for bottom-up and top-down attention have been extensively investigated, with ventral and dorsal attention networks (VAN and DAN) specialized to support exogenous and endogenous attention respectively <sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c7">7</xref></sup>. How these two networks flexibly interact to achieve integrated attentional selection, under situations that both salient stimulus and intentional goal are present, remains inadequately understood <sup><xref ref-type="bibr" rid="c8">8</xref></sup>.</p>
<p>Critical to the integration of exogenous and endogenous attention is the concept of “priority map”, defined as a map that reflects both the low-level salience and top-down influences <sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c11">11</xref></sup>. An important question arises concerning the location of the priority map in the brain. For a brain region to qualify as the neural substrate supporting a priority map, it must satisfy the following criteria <sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>: 1) having spatially restricted receptive fields, 2) exhibiting robust attentional modulation to predict attentional focus regardless of stimulus attributes but sensitive to attentional load, 3) displaying little feature specificity and high visual responsivity, and 4) exhibiting integration of bottom-up and top-down inputs. While the saliency map - where objects compete for greater representation and attentional allocation <sup><xref ref-type="bibr" rid="c14">14</xref></sup> across all feature dimensions - plays an important role in the mechanism of bottom-up attention, the priority map (jointly detemined by saliency and task guidance) is crucial for the cooperation of bottom-up and top- down attention, making it a key aspect to understand for the implementation of attention.</p>
<p>Previous research has indicated that the prefrontal cortex (PFC) and posterior parietal cortex (PPC) are involved in both top-down and bottom-up attention, rendering them potential candidates for supporting a priority map guiding attentional selection <sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c18">18</xref></sup>. In addition, the superior colliculus (SC), especially its intermediate layers, receives signals from the frontal eye fields (FEF) and lateral intraparietal area (LIP) / intraparietal sulcus (IPS)<sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup>, could also support a priority representation <sup><xref ref-type="bibr" rid="c22">22</xref></sup>. While visual areas, driven by stimulus features, were considered unlikely to exhibit priority representation <sup><xref ref-type="bibr" rid="c9">9</xref></sup>, a recent study in monkeys revealed that the posterior inferotemporal cortex (PITd) could encode the locus of attention independent of features <sup><xref ref-type="bibr" rid="c12">12</xref></sup>. The structural connectivity between PITd and parieto-frontal attentional areas (specifically LIP and FEF) further supports the idea that PITd is part of attentional networks <sup><xref ref-type="bibr" rid="c23">23</xref></sup>. In the context of human brains, recent results have identified an area in the posterior inferotemporal cortex (hPIT) as a retinotopic and functional homologue of the macaque PITd, which was proposed to be a putative node for the human endogenous attentional control network <sup><xref ref-type="bibr" rid="c24">24</xref></sup>. However, since only endogenous attention was investigated in the previous experiments on monkey PITd as well as human PIT, it remains unclear whether hPIT is important for the integration of endogenous and exogenous attention.</p>
<p>The current study investigated if attention can modulate activation in hPIT independent of stimulus properties and cognitive demands. Instead of relying solely on the attentive motion task <sup><xref ref-type="bibr" rid="c24">24</xref></sup>, hPIT was localized and validated using three distinct spatial attention tasks. Furthermore, we aim to discern the role of hPIT in both top-down and bottom-up attention, by manipulating the presence or absence of visual stimuli. Distinct from nodes of classical endogenous attention network, hPIT was more strongly modulated by attention in the presence of visual input. Furthermore, attention load, rather than object category, significantly impacts the modulation in hPIT. Additionally, hPIT showed strong functional connection with nodes in both ventral and dorsal attention network. Our results strongly suggest that hPIT, different from its adjacent object-processing areas and other parietal and frontal nodes of attention network, functions as an attention priority map.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Localization and Validation of hPIT from Task-Invariant Activation to Spatial Attention</title>
<p>To identify brain area(s) that encode the location of attention invariant to stimulus type and cognitive demand <sup><xref ref-type="bibr" rid="c12">12</xref></sup>, we employed three different spatial attention tasks in Exp. 1. In all three tasks, participants were instructed to fixate on a center dot and pay attention to circular aperture on one side, as directed by the cue. First task was about motion discrimination, and the second and third task required the discrimination of color proportion and shape proportion.</p>
<p>To isolate cortical areas modulated by endogenous spatial attention, contrasts between the conditions [attend contralateral – attend ipsilateral] were calculated for each of the three task blocks. Significant attention-modulated voxel maps were generated for individual participant (in most cases p &lt; 0.05, cluster size &gt; 20, but for activated voxel number &gt; 10000, using p &lt; 0.01). Then we took the intersection of these three maps and projected it to the cortex in every participant (example shown in <xref rid="fig1" ref-type="fig">Fig 1A</xref>), as our goal was to locate hPIT in inferotemporal cortex of human brain which should exhibit the properties of priority map, specifically in this context, insensitive to stimulus and cognition dimension. On the intersection of three maps, we found bilateral brain areas significantly modulated by spatial attention in intra-parietal sulcus (exclude the right hemisphere of S13) and inferotemporal cortex in all participants, and in most participants also the middle temporal cortex (exclude S07). Only three participants (S03, S04, S11) showed intersection area in the prefrontal cortex.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Functional localization of hPIT.</title>
<p>(A): The intersection of three maps of three block tasks on one typical participant (S02). The cortical areas which are attached in red has shown significant activation in all three attention tasks. (B): The exact position of hPIT. Left: Positions of FFA and hPIT in statistical parametric maps of the contrast [attend face – attend scene] (p=0.05) (top) and hPIT in intersection map on the right hemisphere of S02. Right: The location of hPIT on both hemispheres of S02 circled by white line. The cortical areas which are attached in red has shown significant activation in all three attention tasks. (C): The positions of hPIT, OFA and FFA of 15 participants overlapped on surface of MNI152_2009c, with larger numerical values (manifested as deeper colors) indicating a higher degree of overlap among participants. The color scale ranging from grey to red delineates the spatial distribution of the hPIT, while the scale from grey to blue represents the FFA, and grey to yellow signifies the OFA.</p></caption>
<graphic xlink:href="641975v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As illustrated in <xref rid="fig1" ref-type="fig">Fig 1B</xref>, the hPIT was manually identified bilaterally on the intersection maps within the posterior and ventral part of mid-fusiform sulcus as a contiguous cluster of voxels that are non- overlapping with fusiform face area (FFA) nor parahippocampal place area (PPA). In all participants, the hPIT is ventromedial to the occipital face area (OFA) and posterior to the FFA. The locations of hPIT, OFA and FFA identified in our study are shown in <xref rid="fig1" ref-type="fig">Fig 1C</xref> on the surface of standard brain MNI152_2009c. The MNI coordinates delineating the center of mass (COM) for the ROIs are specified as follows: hPIT (-34, -72, -14), (33, -73, -13); FFA (-43, -55, -19), (41, -54, -17); OFA (-46, -79, -3), (46, -74, -8). The average cluster size of the hPIT is 210 voxels in left hemisphere and 191 voxels in right hemisphere, with 2x2x2 mm<sup>3</sup> voxels. Detailed location and cluster size of hPIT in every participant’s cortical surface are reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary Fig. 1</xref>.</p>
<p>Enhanced activation to attended than unattended moving dots in experiment 1, constrained by cerebral atlas, allowed us to also locate cortical areas that are critical nodes of attention network <sup>5,6</sup> bilaterally, including medial temporal visual area (MT), IPS (classified into IPS_P, the posterior part, and IPS_A, the anterior part), FEF, temporal parietal junction (TPJ), and ventral frontal cortex (VFC) (see Methods).</p>
<p>Additionally, V1 was included as a ROI, so that we can see the consequences of attentional modulation in the primary visual cortex. Furthermore, the posterior part of lateral–occipital cortex (LOp) and FFA) were localized using cortical parcellation and functional contrast as controls, since they were adjacent to hPIT physically.</p>
</sec>
<sec id="s2b">
<title>Attentional modulation of hPIT With and Without Bottom-up input</title>
<p>To investigate the modulation in hPIT and other brain areas applied by top-down attention (without stimulus) and top-down combined with bottom-up attention (with stimulus), we manipulated the presence or absence of visual stimuli in Exp. 2. Using event-related design, participants were required to direct their attention to the left or right visual field based on central cues. A dot was then presented on the attended side on half of the trials (50% probability), requiring participants to report, by pressing keys, its location relative to two reference points that were constantly presented (<xref rid="fig2" ref-type="fig">Fig. 2</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Schematic depiction of the experimental design for Experiment 2.</title>
<p>Following the flashing of central point, a cue is presented. In the dot condition, a dot appears on the attended side, prompting participants to report its relative position. In the blank condition, participants are instructed not to press any keys. (Light grey dots: serve as indicators, showing participants the potential target locations during the experiment.).</p></caption>
<graphic xlink:href="641975v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>ROI-based analysis was performed to BOLD activation signals from hPIT and other ROIs identified in Exp. 1. <xref rid="fig3" ref-type="fig">Fig. 3A</xref> shows the averaged beta of contrast [attended - unattended (attend contralateral - attend ipsilateral)] from bilateral ROIs for each participant, illustrating activation patterns of our ROIs under both blank and dot conditions. A Two-Way ANOVA (n=15), considering blank/dot and ROI as factors, revealed significant main effects for both: ROI [F (2.246, 31.44) =19.20, P&lt;0.0001] and condition blank/dot [F (1.000, 14.00) =20.41, P=0.0005]. A significant interaction between these two factors was also observed [F (4.061, 56.86) = 10.98, P&lt;0.0001].</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Attention modulation in different brain regions.</title>
<p>(A): The modulation pattern of V1, hPIT, MT, IPS, FEF, TPJ and VFC in condition blank (blue bar) and condition dot (pink bar), using beta of contrast: [attended -unattended (attend contralateral - attend ipsilateral)]. The modulation difference of attention between condition blank and condition dot reached significant level in PITd and MT. (B): The modulation pattern of hPIT, LOp, and FFA in condition blank (blue bar) and condition dot (pink bar). Error bars indicate 95% confidence interval. ∗∗∗ indicates the paired t test with significance of P &lt; 0.001. ∗∗ indicates the paired t test with significance of P &lt; 0.01. ∗ indicates the paired t test with significance of P &lt; 0.05.</p></caption>
<graphic xlink:href="641975v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The condition wherein participants were required to attend to one side in the absence of any stimulus except the constant background, served to elucidate the impact of top-down attentional modulation. As illustrated by the blue bars, top-down modulation was apparent in multiple ROIs (V1, hPIT, MT, IPS_P, IPS_A, FEF, VFC), but it was strongest in hPIT. To establish quantitative metrics, a post-hoc multiple comparison test (Dunnett, one-sided) was utilized to compare the strength of attentional modulation between hPIT and other ROIs. The attentional modulation in hPIT was significantly stronger than all the other ROIs : V1 [q (14) =2.648, adjusted P= 0.0451], MT [q (14) =2.856, adjusted P= 0.031], IPS_P [q (14) =2.680, adjusted P= 0.0426], IPS_A [q (14) =3.883, adjusted P= 0.005], FEF [q (14) =3.823, adjusted P= 0.005], TPJ [q (14) =4.261, adjusted P= 0.002], VFC [q (14) =3.161, adjusted P= 0.018].</p>
<p>With the introduction of a small dot as a target stimulus, we were able to examine the combined influence of top-down and bottom-up attention. As indicated by the pink bars, hPIT again exhibited the strongest attentional modulation effect, however, here the modulation effect reflects the combined effect of top-down and bottom-up attention. Post-hoc comparison (Dunnett, one-sided) revealed again that hPIT had significantly higher combined attentional modulation effect than all the other ROIs: V1[q (14) =5.717, adjusted P&lt; 0.001], MT [q (14) =5.443, adjusted P&lt; 0.001], IPS_P [q (14) =6.742, adjusted P&lt; 0.001], IPS_A [q (14) =7.804, adjusted P&lt; 0.001], FEF [q (14) =7.939, adjusted P&lt; 0.001], TPJ [q (14) =7.944, adjusted P&lt; 0.001], and VFC [q (14) =7.554, adjusted P&lt; 0.001]. Further, among all the ROIs, hPIT and MT showed significantly stronger elevation in attentional modulation in the dot than the blank condition [Bonferroni, hPIT: t (14) =5.321, adjusted P&lt; 0.001; MT: t (14) =4.326, adjusted P =0.003].</p>
<p>Comparatively, the combined attentional effect was stronger in hPIT than in MT [paired t test, one-tailed, t (14) =1.969, p=0.035].</p>
<p>As a supplement, comparison of attentional modulation across ROIs adjacent to hPIT was conducted (<xref rid="fig3" ref-type="fig">Fig.3B</xref>). Results of Two-way ANOVA showed significant main effect of condition blank/dot [df=1, F (1.000, 14.00) = 30.23, P&lt;0.0001] and ROIs [df=2, F (1.921, 26.90) = 9.788, P=0.0007], as well as the interaction [df=2, F (1.311, 18.36) = 5.691, P=0.0210]. Multiple comparisons (Tukey’s) demonstrated that the modulation in hPIT was significantly stronger than LOp and FFA in both conditions [Blank: q (14) =3.822, P=0.043 for LOp, q (14) =4.581, P=0.015 for FFA; Dot: q (14) =7.054, P=0.001 for LOp, q (14) =4.137, P=0.028 for FFA]. These findings highlight hPIT’s distinctive pattern of attentional engagement compared to the other two areas.</p>
<p>Generally, while most ROIs showed significant modulation by attention in both the blank and dot conditions (see <xref ref-type="supplementary-material" rid="supp1">supplementary Table 1</xref>), hPIT is unique in that it showed the strongest attentional modulation in each condition as well as showing the largest elevation effect from blank to dot condition, suggesting that hPIT is deeply engaged in both bottom-up attention and top-down attention.</p>
</sec>
<sec id="s2c">
<title>Image Category-invariant response but load-sensitive attentional effect in hPIT</title>
<p>To effectively guide spatial attention, hPIT should be broadly responsive to different stimulus features (i.e., lack of feature selectivity) but show high sensitivity to locations with salient and task-relevant features. In addition, such spatial sensitivity should be modulated by attentional load. In order to explore stimulus feature and attentional load sensitivity of hPIT, in experiment 3, images of three different categories (face, scene, scramble) were presented, participants were required to pay attention to the image on the left or right side according to the cue and report its moving direction. To modulate the attentional load, in each run, participants were instructed to make more demanding judgements (higher load) about the content of one pre-specified category of the attended images. For the pre-specified category, the additional responses were female or male for faces, indoor or outdoor for scenes, overlay grating tilted clockwise counterclockwise for the scrambled images (<xref rid="fig4" ref-type="fig">Fig. 4</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Schematic overview of the experimental paradigm for the experiment 3.</title>
<p>At the start of the “face run”, participants were instructed to pay heightened attention to the face images throughout that specific run. Following the central point’s flashing sequence, a cue and bilateral images was presented. Participants first judged if the movement direction of the attended image aligned with the central arrow. Upon the central point changing to green, participants then responded based on the content of the attended image.</p></caption>
<graphic xlink:href="641975v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><xref rid="fig5" ref-type="fig">Fig.5A</xref> shows the response of hPIT to different categories of images in different conditions, indicated by the beta values. To test if hPIT showed any effect of category preference, spatial attention and attention load sensitivity, we first performed a three-way ANOVA (n=15, spatial attention x stimulus category x task load). Results showed that the main effect of attention was significant [F (0.7628, 10.68) = 119.8, P&lt;0.0001], the interaction between attention and category was significant [F (2.000, 28.00) = 4.068, P=0.0281], and the interaction between attention and task load was significant [F (0.6964, 9.749) = 8.307, P=0.0230]. Other main effects and interactions did not reach significance.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Response and attention modulation of hPIT to images of different categories.</title>
<p>(A): The activation level of hPIT when attending to or not attending to images of different categories. Red bars indicate condition with attention in the receptive field (attended), while blue bars indicate condition with attention in the other side (unattended). Bars with higher chroma represent high load condition, lower chroma represent low load condition. (B): The modulation pattern of hPIT when presenting different categories of images with different attention load. White bars represent condition with low-load attention; Grey bars represent condition high-load attention. Error bars indicate 95% confidence interval.</p></caption>
<graphic xlink:href="641975v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Since there is attention x category interaction, we further explored stimulus category sensitivity separately in the attend and un-attend conditions. Bayesian repeated measures ANOVA were adopted to compare the responses to face, scene and scramble stimuli under attend and un-attend conditions: attend [BF<sub>10</sub>=0.93], unattend [BF<sub>10</sub>=0.202]. The small Bayesian Factors (&lt;1) under both conditions provide support that hPIT response was insensitive to these three categories, consistent with the criteria for priority map.</p>
<p>In addition, as the main effect of attention is significant, to gain a better understanding of the attention effect related to stimulus categories, we calculated the attentional modulation of hPIT for different image categories and task load (<xref rid="fig5" ref-type="fig">Fig.5B</xref>). A Two-Way ANOVA (n=15, stimulus category x task load) was performed. Significant main effects of category [F (1.852, 25.93) = 4.069, P= 0.0317] and task load [F (1.000, 14.00) = 8.307, P=0.0121] were found while their interaction was non-significant [F (1.527, 21.37) = 2.714, P=0.1004].</p>
<p>In general, unlike its adjacent object-processing areas, the response of hPIT is not sensitive to stimulus category. There is evidence that attentional modulation in hPIT is slightly stronger to faces than scenes and scrambled images, possibly reflecting the intrinsic saliency difference between these image categories. The observation that the attentional modulation in hPIT is sensitive to task load further supports hPIT’s role as an attentional priority map.</p>
</sec>
<sec id="s2d">
<title>Functional connectivity of hPIT to attentional networks</title>
<p>To unravel the functional connectivity of hPIT with the whole brain (especially the nodes of attention network), resting-state data were analyzed, with ROIs defined by task-based data (experiment 1 and 3, <xref rid="fig6" ref-type="fig">Fig. 6A</xref>) and cortical parcellation atlas <sup><xref ref-type="bibr" rid="c25">25</xref></sup>. Firstly, to visualize the patterns of functional connectivity, we calculated the average time course of hPIT, FFA and LOp using small sphere-shaped ROIs (2 mm radius) to avoid signal contamination for better visualization, and obtained the correlation coefficient between the seeds and all other voxels of brain to generate functional connectivity maps from the three spatially restricted seeds (<xref rid="fig6" ref-type="fig">Fig.6B</xref>). It is apparent that hPIT had strong functional connections with nodes of dorsal (FEF, IPS, MT) and ventral (VFC, TPJ) attention networks. Then the functional correlation using whole ROIs (hPIT, FFA, and LOp, example shown in <xref rid="fig6" ref-type="fig">Fig.6C</xref>) with bilateral attention network nodes were calculated on each participant. The connection strength of bilateral hPIT, FFA, and LOp with DAN and VAN were compared using Two-Way ANOVA [n=15, attention networks x seed ROIs], results showed significant main effects of seed ROIs [F (1.732, 24.25) = 8.653, P=0.0021] and attentional networks [F (1.000, 14.00) = 60.56, P&lt;0.0001] (<xref rid="fig6" ref-type="fig">Fig.6D</xref>). Specifically, the correlation coefficients of hPIT with attention networks were stronger than the other two seed areas (Dunnett, one-sided): PIT vs. FFA [q (14) = 2.135, adjusted P= 0.0452], PIT vs. LOp [q (14) = 4.725, adjusted P=0.0003]. The stronger connection of hPIT with each attentional nodes could be visualized in a circular connectivity plot (<xref rid="fig6" ref-type="fig">Fig.6E</xref>). The strong functional connections of hPIT with attention networks, especially the dorsal attention network, supports the important role it plays in attention control.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Functional connectivity analysis of hPIT and its neighboring areas.</title>
<p>(A): Activation map showing beta of contrast [attend contra moving dot - baseline] (p &lt; 0.01, uncorrected) and the location of critical nodes in dorsal and ventral attention network on one typical subject (S03). (B): Thresholded map showing functional connectivity of seed sphere right-hemi hPIT, right-hemi FFA, and right-hemi LOp (Spearman’s rank correlation coefficient &gt; 0.2, p&lt;0.05, uncorrected), averaged across subjects and projected onto the surface of standard brain MNI152_2009c. Color bar attached indicate the intensity of activation (A) and correlation (B). (C): The relative location of LOp, hPIT and FFA on the inflated cortical surface of parcellation map (Glasser’s atlas). (D): Strength of functional connectivity of seed hPIT, FFA and LOp with DAN and VAN. Error bars indicate 95% confidence interval. ∗∗∗ indicates the significance of P &lt; 0.001. ∗ indicates the significance of P &lt; 0.05. (E): Circular plot for functional connectivity of seed hPIT, FFA and LOp with nodes of attention network of right hemisphere, with pink lines indicating connection with nodes of DAN, blue lines indicating nodes of VAN. Connections to left hemisphere nodes show similar but weaker trends. Opacity of each line connecting seed and nodes represents the rank of its connectivity strength (the strongest 100%, the middle 44%, the weakest 11%). Width of each line is scaled to its cubic numerical intensities of connectivity.</p></caption>
<graphic xlink:href="641975v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>It is traditionally accepted that the networks of frontal and parietal cortex play important roles in the control and engagement of endogenous and exogenous attention, while regions of occipito-temporal cortex specialize in the processing of colors and shapes, leading to the representation of scenes and objects, such as faces, bodies, words <sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c26">26</xref>–<xref ref-type="bibr" rid="c33">33</xref></sup>. Our results provide new distinct insights about the role of the human posterior inferotemporal cortex in the control and implementation of attention. We identified a specific area within the posterior inferotemporal cortex, hPIT, where its activation exhibited little category selectivity, but was strongly modulated by attention across tasks, and reflected a combined effect of top- down attention and bottom-up attention. Further, the attentional modulation in hPIT was significantly affected by attention load as well as image category, consistent with the nature of attention. In addition, functional connectivity analysis revealed that hPIT, compared to its neighbor areas FFA and Lop, exhibited stronger connectivity with nodes of the attention network.</p>
<p>The hPIT is in close proximity to other object-processing regions in the inferotemporal cortex, including the FFA, the visual word form area (VWFA), and lateral occipital complex (LOC). The hPIT ((-34, -72, - 14), (33, -73, -13)) is posterior to the FFA (40, -55, -10) <sup><xref ref-type="bibr" rid="c34">34</xref></sup> and VWFA (−43, −56, −16) <sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>. LOC is localized by more activation when viewing objects than scramble, consisting of two subregions: LO (in our paper, LOp) and Loa/pFs <sup><xref ref-type="bibr" rid="c37">37</xref></sup>. The hPIT is inferior to the LOp [bound by (-41, -77, 3) and (-36, -71, -13)], posterior to the Loa/pFs (−38, −50, −17) <sup><xref ref-type="bibr" rid="c38">38</xref></sup>.</p>
<p>Why does a brain region in the inferotemporal cortex exhibit the properties of an attention priority map, integrating the top-down and bottom-up attention? Previous studies have suggested that LIP/IPS, FEF, and SC are closely linked to attention guidance and eye movement and controls: LIP as a representation of attentional priority that remaps across saccades, FEF as an eye movement controller receiving LIP responses, and SC as reflecting the final saccade <sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c45">45</xref></sup>. On the other hand, both top-down and bottom-up attention are frequently associated with objects, ideally an area serving as attentional priority map should be positioned where it is easy to assess object information and also has broad connections with other regions of the attentional networks. The hPIT’s location in the inferotemporal cortex facilitates the integration of key information from adjacent areas specialized in object-processing. As shown in our functional connectivity results as well as in previous studies <sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref></sup>, hPIT is connected with key areas of attention and eye movement controls, such as LIP/IPS, FEF, and SC. The placement of an area serving attentional priority map in the inferotemporal cortex, i.e., hPIT, has strategic advantages for both bottom- up information transmission and top-down attentional modulation.</p>
<p>In addition to the strong connection between hPIT and the dorsal attention network, consistent with a recent study <sup><xref ref-type="bibr" rid="c48">48</xref></sup>, our functional connectivity analysis also revealed positive connections with TPJ and VFC, as shown in individual connectivity maps, though these connections were weaker compared to those with dorsal attention nodes. The connectivity with both the dorsal and ventral attention networks further supports hPIT’s unique role in bridging the bottom-up and top-down attention.</p>
<p>Our study is limited in that the effects of feature-based attention were not specifically examined in our experiments. In addition, our current study lacks the temporal dynamic information of processing in hPIT and its downstream and upstream brain areas due to the use of fMRI measurements. Future studies would benefit from utilizing imaging methods with higher time resolution, with designs that address both spatial and feature-based attention.</p>
<p>In summary, the hPIT showed strong attentional modulation across stimuli and tasks, with sensitivity to attentional load, and more robust attentional modulation in the presence compared to the absence of visual input. The hPIT also demonstrated function connectivity to both dorsal and ventral attention networks. Together, our findings demonstrate the distinct role of hPIT in attention control, namely as an attentional priority map that integrates endogenous and exogenous attentional processes.</p>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<sec id="s4a">
<label>1.</label><title>Participants</title>
<p>Fifteen health volunteers (8 males and 7 females, age ranged from 22 to 28 years old) with normal or corrected to normal vision participated in this study. None of the participants reported history of neurological or psychiatric symptoms. Written informed consent was obtained from all participants, and the study protocol was approved by the Institutional Review Board of the Institute of Biophysics, Chinese Academy of Sciences. Each participant completed three separate experimental sessions, conducted on three different days.</p>
</sec>
<sec id="s4b">
<label>2</label><title>Stimuli and procedures</title>
<p>Visual stimuli for the fMRI experiment were programmed using Matlab (MathWorks, Natick, MA, USA) with Psychtoolbox (<ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link>) and displayed via an MRI-compatible projector (1024 × 768@60 Hz) onto a screen positioned at the rear of the MRI scanner. Participants viewed the stimuli through a mirror attached to the head coil. Because of the complexity of the task, each participant was trained on 2 or 3 separate days, before the fMRI sessions. This training was designed to familiarize participants with the tasks to relieve some possible tension during scanning and minimize the effects of learning.</p>
<sec id="s4b1">
<label>2.1</label><title>Stimuli and procedures for experiment 1</title>
<p>In Experiment 1 (Day 1), participants completed three distinct spatial attention tasks across three separate experimental blocks (Experiments 1a, 1b, 1c). Throughout each task, participants were instructed to maintain fixation on a central point.</p>
<p>Experiment 1a (<xref rid="fig7" ref-type="fig">Fig.7A</xref>): Participants focused on a unilateral moving dot display and was required to discriminate the direction of the coherent motion. As shown on <xref rid="fig7" ref-type="fig">Fig.7A</xref>, each trial began with an initial cue indicating the target side, represented by a bar (0.6° visual angle) attached to the fixation point. Random dot stimuli were then presented within a circular aperture (radius: 3° visual angle) centered 9° from the fixation point. The dot display consisted of dots (size: 0.15° visual angle; density: 5 dots per degree of visual angle) in motion for 2.5 seconds. During this sequence, a 0.5-second interval of coherent motion (coherence: 50%, velocity: 6°/s) occurred randomly between 0.5 and 1.55 seconds after motion onset, while the remaining duration involved random dot movement. Following the motion sequence, the circular aperture disappeared, and an arrow appeared at the center of the screen for 1.5 seconds. Participants were required to press a key to indicate whether the direction of coherent motion matched the direction indicated by the arrow. Each block consisted of three trials with the same attended side, followed by a 6- second rest period.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Schematic representation of the experimental paradigm for the Experiment 1.</title>
<p>(A): The task was to report whether the direction of coherent motion on the attended side matched that of the white arrow. Note: The black arrow, representing one potential direction for the coherent dot movement, is used for illustrative purposes and was not actually presented during the experiment. (B): Pattern 1 and pattern 2, consisting of iso-luminant red and green dots, were presented sequentially. The task was to compare the color ratios of these patterns on the attended side and respond accordingly if any changes in the color ratio were detected. (C): Pattern 1 and pattern 2, consisting of equal number of small shapes (circles and squares) were presented sequentially. The task was to compare the shape ratios of these patterns on the attended side and respond accordingly if any changes in the shape ratio were detected.</p></caption>
<graphic xlink:href="641975v1_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Experiment 1b (<xref rid="fig7" ref-type="fig">Fig.7B</xref>): Participants were instructed to attend to the proportion of red and green dots presented within a unilateral circular aperture. The red and green dots were adjusted individually by each participant to achieve iso-luminance, ensuring perceptual equality between the colors. The positioning of the circular aperture were consistent with Experiment 1a, though the dot size was increased to 0.3° of visual angle in this task. At the start of each trial, within the first second, dots were displayed in random proportions of red (20%, 40%, 60%, 80%) with the remainder being green (pattern 1). Following this, the position of the dots remained static during the 2nd second, but their color could potentially change (pattern2). The proportion of red dots remain to be one of the pre-set ratios. In the final, third second of each trial, the bilateral dots disappeared. Participants were then required to press a key to indicate whether the color ratio of the dots had changed from pattern 1 to pattern 2. Each block in this experiment consisted of four trials, with participants attending to the same side throughout, followed by a 6-second rest period.</p>
<p>Experiment 1c (<xref rid="fig7" ref-type="fig">Fig.7C</xref>): In this experiment, participants were instructed to attend to a unilateral geometrical pattern display, focusing on the proportion of different shapes. Each trial began with the first pattern presented for 2 seconds, followed by a gradual transition in which the second pattern emerged over the next 2 seconds and then remained visible. At the end of this sequence, participants were required to press a key indicating whether the shape proportion between the two patterns had changed. Each block consisted of two trials with attention directed to the same side, followed by a 6-second rest period.</p>
<p>There were 16 blocks in one run, 8 attending left and 8 attending right for experiment 1a, 1b and 1c. On the first day of scanning, participants completed 6 runs of Experiment 1a, 4 runs of Experiment 1b, and 4 runs of Experiment 1c.</p>
</sec>
<sec id="s4b2">
<label>2.2</label><title>Stimuli and procedures for experiment 2</title>
<p>In experiment 2 (day 2), participants first underwent a 488-second resting-state functional scan, during which they passively viewed a grey screen. For the subsequent attention tasks (event-related design), participants were instructed to maintain fixation on a central dot while directing their attention to one side.</p>
<p>The procedure (<xref rid="fig2" ref-type="fig">Fig.2</xref>) began with the central dot (10 pixels in diameter) flashing three times over 2 seconds, serving as an alert for the upcoming cue and target. Following this, a cue was presented for 0.8s. After a variable delay, randomly selected from intervals of 0.1, 0.15, 0.2, or 0.25 seconds, a target dot (8 pixels in diameter, RGB: [0.7,0.7,0.7]) either appeared on the cued side (dot condition) or did not appear (blank condition). In the dot condition, participants were required to report the location of the target by pressing a key. In the blank condition, participants were instructed to refrain from any response.</p>
<p>Each run consisted of 16 trials, with individual trial durations of 14, 16, or 18 seconds. On this second day of scanning, participants completed one run of the resting-state scan, followed by 8 task runs.</p>
</sec>
<sec id="s4b3">
<label>2.3</label><title>Stimuli and procedures for experiment 3</title>
<p>In experiment 3 (day 3), participants were asked to attend unilateral images presented within a circular aperture, consistent with the method used in Experiment 1. The images were drawn from three distinct categories:</p>
<p>Faces: Asian faces with an equal gender distribution (50% female).</p>
<p>Scenes: Equally divided between indoor and outdoor settings (50% outdoor).</p>
<p>Scrambles: Phase-scrambled images superimposed with a grating, with 50% of the gratings spanning the 1st and 3rd quadrants.</p>
<p>To ensure visual consistency across images, they were standardized for luminance histograms and spatial frequency using the SHINE toolbox (as referenced from Willenbockel et al., 2010).</p>
<p>At the onset of each run, participants were instructed to pay extra attention to a designated category throughout that run. The procedure (<xref rid="fig4" ref-type="fig">Fig.4</xref>) began with the center dot flashing three times over two seconds, signaling the forthcoming presentation of the cue and target. Following this, both the cue and bilateral images from the same category were presented simultaneously. This images briefly shifted in one direction before returning to their original position (0.25s+0.25s). During the next 1.5 seconds, participants first pressed a key to indicate whether the direction of the images’ motion matched that of the arrow at the center. A green dot then appeared at the center, reminding participants to categorize the image in their attended location. If the image matched the category to which they had been instructed to pay attention, participants were required to identify specific content about the image (e.g., determining whether a face was female or male) and responded by pressing either key ’1’ or ’2’. If the image did not belong to the designated category, participants simply pressed ’3’.</p>
<p>Each run consisted of 32 trials: 50% of the trials featured images from the emphasized category, and the remaining 50% were evenly distributed between the other two categories (25% each). Each trial lasted either 8, 10, or 12 seconds. Across Experiment 3, there were a total of nine runs, with each category being the focus of three runs. The accuracy of extra judgements is 82.9% for face, 81.8% for scene and 84.3% for scramble [F (1.373, 19.23) = 1.096, P=0.3308].</p>
</sec>
</sec>
<sec id="s4c">
<label>3</label><title>MRI Data Acquisition</title>
<p>MRI scanning was conducted using a 3T Siemens Prisma scanner at the Beijing MRI Center for Brain Research (BMCBR), utilizing a standard 20-channel head coil. High-resolution T1-weighted anatomical images were acquiredat the start of each session (TR = 3000 ms; TE = 3.02 ms; 176 slices; slice thickness = 1 mm; no inter-slice gap; field of view = 256 mm; flip angle = 8°; image matrix: 256×256).</p>
<p>Functional data were collected with gradient-echo EPI sequences (TR = 2000 ms; TE = 30.0 ms; 52 slices; slice thickness = 2 mm; no inter-slice gap; voxel resolution 2.0 x 2.0 x 2.0 mm, field of view = 192 mm; flip angle = 80°; image matrix: 96×96).</p>
</sec>
<sec id="s4d">
<label>4</label><title>MRI Data Analysis</title>
<p>fMRI data were preprocessed and analyzed using FreeSurfer and AFNI and custom Python code. The preprocessing steps included de-spiking, slice timing correction, EPI distortion correction (PE blip-up), rigid body motion correction, spatial smoothing (4 mm FWHM Gaussian kernel, for the task runs), and per run scaling (as percent signal change).</p>
<p>For task runs, general linear models were used to estimate BOLD signal change from baseline for each stimulus condition. For each individual, bilateral FFA, OFA and PPA were defined based on the functional contrast between faces and scenes from experiment 3. Bilateral V1 were defined using function contrast [attend contra moving dot - baseline] (P &lt; 0.001, uncorrected), and IPS, FEF and MT were defined using the same contrast (P &lt; 0.01, uncorrected) and referring the cerebral atlas (Glasser, 2016). Due to the lack of standardized anatomical definitions, bilateral VFC and TPJ were identified using the same functional contrast (P &lt; 0.01, uncorrected) <sup><xref ref-type="bibr" rid="c5">5</xref></sup>. Bilateral hPIT was localized using data from all three tasks in Experiment 1, by intersecting activation maps on the inferotemporal surface, while avoiding the locations of FFA and PPA.</p>
<p>The resting-state data was preprocessed similarly to the functional data, with additional steps for removing white matter and cerebrospinal fluid signals. Subsequently, confound regression, spatial smoothing (4 mm), and bandpass filtering (0.01–0.1 Hz) were performed using AFNI’s 3dTproject.</p>
</sec>
</sec>

</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by STI2030-Major Projects (Grant Nos. 2021ZD0204200 and 2021ZD0203800); and Key Research Program of Frontier Sciences, Chinese Academy of Science (Grant No. KJZD-SW- L08).</p>
</ack>
<sec id="suppd1e849" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supporting Information</label>
<media xlink:href="supplements/641975_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Duncan</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Neural Mechanisms of Selective Visual Attention</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>18</volume>, <fpage>193</fpage>–<lpage>222</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Connor</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Egeth</surname>, <given-names>H. E.</given-names></string-name> &amp; <string-name><surname>Yantis</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Visual attention: bottom-up versus top-down</article-title>. <source>Current Biology</source> <volume>14</volume>, <fpage>R850</fpage>–<lpage>R852</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Katsuki</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Constantinidis</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Bottom-Up and Top-Down Attention: Different Processes and Overlapping Neural Systems</article-title>. <source>Neuroscientist</source> <volume>20</volume>, <fpage>509</fpage>–<lpage>521</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buschman</surname>, <given-names>T. J.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>E. K</given-names></string-name></person-group>. <article-title>Top-Down Versus Bottom-Up Control of Attention in the Prefrontal and Posterior Parietal Cortices</article-title>. <source>Science</source> <volume>315</volume>, <fpage>1860</fpage>–<lpage>1862</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vossel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Geng</surname>, <given-names>J. J.</given-names></string-name> &amp; <string-name><surname>Fink</surname>, <given-names>G. R</given-names></string-name></person-group>. <article-title>Dorsal and Ventral Attention Systems: Distinct Neural Circuits but Collaborative Roles</article-title>. <source>Neuroscientist</source> <volume>20</volume>, <fpage>150</fpage>–<lpage>159</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Shulman</surname>, <given-names>G. L</given-names></string-name></person-group>. <article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title>. <source>Nat Rev Neurosci</source> <volume>3</volume>, <fpage>201</fpage>–<lpage>215</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Shulman</surname>, <given-names>G. L</given-names></string-name></person-group>. <article-title>The Reorienting System of the Human Brain: From Environment to Theory of Mind</article-title>. <source>Neuron</source> <volume>58</volume>, <fpage>306</fpage>–<lpage>324</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noudoost</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name> &amp; <string-name><surname>Moore</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Top-down control of visual attention</article-title>. <source>Current Opinion in Neurobiology</source> <volume>20</volume>, <fpage>183</fpage>–<lpage>190</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bisley</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Mirpour</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>The neural instantiation of a priority map</article-title>. <source>Current Opinion in Psychology</source> <volume>29</volume>, <fpage>108</fpage>–<lpage>112</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bisley</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Goldberg</surname>, <given-names>M. E.</given-names></string-name></person-group> <article-title>Attention, Intention, and Priority in the Parietal Lobe</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>33</volume>, <fpage>1</fpage>–<lpage>21</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treue</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Visual attention: the where, what, how and why of saliency</article-title>. <source>Current Opinion in Neurobiology</source> <volume>13</volume>, <fpage>428</fpage>–<lpage>432</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stemmann</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Freiwald</surname>, <given-names>W. A</given-names></string-name></person-group>. <article-title>Evidence for an attentional priority map in inferotemporal cortex</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>116</volume>, <fpage>23797</fpage>–<lpage>23805</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ptak</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>The Frontoparietal Attention Network of the Human Brain: Action, Saliency, and a Priority Map of the Environment</article-title>. <source>Neuroscientist</source> <volume>18</volume>, <fpage>502</fpage>–<lpage>515</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fecteau</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Munoz</surname>, <given-names>D</given-names></string-name></person-group>. <article-title>Salience, relevance, and firing: a priority map for target selection</article-title>. <source>Trends in Cognitive Sciences</source> <volume>10</volume>, <fpage>382</fpage>–<lpage>390</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Katsuki</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Constantinidis</surname>, <given-names>C</given-names></string-name></person-group>. <article-title>Early involvement of prefrontal cortex in visual bottom-up attention</article-title>. <source>Nat Neurosci</source> <volume>15</volume>, <fpage>1160</fpage>–<lpage>1166</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arcizet</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Mirpour</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Bisley</surname>, <given-names>J. W</given-names></string-name></person-group>. <article-title>A Pure Salience Response in Posterior Parietal Cortex</article-title>. <source>Cerebral Cortex</source> <volume>21</volume>, <fpage>2498</fpage>–<lpage>2506</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrington</surname>, <given-names>T. M.</given-names></string-name> &amp; <string-name><surname>Assad</surname>, <given-names>J. A</given-names></string-name></person-group>. <article-title>Neural Activity in the Middle Temporal Area and Lateral Intraparietal Area during Endogenously Cued Shifts of Attention</article-title>. <source>J. Neurosci</source>. <volume>29</volume>, <fpage>14160</fpage>–<lpage>14176</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ninomiya</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Sawamura</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Inoue</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Takada</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Segregated Pathways Carrying Frontally Derived Top-Down Signals to Visual Areas MT and V4 in Macaques</article-title>. <source>J. Neurosci</source>. <volume>32</volume>, <fpage>6851</fpage>–<lpage>6858</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schall</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Purcell</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Heitz</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Logan</surname>, <given-names>G. D.</given-names></string-name> &amp; <string-name><surname>Palmeri</surname>, <given-names>T. J</given-names></string-name></person-group>. <article-title>Neural mechanisms of saccade target selection: gated accumulator model of the visual–motor cascade</article-title>. <source>Eur J of Neuroscience</source> <volume>33</volume>, <fpage>1991</fpage>–<lpage>2002</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benarroch</surname>, <given-names>E</given-names></string-name></person-group>. <article-title>What Are the Functions of the Superior Colliculus and Its Involvement in Neurologic Disorders?</article-title> <source>Neurology</source> <volume>100</volume>, <fpage>784</fpage>–<lpage>790</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rushworth</surname>, <given-names>M. F. S.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E. J.</given-names></string-name> &amp; <string-name><surname>Johansen-Berg</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>Connection Patterns Distinguish 3 Regions of Human Parietal Cortex</article-title>. <source>Cerebral Cortex</source> <volume>16</volume>, <fpage>1418</fpage>–<lpage>1430</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>White</surname>, <given-names>B. J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Superior colliculus neurons encode a visual saliency map during free viewing of natural dynamic video</article-title>. <source>Nat Commun</source> <volume>8</volume>, <issue>14263</issue> (<year>2017</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sani</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>McPherson</surname>, <given-names>B. C.</given-names></string-name>, <string-name><surname>Stemmann</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Pestilli</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Freiwald</surname>, <given-names>W. A</given-names></string-name></person-group>. <article-title>Functionally defined white matter of the macaque monkey brain reveals a dorso-ventral attention network</article-title>. <source>eLife</source> <volume>8</volume>, <elocation-id>e40520</elocation-id> (<year>2019</year>). <pub-id pub-id-type="doi">10.7554/eLife.40520</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sani</surname>, <given-names>I.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The human endogenous attentional control network includes a ventro-temporal cortical node</article-title>. <source>Nat Commun</source> <volume>12</volume>, <issue>360</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A multi-modal parcellation of human cerebral cortex</article-title>. <source>Nature</source> <volume>536</volume>, <fpage>171</fpage>–<lpage>178</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ungerleider</surname>, <given-names>S. K. A. L. G</given-names></string-name></person-group>. <article-title>Mechanisms of Visual Attention in the Human Cortex</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>23</volume>, <fpage>315</fpage>–<lpage>341</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Armstrong</surname>, <given-names>K. M.</given-names></string-name> &amp; <string-name><surname>Fallah</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Visuomotor Origins of Covert Spatial Attention</article-title>. <source>Neuron</source> <volume>40</volume>, <fpage>671</fpage>–<lpage>683</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Zirnsak</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Neural Mechanisms of Selective Visual Attention</article-title>. <source>Annu. Rev. Psychol</source>. <volume>68</volume>, <fpage>47</fpage>–<lpage>72</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baldauf</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Desimone</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Neural Mechanisms of Object-Based Attention</article-title>. <source>Science</source> <volume>344</volume>, <fpage>424</fpage>–<lpage>427</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lafer-Sousa</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Conway</surname>, <given-names>B. R</given-names></string-name></person-group>. <article-title>Parallel, multi-stage processing of colors, faces and shapes in macaque inferior temporal cortex</article-title>. <source>Nat Neurosci</source> <volume>16</volume>, <fpage>1870</fpage>–<lpage>1878</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name> &amp; <string-name><surname>Downing</surname>, <given-names>P. E</given-names></string-name></person-group>. <article-title>Selectivity for the Human Body in the Fusiform Gyrus</article-title>. <source>Journal of Neurophysiology</source> <volume>93</volume>, <fpage>603</fpage>–<lpage>608</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCandliss</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Dehaene</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>The visual word form area: expertise for reading in the fusiform gyrus</article-title>. <source>Trends in Cognitive Sciences</source> <volume>7</volume>, <fpage>293</fpage>–<lpage>299</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Weiner</surname>, <given-names>K. S</given-names></string-name></person-group>. <article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title>. <source>Nat Rev Neurosci</source> <volume>15</volume>, <fpage>536</fpage>–<lpage>548</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>McDermott</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Chun</surname>, <given-names>M. M</given-names></string-name></person-group>. <article-title>The Fusiform Face Area: A Module in Human Extrastriate Cortex Specialized for Face Perception</article-title>. <source>J. Neurosci</source>. <volume>17</volume>, <fpage>4302</fpage>–<lpage>4311</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rauschecker</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Bowen</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Parvizi</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Wandell</surname>, <given-names>B. A</given-names></string-name></person-group>. <article-title>Position sensitivity in the visual word form area</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>109</volume>, (<year>2012</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Weng</surname>, <given-names>X</given-names></string-name></person-group>. <article-title>Localization and Functional Characterization of an Occipital Visual Word form Sensitive Area</article-title>. <source>Sci Rep</source> <volume>8</volume>, <fpage>6723</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kourtzi</surname>, <given-names>Z.</given-names></string-name> &amp; <string-name><surname>Kanwisher</surname>, <given-names>N</given-names></string-name></person-group>. <article-title>The lateral occipital complex and its role in object recognition</article-title>. <source>Vision Research</source> <volume>41</volume>, <fpage>1409</fpage>–<lpage>1422</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Differential Processing of Objects under Various Viewing Conditions in the Human Lateral Occipital Complex</article-title>. <source>Neuron</source> <volume>24</volume>, <fpage>187</fpage>–<lpage>203</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thompson</surname>, <given-names>K. G.</given-names></string-name>, <string-name><surname>Hanes</surname>, <given-names>D. P.</given-names></string-name>, <string-name><surname>Bichot</surname>, <given-names>N. P.</given-names></string-name> &amp; <string-name><surname>Schall</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Perceptual and motor processing stages identified in the activity of macaque frontal eye field neurons during visual search</article-title>. <source>Journal of Neurophysiology</source> <volume>76</volume>, <fpage>4040</fpage>–<lpage>4055</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McPeek</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Keller</surname>, <given-names>E. L</given-names></string-name></person-group>. <article-title>Saccade Target Selection in the Superior Colliculus During a Visual Search Task</article-title>. <source>Journal of Neurophysiology</source> <volume>88</volume>, <fpage>2019</fpage>–<lpage>2034</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buschman</surname>, <given-names>T. J.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>E. K.</given-names></string-name></person-group> <article-title>Serial, Covert Shifts of Attention during Visual Search Are Reflected by the Frontal Eye Fields and Correlated with Population Oscillations</article-title>. <source>Neuron</source> <volume>63</volume>, <fpage>386</fpage>–<lpage>396</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Desimone</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Feature-Based Attention in the Frontal Eye Field and Area V4 during Visual Search</article-title>. <source>Neuron</source> <volume>70</volume>, <fpage>1205</fpage>–<lpage>1217</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Paré</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Neuronal activity in superior colliculus signals both stimulus identity and saccade goals during visual conjunction search</article-title>. <source>Journal of Vision</source> <volume>7</volume>, <issue>15</issue> (<year>2007</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Foley</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Kelly</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Mhatre</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lopes</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Gottlieb</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Parietal neurons encode expected gains in instrumental information</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>114</volume>, (<year>2017</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersen</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Brotchie</surname>, <given-names>P. R.</given-names></string-name> &amp; <string-name><surname>Mazzoni</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Evidence for the lateral intraparietal area as the parietal eye field</article-title>. <source>Current Opinion in Neurobiology</source> <volume>2</volume>, <fpage>840</fpage>–<lpage>846</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boshra</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Kastner</surname>, <given-names>S</given-names></string-name></person-group>. <article-title>Attention control in the primate brain</article-title>. <source>Current Opinion in Neurobiology</source> <volume>76</volume>, <issue>102605</issue> (<year>2022</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bogadhi</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Bollimunta</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Leopold</surname>, <given-names>D. A.</given-names></string-name> &amp; <string-name><surname>Krauzlis</surname>, <given-names>R. J</given-names></string-name></person-group>. <article-title>Spatial Attention Deficits Are Causally Linked to an Area in Macaque Temporal Cortex</article-title>. <source>Current Biology</source> <volume>29</volume>, <fpage>726</fpage>–<lpage>736.e4</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meng</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Zhou</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>Orienting role of the putative human posterior infero-temporal area in visual attention</article-title>. <source>Cortex</source> <volume>175</volume>, <fpage>54</fpage>–<lpage>65</lpage> (<year>2024</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107111.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Shuo</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Washington University in St. Louis</institution>
</institution-wrap>
<city>St. Louis</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study reports that the human posterior inferotemporal cortex (hPIT) functions as an attentional priority map, integrating both top-down and bottom-up attentional signals rather than serving solely as an object-processing region. The experiments and analyses are well conducted and provide <bold>convincing</bold> evidence that hPIT bridges dorsal and ventral attention networks and is robustly modulated by attention across diverse visual tasks. The study will be relevant for researchers investigating visual attention, high-level visual cortex, and the neural mechanisms that integrate endogenous and exogenous attentional control.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107111.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript titled &quot;The distinct role of human PIT in attention control&quot; by Huang et al. investigates the role of the human posterior inferotemporal cortex (hPIT) in spatial attention. Using fMRI experiments and resting-state connectivity analyses, the authors present compelling evidence that hPIT is not merely an object-processing area, but also functions as an attentional priority map, integrating both top-down and bottom-up attentional processes. This challenges the traditional view that attentional control is localized primarily in frontoparietal networks.</p>
<p>The manuscript is strong and of high potential interest to the cognitive neuroscience community. Below, I raise questions and suggestions to help with the reliability, methodology, and interpretation of the findings.</p>
<p>(1) The authors argue that hPIT satisfies the criteria for a priority map, but a clearer justification would strengthen this claim. For example, how does hPIT meet all four widely recognized criteria, such as spatial selectivity, attentional modulation, feature invariance, and input integration, when compared to classical regions such as LIP or FEF? A more systematic summary of how hPIT meets these benchmarks would be helpful. Additionally, to what extent are the observed attentional modulations in hPIT independent of general task difficulty or behavioral performance?</p>
<p>(2) The authors report that hPIT modulation is invariant to stimulus category, but there appear to be subtle category-related effects in the data. Were the face, scene, and scrambled images matched not only in terms of luminance and spatial frequency, but also in terms of factors such as semantic familiarity and emotional salience? This may influence attentional engagement and bias interpretation.</p>
<p>(3) The result that attentional load modulates hPIT is important and adds depth to the main conclusions. However, some clarifications would help with the interpretation. For example, were there observable individual differences in the strength of attentional modulation? How consistent were these effects across participants?</p>
<p>(4) The resting-state data reveal strong connections between hPIT and both dorsal and ventral attention networks. However, the analysis is correlational. Are there any complementary insights from task-based functional connectivity or latency analyses that support a directional flow of information involving hPIT? In addition, do the authors interpret hPIT primarily as a convergence hub receiving input from both DAN and VAN, or as a potential control node capable of influencing activity in these networks? Also, were there any notable differences between hemispheres in either the connectivity patterns or attentional modulation?</p>
<p>(5) A few additional questions arise regarding the anatomical characteristics of hPIT: How consistent were its location and size across participants? Were there any cases where hPIT could not be reliably defined? Given the proximity of hPIT to FFA and LOp, how was overlap avoided in ROI definition? Were the functional boundaries confirmed using independent contrasts?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107111.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>This study investigates the role of the human posterior inferotemporal cortex (hPIT) in attentional control, proposing that hPIT serves as an attentional priority map that integrates both top-down (endogenous) and bottom-up (exogenous) attentional processes. The authors conducted three types of fMRI experiments and collected resting-state data from 15 participants. In Experiment 1, using three different spatial attention tasks, they identified the hPIT region and demonstrated that this area is modulated by attention across tasks. In Experiment 2, by manipulating the presence or absence of visual stimuli, they showed that hPIT exhibits strong attentional modulation in both conditions, suggesting its involvement in both bottom-up and top-down attention. Experiment 3 examined the sensitivity of hPIT to stimulus features and attentional load, revealing that hPIT is insensitive to stimulus category but responsive to task load - further supporting its role as an attentional priority map. Finally, resting-state functional connectivity analyses showed that hPIT is connected to both dorsal and ventral attention networks, suggesting its potential role as a bridge between the two systems. These findings extend prior work on monkey PITd and provide new insights into the integration of endogenous and exogenous attention.</p>
<p>Strengths</p>
<p>(1) The study is innovative in its use of specially designed spatial attention tasks to localize and validate hPIT, and in exploring the region's role in integrating both endogenous and exogenous attention, as prior works focus primarily on its involvement in endogenous attention.</p>
<p>(2) The authors provided very comprehensive experiment designs with clear figures and detailed descriptions.</p>
<p>(3) A broad range of analyses was conducted to support the hypothesis that hPIT functions as an attentional priority map -- including experiments of attentional modulation under both top-down and bottom-up conditions, sensitivity to stimulus features and task load, and resting-state functional connectivity. These analyses showed consistent results.</p>
<p>(4) Multiple appropriate statistical analyses - including t-tests, ANOVAs, and post-hoc tests - were conducted, and the results are clearly reported.</p>
<p>Weaknesses</p>
<p>(1) The sample size is relatively small (n = 15), and inter-subject variability is big in Figures 5 and 6, as seen in the spread of individual data points and error bars. The analysis of attention-modulated voxel map intersections appears to be influenced by multiple outliers.</p>
<p>(2) The authors acknowledge important limitations, including the lack of exploration of feature-based attention and the temporal constraints inherent to fMRI.</p>
<p>(3) Prior research has established that regions such as the prefrontal cortex (PFC) and posterior parietal cortex (PPC) are involved in both endogenous and exogenous attention and have been proposed as attentional priority maps. It remains unclear what is uniquely contributed by hPIT, how it functionally interacts with these classical attentional hubs, and whether its role is complementary or redundant. The study would benefit from more direct comparisons with these regions.</p>
<p>(4) The functional connectivity analysis is only performed on resting-state data, and this approach does not capture context-dependent interactions. Task-based data analysis can provide stronger evidence.</p>
<p>(5) The study does not report whether attentional modulation in hPIT is consistent across the two hemispheres. A comparison of hemispheric effects could provide important insight into lateralization and inter-individual variability, especially given the bilateral localization of hPIT.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107111.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Huang</surname>
<given-names>Siyuan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0004-2347-293X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Lan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>He</surname>
<given-names>Sheng</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>The manuscript titled &quot;The distinct role of human PIT in attention control&quot; by Huang et al. investigates the role of the human posterior inferotemporal cortex (hPIT) in spatial attention. Using fMRI experiments and resting-state connectivity analyses, the authors present compelling evidence that hPIT is not merely an object-processing area, but also functions as an attentional priority map, integrating both top-down and bottom-up attentional processes. This challenges the traditional view that attentional control is localized primarily in frontoparietal networks.</p>
<p>The manuscript is strong and of high potential interest to the cognitive neuroscience community. Below, I raise questions and suggestions to help with the reliability, methodology, and interpretation of the findings.</p>
</disp-quote>
<p>Thank you for a nice summary of the key points of our study. Below you will find our responses to your questions.</p>
<disp-quote content-type="editor-comment">
<p>(1) The authors argue that hPIT satisfies the criteria for a priority map, but a clearer justification would strengthen this claim. For example, how does hPIT meet all four widely recognized criteria, such as spatial selectivity, attentional modulation, feature invariance, and input integration, when compared to classical regions such as LIP or FEF? A more systematic summary of how hPIT meets these benchmarks would be helpful. Additionally, to what extent are the observed attentional modulations in hPIT independent of general task difficulty or behavioral performance?</p>
</disp-quote>
<p>Great suggestions! For the first suggestion, we will include a clearer justification in the revised manuscript. For the second one, all participants received task practice prior to scanning, and task accuracy exceeded 90% (we will explicitly report the accuracy rate in revision), suggesting the tasks were not overly demanding. Although ceiling effects limit the interpretability of behavioral-performance correlations, we argue that higher task demands would likely require greater attentional effort, leading to stronger modulation in hPIT, which aligns with our findings when we manipulated the attentional load.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors report that hPIT modulation is invariant to stimulus category, but there appear to be subtle category-related effects in the data. Were the face, scene, and scrambled images matched not only in terms of luminance and spatial frequency, but also in terms of factors such as semantic familiarity and emotional salience? This may influence attentional engagement and bias interpretation.</p>
</disp-quote>
<p>The response of hPIT is generally insensitive to stimulus category, however, the reviewer is correct in noticing that attentional modulation in hPIT is slightly stronger to faces than scenes and scrambled images. Although faces used in the task had neutral expressions and the scene pictures were also neutral, it is indeed possible that potential semantic familiarity or emotional salience may contribute to the subtle category-related effects in the results of experiment 3. This point will be noted in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(3) The result that attentional load modulates hPIT is important and adds depth to the main conclusions. However, some clarifications would help with the interpretation. For example, were there observable individual differences in the strength of attentional modulation? How consistent were these effects across participants?</p>
</disp-quote>
<p>Yes, individual differences exist. In the revised manuscript, we will include individual subject data points in the figure 6B.</p>
<disp-quote content-type="editor-comment">
<p>(4) The resting-state data reveal strong connections between hPIT and both dorsal and ventral attention networks. However, the analysis is correlational. Are there any complementary insights from task-based functional connectivity or latency analyses that support a directional flow of information involving hPIT? In addition, do the authors interpret hPIT primarily as a convergence hub receiving input from both DAN and VAN, or as a potential control node capable of influencing activity in these networks? Also, were there any notable differences between hemispheres in either the connectivity patterns or attentional modulation?</p>
</disp-quote>
<p>We agree that besides resting-state connection, task-based functional connectivity analyses would have the potential to provide additional information about whether hPIT serves as a convergence node or a control hub. While fMRI data are not the best to generate directional flow of information due to the low temporal resolution, we will conduct task-based functional connectivity analyses.</p>
<p>We also observed modest hemispheric asymmetries in connectivity—for instance, both left and right hPIT showed stronger connectivity with right-hemisphere attention nodes. This will be described in the revised supplement.</p>
<disp-quote content-type="editor-comment">
<p>(5) A few additional questions arise regarding the anatomical characteristics of hPIT: How consistent were its location and size across participants? Were there any cases where hPIT could not be reliably defined? Given the proximity of hPIT to FFA and LOp, how was overlap avoided in ROI definition? Were the functional boundaries confirmed using independent contrasts?</p>
</disp-quote>
<p>The size and location of hPIT are generally consistent across subjects, as shown in Supplementary Figure 1. The consistency is also supported by figure 4C.
The hPIT is defined by conjunction maps across three tasks and then manually delineated avoiding overlapping voxels with FFA and LOp. The FFA was defined using an independent contrast (Exp3 contrast [face-scene]) and the Lop location was defined by anatomical parcellation (Glasser et al., 2016).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary</p>
<p>This study investigates the role of the human posterior inferotemporal cortex (hPIT) in attentional control, proposing that hPIT serves as an attentional priority map that integrates both top-down (endogenous) and bottom-up (exogenous) attentional processes. The authors conducted three types of fMRI experiments and collected resting-state data from 15 participants. In Experiment 1, using three different spatial attention tasks, they identified the hPIT region and demonstrated that this area is modulated by attention across tasks. In Experiment 2, by manipulating the presence or absence of visual stimuli, they showed that hPIT exhibits strong attentional modulation in both conditions, suggesting its involvement in both bottom-up and top-down attention. Experiment 3 examined the sensitivity of hPIT to stimulus features and attentional load, revealing that hPIT is insensitive to stimulus category but responsive to task load - further supporting its role as an attentional priority map. Finally, resting-state functional connectivity analyses showed that hPIT is connected to both dorsal and ventral attention networks, suggesting its potential role as a bridge between the two systems. These findings extend prior work on monkey PITd and provide new insights into the integration of endogenous and exogenous attention.</p>
<p>Strengths</p>
<p>(1) The study is innovative in its use of specially designed spatial attention tasks to localize and validate hPIT, and in exploring the region's role in integrating both endogenous and exogenous attention, as prior works focus primarily on its involvement in endogenous attention.</p>
<p>(2) The authors provided very comprehensive experiment designs with clear figures and detailed descriptions.</p>
<p>(3) A broad range of analyses was conducted to support the hypothesis that hPIT functions as an attentional priority map -- including experiments of attentional modulation under both top-down and bottom-up conditions, sensitivity to stimulus features and task load, and resting-state functional connectivity. These analyses showed consistent results.</p>
<p>(4) Multiple appropriate statistical analyses - including t-tests, ANOVAs, and post-hoc tests - were conducted, and the results are clearly reported.</p>
</disp-quote>
<p>Thank you for a nice summary of the key points and strengths of our study.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses</p>
<p>(1) The sample size is relatively small (n = 15), and inter-subject variability is big in Figures 5 and 6, as seen in the spread of individual data points and error bars. The analysis of attention-modulated voxel map intersections appears to be influenced by multiple outliers.</p>
</disp-quote>
<p>We agree that the sample size (n = 15) is not ideal, and we acknowledge that some data points in Figures 5 and 6 appear to be potential outliers. However, according to conventional outlier detection criteria, all data points are within three standard deviations of the group mean and were therefore retained for analysis. Moreover, the attention-modulated voxel intersection map shown in Figure 4C is insensitive to outliers, because the intersection map plotted is based on the number of subjects.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors acknowledge important limitations, including the lack of exploration of feature-based attention and the temporal constraints inherent to fMRI.</p>
</disp-quote>
<p>Yes, we hope to address these limitations in future studies.</p>
<disp-quote content-type="editor-comment">
<p>(3) Prior research has established that regions such as the prefrontal cortex (PFC) and posterior parietal cortex (PPC) are involved in both endogenous and exogenous attention and have been proposed as attentional priority maps. It remains unclear what is uniquely contributed by hPIT, how it functionally interacts with these classical attentional hubs, and whether its role is complementary or redundant. The study would benefit from more direct comparisons with these regions.</p>
</disp-quote>
<p>In this study, we define the ROI base on intersection across three different types of spatial attention tasks, and the hPIT stands out in showing spatial attentional modulation across tasks. This could be due to the weak lateralized responses in PFC/PPC. To evaluate whether a region qualifies as a priority map, we applied four criteria (as mentioned in introduction). While dorsal and ventral attention network (DAN and VAN) regions can be considered important components of the priority map system, our findings suggest that among the regions tested, hPIT meets all four criteria. In Experiment 2, we included regions such as VFC (as part of PFC) and IPS (as part of PPC), and our findings suggest these areas are more involved in top-down attention. We agree with the reviewer’s suggestion and will perform additional analysis on PPC and PFC.</p>
<disp-quote content-type="editor-comment">
<p>(4) The functional connectivity analysis is only performed on resting-state data, and this approach does not capture context-dependent interactions. Task-based data analysis can provide stronger evidence.</p>
</disp-quote>
<p>We acknowledge that resting-state FC is limited in assessing task-specific communication. To further investigate the role of hPIT, we plan to conduct task-based functional connectivity analyses.</p>
<disp-quote content-type="editor-comment">
<p>(5) The study does not report whether attentional modulation in hPIT is consistent across the two hemispheres. A comparison of hemispheric effects could provide important insight into lateralization and inter-individual variability, especially given the bilateral localization of hPIT.</p>
</disp-quote>
<p>We thank the reviewer for this suggestion. hPIT was localized bilaterally using the same intersection-based method in Experiment 1. We have now performed additional analysis and found in Experiment 3, the difference in attentional modulation between high and low load conditions was significant in the right hPIT but not in the left. This result will be reported in the revised manuscript.</p>
</body>
</sub-article>
</article>