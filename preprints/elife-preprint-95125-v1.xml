<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95125</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95125</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95125.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Predicting individual traits from models of brain dynamics accurately and reliably using the Fisher kernel</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9287-1254</contrib-id>
<name>
<surname>Ahrends</surname>
<given-names>C</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Woolrich</surname>
<given-names>M</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Vidaurre</surname>
<given-names>D</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Center of Functionally Integrative Neuroscience, Department of Clinical Medicine, Aarhus University</institution>, Aarhus, <country>Denmark</country></aff>
<aff id="a2"><label>2</label><institution>Oxford Centre for Human Brain Activity, Department of Psychiatry, University of Oxford</institution>, Oxford, <country>United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label> Corresponding author; email: <email>dvidaurre@cfin.au.dk</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-03-07">
<day>07</day>
<month>03</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95125</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-12-22">
<day>22</day>
<month>12</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-07-21">
<day>21</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.02.530638"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Ahrends et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Ahrends et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95125-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Predicting an individual’s cognitive traits or clinical condition using brain signals is a central goal in modern neuroscience. This is commonly done using either structural aspects, or aggregated measures of brain activity that average over time. But these approaches are missing what can be the most representative aspect of these complex human features: the uniquely individual ways in which brain activity unfolds over time, that is, the dynamic nature of the brain. The reason why these dynamic patterns are not usually taken into account is that they have to be described by complex, high-dimensional models; and it is unclear how best to use information from these models for a prediction. We here propose an approach that describes dynamic functional connectivity and amplitude patterns using a Hidden Markov model (HMM) and combines it with the Fisher kernel, which can be used to predict individual traits. The Fisher kernel is constructed from the HMM in a mathematically principled manner, thereby preserving the structure of the underlying HMM. In this way, the unique, individual signatures of brain dynamics can be explicitly leveraged for prediction. We here show in fMRI data that the HMM-Fisher kernel approach is not only more accurate, but also more reliable than other methods, including ones based on time-averaged functional connectivity. This is important because reliability is critical for many practical applications, especially if we want to be able to meaningfully interpret model errors, like for the concept of brain age. In summary, our approach makes it possible to leverage information about an individual’s brain dynamics for prediction in cognitive neuroscience and personalised medicine.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Substantial revisions and additions, including simulations, effects of removing features, and comparisons with time-averaged methods</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/ahrends/FisherKernel">https://github.com/ahrends/FisherKernel</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Observing a person’s behaviour over time is how we understand the individual’s personality, cognitive traits, or psychiatric condition such as schizophrenia or depression. The same applies at the brain level, by observing the patterns by which its activity unfolds over time. However, although research into brain dynamics has gained traction over the last years, there is currently no established methodology to use this level of description to characterise subject differences or predict individual traits. Since brain dynamics are not an explicit feature of the data, intermediate representations of the dynamics are used as features for prediction. These representations can be estimated from modalities such as fMRI or MEG using models of varying complexity (<xref ref-type="bibr" rid="c20">Lurie et al., 2019</xref>), and are usually parametrised by a high number of parameters with complex mathematical relationships between them that reflect the model assumptions. How to leverage these parameters for prediction is an open question.</p>
<p>We here propose the Fisher kernel for predicting individual traits from brain dynamics, using information of how brain activity fluctuates over time during unconstrained cognition —that is, during wakeful rest. We focus on models of brain dynamics that capture within-session changes in functional connectivity and amplitude from fMRI scans, and how the parameters from these models can be used to predict behavioural variables or traits. In particular, we use the Hidden Markov Model (HMM) as a model of time-varying amplitude and functional connectivity (FC) dynamics (<xref ref-type="bibr" rid="c42">Vidaurre et al., 2017</xref>), which was previously shown to be able to predict certain subject traits, such as fluid intelligence, more accurately than structural or static (time-averaged) FC representations (<xref ref-type="bibr" rid="c40">Vidaurre et al., 2021</xref>). Specifically, the Fisher kernel allows using the entire set of parameters of a generative probabilistic model of brain dynamics in a computationally efficient way. It takes the complex relationships between the parameters into account, preserving the structure of the underlying model of brain dynamics (here, the HMM) (<xref ref-type="bibr" rid="c17">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="c18">Jaakkola &amp; Haussler, 1998</xref>).</p>
<p>For empirical evaluation, we consider two criteria that are important in both scientific and practical applications. First, predictions should be as accurate as possible; i.e., the correlation between predicted and actual values should be high. Second, they should be reliable, in the sense that a predictive model should never produce excessively large errors, and the outcome should be robust to the choice of which subjects are included in the training set. The latter criterion is especially important if we want to be able to meaningfully interpret prediction errors, e.g., in assessing brain age (<xref ref-type="bibr" rid="c9">Cole &amp; Franke, 2017</xref>; <xref ref-type="bibr" rid="c10">Denissen et al., 2022</xref>; <xref ref-type="bibr" rid="c32">Smith et al., 2019</xref>). Despite this crucial role in interpreting model errors, reliability is not often taken into account in models predicting individual traits from neuroimaging features.</p>
<p>In summary, we show that using the Fisher kernel approach, which preserves the complex structure of the underlying HMM, we can predict individual traits from brain dynamics accurately and reliably. We show that our approach significantly outperforms methods that do not take the mathematical structure of the model into account, as well as methods based on time-averaged FC that do not consider brain dynamics. For interpretation, we also investigate which aspects of the brain dynamics drive the prediction accuracy. Bringing accuracy, reliability and interpretation together, this work therefore opens possibilities for practical applications such as the development of biomarkers and the investigation of individual differences in cognitive traits.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Methods</title>
<p>We here aim to predict behavioural and demographic variables from a model of brain dynamics using different kernel functions. The general workflow is illustrated in <bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>. We start with the concatenated fMRI time-series of a group of subjects (<bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>, step 1). We estimate a state-space model of brain dynamics at the group level, where the state descriptions, initial state probabilities, and the state transition probability matrix are shared across subjects (<bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>, step 2). Next, we estimate subject-specific versions of this group-level brain dynamics model by dual estimation, where the group-level HMM parameters are re-estimated to fit the individual-level timeseries (<xref ref-type="bibr" rid="c40">Vidaurre et al., 2021</xref>) (<bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>, step 3).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><p>Workflow of the Fisher kernel prediction approach. To generate a description of brain dynamics, we (1) concatenate all subjects’ individual timeseries; then (2) estimate a Hidden Markov Model (HMM) on these timeseries to generate a group-level description of brain dynamics; then (3) dual-estimate into subject-level HMM models. Steps 1-3 are the same for all kernels. In order to then use this description of all subjects’ individual brain dynamics, we map each subject into a feature space (4). This mapping can be done in different ways: In the naïve kernels (a), the manifold on which the parameters lie is ignored and examples are treated as if they were in Euclidean space. The Fisher kernel (b), on the other hand, reflects the structure of the parameters in their original Riemannian manifold by working in the gradient space. We then construct kernel matrices k, where each pair of subjects has a similarity value given their parameters in the respective embedding space. Finally, we feed k to kernel ridge regression to predict a variety of demographic and behavioural traits in a cross-validated fashion (5).</p></caption>
<graphic xlink:href="530638v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we use this description of the individuals’ brain dynamics to predict their individual traits. For this, we first map the parameters from the model of brain dynamics into a feature space (<bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>, step 4). This works in different ways for the different kernels: In the naïve kernel (step 4a), the features are simply the parameters in the Euclidean space; while in the Fisher kernel (step 4b), the features are mapped into the gradient space. We then estimate the similarity between each pair of subjects in this feature space using kernel functions. Finally, we use these kernels to predict the behavioural variables using kernel ridge regression (<bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>, step 5). The first three steps are identical for all kernels and therefore carried out only once. The fourth step (mapping the examples and constructing the kernels) is carried out once for each of the different kernels. The last step is repeated 3,500 times for each kernel to predict a set of 35 different behavioural variables using 100 randomised iterations of 10-fold nested cross validation. We evaluate 24,500 predictive models using different kernels constructed from the same model of brain dynamics in terms of their ability to predict phenotypes, as well as 14,000 predictive models based on time-averaged features.</p>
<sec id="s2a">
<label>2.1</label>
<title>HCP imaging and behavioural data</title>
<p>We used data from the open-access Human Connectome Project (HCP) S1200 release (Smith, Beckmann, et al., 2013; <xref ref-type="bibr" rid="c37">Van Essen et al., 2013</xref>), which contains MR imaging data and various demographic and behavioural data from 1,200 healthy, young adults (age 22-35). All data described below, i.e., timecourses of the resting-state fMRI data and demographic and behavioural variables, are publicly available at <ext-link ext-link-type="uri" xlink:href="https://db.humanconnectome.org">https://db.humanconnectome.org</ext-link>.</p>
<p>Specifically, we used resting state fMRI data of 1,001 subjects, for whom any of the behavioural variables of interest were available. Each participant completed four resting state scanning sessions of 14 mins and 33 seconds duration each. This resulted in 4,800 timepoints per subject. For the main results, we used all four resting state scanning sessions of each participant to fit the model of brain dynamics (but see <bold>Supplementary Figure 1</bold> for results with just one session). The acquisition parameters are described in the HCP acquisition protocols and in <xref ref-type="bibr" rid="c37">Van Essen et al. (2013)</xref>; <xref ref-type="bibr" rid="c38">Van Essen et al. (2012)</xref>. Briefly, structural and functional MRI data were acquired on a 3T MRI scanner. The resting state fMRI data was acquired using multiband echo planar imaging sequences with an acceleration factor of 8 at 2 mm isotropic spatial resolution and a repetition time (TR) of 0.72 seconds. The preprocessing and timecourse extraction pipeline is described in detail in Smith, Beckmann, et al. (2013). For the resting state fMRI scans, preprocessing consisted of minimal spatial preprocessing and surface projection (<xref ref-type="bibr" rid="c13">Glasser et al., 2013</xref>), followed by temporal preprocessing. Temporal preprocessing consisted of single-session Independent Component Analysis (ICA) (<xref ref-type="bibr" rid="c7">Beckmann, 2012</xref>) and removal of noise components (<xref ref-type="bibr" rid="c14">Griffanti et al., 2014</xref>; <xref ref-type="bibr" rid="c25">Salimi-Khorshidi et al., 2014</xref>). Data were high-pass-filtered with a cut-off at 2,000 seconds to remove linear trends.</p>
<p>The parcellation was estimated from the data using multi-session spatial ICA on the temporally concatenated data from all subjects. Using this approach, a data-driven functional parcellation with 50 parcels was estimated, where all voxels are weighted according to their activity in each parcel, resulting in a weighted, overlapping parcellation. While other parcellations are available for the resting-state fMRI HCP dataset, we chose this parcellation because dynamic changes in FC can be better detected in this parcellation compared to other functional or anatomical parcellations or more fine-grained parcellations (<xref ref-type="bibr" rid="c1">Ahrends et al., 2022</xref>). Timecourses were extracted using dual regression (<xref ref-type="bibr" rid="c8">Beckmann et al., 2009</xref>), where group-level components are regressed onto each subject’s fMRI data to obtain subject-specific versions of the parcels and their timecourses. We normalised the timecourses of each subject to ensure that the model of brain dynamics and, crucially, the kernels were not driven by (averaged) amplitude and variance differences between subjects.</p>
<p>Subjects in the HCP study completed a range of demographic and behavioural questionnaires. Following <xref ref-type="bibr" rid="c40">Vidaurre et al. (2021)</xref>, we here focus on a subset of those items, including age and various cognitive variables. The cognitive variables span items assessing memory, executive function, fluid intelligence, language, processing speed, spatial orientation, and attention. The full list of the 35 behavioural variables used here, as well as their categorisation within the HCP dataset can be found in <bold>Supplementary Table 1.</bold></p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>The Hidden Markov Model as a model of brain dynamics</title>
<p>To estimate brain dynamics, we here use the Hidden Markov Model (<xref ref-type="bibr" rid="c41">Vidaurre et al., 2016</xref>; <xref ref-type="bibr" rid="c42">Vidaurre et al., 2017</xref>). However, the kernels, which are explained in detail in the following section, can be constructed from any generative probabilistic model.</p>
<p>The Hidden Markov model (HMM) is a generative probabilistic model, which assumes that an observed time-series, such as BOLD signal in a given parcellation, was generated by a sequence of “hidden states” (<xref ref-type="bibr" rid="c5">Baum &amp; Eagon, 1967</xref>; <xref ref-type="bibr" rid="c6">Baum &amp; Petrie, 1966</xref>). We here model the states as Gaussian distributions, defined both in terms of mean and covariance —which can be interpreted as distinct patterns of amplitude and FC (<xref ref-type="bibr" rid="c42">Vidaurre et al., 2017</xref>). Since we are often interested in modelling time-varying FC specifically, we repeated all computations for a variety of the HMM where state means were pinned to zero (given that the data was demeaned and the global average is zero) and only the covariance was allowed to vary across states (<xref ref-type="bibr" rid="c42">Vidaurre et al., 2017</xref>), which is shown in <bold>Supplementary Figure 2</bold>.</p>
<p>The HMM is described by a set of parameters <italic>θ</italic>, containing the state probabilities <italic>π</italic>, the transition probabilities <italic>A</italic>, the mean vectors <italic>μ</italic> of all states, and the covariance matrices <italic>σ</italic> of all states:
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="530638v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>K</italic> is the number of states and <italic>M</italic> is the number of parcels in the parcellation. The entire set of parameters <italic>θ</italic> is estimated from the data. Here, we chose a small number of states, <italic>K</italic> = 6, to ensure that the group-level HMM states are general enough to be found in all subjects, since a larger number of states increases the chances of certain states being present only in a subset of subjects. Note that the same HMM estimation is used for all kernels.</p>
<p>The HMM is a generative model. The generative process in this case works by sampling from a Gaussian distribution with mean <italic>μ</italic><sub><italic>k</italic></sub> and covariance <italic>σ</italic><sub><italic>k</italic></sub> when state <italic>k</italic> is active:
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="530638v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>q</italic><sub><italic>t</italic></sub> is the currently active state. Which state is active depends on the previous state <italic>q</italic><sub><italic>t</italic>−1</sub> and is determined by the transition probabilities <italic>A</italic>, so that the generated state sequence is sampled from a categorical distribution with parameters:
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="530638v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>A</italic><sub><italic>K</italic></sub> indicates the k-th row of the transition probability matrix.</p>
<p>The space of parameters <italic>θ</italic> forms a Riemannian manifold <italic>R</italic><sub><italic>θ</italic></sub>, where the relationships between the different parameters of the HMM are acknowledged by construction. The Fisher kernel, as described below, is built upon a projection on this manifold, so predictions based on this kernel account for the mathematical structure of the HMM.</p>
<p>Here, we fit the HMM to the concatenated timeseries of all subjects (see <bold><xref rid="fig1" ref-type="fig">Figure 1a</xref></bold>, step 1). We refer to the group-level estimate as HMM<sup>0</sup>, which is defined by the parameters <italic>θ</italic><sup>0</sup> (see <bold><xref rid="fig1" ref-type="fig">Figure 1a</xref></bold>, step 2):
<disp-formula id="ueqn4">
<alternatives><graphic xlink:href="530638v2_ueqn4.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In order to use the information from the HMM to predict subjects’ phenotypes, we estimate subject-specific versions of the group-level HMM (see <bold><xref rid="fig1" ref-type="fig">Figure 1a</xref></bold>, step 3) through dual estimation (<xref ref-type="bibr" rid="c42">Vidaurre et al., 2017</xref>). Dual estimation refers to the process of fitting the previously estimated group-level model again to a single subject’s timeseries, so that the parameters from the group-level model HMM<sup>0</sup> are adapted to fit the individual. We will refer to the subject-specific estimate for subject <italic>n</italic> as HMM<sup><italic>n</italic></sup>, with parameters <italic>θ</italic><sup><italic>n</italic></sup>.</p>
<p>These subject-specific HMM parameters are the features that we construct kernels from. To understand which features are most important for the predictions, we also construct versions of the kernels that include only subsets of the features. Specifically, we can group the features into two subsets: 1. the state features, describing <italic>what</italic> states look like, containing the mean vectors <italic>μ</italic>, and the covariance matrices <italic>σ</italic> of all states, and 2. the transition features, describing <italic>how</italic> individuals transition between these states, containing the initial state probabilities <italic>π</italic> and the transition probabilities <italic>A</italic>. By removing one or the other set of features and evaluating how model performance changes compared to the full kernels, we can draw conclusions about the importance of these two different types of changes for the predictions. Since the state features are considerably more numerous than the transition features (15,300 state features compared to 42 transition features in this case), we also construct a version of the kernels where state features have been reduced to the same number as the transition features using PCA, i.e., we use all 42 transition features and the first 42 PCs of the state features.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Kernels from Hidden Markov Models</title>
<p>Kernels (<xref ref-type="bibr" rid="c29">Shawe-Taylor &amp; Cristianini, 2004</xref>) are a convenient approach to work with high-dimensional, complex features, such as parameters from a model of brain dynamics. In general, kernels are similarity functions between subjects, and they can be used straightforwardly in a prediction algorithm. While feature matrices can be very high dimensional, a kernel is represented by a (no. of subjects by no. of subjects) matrix. Kernel methods can readily be adapted to deal with nonlinear decision boundaries in prediction, by projecting the data into a high-dimensional (possibly infinite-dimensional) space through an embedding <italic>x</italic> → <italic>ϕ</italic><sub><italic>x</italic></sub> then, by estimating a linear separating hyperplane on this space, we can effectively have a nonlinear estimator on the original space (<xref ref-type="bibr" rid="c29">Shawe-Taylor &amp; Cristianini, 2004</xref>). In practice, instead of working explicitly in a higher-dimensional embedding space, the so-called kernel trick uses a kernel function <italic>κ</italic>(<italic>n,m) containing</italic> the similarity between data points <italic>n</italic> and <italic>m</italic> (here, subjects) in the higher-dimensional embedding space (<xref ref-type="bibr" rid="c28">Schölkopf et al., 2002</xref>; <xref ref-type="bibr" rid="c29">Shawe-Taylor &amp; Cristianini, 2004</xref>), which can be simpler to calculate. Once <italic>κ</italic> (·,·) is computed for each pair of subjects, this is all that is needed for the prediction. This makes kernels computationally very efficient, since in most cases the number of subjects will be smaller than the number of features —which, in the case of HMMs, can be very large (potentially, in the order of millions). However, finding the right kernel can be a challenge because there are many available alternatives for the embedding.</p>
<p>Here, in combination with a linear predictive model, we apply a kernel that is specifically conceived to be used to compare instances of a generative model like the HMM. We expected that this will result in better predictions than existing methods. Using the same HMM estimate, we compare three different kernels, which map the HMM parameters into three distinct spaces, corresponding to different embeddings (see <bold><xref rid="fig1" ref-type="fig">Figure 1b</xref></bold>, step 4): the naïve kernel, the naïve normalised kernel, and the Fisher kernel. While the first two kernels (naïve and naïve normalised kernel) do not take the relationships between the HMM’s parameters into account, the Fisher kernel preserves the structure of the HMM and weights the parameters according to how much they contribute to generating a particular subject’s timeseries. We then construct linear and Gaussian versions of the different kernels (see <bold><xref rid="fig1" ref-type="fig">Figure 1b</xref></bold>, step 5), which take the general form <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline1.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for the linear kernel and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline2.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for the Gaussian kernel. We compare these to a kernel constructed using Kullback-Leibler divergence, previously used for predicting behavioural phenotypes (<xref ref-type="bibr" rid="c40">Vidaurre et al., 2021</xref>).</p>
<sec id="s2c1">
<label>2.3.1</label>
<title>Naïve kernel</title>
<p>The naïve kernel is based on a simple vectorisation of the subject-specific version of the HMM’s parameters, each on their own scale. This means that the kernel does not take relationships between the parameters into account and the parameters are here on different scales. This procedure can be thought of as computing Euclidean distances between two sets of HMM parameters, ignoring the actual geometry of the space of parameters. For each subject <italic>n</italic>, we vectorise parameters <italic>θ</italic><sup><italic>n</italic></sup> obtained through dual estimation of the group-level parameters <italic>θ</italic><sup>0</sup> to map the example <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline3.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> to:
<disp-formula id="ueqn5">
<alternatives><graphic xlink:href="530638v2_ueqn5.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We will refer to this vectorised version of the subject-specific HMM parameters as “naïve <italic>θ</italic>”. The naïve <italic>θ</italic> are the features used in the naïve kernel <italic>κ</italic><sub><italic>N</italic></sub>. We first construct a linear kernel from the naïve <italic>θ</italic> features using the inner product of the feature vectors. The linear naïve kernel <italic>κ</italic><sub><italic>Nl</italic></sub> between subjects <italic>n</italic> and <italic>m</italic> is thus defined as:
<disp-formula id="ueqn6">
<alternatives><graphic xlink:href="530638v2_ueqn6.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where ⟨<italic>θ</italic><sup><italic>n</italic></sup>, <italic>θ</italic><sup><italic>m</italic></sup>⟩ denotes the inner product between <italic>θ</italic><sup><italic>n</italic></sup> and <italic>θ</italic><sup><italic>m</italic></sup>. Using the same feature vectors, we can also construct a Gaussian kernel from the naïve <italic>θ</italic>. The Gaussian naïve kernel <italic>κ</italic><sub><italic>Ng</italic></sub> for subjects <italic>n</italic> and <italic>m</italic> is defined as:
<disp-formula id="ueqn7">
<alternatives><graphic xlink:href="530638v2_ueqn7.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>τ</italic> is the radius of the radial basis function, and ‖<italic>θ</italic><sup><italic>n</italic></sup> − <italic>θ</italic><sup><italic>m</italic></sup>‖ is the L<sub>2</sub>-norm of the difference of the feature vectors (naïve <italic>θ</italic> for subjects <italic>n</italic> and <italic>m</italic>). Compared to a linear kernel, a Gaussian kernel embeds the features into a more complex space, which can potentially improve the accuracy. However, this kernel has an additional parameter <italic>τ</italic> that needs to be chosen, typically through cross-validation. This makes a Gaussian kernel computationally more expensive and, if the additional parameter <italic>τ</italic> is poorly estimated, more error prone. The effect of the hyperparameters on errors is shown in <bold>Supplementary Figure 3</bold>.</p>
<p>While the naïve kernel takes all the information from the HMM into account by using all parameters from a subject-specific version of the model, it uses these parameters in a way that ignores the structure of the model that these parameters come from. In this way, the different parameters in the feature vector are difficult to compare, since e.g., a change of 0.1 in the transition probabilities between two states is not of the same magnitude as a change of 0.1 in one entry of the covariance matrix of a specific state. In the naïve kernel, these two very different types of changes would be treated indistinctly.</p>
</sec>
<sec id="s2c2">
<label>2.3.2</label>
<title>Naïve normalised kernel</title>
<p>To address the problem of parameters being on different scales, the naïve normalised kernel makes the scale of the subject-specific vectorised parameters (i.e., the naïve <italic>θ</italic>) comparable across parameters. Here, the mapping x → <italic>ϕ</italic><sub><italic>x</italic></sub> consists of a vectorisation and normalisation across subjects of the subject-specific HMM parameters, by subtracting the mean over subjects from each parameter and dividing by the standard deviation. This kernel does not respect the geometry of the space of parameters either.</p>
<p>As for the naïve kernel, we can then construct a linear kernel from these vectorised, normalised parameters <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline4.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> by computing the inner product for all pairs of subjects <italic>n</italic> and <italic>m</italic> to obtain the linear naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub>:
<disp-formula id="ueqn8">
<alternatives><graphic xlink:href="530638v2_ueqn8.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We can also compute a Gaussian kernel from the naïve normalised feature vectors to obtain the Gaussian version of the naïve normalised kernel <italic>κ</italic><sub><italic>NNg</italic></sub>:
<disp-formula id="ueqn9">
<alternatives><graphic xlink:href="530638v2_ueqn9.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In this way, we have constructed a kernel in which parameters are all on the same scale, but which still ignores the complex relationships between parameters originally encoded by the underlying model of brain dynamics.</p>
</sec>
<sec id="s2c3">
<label>2.3.3</label>
<title>Fisher kernel</title>
<p>The Fisher kernel (<xref ref-type="bibr" rid="c17">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="c18">Jaakkola &amp; Haussler, 1998</xref>) is specifically designed to preserve the structure of a generative probabilistic model (here, the HMM). This can be thought of as a “proper” projection on the manifold, as illustrated in <bold><xref rid="fig1" ref-type="fig">Figure 1b</xref></bold>, step 4b. Similarity between subjects is here defined in reference to a group-level model of brain dynamics. The mapping <italic>x</italic> → <italic>ϕ</italic><sub><italic>x</italic></sub> is given by the “Fisher score”, which indicates how (i.e., in which direction in the Riemannian parameter space) we would have to change the group-level model to better explain a particular subject’s timeseries. The similarity between subjects can then be described based on this score, so that two subjects are defined as similar if the group-level model would have to be changed in a similar direction for both, and dissimilar otherwise.</p>
<p>More precisely, the Fisher score is given by the gradient of the log-likelihood with respect to each model parameter:
<disp-formula id="ueqn10">
<alternatives><graphic xlink:href="530638v2_ueqn10.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>x</italic><sup><italic>n</italic></sup> is the timeseries of subject <italic>n</italic>, and ℒ<sub><italic>θ</italic></sub> (<italic>x</italic>) = <italic>P</italic> (<italic>x</italic>|<italic>θ</italic>) represents the likelihood of the timeseries <italic>x</italic> given the model parameters <italic>θ</italic>. This way, the Fisher score maps an example (i.e., a subject’s timeseries) <italic>x</italic><sup><italic>n</italic></sup> into a point in the gradient space of the Riemannian manifold <italic>R</italic><sub><italic>θ</italic></sub> defined by the HMM parameters.</p>
<p>The invariant Fisher kernel <italic>κ</italic><sub><italic>F</italic>−</sub> is the inner product of the Fisher score <italic>g</italic>, scaled by the Fisher information matrix <italic>F</italic>, which gives a local metric on the Riemannian manifold <italic>R</italic><sub><italic>θ</italic></sub> :
<disp-formula id="ueqn11">
<alternatives><graphic xlink:href="530638v2_ueqn11.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
for subjects <italic>n</italic> and <italic>m</italic>. <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline5.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is the Fisher information matrix, defined as
<disp-formula id="ueqn12">
<alternatives><graphic xlink:href="530638v2_ueqn12.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where the expectation is with respect to x under the distribution <italic>P</italic> (<italic>x</italic>|<italic>θ</italic>). The Fisher information matrix <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline6.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> can be approximated empirically:
<disp-formula id="ueqn13">
<alternatives><graphic xlink:href="530638v2_ueqn13.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
which is simply the covariance matrix of the gradients <italic>g</italic>. Using <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline7.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> essentially serves to whiten the gradients; therefore, given the large computational cost associated with <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline8.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula>, we here disregard the Fisher information matrix and reduce the invariant Fisher kernel to the so-called practical Fisher kernel (<xref ref-type="bibr" rid="c17">Jaakkola et al., 1999</xref>; <xref ref-type="bibr" rid="c18">Jaakkola &amp; Haussler, 1998</xref>; <xref ref-type="bibr" rid="c36">van der Maaten, 2011</xref>), for which the linear version <italic>κ</italic><sub><italic>Fl</italic></sub> takes the form:
<disp-formula id="ueqn14">
<alternatives><graphic xlink:href="530638v2_ueqn14.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
In this study, we will use the practical Fisher kernel for all computations.</p>
<p>One issue when working with the linear Fisher kernel is that the gradients of typical examples (i.e., subjects whose timeseries can be described by similar parameters as the group-level model) are close to zero, while gradients of atypical examples (i.e., subjects who are very different from the group-level model) can be very large. This may lead to an underestimation of the similarity between two typical examples because their inner product is very small, although they are very similar. To mitigate this, we can plug the gradient features (i.e., the Fisher scores <italic>g</italic>) into a Gaussian kernel, which essentially normalises the kernel. For subjects <italic>n</italic> and <italic>m</italic>, where <italic>x</italic><sup><italic>n</italic></sup> is the timeseries of subject <italic>n</italic> and <italic>x</italic><sup><italic>m</italic></sup> is the timeseries of subject <italic>m</italic>, the Gaussian Fisher kernel <italic>κ</italic><sub><italic>Fg</italic></sub> is defined as
<disp-formula id="ueqn15">
<alternatives><graphic xlink:href="530638v2_ueqn15.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where ‖<italic>g</italic> (<italic>θ</italic><sup>0</sup>, <italic>x</italic><sup><italic>n</italic></sup>) − <italic>g</italic> (<italic>θ</italic><sup>0</sup>, <italic>x</italic><sup><italic>m</italic></sup>)‖ is the distance between examples <italic>n</italic> and <italic>m</italic> in the gradient space, and <italic>τ</italic> is the width of the Gaussian kernel.</p>
</sec>
<sec id="s2c4">
<label>2.3.4</label>
<title>Kullback-Leibler divergence</title>
<p>Kullback-Leibler (KL) divergence is an information-theoretic distance measure which estimates divergence between probability distributions —in this case between subject-specific versions of the HMM. Here, KL divergence of subject <italic>n</italic> from subject <italic>m</italic>, KL(HMM<sup><italic>n</italic></sup>||HMM<sup><italic>m</italic></sup>) can be interpreted as how much new information the HMM of subject n contains if the true distribution was the HMM of subject m. KL divergence is not symmetric, i.e., KL(HMM<sup><italic>n</italic></sup>||HMM<sup><italic>m</italic></sup>) is different than KL (HMM<sup><italic>m</italic></sup> ||HMM<sup><italic>n</italic></sup>). We use an approximation of KL divergence which is described in detail in <xref ref-type="bibr" rid="c11">Do (2003)</xref>, as in <xref ref-type="bibr" rid="c40">Vidaurre et al. (2021)</xref>. That is, given two models HMM<sup><italic>n</italic></sup> from HMM<sup><italic>m</italic></sup> for subject n and subject <italic>m</italic>, we have
<disp-formula id="ueqn16">
<alternatives><graphic xlink:href="530638v2_ueqn16.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Where <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline9.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are the transition probabilities from state <italic>k</italic> into any other state according to HMM<sup><italic>n</italic></sup> and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline10.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are the state Gaussian distributions for state <italic>k</italic> and HMM<sup><italic>n</italic></sup> (respectively <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline11.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline12.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> for HMM<sup><italic>m</italic></sup>) (see <xref ref-type="bibr" rid="c21">MacKay et al. (2003)</xref>). Since the transition probabilities are Dirichlet-distributed and the state distributions are Gaussian distributed, KL divergence for those has a closed-form solution. Variables <italic>ν</italic><sub><italic>k</italic></sub> can be computed numerically such that
<disp-formula id="ueqn17">
<alternatives><graphic xlink:href="530638v2_ueqn17.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
To be able to use KL divergence as a kernel, we symmetrise the KL divergence matrix as
<disp-formula id="ueqn18">
<alternatives><graphic xlink:href="530638v2_ueqn17a.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
This symmetrised KL divergence can be plugged into a radial basis function, analogous to the Gaussian kernels to obtain a similarity matrix <italic>κ</italic><sub><italic>KL</italic></sub>
<disp-formula id="ueqn19">
<alternatives><graphic xlink:href="530638v2_ueqn18.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The resulting KL similarity matrix can be used in the predictive model in a similar way as the kernels described above.</p>
</sec>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Predictive model: Kernel ridge regression</title>
<p>Similarly to <xref ref-type="bibr" rid="c40">Vidaurre et al. (2021)</xref>, we use kernel ridge regression (KRR) to predict demographic and behavioural variables from the different kernels, although these could be used in any kernel-based prediction model or classifier, such as a support vector machine, which we demonstrate in simulations. KRR is the kernelised version of ridge regression (<xref ref-type="bibr" rid="c26">Saunders et al., 1998</xref>):
<disp-formula id="ueqn20">
<alternatives><graphic xlink:href="530638v2_ueqn19.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>α</italic> are the regression weights; <italic>h</italic> is the (number of subjects in test set by number of subjects in training set) kernel matrix between the subjects in the training set and the subjects in the test set; <italic>ŷ</italic> are the predictions in the (out-of-sample) test set; <italic>S</italic><sub><italic>train</italic></sub> are the number of subjects in the training set; and <italic>S</italic><sub><italic>test</italic></sub> are the number of subjects in the test set. The regression weights <italic>α</italic> can be estimated using the kernels specified above as
<disp-formula id="ueqn21">
<alternatives><graphic xlink:href="530638v2_ueqn20.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>λ</italic> is a regularisation parameter that we can choose through cross-validation; <italic>I</italic> is the identity matrix; <italic>κ</italic> is the (<italic>S</italic><sub><italic>train</italic></sub> by <italic>S</italic><sub><italic>train</italic></sub>) kernel matrix of the subjects in the training set; and <italic>y</italic> are the training examples.</p>
<p>We use KRR to separately predict each of the 35 demographic and behavioural variables from each of the different methods, removing subjects with missing entries from the prediction. The hyperparameters <italic>λ</italic> (and <italic>τ</italic> in the case of Gaussian kernels) were selected in a nested cross-validated (CV) fashion using 10 folds for both inner and outer loops assigning 90% of the data to the training set and 10% to the test set for each fold. We used a CV scheme to account for family structure in the HCP dataset so that subjects from the same family were never split across folds (<xref ref-type="bibr" rid="c44">Winkler et al., 2015</xref>). We repeated the nested 10-fold CV 100 times, so that different combinations of subjects were randomly assigned to the folds at each new CV iteration to obtain a distribution of model performance values for each variable. This is to explicitly show how susceptible each model was to changes in the training folds, which we can take as a measure of the robustness of the estimators, as described below.</p>
<sec id="s2d1">
<label>2.4.1</label>
<title>Evaluation criteria</title>
<p>We evaluate the models in terms of two outcome criteria: prediction accuracy and reliability. The first criterion, prediction accuracy, concerns how close the model’s predictions were to the true values on average. We here quantify prediction accuracy as Pearson’s correlation coefficient <italic>r</italic> (<italic>ŷ,y</italic>) between the model-predicted values <italic>ŷ</italic> and the actual values <italic>y</italic> of each variable.</p>
<p>The second criterion, reliability, concerns two aspects: i) that the model will never show excessively large errors for single subjects that could harm interpretation; and ii) that the model’s accuracy will be consistent across random variations of the training set —in this case by using different (random) partitions for the CV folds. As mentioned, this is important if we want to interpret prediction errors for example in clinical contexts, which assumes that the error size of a model in a specific subject reflects something biologically meaningful, e.g., whether a certain disease causes the brain to “look” older to a model than the actual age of the subject (<xref ref-type="bibr" rid="c10">Denissen et al., 2022</xref>). Maximum errors inform us about single cases where a model (that may typically perform well) fails. The maximum absolute error (MAXAE) is the single largest error made by the kernel ridge regression model in each iteration, i.e.,
<disp-formula id="ueqn22">
<alternatives><graphic xlink:href="530638v2_ueqn21.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Since the traits we predict are on different scales, the MAXAE is difficult to interpret, e.g., a MAXAE of 10 would be considered small if the true range of the variable we are predicting was 1,000, while it would be considered large if the true range of the variable was 1. To make the results comparable across the different traits, we therefore normalise the MAXAE by dividing it by the range of the respective variable. In this way, we obtain the NMAXAE:
<disp-formula id="ueqn23">
<alternatives><graphic xlink:href="530638v2_ueqn22.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Since the NMAXAEs follow extreme value distributions, it is more meaningful to consider the proportion of the values exceeding relevant thresholds than testing for differences in the means of these distributions (<xref ref-type="bibr" rid="c15">Gumbel, 1958</xref>). We here consider the risk of large errors (NMAXAE &gt; 10), very large errors (NMAXAE &gt; 100), and extreme errors (NMAXAE &gt; 1,000) as the percentage of runs (across variables and CV iterations) where the model’s NMAXAE exceeds the given threshold. Since NMAXAE is normalised by the range of the actual variable, these thresholds correspond to one, two, and three orders of magnitude of the actual variable’s range. If we are predicting age, for instance, and the true ages of the subjects range from 25 years to 35 years, an NMAXAE of 1 would mean that the model’s least accurate prediction is off by 10 years, an NMAXAE of 10 would mean that the least accurate prediction is off by 100 years, an NMAXAE of 100 would be off by 1,000 years, and an NMAXAE of 1,000 would be off by 10,000 years. A model that makes such large errors, even in single cases, would be unusable for interpretation. Our reliability criterion in terms of maximum errors is therefore that the risk of large errors (NMAXAE &gt; 10) should be 0%.</p>
<p>For a model to be reliable, it should also be robust in the sense of susceptibility to changes in the training examples. Robustness is an important consideration in prediction studies, as it determines how reproducible a study or method is —which is often a shortcoming in neuroimaging-based prediction studies (<xref ref-type="bibr" rid="c39">Varoquaux et al., 2017</xref>). We evaluated the robustness by iterating nested 10-fold CV 100 times for each variable, randomising the subjects in the folds at each iteration, so that the models would encounter a different combination of subjects in the training and test sets each time they are run. Looking at this range of 100 predictions for each variable, we can assess whether the model’s performance changes drastically depending on which combinations of subjects it encountered in the training phase, or whether the performance is the same regardless of the subjects encountered in the training phase. The former would be an example of a model that is susceptible to changes in training examples and the latter an example of a model that is robust. We here quantify robustness as the standard deviation (S.D.) of the prediction accuracy <italic>r</italic> across the 100 CV iterations of each variable, where a small mean S.D. over variables indicates higher robustness.</p>
<p>We test for significant differences in mean prediction accuracy and robustness between the methods using permutation t-tests with 20,000 permutations across variables and iterations. For maximum errors, we test whether NMAXAE distributions are significantly different between the methods using permutation-based Kolmogorov-Smirnov tests. P-Values are corrected across multiple comparisons using Bonferroni-correction to control the family-wise error rate (FWER). To compare the Fisher kernel to the naïve and the naïve normalised kernels, we first evaluate the linear and Gaussian versions of the kernels together. We then compare the linear and Gaussian versions of each kernel with each other. We also compare each kernel to the KL divergence model. Finally, we compare the linear Fisher kernel with the methods based on time-averaged FC features described in the following section.</p>
</sec>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Regression models based on time-averaged FC features</title>
<p>To compare our approach’s performance to simpler methods that do not take dynamics into account, we compared them to four different regression models based on time-averaged FC features. For each subject, time-averaged FC was computed as z-transformed Pearson’s correlation between each pair of regions. The first time-averaged model is, analogous to the HMM-derived KL divergence model, a time-averaged KL divergence model. The model is described in detail in <xref ref-type="bibr" rid="c40">Vidaurre et al. (2021)</xref>. Briefly, we construct symmetrised KL divergence matrices of each subject’s time-averaged FC and predict from these matrices using the KRR pipeline described above. We refer to this model as time-averaged KL divergence. The other three time-averaged FC benchmark models do not involve kernels but predict directly from the features instead. Namely, we use two variants of an Elastic Net model (<xref ref-type="bibr" rid="c46">Zou &amp; Hastie, 2005</xref>), one using the unwrapped time-averaged FC matrices as input, and one using the time-averaged FC matrices in Riemannian space (<xref ref-type="bibr" rid="c4">Barachant et al., 2013</xref>; Smith, Vidaurre, et al., 2013), which we refer to as Elastic Net and Elastic Net (Riem.), respectively. Finally, we compare our models to the approach taken in <xref ref-type="bibr" rid="c24">Rosenberg et al. (2016)</xref>, where relevant edges of the time-averaged FC matrices are first selected, and then used as predictors in a regression model. We refer to this model as Selected Edges. All time-averaged FC models are fitted using the (nested) cross-validation strategy, accounting for family structure in the dataset, and repeated 100 times with randomised folds, as described above.</p>
</sec>
<sec id="s2f">
<label>2.6</label>
<title>Simulations</title>
<p>To further understand the behaviour of the different kernels, we simulate data and compare the kernels’ ability to recover the ground truth. Specifically, we aim to understand which type of parameter change the kernels are most sensitive to. We generate timeseries for two groups of subjects, timeseries <italic>X</italic><sup>1</sup> for group 1 and timeseries <italic>X</italic><sup>2</sup> for group 2, from two separate HMMs with respective sets of parameters <italic>θ</italic><sup>1</sup> and <italic>θ</italic><sup>2</sup>:
<disp-formula id="ueqn24">
<alternatives><graphic xlink:href="530638v2_ueqn23.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
We simulate timeseries from HMMs with these parameters through the generative process described in 2.2.</p>
<p>For the simulations, we use the group-level HMM of the real dataset with <italic>K</italic> = 6 states used in the main text as basis for group 1, i.e., <italic>HMM</italic> (<italic>θ</italic><sup>1</sup>) HMM (<italic>θ</italic><sup>0</sup>). We then manipulate two different types of parameters, the state means <italic>μ</italic> and the transition probabilities <italic>A</italic>, while keeping all remaining parameters the same between the groups. In the first case, we manipulate one state’s mean between the groups, i.e.:
<disp-formula id="ueqn25">
<alternatives><graphic xlink:href="530638v2_ueqn24.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>μ</italic><sup>2</sup> is obtained by simply adding a Gaussian noise vector to the state mean vector of one state:
<disp-formula id="ueqn26">
<alternatives><graphic xlink:href="530638v2_ueqn25.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <italic>φ</italic> is the Gaussian noise vector of size 1 x <italic>M, M</italic> is the number of parcels, here 50, and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline13.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline14.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are the first rows (corresponding to the first state) of the state mean matrices for groups 1 and 2, respectively. We control the amplitude of <italic>φ</italic> so that the difference between <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline15.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline16.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> is smaller than the minimum distance between any pair of states within one HMM.</p>
<p>This is to ensure that the HMM recovers the difference between groups as difference in one state’s mean vector, rather than detecting a new state for group 2 that does not occur in group 1 and consequently collapsing two other states. Since the state means <italic>μ</italic> and the state covariances <italic>σ</italic> are the first- and second-order parameters of the same respective distributions, it is to be expected that, although we only directly manipulate <italic>μ, σ</italic> changes as well.</p>
<p>In the second case, we manipulate the transition probabilities for one state between the groups, while keeping all other parameters the same, i.e.:
<disp-formula id="ueqn27">
<alternatives><graphic xlink:href="530638v2_ueqn26.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where <italic>A</italic><sup>2</sup> is obtained by randomly permuting the probabilities of one state to transition into any of the other states, excluding self-transition probability:
<disp-formula id="ueqn28">
<alternatives><graphic xlink:href="530638v2_ueqn27.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Here, <italic>p</italic> is a random permutation vector of size (<italic>K</italic>-1) x 1, <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline17.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline18.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are the self-transition probabilities of state 1, and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline19.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="530638v2_inline20.gif" mimetype="image" mime-subtype="gif"/></alternatives></inline-formula> are the probabilities of state 1 to transition into each of the other states.</p>
<p>We then concatenate the generated timeseries
<disp-formula id="ueqn29">
<alternatives><graphic xlink:href="530638v2_ueqn28.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
containing 100 subjects for group 1 and 100 subjects for group 2, with 1,200 timepoints and 50 parcels per subject. Note that we do not introduce any differences between subjects within a group, so that the between-group difference in HMM parameters should be the most dominant distinction and easily recoverable by the classifiers. We then apply the pipelines described above, running a new group-level HMM and constructing the linear versions of the naïve kernel, the naïve normalised kernel, and the Fisher kernel on these synthetic timeseries. The second case of simulations, manipulating the transition probabilities, only introduces a difference in few (<italic>K</italic> - 1) features and keeps the majority of the features the same between the groups, while the first case introduces a difference in a large number of features (<italic>M</italic> features directly, by changing one state’s mean vector, and an additional <italic>M</italic> x <italic>M</italic> features indirectly, as this state’s covariance matrix will also be affected). To account for this difference, we additionally construct a version of the kernels for the second case of simulations that includes only <italic>π</italic> and <italic>A</italic>, removing the state parameters <italic>μ</italic> and <italic>σ</italic>. Finally, we use a support vector machine (SVM) in combination with the different kernels to recover the group labels and measure the error produced by each kernel. We repeat the whole process (generating timeseries, constructing kernels, and running the SVM) 10 times, in each iteration randomising on 3 levels: generating new random noise/permutation vectors to simulate the timeseries, randomly initialising the HMM parameters when fitting the group-level HMM to the simulated timeseries, and randomly assigning subjects to 10 CV folds in the SVM.</p>
</sec>
<sec id="s2g">
<label>2.7</label>
<title>Implementation</title>
<p>The code used in this study was implemented in Matlab and is publicly available within the repository of the HMM-MAR toolbox at <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/HMM-MAR">https://github.com/OHBA-analysis/HMM-MAR</ext-link>. A Python-version of the Fisher kernel is also available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vidaurre/glhmm">https://github.com/vidaurre/glhmm</ext-link>. Code for Elastic Net prediction from time-averaged FC features is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vidaurre/NetsPredict/blob/master/nets_predict5.m">https://github.com/vidaurre/NetsPredict/blob/master/nets_predict5.m</ext-link>. The procedure for Selected Edges time-averaged FC prediction is described in detail in <xref ref-type="bibr" rid="c30">Shen et al. (2017)</xref> and code is provided at <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/bioimagesuite/behavioralprediction.m">https://www.nitrc.org/projects/bioimagesuite/behavioralprediction.m</ext-link>. All code used in this paper, including scripts to reproduce the figures and additional application examples of the Fisher Kernel can be found in the repository <ext-link ext-link-type="uri" xlink:href="https://github.com/ahrends/FisherKernel">https://github.com/ahrends/FisherKernel</ext-link>.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Results</title>
<p>We compared different prediction methods derived from a model of brain dynamics with each other, and with models that do not account for dynamics. We assess the performance of the predictive models in terms of two outcome criteria: prediction accuracy and reliability. We show the superiority in both accuracy and reliability of the Fisher kernel, a principled approach for prediction using brain dynamics, because of its capacity to preserve the underlying structure of the brain dynamics model.</p>
<sec id="s3a">
<label>3.1</label>
<title>The Fisher kernel predicts more accurately than other methods</title>
<p>We found that the Fisher kernel has the highest prediction accuracy on average across the range of variables and CV iterations, as shown in <bold><xref rid="fig2" ref-type="fig">Figure 2a</xref></bold>. Specifically, the Fisher kernel has a significantly higher correlation coefficient between model-predicted and actual values than the naïve kernel (mean <italic>r κ</italic><sub><italic>F</italic></sub>: 0.196 vs. mean <italic>r κ</italic><sub><italic>N</italic></sub>: 0.127, <italic>p</italic>=0.0008) and the naïve normalised kernel (mean <italic>r κ</italic><sub><italic>NN</italic></sub> : 0.159, <italic>p</italic>=0.0008). Comparing the two naïve kernels, the normalised version significantly improved upon the non-normalised naïve kernel (<italic>κ</italic><sub><italic>N</italic></sub> vs. <italic>κ</italic><sub><italic>NN</italic></sub>: <italic>p</italic>=0.0008).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><p>Distributions of model performance by kernel. The best-performing methods are highlighted by grey columns in each plot. <bold>a)</bold> Pearson’s correlation coefficients (r) between predicted and actual variable values as a measure of prediction accuracy (y-axis) of each method (x-axis). Larger values indicate that the model predicts more accurately. The linear Fisher kernel has the highest average accuracy. <bold>b)</bold> Normalised maximum errors (NMAXAE) as a measure of excessive errors (y-axis) by kernel (x-axis). Large maximum errors indicate that the model predicts very poorly in single cases. Differences between the kernels mainly lie in the tails of the distributions, where the KL divergence model produces extreme maximum errors in some runs (NMAXAE &gt; 1,000), while the linear naïve normalised kernel and the linear Fisher kernel have the smallest risk of excessive errors (NMAXAE &lt; 10). The y-axis is plotted on the log-scale. Asterisks here indicate significant results of Kolmogorov-Smirnov permutation tests. <bold>c)</bold> Robustness of correlation between model-predicted and actual values. The plot shows the distribution across variables of the standard deviation of correlation coefficients over CV iterations on the y-axis for each kernel (on the x-axis). Smaller values indicate greater robustness. The linear naïve normalised kernel and the linear Fisher kernel are most robust. a),b) Each violin plot shows the distribution over 3,500 runs (100 iterations of 10-fold CV for all 35 variables) that were predicted from each kernel. c) Each violin plot shows the distribution over 35 variables that were predicted from each kernel. Asterisks indicate significant Bonferroni-corrected p-values using 20,000 permutations: *: p &lt; 0.05, **: p &lt; 0.01, ***: p &lt; 0.001.</p></caption>
<graphic xlink:href="530638v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We next compared the linear with the Gaussian versions of each kernel. The linear versions performed significantly better than the Gaussian versions for the Fisher kernel (mean <italic>r κ</italic><sub><italic>Fl</italic></sub>: 0.204 vs. mean <italic>r κ</italic><sub><italic>Fg</italic></sub>: 0.187, <italic>p</italic>=0.0008) and the naïve normalised kernel (mean <italic>r κ</italic><sub><italic>NNl</italic></sub> 0.187 vs. mean <italic>r κ</italic><sub><italic>NNg</italic></sub> : 0.131, <italic>p</italic>=0.0008). The opposite was the case for the naïve kernel, where the Gaussian version significantly outperformed the linear version (mean <italic>r κ</italic><sub><italic>Nl</italic></sub> : 0.103 vs. mean <italic>r κ</italic><sub><italic>Ng</italic></sub>: 0.150, <italic>p</italic>=0.0008).</p>
<p>We next compared all models to the KL divergence model. The linear and the Gaussian Fisher kernels and the linear version of the naïve normalised kernel significantly outperformed the KL divergence model (mean <italic>r κ</italic><sub><italic>KL</italic></sub>:0.154 vs. <italic>κ</italic><sub><italic>Fl</italic></sub> : <italic>p</italic>=0.0008; vs. <italic>κ</italic><sub><italic>Fg</italic></sub>: <italic>p</italic>=0.0008; vs. <italic>κ</italic><sub><italic>NNl</italic></sub> : <italic>p</italic>=0.0008). The Gaussian version of the naïve normalised kernel performed significantly worse than the KL divergence model (<italic>p</italic>=0.0008). Both versions of the naïve kernel were less accurate than the KL divergence model in terms of correlation between model-predicted and actual values: This comparison was significant for the linear version of the naïve kernel (<italic>p=</italic>0.0008), but not for the Gaussian version (<italic>p</italic>=0.856).</p>
<p>The models based on time-averaged features had significantly lower accuracies than the linear Fisher kernel (<italic>κ</italic><sub><italic>Fl</italic></sub> vs. time-averaged KL div.: <italic>p</italic>=0.0008; vs. Elastic Net: <italic>p</italic>=0.0008; vs. Elastic Net Riem.: <italic>p</italic>=0.0008; vs. Selected Edges: <italic>p</italic>=0.0008), as shown in <bold><xref rid="fig2" ref-type="fig">Figure 2a</xref></bold>. We observed that the non-kernel-based time-averaged FC models, i.e., the two Elastic Net models and the Selected Edges model, make predictions at a smaller range than the actual variables, close to the variables’ means. This leads to weak relationships between predicted and actual variables but smaller errors, as shown in <bold>Supplementary Figure 3</bold>. The distributions of correlation coefficients for the different methods are shown in <bold><xref rid="fig2" ref-type="fig">Figure 2a</xref></bold>. The performance of all methods is also summarised in <bold>Supplementary Table 2</bold>.</p>
<p>We found that the Fisher kernel also predicted more accurately than other kernels when fitting the HMM to only the first resting-state session of each subject, as shown in <bold>Supplementary Figure 1</bold>. However, the overall accuracies of all kernels were lower in this case, indicating that predicting traits benefits from a large amount of available data per subject. Similarly, the Fisher kernel outperformed the other kernels when HMM states were defined only in terms of covariance (not mean) at comparable accuracy, as shown in <bold>Supplementary Figure 2</bold>.</p>
<p>As shown in <bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold>, there are differences in how well these demographic or behavioural variables <italic>can</italic> be predicted from a model of brain dynamics. While there are some variables which may show a strong link to brain dynamics, other variables may be more related to static or structural measures (<xref ref-type="bibr" rid="c19">Liégeois et al., 2019</xref>; <xref ref-type="bibr" rid="c40">Vidaurre et al., 2021</xref>), or just be difficult to predict in general. <bold><xref rid="tbl1" ref-type="table">Table 1</xref></bold> shows the variables that can be predicted at relatively high accuracy by at least one of the time-varying methods. We here consider a mean Pearson’s <italic>r</italic> across CV iterations greater than 0.3 as “relatively high” accuracy. Among these, five variables (variables ReadEng_AgeAdj, PicVocab_Unadj, PicVocab_AgeAdj, VSPLOT_OFF, and WM_Task_Acc) are best predicted by the Fisher kernel, one variable is best predicted by the naïve normalised kernel (variable VSPLOT_TC), and two variables are best predicted by the time-averaged KL divergence model (variables Age and ReadEng_Unadj). The linear naïve kernel and the time-averaged Elastic Net models produce the least accurate predictions in these variables. Examples of best-predicted variables for all HMM-derived kernels can be found in <bold>Supplementary Figure 6</bold>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1</label>
<caption><p>Variables predicted at relatively high accuracy by at least one method. The table shows mean correlation coefficients for all methods in those variables that could be predicted at relatively high accuracy (correlation coefficient &gt; 0.3) by at least one method. The best performing model is highlighted in bold.</p></caption>
<graphic xlink:href="530638v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><p>Model performance estimates over CV iterations by behavioural variable and method. Boxplots show the distribution over 100 iterations of 10-fold CV of correlation coefficient values (x-axis) of each method, separately for each of the 35 predicted variables (y-axes). The Fisher kernel (green) not only predicts at higher accuracy for many variables, but also shows the narrowest range, indicating high robustness. Black lines within each boxplot represent the median.</p></caption>
<graphic xlink:href="530638v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In summary, the linear Fisher kernel has the highest prediction accuracy, significantly outperforming the naïve kernels, Gaussian version of the Fisher kernel, and the KL divergence model. The linear Fisher kernel also has a higher prediction accuracy than benchmark methods using time-averaged FC features. The linear Fisher kernel, closely followed by its Gaussian version, is the kernel that produces the best predictions in the traits that can be predicted well by any one kernel.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>The Fisher kernel has a lower risk of excessive errors and is more robust than other methods</title>
<p>We now show empirically that the Fisher kernel is more reliable than other kernels, both in terms of risk of large errors and in terms of robustness over CV iterations.</p>
<p>The linear versions of the Fisher kernel and the naïve normalised kernel have the overall lowest risk of large errors, as shown in <bold><xref rid="fig2" ref-type="fig">Figure 2b</xref></bold>. We assessed the risk of large errors (NMAXAE &gt; 10), very large errors (NMAXAE &gt; 100), and extreme errors (NMAXAE &gt; 1,000), corresponding to one, two, and three orders of magnitude of the range of the actual variable. For the Fisher kernel, the risk of large errors is low: 0.057% in the Gaussian version <italic>κ</italic><sub><italic>Fg</italic></sub> and 0% in the linear version <italic>κ</italic><sub><italic>Fl</italic></sub>. That means that the linear Fisher kernel never makes large errors exceeding the range of the actual variable by orders of magnitude. In the naïve kernel, both the linear <italic>κ</italic><sub><italic>Nl</italic></sub> and the Gaussian version <italic>κ</italic><sub><italic>Ng</italic></sub> have a low risk of large errors at 0.029% for the linear version and 0.057% for the Gaussian version. The Gaussian naïve normalised kernel <italic>κ</italic><sub><italic>NNg</italic></sub> on the other hand has a slightly higher risk of large errors at 1.140% and a risk of very large errors at 0.057%, while its linear version <italic>κ</italic><sub><italic>NNl</italic></sub> has a 0% risk of large errors. The risk of large errors was highest in the KL divergence model <italic>κ</italic><sub><italic>KL</italic></sub> at 0.890% risk of large errors, 0.110% risk of very large errors, and even 0.057% risk of extreme errors. While the time-averaged KL divergence model had similarly large maximum errors as the time-varying KL divergence model, the three other time-averaged models had no risk of excessive errors, similar to the linear Fisher kernel. The maximum error distributions are shown in <bold><xref rid="fig2" ref-type="fig">Figure 2b</xref></bold>. The summary of maximum errors is also reported in <bold>Supplementary Table 2</bold> and <bold>Supplementary Table 3. Supplementary Figure 4</bold> also shows the quantile-quantile plots of the normalised maximum errors.</p>
<p>A reason for the higher risk of large errors in the Gaussian versions of the kernels is likely that the radius <italic>τ</italic> of the radial basis function needs to be selected (using cross-validation), introducing an additional factor of variability and leaving more room for error. <bold>Supplementary Figure 5</bold> shows the relation between the estimated hyperparameters (the regularisation parameter <italic>λ</italic> and the radius <italic>τ</italic> of the radial basis function) and how large errors in the predictions may be related to poor estimation of these parameters.</p>
<p>With respect to robustness, we found that the linear Fisher kernel <italic>κ</italic><sub><italic>Fl</italic></sub> and the linear naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub> had the most robust performance on average across the range of variables tested, as shown in <bold><xref rid="fig2" ref-type="fig">Figure 2c</xref></bold>. Robustness was quantified as the standard deviation of the correlation between model-predicted and actual values over 100 iterations of 10-fold CV. A low standard deviation indicates high robustness, since the method’s performance does not differ greatly depending on the specific subjects it was trained and tested on. The Fisher kernel was significantly more robust than both the naïve kernel and the naïve normalised kernel (<italic>κ</italic><sub><italic>F</italic></sub> mean S.D. <italic>r</italic>: 0.015 vs. naïve kernel <italic>κ</italic><sub><italic>N</italic></sub> mean S.D. <italic>r</italic>: 0.019, <italic>p</italic>=0.02; vs. naïve normalised kernel <italic>κ</italic><sub><italic>NN</italic></sub> mean S.D. <italic>r</italic>: 0.024, <italic>p</italic>=0.0008). The naïve kernel and the naïve normalised kernel did not significantly differ from each other (<italic>p</italic>=0.49). We found that the linear versions of the Fisher kernel and the naïve normalised kernel were significantly more robust than their respective Gaussian versions (<italic>κ</italic><sub><italic>Fl</italic></sub> mean S.D. <italic>r</italic>: 0.011 vs. <italic>κ</italic><sub><italic>Fg</italic></sub> mean S.D. <italic>r</italic>: 0.018, <italic>p</italic>=0.0008; <italic>κ</italic><sub><italic>NNl</italic></sub> mean S.D. <italic>r</italic>: 0.011 vs. <italic>κ</italic><sub><italic>NNg</italic></sub> mean S.D. <italic>r</italic>: 0.036, <italic>p</italic>=0.0008), while there was no significant difference between the linear and the Gaussian naïve kernel (<italic>κ</italic><sub><italic>Nl</italic></sub> S.D. <italic>r</italic>: 0.020 vs. <italic>κ</italic><sub><italic>Ng</italic></sub> S.D. <italic>r</italic>: 0.018, <italic>p</italic>&gt;1). All but the Gaussian naïve normalised kernel <italic>κ</italic><sub><italic>NNg</italic></sub> significantly outperformed the KL divergence model <italic>κ</italic><sub><italic>KL</italic></sub> in terms of robustness (<italic>κ</italic><sub><italic>KL</italic></sub> mean S.D. <italic>r</italic>: 0.042 vs. <italic>κ</italic><sub><italic>Nl</italic></sub> <italic>p</italic>=0.0008; vs. <italic>κ</italic><sub><italic>Ng</italic></sub> <italic>p</italic>=0.0008; vs. <italic>κ</italic><sub><italic>NNl</italic></sub> <italic>p</italic>=0.0008; vs. <italic>κ</italic><sub><italic>NNg</italic></sub> <italic>p</italic>&gt;1; vs. <italic>κ</italic><sub><italic>Fl</italic></sub> <italic>p</italic>=0.0008; vs. <italic>κ</italic><sub><italic>Fg</italic></sub> <italic>p</italic>=0.0008). This large variation in model performance depending on the CV fold structure in the KL divergence model is problematic. The same problem occurs, even more drastically, when using time-averaged FC features in a KL divergence model, indicating robustness issues of the KL divergence measure in this type of predictive model independent of the underlying features. Like the time-varying KL divergence model, the time-averaged KL divergence model was significantly less robust than the linear Fisher kernel (<italic>p=</italic>0.0008). Also the other time-averaged models were significantly less robust than the linear Fisher kernel (<italic>κ</italic><sub><italic>Fl</italic></sub> vs. Elastic Net: <italic>p</italic>=0.0008; vs. Elastic Net Riem.: <italic>p</italic>=0.0008; vs. Selected Edges: <italic>p</italic>=0.007). The ranges in model performance across CV iterations for each variable of the different kernels are shown in <bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold>. Robustness of all kernels is also summarised in <bold>Supplementary Table 3</bold>.</p>
<p>Overall, the linear Fisher kernel and the linear naïve normalised kernel were the most reliable: For both kernels, the risk of large errors was zero and the variability over CV iterations was the smallest. The Gaussian versions of all kernels had higher risks of large errors and a larger range in model performance over CV iterations, indicating that their performance was less reliable. The KL divergence model was the most problematic in terms of reliability with a risk of extreme errors ranging up to three orders of magnitude of the actual variable’s range and considerable susceptibility to changes in CV folds.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Fisher kernel predictions are driven by individual differences in state features</title>
<p>The Fisher kernel is most sensitive to individual differences in state parameters, both in simulated timeseries and in the real data. That means that it is more relevant for the predictions what states look like in the individuals than how the individuals transition between states. However, the Fisher kernel can be modified to recover differences in transition probabilities if these are relevant for specific traits.</p>
<p>As shown in <bold><xref rid="fig4" ref-type="fig">Figure 4a</xref></bold>, panel 1, when we simulated two groups of subjects that are different in terms of the mean amplitude of one state, the Fisher kernel was able to recover this difference in all runs with 0% error, meaning that it identified all subjects correctly in all runs. The Fisher kernel significantly outperformed the other two kernels (Fisher kernel <italic>κ</italic><sub><italic>Fl</italic></sub> vs. naïve kernel <italic>κ</italic><sub><italic>Nl</italic></sub> : <italic>p=</italic>0.0003, vs. naïve normalised kernel <italic>κ</italic><sub><italic>NNl</italic></sub> : <italic>p=</italic>0.0001), while there was no significant difference between the naïve and the naïve normalised kernel (<italic>p</italic>&gt;1). Both the naïve kernel and the naïve normalised kernel (<bold><xref rid="fig4" ref-type="fig">Figure 4a</xref></bold>, panels 2, 3) do not show a group-difference (i.e., there is no clear dissimilarity between the first and the second half of subjects) and are prone to producing outliers. In the Fisher kernel, there are no outliers in the features or kernels, and the group difference is obvious with a strong checkerboard pattern in the kernels (<bold><xref rid="fig4" ref-type="fig">Figure 4a</xref></bold>, panel 4).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><p>Simulations. If the group difference was apparent in the features, there would be a clear difference between red and blue dots in the feature plot. If the group difference was apparent in the kernels, there would be a checkerboard-pattern with four squares in the kernel matrices: high similarity within the first half and within the second half of subjects, and low similarity between the first and the second half of subjects. Each kernel should also have the strongest similarity on the diagonal because each subject should be more similar to themselves than to any other subject. In the second kernel plots, we remove the diagonal for visualisation purposes to show the group difference more clearly. <bold>a)</bold> Simulating two groups of subjects that are different in their state means. The error distributions of all 10 iterations show that the Fisher kernel recovers the simulated group difference in all runs with 0% error (1). Features, kernel matrices, and kernel matrices with the diagonal removed for the first iteration for the linear naïve kernel (2), the linear naïve normalised kernel (3), and the linear Fisher kernel (4). The Fisher kernel matrices show an obvious checkerboard pattern corresponding to the within-group similarity and the between-group dissimilarity of the first and the second half of subjects. <bold>b)</bold> Simulating two groups of subjects that are different in their transition probabilities. Neither kernel is able to reliably recover the group difference, as shown in the error distribution of all 10 iterations (1), and the features and kernel matrices of one example iteration (2-4). <bold>c)</bold> Simulating two groups of subjects that are different in their transition probabilities but excluding state parameters when constructing the kernels. The Fisher kernel performs best in recovering the group difference as shown by the error distributions of all 10 iterations (1).</p></caption>
<graphic xlink:href="530638v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When we simulated differences in transition probabilities between two groups, neither of the kernels were able to reliably recover this difference and the Fisher kernel performed significantly worse than the other two kernels on average (compared to naïve kernel: <italic>p=</italic>0.006, compared to naïve normalised kernel: <italic>p</italic>=0.004), as shown in <bold><xref rid="fig4" ref-type="fig">Figure 4b</xref></bold>, panel 1. Like the previous case, where we simulated differences in state means, the naïve kernel and the naïve normalised kernel did not significantly differ from each other (<italic>p</italic>&gt;1), and they produced outliers in features and kernels (<bold><xref rid="fig4" ref-type="fig">Figure 4b</xref></bold>, panels 2-3).</p>
<p>This indicates that all kernels, but particularly the Fisher kernel, are most sensitive to differences in state parameters rather than differences in transition probabilities. To understand whether the difference in transition probabilities can be recovered when it is not overshadowed by the more dominant state parameters, we run the second case of simulations again, where we introduce a group difference in terms of transition probabilities, but this time we exclude the state parameters when we construct the kernels. As shown in <bold><xref rid="fig4" ref-type="fig">Figure 4c</xref></bold>, panel 1, the Fisher kernel is now able to recover the group difference with minimal errors, while the naïve normalised kernel improves but does not perform as well as the Fisher kernel, and the naïve kernel performs below chance. Here, the Fisher kernel significantly outperformed both the naïve kernel (<italic>p</italic>=0.0001) and the naïve normalised kernel (<italic>p</italic>=0.02), and the naïve normalised kernel was significantly more accurate than the naïve kernel (<italic>p</italic>=0.0004). This shows that the Fisher kernel is able to recover the group difference in transition probabilities, if this difference is not overshadowed by the state parameters.</p>
<p>In real data, the features driving the prediction may differ from trait to trait: For some traits, state parameters may be more relevant, while for other traits, transitions may be more relevant. We therefore compare the effects of removing state features and the effects of removing transition features in all traits in the real data.</p>
<p>We found that state parameters were the most relevant features for the Fisher kernel predictions in all traits: As shown in <bold><xref rid="fig5" ref-type="fig">Figure 5a</xref></bold>, the prediction accuracy of the Fisher kernel significantly suffered when state features were removed (<italic>p</italic>=0.0009), while removing transition features had no significant effect (<italic>p</italic>&gt;1). We observed the same effect in the naïve normalised kernel (no state features: <italic>p</italic>=0.0009; no transition features: <italic>p</italic>&gt;1), while the naïve kernel was on average improved by removing state features (no state features: <italic>p</italic>=0.0009; no transition features: <italic>p</italic>&gt;1). One reason for the dominance of state parameters may simply be that the state parameters outnumber the other parameters: In the full kernels, we have 15,300 state features (300 features associated with the state means and 15,000 features associated with the state covariances), but only 42 transition features (6 features associated with the state probabilities and 36 features associated with the transition probabilities). To compensate for this imbalance, we also constructed a version of the kernels where state parameters were reduced to the same amount as transition features using PCA, so that we have 84 features in total (42 transition features and the first 42 PCs of the 15,300 state features). These PCA-kernels performed significantly better than the ones where state features were removed in the Fisher kernel (<italic>p</italic>=0.0009), but worse than the kernels including all features at the original dimensionality (<italic>p</italic>=0.0009). This indicates that the fact that state parameters are more numerous than transition parameters does not explain why kernels including state features perform better. Instead, the content of the state features is more relevant for the prediction than the content of the transition features.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Effects of removing sets of features on prediction accuracies. <bold>a)</bold> In the overall prediction accuracies, removing state features significantly decreased performance in the Fisher kernel and the naïve normalised kernel, while removing transition features had no significant effect. <bold>b)</bold> and <bold>c)</bold> Removing features has similar effects on all variables, both better predicted (b) and worse predicted ones (c).</p></caption>
<graphic xlink:href="530638v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When looking at the performance separately for each variable (<bold><xref rid="fig5" ref-type="fig">Figure 5b</xref> and <xref rid="fig5" ref-type="fig">c</xref></bold>), we found that all but one variable were better predicted by the version of the kernel which included state features than the ones where state features were removed, while it did not seem to matter whether transition features were included. This indicates that the simulation case described above, where the relevant changes are in the transition probabilities, did not occur in the real data. In certain variables, reducing state features using PCA improved the accuracy compared to the full kernels, which is not unexpected since feature dimensionality reduction is known to be able to improve prediction accuracy by removing redundant features (<xref ref-type="bibr" rid="c23">Mwangi et al., 2014</xref>).</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Discussion</title>
<p>We showed that the Fisher kernel accurately and reliably predicts traits from brain dynamics models trained on neuroimaging data, resulting in better generalisability and interpretability compared to other methods. It preserves the structure of the underlying brain dynamics model, making it ideal for combining generative and predictive models. We have compared the Fisher kernel to kernels which ignore the structure of the brain dynamics model (“naïve” kernels), to a previously used model based on Kullback-Leibler divergence (<xref ref-type="bibr" rid="c40">Vidaurre et al., 2021</xref>), and to models based on time-averaged FC. The linear Fisher kernel had a higher prediction accuracy than all other methods and was among the most reliable methods: It never produced excessive errors and was robust to changes in training sets.</p>
<p>We here focussed on fMRI, but the method can also be applied to other modalities like MEG or EEG and it can straightforwardly be implemented in any kernel-based prediction model or classifier, including kernel ridge regression, support vector machines (SVM), kernel fisher discriminant analysis (k-FDA), kernel logistic regression (KLR), or nonlinear PCA.</p>
<p>Our results were consistent in two alternative settings: when using less data per subject and when modelling brain dynamics only in terms of functional connectivity. This supports the generalisability of the results. It should be noted though that we observed overall lower accuracies when using only one scanning session consisting of 1,200 timepoints per subject, indicating that predicting phenotypes from models of brain dynamics requires a sufficiently large amount of data for each individual. Using a set of 35 different demographic and cognitive traits, we found that the traits that were best predicted by kernels from models of brain dynamics were language and spatial orientation skills, and here the Fisher kernel was generally the most accurate. We also found that the Gaussian version of the kernels are generally more error-prone and susceptible to changes in the training set, although they may predict more accurately in certain runs. Implementing Gaussian kernels in a predictive model is also computationally more expensive, making them less practical. While we here only tested robustness in terms of susceptibility to changes in CV folds, it remains to be shown to what extent model performance is sensitive to the random initialisation of the HMM, which affects the parameter estimation (<xref ref-type="bibr" rid="c2">Alonso &amp; Vidaurre, 2023</xref>). Finally, we showed that the Fisher kernel is most sensitive to changes in state descriptions, i.e., individual differences in the amplitude or functional connectivity of certain brain states. While this could be a disadvantage if a trait was more closely related to how an individual transitions between brain states, we found that this was not the case in any of the traits we tested here. This is surprising, given that we predicted a variety of cognitive traits, expecting that some traits would, for instance, be more closely related to an individual’s FC in a certain brain state, while other traits would be better explained by their temporal pattern of switching between brain states. Other constructs than the ones we tested here may of course be more related to individual transition patterns. For this case, we showed in simulations that the Fisher kernel can be modified to recognise changes in transitions if they are of interest for the specific research question.</p>
<p>Models of brain dynamics in unconstrained cognition do not directly lend themselves to being used as linear predictors, because of the high number of parameters and their complex relationships. Kernels are a computationally efficient approach that allow working with nonlinear, high-dimensional features, such as parameters from a model of brain dynamics, in a simple linear predictive model. How to construct a kernel is however not always straightforward (<xref ref-type="bibr" rid="c3">Azim &amp; Ahmed, 2018</xref>; <xref ref-type="bibr" rid="c29">Shawe-Taylor &amp; Cristianini, 2004</xref>). We here demonstrated that constructing a kernel that preserves the structure of the underlying model, like the Fisher kernel, can achieve better, more reliable performance than other methods when using models of brain dynamics to predict subject traits.</p>
<p>Such a kernel can be useful in various applications. For instance, there is growing interest in combining different data types or modalities, such as structural, static, and dynamic measures to predict phenotypes (<xref ref-type="bibr" rid="c12">Engemann et al., 2020</xref>; <xref ref-type="bibr" rid="c27">Schouten et al., 2016</xref>). While directly combining the features from each modality can be problematic, kernels that are properly derived for each modality can be easily combined using approaches such as Multi Kernel Learning (MKL) (Gönen &amp; Alpaydın, 2011), which can improve prediction accuracy of multimodal studies (<xref ref-type="bibr" rid="c35">Vaghari et al., 2022</xref>). In a clinical context, describing individual variability for diagnosis and individual patients’ outcome prediction is the ultimate goal (<xref ref-type="bibr" rid="c22">Marquand et al., 2016</xref>; <xref ref-type="bibr" rid="c34">Stephan et al., 2017</xref>; <xref ref-type="bibr" rid="c43">Wen et al., 2022</xref>; <xref ref-type="bibr" rid="c45">Wolfers et al., 2015</xref>), but the focus so far has mostly been on static or structural information, leaving the potentially crucial information from brain dynamics untapped. In order to be able to use predictive models from brain dynamics in a clinical context, predictions must be reliable, particularly if we want to interpret model errors, as in models of “brain age”. As we demonstrated, there can be extreme errors and large variation in some predictive models, and these issues are not resolved by estimating model performance in a standard cross-validated fashion. We here showed that taking the structure of the underlying model into account, and thoroughly assessing not only accuracy but also errors and robustness, we can reliably use information from brain dynamics to predict individual traits. This will allow gaining crucial insights into cognition and behaviour from how brain function changes over time, beyond structural and static information.</p>
</sec>
<sec id="d1e2069" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e2149">
<label>Supplementary Material</label>
<media xlink:href="supplements/530638_file02.docx"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>DV is supported by a Novo Nordisk Foundation Emerging Investigator Fellowship (NNF19OC-0054895) and an ERC Starting Grant (ERC-StG-2019-850404). We thank Ben Griffin and Steve Smith for useful discussions and technical collaboration.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ahrends</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Stevner</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pervaiz</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Kringelbach</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Vuust</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name> (<year>2022</year>). <article-title>Data and model considerations for estimating time-varying functional connectivity in fMRI</article-title>. <source>Neuroimage</source>, <volume>252</volume> <fpage>119026</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119026</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="web"><string-name><surname>Alonso</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name> (<year>2023</year>). <article-title>Towards stability of dynamic FC estimates in neuroimaging and electrophysiology: solutions and limits</article-title>. <source>bioRxiv</source>, 2023.2001.2018.524539. <pub-id pub-id-type="doi">10.1101/2023.01.18.524539</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="book"><string-name><surname>Azim</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Ahmed</surname>, <given-names>S.</given-names></string-name> (<year>2018</year>). <source>Composing Fisher Kernels from Deep Neural Models</source>. <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Barachant</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bonnet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Congedo</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Jutten</surname>, <given-names>C.</given-names></string-name> (<year>2013</year>). <article-title>Classification of covariance matrices using a Riemannian-based kernel for BCI applications</article-title>. <source>Neurocomputing</source>, <volume>112</volume> <fpage>172</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2012.12.039</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Baum</surname>, <given-names>L. E.</given-names></string-name>, &amp; <string-name><surname>Eagon</surname>, <given-names>J. A.</given-names></string-name> (<year>1967</year>). <article-title>An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology</article-title>. <source>Bulletin of the American Mathematical Society</source>, <volume>73</volume>(<issue>3</issue>), <fpage>360</fpage>–<lpage>363</lpage>, 364. https://doi.org/</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Baum</surname>, <given-names>L. E.</given-names></string-name>, &amp; <string-name><surname>Petrie</surname>, <given-names>T.</given-names></string-name> (<year>1966</year>). <article-title>Statistical Inference for Probabilistic Functions of Finite State Markov Chains</article-title>. <source>The Annals of Mathematical Statistics</source>, <volume>37</volume>(<issue>6</issue>), <fpage>1554</fpage>–<lpage>1563</lpage>, 1510. <pub-id pub-id-type="doi">10.1214/aoms/1177699147</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name> (<year>2012</year>). <article-title>Modelling with independent components</article-title>. <source>Neuroimage</source>, <volume>62</volume>(<issue>2</issue>), <fpage>891</fpage>–<lpage>901</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.020</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Mackay</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Filippini</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2009</year>). <article-title>Group comparison of resting-state FMRI data using multi-subject ICA and dual regression</article-title>. <source>Neuroimage</source>, <volume>47</volume> <fpage>S148</fpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(09)71511-3</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Cole</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Franke</surname>, <given-names>K.</given-names></string-name> (<year>2017</year>). <article-title>Predicting Age Using Neuroimaging: Innovative Brain Ageing Biomarkers</article-title>. <source>Trends Neurosci</source>, <volume>40</volume>(<issue>12</issue>), <fpage>681</fpage>–<lpage>690</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2017.10.001</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Denissen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>De Cock</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Costers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Baijot</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Laton</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Penner</surname>, <given-names>I. K.</given-names></string-name>, <string-name><surname>Grothe</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kirsch</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>D’Hooghe</surname> <given-names>M B.</given-names></string-name>, <string-name><surname>D’Haeseleer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dive</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>De Mey</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Van Schependom</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sima</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Nagels</surname>, <given-names>G.</given-names></string-name> (<year>2022</year>). <article-title>Brain age as a surrogate marker for cognitive performance in multiple sclerosis</article-title>. <source>Eur J Neurol</source>, <volume>29</volume>(<issue>10</issue>), <fpage>3039</fpage>–<lpage>3049</lpage>. <pub-id pub-id-type="doi">10.1111/ene.15473</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Do</surname>, <given-names>M. N.</given-names></string-name> (<year>2003</year>). <article-title>Fast approximation of Kullback-Leibler distance for dependence trees and hidden Markov models [Article]</article-title>. <source>IEEE Signal Processing Letters</source>, <volume>10</volume>(<issue>4</issue>), <fpage>115</fpage>–<lpage>118</lpage>. <pub-id pub-id-type="doi">10.1109/LSP.2003.809034</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Kozynets</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sabbagh</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lemaître</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Liem</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name> (<year>2020</year>). <article-title>Combining magnetoencephalography with magnetic resonance imaging enhances learning of surrogate-biomarkers</article-title>. <source>eLife</source>, <volume>9</volume> <fpage>e54055</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.54055</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Sotiropoulos</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jbabdi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Polimeni</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Consortium</surname>, <given-names>W. U.-M. H.</given-names></string-name> (<year>2013</year>). <article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title>. <source>Neuroimage</source>, <volume>80</volume> <fpage>105</fpage>–<lpage>124</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Griffanti</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Auerbach</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Douaud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sexton</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Zsoldos</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ebmeier</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Filippini</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mackay</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Baselli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2014</year>). <article-title>ICA-based artefact removal and accelerated fMRI acquisition for improved resting state network imaging</article-title>. <source>Neuroimage</source>, <volume>95</volume> <fpage>232</fpage>–<lpage>247</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.034</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="book"><string-name><surname>Gumbel</surname>, <given-names>E. J.</given-names></string-name> (<year>1958</year>). <source>Statistics of Extremes</source>. <publisher-name>Columbia University Press</publisher-name>. <pub-id pub-id-type="doi">10.7312/gumb92958</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Gönen</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Alpaydin</surname>, <given-names>E.</given-names></string-name> (<year>2011</year>). <article-title>Multiple kernel learning algorithms</article-title>. <source>J Mach Learn Res</source>, <volume>12</volume> <fpage>2211</fpage>–<lpage>2268</lpage>. <pub-id pub-id-type="doi">10.5555/1953048.2021071</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="other"><string-name><surname>Jaakkola</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Diekhans</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Haussler</surname>, <given-names>D.</given-names></string-name> (<year>1999</year>). <article-title>Using the Fisher kernel method to detect remote protein homologies</article-title>. <source>Proc Int Conf Intell Syst Mol Biol</source>, <fpage>149</fpage>–<lpage>158</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Jaakkola</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Haussler</surname>, <given-names>D.</given-names></string-name> (<year>1998</year>). <source>Exploiting Generative Models in Discriminative Classifiers. NIPS</source>,</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Liégeois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kong</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Orban</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Van De Ville</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ge</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Sabuncu</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Yeo</surname>, <given-names>B. T. T.</given-names></string-name> (<year>2019</year>). <article-title>Resting brain dynamics at different timescales capture distinct aspects of human behavior</article-title>. <source>Nat Commun</source>, <volume>10</volume>(<issue>1</issue>), <fpage>2317</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-10317-7</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Lurie</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Kessler</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Betzel</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kheilholz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kucyi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Liégeois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Lindquist</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>McIntosh</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Shine</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>W. H.</given-names></string-name>, <string-name><surname>Bielczyk</surname>, <given-names>N. Z.</given-names></string-name>, <string-name><surname>Douw</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kraft</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Muthuraman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pasquini</surname>, <given-names>L.</given-names></string-name>, … <string-name><surname>Calhoun</surname>, <given-names>V. D.</given-names></string-name> (<year>2019</year>). <article-title>Questions and controversies in the study of time-varying functional connectivity in resting fMRI</article-title>. <source>Netw Neurosci</source>, <volume>4</volume>(<issue>1</issue>), <fpage>30</fpage>–<lpage>69</lpage>. <pub-id pub-id-type="doi">10.1162/netn_a_00116</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="book"><string-name><surname>MacKay</surname>, <given-names>D. J. C.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>D. J. C. M.</given-names></string-name>, &amp; <string-name><surname>Press</surname>, <given-names>C. U.</given-names></string-name> (<year>2003</year>). <source>Information Theory, Inference and Learning Algorithms</source>. <publisher-name>Cambridge University Press</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://books.google.dk/books?id=AKuMj4PN_EMC">https://books.google.dk/books?id=AKuMj4PN_EMC</ext-link></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Marquand</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Rezek</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Buitelaar</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name> (<year>2016</year>). <article-title>Understanding Heterogeneity in Clinical Cohorts Using Normative Models: Beyond Case-Control Studies</article-title>. <source>Biological psychiatry</source>, <volume>80</volume>(<issue>7</issue>), <fpage>552</fpage>–<lpage>561</lpage>. <pub-id pub-id-type="doi">10.1016/j.biopsych.2015.12.023</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Mwangi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>T. S.</given-names></string-name>, &amp; <string-name><surname>Soares</surname>, <given-names>J. C.</given-names></string-name> (<year>2014</year>). <article-title>A Review of Feature Reduction Techniques in Neuroimaging</article-title>. <source>Neuroinformatics</source>, <volume>12</volume>(<issue>2</issue>), <fpage>229</fpage>–<lpage>244</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-013-9204-3</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Rosenberg</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Papademetris</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name>, &amp; <string-name><surname>Chun</surname>, <given-names>M. M.</given-names></string-name> (<year>2016</year>). <article-title>A neuromarker of sustained attention from whole-brain functional connectivity</article-title>. <source>Nat Neurosci</source>, <volume>19</volume>(<issue>1</issue>), <fpage>165</fpage>–<lpage>171</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4179</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Douaud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Griffanti</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2014</year>). <article-title>Automatic denoising of functional MRI data: combining independent component analysis and hierarchical fusion of classifiers</article-title>. <source>Neuroimage</source>, <volume>90</volume> <fpage>449</fpage>–<lpage>468</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.11.046</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="other"><string-name><surname>Saunders</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gammerman</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Vovk</surname>, <given-names>V.</given-names></string-name> (<year>1998</year>). <source>Ridge regression learning algorithm in dual variables</source>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Schouten</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Koini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>de Vos</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Seiler</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>van der Grond</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lechner</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hafkemeijer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Möller</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>de Rooij</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Rombouts</surname>, <given-names>S. A. R. B.</given-names></string-name> (<year>2016</year>). <article-title>Combining anatomical, diffusion, and resting state functional magnetic resonance imaging for individual classification of mild and moderate Alzheimer’s disease</article-title>. <source>NeuroImage: Clinical</source>, <volume>11</volume> <fpage>46</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2016.01.002</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="book"><string-name><surname>Schölkopf</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Smola</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Bach</surname>, <given-names>F.</given-names></string-name> (<year>2002</year>). <source>Learning with kernels: support vector machines, regularization, optimization, and beyond</source>. <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><string-name><surname>Shawe-Taylor</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Cristianini</surname>, <given-names>N.</given-names></string-name> (<year>2004</year>). <source>Kernel Methods for Pattern Analysis</source>. <publisher-name>Cambridge University Press</publisher-name>. <pub-id pub-id-type="doi">10.1017/CBO9780511809682</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Rosenberg</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Chun</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Papademetris</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name> (<year>2017</year>). <article-title>Using connectome-based predictive modeling to predict individual behavior from brain connectivity</article-title>. <source>Nat Protoc</source>, <volume>12</volume>(<issue>3</issue>), <fpage>506</fpage>–<lpage>518</lpage>. <pub-id pub-id-type="doi">10.1038/nprot.2016.178</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Auerbach</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Bijsterbosch</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Douaud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Duff</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Feinberg</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Griffanti</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Harms</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Kelly</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Power</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Vu</surname>, <given-names>A. T.</given-names></string-name>, … <string-name><surname>Consortium</surname>, <given-names>W. U.-M. H.</given-names></string-name> (<year>2013</year>). <article-title>Resting-state fMRI in the Human Connectome Project</article-title>. <source>Neuroimage</source>, <volume>80</volume> <fpage>144</fpage>–<lpage>168</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.039</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Alfaro-Almagro</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, &amp; <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name> (<year>2019</year>). <article-title>Estimation of brain age delta from brain imaging</article-title>. <source>Neuroimage</source>, <volume>200</volume> <fpage>528</fpage>–<lpage>539</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.017</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Barch</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Uğurbil</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name> (<year>2013</year>). <article-title>Functional connectomics from resting-state fMRI</article-title>. <source>Trends Cogn Sci</source>, <volume>17</volume>(<issue>12</issue>), <fpage>666</fpage>–<lpage>682</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2013.09.016</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Stephan</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Schlagenhauf</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Huys</surname>, <given-names>Q. J. M.</given-names></string-name>, <string-name><surname>Raman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Aponte</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Brodersen</surname>, <given-names>K. H.</given-names></string-name>, <string-name><surname>Rigoux</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Moran</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Daunizeau</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, &amp; <string-name><surname>Heinz</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <article-title>Computational neuroimaging strategies for single patient predictions</article-title>. <source>Neuroimage</source>, <volume>145</volume> <fpage>180</fpage>–<lpage>199</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.06.038</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Vaghari</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kabir</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Henson</surname>, <given-names>R. N.</given-names></string-name> (<year>2022</year>). <article-title>Late combination shows that MEG adds to MRI in classifying MCI versus controls</article-title>. <source>Neuroimage</source>, <volume>252</volume> <fpage>119054</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119054</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="book"><string-name><surname>van der Maaten</surname>, <given-names>L.</given-names></string-name> (<year>2011</year>). <chapter-title>Learning Discriminative Fisher Kernels</chapter-title>. <source>Proceedings of the 28th International Conference on Machine Learning</source>, <publisher-loc>Bellevue, WA, USA</publisher-loc>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Barch</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name> (<year>2013</year>). <article-title>The WU-Minn Human Connectome Project: an overview</article-title>. <source>Neuroimage</source>, <volume>80</volume> <fpage>62</fpage>–<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Auerbach</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Barch</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E. J.</given-names></string-name>, <string-name><surname>Bucholz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Curtiss</surname>, <given-names>S. W.</given-names></string-name>, <string-name><surname>Della Penna</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Feinberg</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Harel</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Heath</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Larson-Prior</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marcus</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Michalareas</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, … <string-name><surname>Consortium</surname>, <given-names>W. U.-M. H.</given-names></string-name> (<year>2012</year>). <article-title>The Human Connectome Project: a data acquisition perspective</article-title>. <source>Neuroimage</source>, <volume>62</volume>(<issue>4</issue>), <fpage>2222</fpage>–<lpage>2231</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Raamana</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Hoyos-Idrobo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schwartz</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name> (<year>2017</year>). <article-title>Assessing and tuning brain decoders: Cross-validation, caveats, and guidelines</article-title>. <source>Neuroimage</source>, <volume>145</volume> <fpage>166</fpage>–<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.038</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Llera</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name> (<year>2021</year>). <article-title>Behavioural relevance of spontaneous, transient brain network interactions in fMRI</article-title>. <source>Neuroimage</source>, <volume>229</volume> <fpage>117713</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117713</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Quinn</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Dupret</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tejero-Cantero</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name> (<year>2016</year>). <article-title>Spectrally resolved fast transient brain states in electrophysiological data</article-title>. <source>Neuroimage</source>, <volume>126</volume> <fpage>81</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.047</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name> (<year>2017</year>). <article-title>Brain network dynamics are hierarchically organized in time</article-title>. <source>Proc Natl Acad Sci U S A</source>, <volume>114</volume>(<issue>48</issue>), <fpage>12827</fpage>–<lpage>12832</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1705120114</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Wen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Varol</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Sotiras</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Chand</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>Erus</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Shou</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Abdulkadir</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hwang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Dwyer</surname>, <given-names>D. B.</given-names></string-name>, <string-name><surname>Pigoni</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dazzan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kahn</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Schnack</surname>, <given-names>H. G.</given-names></string-name>, <string-name><surname>Zanetti</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Meisenzahl</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Busatto</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Crespo-Facorro</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Rafael</surname>, <given-names>R. G.</given-names></string-name>, … <string-name><surname>Alzheimer’s Disease Neuroimaging</surname>, <given-names>I.</given-names></string-name> (<year>2022</year>). <article-title>Multi-scale semi-supervised clustering of brain images: Deriving disease subtypes</article-title>. <source>Med Image Anal</source>, <volume>75</volume> <fpage>102304</fpage>. <pub-id pub-id-type="doi">10.1016/j.media.2021.102304</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Winkler</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2015</year>). <article-title>Multi-level block permutation</article-title>. <source>Neuroimage</source>, <volume>123</volume> <fpage>253</fpage>–<lpage>268</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.092</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Wolfers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Buitelaar</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Franke</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Marquand</surname>, <given-names>A. F.</given-names></string-name> (<year>2015</year>). <article-title>From estimating activation locality to predicting disorder: A review of pattern recognition for neuroimaging-based psychiatric diagnostics</article-title>. <source>Neurosci Biobehav Rev</source>, <volume>57</volume> <fpage>328</fpage>–<lpage>349</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.08.001</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Zou</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Hastie</surname>, <given-names>T.</given-names></string-name> (<year>2005</year>). <article-title>Regularization and Variable Selection Via the Elastic Net</article-title>. <source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source>, <volume>67</volume>(<issue>2</issue>), <fpage>301</fpage>–<lpage>320</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95125.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study combines the use of Fisher Kernels with Hidden Markov models aiming to improve brain-behaviour prediction. The evidence supporting the authors' conclusions is <bold>solid</bold>, comparing brain-behaviour prediction accuracies across a range of different traits. This work is timely and will be of interest to neuroscientists working on functional connectivity for brain-behaviour association.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95125.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors attempt to validate Fisher Kernels on the top of HMM as a way to better describe human brain dynamics at resting state. The objective criterion was the better prediction of the proposed pipeline of the individual traits.</p>
<p>Strengths:</p>
<p>
The authors analyzed rs-fMRI dataset from the HCP providing results also from other kernels.</p>
<p>
The authors also provided findings from simulation data.</p>
<p>Weaknesses:</p>
<p>(1) The authors should explain in detail how they applied cross-validation across the dataset for both optimization of parameters, and also for cross-validation of the models to predict individual traits.</p>
<p>(2) They discussed throughout the paper that their proposed (HMM+Fisher) kernel approach outperformed dynamic functional connectivity (dFC). However, they compared the proposed methodology with just static FC.</p>
<p>(3) If the authors wanted to claim that their methodology is better than dFC, then they have to demonstrate results based on dFC with the trivial sliding window approach.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95125.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript presents a valuable investigation into the use of Fisher Kernels for extracting representations from temporal models of brain activity, with the aim of improving regression and classification applications. The authors provide solid evidence through extensive benchmarks and simulations that demonstrate the potential of Fisher Kernels to enhance the accuracy and robustness of regression and classification performance in the context of functional magnetic resonance imaging (fMRI) data. This is an important achievement for the neuroimaging community interested in predictive modeling from brain dynamics and, in particular, state-space models.</p>
<p>Strengths:</p>
<p>(1) The study's main contribution is the innovative application of Fisher Kernels to temporal brain activity models, which represents a valuable advancement in the field of human cognitive neuroimaging.</p>
<p>(2) The evidence presented is solid, supported by extensive benchmarks that showcase the method's effectiveness in various scenarios.</p>
<p>(3) Model inspection and simulations provide important insights into the nature of the signal picked up by the method, highlighting the importance of state rather than transition probabilities.</p>
<p>(4) The documentation and description of the methods are solid including sufficient mathematical details and availability of source code, ensuring that the study can be replicated and extended by other researchers.</p>
<p>Weaknesses:</p>
<p>(1) The generalizability of the findings is currently limited to the young and healthy population represented in the Human Connectome Project (HCP) dataset. The potential of the method for other populations and modalities remains to be investigated.</p>
<p>(2) The possibility of positivity bias in the HMM, due to the use of a population model before cross-validation, needs to be addressed to confirm the robustness of the results.</p>
<p>(3) The statistical significance testing might be compromised by incorrect assumptions about the independence between cross-validation distributions, which warrants further examination or clearer documentation.</p>
<p>(4) The inclusion of the R^2 score, sensitive to scale, would provide a more comprehensive understanding of the method's performance, as the Pearson correlation coefficient alone is not standard in machine learning and may not be sufficient (even if it is common practice in applied machine learning studies in human neuroimaging).</p>
<p>(5) The process for hyperparameter tuning is not clearly documented in the methods section, both for kernel methods and the elastic net.</p>
<p>(6) For the time-averaged benchmarks, a comparison with kernel methods using metrics defined on the Riemannian SPD manifold, such as employing the Frobenius norm of the logarithm map within a Gaussian kernel, would strengthen the analysis, cf. Jayasumana (<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.4172">https://arxiv.org/abs/1412.4172</ext-link>) Table 1, log-euclidean metric.</p>
<p>(7) A more nuanced and explicit discussion of the limitations, including the reliance on HCP data, lack of clinical focus, and the context of tasks for which performance is expected to be on the low end (e.g. cognitive scores), is crucial for framing the findings within the appropriate context.</p>
<p>(8) While further benchmarks could enhance the study, the authors should provide a critical appraisal of the current findings and outline directions for future research, considering the scope and budget constraints of the work.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95125.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work, the authors use a Hidden Markov Model (HMM) to describe dynamic connectivity and amplitude patterns in fMRI data, and propose to integrate these features with the Fisher Kernel to improve the prediction of individual traits. The approach is tested using a large sample of healthy young adults from the Human Connectome Project. The HMM-Fisher Kernel approach was shown to achieve higher prediction accuracy with lower variance on many individual traits compared to alternate kernels and measures of static connectivity. As an additional finding, the authors demonstrate that parameters of the HMM state matrix may be more informative in predicting behavioral/cognitive variables in this data compared to state-transition probabilities.</p>
<p>Strengths:</p>
<p>- Overall, this work helps to address the timely challenge of how to leverage high-dimensional dynamic features to describe brain activity in individuals.</p>
<p>
- The idea to use a Fisher Kernel seems novel and suitable in this context.</p>
<p>
- Detailed comparisons are carried out across the set of individual traits, as well as across models with alternate kernels and features.</p>
<p>
- The paper is well-written and clear, and the analysis is thorough.</p>
<p>Potential weaknesses:</p>
<p>- One conclusion of the paper is that the Fisher Kernel &quot;predicts more accurately than other methods&quot; (Section 2.1 heading). I was not certain this conclusion is fully justified by the data presented, as it appears that certain individual traits may be better predicted by other approaches (e.g., as shown in Figure 3) and I found it hard to tell if certain pairwise comparisons were performed -- was the linear Fisher Kernel significantly better than the linear Naive normalized kernel, for example?</p>
<p>- While 10-fold cross-validation is used for behavioral prediction, it appears that data from the entire set of subjects is concatenated to produce the initial group-level HMM estimates (which are then customized to individuals). I wonder if this procedure could introduce some shared information between CV training and test sets. This may be a minor issue when comparing the HMM-based models to one another, but it may be more important when comparing with other models such as those based on time-averaged connectivity, which are calculated separately for train/test partitions (if I understood correctly).</p>
</body>
</sub-article>
</article>