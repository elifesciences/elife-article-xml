<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105034</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105034</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105034.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Dynamic modulation of social gaze by sex and familiarity in marmoset dyads</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Xing</surname>
<given-names>Feng</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sheffield</surname>
<given-names>Alec G</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Jadi</surname>
<given-names>Monika P</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Chang</surname>
<given-names>Steve WC</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4225-5349</contrib-id>
<name>
<surname>Nandy</surname>
<given-names>Anirvan S</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>anirvan.nandy@yale.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Inderdepartmental Neuroscience Program, Yale University</institution></institution-wrap>, <city>New Haven</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Department of Neuroscience, Yale University</institution></institution-wrap>, <city>New Haven</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Department of Psychiatry, Yale University</institution></institution-wrap>, <city>New Haven</city>, <country country="US">United States</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Department of Psychology, Yale University</institution></institution-wrap>, <city>New Haven</city>, <country country="US">United States</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Wu Tsai Institute, Yale University</institution></institution-wrap>, <city>New Haven</city>, <country country="US">United States</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Kavli Institute for Neuroscience, Yale University</institution></institution-wrap>, <city>New Haven</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Takahashi</surname>
<given-names>Daniel Y</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Federal University of Rio Grande do Norte</institution>
</institution-wrap>
<city>Natal</city>
<country>Brazil</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Wassum</surname>
<given-names>Kate M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of California, Los Angeles</institution>
</institution-wrap>
<city>Los Angeles</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>senior authors, equal contribution</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-03-17">
<day>17</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105034</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-11-26">
<day>26</day>
<month>11</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-11-05">
<day>05</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.16.580693"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Xing et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Xing et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105034-v1.pdf"/>
<abstract>
<title>Summary</title>
<p>Social communication relies on the ability to perceive and interpret the direction of others’ attention, and is commonly conveyed through head orientation and gaze direction in humans and nonhuman primates. However, traditional social gaze experiments in nonhuman primates require restraining head movements, significantly limiting their natural behavioral repertoire. Here, we developed a novel framework for accurately tracking facial features and three-dimensional head gaze orientations of multiple freely moving common marmosets (<italic>Callithrix jacchus</italic>). By combining deep learning-based computer vision tools with triangulation algorithms, we were able to track the facial features of marmoset dyads within an arena. This method effectively generates dynamic 3D geometrical facial frames while overcoming common challenges like occlusion. To detect the head gaze direction, we constructed a virtual cone, oriented perpendicular to the facial frame. Using this pipeline, we quantified different types of interactive social gaze events, including partner-directed gaze and joint gaze to a shared spatial location. We observed clear effects of sex and familiarity on both interpersonal distance and gaze dynamics in marmoset dyads. Unfamiliar pairs exhibited more stereotyped patterns of arena occupancy, more sustained levels of social gaze across social distance, and increased social gaze monitoring. On the other hand, familiar pairs exhibited higher levels of joint gazes. Moreover, males displayed significantly elevated levels of gazes toward females’ faces and the surrounding regions, irrespective of familiarity. Our study reveals the importance of two key social factors in driving the gaze behaviors of a prosocial primate species and lays the groundwork for a rigorous quantification of primate behaviors in naturalistic settings.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Updated with new results (Figure 5) and text.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Primates, including humans, exhibit complex social structures and engage in rich interactions with members of their species, which are crucial for their survival and development. Among social stimuli, the face holds paramount importance with specialized neural systems (<xref ref-type="bibr" rid="c12">Deen et al., 2023</xref>; <xref ref-type="bibr" rid="c23">Hesse &amp; Tsao, 2020</xref>) and is attentively prioritized by primates during much of their social interaction. Notably, the eyes garner the most attention among all facial features, playing a pivotal role in indicating the direction of others’ attention and also possibly their intention (<xref ref-type="bibr" rid="c8">Dal Monte et al., 2015</xref>; <xref ref-type="bibr" rid="c16">Emery, 2000</xref>; <xref ref-type="bibr" rid="c24">Itier et al., 2007</xref>). Indeed, understanding and interpreting the gaze of fellow individuals is a fundamental attribute of the theory of mind (ToM) (<xref ref-type="bibr" rid="c25">Martin &amp; Santos, 2016</xref>; <xref ref-type="bibr" rid="c39">Saxe &amp; Kanwisher, 2003</xref>). While current studies of social gaze using either realistic stimuli or pairs of rhesus macaques (<italic>Macaca mulatta</italic>) in a controlled laboratory setting (<xref ref-type="bibr" rid="c10">Dal Monte et al., 2016</xref>; <xref ref-type="bibr" rid="c32">Mosher et al., 2014</xref>; <xref ref-type="bibr" rid="c38">Ramezanpour &amp; Thier 2020</xref>; <xref ref-type="bibr" rid="c41">Shepherd et al., 2006</xref>; <xref ref-type="bibr" rid="c42">Shepherd &amp; Freiwald, 2018</xref>) provide valuable insights into social gaze behaviors, they are nevertheless limited in their ecological relevance.</p>
<p>To address these limitations, we turned to common marmosets (<italic>Callithrix jacchus</italic>), a highly prosocial primate species known for their social behavioral and cognitive similarities to humans (<xref ref-type="bibr" rid="c28">Miller et al., 2016</xref>). Marmosets are also a model system with increasing applications in computational ethology (<xref ref-type="bibr" rid="c31">Mitchell et al., 2014</xref>; <xref ref-type="bibr" rid="c34">Ngo et al., 2022</xref>). Like humans, they engage in cooperative breeding, a social system in which individuals care for offspring other than their own, usually at the expense of their reproduction (<xref ref-type="bibr" rid="c18">French, 1997</xref>; Solomon &amp; <xref ref-type="bibr" rid="c18">French, 1997</xref>). Gaze directions, inferred from head orientation, hold crucial information about marmoset social interactions (<xref ref-type="bibr" rid="c22">Heiney &amp; Blazquez, 2011</xref>; <xref ref-type="bibr" rid="c46">Spadacenta et al., 2022</xref>). The emergence of computational ethology (<xref ref-type="bibr" rid="c1">Anderson &amp; Perona, 2014</xref>; <xref ref-type="bibr" rid="c11">Datta, Anderson, Branson, Perona, &amp; Leifer, 2019</xref>) has propelled the development of a host of computer vision tools using deep neural networks (e.g., OpenPose by <xref ref-type="bibr" rid="c6">Cao et al., 2017</xref>, DeepLabCut by <xref ref-type="bibr" rid="c26">Mathis et al., 2018</xref>, DANNCE by <xref ref-type="bibr" rid="c15">Dunn et al., 2021</xref>; SLEAP by <xref ref-type="bibr" rid="c36">Pereira et al., 2022</xref>). Moreover, we have recently developed a scalable marmoset apparatus for automated pulling (MarmoAAP) for studying cooperation in marmoset dyads (<xref rid="c27" ref-type="bibr">Meisner et al., 2024</xref>). However, tracking head gaze direction in multiple freely moving marmosets poses a challenging problem, not yet solved by existing computer vision tools. This problem is further complicated by the fact that accurately tracking gaze orientations in primates requires three-dimensional information.</p>
<p>Here, we propose a novel framework based on a modified DeepLabCut pipeline, capable of accurately detecting body parts of multiple marmosets in 2D space and triangulating them in 3D space. By reconstructing the face frame with six facial points in 3D space, we can infer the head gaze of each animal across time. Importantly, marmosets that are not head-restrained use rapid head movements for reorienting and visual exploration (<xref ref-type="bibr" rid="c35">Pandey et al., 2020</xref>), and therefore the head direction serves as an excellent proxy for gaze orientation in unrestrained marmosets. With this framework in place, we investigated the gaze behaviors of male-female pairs of freely moving and interacting marmosets to quantify their social gaze dynamics. We investigated these gaze dynamics along the dimensions of sex and familiarity and found several key differences along both of these important social dimensions, including increased partner gaze in males that is modulated by familiarity, increased joint gaze among familiar pairs, and increased gaze monitoring by males. This fully automated tracking system can thus serve as a powerful tool for investigating primate group dynamics in naturalistic environments.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Experimental setup and reconstruction of video images in 3D space</title>
<p>The experimental setup consisted of two arenas made with acrylic plates that allowed two marmosets to visually interact with each other while being physically separate (<xref rid="figS1" ref-type="fig">Fig. S1A</xref>). Each arena was 60.96 cm long, 30.48 cm wide, and 30.48 cm high. Five sides of the arena, except the bottom, were transparent allowing a clear view of the animal subjects under observation. The bottom side of the arena was perforated with 1-inch diameter holes arranged in a hexagonal pattern to aid the animal’s traction. The arenas were mounted on a rigid frame made of aluminum building blocks, with the smaller sides facing each other, and were separated by a distance of 30.48 cm. A movable opaque divider was placed between the arenas during intermittent breaks to prevent the animals from having visual access to each other (Methods). Two monitors were attached to the aluminum frame, one on each end, for displaying video or image stimuli to the animals. To capture the whole experimental setup, two sets of four GoPro 8 cameras were attached to the frame, where each set of cameras captured the view of one of the arenas.</p>
<p>After obtaining the intrinsic parameters of the cameras by calibration and the extrinsic parameters of the cameras by L-frame analysis (see Methods), we established a world coordinate system of each arena surrounded by the corresponding set of four cameras. Crucially, the two independent world coordinate systems of the two arenas were combined by measuring the distance between the two L-shaped frames and adding this offset to one of the world coordinate systems.</p>
<p>With the established world coordinate system, any point captured by two or more cameras could be triangulated into a common three-dimensional space. Thus, the experimental setup was reconstructed into three-dimensional space by manually labeling the vertices of the arenas and monitors in the image space captured by the cameras (<xref rid="figS1" ref-type="fig">Fig. S1B</xref>). The cameras on the monitor ends (marked as ‘ML1’, ‘ML2’, ‘MR1’, ‘MR2’ in <xref rid="figS1" ref-type="fig">Fig. S1B</xref>) recorded both animal subjects, whereas the cameras in the middle (marked as ‘OL1’, ‘OL2’, ‘OR1’, ‘OR2’ in <xref rid="figS1" ref-type="fig">Fig. S1B</xref>) recorded only one animal subject.</p>
</sec>
<sec id="s2b">
<title>Automatic detection of facial features of two marmosets</title>
<p>Six facial features – the two tufts, the central blaze, two eyes, and the mouth (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>) – were selected for automated tracking using a modified version of a deep neural network (DeepLabCut, <xref ref-type="bibr" rid="c26">Mathis et al., 2018</xref>). The raw video from each camera was fed into the network to compute the probability heatmaps of each facial feature. We modified the original method to detect features of two animals (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). After processing the raw video, two locations with the highest probability over a threshold (95%) were picked from each probability heatmap (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, <italic>feature detection</italic>). Since all the features from the same animal should be clustered in image space, a K-means clustering algorithm (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, <italic>initial clustering</italic>) was used on the candidate features with the constraint that one animal can only have one unique feature (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, <italic>refine clustering</italic>); for example, one animal cannot have two left eyes. After clustering, two clusters of features corresponding to the two animals were obtained. To detect outliers that were not valid features, we first calculated a distribution of within-cluster distances (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, <italic>remove outliers</italic>). Outliers were determined as those points that had nearest-neighbor distances which were two standard deviations above the average within-cluster distance and were excluded from subsequent analyses. Note that the above analyses were performed independently for each video frame.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Pipeline of detecting facial features of two marmosets.</title>
<p>(A) Six facial features of the marmoset (face frame) are color-coded: right tuft (red), central blaze (yellow), left tuft (green), right eye (purple), left eye (blue), and mouth (magenta). (B) Feature tracking pipeline (right) with the corresponding illustration for each step across two adjacent video frames (left). At the end of the pipeline, the facial features are clustered with the identities assigned consistently across frames. Facial points are color-coded as in A. (C) Example frames of four steps in the pipeline shown in B. It can be seen clearly that the facial points are tracked and clustered accurately, and the identities are consistent across frames.</p></caption>
<graphic xlink:href="580693v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To establish temporal continuity across video frames and track animal identities, we first calculated the centroid of each cluster (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, <italic>calculate centroids</italic>). Under the heuristic that centroid trajectories corresponding to each individual animal are smoothly continuous in space and time (i.e., there are no sudden jumps or reversals in centroid location across frames at our sampling rate of 30Hz), we assigned identities to the centroids thus enabling us to track identities over time (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, <italic>establish identities</italic>). The facial features corresponding to a centroid inherited this identity (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, <italic>cluster with identities</italic>). We tested our automated pipeline against a manually labeled data set and found that features were detected with high precision (root mean squared error = 3.31 ± 0.1 pixels [mean +/-s.e.m.]; <italic>n</italic> = 2400 facial features; see Methods).</p>
<p>Our method thus allowed us to accurately detect and track the facial features of two marmosets in the arena (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>), and in more general contexts such as in a home cage with occlusions (Supplementary Video 1).</p>
</sec>
<sec id="s2c">
<title>Inferring head-gaze direction in 3D space</title>
<p>In our experimental setup, the cameras in the middle unambiguously recorded only one animal. The centroids of the facial features in the image space recorded by the middle cameras were triangulated into three-dimensional space and were constrained to be confined within the bounds of the arena. Any missing centroids were filled by interpolation from neighboring frames from both the past and future time points. The triangulated centroids acquired from cameras in the middle were then projected into the image space of cameras on the monitor ends. Since both animals were recorded by the cameras on the monitor ends, facial features from these camera views were detected with identities assigned (as described in the 2D pipeline). In the image space of cameras on the monitor ends, the cluster of facial feature points closer to the projected centroid and within the bounds of the arena were kept for later triangulation.</p>
<p>The detected facial features captured by all four cameras for one animal were subjected to triangulation. For each feature, results from all possible pairs of four cameras were triangulated. All the triangulation results were averaged to yield the final coordinates of the body part in three-dimensional space. Any missing features were filled by interpolation from neighboring frames including the previous and future time points. Testing against a manually labeled and triangulated data set yielded a high precision of 3D feature detection (root mean squared error = 3.7 ± 0.1 mm [mean +/-s.e.m.]; <italic>n</italic> = 2400 facial features; see Methods).</p>
<p>The six facial points constituted a semi-rigid geometric frame as the animal moves in 3D space (‘face frame’; <xref rid="fig1" ref-type="fig">Fig. 1</xref>), allowing us to infer the animal’s head-gaze direction as follows. The gaze orientation was calculated as the normal to the facial plane (‘face norm’) defined by the two eyes and the central blaze. The position of the ear tufts, which were behind this facial plane, was used to determine gaze direction. Since marmoset saccade amplitudes are largely restricted to 10 degrees (median less than 4 degrees) (<xref ref-type="bibr" rid="c31">Mitchell et al., 2014</xref>), we modeled the head gaze as a virtual cone with a solid angle of 10 degrees (‘gaze cone’) emanating from the facial plane (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). Notably, with multiple camera views, the face frame can be reconstructed even when the face was invisible to one of the cameras, such that the reconstructed face frame in 3D can be projected back into the image space to validate the accuracy of the detection and reconstruction (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>). We were thus able to obtain the animal’s continuous movement trajectory and the corresponding gaze direction over time (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>, Supplementary Video 2). To characterize stable gaze epochs for subsequent analyses, we calculated the velocity of the face norm and applied a stringent threshold of 0.05 (normalized units) below which the marmoset’s head movement was considered stationary (<xref rid="fig2" ref-type="fig">Fig 2D</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>3D Reconstruction of facial features and head gaze modeling.</title>
<p>(A) The face frames of two marmosets are reconstructed in 3D using the tracked facial points in <xref rid="fig1" ref-type="fig">Fig. 1 A</xref>. A cone perpendicular to the face frame (gaze cone; 10-degree solid angle) is modeled as the head gaze. (B) Two example frames with the facial points projected from 3D space onto different camera views are shown. The left frame demonstrates that the facial points can be detected using information from other cameras, even if the face is invisible from that viewpoint. (C) Trajectory of the reconstructed face frame and the corresponding gaze cones across time. (D) The histogram of the face norm velocity. The red dotted line (0.05) is the threshold below which the marmoset head direction is considered to be stationary.</p></caption>
<graphic xlink:href="580693v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We first examined our method’s ability to characterize gaze behaviors of freely moving marmosets by presenting either video or image stimuli to individual animals on a monitor screen (see Methods). Marmosets exhibited longer gaze duration to video stimuli compared to image stimuli (<xref rid="figS2" ref-type="fig">Fig. S2A</xref>; Mann Whitney U Test, p &lt; 0.001). However, this difference was not caused by differences in gaze dispersion (<xref rid="figS2" ref-type="fig">Fig. S2B</xref>; Mann Whitney U Test, ns). By examining the frequency of gaze events, we found that marmosets gazed at the monitor more during the early period of video stimuli presentations compared to the late period (<xref rid="figS2" ref-type="fig">Fig. S2C</xref>; Mann Whitney U Test, p &lt; 0.001), while there was no such difference for the image stimuli (<xref rid="figS2" ref-type="fig">Fig. S2C</xref>; Mann Whitney U Test, p = 0.4345). There was also a significant difference in gaze frequency between the early period of video presentations compared to the same period for image presentations (<xref rid="figS2" ref-type="fig">Fig. S2C</xref>; Mann Whitney U Test, p &lt; 10<sup>−10</sup>). Taken together, our results support that dynamic visual stimuli elicit greater overt attention compared to static stimuli in marmosets, similar to macaques (<xref ref-type="bibr" rid="c10">Dal Monte et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Furl et al., 2012</xref>) and humans (<xref ref-type="bibr" rid="c7">Chevallier et al., 2015</xref>).</p>
</sec>
<sec id="s2d">
<title>Positional dynamics of marmoset dyads</title>
<p>With this automated system in place, we recorded the behavior of four pairs of familiar marmosets and four pairs of unfamiliar marmosets. Data from each pair was recorded in one session consisting of ten five-minute free-viewing blocks interleaved with five-minute breaks. Each pair consisted of a male and a female animal. The familiar pairs were defined as cage mates, while each member of an unfamiliar pair was from a different home cage with no visual access to each other while in the colony. We first examined the movement trajectories of marmoset dyads and used the centroids of the face frames across time to represent the trajectories. For an example 5-minute segment (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>), we observed that the marmosets preferred to stay at the two ends of their respective arenas. This was confirmed by a heatmap of projections of the trajectories on the plane parallel to the vertical long side of the arenas (‘XZ’ plane). Furthermore, there were two hotspots along the vertical axis in the heatmap of projections to the vertical short side plane (‘YZ’ plane), suggesting that the animals’ preferred body postures were either upright or crouched.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Positional dynamics of marmoset dyads</title>
<p>(A) Movement trajectories of the face frame centroids for a marmoset pair (orange for female, cyan for male) in an example five-minute block. The heatmaps were calculated using the projections of the trajectories to XY, YZ, and XZ planes. (B) Marginal distributions of movement trajectories along the X and Z axes were calculated for all marmosets and grouped by familiarity and sex (transparent colors for familiar pairs, opaque colors for unfamiliar pairs). Black bars indicate significant differences between pairs of distributions (Mann-Whitney U test, significance level at 5%). (C) Histograms of social distance along the X axis show trimodal distributions for both familiar pairs (gray) and unfamiliar pairs (black). The top inset shows a schematic of the arena configuration with corresponding colors (orange for females and cyan for males). Social distance is defined as the Euclidean distance between the two marmosets. I he fitted red curve for the unfamiliar pairs is a tn-Gaussian distribution, while the fitted red curve for the familiar pairs is a mixture of Gamma and Gaussian distributions, with the first peak as the Gamma distribution. The three regions were designated as ‘Near’, Intermediate’, and ‘Far’. The bottom inset illustrates the reason for this nomenclature. (D) Temporal evolution of social distance for unfamiliar and familiar pairs (which each 5-minute viewing block). The central dark line is the mean and the shaded area is the standard deviation. Black dots indicate significant differences (Mann-Whitney U test, significance level at 5%).</p></caption>
<graphic xlink:href="580693v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To quantify their positional dynamics, we examined the marginal distributions of the movement trajectories along the horizontal (‘X’) and vertical (‘Z’) axes across all sessions and grouped them along the dimensions of sex and familiarity (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>). Along the X axis (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>, left), the distributions were slightly bimodal, with the main peak in the region near to the inner edge of the arenas. Regardless of familiarity, male marmosets tended to stay closer to the inner edge compared to females, as shown by the significant differences in the distributions when X ranged from 0 to 150 mm. However, there were no significant differences between the same sex members of familiar and unfamiliar pairs. For the Z axis (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>, right), the distributions were bimodal (Bimodality coefficients (BC) all exceeded the threshold at 5/9 with a 5% margin; BC(familiar male) = 0.5863; BC(familiar female) = 0.6278; BC(unfamiliar male) = 0.6235; BC(unfamiliar female) = 0.6570; Warren Sarle’s bimodality test), consistent with what we observed in the heatmaps indicating either upright or crouched postures. The positional distributions along the Z axis were not different based on sex or familiarity.</p>
<p>To further characterize the “social distance” dynamics of the freely moving dyads, we calculated the distance between the centroids of the pairs. We then examined the distributions of the social distance along the X-axis separately for familiar and unfamiliar pairs (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>). The distributions were trimodal, and can be explained by the bimodal distribution of movement trajectories of individual marmosets along the X-axis. As mentioned above, marmosets tended to stay at the two ends of their arenas, and thus, combinations of preferred positions at the two ends for the dyads (see insets in <xref rid="fig3" ref-type="fig">Fig. 3C</xref>) resulted in the trimodal distribution. We termed these three peaks as ‘Near’, ‘Intermediate’, and ‘Far’. To quantify these distributions, we fitted the empirical data with mixture models using maximum likelihood estimation (see Methods). The social distance for unfamiliar pairs was best fitted by a mixture of three Gaussians while the distribution for familiar pairs was best fitted by a mixture of Gamma and Gaussian distributions. The first peak (‘Near’) of the familiar-pair distribution was best fitted by a Gamma distribution, implying a higher degree of dispersion when familiar marmosets were close to each other. Upon examining the temporal evolution of the social distance (within each 5-minute viewing block), we found that the social distance of unfamiliar pairs increased over time, whereas this distance fluctuated over time for familiar pairs (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>), further indicating that the social distance dynamics of marmoset dyads depended on familiarity.</p>
</sec>
<sec id="s2e">
<title>Social gaze dynamics of marmoset dyads</title>
<p>We next investigated the interactive aspects of gaze behaviors in freely moving marmoset dyads. The gaze interaction between two animals could be simplified as the relative positions of two gaze cones in three-dimensional space (see Methods; <xref rid="fig4" ref-type="fig">Fig. 4A</xref>; Supplementary Video 3). If the gaze cone of one animal intersected with the facial plane of the second animal (but not vice versa), we termed it ‘partner gaze’. If the gaze cones of both animals intersected with that of the other’s facial place, we termed it ‘reciprocal gaze’. In our dataset, the instances of reciprocal gaze were very low and were thus excluded from further analysis. If the two cones intersected anywhere outside the facial planes, we termed it ‘joint gaze’. All other cases were regarded as ‘no interaction’ between the two animals.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Live interactive gaze analysis of unfamiliar and familiar marmoset dyads</title>
<p>(A) Gaze type categorized based on the relative positions of the gaze cones Joint gaze is defined as two marmosets looking at the same location. A partner gaze is defined as one animal looking at the other animal’s face (but not vice versa) No interaction occurs when the two gaze cones do not intersect (B) Histograms of gaze count as a function of inter-animal distance, shown separately for familiar and unfamiliar pairs (C) Same data as m B shown as pie charts of percentages in social gaze states (D) Gaze state transition diagrams for familiar and unfamiliar pairs The nodes ate the gaze states and the edges connecting the nodes represent the transition between states Edge colors indicate transition probabilities (E) Partner gaze self-transition probabilities for familiar and unfamiliar pairs (χ2 test) (F) Delta transition matrix between the unfamiliar pair and familiar pair state transition diagrams. Transitions that are significantly different across familiarity are marked by astensks (χ2 test, male to male, p &lt; 0.05; female to male, p &lt; 0.05; joint to joint p &lt; 0.0001). (G) Left. The schematic illustrates hew gazing toward the surrounding region of a partner’s face area was measured Right Counts of gaze towards the surrounding region of the partner s face by familiarity and sex. (Mann-Whitney U test, **** means p &lt; 0.001. **** means p &lt; 0.0001)</p></caption>
<graphic xlink:href="580693v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We identified stable gaze epochs by thresholding the face norm velocity (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>). Stable epochs were categorized into gaze states based on the gaze event types described above. We first analyzed the fraction of gaze states in the three position ranges identified from the social distance analysis (<xref rid="fig4" ref-type="fig">Fig. 4B,C</xref>). We found that male marmosets gazed more toward their partner females’ faces regardless of familiarity (p &lt; 0.01, <italic>χ</italic><sup>2</sup> test). Fraction of male → female partner gaze incidents decreased with increasing social distance for familiar pairs, while they remained constant in the case of unfamiliar pairs (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>). Moreover, females in unfamiliar pairs exhibited significantly higher partner gazes (female→male) compared to those in familiar pairs (p &lt; 0.01, <italic>χ</italic><sup>2</sup> test; <xref rid="fig4" ref-type="fig">Fig. 4C</xref>). The total counts of social gaze states (joint gaze and partner gaze) were higher for familiar pairs when they were near, but these decreased more dramatically with increasing distance (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>).</p>
<p>To investigate the dynamics of these gaze states, we computed state transition probabilities among distinct gaze event types for familiar and unfamiliar dyads. We applied a Markov chain model (see Methods) (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>), in which the nodes were the gaze states and the edges connecting the nodes represented the transitions between gaze states. We first focused on the recurrent (self-transition) edges for the partner gaze states. Recurrent edges indicate a transition back to the same stable gaze state after a break likely due to physical movement, and reflect the robustness of the state despite movement. In line with our previous results (<xref rid="fig4" ref-type="fig">Fig. 4B,C</xref>), males exhibit significantly higher recurrent partner gazes compared to females, irrespective of familiarity (<italic>χ</italic><sup>2</sup> test, unfamiliar male vs unfamiliar female, p &lt; 10<sup>−10</sup>; familiar male vs familiar female, p &lt; 0.01; <xref rid="fig4" ref-type="fig">Fig. 4E</xref>).</p>
<p>A comparison of state transition probabilities across the dimension of familiarity yielded several noteworthy findings (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>). First, recurrent male partner gaze (male female) was significantly enhanced in unfamiliar pairs (p &lt; 0.05, <italic>χ</italic><sup>2</sup> test), suggesting a heightened interest in unfamiliar females. Second, there was a higher probability of transition from a female partner gaze to a male partner gaze in familiar pairs compared to unfamiliar pairs, suggesting that familiar males have a greater awareness of and tendency to reciprocate their partners’ gaze (p &lt; 0.05, <italic>χ</italic><sup>2</sup> test). Third, there was a higher probability of recurrent joint gazes in familiar pairs compared to unfamiliar pairs, suggesting that familiar pairs explore common objects more than unfamiliar pairs (p &lt; 10<sup>−4</sup>, <italic>χ</italic><sup>2</sup> test).</p>
<p>Monitoring others to anticipate their future actions is critical for successful social interactions (<xref ref-type="bibr" rid="c20">Hari et al., 2015</xref>). In particular, successful interactive gaze exchanges require constant monitoring of other’s gaze. We analyzed the gaze distribution in the surrounding region of a partner’s face to estimate gaze monitoring tied to increased social attention (<xref ref-type="bibr" rid="c9">Dal Monte et al., 2022</xref>). We quantified this by the distance between the centroid of the partner’s face-frame and the point of intersection of the gaze cone with the partner’s facial plane (<xref rid="fig4" ref-type="fig">Fig. 4G</xref>, left). Unfamiliar marmosets (both males and females) showed significantly higher (Mann-Whitney U test, unfamiliar male vs familiar male, p &lt; 0.0001; unfamiliar female vs familiar female, p &lt; 0.001) incidences of gaze toward the surrounding region of the partner’s face (<xref rid="fig4" ref-type="fig">Fig 4G</xref>, right; compare darker lines with the lighter lines). Further, males exhibited markedly higher (Mann-Whitney U test, p &lt; 0.0001) incidences of gaze toward the partner females’ face (<xref rid="fig4" ref-type="fig">Fig 4G</xref>, right; compare cyan lines with the orange lines).</p>
<p>Joint gaze is crucial in primates as it underpins social communication and coordination, serving as a foundation for more complex behaviors like cooperation and shared attention (<xref ref-type="bibr" rid="c16">Emery, 2000</xref>; <xref ref-type="bibr" rid="c47">Tomasello et al., 2005</xref>). We next analyzed joint gaze behaviors between marmoset dyads, first projecting the locations of joint gazes onto different 2D planes around the arena (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). Significant asymmetry was observed in the distribution of joint gazes on the ‘XY’ and ‘XZ’ planes, with the majority of joint gazes occurring within the female’s arena, regardless of familiarity. This is consistent with our earlier results showing more partner-directed gazes from males and increased attention to the region surrounding the female partner (<xref rid="fig4" ref-type="fig">Fig. 4B,C,G</xref>). Notably, joint gazes between familiar pairs were more centrally distributed compared to those between unfamiliar pairs. When examining the ‘XZ’ and ‘YZ’ planes, we found that most joint gazes were concentrated along the lower Z-axis, indicating that marmoset pairs tend to establish shared attention closer to ground level. Given the importance of social distance in shaping marmoset behaviors, we further investigated the distribution of joint gazes as a function of social distance. Striking differences were found between familiar and unfamiliar pairs (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). The distribution for familiar pairs was best fitted by a tri-Gaussian model, consistent with the distribution of social distances observed in <xref rid="fig3" ref-type="fig">Fig. 3C</xref>, suggesting that familiar pairs can establish joint gazes regardless of distance. In contrast, the distribution for unfamiliar pairs was best described by a lognormal model, indicating that unfamiliar pairs tend to establish joint gazes only when in close proximity.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Joint gaze analysis between familiar and unfamiliar dyads.</title>
<p>(A) Heatmaps of joint gaze locations projected onto 2D planes (From left to right: ‘XY’, ‘XZ’, ‘YZ’). The icon on the left show a schematic of a joint gaze along with the arena configuration with corresponding colors (orange for females and cyan for males). The colored rectangles superimposed on the heatmaps show the projections of the arenas onto those 2D planes. The white dotted line indicates the midline between the two arenas. (B) The probability of joint gaze at varying social distances shows distinct distributions for unfamiliar pairs (black) and familiar pairs (gray). The fitted red curve for the unfamiliar pairs is a lognormal distribution, while the fitted red curve for the familiar pairs is a tri-Gaussian distribution. Thin fitted red curves in both panels are the data from individual pairs. The inset shows a schematic of a joint gaze and the social distance metric. Social distance is defined as the Euclidean distance between the two marmosets</p></caption>
<graphic xlink:href="580693v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Overall, we found that both the social dimensions we examined – familiarity and sex – are significant determinants of natural gaze dynamics among marmosets.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we first presented a novel framework for the automated, markerless, and identity-preserving tracking of 3D facial features of multiple marmosets. By building on top of the deep-learning framework provided by DeepLabCut, we used a constellation of cameras to overcome “blindspots” due to occlusion and imposed spatiotemporal smoothness constraints on the detected features to establish and preserve identities across time. The tracked facial features from each animal form a semi-rigid face frame as the animal moves freely in 3D space, thereby allowing us to infer the animal’s gaze direction at each moment in time. It is important to reiterate that unrestrained marmosets use rapid saccadic head-movements for reorienting (<xref ref-type="bibr" rid="c35">Pandey et al., 2020</xref>) and have a limited amplitude range of saccadic eye-movements (<xref ref-type="bibr" rid="c31">Mitchell et al., 2014</xref>). Thus their head direction serves as excellent proxy for gaze orientation in unrestrained conditions.</p>
<p>Current methods for tracking and estimating animal body postures, such as DeepLabCut (<xref ref-type="bibr" rid="c26">Mathis et al., 2018</xref>) and SLEAP (<xref ref-type="bibr" rid="c36">Pereira et al., 2022</xref>), offer substantial advantages in precision and flexibility, particularly through the use of deep learning for markerless tracking across diverse species. These methods excel in laboratory settings, where they can achieve high accuracy without invasive markers and are adaptable for various animals and tasks. However, many of these tools struggle with occlusion, especially in crowded environments or natural habitats, limiting their effectiveness in complex social interactions or open-field studies (<xref ref-type="bibr" rid="c44">Sturman et al., 2020</xref>). Additionally, when tracking marmosets that exhibit significant vertical movement, requiring 3D tracking, the performance of these methods is suboptimal. While 3D tracking systems like DANNCE (<xref ref-type="bibr" rid="c15">Dunn et al., 2021</xref>) offer enhanced spatial accuracy, these methods are limited to single-animal tracking, which is insufficient for studies in social neuroscience.</p>
<p>Primates are highly visual species whose physical explorations of their environment are not confined to two-dimensional surfaces. Gaze is a critical component of primate social behavior and conveys important social signals such as interest, attention, and emotion (<xref ref-type="bibr" rid="c16">Emery, 2000</xref>). Assessment of gaze is therefore important to understand non-verbal communication and interpersonal dynamics. Our 3D gaze tracking approach was able to capture both the positional and gaze dynamics of freely moving marmoset dyads in a naturalistic context. We observed clear effects of sex and familiarity on both interpersonal and gaze dynamics. Unfamiliar pairs exhibited more stereotyped patterns of arena occupancy, more sustained levels of social gaze across distance, and increased gaze monitoring, suggesting elevated levels of social attention compared to familiar pairs. On the other hand, familiar pairs exhibited more recurrent joint gazes in the shared environment compared to unfamiliar pairs. Familiar males also showed a higher tendency to reciprocate their partner’s gaze, suggesting a greater awareness of their partner’s social gaze state.</p>
<p>Supported by the natural ethology of marmosets (<xref ref-type="bibr" rid="c48">Yamamoto et al., 2014</xref>; Solomon &amp; <xref ref-type="bibr" rid="c18">French, 1997</xref>), we found dramatic sex differences in gaze behaviors, with males exhibiting significantly elevated levels of gaze toward females’ faces and the surrounding regions irrespective of familiarity. It is important to note that dominance in marmosets is not strictly determined by gender, as it can vary based on individual personalities and intra-group social dynamics, although breeding females typically dominate social activity within a group (<xref ref-type="bibr" rid="c13">Digby, 1995</xref>; <xref rid="c33" ref-type="bibr">Mustoe et al., 2023</xref>). While we have not explicitly controlled for dominance in this study, whether part of the observed differences can be attributed to dominance effects needs further exploration.</p>
<p>Gaze following plays a crucial role in social communication for humans and nonhuman primates, allowing for joint attention (<xref ref-type="bibr" rid="c17">Emery et al., 1997</xref>; <xref ref-type="bibr" rid="c3">Brooks &amp; Meltzoff, 2005</xref>; <xref ref-type="bibr" rid="c4">Burkart &amp; Heschl, 2006</xref>; <xref ref-type="bibr" rid="c40">Shepherd, 2010</xref>). Previous research demonstrated that head-restrained marmosets exhibited preferential gazing toward marmoset face stimuli observed by a conspecific in a quasi-reflexive manner during a free-choice task (<xref ref-type="bibr" rid="c45">Spadacenta et al., 2019</xref>). Interestingly we did not find any differences in gaze-following behaviors (transition from partner gaze to joint gaze) along the social dimensions we tested here. Future investigation of such behaviors by manipulating social variables such as dominance or kinship could provide a comprehensive understanding of gaze following and joint attention in naturalistic behavioral contexts. The scarcity of reciprocal gazes in our study may be attributed to the task-free experimental setup employed. Indeed, in other joint action tasks requiring cooperation for rewards, marmosets actively engage in reciprocal gaze behaviors (<xref ref-type="bibr" rid="c30">Miss &amp; Burkart, 2018</xref>).</p>
<p>While we focused on the tracking of facial features in this study, our automated system has the potential to extend to 3D whole-body tracking, encompassing limbs, tail, and the main body features of marmosets. In our system, multiple cameras surrounding the arena ensure that each body part of interest can be tracked through at least two cameras, enabling triangulation in 3D space. Our current system uses a pre-trained ResNet model (<xref rid="c7" ref-type="bibr">He et al., 2015</xref>) to track body parts of interest. However, considering the challenges posed by whole-body tracking, such as interference from marmosets’ fur that complicates feature detection, the adoption of cutting-edge transformer networks like the vision transformer model (<xref rid="c14" ref-type="bibr">Dosovitskiy et al., 2020</xref>) might significantly improve detection performance. Such an advancement in tracking and reconstructing the entire marmoset body frame would enable the analysis of such data using unsupervised learning techniques (<xref ref-type="bibr" rid="c2">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="c5">Calhoun et al., 2019</xref>) and thereby provide a deeper understanding of primate social behavior.</p>
<p>In summary, our study lays the groundwork for a rigorous quantification of primate behaviors in naturalistic settings. Not only does this allow us to gain deeper insights beyond what is possible from field notes and observational studies, but it is also a key first step to go beyond current reductionist paradigms and understanding the neural dynamics underlying natural behaviors (<xref ref-type="bibr" rid="c29">Miller et al., 2022</xref>).</p>
</sec>

</body>
<back>
<sec id="s7">
<title>Methods</title>
<sec id="s7a">
<title>Camera calibration</title>
<p>All cameras (GoPro 8) were calibrated using an 8-by-9 black-white checkerboard. For each camera, the checkerboard was placed at various locations to sample the space of the camera’s field of view. To achieve better calibration performance, the checkerboard was tilted and rotated to varying degrees thus producing a range of different views (Zhang, 2000). The corners of the checkerboard were automatically detected via a standard algorithm (<monospace>detectCheckerboardPoints()</monospace> function in the Image Processing and Computer Vision toolbox in MATLAB). The intrinsic parameters of each camera were estimated based on the data obtained from the checkerboard corner detection algorithm (<monospace>estimateCameraParameters()</monospace> function in Image Processing and Computer Vision toolbox in MATLAB).</p>
</sec>
<sec id="s7b">
<title>L-frame analysis</title>
<p>L-shaped frames were used to obtain the extrinsic parameters of the cameras, the rotation matrix, and the transition vector (Timothy et al., 2021). The L-shaped frame was captured by four cameras that recorded one arena. Four points that were unevenly distributed on the L-shaped frame were manually labeled. The information of transformation from world coordinates to camera coordinates was then extracted based on the labeled result (<monospace>cameraPoseToExtrinsics()</monospace> function in Image Processing and Computer Vision toolbox in MATLAB).</p>
</sec>
<sec id="s7c">
<title>Camera recording</title>
<p>GoPro 8 cameras were used and were simultaneously controlled via a Bluetooth remote control (The Remote by GoPro). Videos were recorded at 30 frames/sec with a linear lens. Frame resolution was set at 1920×1080 pixels. A circular polarizer filter was used to mitigate reflection artifacts.</p>
</sec>
<sec id="s7d">
<title>Deep convolutional neural network (DCNN) model training</title>
<p>We used a modified version of DeepLabCut (<xref ref-type="bibr" rid="c26">Mathis et al., 2018</xref>) to perform automated markerless tracking of body parts of interest from two marmosets. The model was trained on 700 hand-labeled image frames extracted from videos of animals in their colony settings. Each image frame was labeled with six facial points: the two tufts, the central blaze, two eyes, and the mouth. The model was trained using GPUs on a large computing cluster for 250,000 iterations until the loss reached a plateau.</p>
</sec>
<sec id="s7e">
<title>Method validation in both 2D and 3D spaces</title>
<p>To validate the tracking method in 2D space, we manually labeled 200 continuous frames containing two marmosets from the recorded videos. The model was then provided with these 200 frames to detect the body parts. We calculated the differences between the ground-truth coordinates and the detected body parts using the root mean square error (RMSE):
<disp-formula id="ueqn1">
<graphic xlink:href="580693v2_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>x</italic><sub><italic>i</italic></sub> is the ground-truth coordinate <inline-formula><inline-graphic xlink:href="580693v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is the detected coordinate, <italic>N</italic> is the total number of body parts.</p>
<p>It is important to note that in cases where certain body parts were occluded in the frames, their coordinates were labeled as (0, 0).</p>
<p>To test the method in 3D space, we manually labeled 200 consecutive pairs of frames containing two marmosets, extracted from two simultaneously recording cameras. The coordinates were triangulated into 3D space. We then fed the model with these 200 pairs of frames to obtain the detected body part coordinates in 3D. The differences between the ground-truth coordinates and the detected body parts were calculated using the same RMSE metric.</p>
</sec>
<sec id="s7f">
<title>Gaze cone calculation</title>
<p>At each time frame, the gaze orientation was calculated as the normal to the facial plane (‘face norm’) defined by the two eyes and the central blaze. The position of the ear tufts, which were behind this facial plane, was used to determine the direction of gaze. A gaze cone was defined as a virtual cone of 10-degree solid angle around this norm.</p>
</sec>
<sec id="s7g">
<title>Head gaze velocity calculation and stable epoch identification</title>
<p>We used the change of the norm over consecutive time frames to calculate the head gaze velocity:
<disp-formula id="ueqn2">
<graphic xlink:href="580693v2_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where v(t) is the velocity at time point t, N(t) is the norm at time point t.</p>
<p>We remove all time points where the head gaze velocity was larger than 0.05 in normalized units. Segments no shorter than three consecutive time frames were identified as stable epochs.</p>
</sec>
<sec id="s7h">
<title>Cone-monitor plane intersection</title>
<p>We modified an existing method (Calinon &amp; Billard, 2006; Sylvain, 2009) to determine the elliptical intersection of a gaze cone and the finite plane defined by the monitor.</p>
</sec>
<sec id="s7i">
<title>Cone-facial plane intersection</title>
<p>We used a numerical method to determine whether the gaze cone of one animal intersected with the facial plane of the other. The facial plane was defined as the finite triangular plane formed by three facial features: two eyes and mouth. Any point X in 3D within the volume bounded by the cone satisfies the inequality:
<disp-formula id="ueqn3">
<graphic xlink:href="580693v2_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>θ</italic> is the solid angle of the gaze cone, <italic>coneDir</italic> is the direction vector of the gaze cone, <italic>coneOrg</italic> is the origin point of the gaze cone. The facial plane intersects with the cone if any point within the finite plane satisfies the inequality.</p>
</sec>
<sec id="s7j">
<title>Cone-cone intersection</title>
<p>To calculate the cone-cone intersection, we used the same numerical method as above. If any point X in 3D simultaneously satisfied the following inequalities:
<disp-formula id="ueqn4">
<graphic xlink:href="580693v2_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and
<disp-formula id="ueqn5">
<graphic xlink:href="580693v2_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
then the two cones were considered to be intersected. Subscripts in the above inequalities indicate the parameters of the two gaze cones under consideration.</p>
</sec>
<sec id="s7k">
<title>Maximum likelihood estimation</title>
<p>We used the maximum likelihood estimation method (<monospace>mle()</monospace> function in the Statistics and Machine Learning Toolbox in MATLAB) to fit a mixture of Gamma and Gaussian distributions.</p>
</sec>
<sec id="s7l">
<title>Markov chain analysis</title>
<p>State transition matrices were obtained based on the behavioral data. These matrices were then used to generate the discrete Markov chains (<monospace>dtmc()</monospace> function in Econometrics Toolbox in MATLAB) and plotted (<monospace>graphplot()</monospace> function in MATLAB).</p>
</sec>
<sec id="s7m">
<title>Warren Sarle’s bimodality coefficient</title>
<p>Sarle’s bimodality coefficient is used to test for bimodality. The coefficient is calculated using the MATLAB function written by Hristo Zhivomirov (2024). The code is based on the theory in <xref ref-type="bibr" rid="c37">Pfister et al., 2013</xref>.</p>
</sec>
</sec>
<sec id="s8">
<title>Experimental model and subject details</title>
<sec id="s8a">
<title>Animals</title>
<p>Nine adult marmosets were used in this study (four males, five females)[Add info about age ranges of each sex]. Four familiar male/female pairs were each from the same cage. Four unfamiliar male/female pairs were selected from the nine animals such that each member of a pair were from different home cages and did not have visual access to each other while in the colony. Animals were kept in a colony maintained at around 75°F, 60% humidity and a 12h:12h light-dark cycle.</p>
</sec>
<sec id="s8b">
<title>Single marmoset gazing at the monitor</title>
<p>A single freely moving marmoset was recorded by four cameras surrounding the arena. Video or image stimuli were displayed at one of five locations (Center, Up, Down, Left and Right) on the monitor (location chosen randomly). Each session contained only one stimulus category (either video or image) and consisted of five blocks. Each block consisted of ten five-second stimuli interleaved with ten five-second breaks. Each block started with a white dot in the center of the screen on a black background lasting for one second. At the end of the block, a juice reward (diluted condensed milk, condensed milk : water = 1:7) was delivered with a syringe pump system (NE-500 programmable OEM syringe pump from Pump Systems Inc.) along with an auditory cue.</p>
</sec>
<sec id="s8c">
<title>Freely interacting marmoset dyads</title>
<p>Two freely moving marmosets, in separate arenas, were recorded by two sets of four cameras surrounding the arenas. Each session consisted of ten five-minute free-viewing blocks interleaved with nine five-minute breaks. A juice reward (diluted condensed milk, condensed milk : water = 1:7) was delivered every minute through two syringe pump systems during the free-viewing blocks. During the breaks, a divider was placed between the two arenas that prevented the marmosets from seeing each other.</p>
</sec>
</sec>
<sec id="s9">
<title>Supplementary Material</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplementary Figure 1.</label>
<caption><title>Experimental setup and reconstruction in 3D space.</title>
<p>(A) Two transparent acrylic arenas allowed marmosets to visually interact with each other. An opaque divider between the arenas was introduced intermittently to prevent visual access between animals. Two monitors on two ends were used to display video or image stimuli. Eight cameras surrounding the arenas ensured full coverage of both animals. LEDs at four positions were used to synchronize the video recordings across the set of cameras. (B) 3D reconstruction of the experimental setup. The two arenas are color-coded as orange and cyan. The cameras colored the same as the arenas indicate that they primarily record the marmoset in the corresponding arena. Two purple L-frames within the arenas were used to establish a world coordinate system in the reconstruction process. Two gray planes on both ends are the reconstructed monitors.</p></caption>
<graphic xlink:href="580693v2_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplementary Figure 2.</label>
<caption><title>Gaze behavior analysis of a single marmoset viewing stimuli on the monitor.</title>
<p>(A) Gaze duration for video stimuli is significantly higher compared to image stimuli (Mann Whitney U Test, p &lt; 0.001). (B) Left, Gaze dispersion is defined as the average distance between the centers of the intersection of the gaze cone and the monitor within a gaze epoch. There was no difference between the video and image stimuli for gaze dispersion (Mann Whitney U Test, ns). (C) Left, Illustration of four gaze epochs (gray bars) to repeated presentations of a stimulus and the gaze counts at different time points within the duration of the presentation. Marmosets have more gazes in the early period than the late period for the video stimuli (Mann Whitney U Test, p &lt; 0.001), however, this is not the case for the image stimuli (Mann Whitney U Test, ns). During the early period, marmosets had significantly higher gaze counts for the video stimuli than the image stimuli (Mann Whitney U Test, p &lt; 10^-10).</p></caption>
<graphic xlink:href="580693v2_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This research was supported by the National Institute of Mental Health (R21 120672, SWCC, ASN, MPJ), Simons Foundation Autism Research Initiative (SFARI 875855, SWCC, ASN, MPJ), Yale Orthwein Scholar Funds (ASN) and by the National Eye Institute core grant for vision research (P30 EY026878 to Yale University). We would like to thank the veterinary and husbandry staff at Yale for excellent animal care. We would like to thank Weikang Shi for helpful discussion on the manuscript.</p>
</ack>
<sec id="d1e975" sec-type="additional-information">
<title>Additional information</title>
<sec id="s4">
<title>Author contributions</title>
<p>ASN, SWCC &amp; MPJ conceptualized the project. FX collected the data with assistance from AGS. FX analyzed the data. ASN supervised the project. FX, ASN, SWCC &amp; MPJ wrote the manuscript.</p>
</sec>
<sec id="s6" sec-type="ethics-statement">
<title>Inclusion and Ethics</title>
<p>We support inclusive, diverse, and equitable conduct of research.</p>
</sec>
</sec>
<sec id="suppd1e975" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e952">
<label>Supplementary Video 1</label>
<media xlink:href="supplements/580693_file02.mp4"/>
</supplementary-material>
<supplementary-material id="d1e959">
<label>Supplementary Video 2</label>
<media xlink:href="supplements/580693_file03.mp4"/>
</supplementary-material>
<supplementary-material id="d1e966">
<label>Supplementary Video 3</label>
<media xlink:href="supplements/580693_file04.mp4"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>David J.</given-names></string-name>, &amp; <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Toward a Science of Computational Ethology</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>1</issue>), <fpage>18</fpage>–<lpage>31</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.005</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berman</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Shaevitz</surname>, <given-names>J. W.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title>. <source>Journal of The Royal Society Interface</source>, <volume>11</volume>(<issue>99</issue>), <fpage>20140672</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brooks</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Meltzoff</surname>, <given-names>A. N.</given-names></string-name></person-group> (<year>2005</year>). <article-title>The development of gaze following and its relation to language</article-title>. <source>Dev Sci</source>, <volume>8</volume>(<issue>6</issue>), <fpage>535</fpage>–<lpage>543</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1467-7687.2005.00445.x</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burkart</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Heschl</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Geometrical gaze following in common marmosets (Callithrix jacchus)</article-title>. <source>J Comp Psychol</source>, <volume>120</volume>(<issue>2</issue>), <fpage>120</fpage>–<lpage>130</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0735-7036.120.2.120</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calhoun</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, &amp; <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Unsupervised identification of the internal states that shape natural behavior</article-title>. <source>Nature neuroscience</source>, <volume>22</volume>(<issue>12</issue>), <fpage>2040</fpage>–<lpage>2049</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-019-0533-x</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Simon</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wei</surname>, <given-names>S.-E.</given-names></string-name>, &amp; <string-name><surname>Sheikh</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Realtime multi-person 2d pose estimation using part affinity fields</article-title>. <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chevallier</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Parish-Morris</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>McVey</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rump</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Sasson</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Herrington</surname>, <given-names>J. D.</given-names></string-name>, &amp; <string-name><surname>Schultz</surname>, <given-names>R. T.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Measuring social attention and motivation in autism spectrum disorder using eye-tracking: Stimulus type matters</article-title>. <source>Autism Res</source>, <volume>8</volume>(<issue>5</issue>), <fpage>620</fpage>–<lpage>628</lpage>. doi:<pub-id pub-id-type="doi">10.1002/aur.1479</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dal Monte</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>V. D.</given-names></string-name>, <string-name><surname>Noble</surname>, <given-names>P. L.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>E. A.</given-names></string-name>, &amp; <string-name><surname>Averbeck</surname>, <given-names>B. B.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Amygdala lesions in rhesus macaques decrease attention to threat</article-title>. <source>Nature Communications</source>, <volume>6</volume>(<issue>1</issue>), <fpage>10161</fpage>. doi:<pub-id pub-id-type="doi">10.1038/ncomms10161</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dal Monte</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Fan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fagan</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Chu</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Putnam</surname>, <given-names>P. T.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>S. W. C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Widespread implementations of interactive social gaze neurons in the primate prefrontal-amygdala networks</article-title>. <source>Neuron</source>, <volume>110</volume>(<issue>13</issue>), <fpage>2183</fpage>–<lpage>2197.e2187.</lpage> doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2022.04.013</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dal Monte</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Piva</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Morris</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Chang</surname>, <given-names>S. W.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Live interaction distinctively shapes social gaze dynamics in rhesus macaques</article-title>. <source>J Neurophysiol</source>, <volume>116</volume>(<issue>4</issue>), <fpage>1626</fpage>–<lpage>1643</lpage>. doi:<pub-id pub-id-type="doi">10.1152/jn.00442.2016</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Datta</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Leifer</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Computational Neuroethology: A Call to Action</article-title>. <source>Neuron</source>, <volume>104</volume>(<issue>1</issue>), <fpage>11</fpage>–<lpage>24</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.038</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deen</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Schwiedrzik</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Sliwa</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Specialized Networks for Social Cognition in the Primate Brain</article-title>. <source>Annual Review of Neuroscience</source>, <volume>46</volume>(<issue>1</issue>), <fpage>381</fpage>–<lpage>401</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-neuro-102522-121410</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Digby</surname>, <given-names>L. J.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Social organization in a wild population ofCallithrix jacchus: II</article-title>. <source>Intragroup social behavior. Primates</source>, <volume>36</volume>(<issue>3</issue>), <fpage>361</fpage>–<lpage>375</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF02382859</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Dosovitskiy</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>An image is worth 16×16 words: Transformers for image recognition at scale</article-title>. <source>arXiv</source> preprint arXiv:2010.11929.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dunn</surname>, <given-names>T. W.</given-names></string-name>, <string-name><surname>Marshall</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Severson</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Aldarondo</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Hildebrand</surname>, <given-names>D. G. C.</given-names></string-name>, <string-name><surname>Chettih</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>W. L.</given-names></string-name>, <string-name><surname>Gellis</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Carlson</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Aronov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Ölveczky</surname>, <given-names>B. P.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Geometric deep learning enables 3D kinematic profiling across species and environments</article-title>. <source>Nature methods</source>, <volume>18</volume>(<issue>5</issue>), <fpage>564</fpage>–<lpage>573</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01106-6</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Emery</surname>, <given-names>N. J.</given-names></string-name></person-group> (<year>2000</year>). <article-title>The eyes have it: the neuroethology, function and evolution of social gaze</article-title>. <source>Neurosci Biobehav Rev</source>, <volume>24</volume>(<issue>6</issue>), <fpage>581</fpage>–<lpage>604</lpage>. doi:<pub-id pub-id-type="doi">10.1016/s0149-7634(00)00025-7</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Emery</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Lorincz</surname>, <given-names>E. N.</given-names></string-name>, <string-name><surname>Perrett</surname>, <given-names>D. I.</given-names></string-name>, <string-name><surname>Oram</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Gaze following and joint attention in rhesus monkeys (Macaca mulatta)</article-title>. <source>J Comp Psychol</source>, <volume>111</volume>(<issue>3</issue>), <fpage>286</fpage>–<lpage>293</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0735-7036.111.3.286</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>French</surname>, <given-names>J. A.</given-names></string-name></person-group> (<year>1997</year>). <chapter-title>Proximate regulation of singular breeding in callitrichid primates</chapter-title>. <person-group person-group-type="editor"><string-name><surname>Solomon</surname>, <given-names>N. G.</given-names></string-name>, <string-name><surname>French</surname>, <given-names>J. A.</given-names></string-name></person-group> <source>Cooperative breeding in mammals</source>, <fpage>34</fpage>–<lpage>75</lpage>. <publisher-name>Cambridge Universty Press</publisher-name></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Furl</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hadj-Bouziane</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Averbeck</surname>, <given-names>B. B.</given-names></string-name>, &amp; <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Dynamic and static facial expressions decoded from motion-sensitive areas in the macaque monkey</article-title>. <source>J Neurosci</source>, <volume>32</volume>(<issue>45</issue>), <fpage>15952</fpage>–<lpage>15962</lpage>. doi:<pub-id pub-id-type="doi">10.1523/jneurosci.1992-12.2012</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hari</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Henriksson</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Malinen</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Parkkonen</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Centrality of Social Interaction in Human Brain Function</article-title>. <source>Neuron</source>, <volume>88</volume>(<issue>1</issue>), <fpage>181</fpage>–<lpage>193</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.022</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Deep residual learning for image recognition</article-title>. <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name> (pp. <fpage>770</fpage>–<lpage>778</lpage>).</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heiney</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Blazquez</surname>, <given-names>P. M.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Behavioral responses of trained squirrel and rhesus monkeys during oculomotor tasks</article-title>. <source>Exp Brain Res</source>, <volume>212</volume>(<issue>3</issue>), <fpage>409</fpage>–<lpage>416</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s00221-011-2746-4</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hesse</surname>, <given-names>J. K.</given-names></string-name>, &amp; <string-name><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A new no-report paradigm reveals that face cells encode both consciously perceived and suppressed stimuli</article-title>. <source>eLife</source>, <volume>9</volume>, <elocation-id>e58360</elocation-id>. doi:<pub-id pub-id-type="doi">10.7554/eLife.58360</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Itier</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Alain</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sedore</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>McIntosh</surname>, <given-names>A. R.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Early face processing specificity: it’s in the eyes!</article-title> <source>J Cogn Neurosci</source>, <volume>19</volume>(<issue>11</issue>), <fpage>1815</fpage>–<lpage>1826</lpage>. doi:<pub-id pub-id-type="doi">10.1162/jocn.2007.19.11.1815</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Santos</surname>, <given-names>L. R.</given-names></string-name></person-group> (<year>2016</year>). <article-title>What Cognitive Representations Support Primate Theory of Mind?</article-title> <source>Trends Cogn Sci</source>, <volume>20</volume>(<issue>5</issue>), <fpage>375</fpage>–<lpage>382</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2016.03.005</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mamidanna</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cury</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>V. N.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature neuroscience</source>, <volume>21</volume>(<issue>9</issue>), <fpage>1281</fpage>–<lpage>1289</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meisner Olivia</surname> <given-names>C</given-names></string-name>, <string-name><given-names>Shi</given-names> <surname>Weikang</surname></string-name>, <string-name><surname>Fagan Nicholas</surname> <given-names>A</given-names></string-name>, <string-name><given-names>Greenwood</given-names> <surname>Joel</surname></string-name>, <string-name><surname>Jadi Monika</surname> <given-names>P</given-names></string-name>, <string-name><surname>Nandy Anirvan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chang Steve</surname> <given-names>WC</given-names></string-name></person-group> (<year>2024</year>) <article-title>Development of a Marmoset Apparatus for Automated Pulling (MarmoAAP) to Study Cooperative Behaviors</article-title> <source>eLife</source> <volume>13</volume>:<elocation-id>RP97088</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.97088.2</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>Leopold</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Mitchell</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Silva</surname>, <given-names>A. C.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Marmosets: A Neuroscientific Model of Human Social Behavior</article-title>. <source>Neuron</source>, <volume>90</volume>(<issue>2</issue>), <fpage>219</fpage>–<lpage>233</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.018</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Gire</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hoke</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Huk</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Kelley</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Leopold</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Niell</surname>, <given-names>C. M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Natural behavior is the language of the brain</article-title>. <source>Curr Biol</source>, <volume>32</volume>(<issue>10</issue>), <fpage>R482</fpage>–<lpage>r493</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2022.03.031</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miss</surname>, <given-names>F. M.</given-names></string-name>, &amp; <string-name><surname>Burkart</surname>, <given-names>J. M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Corepresentation during joint action in marmoset monkeys (Callithrix jacchus)</article-title>. <source>Psychological Science</source>, <volume>29</volume>(<issue>6</issue>), <fpage>984</fpage>–<lpage>995</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mitchell</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Reynolds</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Miller</surname>, <given-names>C. T.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Active vision in marmosets: a model system for visual neuroscience</article-title>. <source>J Neurosci</source>, <volume>34</volume>(<issue>4</issue>), <fpage>1183</fpage>–<lpage>1194</lpage>. doi:<pub-id pub-id-type="doi">10.1523/jneurosci.3899-13.2014</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mosher</surname>, <given-names>C. P.</given-names></string-name>, <string-name><surname>Zimmerman</surname>, <given-names>P. E.</given-names></string-name>, &amp; <string-name><surname>Gothard</surname>, <given-names>K. M.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Neurons in the monkey amygdala detect eye contact during naturalistic social interactions</article-title>. <source>Curr Biol</source>, <volume>24</volume>(<issue>20</issue>), <fpage>2459</fpage>–<lpage>2464</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2014.08.063</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mustoe</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>A tale of two hierarchies: Hormonal and behavioral factors underlying sex differences in social dominance in cooperative breeding callitrichids</article-title>. <source>Horm Behav</source>, <volume>147</volume>, <fpage>105293</fpage>. doi:<pub-id pub-id-type="doi">10.1016/j.yhbeh.2022.105293</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ngo</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Gorman</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>De la Fuente</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Souto</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schiel</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Miller</surname>, <given-names>C. T.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Active vision during prey capture in wild marmoset monkeys</article-title>. <source>Curr Biol</source>, <volume>32</volume>(<issue>15</issue>), <fpage>3423</fpage>–<lpage>3428.e3423.</lpage> doi:<pub-id pub-id-type="doi">10.1016/j.cub.2022.06.028</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pandey</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Simhadri</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Zhou</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Rapid Head Movements in Common Marmoset Monkeys</article-title>. <source>iScience</source>, <volume>23</volume>(<issue>2</issue>), <fpage>100837</fpage>. doi:<pub-id pub-id-type="doi">10.1016/j.isci.2020.100837</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Tabris</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Matsliah</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ravindranath</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title>. <source>Nature Methods</source>, <volume>19</volume>(<issue>4</issue>), <fpage>486</fpage>–<lpage>495</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41592-022-01426-1</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pfister</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Schwarz</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Janczyk</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dale</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Freeman</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Good things peak in pairs: a note on the bimodality coefficient</article-title>. <source>Frontiers in Psychology</source>, <volume>4</volume>. doi:<pub-id pub-id-type="doi">10.3389/fpsyg.2013.00700</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramezanpour</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Thier</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Decoding of the other’s focus of attention by a temporal cortex module</article-title>. <source>Proc Natl Acad Sci U S A</source>, <volume>117</volume>(<issue>5</issue>), <fpage>2663</fpage>–<lpage>2670</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1911269117</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saxe</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2003</year>). <article-title>People thinking about thinking people. The role of the temporo-parietal junction in “theory of mind”</article-title>. <source>NeuroImage</source>, <volume>19</volume>(<issue>4</issue>), <fpage>1835</fpage>–<lpage>1842</lpage>. doi:<pub-id pub-id-type="doi">10.1016/s1053-8119(03)00230-1</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shepherd</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Following Gaze: Gaze-Following Behavior as a Window into Social Cognition</article-title>. <source>Frontiers in Integrative Neuroscience</source>, <volume>4</volume>. doi:<pub-id pub-id-type="doi">10.3389/fnint.2010.00005</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shepherd</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Deaner</surname>, <given-names>R. O.</given-names></string-name>, &amp; <string-name><surname>Platt</surname>, <given-names>M. L.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Social status gates social attention in monkeys</article-title>. <source>Curr Biol</source>, <volume>16</volume>(<issue>4</issue>), <fpage>R119</fpage>–<lpage>120</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2006.02.013</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shepherd</surname>, <given-names>S. V.</given-names></string-name>, &amp; <string-name><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Functional Networks for Social Communication in the Macaque Monkey</article-title>. <source>Neuron</source>, <volume>99</volume>(<issue>2</issue>), <fpage>413</fpage>–<lpage>420.e413.</lpage> doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.027</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Solomon</surname>, <given-names>N. G.</given-names></string-name>, &amp; <string-name><surname>French</surname>, <given-names>J. A.</given-names></string-name></person-group> (<year>1997</year>). <source>Cooperative breeding in mammals</source>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sturman</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>von Ziegler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Schläppi</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Akyol</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Privitera</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Slominski</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Bohacek</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions</article-title>. <source>Neuropsychopharmacology</source>, <volume>45</volume>(<issue>11</issue>), <fpage>1942</fpage>–<lpage>1952</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spadacenta</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Dicke</surname>, <given-names>P.W.</given-names></string-name> &amp; <string-name><surname>Thier</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Reflexive gaze following in common marmoset monkeys</article-title>. <source>Sci Rep</source> <volume>9</volume>, <fpage>15292</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-019-51783-9</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spadacenta</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Dicke</surname>, <given-names>P. W.</given-names></string-name>, &amp; <string-name><surname>Thier</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A prosocial function of head-gaze aversion and head-cocking in common marmosets</article-title>. <source>Primates</source>, <volume>63</volume>(<issue>5</issue>), <fpage>535</fpage>–<lpage>546</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10329-022-00997-z</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tomasello</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Carpenter</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Call</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Behne</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Moll</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Understanding and sharing intentions: the origins of cultural cognition</article-title>. <source>The Behavioral and brain sciences</source>, <volume>28</volume>(<issue>5</issue>), <fpage>675</fpage>–<lpage>735</lpage>. <pub-id pub-id-type="doi">10.1017/S0140525X05000129</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamamoto</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Araujo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Arruda</surname>, <given-names>M. d. F.</given-names></string-name>, <string-name><surname>Lima</surname>, <given-names>A. K. M.</given-names></string-name>, <string-name><surname>Siqueira</surname>, <given-names>J. d. O.</given-names></string-name>, &amp; <string-name><surname>Hattori</surname>, <given-names>W. T.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Male and female breeding strategies in a cooperative primate</article-title>. <source>Behavioural Processes</source>, <volume>109</volume>, <fpage>27</fpage>–<lpage>33</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.beproc.2014.06.009</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105034.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Takahashi</surname>
<given-names>Daniel Y</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Federal University of Rio Grande do Norte</institution>
</institution-wrap>
<city>Natal</city>
<country>Brazil</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study establishes the methodology (machine vision and gaze pose estimation) and behavioral apparatus for examining social interactions between pairs of marmoset monkeys. Their results enable unrestrained social interactions under more rigorous conditions with detailed quantification of position and gaze. It has been difficult to study social interactions using artificial stimuli, as opposed to genuine interactions between unrestrained animals. This study makes an <bold>important</bold> contribution to studying social neuroscience within a laboratory setting; the approach is novel and well-executed, backed by <bold>convincing</bold> evidence.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105034.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The current study by Xing et al. establishes the methodology (machine vision and gaze pose estimation) and behavioral apparatus for examining social interactions between pairs of marmoset monkeys. Their results enable unrestrained social interactions under more rigorous conditions with detailed quantification of position and gaze. It has been difficult to study social interactions using artificial stimuli, as opposed to genuine interactions between unrestrained animals. This study makes an important contribution for studying social neuroscience within a laboratory setting that will be valuable to the field.</p>
<p>Strengths:</p>
<p>Marmosets are an ideal species for studying primate social interactions due to their prosocial behavior and the ease of group housing within laboratory environments. They also predominantly orient their gaze through head movements during social monitoring. Recent advances in machine vision pose estimation set the stage for estimating 3D gaze position in marmosets but require additional innovation beyond DeepLabCut or equivalent methods. A six-point facial frame is designed to accurately fit marmoset head gaze. A key assumption in the study is that head gaze is a reliable indicator of the marmoset's gaze direction, which will also depend on the eye position. Overall, this assumption has been well supported by recent studies in head-free marmosets. Thus the current work introduces an important methodology for leveraging machine vision to track head gaze and demonstrates its utility for use with interacting marmoset dyads as a first step in that study.</p>
<p>Weaknesses:</p>
<p>One weakness that should be easily addressed is that no data is provided to directly assess how accurate the estimated head gaze is based on calibrations of the animals, for example, when they are looking at discrete locations like faces or video on a monitor. This would be useful to get an upper bound on how accurate the 3D gaze vector is estimated to be, for planned use in other studies. Although the accuracy appears sufficient for the current results, it would be difficult to know if it could be applied in other contexts where more precision might be necessary.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105034.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The manuscript describes novel technique development and experiments to track the social gaze of marmosets. The authors used video tracking of multiple cameras in pairs of marmosets to infer head orientation and gaze and then studied gaze direction as a function of distance between animals, relationships, and social conditions/stimuli.</p>
<p>Strengths:</p>
<p>Overall the work is interesting and well done. It addresses an area of growing interest in animal social behavior, an area that has largely been dominated by research in rodents and other non-primate species. In particular, this work addresses something that is uniquely primate (perhaps not unique, but not studied much in other laboratory model organisms), which is that primates, like humans, look at each other, and this gaze is an important social cue of their interactions. As such, the presented work is an important advance and addition to the literature that will allow more sophisticated quantification of animal behaviors. I am particularly enthusiastic with how the authors approach the cone of uncertainty in gaze, which can be both due to some error in head orientation measurements as well as variable eye position.</p>
<p>Weaknesses:</p>
<p>There are a few technical points in need of clarification, both in terms of the robustness of the gaze estimate, and possible confounds by gaze to non-face targets which may have relevance but are not discussed. These are relatively minor, and more suggestions than anything else.</p>
</body>
</sub-article>
</article>