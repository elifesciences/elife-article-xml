<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">86873</article-id>
<article-id pub-id-type="doi">10.7554/eLife.86873</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.86873.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>OpenApePose: a database of annotated ape photographs for pose estimation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Desai</surname>
<given-names>Nisarg</given-names>
</name>
<email>desai054@umn.edu</email>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bala</surname>
<given-names>Praneet</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Richardson</surname>
<given-names>Rebecca</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Raper</surname>
<given-names>Jessica</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zimmermann</surname>
<given-names>Jan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hayden</surname>
<given-names>Benjamin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Neuroscience and Center for Magnetic Resonance Research, University of Minnesota</institution>, <addr-line>Minneapolis MN 55455</addr-line></aff>
<aff id="a2"><label>2</label><institution>Department of Computer Science, University of Minnesota</institution>, <addr-line>Minneapolis MN 55455</addr-line></aff>
<aff id="a3"><label>3</label><institution>Emory National Primate Research Center, Emory University</institution>, <addr-line>Atlanta GA 30329</addr-line></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kalan</surname>
<given-names>Ammie K</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Victoria</institution>
</institution-wrap>
<city>Victoria</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Perry</surname>
<given-names>George H</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Pennsylvania State University</institution>
</institution-wrap>
<city>University Park</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><bold>Corresponding author:</bold> Nisarg Desai, Department of Neuroscience, Center for Magnetic Resonance Research. <email>desai054@umn.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>30</day>
<month>11</month>
<year>2022</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2023-07-13">
<day>13</day>
<month>07</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP86873</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2023-02-16">
<day>16</day>
<month>02</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2022-11-30">
<day>30</day>
<month>11</month>
<year>2022</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2212.00741"/>
</event>
</pub-history>
<permissions>
<copyright-statement>Â© 2023, Desai et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Desai et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-86873-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Because of their close relationship with humans, non-human apes (chimpanzees, bonobos, gorillas, orangutans, and gibbons, including siamangs) are of great scientific interest. The goal of understanding their complex behavior would be greatly advanced by the ability to perform video-based pose tracking. Tracking, however, requires high-quality annotated datasets of ape photographs. Here we present <italic>OpenApePose</italic>, a new public dataset of 71,868 photographs, annotated with 16 body landmarks, of six ape species in naturalistic contexts. We show that a standard deep net (HRNet-W48) trained on ape photos can reliably track out-of-sample ape photos better than networks trained on monkeys (specifically, the <italic>OpenMonkeyPose</italic> dataset) and on humans (<italic>COCO</italic>) can. This trained network can track apes almost as well as the other networks can track their respective taxa, and models trained without one of the six ape species can track the held out species better than the monkey and human models can. Ultimately, the results of our analyses highlight the importance of large specialized databases for animal tracking systems and confirm the utility of our new ape database.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing interests</title>
<p>The authors have no competing interests to declare.</p>
</notes>
</notes>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>The ability to automatically track moving animals using video systems has been a great boon for the life sciences, including biomedicine (<xref ref-type="bibr" rid="c11">Calhoun and Murthy, 2017</xref>; <xref ref-type="bibr" rid="c42">Marshall et al., 2022</xref>; <xref ref-type="bibr" rid="c44">Mathis and Mathis, 2020</xref>; <xref ref-type="bibr" rid="c52">Periera et al., 2020</xref>). Such systems allow data collected from digital video cameras to be used to infer the positions of body landmarks such as head, hands, and feet, without the use of specialized markers. In recent years, the field has witnessed the development of sophisticated tracking systems that can track and identify behavior in species important for biological research, including humans, worms, flies, and mice (e.g., <xref ref-type="bibr" rid="c9">Bohnslav et al., 2021</xref>; <xref ref-type="bibr" rid="c12">Calhoun et al., 2019</xref>; Hsu and Yttri, 2020; <xref ref-type="bibr" rid="c40">Marques et al., 2019</xref>). This problem is more difficult for monkeys, although, even here, significant progress has been made (<xref ref-type="bibr" rid="c3">Bain et al., 2021</xref>; <xref ref-type="bibr" rid="c4">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Dunn et al., 2021</xref>; <xref ref-type="bibr" rid="c35">Labuguen et al., 2021</xref>; <xref ref-type="bibr" rid="c39">Marks et al, 2022</xref>; reviewed in <xref ref-type="bibr" rid="c22">Hayden et al., 2022</xref>).</p>
<p>In theory, species-general systems can achieve good performance with small numbers (hundreds or thousands) of hand-annotated sample images. In practice, however, such systems tend to be of limited functionality. That is, they may show brittle performance, and may tend to perform poorly in edge cases, which may wind up being quite common. In general, large and precisely annotated databases (ones with tens of thousands of images or more) may be needed as training sets to achieve robust performance. The monkey tracking in our monkey-specific system (<italic>OpenMonkeyStudio</italic>), for example, required over 100,000 annotated images, and performance continued to improve even at larger numbers of images in the training set (<xref ref-type="bibr" rid="c4">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>).</p>
<p>However, there is no currently publicly available database specifically for non-human apes, which in turn means that readily usable tracking solutions specific to apes do not exist. Although there is hope that models built on related species, such as humans and/or monkeys may generalize to apes, transfer methods remain a work in progress (<xref ref-type="bibr" rid="c56">Sanakoyeu et al., 2020</xref>). Like monkeys, apes are particularly challenging to track due to their homogeneous body texture and exponentially large number of pose configurations (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>). We recently developed a novel system for tracking the pose of monkeys (<xref ref-type="bibr" rid="c4">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="c5">Bala et al., 2021</xref>; <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>). A critical ingredient of this system was the collection of high-quality annotated images of monkeys, which were used as raw material for training the model. Indeed, the need for high-quality training datasets is a major barrier to progress for much of machine learning (<xref ref-type="bibr" rid="c16">Deng et al., 2009</xref>). Obtaining a database of annotated ape photographs is especially difficult due to apesâ relative rarity in captive settings and due to the proprietary oversight common among primatologists.</p>
<p>The lack of such tracking systems represents a critical gap due to the importance of apes in science. The ape (<italic>Hominoidea</italic>) superfamily includes the great apes (among them, humans, <italic>Hominidae</italic> family) and the lesser apes, gibbons and siamangs (<italic>Hylobatidae</italic> family). These species, which represent humansâ closest relatives in the animal kingdom, have complex social and foraging behavior, a high level of intelligence, and a behavioral repertoire characterized by flexibility and creativity (<xref ref-type="bibr" rid="c57">Smuts et al., 2008</xref>; <xref ref-type="bibr" rid="c58">Strier, 2016</xref>). The ability to perform sophisticated video tracking of apes would bring great benefits to primatology and comparative psychology, as well as to related fields like anthropology and kinesiology (<xref ref-type="bibr" rid="c22">Hayden et al., 2022</xref>). Moreover, tracking systems could be deployed to improve ape welfare and to supplement <italic>in-situ</italic> conservation efforts (<xref ref-type="bibr" rid="c33">Knaebe et al., 2022</xref>).</p>
<p>Here we provide a dataset of annotated ape photographs, which we call <italic>OpenApePose</italic>. This dataset includes four species from the <italic>Hominidae</italic> family: bonobos, chimpanzees, gorillas, orangutans, and several species from the <italic>Hylobatidae</italic> family<italic>,</italic> pooled into two categories of gibbons and siamangs. This dataset consists primarily of photographs taken at zoos, and also includes images from online sources, including publicly available photographs and videos. Our database is designed to have a rich sampling of poses and backgrounds, as well as a range of image features. We provide high precision annotation of sixteen body landmarks. We show that tracking models built using this database do a good job tracking from a large sample of ape images, and do a better job than networks trained with monkey (<italic>OpenMonkeyPose</italic>, <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>) or human (<italic>COCO</italic>, <xref ref-type="bibr" rid="c37">Lin et al., 2014</xref>) databases. We also show that tracking quality is comparable to these two databases tracking their own species (although performance lags slightly behind both). We believe this database will provide an important resource for future investigations of ape behavior.</p>
</sec>
<sec id="s2" sec-type="results">
<title>Results</title>
<sec id="s2a">
<title>OpenApePose dataset</title>
<p>We collected several hundred thousand images of five species of apes: chimpanzee, bonobo, gorilla, orangutan, siamang, and a sixth category, including non-siamang gibbons (<bold><xref ref-type="fig" rid="fig1">Figure 1</xref></bold>). Images were collected from zoos, sanctuaries, and field sites. We also added the ape images from the <italic>OpenMonkeyPose</italic> dataset (16,984 images) to our new dataset, which we call <italic>OpenApePose</italic>. Combined, our final dataset has 71,868 annotated ape images. Our image set contains 11,685 bonobos (<italic>Pan paniscus</italic>), 18,010 chimpanzees (<italic>Pan troglodytes</italic>), 12,905 gorillas (<italic>Gorilla gorilla</italic>), 12,722 orangutans (<italic>Pongo sp.</italic>), and 9274 gibbons (genus <italic>Hylobates</italic> and <italic>Nomascus</italic>) and 7,272 siamangs (<italic>Symphalangus syndactylus</italic>, <bold><xref ref-type="fig" rid="fig2">Figure 2A</xref></bold>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
    <caption><title>Sampling of annotated images in the <italic>OpenApePose</italic> dataset.</title><p>Thirty-two photographs chosen to illustrate the range of photographs available in our larger set, illustrating the variety in species, pose, and background. Each annotated photograph contains an annotation for sixteen different body landmarks (shown here with connecting lines).</p></caption>
<graphic xlink:href="2212.00741v1_fig1.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
    <caption><title>Properties of our OpenApePose database.</title><p><bold>A.</bold> Number of annotated images per different species in the OpenApePose dataset. <bold>B.</bold> Illustration of our annotations. All 16 annotated points are indicated and labeled on a gorilla image drawn from the database.</p></caption>
<graphic xlink:href="2212.00741v1_fig2.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>We manually sorted and cropped the images such that each cropped image contains the full body of at least one ape while minimizing repetitive poses to ensure a greater diversity of poses in the full dataset. We ensured that all cropped images have a resolution greater than or equal to 300Ã300 pixels. Next, we used a commercial annotation service (Hive AI) to manually annotate the 16 landmarks (we used the same system in <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>; See <bold><xref ref-type="sec" rid="s4">Methods</xref></bold>). The 16 landmarks together comprise a <italic>pose</italic> (<bold><xref ref-type="fig" rid="fig2">Figure 2B</xref></bold>).</p>
<p>We used these landmarks to infer a bounding box, defined as the distance + 20% pixels between the farthest landmarks on the two axes. Our landmarks were: (1) nose, (2-3) left and right eye, (4) head, (5) neck, (6-7) left and right shoulder, (8-9) elbows, (10-11) wrists, (12) sacrum, that is, the center-point between the two hips, (13-14) knees, and (15-16) ankles. These are the same landmarks we used in our corresponding monkey dataset (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>), although in that set we also included a landmark for the tip of the tail. (We donât include that here because apes donât have tails). Each data instance is made of: image, species, bounding box, and pose.</p>
<p>Our previous monkey-centered dataset was presented in the form of a <italic>challenge</italic> (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>). Our ape dataset, by contrast, is presented solely as a resource. The annotations and all 71,868 images are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/desai-nisarg/OpenApePose">https://github.com/desai-nisarg/OpenApePose</ext-link>.</p>
</sec>
<sec id="s2b">
<title>Overview of OpenApePose dataset</title>
<p>To illustrate the range of poses in the OpenApePose dataset, we visualize the space spanned by its poses using Uniform Manifold Approximation and Projection (UMAP, <xref ref-type="bibr" rid="c46">McInnes et al., 2018</xref>, <bold><xref ref-type="fig" rid="fig3">Figure 3</xref></bold>). To obtain standard and meaningful spatial representations, we use normalized landmark coordinates based on image sizeâthe x-coordinate normalized using image width and the y-coordinate normalized using the image height. We then center each pose to a reference root landmark (the sacrum), such that the normalized coordinate of each landmark is with respect to the sacrum landmark. We then create the UMAP visualizations by performing dimension reduction using the <monospace>UMAP()</monospace> function in the <monospace>umap-learn</monospace> Python package (<xref ref-type="bibr" rid="c46">McInnes et al., 2018</xref>). We use the euclidean distance metric with <monospace>n_neighbors=15</monospace> and <monospace>min_dist=0.001</monospace>, which allowed us a reasonable balance in combining similar poses and separating dissimilar ones.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
    <caption><title><bold>UMAP visualization of the distribution of poses with the species IDs labeled.</bold></title><p>X- and Y- dimensions indicate positions in a UMAP space. Each dot indicates a single photograph/pose. Dot colors indicate species (see inscribed legend, right). We include, as insets, example poses, with an arrow pointing to their position in the UMAP plot.</p></caption>
<graphic xlink:href="2212.00741v1_fig3.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>We label the six different species in the database to visualize their distribution in the dimensions reduced using UMAP. We observe that the <italic>Hylobatidae</italic> family (gibbons and siamangs) form somewhat separate pose clusters from the <italic>Hominidae</italic> family (bonobos, chimpanzees, gorillas, and orangutans, <bold><xref ref-type="fig" rid="fig3">Figure 3</xref></bold>). These clusters likely reflect the differences in locomotion styles between these families, <italic>Hylobatidae</italic> being true brachiators, whereas <italic>Hominidae</italic> spend more time on moving on the ground. Of the <italic>Hominidae,</italic> the orangutans spend the most time in the trees like the <italic>Hylobatidae,</italic> and this is reflected in the overlap of their poses with the <italic>Hylobatidae</italic>.</p>
</sec>
<sec id="s2c">
<title>Demonstrating the effectiveness of the OpenApePose dataset</title>
<p>We next performed an assessment of the OpenApePose dataset for pose estimation. To do this, we used a standard tracking deep net system HRNet-W48, which currently remains state-of-the-art for pose estimation (<xref ref-type="bibr" rid="c59">Sun et al., 2019</xref>). The deep high-resolution net (HRNet) architecture achieves superior performance as it works with high resolution pose representations from the get-go, as compared to conventional architectures that work with lower resolution representations and extrapolate to higher resolutions from low resolutions (<italic>ibid.</italic>). We previously showed that this system does a good job tracking monkeys with a monkey database (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>).</p>
<p>We split the benchmark dataset into training (43,120 images, 60%), validation (14,374 images, 20%), and testing (14,374 images, 20%) datasets using the <monospace>train_test_split()</monospace> function in the <monospace>scikit-learn</monospace> Python library (<xref ref-type="bibr" rid="c50">Pedregosa et. al 2011</xref>).</p>
<p>We first investigated the ability of a model trained on the ape training set to accurately predict landmarks on apes from the test set (that is, a set that contains only images that were not used in training). To evaluate the performance of the HRNet-W48 models trained on this dataset, we used a standard approach of calculating <italic>percent correct keypoints</italic> (PCK) at a given threshold (here, 0.2, see <bold><xref ref-type="sec" rid="s4">Methods</xref></bold>) and at a series of other thresholds (0.01-1, at 0.01 increments, <bold><xref ref-type="fig" rid="fig4">Figure 4A</xref></bold>). The PCK@0.2 for this model was 0.876 and the area under the curve of PCK at all thresholds (AUC) for this model was 0.897. We used a bootstrap procedure to estimate significance and compare the model performance across different datasets (see <bold><xref ref-type="sec" rid="s4">Methods</xref></bold>). To assess significance, we calculated the AUCs of 100 random test subsets of 500 images each, sampled from the original held-out test set. We used the standard deviation of the AUCs as the error bars (<bold><xref ref-type="fig" rid="fig4">Figure 4B</xref></bold>), performed pairwise t-tests on mean AUCs, and used Bonferroni-adjusted p-values to test for significance.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4</label>
<caption><p><bold>A.</bold> Keypoint detection performance of HRNet-W48 models measured using PCK values at different thresholds. <bold>Left:</bold> Models trained on the full training sets of COCO, OpenApePose (OAP), and OpenMonkeyPose (OMP), and tested on the same dataset, as well as across datasets. <bold>Right:</bold> Models trained on different sizes of the full OAP training set, and tested on the OAP testing set. <bold>B.</bold> Barplots showing the keypoint detection performance of state of the art (HRNet-W48) models as measured using percent keypoints correct at 0.2 (PCK@0.2) and area-under-the-curve (AUC) of the PCK curves at thresholds ranging from 0.01-1. Error bars: standard deviation of the performance metrics. Models are trained on different sizes of the full training set of OAP and tested on held-out OAP test sets. <bold>C.</bold> Same as 4B but: models are trained on full training sets of COCO, OAP, and OMP, and tested on the same dataset, as well as across datasets.</p></caption>
<graphic xlink:href="2212.00741v1_fig4.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>For comparison, we used a model trained on the dataset consisting of 94,550 monkeys, split into training (56,694 images, 60%), validation (18,928 images, 20%), and testing (18,928 images, 20%) to predict apes (specifically, we used OpenMonkeyPose, <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>). (Note that the original OpenMonkeyPose dataset contained some apes; for fair cross-family comparison, we are using a version of OpenMonkeyPose with the apes removed; the 94,550 number above reflects the number of monkeys alone). The monkey dataset showed poorer performance when it comes to estimating landmarks on photos of apes. Specifically, at a threshold of 0.2, the PCK was 0.584, which is lower than the analogous value for OpenApePose (PCK@0.2 = 0.876, p-adjusted &lt; 0.001). Likewise, the area under the curve was also substantially lower (0.743, compared to 0.897 for OpenApePose, p-adjusted &lt; 0.001). In other words, for tracking apes, models trained on monkey images have some value, but they are not nearly as good as models trained on apes.</p>
</sec>
<sec id="s2d">
<title>Comparison with human pose estimation</title>
<p>A long-term goal of primate pose estimation datasets such as OpenApePose and OpenMonkeyPose is to achieve performance comparable to that of human pose estimation. Hence, as a further comparison, we used a previously published standard model trained on the dataset consisting of 262,465 humans (<italic>COCO</italic>) to predict apes (<xref ref-type="bibr" rid="c37">Lin et al., 2014</xref>). This dataset showed poorer performance at predicting landmarks on apes than the model trained on the OAP dataset. Specifically, the PCK@0.2 value of 0.569 was lower than the PCK@0.2 value of 0.876 for OAP (p-adjusted &lt; 0.001) and the AUC value of 0.710 was lower than the AUC value of 0.897 for OAP (p-adjusted &lt; 0.001).</p>
<p>COCO was worse at pose estimation for apes than the OpenMonkeyPose dataset was (PCK@0.2: 0.569 vs. 0.584, p-adjusted &lt; 0.001; and AUC: 0.710 vs. 0.743, p-adjusted &lt; 0.001), despite the fact that it is a much larger dataset (262,465 vs. 56,694 images). Moreover, humans are, biologically speaking, apes, so one may expect the COCO dataset to have an advantage on ape tracking over a monkey dataset such as OMP. This does not appear to be the case. However, it is interesting to note that the COCO model predicts landmarks on apes better than it predicts landmarks on monkeys (PCK@0.2 values: 0.568 vs 0.332, p-adjusted &lt; 0.001; AUC values: 0.710 vs 0.578, p-adjusted &lt; 0.001). This advantage, at least, does recapitulate phylogeny.</p>
<p>While the OpenApePose-trained model predicted apes at an AUC value of 0.897, the OpenMonkeyPose dataset predicted monkeys at an AUC value of 0.929. These values are close, but significantly different (p-adjusted &lt; 0.001). We surmise that the superior performance of OpenMonkeyPose dataset may be due to the diversity of species, and to its larger size. Finally, the model based on the COCO dataset predicted human poses even better still, at an AUC value of 0.956, than either the OMP or OAP within group predictions. This advantage presumably reflects, among other things, the larger size of the dataset.</p>
</sec>
<sec id="s2e">
<title>How big does an ape tracking dataset need to be?</title>
<p>We next assessed the performance of our ape dataset at different sizes (<bold><xref ref-type="fig" rid="fig4">Figure 4B</xref></bold>). To do so, we used a decimation procedure in which we assessed the performance of the dataset after randomly removing different numbers of images. Specifically, we subsampled our OpenApePose dataset at a range of sizes (10%, 30%, 50%, 70%, and 90% of the full training set size). Note that our subsampling procedure was randomized to balance across different species. We then tested each of the resulting models on our independent test set.</p>
<p>We found a gradual increase in performance with training set size. Specifically, the performance at 30% was greater than the performance at 10% (PCK@0.2: 0.747 vs. 0.617 and AUC: 0.824 vs. 0.755). Likewise, the performance at 50% was greater than the performance at 30% (PCK@0.2: 0.776 vs. 0.747 and AUC: 0.842 vs. 0.824), performance at 70% was greater than the performance at 50% (PCK@0.2: 0.878 vs. 0.776 and AUC: 0.899 vs. 0.842), and the performance at 90% was comparable to 70% (PCK@0.2: 0.886 vs. 0.878, AUC: 0.903 vs. 0.899), although it too was significantly greater (p-adjusted &lt;0.001 for all comparisons above). However, the performance at 100% was not significantly greater than the performance at 70% (PCK@0.2: 0.876 vs. 0.878, and AUC: 0.897 vs. 0.899, p-adjusted &gt; 0.9 for both). These results suggest that performance begins to saturate at around 70% size and that increasingly larger sets may not provide additional improvement in tracking and might lead to overfitting.</p>
<p>Interestingly, a similar pattern is observed when tracking monkey poses. While the Convolutional Pose Machines (CPM) models trained on different sizes of the OpenMonkeyPose training sets continue to show improvements as the training set size increases (See <bold>Figure 9A</bold> in <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>), the HRNet-W48 models show similar saturation beyond 80% training set size (<bold>Figure 9B</bold> in <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>), just like we observed in the OpenApePose models (see above). (Note that, for OpenMonkeyPose, the HRNet-W48 model performed better across the board, which is why we prefer it to the CPM approach here). This difference between the two model classes points towards the arms race between dataset size and algorithmic development as the limiting factors for performance. Ultimately, for OpenApePose, future algorithmic developments may facilitate greater performance than increasing the dataset size beyond the number we offer here.</p>
</sec>
<sec id="s2f">
<title>What is the hardest ape species to track?</title>
<p>Finally, we assessed the performance of the model on each species of ape separately. We regenerated the OpenApePose model six times, each time with all images of one of the six taxonomic groups removed. We then tested the models on the images of that group in the OAP test set. Note that this procedure has a second benefit, which is that it automatically ensures that any similar images (such as those collected in the same zoo enclosure or of the same individual) are excluded, and therefore reduces the chance of overfitting artifacts. (However, as we show below, doing this does not markedly reduce performance, suggesting that this type of overfitting is not a major issue in our analyses presented above).</p>
<p>We include a plot including the performance of each of these models on all different species in the supplementary materials (<bold><xref ref-type="fig" rid="fig6">Figure S1</xref></bold>). We also include in the plot the performance of the OpenMonkeyPose model on the species excluded from the OpenApePose dataset. We observe that the OpenApePose model with a specific species removed still performs better on that species than the OpenMonkeyPose model (<bold><xref ref-type="fig" rid="fig6">Figure S1</xref></bold>). Here, we include a plot with performance of the full OpenApePose model on different species, performance of the models with one species removed at a time on that species, and of the OpenMonkeyPose model without apes on each of the species (<bold><xref ref-type="fig" rid="fig5">Figure 5A-C</xref></bold>). Not surprisingly, we find that all the models excluding a species perform worse than the full model on the same species (<bold><xref ref-type="fig" rid="fig5">Figure 5AB</xref></bold>; PCK@0.2 and AUC for <bold>Bonobos:</bold> 0.871 vs. 0.881 and 0.896 vs. 0.903; <bold>Chimpanzees:</bold> 0.754 vs. 0.882 and 0.836 vs. 0.902; <bold>Gibbons:</bold> 0.763 vs. 0.855 and 0.827 vs. 0.883; <bold>Gorillas:</bold> 0.869 vs. 0.893 and 0.896 vs. 0.908; <bold>Orangutans:</bold> 0.774 vs. 0.859 and 0.839 vs. 0.886; <bold>Siamangs:</bold> 0.797 vs. 0.869 and 0.848 vs. 0.889; p-adjusted &lt; 0.001 for all comparisons). This result suggests that there is indeed some species-specific information in the model that aids in tracking, and raises the possibility that larger sets devoted to a single species may be superior to our more general multi-species dataset. At the same time, this finding highlights a major finding of this project - that, given current models, large tailored species-specific annotated sets are superior to large multi-species sets. In other words, current models have limited capacity of generalizing across species, even within taxonomic families.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
    <caption><title>Keypoint detection performance of HRNet-W48 models tested on each species from the OpenApePose (OAP) test set and trained on (A) the full OAP training set, (B) the OAP training set with the corresponding species excluded, and (C) the full OpenMonkeyPose (OMP) dataset with apes excluded.</title><p><bold>Left</bold> panel includes the probability of correct keypoint (PCK) values at different thresholds ranging from 0-1. <bold>Middle</bold> panel indicates the mean area under the PCK curve for each species. <bold>Right</bold> panel indicates the mean PCK values at a threshold of 0.2 for each species.</p></caption>
<graphic xlink:href="2212.00741v1_fig5.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
<p>Comparing the different species, we find that the species are all very close in performance (<bold><xref ref-type="fig" rid="fig5">Figure 5B</xref></bold>). Among these close values, the dataset missing gorillas was the most accurate - suggesting that gorillas are the least difficult to track, perhaps because their bodies are the least variable (PCK@0.2: 0.869; AUC: 0.896). Conversely, the dataset missing gibbons was the least accurate, suggesting that gibbons are the most difficult to track (PCK@0.2: 0.763; AUC: 0.827). This observation is consistent with our own intuitions at hand-annotating images - gibbonsâ habit of brachiation, combined with the variety of poses they exhibit, makes guessing their landmarks particularly tricky for human annotators as well. Overall, however, all ape species were relatively well tracked even when all members of their species were excluded from the dataset.</p>
<p>Note that models with one ape species removed still perform better at tracking the held-out species more accurately than the OpenMonkeyPose model on that species (<bold><xref ref-type="fig" rid="fig5">Figure 5BC</xref></bold>; PCK@0.2 and AUC for <bold>Bonobos</bold>: 0.871 vs. 0.542 and 0.896 vs. 0.727; <bold>Chimpanzees:</bold> 0.754 vs. 0.688 and 0.836 vs. 0.803; <bold>Gibbons:</bold> 0.763 vs. 0.587 and 0.827 vs. 0.730; <bold>Gorillas:</bold> 0.869 vs. 0.564 and 0.896 vs. 0.744; <bold>Orangutans:</bold> 0.774 vs. 0.529 and 0.839 vs. 0.707; <bold>Siamangs:</bold> 0.797 vs. 0.556 and 0.848 vs. 0.711; p-adjusted &lt; 0.001 for all comparisons). In other words, the close phylogenetic relationship between ape species does seem to bring about benefits in tracking.</p>
</sec>
</sec>
<sec id="s3" sec-type="discussion">
<title>Discussion</title>
<p>The ape superfamily is an especially charismatic clade, and one that has long been fascinating to both the lay public and to scientists. Here we present a new resource, a large (71,868 images) and fully annotated (16 landmarks) database of photographs of six species of non-human apes. These photographs were collected and curated with the goal of serving as a training set for machine vision learning models, especially ones designed to track apes in videos. As such, the apes in our dataset come in a range of poses; photographs are taken from a range of angles, and our photographs have a range of backgrounds. Our database can be found on our website (<ext-link ext-link-type="uri" xlink:href="https://github.com/desai-nisarg/OpenApePose">https://github.com/desai-nisarg/OpenApePose</ext-link>).</p>
<p>To test and validate our set, we made use of the HRNet architecture, specifically HRNet-W48. As opposed to architectures such as convolutional pose machines (Wei et al. 2016), hourglass (Newell et al. 2016), simple baselines (ResNet, Xiao et al. 2018), HRNet works with higher resolution feature representations, that facilitate better performance. In contrast, other systems, most famously DeepLabCut, uses ResNets, EfficientNets, and MobileNets V2 as backbones. Pose estimation studies often compare a variety of these architectures to test performance, but increasingly, studies find HRNet to outperform other architectures (<xref ref-type="bibr" rid="c69">Yu et al. 2021</xref>; <xref ref-type="bibr" rid="c30">Li et al. 2020</xref>). (Our own past work on monkey tracking finds this as well, <xref ref-type="bibr" rid="c65">Yao et al. 2022</xref>). Because our goal here is not to evaluate these systems, but rather to introduce our annotated database, we provide data only for the HRNet system.</p>
<p>With growing interest in animal detection, pose estimation and behavior classification (<xref ref-type="bibr" rid="c3">Bain et al. 2021</xref>, <xref ref-type="bibr" rid="c55">Sakib et. al. 2020</xref>, <xref ref-type="bibr" rid="c51">Pereira et al 2019</xref>, <xref ref-type="bibr" rid="c45">Mathis et al. 2021</xref>), researchers have leveraged advances in human pose estimation and have made several animal datasets publicly available. For example, there are existing datasets on tigers (n ~ 8000, <xref ref-type="bibr" rid="c30">Li et al., 2020</xref>), Cheetahs (n ~ 7500, <xref ref-type="bibr" rid="c29">Joska et al., 2021</xref>), horses (n ~ 8000, <xref ref-type="bibr" rid="c45">Mathis et al., 2021</xref>), dogs (n ~ 22,000, Briggs et al. 2020, <xref ref-type="bibr" rid="c31">Khosla et al., 2011</xref>), cows (n ~ 2000, Russello et al., 2021), five domestic animals (<xref ref-type="bibr" rid="c13">Cao et al., 2019</xref>), and fifty-four species of mammals (<xref ref-type="bibr" rid="c69">Yu et al., 2021</xref>), and there are large datasets containing millions of frames of rats enabling single and multi-animal 3D pose estimation and behavior tracking (<xref ref-type="bibr" rid="c17">Dunn et al., 2021</xref>; <xref ref-type="bibr" rid="c41">Marshall et al., 2021</xref>). Relative to these other datasets (with the exception of rat datasets), our ape dataset is much larger (n ~ 71,000). Moreover, our dataset contains multiple closely related species and a wide range of backgrounds and poses. Another major strength of our dataset is that it contains many different of unique individuals, which is rare as most of such datasets include only a few unique individuals.</p>
<p>We anticipate that the main benefit of our database will be for future researchers to develop algorithms that can perform tracking of apes in photos and videos, including videos collected in field sites. Relative to simpler animals like worms and mice, primates are highly complex and have a great deal more variety in their poses. As such, in the absence of better deep learning techniques, the best way to come up with generalizable models is to have large and variegated datasets for each animal type of interest. Our results here indicate that even monkeys and apes - which are in the same order and have superficially similar body shapes and movements - are sufficiently different that monkey photos donât work as well for ape pose tracking. Likewise, despite the remarkable growth of human tracking systems, these systems do not readily generalize to apes, despite our close phylogenetic similarity to them. While there is growing interest in leveraging human-tracking systems to develop better animal-tracking systems, such systems are still in their infancy (Sanakoyeu et al., 2019, <xref ref-type="bibr" rid="c69">Yu et al. 2021</xref>, <xref ref-type="bibr" rid="c45">Mathis et al. 2021</xref>, <xref ref-type="bibr" rid="c1">ArnkÃ¦rn et al. 2022</xref>, <xref ref-type="bibr" rid="c13">Cao et al. 2019</xref>, <xref ref-type="bibr" rid="c32">Kleanthous et al. 2022</xref>, <xref ref-type="bibr" rid="c6">Bethell et al. 2022</xref>). At the same time, there are better and more usable general pose estimation systems for animals, such as DeepLabCut (<xref ref-type="bibr" rid="c43">Mathis et al. 2018</xref>), SLEAP (<xref ref-type="bibr" rid="c53">Pereira et al. 2022</xref>), LEAP (<xref ref-type="bibr" rid="c51">Pereira et al. 2019</xref>), DeepPoseKit (Graving et al. 2019) that allow pose estimation with small numbers (thousands) of images. These poses can be combined with downstream analysis algorithms and software tools such as MoSeq (<xref ref-type="bibr" rid="c64">Wiltschko et al. 2020</xref>), SimBA (<xref ref-type="bibr" rid="c48">Nilsson et al. 2020</xref>), and B-SOiD (<xref ref-type="bibr" rid="c28">Hsu et al. 2021</xref>) for behavior tracking. However, it is clear that such systems can benefit from much larger stimulus sets.</p>
<p>While our dataset is readily usable for training pose estimation and behavior tracking models, it has several limitations that could be addressed in the future. First, while we have attempted to include as many backgrounds, poses, and individuals as possible, our dataset is mostly dominated by images taken in captive settings at zoos and sanctuaries. This may not reflect the conditions in wild settings accurately, and may result in reduced performance for applications involving tracking apes in the wild from camera trap footage etc. Nevertheless, OpenApePose still remains the most diverse of currently available datasets. Future attempts at building such datasets should aim to include more images from the wild. Second, this dataset only enables 2D pose tracking as it does not include simultaneous multi-view images that are required for 3D pose estimation (<xref ref-type="bibr" rid="c4">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="c30">Kearney et al., 2020</xref>; <xref ref-type="bibr" rid="c17">Dunn et al., 2021</xref>; <xref ref-type="bibr" rid="c41">Marshall et al., 2021</xref>). Building a dataset that enables 3D pose estimation and has the strengths of OAP in terms of the diversity of individuals and poses would require building multiview camera setups outside of laboratories such as the one at Minnesota zoo by <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>. Third, while many images in our dataset include multiple individuals, we only have one individual labeled in each image. This limits, but does not eliminate, our ability to track multiple individuals simultaneously. Using OpenMMlab, we have had some success tracking multiple individuals using the OAP model. However, datasets with multiple individuals simultaneously will further facilitate multi-animal tracking. Lastly, our dataset does not contain high resolution tracking of finer features, such as face, hands etc. Indeed, many primatologists would be interested in systems that can track facial expression and fine hand movements (<xref ref-type="bibr" rid="c27">Hobaiter et al., 2014</xref>; <xref ref-type="bibr" rid="c26">Hobaiter et al., 2021</xref>). Because we have made our image database public, it can be used as a starting point for those researchers seeking to customize to their research goals. Indeed, it may be possible to add hand and face expression annotations to our system to serve these purposes.</p>
<p>There are several important ethical reasons why apes cannot - and should not - serve as subjects in invasive neuroscientific experiments. That does not mean, however, that we cannot draw inferences about their psychology and cognition based on careful observation of their behavior. Indeed, analysis of behavior is an important tool in neuroscience (<xref ref-type="bibr" rid="c49">Niv, 2021</xref>; <xref ref-type="bibr" rid="c34">Krakauer et al., 2017</xref>). In our previous work, we have argued for the virtues of primate tracking systems to work hand in hand with invasive neuroscience techniques to improve the reliability of neuroscientific data (<xref ref-type="bibr" rid="c22">Hayden et al., 2022</xref>). However, we have also argued that tracking has another entirely different benefit - it has the potential ability to provide data of such high quality that it can, in some cases, serve to adjudicate between hypotheses that would otherwise require brain measures (<xref ref-type="bibr" rid="c33">Knaebe et al., 2022</xref>). For this reason, tracking data has the potential to reduce the need for non-behavior neuroscientific tools and for invasive and/or stressful recording techniques. We are optimistic that better ape tracking systems will greatly expand the utility of apes in non-invasive studies of the mind and brain. We hope that our dataset will help advance such systems.</p>
</sec>
<sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Data collection</title>
<p>The OpenApePose dataset consists of 71,868 photographs of apes. We collected images between August 1, 2021 to September 2022 from zoos, sanctuaries, and internet videos. Note that a subset of these images (16,984 images from the train, validation, and test sets combined) also appeared in the OpenMonkeyPose dataset (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>). The remainder are new here.</p>
<sec id="s4a1">
<title>Zoos and sanctuaries</title>
<p>We obtained images of apes from several zoos. These include zoos in Atlanta, Chicago, Cincinnati, Columbus, Dallas, Denver, Detroit, Erie (Pennsylvania), Fort Worth, Houston, Indianapolis, Jacksonville, Kansas City, Madison, Memphis, Miami, Milwaukee, Minneapolis, Phoenix, Sacramento, San Diego, Saint Paul, San Francisco, Seattle, and Toronto, as well as sanctuaries including the Chimpanzee Conservation Center, Project Chimps, Chimp Haven, and the Ape Initiative (Des Moines). These zoo photographs were taken either by ourselves, our lab members, or by photographers hired on temporary contracts using TaskRabbit (<ext-link ext-link-type="uri" xlink:href="https://www.taskrabbit.com/">https://www.taskrabbit.com/</ext-link>) to take pictures at these zoos. Additionally, several other independent individuals contributed images: Esmay Van Strien, Jeff Whitlock, Jennifer Williams, Jodi Carrigan, Katarzyna Krolik, Lori Ellis, Mary Pohlmann, and Max Block. All photographs were carefully screened for quality and variety of poses first by a specially trained technician and then by N.D.</p>
</sec>
<sec id="s4a2">
<title>Internet sources</title>
<p>We also obtained a smaller number of images from internet sources including Facebook, Instagram, and YouTube. From YouTube videos, we took screenshots of apes exhibiting diverse poses during different behaviors. Use of photographic images from these sources is protected by Fair Use Laws and has been expressly approved by the legal office at the University of Minnesota. Specifically, our use of the images satisfies four properties of principles of Fair Use. First, our usage is transformative (a crucial part of their value is in their annotations, which improve their value to scientists); second, they were published in a public forum (YouTube or on public websites); third, we are using a small percentage of the frames in the videos (at 24 fps, we are using at most 1/24 of the frames); fourth, our usage does not reduce the market value for the images, which are, after all, freely available.</p>
</sec>
</sec>
<sec id="s4b">
<title>Landmark Annotation</title>
<p>We initially obtained hundreds of thousands of images from these sources. The majority of these images (&gt;75%) did not pass our quality checks. Specifically, they were either blurry or too small or were too similar to others or showed too much occlusion images. This process led to 52,946 images in total.</p>
<p>We used a commercial service (Hive AI) to manually annotate 16 landmarks in these images, a process similar to one we used previously (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>). We use the same set of landmarks as we did in our complementary monkey dataset, with the exception of the tip of the tail (apes donât have tails). The landmarks we used are: (1) nose, (2,3) left and right eye, (4) crown of the head, (5) nape of the neck, (6,7) left and right shoulder, (8,9) left and right elbow, (10,11) left and right wrist, (12) sacrum, or center-point between the hips, (13,14) left and right knee, (15,16) left and right foot. An example image illustrating these annotations is shown in <bold><xref ref-type="fig" rid="fig2">Figure 2A</xref></bold>. We ensured that the annotations were accurate by visualizing 5 random samples of 100 images with the annotations overlayed on the images, for each batch of 10,000 images, resulting in a total of ~2,500 inspected images. Only one of the five batches showed errors, and we sent the batch back to Hive for correction. Ape images from OpenMonkeyPose were inspected as described in <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>. We converted the annotations in a JSON format that is consistent with our previous OpenMonkeyPose dataset, and similar to other common datasets such as COCO. More details on the annotations are on the GitHub page: <ext-link ext-link-type="uri" xlink:href="https://github.com/desai-nisarg/OpenApePose">https://github.com/desai-nisarg/OpenApePose</ext-link>.</p>
</sec>
<sec id="s4c">
<title>Dataset Evaluation</title>
<p>To facilitate the evaluation of generalizability of the OpenApePose dataset, we split the full dataset into 3 sets: training (60%: 43,120 images), validation (20%: 14,374 images), and testing (20%: 14,374 images) using the <monospace>train_test_split()</monospace> function in the <monospace>scikit-learn</monospace> Python library (Pedregosa et. al 2011).</p>
<sec id="s4c1">
<title>Model training</title>
<p>To train our models, we used the pipelines and tools available in the <monospace>OpenMMlab</monospace> Python library (<xref ref-type="bibr" rid="c14">Chen et al., 2019</xref>). <monospace>OpenMMlab</monospace> includes a wide range of libraries for computer vision applications including, but not limited to, object detection, segmentation, action recognition, pose estimation etc. For our project, we used the <monospace>MMPose</monospace> package in <monospace>OpenMMlab</monospace> (<xref ref-type="bibr" rid="c47">MMPose Contributors, 2020</xref>). <monospace>MMPose</monospace> supports a range of pose estimation datasets on humans as well as many other animals, and includes pretrained models from these datasets that could be tuned for specific needs. It also provides tools for training a variety of neural network architectures from scratch on existing or new datasets.</p>
<p>In our previous work (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>), we tested different top-down neural network architectures for training pose estimation models on our OpenMonkeyPose database (<bold>Figure 9C</bold> in <xref ref-type="bibr" rid="c65">Yao et al. 2022</xref>). This included Convolutional Pose Machines (CPM), Hourglass, ResNet101, ResNet152, HRNet-W32, and HRNet-W48. We found that the best performing architecture was the deep high-resolution net, HRNet-W48 (<bold>Table 2</bold> in <xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>). As opposed to the conventional approaches where higher resolution representations are recovered from lower resolution representations, the deep high-resolution net architecture works with higher resolution representations during the whole learning process. This results in more accurate pose representations for human pose estimation as demonstrated in the original paper (<xref ref-type="bibr" rid="c59">Sun et al., 2019</xref>), and also for primate pose estimation, as we observed for our monkey datasets (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>; <xref ref-type="bibr" rid="c4">Bala et al., 2020</xref>). HRNet-W48 currently remains the best performing architecture for pose estimation and hence, for this study, we train HRNet-W48 models for comparing the performance on our proposed dataset. We trained all models for 210 epochs.</p>
</sec>
<sec id="s4c2">
<title>Other datasets tested</title>
<p>We compared the performance of the HRNet-W48 model trained on our OpenApePose dataset with the performance of the pretrained HRNet-W48 model on the COCO dataset (<xref ref-type="bibr" rid="c59">Sun et al., 2019</xref>), as well as of the HRNet-W48 model trained from scratch on our OpenMonkeyPose dataset (<xref ref-type="bibr" rid="c65">Yao et al., 2022</xref>) with apes removed. The original OpenMonkeyPose dataset included 16,984 images of apesâ10,223 in the training, 3378 in the validation, and 3383 in the testing set. Hence, for a fair comparison between HRNet-W48 models trained on monkeys vs. apes, we moved these ape images from the OpenMonkeyPose dataset to the OpenApePose dataset (we provide the annotations for the OpenApePose dataset with the ape images from OpenMonkeyPose included in OpenApePose, as well as separately to enable future replications and comparisons).</p>
<p>On these datasets we performed the following comparisons. First, we performed within-dataset performance comparisons. We compared the performance of OpenApePose in predicting the poses of apes, to the performance of OpenMonkeyPose in predicting the poses of monkeys. Second, we compare the performance of OpenApePose in predicting apes to the performance of the current state-of-the-art human pose estimation model (HRNet-W48 model trained on COCO human keypoint dataset, 2017). Third, we assess the importance of dataset size by systematically reducing the OpenApePose training set size while keeping the proportion to the species constant. We train an HRNet-W48 model from scratch on training sets 10% (4,312 images), 30% (12,936 images), 50% (21,560 images), 70% (30,184 images), and 90% (38,808 images) of the size of the full training set of 43,120 images. Lastly, to assess if the models were overfitting on the species in the OpenApePose dataset over being generalizable to non-human apes, we train six separate HRNet-W48 models from scratch each with all images from one of the six species (bonobos, chimpanzees, gibbons, gorillas, orangutans, and siamangs) excluded from the training set. We test these models on the test set images of the species excluded from the training set and compare it with the performance of the OpenMonkeyPose model on that species.</p>
</sec>
<sec id="s4c3">
<title>Performance Metrics</title>
<p>To evaluate the performance of our models, we used two metrics: (i) the probability of correct keypoint (PCK) at a threshold of 0.2, and (ii) the area under the curve of PCK thresholds ranging from 0-1 in 0.01 increments.</p>
<p>The PCK@<bold><italic>Îµ</italic></bold> is the PCK value at a given error threshold (<bold><italic>Îµ</italic></bold>), defined as: <inline-formula id="id1"><alternatives><mml:math display="inline" id="i1"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>16</mml:mn><mml:mi>I</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>I</mml:mi></mml:msubsup><mml:mrow><mml:mtext>â</mml:mtext><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>16</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mtext>â</mml:mtext><mml:mi>Î´</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mo>â¥</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>â¥</mml:mo></mml:mrow><mml:mi>W</mml:mi></mml:mfrac><mml:mo>&lt;</mml:mo><mml:mi>Îµ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math><inline-graphic xlink:href="2212.00741v1_ieqn1.jpg" mimetype="image" mime-subtype="jpeg"/></alternatives></inline-formula>, where <italic>I</italic> is the number of images, <italic>i</italic> indicates the <italic>i</italic>-th image instance, and <italic>j</italic> indicates the <italic>j</italic>-th joint, W is the width of the bounding box, and Î´(.) is the function that returns 1 for a true statement and 0 for a false statement. This formulation ensures that the error tolerance accounts for the size of the image via the size of the bounding box, e.g. for a bounding box that is 300 pixels wide, a PCK@0.2 value considers a prediction within 300Ã0.2 = 60 pixels, to be a correct prediction.</p>
<p>We calculate the PCK@<bold><italic>Îµ</italic></bold> value for <bold><italic>Îµ</italic></bold> ranging from 0 to 1 with 0.01 increments. We plot the PCK@<bold><italic>Îµ</italic></bold> values for different <bold><italic>Îµ</italic></bold> (normalized distances) and calculate the area under the curve to estimate the performance of the HRNet-W48 models.</p>
</sec>
<sec id="s4c4">
<title>Statistical significance testing</title>
<p>To perform statistical significance tests of differences in model performance for the aforementioned performance metrics, we take a bootstrap approach. We simulate 100 different test sets by randomly sampling 500 images without replacement, 100 times, from a test set of interest. We then calculate the performance metrics of PCK@0.2 and the area under the curve (AUC) of PCK@<bold><italic>Îµ</italic></bold> vs. <bold><italic>Îµ</italic></bold>; for <bold><italic>Îµ</italic></bold> â [0, 1]. This allows us to estimate the variation in the performance of the HRNet-W48 models across different test sets. We test the differences in performance using pairwise t-tests for different training and testing set combinations. We report the p-values adjusted for multiple comparisons using Bonferroni correction.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank the Hayden/Zimmermann lab for valuable discussions and help with taking photographs. We thank Kriti Rastogi and Muskan Ali for their help with ape image collection. We thank Estelle Reballand from Chimpanzee Conservation Center, Fred Rubio from Project Chimps, Adam Thompson from Zoo Atlanta, Reba Collins from Chimp Haven, and Amanda Epping and Jared Taglialatela from Ape Initiative for permissions to take photographs from these sanctuaries as well as contributing images for the dataset. This work was supported by NIH MH128177 (to JZ), P30 DA048742 (JZ, BH), MH125377 (BH), NSF 2024581 (JZ, BH) and a UMN AIRP award (JZ, BH) from the Digital Technologies Initiative (JZ, BH), from the Minnesota Institute of Robotics (JZ), and Emory National Primate Research Center, NIH Office of the Director (P51-OD011132).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>ArnkÃ¦rn</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Schoeler</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ullah</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Cheikh</surname>, <given-names>F. A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Deep learning-based multiple animal pose estimation</article-title>. <source>Electronic Imaging</source>, <volume>34</volume>, <fpage>1</fpage>â<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Azab</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Correlates of economic decisions in the dorsal and subgenual anterior cingulate cortices</article-title>. <source>European Journal of Neuroscience</source>, <volume>47</volume>(<issue>8</issue>), <fpage>979</fpage>â<lpage>993</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bain</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nagrani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schofield</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Berdugo</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bessa</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Owen</surname>, <given-names>J.</given-names></string-name>, â¦ &amp; <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Automated audiovisual behavior recognition in wild primates</article-title>. <source>Science advances</source>, <volume>7</volume>(<issue>46</issue>), <fpage>eabi4883</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bala</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Eisenreich</surname>, <given-names>B. R.</given-names></string-name>, <string-name><surname>Yoo</surname>, <given-names>S. B. M.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>H. S.</given-names></string-name>, &amp; <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio</article-title>. <source>Nature communications</source>, <volume>11</volume>(<issue>1</issue>), <fpage>1</fpage>â<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bala</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>H. S.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Self-supervised Secondary Landmark Detection via 3D Representation Learning</article-title>. <source>arXiv preprint arXiv:2110.00543</source>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bethell</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Khan</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Hussain</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A deep transfer learning model for head pose estimation in rhesus macaques during cognitive tasks: Towards a nonrestraint noninvasive 3Rs approach</article-title>. <source>Applied Animal Behaviour Science</source>, <volume>255</volume>, <fpage>105708</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blanchard</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Wolfe</surname>, <given-names>L. S.</given-names></string-name>, <string-name><surname>Vlaev</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Winston</surname>, <given-names>J. S.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Biases in preferences for sequences of outcomes in monkeys</article-title>. <source>Cognition</source>, <volume>130</volume>(<issue>3</issue>), <fpage>289</fpage>â<lpage>299</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blanchard</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Piantadosi</surname>, <given-names>S. T.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Robust mixture modeling reveals category-free selectivity in reward region neuronal ensembles</article-title>. <source>Journal of neurophysiology</source>, <volume>119</volume>(<issue>4</issue>), <fpage>1305</fpage>â<lpage>1318</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bohnslav</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Wimalasena</surname>, <given-names>N. K.</given-names></string-name>, <string-name><surname>Clausing</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>Y. Y.</given-names></string-name>, <string-name><surname>Yarmolinsky</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Cruz</surname>, <given-names>T.</given-names></string-name>, â¦ &amp; <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name></person-group> (<year>2021</year>). <article-title>DeepEthogram, a machine learning pipeline for supervised behavior classification from raw pixels</article-title>. <source>Elife</source>, <volume>10</volume>, <fpage>e63377</fpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Biggs</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Boyne</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Charles</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fitzgibbon</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Cipolla</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Who left the dogs out? 3d animal reconstruction with expectation maximization in the loop</article-title>. In <conf-name>European Conference on Computer Vision</conf-name> (pp. <fpage>195</fpage>â<lpage>211</lpage>). <publisher-name>Springer</publisher-name>, <publisher-loc>Cham</publisher-loc>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calhoun</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Quantifying behavior to solve sensorimotor transformations: advances from worms and flies</article-title>. <source>Current opinion in neurobiology</source>, <volume>46</volume>, <fpage>90</fpage>â<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calhoun</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name>, &amp; <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Unsupervised identification of the internal states that shape natural behavior</article-title>. <source>Nature neuroscience</source>, <volume>22</volume>(<issue>12</issue>), <fpage>2040</fpage>â<lpage>2049</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Cao</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Fang</surname>, <given-names>H. S.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Tai</surname>, <given-names>Y. W.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Cross-domain adaptation for animal pose estimation</article-title>. In <conf-name>Proceedings of the IEEE/CVF International Conference on Computer Vision</conf-name> (pp. <fpage>9498</fpage>â<lpage>9507</lpage>).</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>Kai</given-names></string-name>, <string-name><given-names>Wang</given-names> <surname>Jiaqi</surname></string-name>, <string-name><given-names>Pang</given-names> <surname>Jiangmiao</surname></string-name>, <string-name><given-names>Cao</given-names> <surname>Yuhang</surname></string-name>, <string-name><given-names>Xiong</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>Li</given-names> <surname>Xiaoxiao</surname></string-name>, <string-name><given-names>Sun</given-names> <surname>Shuyang</surname></string-name> <etal>et al</etal></person-group>. â<article-title>MMDetection: Open mmlab detection toolbox and benchmark</article-title>.â <source>arXiv preprint arXiv:1906.07155</source> (<year>2019</year>).</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cisek</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Neuroscience needs evolution</article-title>. <source>Philosophical Transactions of the Royal Society B</source>, <volume>377</volume>(<issue>1844</issue>), <fpage>20200518</fpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Deng</surname>, <given-names>Jia</given-names></string-name>, <string-name><given-names>Dong</given-names> <surname>Wei</surname></string-name>, <string-name><given-names>Socher</given-names> <surname>Richard</surname></string-name>, <string-name><given-names>Li</given-names> <surname>Li-Jia</surname></string-name>, <string-name><given-names>Li</given-names> <surname>Kai</surname></string-name>, and <string-name><given-names>Fei-Fei</given-names> <surname>Li</surname></string-name></person-group>. â<article-title>Imagenet: A large-scale hierarchical image database</article-title>.â In <conf-name>2009 IEEE conference on computer vision and pattern recognition</conf-name>, pp. <fpage>248</fpage>â<lpage>255</lpage>. <conf-loc>Ieee</conf-loc>, <year>2009</year>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dunn</surname>, <given-names>T. W.</given-names></string-name>, <string-name><surname>Marshall</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Severson</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Aldarondo</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Hildebrand</surname>, <given-names>D. G.</given-names></string-name>, <string-name><surname>Chettih</surname>, <given-names>S. N.</given-names></string-name>, â¦ &amp; <string-name><surname>Ãlveczky</surname>, <given-names>B. P.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Geometric deep learning enables 3D kinematic profiling across species and environments</article-title>. <source>Nature methods</source>, <volume>18</volume>(<issue>5</issue>), <fpage>564</fpage>â<lpage>573</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ebitz</surname>, <given-names>R. B.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The population doctrine in cognitive neuroscience</article-title>. <source>Neuron</source>, <volume>109</volume>(<issue>19</issue>), <fpage>3055</fpage>â<lpage>3068</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ebitz</surname>, <given-names>R. B.</given-names></string-name>, <string-name><surname>Sleezer</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Jedema</surname>, <given-names>H. P.</given-names></string-name>, <string-name><surname>Bradberry</surname>, <given-names>C. W.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Tonic exploration governs both flexibility and lapses</article-title>. <source>PLoS computational biology</source>, <volume>15</volume>(<issue>11</issue>), <fpage>e1007475</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Farashahi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Donahue</surname>, <given-names>C. H.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Soltani</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Flexible combination of reward information across primates</article-title>. <source>Nature human behaviour</source>, <volume>3</volume>(<issue>11</issue>), <fpage>1215</fpage>â<lpage>1224</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fine</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The whole prefrontal cortex is premotor cortex</article-title>. <source>Philosophical Transactions of the Royal Society B</source>, <volume>377</volume>(<issue>1844</issue>), <fpage>20200524</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>H. S.</given-names></string-name>, &amp; <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Automated pose estimation in primates</article-title>. <source>American journal of primatology</source>, <volume>84</volume>(<issue>10</issue>), <fpage>e23348</fpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, &amp; <string-name><surname>Moreno-Bote</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2018</year>). <article-title>A neuronal theory of sequential economic choice</article-title>. <source>Brain and Neuroscience Advances</source>, <volume>2</volume>, <fpage>2398212818766675</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, &amp; <string-name><surname>Niv</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The case against economic values in the orbitofrontal cortex (or anywhere else in the brain)</article-title>. <source>Behavioral Neuroscience</source>, <volume>135</volume>(<issue>2</issue>), <fpage>192</fpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heilbronner</surname>, <given-names>S. R.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Dorsal anterior cingulate cortex: a bottom-up view</article-title>. <source>Annual review of neuroscience</source>, <volume>39</volume>, <fpage>149</fpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Hobaiter</surname>, <given-names>Catherine</given-names></string-name>, <string-name><surname>Badihi</surname>, <given-names>Gal</given-names></string-name>, <string-name><surname>Daly</surname>, <given-names>Gabriela Bezerra de Melo</given-names></string-name>, <string-name><surname>Eleuteri</surname>, <given-names>Vesta</given-names></string-name>, <string-name><surname>Graham</surname>, <given-names>Kirsty Emma</given-names></string-name>, <string-name><surname>Grund</surname>, <given-names>Charlotte</given-names></string-name>, <string-name><surname>Henderson</surname>, <given-names>Matthew</given-names></string-name>, <string-name><surname>Rodrigues</surname>, <given-names>Evelina Daniela</given-names></string-name>, <string-name><surname>Safryghin</surname>, <given-names>Alexandra</given-names></string-name>, <string-name><surname>Soldati</surname>, <given-names>Adrian</given-names></string-name>, &amp; <string-name><surname>Wiltshire</surname>, <given-names>Charlotte</given-names></string-name></person-group>. (<year>2021</year>). <article-title>The Great Ape Dictionary video database (1.0.0) [Data set]</article-title>. <source>Zenodo</source>. <pub-id pub-id-type="doi">10.5281/zenodo.5600472</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hobaiter</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Byrne</surname>, <given-names>R. W.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The meanings of chimpanzee gestures</article-title>. <source>Current Biology</source>, <volume>24</volume>(<issue>14</issue>), <fpage>1596</fpage>â<lpage>1600</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hsu</surname>, <given-names>A. I.</given-names></string-name>, &amp; <string-name><surname>Yttri</surname>, <given-names>E. A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title>. <source>Nature communications</source>, <volume>12</volume>(<issue>1</issue>), <fpage>1</fpage>â<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Joska</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Clark</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Muramatsu</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Jericevich</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Nicolls</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, â¦ &amp; <string-name><surname>Patel</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2021</year>, <month>May</month>). <article-title>AcinoSet: a 3D pose estimation dataset and baseline models for Cheetahs in the wild</article-title>. In <conf-name>2021 IEEE International Conference on Robotics and Automation (ICRA)</conf-name> (pp. <fpage>13901</fpage>â<lpage>13908</lpage>). <conf-loc>IEEE</conf-loc>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Kearney</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Parsons</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>K. I.</given-names></string-name>, &amp; <string-name><surname>Cosker</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Rgbd-dog: Predicting canine pose from rgbd sensors</article-title>. In <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name> (pp. <fpage>8336</fpage>â<lpage>8345</lpage>).</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Khosla</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jayadevaprakash</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Li</surname>, <given-names>F. F.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Novel dataset for fine-grained image categorization: Stanford dogs</article-title>. In <conf-name>Proc. CVPR workshop on fine-grained visual categorization (FGVC)</conf-name> (Vol. <volume>2</volume>, No. <issue>1</issue>). <conf-loc>Citeseer</conf-loc>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleanthous</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hussain</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Khan</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Sneddon</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Liatsis</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Deep transfer learning in sheep activity recognition using accelerometer data</article-title>. <source>Expert Systems with Applications</source>, <volume>207</volume>, <fpage>117925</fpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knaebe</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Weiss</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The promise of behavioral tracking systems for advancing primate animal welfare</article-title>. <source>Animals</source>, <volume>12</volume>(<issue>13</issue>), <fpage>1648</fpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Ghazanfar</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Gomez-Marin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>MacIver</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Neuroscience needs behavior: correcting a reductionist bias</article-title>. <source>Neuron</source>, <volume>93</volume>(<issue>3</issue>), <fpage>480</fpage>â<lpage>490</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Labuguen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Matsumoto</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Negrete</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Nishimaru</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Nishijo</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Takada</surname>, <given-names>M.</given-names></string-name>, â¦ &amp; <string-name><surname>Shibata</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2021</year>). <article-title>MacaquePose: A novel âin the wildâ macaque monkey pose dataset for markerless motion capture</article-title>. <source>Frontiers in behavioral neuroscience</source>, <volume>14</volume>, <fpage>581154</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Qian</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Lin</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2019</year>). <article-title>ATRW: a benchmark for Amur tiger re-identification in the wild</article-title>. <source>arXiv preprint arXiv:1906.05586</source>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Lin</surname>, <given-names>Tsung-Yi</given-names></string-name>, <string-name><given-names>Maire</given-names> <surname>Michael</surname></string-name>, <string-name><given-names>Belongie</given-names> <surname>Serge</surname></string-name>, <string-name><given-names>Hays</given-names> <surname>James</surname></string-name>, <string-name><given-names>Perona</given-names> <surname>Pietro</surname></string-name>, <string-name><given-names>Ramanan</given-names> <surname>Deva</surname></string-name>, <string-name><given-names>DollÃ¡r</given-names> <surname>Piotr</surname></string-name>, and <string-name><given-names>C. Lawrence</given-names> <surname>Zitnick</surname></string-name></person-group>. â<article-title>Microsoft coco: Common objects in context</article-title>.â In <conf-name>European conference on computer vision</conf-name>, pp. <fpage>740</fpage>â<lpage>755</lpage>. <conf-loc>Springer, Cham</conf-loc>, <year>2014</year>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maisson</surname>, <given-names>D. J. N.</given-names></string-name>, <string-name><surname>Cash-Padgett</surname>, <given-names>T. V.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>M. Z.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, <string-name><surname>Heilbronner</surname>, <given-names>S. R.</given-names></string-name>, &amp; <string-name><surname>Zimmermann</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Choice-relevant information transformation along a ventrodorsal axis in the medial prefrontal cortex</article-title>. <source>Nature communications</source>, <volume>12</volume>(<issue>1</issue>), <fpage>1</fpage>â<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marks</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jin</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Sturman</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>von Ziegler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kollmorgen</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>von der Behrens</surname>, <given-names>W.</given-names></string-name>, â¦ &amp; <string-name><surname>Yanik</surname>, <given-names>M. F.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Deep-learning-based identification, tracking, pose estimation and behaviour classification of interacting primates and mice in complex environments</article-title>. <source>Nature Machine Intelligence</source>, <volume>4</volume>(<issue>4</issue>), <fpage>331</fpage>â<lpage>340</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marques</surname>, <given-names>J. C.</given-names></string-name>, <string-name><given-names>M.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Schaak</surname></string-name>, <string-name><given-names>D. N.</given-names> <surname>Robson</surname></string-name>, and <string-name><given-names>J. M.</given-names> <surname>Li</surname></string-name></person-group>. <article-title>Internal state dynamics shape brainwide activity and foraging behaviour</article-title>. <source>Nature</source>, <volume>577</volume>(<issue>7789</issue>):<fpage>239</fpage>â<lpage>243</lpage>, <year>2019</year>. ISSN 0028-0836. doi: <pub-id pub-id-type="doi">10.1038/s41586-019-1858-z</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Marshall</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Klibaite</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Gellis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Aldarondo</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Ãlveczky</surname>, <given-names>B. P.</given-names></string-name>, &amp; <string-name><surname>Dunn</surname>, <given-names>T. W.</given-names></string-name></person-group> (<year>2021</year>). <source>The pair-r24m dataset for multi-animal 3d pose estimation</source>. <comment>bioRxiv</comment>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marshall</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Dunn</surname>, <given-names>T. W.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Leaving flatland: Advances in 3D behavioral measurement</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>73</volume>, <fpage>102522</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mamidanna</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cury</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>V. N.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature neuroscience</source>, <volume>21</volume>(<issue>9</issue>), <fpage>1281</fpage>â<lpage>1289</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title>. <source>Current opinion in neurobiology</source>, <volume>60</volume>, <fpage>1</fpage>â<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Biasi</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Yuksekgonul</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rogers</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Pretraining boosts out-of-domain robustness for pose estimation</article-title>. In <conf-name>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</conf-name> (pp. <fpage>1859</fpage>â<lpage>1868</lpage>).</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>McInnes</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Healy</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Melville</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Umap: Uniform manifold approximation and projection for dimension reduction</article-title>. <source>arXiv preprint arXiv:1802.03426</source>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>MMPose Contributors</collab></person-group>, â<article-title>OpenMMLab Pose Estimation Toolbox and Benchmark</article-title>,â <ext-link ext-link-type="uri" xlink:href="https://github.com/open-mmlab/mmpose">https://github.com/open-mmlab/mmpose</ext-link>, <year>2020</year>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Nilsson</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Goodwin</surname>, <given-names>N. L.</given-names></string-name>, <string-name><surname>Choong</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Hwang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wright</surname>, <given-names>H. R.</given-names></string-name>, <string-name><surname>Norville</surname>, <given-names>Z. C.</given-names></string-name>, â¦ &amp; <string-name><surname>Golden</surname>, <given-names>S. A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Simple Behavioral Analysis (SimBA)âan open source toolkit for computer classification of complex social behaviors in experimental animals</article-title>. <source>BioRxiv</source>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Niv</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2021</year>). <article-title>The primacy of behavioral research for understanding the brain</article-title>. <source>Behavioral Neuroscience</source>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname>, <given-names>Fabian</given-names></string-name>, <string-name><given-names>GaÃ«l</given-names> <surname>Varoquaux</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Michel</surname></string-name>, <string-name><given-names>Bertrand</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Grisel</surname></string-name>, <string-name><given-names>Mathieu</given-names> <surname>Blondel</surname></string-name> <etal>et al</etal></person-group>. â<article-title>Scikit-learn: Machine learning in Python</article-title>.â <source>the Journal of Machine Learning Research</source> <volume>12</volume> (<issue>2011</issue>): <fpage>2825</fpage>â<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Aldarondo</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Willmore</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kislin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S. S. H.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Shaevitz</surname>, <given-names>J. W.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Fast animal pose estimation using deep neural networks</article-title>. <source>Nature methods</source>, <volume>16</volume>(<issue>1</issue>), <fpage>117</fpage>â<lpage>125</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Shaevitz</surname>, <given-names>J. W.</given-names></string-name>, &amp; <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Quantifying behavior to understand the brain</article-title>. <source>Nature neuroscience</source>, <volume>23</volume>(<issue>12</issue>), <fpage>1537</fpage>â<lpage>1549</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Tabris</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Matsliah</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ravindranath</surname>, <given-names>S.</given-names></string-name>, â¦ &amp; <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title>. <source>Nature methods</source>, <volume>19</volume>(<issue>4</issue>), <fpage>486</fpage>â<lpage>495</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russello</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>van der Tol</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kootstra</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2022</year>). <article-title>T-LEAP: Occlusion-robust pose estimation of walking cows using temporal information</article-title>. <source>Computers and Electronics in Agriculture</source>, <volume>192</volume>, <fpage>106559</fpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Sakib</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Burghardt</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Visual recognition of great ape behaviours in the wild</article-title>. <source>arXiv preprint arXiv:2011.10759</source>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Sanakoyeu</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Khalidov</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>McCarthy</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Vedaldi</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Neverova</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Transferring dense pose to proximal animal classes</article-title>. In <conf-name>Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</conf-name> (pp. <fpage>5233</fpage>â<lpage>5242</lpage>).</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><surname>Smuts</surname>, <given-names>Barbara B.</given-names></string-name>, <string-name><given-names>Dorothy L.</given-names> <surname>Cheney</surname></string-name>, <string-name><given-names>Robert M.</given-names> <surname>Seyfarth</surname></string-name>, and <string-name><given-names>Richard W.</given-names> <surname>Wrangham</surname></string-name></person-group>, eds. <source>Primate societies</source>. <publisher-name>University of Chicago Press</publisher-name>, <year>2008</year>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Strier</surname>, <given-names>K. B.</given-names></string-name></person-group> (<year>2016</year>). <source>Primate behavioral ecology</source>. <publisher-name>Routledge</publisher-name>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="conference"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Xiao</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>D.</given-names></string-name> <string-name><surname>and Wang</surname>, <given-names>J.</given-names></string-name></person-group>, <year>2019</year>. <article-title>Deep high-resolution representation learning for human pose estimation</article-title>. In <conf-name>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</conf-name> (pp. <fpage>5693</fpage>â<lpage>5703</lpage>).</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>M. Z.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Monkeys are curious about counterfactual outcomes</article-title>. <source>Cognition</source>, <volume>189</volume>, <fpage>1</fpage>â<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>M. Z.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Latent learning, cognitive maps, and curiosity</article-title>. <source>Current Opinion in Behavioral Sciences</source>, <volume>38</volume>, <fpage>1</fpage>â<lpage>7</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>M. Z.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, &amp; <string-name><surname>Heilbronner</surname>, <given-names>S. R.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A structural and functional subdivision in central orbitofrontal cortex</article-title>. <source>Nature communications</source>, <volume>13</volume>(<issue>1</issue>), <fpage>1</fpage>â<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Widge</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Heilbronner</surname>, <given-names>S. R.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Prefrontal cortex and cognitive control: new insights from human electrophysiology</article-title>. <source>F1000Research</source>, <volume>8</volume>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiltschko</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Tsukahara</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Zeine</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Anyoha</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gillis</surname>, <given-names>W. F.</given-names></string-name>, <string-name><surname>Markowitz</surname>, <given-names>J. E.</given-names></string-name>, â¦ &amp; <string-name><surname>Datta</surname>, <given-names>S. R.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title>. <source>Nature neuroscience</source>, <volume>23</volume>(<issue>11</issue>), <fpage>1433</fpage>â<lpage>1443</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bala</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mohan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bliss-Moreau</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Coleman</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Freeman</surname>, <given-names>S. M.</given-names></string-name>, â¦ &amp; <string-name><surname>Park</surname>, <given-names>H. S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>OpenMonkeyChallenge: Dataset and Benchmark Challenges for Pose Estimation of Non-human Primates</article-title>. <source>International Journal of Computer Vision</source>, <fpage>1</fpage>â<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoo</surname>, <given-names>S. B. M.</given-names></string-name>, <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name>, &amp; <string-name><surname>Pearson</surname>, <given-names>J. M.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Continuous decisions</article-title>. <source>Philosophical Transactions of the Royal Society B</source>, <volume>376</volume>(<issue>1819</issue>), <fpage>20190664</fpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoo</surname>, <given-names>S. B. M.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2020</year>). <article-title>The transition from evaluation to selection involves neural subspace reorganization in core reward regions</article-title>. <source>Neuron</source>, <volume>105</volume>(<issue>4</issue>), <fpage>712</fpage>â<lpage>724</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yoo</surname>, <given-names>S. B. M.</given-names></string-name>, <string-name><surname>Tu</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Piantadosi</surname>, <given-names>S. T.</given-names></string-name>, &amp; <string-name><surname>Hayden</surname>, <given-names>B. Y.</given-names></string-name></person-group> (<year>2020</year>). <article-title>The neural basis of predictive pursuit</article-title>. <source>Nature neuroscience</source>, <volume>23</volume>(<issue>2</issue>), <fpage>252</fpage>â<lpage>259</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Guan</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Tao</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Ap-10k: A benchmark for animal pose estimation in the wild</article-title>. <source>arXiv preprint arXiv:2108.12617</source>.</mixed-citation></ref>
</ref-list>
<sec id="d1e3770">
<title>Supplement</title>
<table-wrap id="tab1">
<label>Table S1</label>
<caption><p>Pairwise t-tests comparing the AUC values for different training and testing set combinations. AUCs were obtained across 100 random samples of 500 images from the test sets. P-values adjusted for multiple comparisons using Bonferroni correction. ** not significant</p></caption>
<alternatives>
<table frame="hsides" rules="all">
<thead>
<tr>
<th align="left">Experiment 1</th>
<th align="left">Experiment 2</th>
<th align="left">Mean 1</th>
<th align="left">Mean 2</th>
<th align="left">difference</th>
<th align="left">p.ad j</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (100%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.897</td>
<td align="left">0.031</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (90%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.903</td>
<td align="left">0.026</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (70%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.899</td>
<td align="left">0.03</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (50%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.842</td>
<td align="left">0.087</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.824</td>
<td align="left">0.105</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.755</td>
<td align="left">0.174</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.929</td>
<td align="left">0.736</td>
<td align="left">0.193</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.743</td>
<td align="left">0.186</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.929</td>
<td align="left">0.578</td>
<td align="left">0.351</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.929</td>
<td align="left">0.71</td>
<td align="left">0.219</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.929</td>
<td align="left">0.958</td>
<td align="left">-0.029</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (90%) on OAP</td>
<td align="left">0.897</td>
<td align="left">0.903</td>
<td align="left">-0.006</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (70%) on OAP</td>
<td align="left">0.897</td>
<td align="left">0.899</td>
<td align="left">-0.001</td>
<td align="left">1**</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (50%) on OAP</td>
<td align="left">0.897</td>
<td align="left">0.842</td>
<td align="left">0.055</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.897</td>
<td align="left">0.824</td>
<td align="left">0.073</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.897</td>
<td align="left">0.755</td>
<td align="left">0.143</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.897</td>
<td align="left">0.736</td>
<td align="left">0.161</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.897</td>
<td align="left">0.743</td>
<td align="left">0.155</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.897</td>
<td align="left">0.578</td>
<td align="left">0.32</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.897</td>
<td align="left">0.71</td>
<td align="left">0.188</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.897</td>
<td align="left">0.958</td>
<td align="left">-0.06</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (70%) on OAP</td>
<td align="left">0.903</td>
<td align="left">0.899</td>
<td align="left">0.005</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (50%) on OAP</td>
<td align="left">0.903</td>
<td align="left">0.842</td>
<td align="left">0.061</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.903</td>
<td align="left">0.824</td>
<td align="left">0.079</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.903</td>
<td align="left">0.755</td>
<td align="left">0.149</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.903</td>
<td align="left">0.736</td>
<td align="left">0.167</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.903</td>
<td align="left">0.743</td>
<td align="left">0.161</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.903</td>
<td align="left">0.578</td>
<td align="left">0.326</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.903</td>
<td align="left">0.71</td>
<td align="left">0.193</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.903</td>
<td align="left">0.958</td>
<td align="left">-0.054</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OAP (50%) on OAP</td>
<td align="left">0.899</td>
<td align="left">0.842</td>
<td align="left">0.056</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.899</td>
<td align="left">0.824</td>
<td align="left">0.074</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.899</td>
<td align="left">0.755</td>
<td align="left">0.144</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.899</td>
<td align="left">0.736</td>
<td align="left">0.162</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.899</td>
<td align="left">0.743</td>
<td align="left">0.156</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.899</td>
<td align="left">0.578</td>
<td align="left">0.321</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.899</td>
<td align="left">0.71</td>
<td align="left">0.189</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.899</td>
<td align="left">0.958</td>
<td align="left">-0.059</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.842</td>
<td align="left">0.824</td>
<td align="left">0.018</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.842</td>
<td align="left">0.755</td>
<td align="left">0.087</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.842</td>
<td align="left">0.736</td>
<td align="left">0.106</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.842</td>
<td align="left">0.743</td>
<td align="left">0.1</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.842</td>
<td align="left">0.578</td>
<td align="left">0.264</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.842</td>
<td align="left">0.71</td>
<td align="left">0.132</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.842</td>
<td align="left">0.958</td>
<td align="left">-0.115</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.824</td>
<td align="left">0.755</td>
<td align="left">0.07</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.824</td>
<td align="left">0.736</td>
<td align="left">0.088</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.824</td>
<td align="left">0.743</td>
<td align="left">0.082</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.824</td>
<td align="left">0.578</td>
<td align="left">0.247</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.824</td>
<td align="left">0.71</td>
<td align="left">0.114</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.824</td>
<td align="left">0.958</td>
<td align="left">-0.133</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.755</td>
<td align="left">0.736</td>
<td align="left">0.019</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.755</td>
<td align="left">0.743</td>
<td align="left">0.012</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.755</td>
<td align="left">0.578</td>
<td align="left">0.177</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.755</td>
<td align="left">0.71</td>
<td align="left">0.045</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.755</td>
<td align="left">0.958</td>
<td align="left">-0.203</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OMP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.736</td>
<td align="left">0.743</td>
<td align="left">-0.006</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OMP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.736</td>
<td align="left">0.578</td>
<td align="left">0.159</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OMP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.736</td>
<td align="left">0.71</td>
<td align="left">0.026</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OMP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.736</td>
<td align="left">0.958</td>
<td align="left">-0.221</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.743</td>
<td align="left">0.578</td>
<td align="left">0.165</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.743</td>
<td align="left">0.71</td>
<td align="left">0.033</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.743</td>
<td align="left">0.958</td>
<td align="left">-0.215</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">COCO on OMP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.578</td>
<td align="left">0.71</td>
<td align="left">-0.132</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">COCO on OMP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.578</td>
<td align="left">0.958</td>
<td align="left">-0.38</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">COCO on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.71</td>
<td align="left">0.958</td>
<td align="left">-0.248</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<graphic xlink:href="2212.00741v1_tbl1a.jpg" mimetype="image" mime-subtype="jpeg"/>
<graphic xlink:href="2212.00741v1_tbl1b.jpg" mimetype="image" mime-subtype="jpeg"/>
<graphic xlink:href="2212.00741v1_tbl1c.jpg" mimetype="image" mime-subtype="jpeg"/>
<graphic xlink:href="2212.00741v1_tbl1d.jpg" mimetype="image" mime-subtype="jpeg"/>
</alternatives>
</table-wrap>
<table-wrap id="tab2">
<label>Table S2</label>
<caption><p>Pairwise t-tests comparing the PCK@0.2 values for different training and testing set combinations. PCK@0.2 values were obtained across 100 random samples of 500 images from the test sets. P-values adjusted for multiple comparisons using Bonferroni correction. ** not significant</p></caption>
<alternatives>
<table frame="hsides" rules="all">
<thead valign="top">
<tr>
<th align="left">Experiment 1</th>
<th align="left">Experiment 2</th>
<th align="left">Mean 1</th>
<th align="left">Mean 2</th>
<th align="left">difference</th>
<th align="left">p.ad j</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (100%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.876</td>
<td align="left">0.053</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (90%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.886</td>
<td align="left">0.043</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (70%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.878</td>
<td align="left">0.051</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (50%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.776</td>
<td align="left">0.154</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.747</td>
<td align="left">0.182</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.617</td>
<td align="left">0.313</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.929</td>
<td align="left">0.587</td>
<td align="left">0.342</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.929</td>
<td align="left">0.584</td>
<td align="left">0.345</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.929</td>
<td align="left">0.332</td>
<td align="left">0.597</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.929</td>
<td align="left">0.569</td>
<td align="left">0.361</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OMP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.929</td>
<td align="left">0.962</td>
<td align="left">-0.033</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (90%) on OAP</td>
<td align="left">0.876</td>
<td align="left">0.886</td>
<td align="left">-0.01</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (70%) on OAP</td>
<td align="left">0.876</td>
<td align="left">0.878</td>
<td align="left">-0.002</td>
<td align="left">1**</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (50%) on OAP</td>
<td align="left">0.876</td>
<td align="left">0.776</td>
<td align="left">0.1</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.876</td>
<td align="left">0.747</td>
<td align="left">0.129</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.876</td>
<td align="left">0.617</td>
<td align="left">0.26</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.876</td>
<td align="left">0.587</td>
<td align="left">0.289</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.876</td>
<td align="left">0.584</td>
<td align="left">0.292</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.876</td>
<td align="left">0.332</td>
<td align="left">0.544</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.876</td>
<td align="left">0.569</td>
<td align="left">0.307</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.876</td>
<td align="left">0.962</td>
<td align="left">-0.086</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (70%) on OAP</td>
<td align="left">0.886</td>
<td align="left">0.878</td>
<td align="left">0.008</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (50%) on OAP</td>
<td align="left">0.886</td>
<td align="left">0.776</td>
<td align="left">0.11</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.886</td>
<td align="left">0.747</td>
<td align="left">0.139</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.886</td>
<td align="left">0.617</td>
<td align="left">0.27</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.886</td>
<td align="left">0.587</td>
<td align="left">0.299</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.886</td>
<td align="left">0.584</td>
<td align="left">0.302</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.886</td>
<td align="left">0.332</td>
<td align="left">0.554</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.886</td>
<td align="left">0.569</td>
<td align="left">0.317</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (90%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.886</td>
<td align="left">0.962</td>
<td align="left">-0.076</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OAP (50%) on OAP</td>
<td align="left">0.878</td>
<td align="left">0.776</td>
<td align="left">0.103</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.878</td>
<td align="left">0.747</td>
<td align="left">0.131</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.878</td>
<td align="left">0.617</td>
<td align="left">0.262</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.878</td>
<td align="left">0.587</td>
<td align="left">0.291</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.878</td>
<td align="left">0.584</td>
<td align="left">0.294</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.878</td>
<td align="left">0.332</td>
<td align="left">0.546</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.878</td>
<td align="left">0.569</td>
<td align="left">0.31</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (70%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.878</td>
<td align="left">0.962</td>
<td align="left">-0.084</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">OAP (30%) on OAP</td>
<td align="left">0.776</td>
<td align="left">0.747</td>
<td align="left">0.029</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.776</td>
<td align="left">0.617</td>
<td align="left">0.159</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.776</td>
<td align="left">0.587</td>
<td align="left">0.189</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.776</td>
<td align="left">0.584</td>
<td align="left">0.191</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.776</td>
<td align="left">0.332</td>
<td align="left">0.443</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.776</td>
<td align="left">0.569</td>
<td align="left">0.207</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (50%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.776</td>
<td align="left">0.962</td>
<td align="left">-0.187</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">OAP (10%) on OAP</td>
<td align="left">0.747</td>
<td align="left">0.617</td>
<td align="left">0.131</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.747</td>
<td align="left">0.587</td>
<td align="left">0.16</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.747</td>
<td align="left">0.584</td>
<td align="left">0.163</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.747</td>
<td align="left">0.332</td>
<td align="left">0.415</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.747</td>
<td align="left">0.569</td>
<td align="left">0.178</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (30%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.747</td>
<td align="left">0.962</td>
<td align="left">-0.215</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">OAP (100%) on OMP</td>
<td align="left">0.617</td>
<td align="left">0.587</td>
<td align="left">0.03</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.617</td>
<td align="left">0.584</td>
<td align="left">0.032</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.617</td>
<td align="left">0.332</td>
<td align="left">0.284</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.617</td>
<td align="left">0.569</td>
<td align="left">0.048</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (10%) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.617</td>
<td align="left">0.962</td>
<td align="left">-0.346</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OMP</td>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">0.587</td>
<td align="left">0.584</td>
<td align="left">0.003</td>
<td align="left">1**</td>
</tr>
<tr>
<td align="left">OAP (100%) on OMP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.587</td>
<td align="left">0.332</td>
<td align="left">0.255</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OMP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.587</td>
<td align="left">0.569</td>
<td align="left">0.018</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OAP (100%) on OMP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.587</td>
<td align="left">0.962</td>
<td align="left">-0.375</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">COCO on OMP</td>
<td align="left">0.584</td>
<td align="left">0.332</td>
<td align="left">0.252</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.584</td>
<td align="left">0.569</td>
<td align="left">0.016</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">OMP (no Apes) on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.584</td>
<td align="left">0.962</td>
<td align="left">-0.378</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">COCO on OMP</td>
<td align="left">COCO on OAP</td>
<td align="left">0.332</td>
<td align="left">0.569</td>
<td align="left">-0.236</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">COCO on OMP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.332</td>
<td align="left">0.962</td>
<td align="left">-0.63</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">COCO on OAP</td>
<td align="left">COCO on COCO</td>
<td align="left">0.569</td>
<td align="left">0.962</td>
<td align="left">-0.394</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<graphic xlink:href="2212.00741v1_tbl2a.jpg" mimetype="image" mime-subtype="jpeg"/>
<graphic xlink:href="2212.00741v1_tbl2b.jpg" mimetype="image" mime-subtype="jpeg"/>
<graphic xlink:href="2212.00741v1_tbl2c.jpg" mimetype="image" mime-subtype="jpeg"/>
<graphic xlink:href="2212.00741v1_tbl2d.jpg" mimetype="image" mime-subtype="jpeg"/>
<graphic xlink:href="2212.00741v1_tbl2e.jpg" mimetype="image" mime-subtype="jpeg"/>
</alternatives>
</table-wrap>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure S1</label>
    <caption><title>The probability of correct keypoint (PCK) values (y-axis) at different thresholds ranging from 0-1 (x-axis) of HRNet-W48 models tested on each species from the OpenApePose (OAP) test set and trained on the OAP training set with the corresponding species excluded.</title><p>Dotted lines indicate the performance on the species excluded from training in the case of OAP and the performance of the OpenMonkeyPose model trained on monkeys on the excluded species.</p></caption>
<graphic xlink:href="2212.00741v1_fig6.jpg" mimetype="image" mime-subtype="jpeg"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86873.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kalan</surname>
<given-names>Ammie K</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Victoria</institution>
</institution-wrap>
<city>Victoria</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
<kwd>Landmark</kwd>
</kwd-group>
</front-stub>
<body>
<p>The OpenApePose dataset presented in this manuscript represents an <bold>important</bold> contribution to the field of primate behaviour and computer-vision science with methodological applications that are sure to be applicable for a wide variety of taxa. The analysis supporting the utility of this database is <bold>solid</bold> and <bold>compelling</bold> but would benefit from some additional clarity, particularly with regards to the <bold>landmark</bold> annotations, model parameters and division of the dataset for training, validation and testing.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86873.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This work provides a new dataset of 71,688 images of different ape species across a variety of environmental and behavioral conditions, along with pose annotations per image. The authors demonstrate the value of their dataset by training pose estimation networks (HRNet-W48) on both their own dataset and other primate datasets (OpenMonkeyPose for monkeys, COCO for humans), ultimately showing that the model trained on their dataset had the best performance (performance measured by PCK and AUC). In addition to their ablation studies where they train pose estimation models with either specific species removed or a certain percentage of the images removed, they provide solid evidence that their large, specialized dataset is uniquely positioned to aid in the task of pose estimation for ape species.</p>
<p>The diversity and size of the dataset make it particularly useful, as it covers a wide range of ape species and poses, making it particularly suitable for training off-the-shelf pose estimation networks or for contributing to the training of a large foundational pose estimation model. In conjunction with new tools focused on extracting behavioral dynamics from pose, this dataset can be especially useful in understanding the basis of ape behaviors using pose.</p>
<p>Since the dataset provided is the first large, public dataset of its kind exclusively for ape species, more details should be provided on how the data were annotated, as well as summaries of the dataset statistics. In addition, the authors should provide the full list of hyperparameters for each model that was used for evaluation (e.g., mmpose config files, textual descriptions of augmentation/optimization parameters).</p>
<p>Overall this work is a terrific contribution to the field and is likely to have a significant impact on both computer vision and animal behavior.</p>
<p>Strengths:</p>
<p>
- Open source dataset with excellent annotations on the format, as well as example code provided for working with it.</p>
<p>
- Properties of the dataset are mostly well described.</p>
<p>
- Comparison to pose estimation models trained on humans vs monkeys, finding that models trained on human data generalized better to apes than the ones trained on monkeys, in accordance with phylogenetic similarity. This provides evidence for an important consideration in the field: how well can we expect pose estimation models to generalize to new species when using data from closely or distantly related ones?</p>
<p>
- Sample efficiency experiments reflect an important property of pose estimation systems, which indicates how much data would be necessary to generate similar datasets in other species, as well as how much data may be required for fine-tuning these types of models (also characterized via ablation experiments where some species are left out).</p>
<p>
- The sample efficiency experiments also reveal important insights about scaling properties of different model architectures, finding that HRNet saturates in performance improvements as a function of dataset size sooner than other architectures like CPMs (even though HRNets still perform better overall).</p>
<p>Weaknesses:</p>
<p>
- More details on training hyperparameters used (preferably full config if trained via mmpose).</p>
<p>
- Should include dataset datasheet, as described in Gebru et al 2021 (arXiv:1803.09010).</p>
<p>
- Should include crowdsourced annotation datasheet, as described in Diaz et al 2022 (arXiv:2206.08931). Alternatively, the specific instructions that were provided to Hive/annotators would be highly relevant to convey what annotation protocols were employed here.</p>
<p>
- Should include model cards, as described in Mitchell et al (arXiv:1810.03993).</p>
<p>
- It would be useful to include more information on the source of the data as they are collected from many different sites and from many different individuals, some of which may introduce structural biases such as lighting conditions due to geography and time of year.</p>
<p>
- Is there a reason not to use OKS? This incorporates several factors such as landmark visibility, scale, and landmark type-specific annotation variability as in Ronchi &amp; Perona 2017 (arXiv:1707.05388). The latter (variability) could use the human pose values (for landmarks types that are shared), the least variable keypoint class in humans (eyes) as a conservative estimate of accuracy, or leverage a unique aspect of this work (crowdsourced annotations) which affords the ability to estimate these values empirically.</p>
<p>
- A reporting of the scales present in the dataset would be useful (e.g., histogram of unnormalized bounding boxes) and would align well with existing pose dataset papers such as MS-COCO (arXiv:1405.0312) which reports the distribution of instance sizes and instance density per image.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86873.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors present the OpenApePose database constituting a collection of over 70000 ape images which will be important for many applications within primatology and the behavioural sciences. The authors have also rigorously tested the utility of this database in comparison to available Pose image databases for monkeys and humans to clearly demonstrate its solid potential. However, the variation in the database with regards to individuals, background, source/setting is not clearly articulated and would be beneficial information for those wishing to make use of this resource in the future. At present, there is also a lack of clarity as to how this image database can be extrapolated to aid video data analyses which would be highly beneficial as well.</p>
<p>I have two major concerns with regard to the manuscript as it currently stands which I think if addressed would aid the clarity and utility of this database for readers.</p>
<p>1. Human annotators are mentioned as doing the 16 landmarks manually for all images but there is no assessment of inter-observer reliability or the such. I think something to this end is currently missing, along with how many annotators there were. This will be essential for others to know who may want to use this database in the future.</p>
<p>Relevant to this comment, in your description of the database, a table or such could be included, providing the number of images from each source/setting per species and/or number of individuals. Something to give a brief overview of the variation beyond species. (subspecies would also be of benefit for example).</p>
<p>2. You mention around line 195 that you used a specific function for splitting up the dataset into training, validation, and test but there is no information given as to whether this was simply random or if an attempt to balance across species, individuals, background/source was made. I would actually think that a balanced approach would be more appropriate/useful here so whether or not this was done, and the reasoning behind that must be justified.</p>
<p>This is especially relevant given that in one test you report balancing across species (for the sample size subsampling procedure).</p>
<p>And another perhaps major concern that I think should also be addressed somewhere is the fact that this is an image database tested on images while the abstract and manuscript mention the importance of pose estimation for video datasets, yet the current manuscript does not provide any clear test of video datasets nor engage with the practicalities associated with using this image-based database for applications to video datasets. Somewhere this needs to be added to clarify its practical utility.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.86873.1.sa3</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Desai</surname>
<given-names>Nisarg</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bala</surname>
<given-names>Praneet</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Richardson</surname>
<given-names>Rebecca</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Raper</surname>
<given-names>Jessica</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zimmermann</surname>
<given-names>Jan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hayden</surname>
<given-names>Benjamin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authorsâ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>This work provides a new dataset of 71,688 images of different ape species across a variety of environmental and behavioral conditions, along with pose annotations per image. The authors demonstrate the value of their dataset by training pose estimation networks (HRNet-W48) on both their own dataset and other primate datasets (OpenMonkeyPose for monkeys, COCO for humans), ultimately showing that the model trained on their dataset had the best performance (performance measured by PCK and AUC). In addition to their ablation studies where they train pose estimation models with either specific species removed or a certain percentage of the images removed, they provide solid evidence that their large, specialized dataset is uniquely positioned to aid in the task of pose estimation for ape species.</p>
<p>The diversity and size of the dataset make it particularly useful, as it covers a wide range of ape species and poses, making it particularly suitable for training off-the-shelf pose estimation networks or for contributing to the training of a large foundational pose estimation model. In conjunction with new tools focused on extracting behavioral dynamics from pose, this dataset can be especially useful in understanding the basis of ape behaviors using pose.</p>
</disp-quote>
<p>We thank the reviewer for the kind comments.</p>
<disp-quote content-type="editor-comment">
<p>Since the dataset provided is the first large, public dataset of its kind exclusively for ape species, more details should be provided on how the data were annotated, as well as summaries of the dataset statistics. In addition, the authors should provide the full list of hyperparameters for each model that was used for evaluation (e.g., mmpose config files, textual descriptions of augmentation/optimization parameters).</p>
</disp-quote>
<p>We have added more details on the annotation process and have included the list of instructions sent to the annotators. We have also included mmpose configs with the code provided. The following files include the relevant details:</p>
<p>File including the list of instructions sent to the annotators:  OpenMonkeyWild Photograph Rubric.pdf</p>
<p>Mmpose configs:</p>
<p>i)  TopDownOAPDataset.py</p>
<p>ii) animal_oap_dataset.py</p>
<p>iii)    <bold>init</bold>.py</p>
<p>iv) hrnet_w48_oap_256x192_full.py</p>
<p>Anaconda environment files:</p>
<p>i)  OpenApePose.yml</p>
<p>ii) requirements.txt</p>
<disp-quote content-type="editor-comment">
<p>Overall this work is a terrific contribution to the field and is likely to have a significant impact on both computer vision and animal behavior.</p>
<p>Strengths:</p>
<list list-type="bullet">
<list-item><p>Open source dataset with excellent annotations on the format, as well as example code provided for working with it.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Properties of the dataset are mostly well described.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Comparison to pose estimation models trained on humans vs monkeys, finding that models trained on human data generalized better to apes than the ones trained on monkeys, in accordance with phylogenetic similarity. This provides evidence for an important consideration in the field: how well can we expect pose estimation models to generalize to new species when using data from closely or distantly related ones?  - Sample efficiency experiments reflect an important property of pose estimation systems, which indicates how much data would be necessary to generate similar datasets in other species, as well as how much data may be required for fine-tuning these types of models (also characterized via ablation experiments where some species are left out).</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>The sample efficiency experiments also reveal important insights about scaling properties of different model architectures, finding that HRNet saturates in performance improvements as a function of dataset size sooner than other architectures like CPMs (even though HRNets still perform better overall).</p>
</list-item></list>
</disp-quote>
<p>We thank the reviewer for the kind comments.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<list list-type="bullet">
<list-item><p>More details on training hyperparameters used (preferably full config if trained via mmpose).</p>
</list-item></list>
</disp-quote>
<p>We have now included mmpose configs and anaconda environment files that allow researchers to use the dataset with specific versions of mmpose and other packages we trained our models with. The list of files is provided above.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Should include dataset datasheet, as described in Gebru et al 2021 (arXiv:1803.09010).</p>
</list-item></list>
</disp-quote>
<p>We have included a datasheet for our dataset in the appendix lines 621-764.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Should include crowdsourced annotation datasheet, as described in Diaz et al 2022
(arXiv:2206.08931). Alternatively, the specific instructions that were provided to Hive/annotators would be highly relevant to convey what annotation protocols were employed here.</p>
</list-item></list>
</disp-quote>
<p>We have included the list of instructions sent to the Hive annotators in the supplementary materials. File: OpenMonkeyWild Photograph Rubric.pdf</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Should include model cards, as described in Mitchell et al (arXiv:1810.03993).</p>
</list-item></list>
</disp-quote>
<p>We have included a model card for the included model in the results section line 359. See Author response image 1.</p>
<disp-formula id="sa3equ1">
<bold>Author response image 1.</bold>
â
<graphic mime-subtype="jpg" xlink:href="elife-86873-sa3-equ1.jpg" mimetype="image"/>
</disp-formula>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>It would be useful to include more information on the source of the data as they are collected from many different sites and from many different individuals, some of which may introduce structural biases such as lighting conditions due to geography and time of year.</p>
</list-item></list>
</disp-quote>
<p>We agree that the source could introduce structural biases. This is why we included images from so many different sources and captured images at different times from the same sourceâin hopes that a large variety of background and lighting conditions are represented. However, doing so limits our ability to document each source background and lighting condition separately.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Is there a reason not to use OKS? This incorporates several factors such as landmark visibility, scale, and landmark type-specific annotation variability as in Ronchi &amp; Perona 2017 (arXiv:1707.05388). The latter (variability) could use the human pose values (for landmarks types that are shared), the least variable keypoint class in humans (eyes) as a conservative estimate of accuracy, or leverage a unique aspect of this work (crowdsourced annotations) which affords the ability to estimate these values empirically.</p>
</list-item></list>
</disp-quote>
<p>The focus of this work is on overall keypoint localization accuracy and hence we wanted a metric that is easy to interpret and implement, in this case we made use of PCK  (Percentage of Correct Keypoints). PCK is a simple and widely used metric that measures the percentage of correctly localized keypoints within a certain distance threshold from their corresponding groundtruth keypoints.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>A reporting of the scales present in the dataset would be useful (e.g., histogram of unnormalized bounding boxes) and would align well with existing pose dataset papers such as MS-COCO (arXiv:1405.0312) which reports the distribution of instance sizes and instance density per image.</p>
</list-item></list>
</disp-quote>
<p>RESPONSE: We have now included a histogram of unnormalized bounding boxes in the manuscript, Author response image 2.</p>
<fig id="sa3fig1">
<label>Author response image 2.</label>
<graphic mime-subtype="jpg" xlink:href="elife-86873-sa3-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>The authors present the OpenApePose database constituting a collection of over 70000 ape images which will be important for many applications within primatology and the behavioural sciences. The authors have also rigorously tested the utility of this database in comparison to available Pose image databases for monkeys and humans to clearly demonstrate its solid potential.</p>
</disp-quote>
<p>We thank the reviewer for the kind comments.</p>
<disp-quote content-type="editor-comment">
<p>However, the variation in the database with regards to individuals, background, source/setting is not clearly articulated and would be beneficial information for those wishing to make use of this resource in the future. At present, there is also a lack of clarity as to how this image database can be extrapolated to aid video data analyses which would be highly beneficial as well.</p>
<p>I have two major concerns with regard to the manuscript as it currently stands which I think if addressed would aid the clarity and utility of this database for readers.</p>
<p>1. Human annotators are mentioned as doing the 16 landmarks manually for all images but there is no assessment of inter-observer reliability or the such. I think something to this end is currently missing, along with how many annotators there were. This will be essential for others to know who may want to use this database in the future.</p>
</disp-quote>
<p>We thank the reviewer for pointing this out. Inter-observer reliability is important for ensuring the quality of the annotations. We first used Amazon MTurk to crowd source annotations and found that the inter-observer reliability and the annotation quality was poor. This was the reason for choosing a commercial service such as Hive AI. As the crowd sourcing and quality control are managed by Hive through their internal procedures, we do not have access to data that can allow us to assess inter-observer reliability. However, the annotation quality was assessed by first author ND through manual inspections of the annotations visualized on all of the images the database. Additionally, our ablation experiments with high out of sample performances further vaildate the quality of the annotations.</p>
<disp-quote content-type="editor-comment">
<p>Relevant to this comment, in your description of the database, a table or such could be included, providing the number of images from each source/setting per species and/or number of individuals. Something to give a brief overview of the variation beyond species. (subspecies would also be of benefit for example).</p>
</disp-quote>
<p>Our goal was to obtain as many images as possible from the most commonly studied ape species. In order to ensure a large enough database, we focused only on the species and combined images from as many sources as possible to reach our goal of ~10,000 images per species. With the wide range of people involved in obtaining the images, we could not ensure that all the photographers had the necessary expertise to differentiate individuals and subspecies of the subjects they were photographing. We could only ensure that the right species was being photographed. Hence, we cannot include more detailed information.</p>
<disp-quote content-type="editor-comment">
<p>1. You mention around line 195 that you used a specific function for splitting up the dataset into training, validation, and test but there is no information given as to whether this was simply random or if an attempt to balance across species, individuals, background/source was made. I would actually think that a balanced approach would be more appropriate/useful here so whether or not this was done, and the reasoning behind that must be justified.</p>
<p>This is especially relevant given that in one test you report balancing across species (for the sample size subsampling procedure).</p>
</disp-quote>
<p>We created the training set to reflect the species composition of the whole dataset, but used test sets balanced by species. This was done to give a sense of the performance of a model that could be trained with the entire dataset, that does not have the species fully balanced. We believe that researchers interested in training models using this dataset for behavior tracking applications would use the entire dataset to fully leverage the variation in the dataset. However, for those interested in training models with balanced species, we provide an annotation file with all the images included, which would allow researchers to create their own training and test sets that meet their specific needs. We have added this justification in the manuscript to guide the other users with different needs. Lines 530-534: âWe did not balance our training set for the species as we wanted to utilize the full variation in the dataset and assess models trained with the proportion of species as reflected in the dataset. We provide annotations including the entire dataset to allow others to make create their own training/validation/test sets that suit their needs.â</p>
<disp-quote content-type="editor-comment">
<p>And another perhaps major concern that I think should also be addressed somewhere is the fact that this is an image database tested on images while the abstract and manuscript mention the importance of pose estimation for video datasets, yet the current manuscript does not provide any clear test of video datasets nor engage with the practicalities associated with using this image-based database for applications to video datasets. Somewhere this needs to be added to clarify its practical utility.</p>
</disp-quote>
<p>We thank the reviewer for this important suggestion. Since we can separate a video into its constituent frames, one can indeed use the provided model or other models trained using this dataset for inference on the frames, thus allowing video tracking applications. We now include a short video clip of a chimpanzee with inferences from the provided model visualized in the supplementary materials.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<list list-type="bullet">
<list-item><p>Please provide a more thorough description of the annotation procedure (i.e., the instructions given to crowd workers)! See public review for reference on dataset annotation reporting cards.</p>
</list-item></list>
</disp-quote>
<p>We have included the list of instructions for Hive annotators in the supplementary materials.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>An estimate of the crowd worker accuracy and variability would be super valuable!</p>
</list-item></list>
</disp-quote>
<p>While we agree that this is useful, we do not have access to Hive internal data on crowd worker IDs that could allow us to estimate these metrics. Furthermore, we assessed each image manually to ensure good annotation quality.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>In the methods section it is reported that images were discarded because they were either too blurry, small, or highly occluded. Further quantification could be provided. How many images were discarded per species?</p>
</list-item></list>
</disp-quote>
<p>Itâs not really clear to us why this is interesting or important. We used a large number of photographers and annotators, some of whom gave a high ratio of great images; some of whom gave a poor ratio. But itâs not clear what those ratios tell us.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Placing the numerical values at the end of the bars would make the graphs more readable in Figures 4 and 5.</p>
</list-item></list>
</disp-quote>
<p>We thank the reviewer for this suggestion. While we agree that this can help, we do not have space to include the number in a font size that would be readable. Smaller font sizes that are likely to fit may not be readable for all readers. We have included the numerical values in the main text in the results section for those interested and hope that the figures provide a qualitative sense of the results to the readers.</p>
</body>
</sub-article>
</article>