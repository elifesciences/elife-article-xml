<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106871</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106871</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106871.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Neural signatures of motor memories emerge in neural network models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7725-0989</contrib-id>
<name>
<surname>Chang</surname>
<given-names>Joanna C</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4507-8648</contrib-id>
<name>
<surname>Clopath</surname>
<given-names>Claudia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>c.clopath@imperial.ac.uk</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2146-0703</contrib-id>
<name>
<surname>Gallego</surname>
<given-names>Juan A</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>jgallego@imperial.ac.uk</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Department of Bioengineering, Imperial College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ponte Costa</surname>
<given-names>Rui</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors jointly supervised this work.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: J.A.G. receives funding from Meta Platform Technologies, LLC. The remaining authors declare no competing interests.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-08-27">
<day>27</day>
<month>08</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106871</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-04-02">
<day>02</day>
<month>04</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-04-03">
<day>03</day>
<month>04</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.04.02.646788"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Chang et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Chang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106871-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Animals can learn and seamlessly perform a great number of behaviors. However, it is unclear how neural activity can accommodate new behaviors without interfering with those an animal has already acquired. Recent studies in monkeys performing motor and brain-computer interface (BCI) learning tasks have identified neural signatures—so-called “memory traces” and “uniform shifts”—that appear in the neural activity of a familiar task after learning a new task. Here we asked when these signatures arise and how they are related to continual learning. By modeling a BCI learning paradigm, we show that both signatures emerge naturally as a consequence of learning, without requiring a specific mechanism. In general, memory traces and uniform shifts reflected savings by capturing how information from different tasks coexisted in the same neural activity patterns. Yet, although the properties of these two different signatures were both indicative of savings, they were uncorrelated with each other. When we added contextual inputs that separated the activity for the different tasks, these signatures decreased even when savings were maintained, demonstrating the challenges of defining a clear relationship between neural activity changes and continual learning.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Animals can acquire, retain, and execute many different behaviors in a lifelong cycle in which new behaviors are learned without interfering with familiar ones. Learning leads to changes in synaptic connectivity <sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c4">4</xref></sup> and neural activity <sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c12">12</xref></sup>. These changes can be retained on different timescales, from hours<sup><xref ref-type="bibr" rid="c13">13</xref></sup> to decades<sup><xref ref-type="bibr" rid="c14">14</xref></sup>, and correlate with activity changes in different brain regions, from hippocampus<sup><xref ref-type="bibr" rid="c5">5</xref></sup> to the basal ganglia<sup><xref ref-type="bibr" rid="c15">15</xref></sup>, cerebellum<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>, and the motor cortices<sup><xref ref-type="bibr" rid="c9">9</xref>–<xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup>. While it is evident that learning new behaviors leads to new patterns in neural activity, it is unclear how brains can accommodate these new neural activity patterns with old ones. Specifically, how do brains integrate activity patterns that improve future performance of newly learned behaviors—a phenomenon called savings<sup><xref ref-type="bibr" rid="c17">17</xref></sup> (<xref rid="fig1" ref-type="fig">Figure 1A</xref>)—without decreasing performance for older, familiar behaviors?</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Potential neural signatures of learning.</title>
<p><bold>A)</bold> Learning a new task may cause changes in the neural activity used for a familiar task, and these signatures may help maintain knowledge about the new task to lead to retention savings if the new task is encountered again. <bold>B)</bold> Losey et al<sup><xref ref-type="bibr" rid="c18">18</xref></sup> studied a BCI learning task where monkeys performed a center-out reaching task with a familiar Map A (Task A1, pink), learned a new Map B (Task B1, light blue), then returned to the familiar Map A (Task A2, red). They identified a memory trace that made neural activity (colored dots) during movement execution for the familiar Map A more useful for a new Map B after learning this new Map B. Importantly, the memory trace existed without compromising the performance for Map A. <bold>C)</bold> Sun et al<sup><xref ref-type="bibr" rid="c11">11</xref></sup> studied a motor learning task where monkeys performed a baseline center-out reaching task (pink), learned to counteract a force field (light blue), then returned to the baseline task (red). They identified a uniform shift (colored dots) in the neural population space during movement preparation when monkeys learned to adapt to the force field that was sustained after returning to the baseline task.</p></caption>
<graphic xlink:href="646788v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Several recent studies have begun to elucidate how brains incorporate motor memories of new behaviors by adopting a neural population view <sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup>. In this view, brain function is thought to be based on collective patterns of co-variation across a neural population rather than on the independent activity of its constituent neurons<sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup>. Since relatively few activity patterns are needed to explain the majority of variance in a neural population during a given task, neural population activity can be described mathematically based on a low-dimensional surface, a neural manifold<sup><xref ref-type="bibr" rid="c20">20</xref></sup>. Neural manifolds are thought to reflect biophysical (e.g., connectivity-related) constraints on neural activity <sup><xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c22">22</xref>–<xref ref-type="bibr" rid="c24">24</xref></sup> and have been leveraged to gain insights into the neural basis for learning<sup><xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>.</p>
<p>Adopting this view, Sadtler et al<sup><xref ref-type="bibr" rid="c22">22</xref></sup> probed sequential learning using a brain-computer interface (BCI) paradigm where a linear map transformed neural population activity in monkey motor cortex to cursor movement, (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). In each session, monkeys switched from a familiar Map A within their existing neural manifold (Task A1) to a new Map B (Task B1) and then back to the familiar Map A (Task A2). In different sessions, two different kinds of Map Bs were used to support the hypothesis that neural manifolds capture constraints on neural population activity: Map Bs that required activity patterns within an existing manifold (“within-manifold perturbations”) could be learned in a matter of minutes, whereas Map Bs that required activity outside the manifold (“outside-manifold perturbations”) could not be learned within a session, requiring progressive learning over several days<sup><xref ref-type="bibr" rid="c10">10</xref></sup>. Focusing on within-manifold perturbations in this BCI paradigm, Losey et al<sup><xref ref-type="bibr" rid="c18">18</xref></sup> demonstrated that learning a new Map B altered the neural activity of the familiar Map A when monkeys were re-exposed to it (Task A2, <xref rid="fig1" ref-type="fig">Figure 1B</xref>). Crucially, this change made the activity during the familiar Map A more beneficial for the new Map B without negatively impacting performance of the familiar task (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). The authors could show this directly by taking advantage of their BCI paradigm which, in contrast to traditional motor tasks where the relationship between motor cortex activity and behavior is not known, gave them full access to the relationship between neural activity and behavior since it is fully defined by the corresponding map: when they “projected” Task A2 activity onto Map B offline, it led to better performance for Map B than Task A1 activity. Thus, they proposed that Task A2 activity retained a “memory trace” of the new Map B that did not compromise the performance for the familiar Map A<sup><xref ref-type="bibr" rid="c18">18</xref></sup>.</p>
<p>Using a classic motor learning task in which participants—in this case, monkeys—performing reaching movements need to learn to counteract a force field<sup><xref ref-type="bibr" rid="c25">25</xref></sup>, Sun et al<sup><xref ref-type="bibr" rid="c11">11</xref></sup> identified a “uniform shift” in the motor cortical population activity during movement preparation that emerged during learning. Similar to the “memory trace” in the previous BCI study, this shift was sustained after monkeys returned to the baseline task without a force field (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). Since this shift was in an axis orthogonal to the directions in the neural manifold that were predictive of motor output, the authors argued that it could potentially index a motor memory of the new force field task, again in a way that would not compromise performance for the familiar baseline task<sup><xref ref-type="bibr" rid="c11">11</xref></sup>. Together, these BCI learning and motor learning studies suggest that learning new tasks may produce changes in neural activity that incorporate information about the new task in the activity of familiar tasks, potentially leading to savings if the new task is encountered again (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). As a result, these signatures may be important for continual learning, that is, for learning tasks sequentially without forgetting previously acquired knowledge.</p>
<p>Here, we aimed to examine when these two potential signatures—the sustained uniform shift during preparation identified by Sun et al<sup><xref ref-type="bibr" rid="c11">11</xref></sup> and the memory traces during execution identified by Losey et al<sup><xref ref-type="bibr" rid="c18">18</xref></sup>—arise and how they may relate to continual learning. To this end, we studied both signatures during the same learning paradigm using recurrent neural networks (RNNs). We used RNNs because similar models have been shown to replicate key features of neural activity and motor output from experimental recordings<sup><xref ref-type="bibr" rid="c26">26</xref>–<xref ref-type="bibr" rid="c30">30</xref></sup>, enabling investigation of scenarios that are difficult to probe experimentally<sup><xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>.</p>
<p>We modeled an adapted version of the sequential learning BCI task described above, which allowed us to compare both the behavior and the changes in network activity—including the two potential signatures of motor memories we are interested in—to those observed in monkey motor cortex in the original studies. We found that both sustained uniform shifts and memory traces emerged naturally in our RNNs, without modeling any explicit additional mechanisms to force them to arise. To relate these signatures to continual learning, we examined a common way to measure the extent of continual learning: savings, or improved relearning of a task upon re-exposure, compared to the initial exposure <sup><xref ref-type="bibr" rid="c17">17</xref></sup>. Savings can be observed as either an increase in performance due to better retention immediately after re-exposure (“retention savings”), or a faster rate of learning during prolonged exposure (“learning rate savings”). Here, we focused on retention savings since learning rate savings might be confounded by different initial errors for our models trained with gradient descent. While the magnitude of the memory traces were closely correlated with retention savings, that of the uniform shift was not correlated with neither the magnitude of the memory traces nor the retention savings. Extending Losey et al’s <sup><xref ref-type="bibr" rid="c18">18</xref></sup> analyses to outside-manifold perturbations, we saw that memory traces were larger for outside-manifold perturbations when compared to within-manifold perturbations. This was due to greater forgetting in within-manifold perturbations when reverting back to the familiar map in Task A2. Inspired by behavioral studies on context-dependent learning <sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>, we added a context cue to better separate activity patterns and reduce interference to prevent forgetting. With context cues, a uniform shift was observed during learning of the new map in Task B1, but it did not persist during Task A2. Similarly, the magnitude of memory traces decreased and was no longer indicative of savings, even though there was still increased savings of the new map. Therefore, uniform shifts and memory traces arise naturally when learning a new task that shares activity patterns with a familiar task, but these signatures of learning for a novel task are no longer observed in the activity of the familiar task if they are separated by features like strong context cues.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>An RNN model of sequential learning naturally recapitulates experimental results</title>
<p>To understand how memory traces<sup><xref ref-type="bibr" rid="c18">18</xref></sup> and uniform shifts<sup><xref ref-type="bibr" rid="c11">11</xref></sup> arise and to explore their features in the context of continual learning, we used RNNs to model the same BCI experiments as in Losey et al<sup><xref ref-type="bibr" rid="c18">18</xref></sup> (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). In these experiments, monkeys used their motor cortical activity to control a computer cursor through a BCI map that translated neural activity onto cursor velocity. To study how neural activity changed across different tasks, monkeys had to control the BCI to produce center-out reaches using different BCI maps that were presented in a sequential block design.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>RNNs recapitulate experimental results in sequential learning.</title>
<p><bold>A)</bold> Networks were trained to perform center-out reaches by using different BCI maps which linearly transformed network activity to output velocities. Map A was the intuitive map based on the “intrinsic manifold” –a low-dimensional flat surface that captured the dominant covariation patterns of the network activity after baseline learning–, while Map B was a permutation of Map A. Produced velocities were fed back into the network after a 120 ms delay. <bold>B)</bold> Sequential training procedure: we first trained the recurrent, input, and output network weights during an initial training phase where networks had to produce random center-out reaches. Then, networks were trained to produce center-out reaches to eight fixed targets using different BCI maps (A or B) as a fixed output layer. Networks learned to produce output through these maps in blocks: from Map A (Task A1), to Map B (Task B1), then back to Map A (Task A2). Loss curves during each task block for an example network. Inset: position trajectories at the end of each block. <bold>C)</bold> Mahalanobis distance between population activity means for Task A1 and Task A2 for each target (black), and for controls where the task labels were shuffled (gray). Dotted lines, means across 10 different maps for each random seed (<italic>n</italic>=8 random seeds).</p></caption>
<graphic xlink:href="646788v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Here, we first used gradient descent to pretrain RNNs to produce velocities for varied delayed center-out reaches to model the monkeys’ ability to produce the necessary neural activity for controlling their arms (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). We incorporated a delay period to examine possible uniform shifts during preparation, even if the original BCI experiments did not have one. RNNs were also given delayed feedback of their produced velocities<sup><xref ref-type="bibr" rid="c26">26</xref></sup> (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), and their performance was quantified as the mean-squared error (MSE) between the produced and synthetic target velocities (Methods). Following this initial training, RNNs were trained to perform the eight-target centerout BCI task using different BCI maps, which were modeled as fixed output weights. Each BCI map consisted of two transformations. First, activity from all 400 units in our RNNs was projected onto an eight-dimensional flat neural manifold, which we identified using Principal Component Analysis (PCA) and thus captured the majority (89-91%) of the variance in the network activity. Second, activity within this manifold was projected to the two-dimensional subspace defined by the corresponding BCI map to obtain the output cursor velocity along the horizontal and vertical axes (<xref rid="fig2" ref-type="fig">Figure 2A</xref>).</p>
<p>To probe whether and how learning a new task affected the neural activity of a familiar task, the RNNs were trained on the two different maps in sequential blocks: first with Map A (Task A1), then with a new Map B (Task B1), then back to the familiar Map A (Task A2, <xref rid="fig2" ref-type="fig">Figure 2B</xref>). Replicating the experiments<sup><xref ref-type="bibr" rid="c22">22</xref></sup>, Map A was determined based on the “intuitive manifold” map that captured the majority of the variance in the activity following baseline training, and Map B was a within-manifold perturbation of Map A that required activity in the same low-dimensional neural manifold (Methods).</p>
<p>The RNNs were able to learn Map A and Map B in sequence, with similar rates of learning and performance as the experimental results<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup> (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Recapitulating experimental results, neural activity patterns for a given target changed from Task A1 to Task A2 in our models, even if the mapping during Tasks A1 and A2 was the same<sup><xref ref-type="bibr" rid="c18">18</xref></sup> (<xref rid="fig2" ref-type="fig">Figure 2C</xref>, <xref rid="figS1" ref-type="fig">Figure S1A</xref>). These changes were significant across different targets and significantly greater than a shuffled control (<xref rid="fig2" ref-type="fig">Figure 2C</xref>, <italic>P</italic> = 8.9 <italic>·</italic> 10<sup>−107</sup>, one-sided Wilcoxon signed-rank test). Thus, learning a new task (even a new version of the same task, in this case defined by Map B) altered the RNN activity for a familiar task without the need for an additional learning mechanism.</p>
</sec>
<sec id="s2b">
<title>Uniform shifts and memory traces arise from sequential learning without adding explicit mechanisms</title>
<p>Examining the properties of these activity changes revealed that they could be partially attributed to a uniform shift in the preparatory population activity from Task A1 to Task B1, which was orthogonal to the directions in neural space predictive of motor output (<xref rid="fig3" ref-type="fig">Figure 3A</xref>, Methods), as experimentally observed in monkey motor cortex<sup><xref ref-type="bibr" rid="c11">11</xref></sup>. These activity shifts generally persisted in Task A2 (<xref rid="fig3" ref-type="fig">Figure 3A-C</xref>; <xref rid="figS2" ref-type="fig">Figure S2A,B</xref>), further replicating experimental results<sup><xref ref-type="bibr" rid="c11">11</xref></sup>. Due to these changes, the different blocked tasks (Task A1, Task B1, and Task A2), could be easily distinguished based on their preparatory unit activity alone using linear discriminant analysis classifiers (<xref rid="figS1" ref-type="fig">Figure S1A</xref>), demonstrating that the tasks were indeed using different activity patterns.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Uniform shifts and memory traces arise from sequential learning without explicit mechanisms.</title>
<p><bold>A)</bold> Population activity during preparation (first 200 ms of each trial) for an example session. Circles, time-averaged, trial-averaged activity for each target (mean target activity) in each task block. Triangles, time-, trial-, and target-averaged activity (i.e., mean task activity) in each block. The uniform shift axis is quantified by the direction of the vector connecting the mean task activity from Task A1 to Task B1. The uniform shift across tasks is calculated by projecting the mean task activity onto the uniform shift axis and calculating the resulting distance between tasks. <bold>B)</bold> Uniform shift from Task A1 to Task B1 and Task A1 to Task A2. Dotted lines, means across 10 different maps for each random seed. <bold>C)</bold> Normalized uniform shift, calculated by dividing by the uniform shift from Task A1 to Task B1 for each session. <bold>D)</bold> Uniform shift from Task A1 to Task B1 across targets. Big circle and error bars, means and 95% confidence intervals with bootstrapping for 10 different maps for each random seed (<italic>n</italic>=8 random seeds); small circles, different maps. Note that uniform shifts are consistently greater than zero (dotted line) indicating a shift in population activity. <bold>E)</bold> Task A1 and Task A2 activity were projected into Map A online or into Map B offline. The resulting output was used to assess how useful the activity is for each map, i.e., how close to the target output it brings the cursor. <bold>F)</bold> MSE between the produced and target output for each projection for an example target in an example session. Dashed lines, mean MSE across trials for that target. The memory trace is quantified as the decrease in mean MSE (i.e., the increase in performance) from Task A1→Map B to Task A2→Map B. <bold>G)</bold> MSE across all targets for projections onto the two different maps. Circles and error bars, means and 95% confidence intervals with bootstrapping for 10 different maps for each random seed (<italic>n</italic>=8 random seeds). Traces, different sessions. <bold>H)</bold> Same as D but for the memory trace. Note that memory traces are consistently greater than zero (dotted line) indicating increased offline performance for Task B with activity used for Task A2 compared to that for Task A1.</p></caption>
<graphic xlink:href="646788v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Having established that uniform shifts emerge naturally as a result of continual learning in RNNs using standard training techniques, we next asked whether we could observe similar memory traces as those found in experimental recordings<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. Specifically, we were interested in whether memory traces spontaneously arise from our simple sequential learning model, or if an additional explicit mechanism was necessary for them to arise.</p>
<p>The advantage of a BCI setup is that we can probe how useful the neural activity for a familiar map is for a new BCI map before and after learning the new BCI map. While Task A1 and Task A2 used the same Map A online, we could project their activity through Map B offline to evaluate how much the activity changed from Task A1 to Task A2 to accommodate new activity patterns that were directly useful for Map B. In other words, we asked how much learning Map B left a memory trace of this new task in the activity of the familiar task defined by Map A (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). When projected through Map A, Task A1 and Task A2 activity both produced output position trajectories that were similar to the target output, without a clear difference in performance (example in <xref rid="fig3" ref-type="fig">Figure 3E</xref>). However, when projected through Map B, Task A2 activity produced output trajectories that were closer to the target output than that of Task A1 activity (<xref rid="fig3" ref-type="fig">Figure 3E</xref>), indicating the existence of a memory trace of Map B. We quantified this memory trace as the increase in performance, or decrease in MSE, for Map B from Task A1 to Task A2 (<xref rid="fig3" ref-type="fig">Figure 3F</xref>). From here, we will denote “Task X activity projected through Map Y” as “Task X→Map Y”, such that the memory trace is the decrease in MSE from Task A1→Map B to Task A2→Map B.</p>
<p>We observed that memory traces consistently arose across different targets (<xref rid="fig3" ref-type="fig">Figure 3G,H</xref>; <xref rid="figS3" ref-type="fig">Figure S3A</xref>). These increases in performance for Map B from Task A1 to Task A2 were significantly greater than a control based on the performance for Map A (<xref rid="fig3" ref-type="fig">Figure 3G</xref>, <italic>P</italic> = 3.9 <italic>·</italic> 10<sup>−15</sup>, one-sided Wilcoxon signed-rank test), demonstrating that the presence of these memory traces did not result in a trade-off or loss of performance for Map A. This indicates that information from both maps coexisted in the activity of the familiar task as RNNs performed Task A2. Importantly, these memory traces could be specifically attributed to the process of learning Map B: when we reset the input and recurrent network weights after Task B or randomly reallocated the weight changes that occurred during Task B, the memory traces disappeared (<xref rid="figS3" ref-type="fig">Figure S3B,C</xref>). As a result, our models showed that both sustained uniform shifts and memory traces can arise spontaneously through sequential learning, potentially allowing the networks to fit both tasks in their activity repertoire even without any added mechanisms to reduce interference.</p>
</sec>
<sec id="s2c">
<title>The magnitude of memory traces, but not of uniform shifts, correlates with savings and initial behavioral error</title>
<p>Having established that our RNN models readily reproduce potential neural signatures of motor memories observed during BCI learning<sup><xref ref-type="bibr" rid="c18">18</xref></sup> and motor learning<sup><xref ref-type="bibr" rid="c11">11</xref></sup> experiments, we asked whether these signatures have a clear relationship with behavior.</p>
<p>First, we studied whether they captured the formation of a motor memory. Behaviorally, exposure to a perturbation leads to the formation of a motor memory, which is expressed by increased task performance upon re-exposure. One way motor memories manifest is through retention savings, defined as the increase in task performance immediately after (re-)exposure. We quantified these retention savings by comparing performance when networks were re-exposed to Map B during a new Task B2 compared to initial exposure to this same map during Task B1. Interestingly, the magnitude of the memory trace was closely related (<xref rid="figS4" ref-type="fig">Figure S4A</xref>) and highly correlated with the magnitude of the savings (<xref rid="fig4" ref-type="fig">Figure 4A</xref>, <xref rid="figS5" ref-type="fig">Figure S5D</xref>), but the magnitude of the uniform shift was not (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, <xref rid="fig4" ref-type="fig">Figure 4I</xref>). Thus, even if both signatures spontaneously arose during continual learning, their relationship with behavior is different, and only the memory trace seems to be related to immediate retention savings.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>The magnitude of memory traces but not that of uniform shifts correlates with retention savings.</title>
<p><bold>A)</bold> Mean memory trace across targets compared to the retention savings. Small circles, mean for each map (<italic>n</italic>=10 maps) for each random seed (<italic>n</italic>=8 random seeds); large circle, example shared across A-D. <bold>B)</bold> Mean memory trace across targets compared to the initial velocity error (MSE) when Task A1 activity is projected into the new Map Bs before learning in Task B1 (i.e. Task A1→Map B MSE). <bold>C)</bold> Uniform shift from Task A1 to Task B1 across targets compared to the retention savings. <bold>D)</bold> Uniform shift from Task A1 to Task B1 across targets compared to the initial velocity error (MSE).</p></caption>
<graphic xlink:href="646788v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Having defined these relationships, we asked whether we could predict their magnitude from task properties alone. The BCI learning paradigm, combined with our modeling setup that allows us to run many “experiments”, offers a unique opportunity to answer this question. We characterized the Map B perturbations based on the directions of the neural manifold they affected (<xref rid="figS5" ref-type="fig">Figure S5A</xref>), since recent theoretical work suggests that large variance directions may be easier to control than their lower variance counterparts<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. While “high variance perturbations” led to larger memory traces than “low variance perturbations” (<xref rid="figS5" ref-type="fig">Figure S5B</xref>), such difference in variance did not affect the magnitude of the uniform shifts (<xref rid="figS5" ref-type="fig">Figure S5C</xref>). Other measures such as the principal angles between BCI maps, or their relative amount of neural variance explained also had inexistent or weak associations with uniform shifts and memory traces (<xref rid="figS5" ref-type="fig">Figure S5E,F,G,J,K,L</xref>). In contrast, the initial behavioral disruption imposed by the perturbation was a strong predictor of the magnitude of the memory trace (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, <xref rid="figS5" ref-type="fig">Figure S5H</xref>), although it did not predict that of the uniform shift (<xref rid="fig4" ref-type="fig">Figure 4D</xref>, <xref rid="figS5" ref-type="fig">Figure S5M</xref>). This suggests that more information is retained about a given task as measured by the memory trace if the task requires more learning at the behavioral level.</p>
</sec>
<sec id="s2d">
<title>Experimentally hard-to-learn outside-manifold perturbations lead to larger memory traces and smaller uniform shifts</title>
<p>We have studied uniform shifts and memory traces for BCI maps that impose within-manifold perturbations that require new activity patterns within the existing neural manifold. Here, we extended our investigation to outside-manifold perturbations that require activity patterns that are not available within the existing manifold (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). Behaviorally, these perturbations are very different: while within-manifold perturbations can be learned within a few minutes <sup><xref ref-type="bibr" rid="c22">22</xref></sup>, outside-manifold perturbations cannot, requiring several days <sup><xref ref-type="bibr" rid="c10">10</xref></sup>.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Learning new outside-manifold maps leads to larger memory traces than learning new within-manifold maps due to less forgetting.</title>
<p><bold>A)</bold> Networks were trained on different Map Bs representing different within- and outside-manifold perturbations that had the same Task A1→Map B MSE. <bold>B)</bold> Map B MSE during initial learning of the new Map B in Task B1. Lines and shaded surfaces, mean and 95% confidence interval across 10 maps for each perturbation across networks of different seeds (<italic>n</italic>=8 random seeds). Lines, different maps for different seeds. <bold>C)</bold> Map B MSE during reversion to the familiar Map A in Task A2. <bold>D)</bold> Mean memory trace across targets compared to the Task A1→Map B MSE. Circles, mean for each map for each random seed. Note that points with the same Task A1→Map B MSE belong to the same seed since this was controlled for. <bold>E)</bold> Uniform shift from Task A1 to Task B1 across targets compared to the Task A1→Map B MSE. <bold>F)</bold> Uniform shift from Task B1 to Task A2 compared to forgetting, defined as the decrease in performance from Task B1→Map B to Task A2→Map B, for within-manifold perturbations. Lines and shaded surfaces, mean and 95% confidence interval across different maps for different seeds; circles, different maps for different seeds.</p></caption>
<graphic xlink:href="646788v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Having established that the initial task error was a strong predictor of memory trace magnitude for within-manifold perturbations, we asked how uniform shifts and memory traces change when we control for this factor for both inside- and outside-manifold perturbations (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). To do so, we projected Task A1 activity into thousands of within-manifold perturbation and outside-manifold perturbation BCI maps and selected a subset of maps of each type with the same MSE to use as the new Map Bs (<italic>n</italic> = 10 for each perturbation type, Methods, <xref rid="figS6" ref-type="fig">Figure S6A</xref>). Recapitulating experimental findings <sup><xref ref-type="bibr" rid="c22">22</xref></sup>, outside-manifold perturbations were harder to learn than within-manifold perturbations, with larger errors at the end of Task B1 when the same number of training trials were used to learn the maps (<xref rid="fig5" ref-type="fig">Figure 5B</xref>, <italic>P</italic> = 8.2<italic>·</italic>10<sup>−30</sup>, one-sided Student’s t-test).</p>
<p>Learning outside-manifold maps less well would suggest smaller initial retention savings of these maps compared to within-manifold maps. However, when we examined the memory traces at the end of Task A2, outside-manifold perturbation led to surprisingly larger memory traces and savings than within-manifold maps (<xref rid="fig5" ref-type="fig">Figure 5D</xref>, <italic>P</italic> = 2.3 <italic>·</italic> 10<sup>−7</sup>, one-sided Mann-Whitney U test; <xref rid="figS6" ref-type="fig">Figure S6C</xref>, <xref rid="figS7" ref-type="fig">Figure S7A</xref>).</p>
<p>To reconcile these findings, we examined performance for the new maps during re-exposure to the original map (i.e., Task A2) and saw that the performance for within-manifold maps dropped more quickly than that for outside-manifold maps (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). This suggests that during Task A2, information about within-manifold maps was more easily overwritten and forgotten, likely because these maps were too similar to coexist together without interference. To directly examine this, we trained networks initialized from the same random seeds to learn both Map A and Map B simultaneously right after initialization. RNNs had more difficulty learning two within-manifold maps simultaneously than one within-manifold and one outside-manifold map, showing that it was harder for within-manifold maps to coexist (<xref rid="figS7" ref-type="fig">Figure S7B</xref>). This supports our interpretation that the larger memory trace after learning an outside-manifold perturbation mapping happened because, although these maps were learned less well in the first place, more information about the new maps was retained after switching back to the familiar task (<xref rid="fig5" ref-type="fig">Figure 5C</xref>), leading to greater retention savings. The greater savings during outside-manifold perturbations were again correlated with greater memory traces (<xref rid="figS6" ref-type="fig">Figure S6B</xref>; note that this effect did not result from a tradeoff in performance for the familiar map, <xref rid="figS8" ref-type="fig">Figure S8</xref>). In contrast, our second potential neural signature of motor memory exhibited the opposite behavior: the magnitude of the uniform shift from Task A1 to Task B1 was smaller for outside-manifold perturbations than within-manifold perturbations (<xref rid="fig5" ref-type="fig">Figure 5E</xref>, <italic>P</italic> = 1.6 <italic>·</italic> 10<sup>−22</sup>, one-sided Mann-Whitney U test; <xref rid="figS2" ref-type="fig">Figure S2E,F</xref>), and was not directly correlated with retention savings (<xref rid="figS6" ref-type="fig">Figure S6H</xref>). However, the degree of forgetting of Map B was correlated with retraction along the uniform shift axis during Task A2 for within-manifold perturbations (<xref rid="fig5" ref-type="fig">Figure 5F</xref>), showing that, the degree of retention of the uniform shift, rather than its absolute length, was indicative of savings. Together, these results show that more similar tasks, like within-manifold perturbation maps, may be initially easier to learn, but may also cause greater interference and less retention during continual learning.</p>
</sec>
<sec id="s2e">
<title>Explicit context cues alter memory traces, uniform shifts, and their relationship to savings</title>
<p>Since forgetting due to interference was a driving factor in reducing savings for within-manifold perturbations, we examined ways to reduce interference to promote continual learning. Thus far, we have not introduced any explicit mechanisms to separate the activity patterns for different tasks. However, experimental work shows that humans and monkeys are better at learning tasks in blocks than when they are interleaved<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup>, a phenomenon that is likely due to the incorporation of contextual information<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup>. Based on this, we predicted that explicitly cuing our RNNs on the identity of the task would allow different task-specific activity patterns to be learned sequentially without overwriting. This intuition was built on the notion that an explicit cue signal would promote activity patterns for the different tasks to emerge in different parts of activity space.</p>
<p>To test this intuition, we added explicit context cues as inputs to our networks that indicated whether they were controlling the familiar Map A or the new Map B using one-hot encoded binary vectors (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). We trained networks on the same sequential learning setup as before, but now with this additional context cue: Context A was used during baseline learning, Task A1, and Task A2, while Context B was used during Task B1. Using this setup, we asked how continual learning was modulated by the degree of discriminability between contexts—since such discriminability will likely indicate the degree that activity is driven to occupy different parts of state space—, and how this would translate into differences in uniform shifts and memory traces. We controlled the degree of discriminability between contexts by multiplying the binary vectors representing the context (Map A or Map B) by different factors to create context cues of varying magnitudes, and separately trained networks with each context magnitude.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Context cues alter uniform shifts and memory traces.</title>
<p><bold>A)</bold> Networks were given an additional context cue that signaled whether the familiar Map A or the new Map B was being used. The context signal was a one-hot encoded vector, and the vector was multiplied by different factors to create different context magnitudes. <bold>B)</bold> Uniform shifts from Task A1 to Task B1 for different context magnitudes. Line and shaded surfaces, mean and 95% confidence interval across 10 maps for each perturbation across networks of different seeds (<italic>n</italic>=8 random seeds). <bold>C)</bold> Normalized uniform shifts from Task A1 to Task B1 and from Task A1 to Task A2, averaged across targets for each session. Shifts were normalized by the length of the shift from Task A1 to Task B1. Opacities, different context magnitudes (most transparent for smallest magnitude). Dashed lines, mean for each context magnitude. <bold>D)</bold> Memory trace for different context magnitudes. Line and shaded surfaces, mean and 95% confidence interval across maps for each perturbation across networks of different seeds. <bold>E)</bold> MSE when activity from the end of Task B1, the beginning of Task B2, and the end of Task A2 are projected into Map B for different context magnitudes for within-manifold perturbations. Task B1→Map B MSE indicates learning difficulty, the difference between Task B1→Map B and beginning of Task B2→Map B MSE indicates forgetting, and the difference between the beginning of Task B2→Map B and Task B1→Map B MSE indicates activity changes due to context. <bold>F)</bold> Same as E but for outside-manifold perturbations. <bold>G)</bold> Mean memory trace across targets compared to the retention savings. Small circles, mean for each map for each random seed.</p></caption>
<graphic xlink:href="646788v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As a result of introducing context, the trends in uniform shifts and memory traces that we previously saw dramatically changed. First, uniform shifts were still evident when learning the new map, and the magnitude of the shifts increased with greater context magnitudes (<xref rid="fig6" ref-type="fig">Figure 6B</xref>, <xref rid="figS2" ref-type="fig">Figure S2G</xref>,<xref rid="figS9" ref-type="fig">Figure S9A</xref>). However, the uniform shifts were not sustained: upon returning to the familiar Map A in Task A2, the uniform shift disappeared (<xref rid="fig6" ref-type="fig">Figure 6C</xref>, <xref rid="figS2" ref-type="fig">Figure S2G</xref>). This suggests that context signals pushed activity for the new map to occupy different spaces from the familiar map, but these changes were not incorporated into activity for the familiar map. Thus, context can lead to a dramatic change in this signature of learning.</p>
<p>Second, the inclusion of context cues also changed the trends in memory traces. Initially, with small context magnitudes, the trends were similar to those in networks without any context cues (<xref rid="fig5" ref-type="fig">Figure 5</xref>): outside-manifold maps led to greater memory traces than within-manifold maps (<xref rid="fig6" ref-type="fig">Figure 6D</xref>). Moreover, even though within-manifold perturbations were easier to learn during Task B1, there was also greater forgetting during Task A2, leading to greater Task A2→Map B MSE and smaller memory traces overall (<xref rid="fig6" ref-type="fig">Figure 6E,F</xref>; Recall that the memory trace is quantified as the increase in performance, or decrease in MSE, from Task A1→Map B to Task A2→Map B, so it is inversely proportional to the Task A2→Map B MSE when we have controlled for the initial output error, the Task A1→Map B MSE. We can thus use the trends in Task A2→Map B MSE to examine the trends in memory traces in <xref rid="fig6" ref-type="fig">Figure 6E</xref> and <xref rid="fig6" ref-type="fig">F</xref>).).</p>
<p>As the context magnitude increased, however, the magnitude of the memory traces became indistinguishable for outside-manifold perturbation and within-manifold maps (<xref rid="fig6" ref-type="fig">Figure 6D</xref>). This trend was caused by larger context magnitudes leading to more disparate activity patterns which occupied a different part of state space ((<xref rid="figS10" ref-type="fig">Figure S10</xref>), making learning new maps more difficult. With less learning (<xref rid="fig6" ref-type="fig">Figure 6E,F</xref>), there was less knowledge about the new map that could be retained, contributing to a general decrease in memory traces with larger context magnitudes (<xref rid="fig6" ref-type="fig">Figure 6 E,F</xref>). With activity patterns in a different part of activity space due to the increased context magnitude, there was also less overwriting and forgetting of Map B during Task A2 (<xref rid="fig6" ref-type="fig">Figure 6E,F</xref>). Thus, greater context magnitudes made within-manifold perturbations more similar to outside-manifold perturbations: both were equally difficult to learn and equally resistant to forgetting, with essentially perfect retention savings (<xref rid="fig6" ref-type="fig">Figure 6E,F</xref>). However, unlike our previous networks without context cues, here the savings were no longer closely correlated with the memory trace (<xref rid="fig6" ref-type="fig">Figure 6G</xref>, <xref rid="figS9" ref-type="fig">Figure S9</xref>, <xref rid="figS4" ref-type="fig">Figure S4A</xref>). With context however, the context cue shifted Task B2 into a different part of state space from Task A2, such that Task A2 activity became less relevant for Map B (<xref rid="figS4" ref-type="fig">Figure S4A</xref>). This context separation made larger context magnitudes lead to smaller memory traces of Task B even when there was perfect retention savings. Together, these results show that there can still be motor memories of a new task that lead to savings even when the uniform shift is no longer sustained or if the memory trace is decreased. With little or no context, memory traces were good indicators for initial retention savings and the degree of retention of the uniform shift was a good indicator for forgetting. However, these relationships deteriorated when a large context input was added.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Using RNNs, we showed that learning a new task naturally causes activity changes that are observed when returning to a familiar task, recapitulating results from several experimental learning studies in monkeys <sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. Learning new tasks both caused uniform shifts in the preparatory activity <sup><xref ref-type="bibr" rid="c11">11</xref></sup> that were sustained when reverting to a familiar task, and left memory traces that made activity for the familiar task more appropriate for the new task during movement execution<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. These changes appeared naturally, without the addition of an additional mechanism, and allowed information about both tasks to coexist in the same neural activity patterns, facilitating retention savings. However, when we added contextual inputs that separated the activity for the different tasks, both the retention of the uniform shift and the memory trace were reduced, showing that these signatures of motor memory may not be present even if savings are maintained.</p>
<sec id="s3a">
<title>Relationship to continual learning</title>
<p>Our results show that uniform shifts and memory traces indeed are signatures for continual learning, but have certain caveats. To measure the extent of continual learning, we have focused on the retention savings, the increase in performance immediately after re-exposure due to greater retention. While Sun et al<sup><xref ref-type="bibr" rid="c11">11</xref></sup> specifically correlated the magnitude of the uniform shift during relearning to learning rate savings, Losey et al<sup><xref ref-type="bibr" rid="c18">18</xref></sup> did not directly relate memory traces to any measures of savings. Here, we focused on retention savings since learning rate savings might be confounded by starting at different motor output errors at the beginning of re-exposure (Task B2) for our models trained with gradient descent. In our models, the magnitude of memory traces was closely correlated with retention savings following both within-manifold and outside-manifold perturbations (<xref rid="fig4" ref-type="fig">Figure 4A</xref>, <xref rid="figS4" ref-type="fig">Figure S4A</xref>, <xref rid="figS5" ref-type="fig">Figure S5D</xref>) for networks without contextual inputs, while the magnitude of uniform shifts was not (<xref rid="fig4" ref-type="fig">Figure 4D</xref>, <xref rid="figS5" ref-type="fig">Figure S5I</xref>). Rather, uniform shifts were indirectly related to retention savings: the relative retention of the uniform shifts was correlated with the amount of forgetting (<xref rid="fig5" ref-type="fig">Figure 5F</xref>), which affected the amount of savings for within-manifold perturbations (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). Thus, our simulations and analyses show that while memory traces and uniform shifts could both be related to continual learning, they are uncorrelated with each other (<xref rid="figS6" ref-type="fig">Figure S6B</xref>).</p>
<p>Importantly, while these measures could be useful signatures for continual learning, they are not always indicative of savings, and they are not necessary for continual learning to occur. By including context signals, we pushed activity to occupy different parts of neural activity space, leading to a decrease in memory traces and a lack of retention of uniform shifts (<xref rid="fig6" ref-type="fig">Figure 6C,D</xref>). This suggests that if context inputs or other “top-down” inputs drive tasks to use very different activity patterns for producing motor output across different tasks—in the extreme case, pushing different tasks to be performed by non-overlapping subpopulations of neurons—, these signatures can disappear even if motor memories of new tasks are retained. Here, our RNN specifically modeled neural activity in motor cortex which is directly used for BCI control, so such inputs may originate from activity upstream of motor cortex. However, motor learning and adaptation involve contributions from many regions other than motor cortex, including parietal cortex<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup>, cerebellum<sup><xref ref-type="bibr" rid="c41">41</xref>–<xref ref-type="bibr" rid="c43">43</xref></sup>, and basal ganglia<sup><xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>. While learning was constrained to our RNN model of motor cortex for the majority of this work, we also examined results when we added a second upstream RNN module and constrained learning to happen in this module. Even when learning was restricted to this upstream module, memory traces and sustained uniform shifts still arose (<xref rid="figS11" ref-type="fig">Figure S11</xref>), suggesting that the presence of these phenomena does not necessitate learning in the area that produces the behavioral output, in this case the motor cortex.</p>
</sec>
<sec id="s3b">
<title>Effects of context on within-manifold and outside-manifold perturbation learning</title>
<p>By modeling the BCI paradigm from Sadtler et al<sup><xref ref-type="bibr" rid="c22">22</xref></sup> with many different within- and outside-manifold perturbations, we were able to further examine the differences between these perturbations. Previous work has largely distinguished the two classes of perturbations based on factors such as learning timescales<sup><xref ref-type="bibr" rid="c22">22</xref></sup>, neural changes required for learning<sup><xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup>, reliance on correct feedback<sup><xref ref-type="bibr" rid="c27">27</xref></sup>, and modeled synaptic weight changes<sup><xref ref-type="bibr" rid="c47">47</xref></sup>. Here, we showed that there was variability in the memory traces and uniform shifts within each class of perturbations, highlighting differences between different perturbations in each class. While outside-manifold perturbations generally led to larger memory traces, some still overlapped with the distribution of within-manifold perturbations, suggesting that the changes that within- and outside-manifold perturbations impose on neural activity may live on a continuum, rather than fall within two discrete categories. Moreover, adding and increasing the magnitude of a context signal led to less forgetting for within-manifold perturbations (<xref rid="fig6" ref-type="fig">Figure 6E</xref>), causing them to adopt similar results as outside-manifold perturbations (<xref rid="fig6" ref-type="fig">Figure 6D,E,F</xref>).</p>
<p>These unexpected similarities between within- and outside-manifold perturbations in the presence of large context cues were likely caused by these cues pushing supposedly within-manifold perturbations to be outside the estimated manifold, as the original manifold estimated during baseline learning did not capture activity patterns imposed by different contexts. This is supported by large decreases in the activity’s overlap with the manifold when the context switched at the beginning of Task B, even before learning occurred (<xref rid="figS10" ref-type="fig">Figure S10</xref>). Previous work has shown that neural manifolds are generally nonlinear<sup><xref ref-type="bibr" rid="c48">48</xref></sup>, so large upstream inputs like context signals may drive activity into a part of activity space that is not captured by the same flat manifold estimated using a linear dimensionality reduction method—as we have done here and in the original experimental work. Thus, depending on the magnitude of these signals, we may explore activity that is more or less within the estimated flat manifold, rather than distinctly fully within or outside the actual nonlinear manifold. Indeed, this continuum between within- and outside-manifold perturbations may explain why even in experimental settings, some within-manifold perturbations required similar amounts of learning as some outside-manifold perturbations <sup><xref ref-type="bibr" rid="c22">22</xref></sup>. As a result, a wide range of inputs and tasks must be considered to capture better estimates of the “true” underlying manifold, to more fully capture different neural constraints on learning.</p>
<p>Our context manipulations highlighted tradeoffs between interference and generalization within continual learning. Greater context signals were able to prevent overwriting by avoiding interference, particularly when learning a new within-manifold map. However, by separating the activity for different maps, context signals forced RNNs to create new activity patterns for a new within-manifold Map B instead of generalizing existing patterns that were already available for the within-manifold Map A, making learning Map B slower and more difficult (<xref rid="fig6" ref-type="fig">Figure 6E,F</xref>). While our implementation of context forced activity to be in very different spaces, minimizing interference but abolishing generalization, actual neural activity in different real-world contexts may be organized so as to find a compromise between interference and generalization. For example, a comparison of different but related upper limb tasks indicated that their underlying patterns were shifted with respect to each other but these tasks still shared dynamical and geometric features<sup><xref ref-type="bibr" rid="c49">49</xref></sup>. How the brain accomplishes such a compromise with different contexts to promote continual learning would be an interesting avenue for future work.</p>
</sec>
<sec id="s3c">
<title>Neural mechanisms for uniform shifts and memory traces</title>
<p>While uniform shifts and memory traces were consistently present in our RNNs without additional context inputs, we did not explicitly model any mechanisms to maintain information about different tasks in the same activity. Simply through sequential learning through gradient descent, the RNNs were pushed towards solutions that accommodated information about both tasks, rather than reverting to the original state before the new task was learned. Without explicit mechanisms, the magnitude of uniform shifts and memory traces observed may serve as a baseline minimum. While we have shown that this baseline is affected by many factors like the initial behavioral error, the brain may implement additional mechanisms that might organize these neural activity changes in a way that further fosters continual learning.</p>
<p>Within the machine learning literature, many neuroscience-inspired methods exist that try to encourage continual learning without catastrophic forgetting of previous tasks, including methods that regulate levels of synaptic plasticity to protect existing knowledge <sup><xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c51">51</xref></sup>, push activity changes to be in orthogonal subspaces in the population space <sup><xref ref-type="bibr" rid="c52">52</xref>,<xref ref-type="bibr" rid="c53">53</xref></sup>, incorporate context to separate activity <sup><xref ref-type="bibr" rid="c54">54</xref>,<xref ref-type="bibr" rid="c55">55</xref></sup>, and use complementary learning systems that promote replay and consolidation<sup><xref ref-type="bibr" rid="c56">56</xref></sup>. By implementing different methods, contrasting their effects on uniform shifts and memory traces, and comparing them to experimental results, we may be able to pinpoint specific ways by which the brain may accommodate different neural activity patterns underpinning different behaviors. Since these motor memory signatures are directly experimentally measurable, they have the potential to serve as markers, enabling us to disentangle and distinguish between different types of learning.</p>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>Animals build an increasingly rich behavioral repertoire by acquiring different skills during development and a lifetime of learning. Yet, how the brain accommodates novel activity patterns necessary for the generation of these new behaviors remains an open question. Recent experimental work in monkeys has proposed two potential motor cortical signatures of motor memories underpinning learning. Here, we leveraged the full access afforded by RNN models to simulate this continual learning process and systematically examine how properties of these neural signatures relate to the formation of a motor memory. We found that these signatures emerge naturally as a byproduct of sequential learning, and that their relationship with motor memories is complex: their correlation depends on the details of the perturbation that causes learning and even on whether this perturbation is explicitly cued, as expected from recent behavioral work in humans. Thus, our work identifies fundamental challenges to define a clear relationship between neural activity changes and the formation of motor memories, but also provides insights and metrics to interpret future experimental results.</p>
</sec>
</sec>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Tasks</title>
<p>We trained RNNs to perform a standard delayed center-out reach task, which is commonly used in experimental settings to examine motor control. In experiments, the subject would control a cursor on a computer screen. The cursor starts in the center of a circle, and the subject has to reach one of eight possible targets evenly spaced around the circle. The subject is shown which target to move to with a target cue, but they have to delay movement until a later go cue –a design typically used to study motor planning. In our case, each trial lasted 2.0 <italic>s</italic>: the target cue was presented from the beginning of the trial, the go cue was presented at 0.2 <italic>s</italic>, and each reach lasted 1.5 <italic>s</italic>. We trained RNNs to produce velocities for different reaches, and the target velocities were defined by Gaussian profiles for each timestep <italic>t</italic> from the go cue, where each timestep equals 10 <italic>ms</italic> for a total of <italic>T</italic> = 150 time steps for each reach:
<disp-formula id="ueqn1">
<graphic xlink:href="646788v1_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The target velocities were 0 before the go cue and after the end of the reach.</p>
<p>To model how experimental subjects already know how to perform different reaches prior to experiments, we first pre-trained the RNNs to produce different reaches with targets randomly distributed around a circle of 9.4 <italic>cm</italic> during a baseline training phase. Then, to model the experimental task, we trained RNNs to perform reaches to eight targets evenly spaced around the circle using a BCI map. The RNNs were used to model motor cortical neural activity, and the BCI map transformed unit firing rates to x and y output velocities. To examine sequential learning, we trained RNNs to use different BCI maps in blocks: first with Map A (Task A1), then with a new Map B (Task B), then back to the familiar Map A (Task A2).</p>
</sec>
<sec id="s5b">
<title>BCI maps</title>
<p>Map A consisted of two transformations. First, a projection matrix <bold>C</bold> was used to project the firing rates from all <italic>n</italic> = 400 units onto an 8D space, or manifold, that captured the majority of the variance in the activity. This “intuitive manifold” was determined by performing principal component analysis (PCA) on the firing rates at the end of baseline training concatenated over trials. PCA finds <italic>n</italic> orthogonal basis vectors (principal components or PCs) that maximally capture the variance in the population activity, sorted by their corresponding amount of variance explained. We kept the eight leading PCs, which captured more than 80% of the variance in our network activity. Second, a decoder matrix <bold>D</bold> was used to project activity from the manifold to a two-dimensional space indicating the output velocities. The decoder was determined offline by performing linear regression of the produced velocities on the principal components. The full map was then given by:
<disp-formula id="ueqn2">
<graphic xlink:href="646788v1_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Map B was either a within-manifold perturbation or an outside-manifold perturbation of Map A. For within-manifold perturbations, the transformation from the manifold to the output velocities was disrupted with a permutation matrix <italic>η</italic><sub><bold>wm</bold></sub> inserted between the first and second linear transformations:
<disp-formula id="ueqn3">
<graphic xlink:href="646788v1_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To examine the effects of perturbing different dimensions of the manifold, we examined withinmanifold perturbations that permuted only the “high-variance” (first four) dimensions, only the “low-variance” (last four) dimensions, or all of the dimensions. In <xref rid="fig2" ref-type="fig">Figures 2</xref>-<xref rid="fig4" ref-type="fig">4</xref>, <xref rid="figS1" ref-type="fig">S1</xref>, <xref rid="figS2" ref-type="fig">S3</xref>, and <xref rid="figS11" ref-type="fig">S11</xref>, we examined the effects of 10 maps for random permutations of all dimensions. In <xref rid="figS2" ref-type="fig">Figure S2B-D</xref> and <xref rid="figS5" ref-type="fig">S5</xref>, we examined 24 maps each from all possible permutations (4! = 24) of the “high-variance” and “low-variance” dimensions, and 10 maps for random permutations of all dimensions.</p>
<p>For outside-manifold perturbations, the transformation from the firing rates to the manifold was disrupted with a permutation matrix <italic>η</italic><sub><bold>OM</bold></sub> inserted before the first linear transformation:
<disp-formula id="ueqn4">
<graphic xlink:href="646788v1_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Note that <italic>η</italic><sub><italic>W M</italic></sub> is a 8×8 permutation matrix such that there are 8 factorial (40,320) possible <italic>η</italic><sub><italic>W M</italic></sub> s, while <italic>η</italic><sub><italic>OM</italic></sub> is a 400×400 permutation matrix such that there are 400 factorial (¿10868) possibilities. To equalize the dimensions of the search space for within- and outside-manifold perturbations, we followed methods similar to Sadtler et al. 2014. In short, we separated the 400 units into 8 groups and only considered outside-manifold perturbations where we permuted these groups, keeping all units in the same group together. We then considered all within-manifold maps and this limited set of outside-manifold maps as candidate Map Bs. Since we saw that the memory trace was highly correlated with the Task A1→Map B output MSE (<xref rid="fig4" ref-type="fig">Figure 4B</xref>), in <xref rid="fig5" ref-type="fig">Figures 5</xref>, <xref rid="fig6" ref-type="fig">6</xref>, <xref rid="figS2" ref-type="fig">S2E-G</xref>, and <xref rid="figS6" ref-type="fig">S6</xref>-<xref rid="figS10" ref-type="fig">S10</xref>, we controlled for this factor by calculating the Task A1→Map B output MSE for all candidate Map Bs and choosing the 10 within- and 10 outside-manifold maps with MSEs closest to the median of all values.</p>
</sec>
<sec id="s5c">
<title>Neural network model</title>
<sec id="s5c1">
<title>Network architecture</title>
<p>The model dynamics were given by:
<disp-formula id="ueqn5">
<graphic xlink:href="646788v1_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>x</italic><sub><italic>i</italic></sub> was the hidden state of the <italic>i</italic>th unit and <italic>r</italic><sub><italic>i</italic></sub> was the corresponding firing rate following <italic>tanh</italic> activation of <italic>x</italic><sub><italic>i</italic></sub>. The network had <italic>N</italic> = 400 units, <italic>I</italic> inputs, and <italic>F</italic> = 2 feedback signals. The time constant <italic>τ</italic> was set to 0.05 <italic>s</italic>, the integration time step <italic>dt</italic> was set to 0.01 <italic>s</italic>, and the noise <italic>η</italic> was randomly sampled from the Gaussian distribution 𝒩 (0, 0.1) for each time step. The initial states <bold>x</bold><sub><italic>t</italic>=0</sub> were sampled from the uniform distribution 𝒰 (0.1, 0.1). The network was fully recurrently connected, with the recurrent weights <bold>W</bold><sup><bold>rec</bold></sup> initially sampled from the Gaussian distribution <inline-formula><inline-graphic xlink:href="646788v1_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>g</italic> = 1.2. The time-dependent stimulus inputs <bold>s</bold> (specified below) were fed into the network, with input weights <bold>W</bold><sup><bold>in</bold></sup>. The delayed feedback <bold>f</bold> was equivalent to the two-dimensional output velocities <bold>v</bold>, but delayed by 120 ms, with feedback weights <bold>W</bold><sup><bold>fb</bold></sup>. The output <italic>v</italic> corresponded to <italic>x</italic> and <italic>y</italic> velocities of motor trajectories, and they were read out via the linear map:
<disp-formula id="ueqn6">
<graphic xlink:href="646788v1_ueqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
with the output weights <bold>W</bold><sup><bold>out</bold></sup>. <bold>W</bold><sup><bold>in</bold></sup>, <bold>W</bold><sup><bold>fb</bold></sup>, and <bold>W</bold><sup><bold>out</bold></sup> all initially sampled from the uniform distribution 𝒰 (−0.1, 0.1).</p>
<p>To model networks with and without context, two types of sustained inputs <bold>s</bold> were used. For the networks without context, <bold>s</bold> was four-dimensional and consisted of a one-dimensional hold signal, a two-dimensional target signal (2 <italic>cos θ</italic><sup><italic>target</italic></sup>, 2 <italic>sinθ</italic><sup><italic>target</italic></sup>) that specified the reaching direction <italic>θ</italic><sup><italic>target</italic></sup> of the target, and a one-dimensional length signal that specified the normalized length of the reach. All reaches were the same length so they were kept at 2, with the same magnitude as the target and hold signals. For the networks with context, <bold>s</bold> was six-dimensional, with the additional dimensions consisting of a two-dimensional one-hot encoded context signal. Context A (denoted by the vector (2, 0)) was used for baseline training, Task A1, and Task A2 while Context B (denoted by (0, 2)) was used for Task B1 and Task B2. To examine the effects of different context magnitudes, we multiplied these vectors by different factors: 0.5, 1, 2, 4, 6, 8, 10, 12, 14, and 16. For both types of inputs, the hold signal started at 2 and went to 0 at the go cue while the target, length, and context signals were presented throughout the trial. While all weights (<bold>W</bold><sup><bold>in</bold></sup>, <bold>W</bold><sup><bold>rec</bold></sup>, <bold>W</bold><sup><bold>fb</bold></sup>, <bold>W</bold><sup><bold>out</bold></sup>) were initialized at the beginning of baseline training, the input weights specific for Context B were also reinitialized at the beginning of Task A1 since the weights otherwise decreased dramatically towards 0 during baseline training when Context B was not in use.</p>
<p>To model upstream brain regions, we added an additional RNN module similar to Feulner et al<sup><xref ref-type="bibr" rid="c57">57</xref></sup>. This module received sustained inputs as described above, and was fully connected to the downstream module.</p>
</sec>
<sec id="s5c2">
<title>Model training</title>
<p>Networks were optimized to generate velocities of synthetic center-out reaches. To model baseline learning, we pretrained the networks on reaches with targets randomly distributed around a circle of 8 cm. During baseline training, networks had to learn all the weights (<bold>W</bold><sup><bold>in</bold></sup>, <bold>W</bold><sup><bold>rec</bold></sup>, <bold>W</bold><sup><bold>fb</bold></sup>, <bold>W</bold><sup><bold>out</bold></sup>) using the Adam optimizer with an initial learning rate <italic>l</italic> = 10<sup>−4</sup>, first moment estimates decay rate <italic>β</italic><sub>1</sub> = 0.9, second moment estimates decay rate <italic>β</italic><sub>2</sub> = 0.999, and epsilon <italic>ϵ</italic> = 1<italic>e</italic> − 8. Baseline training was implemented with 500 training trials and a batch size <italic>B</italic> = 64. Then, to model sequential learning, we trained the networks to control different BCI maps in sequential blocks: first with Map A (Task A1), then with a new Map B (Task B), then back to the familiar Map A (Task A2). During each block, networks had to learn <bold>W</bold><sup><bold>in</bold></sup>, <bold>W</bold><sup><bold>rec</bold></sup>, and <bold>W</bold><sup><bold>fb</bold></sup>, but the output weights <bold>W</bold><sup><bold>out</bold></sup> were fixed as either Map A (<bold>M</bold><sub><bold>A</bold></sub>) or Map B (<bold>M</bold><sub><bold>B</bold></sub>). The weights were learned using stochastic gradient descent with a fixed learning rate <italic>l</italic> = 5<sup>−3</sup>. We used a faster learning rate during sequential learning to model faster short-term learning compared to long-term baseline learning. Each block was implemented with 300 training trials and a batch size <italic>B</italic> = 64. To examine the effects of learning different tasks, we trained networks separately on different Map Bs following Task A1 (see <italic>Tasks</italic> for delineation of different Map Bs used).</p>
<p>To examine the effects of using the intuitive manifold as the familiar map, we also trained networks on the same pairs of Map As and Map Bs, but we used the Map Bs as the familiar map instead; in other words, training proceeded from baseline training, to Map B, to a new Map A, then back to the familiar Map B. To examine the effects of simultaneous rather than sequential learning, we simultaneously trained networks on the same pairs of maps during baseline learning following network initialization. All training configurations were performed on eight different networks initialized from different random seeds.</p>
<p>The loss <italic>L</italic> was the mean-squared error between the two-dimensional output and target velocities over each time step <italic>t</italic>, with the total number of time steps <italic>T</italic> = 200:
<disp-formula id="ueqn7">
<graphic xlink:href="646788v1_ueqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To produce network activity that aligned more closely to experimentally measured neural activity<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>, we added L2 regularization terms for the activity rates and network weights in the overall loss function LR used for optimization:
<disp-formula id="ueqn8">
<graphic xlink:href="646788v1_ueqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where
<disp-formula id="ueqn9">
<graphic xlink:href="646788v1_ueqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
and
<disp-formula id="ueqn10">
<graphic xlink:href="646788v1_ueqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>β</italic> = 0.2 and <italic>α</italic> = 0.001. Note that the loss recorded in the main text was the loss <italic>L</italic> before regularization. We clipped the gradient norm at 0.2 before applying the optimization step. Following training, we evaluated the networks at each block using testing data for 320 trials (40 per target).</p>
</sec>
</sec>
<sec id="s5d">
<title>Data analysis</title>
<sec id="s5d1">
<title>Distance between population activity means</title>
<p>To assess how learning a new task changes activity for a familiar task, we examined how network population activity changed from Task A1 to Task A2. We obtained the population activity by projecting network activity from each of the three task blocks into the eight-dimensional manifold. To compare our results with experimental results, we followed the methods outlined for the experimental analyses in Losey et al. 2024<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. In brief, we calculated the Mahalanobis distances between the population activity means for Task A1 and Task A2 for each target, using the covariance of Task A1 activity per target. As a control, we recalculated the Mahalanobis distances with shuffled task labels. To further demonstrate that activity is distinctly different between the tasks, we used linear discriminant analysis with five-fold cross-validation to classify the three task blocks using only the preparatory activity (first 200 <italic>ms</italic> of each trial).</p>
</sec>
<sec id="s5d2">
<title>Uniform shifts</title>
<p>To measure uniform shifts in the preparatory space, we followed the methods outlined in Sun et al. <sup><xref ref-type="bibr" rid="c11">11</xref></sup> In short, we examined population activity only in the preparatory period of each trial, which lasted 200 <italic>ms</italic> from trial start to the go cue. To estimate the manifold in the preparatory space, we applied PCA to the preparatory activity at the end of baseline training, then projected preparatory unit activity from all three task blocks into this manifold to get the preparatory population activity. We defined the uniform shift axis as the vector connecting the centroids of the time-averaged, trial-averaged population activity for all targets from Task A1 to Task B. We wanted to make sure that the uniform shift was not capturing changes due to behavioral output differences, so we performed targeted dimensionality reduction (TDR) on the preparatory activity and RNN outputs from Task A1 to identify a neural subspace that was predictive of behavioral output (velocities) in the upcoming reach, and we orthogonalized the uniform shift axis against the TDR axes. For each target, we projected the time-averaged, trial-averaged population activity onto the orthogonalized uniform shift axis to quantify the uniform shift per target. We obtained the normalized shifts by normalizing by the length of the target-averaged uniform shift from Task A1 to Task B. Thus, the average normalized uniform shift for all targets from Task A1 to Task B would be 1, and uniform shifts are sustained if they remain positive and close to 1 from Task A1 to Task A2.</p>
</sec>
<sec id="s5d3">
<title>Memory traces</title>
<p>To measure how learning a new task can leave a memory trace in the activity of a familiar task, we examined how activity patterns for a familiar task may become more useful for a new task after learning the new task. Specifically, we examined how activity patterns in Task A2 may be more appropriate for Map B than the patterns of Task A1. We projected activity from Task A1 and Task A2 into Map B offline and quantified the memory trace <italic>m</italic> as the increase in performance, or decrease in MSE, from Task A1 to Task A2:
<disp-formula id="ueqn11">
<graphic xlink:href="646788v1_ueqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where
<disp-formula id="ueqn12">
<graphic xlink:href="646788v1_ueqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
are the output velocities when Task A1 and Task A2 activity are projected into Map B, respectively.</p>
</sec>
<sec id="s5d4">
<title>Task properties</title>
<p>To understand how properties regarding the relationship between tasks may affect the magnitude of the memory trace, we examined two measures that quantified the relationship between Map A and Map B: the principal angles and the variance accounted for (VAF) between the maps.</p>
<p>Principal angles measure the relative alignment between two <italic>m</italic>-dimensional subspaces in an <italic>n</italic> dimensional space (<italic>m &lt; n</italic>) by quantifying the <italic>m</italic> angles between pairs of basis vectors in the subspaces that minimize the angles between them. We followed the methods from Bjorck and Golub<sup><xref ref-type="bibr" rid="c58">58</xref></sup> to calculate the principal angles: given the maps <bold>M</bold><sub><bold>A</bold></sub> and <bold>M</bold><sub><bold>B</bold></sub>, we perform QR decomposition to obtain orthonormal bases <bold>Q</bold><sub><bold>A</bold></sub> and <bold>Q</bold><sub><bold>B</bold></sub>. We then construct the inner product of these bases and perform singular value decomposition to obtain:
<disp-formula id="ueqn13">
<graphic xlink:href="646788v1_ueqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold>P</bold><sub><italic>A</italic></sub> and <bold>P</bold><sub><italic>B</italic></sub> specify the directions that minimize the principal angles and <bold>Σ</bold> is a diagonal matrix whose entries are the ranked cosines of the principal angles.</p>
<p>Variance accounted for (VAF) measures how much the variance associated with a given task can be accounted for when projected into the map for a different task. We computed the variance when Task A1 activity was projected into Map A and the variance when it was projected into the new bases for Map B found using principal angles:
<disp-formula id="ueqn14">
<graphic xlink:href="646788v1_ueqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
VAF was the ratio of these two variances:
<disp-formula id="ueqn15">
<graphic xlink:href="646788v1_ueqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To understand how the magnitude of the memory traces relate to the initial behavioral disruption imposed by a new task, we measured the initial error using the Task A1→Map B output MSE and the Task A1→Map B angular error. The Task A1→Map B output MSE has been described above, whereas the Task A1→Map B angular error was computed based on the difference between the angle to the target and the mean angle for the entire reach during movement.</p>
<p>To understand how learning a new task changed the activity with respect to the manifold, we calculated the manifold overlap<sup><xref ref-type="bibr" rid="c59">59</xref></sup> of Task B1 activity <bold>r</bold><sup><bold>B1</bold></sup> with the original intuitive manifold. We first calculated the covariance matrix <bold>S</bold> of <bold>r</bold><sup><bold>B1</bold></sup> and projected it into <bold>C</bold>, the first eight principal components that describe the intuitive manifold. To quantify the variance explained by the projection, we divided the trace of the projection by the trace of the corresponding covariance matrix:
<disp-formula id="ueqn16">
<graphic xlink:href="646788v1_ueqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s5d5">
<title>Control analysis</title>
<p>To verify that memory traces of Map B arose specifically due to the process of learning Task B, rather than due to changes that may arise from other processes, we ran two controls. First, we reset the weights learned in Task B (<bold>W</bold><sup><bold>in</bold></sup>, <bold>W</bold><sup><bold>fb</bold></sup>, and <bold>W</bold><sup><bold>rec</bold></sup>) to those at the end of Task A1 before Task A2 to check if memory traces may arise due to generally more learning of Map A. Second, we randomly redistributed the weight changes incurred during Task B before Task A2 to check if memory traces may arise due to random weight changes of the same magnitude that are unrelated to learning Map B.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s6">
<title>Code availability</title>
<p>All analyses were implemented using custom python code (Python 3.8) and open-source software. All the figures are reproducible by running Jupyter notebooks. Code to reproduce all the results is openly available on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/JoannaChang/motor_memory">https://github.com/JoannaChang/motor_memory</ext-link>.</p>
</sec>
<sec id="s9">
<title>Supplementary Figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplementary Figure S1:</label>
<caption><title>Neural activity changes from Task A1 to Task A2.</title>
<p><bold>A)</bold> Mahalanobis distance between population activity means for Task A1 and Task A2 for each target during preparation (first 200 ms of each trial, black), and for controls where the task labels were shuffled (gray). Dotted lines, means across 10 different maps for each random seed (<italic>n</italic>=8 random seeds). <bold>B)</bold> Classification accuracy for classifying tasks (Task A1, Task B1, Task B2) from neural activity during preparation using linear discriminant analysis. Large circle, mean across maps and random seeds; small circles, accuracy for each map for each random seed; colors, different random seeds; dotted line, chance accuracy level.</p></caption>
<graphic xlink:href="646788v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplementary Figure S2:</label>
<caption><title>Uniform shifts for different simulations.</title>
<p><bold>A)</bold> Uniform shifts can be quantified per target for each session rather than averaged across all targets for each session as in <xref rid="fig3" ref-type="fig">Fig. 3B</xref>. To quantify uniform shifts per target, mean target activity (circles) was projected on the uniform shift axis determined based on mean task activity (triangles) for Task A1 and Task B1. The uniform shifts were then the resulting distances between these projected mean states. <bold>B)</bold> Uniform shifts for 10 different maps based on within-manifold permutations that shuffled all manifold dimensions (used in <xref rid="fig2" ref-type="fig">Figs 2</xref>-<xref rid="fig4" ref-type="fig">4</xref>). <bold>B.i)</bold> Uniform shifts from Task A1 to Task B1 and Task A1 to Task A2 for each target. Dotted lines, means across 10 different maps for each random seed. <bold>B.ii)</bold> Uniform shift per target, normalized by the uniform shift averaged across targets from Task A1 to Task B1 for each session. <bold>C)</bold> Uniform shifts for 24 different maps based on within-manifold perturbations that shuffled the first four high-variance dimensions (used in <xref rid="fig4" ref-type="fig">Fig 4</xref>). <bold>C.i)</bold> Uniform shift averaged across targets (as in <xref rid="fig3" ref-type="fig">Fig. 3B</xref>). <bold>C.ii)</bold> Normalized uniform shift averaged across targets (as in <xref rid="fig3" ref-type="fig">Fig. 3C</xref>). <bold>C.iii-iv)</bold> Same as B.i-ii but for high-variance maps. <bold>D)</bold> Same as C but for 24 different maps based on within-manifold perturbations that shuffled the last four low-variance dimensions (used in <xref rid="figS6" ref-type="fig">Fig S6</xref>). <bold>E)</bold> Same as C but for 10 different within-manifold perturbations with the same initial output MSE (used in <xref rid="fig5" ref-type="fig">Fig 5</xref>). <bold>F)</bold> Same as C but for 10 different outside-manifold perturbations with the same initial output MSE (used in <xref rid="fig5" ref-type="fig">Fig 5</xref>). <bold>G)</bold> Same as C but for 10 outside-manifold and 10 within-manifold perturbations with the same initial output MSE for networks with different context magnitudes (used in <xref rid="fig6" ref-type="fig">Fig 6</xref>). Opacities, different context magnitudes (most transparent for smallest magnitude).</p></caption>
<graphic xlink:href="646788v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplementary Figure S3:</label>
<caption><title>Memory traces consistently arise and are due to learning Map B.</title>
<p><bold>A)</bold> MSE between the produced and target output for Task A1 and Task A2 activity when projected into Map A and Map B for each target in an example session (same session as <xref rid="fig3" ref-type="fig">Fig 3E</xref>). <bold>B-C)</bold> To show that memory traces arise specifically due to changes incurred from learning Map B, networks were trained on Task A2 without altered weights (standard, blue), with weight changes from Task B randomly reallocated before Task A2 (dark gray), or with weights reset before Task A2 to those at the end of Task A1 (light gray). <bold>B)</bold> Weight changes from end of Task A1 to beginning of Task A2. <bold>C)</bold> Memory traces for each target for each session for 10 different maps for each random seed (<italic>n</italic>=8 random seeds) after the weight manipulations outlined in B.</p></caption>
<graphic xlink:href="646788v1_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Supplementary Figure S4:</label>
<caption><title>Relationship between memory trace and retention savings.</title>
<p>A) Schematic for how memory trace and retention savings are measured based on MSEs at different stages of learning without context cues. Note that the memory trace is closely related to retention savings. The memory trace is defined as the increase in performance from Task A1→Map B to Task A2→Map B while retention savings is defined as the increase in performance from the beginning of Task B1→Map B to the beginning of Task B2→Map B. Without context cues, the network dynamics are identical at the end of Task A1 (Task A2) compared to the beginning of Task B1 (Task B2), other than minimal contributions from feedback, such that the memory trace is equivalent to retention savings, other than the contributions from feedback. <bold>B)</bold> Schematic for how memory trace and retention savings are measured based on MSEs at different stages of learning with context cues. Note that memory trace is not equivalent to retention savings when context cues are added because the network dynamics at the end of Task A1 (Task A2) are not identical to those at the beginning of Task B1 (Task B2).</p></caption>
<graphic xlink:href="646788v1_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Supplementary Figure S5:</label>
<caption><title>The magnitude of memory traces and uniform shifts compared to different measures.</title>
<p><bold>A</bold>) Networks were trained on different Map Bs representing within-manifold perturbations made from permutations in “all” manifold dimensions (blue, <italic>n</italic>=10 maps), only the first four “high-variance” dimensions (purple, <italic>n</italic>=24 maps), or only the last four “low-variance” dimensions (green, <italic>n</italic>=24 maps). Top: Permutation matrices for example maps for each category. Bottom: Output positions when Task A1 activity is projected into the new maps before learning in Task B1. <bold>B)</bold> Mean memory trace across targets. Big circle and error bars, means and 95% confidence intervals with bootstrapping across different maps for each random seed (n = 8 random seeds); small circles, different maps for different random seeds. <bold>C)</bold> Uniform shift from Task A1 to Task B1 across targets. <bold>D-H)</bold> Mean memory trace across targets compared to different measures: from left to right, retention savings, mean principal angle between Map A and Map B, variance accounted for (VAF) in Task A1 activity by Map B, Task A1→Map B angular error, and Task A1→Map B velocity MSE (Methods). Open circles, different maps for different random seeds; closed circles, examples for each permutation type shared across D-M. <bold>I-M)</bold> Uniform shift from Task A1 to Task B1 across targets compared to different measures.</p></caption>
<graphic xlink:href="646788v1_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Supplementary Figure S6:</label>
<caption><title>Relationship between memory trace and uniform shift and different factors once initial MSE is fixed.</title>
<p><bold>A)</bold> Task A1 activity was projected into thousands of candidate BCI maps representing within- and outside-manifold perturbations to obtain the initial Task A1→Map B MSE (Methods). Dashed lines, median for maps for each random seed. Opacities, different random seeds (<italic>n</italic>=8 random seeds). The 10 maps for each type of perturbation with MSEs closest to the median MSE across all maps for each seed were chosen for training. B) Uniform shift from Task A1 to Task B1 across targets compared to the mean memory trace across targets. Circles, mean for each map for each random seed. C-G) Mean memory trace across targets compared to the retention savings. <bold>C-G)</bold> Mean memory trace across targets compared to different measures: from left to right, retention savings, mean principal angle between Map A and Map B, variance accounted for (VAF) in Task A1 activity by Map B, Task A1→Map B angular error, and Task A1→Map B velocity MSE (Methods). Circles, different maps for different random seeds. <bold>H-L)</bold> Uniform shift from Task A1 to Task B1 across targets compared to different measures.</p></caption>
<graphic xlink:href="646788v1_figS6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Supplementary Figure S7:</label>
<caption><title>Learning within- and outside-manifold maps under different training regimes.</title>
<p><bold>A)</bold> Networks were trained with the same Map A and Map B pairings as in the main text (<xref rid="fig5" ref-type="fig">Fig 5</xref>), but the within- or outside perturbation (Map B) was used as the familiar map while the intuitive map (Map A) was used as the new map, such that learning proceeded from baseline learning, to Map B, to Map A, and back to Map B. Memory trace compared to Task A1→Map B MSE. Circles, mean for each map (<italic>n</italic>=10 maps per perturbation type) for each random seed (<italic>n</italic>=8 random seeds). <bold>B)</bold> Networks were trained with the same Map A and Map B pairings, but they were trained simultaneously during baseline learning after network initialization. Map A MSE compared to Map B MSE after baseline learning. Circles, mean for each map for each random seed; dashed line, identity line. Note that Map B had worse performance than Map A, especially for within-manifold perturbations.</p></caption>
<graphic xlink:href="646788v1_figS7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Supplementary Figure S8:</label>
<caption><title>Larger memory traces for outside-manifold learning are not due to performance tradeoffs.</title>
<p><bold>A)</bold> Memory traces compared to Task B1→Map B MSE. Greater memory traces for outside-manifold perturbations were not due to better initial learning of Map B in Task B1. Circles, mean for each map (<italic>n</italic>=10 maps per perturbation type) for each random seed (<italic>n</italic>=8 random seeds). <bold>B)</bold> Memory traces compared to Task B1→Map A MSE. Greater memory traces for outside-manifold perturbations were not due to worse retention of Map A in Task B1. <bold>C)</bold> Memory traces compared to Task A2→Map A MSE. Greater memory traces for outside-manifold perturbations were not due to worse relearning of Map A in Task A2.</p></caption>
<graphic xlink:href="646788v1_figS8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS9" position="float" fig-type="figure">
<label>Supplementary Figure S9:</label>
<caption><title>Context disrupts the correlation between memory traces and savings.</title>
<p><bold>A)</bold> Uniform shift from Task A1 to Task B1 across targets compared to mean retention savings for different context magnitudes. Circles, mean for each map (<italic>n</italic>=10 maps per perturbation type) for each random seed (<italic>n</italic>=8 random seeds). <bold>B)</bold> Mean memory trace across targets compared to mean retention savings for different context magnitudes.</p></caption>
<graphic xlink:href="646788v1_figS9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS10" position="float" fig-type="figure">
<label>Supplementary Figure S10:</label>
<caption><title>Manifold overlap for different context magnitudes.</title>
<p>Manifold overlap between the activity after learning the new map in Task B1 and the original intuitive manifold for within- and outside-manifold perturbations. Line and shaded surfaces, mean and 95% confidence interval across 10 maps for each perturbation across networks of different seeds (<italic>n</italic>=8 random seeds).</p></caption>
<graphic xlink:href="646788v1_figS10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS11" position="float" fig-type="figure">
<label>Supplementary Figure S11:</label>
<caption><title>Memory traces and sustained uniform shifts arise when learning is confined to upstream of motor cortex.</title>
<p><bold>A)</bold> Schematic for network models with an additional RNN module added upstream. Activity from the downstream module is directly used to control the BCI, similar to how activity from the motor cortex is often used to control BCIs. The additional upstream module models brain regions upstream of the motor cortex. We examined how memory traces and uniform shifts differed when learning was restricted to the upstream module (frozen downstream weights) compared to when learning was allowed in the downstream module (plastic downstream weights). <bold>B)</bold> Mean memory trace across targets. Big circle and error bars, means and 95% confidence intervals with bootstrapping for 10 different maps for each random seed (<italic>n</italic>=8 random seeds). Small circles, different maps; colors, different random seeds. Note that memory traces were consistently greater than zero (grey dotted line), even when learning is restricted upstream. <bold>C)</bold> Uniform shift from Task A1 to Task B1 across targets. <bold>D)</bold> Uniform shift from Task A1 to Task A2 across targets, normalized to the uniform shift from Task A1 to Task B1. Note that the uniform shifts were sustained since they were consistently greater than zero (grey dotted line) and close to the shift from Task A1 to Task B (purple dotted line).</p></caption>
<graphic xlink:href="646788v1_figS11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>J.C.C. received funding from the Wellcome Trust (grant 108908/Z/15/Z). C.C received funding from the BBSRC (BB/N013956/1 and BB/N019008/1), the EPSRC (EP/R035806/1), the Wellcome Trust (200790/Z/16/Z), and Simons Foundation (564408). J.A.G. received funding from the EPSRC (EP/T020970/1) and the European Research Council (ERC-2020-StG-949660). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
</ack>
<sec id="d1e1909" sec-type="additional-information">
<title>Additional information</title>
<sec id="s7">
<title>Author Contributions</title>
<p>J.C.C., J.A.G. and C.C. devised the project. J.C.C. ran simulations, analysed data and generated figures. J.C.C., C.C. and J.A.G. interpreted the data. J.C.C., C.C. and J.A.G. wrote the manuscript. All authors discussed and edited the manuscript. J.A.G. and C.C. jointly supervised the work.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Zuo</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>Repetitive motor learning induces coordinated formation of clustered dendritic spines in vivo</article-title>. <source>Nature</source> <volume>483</volume>, <fpage>92</fpage>–<lpage>95</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleim</surname>, <given-names>J. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cortical synaptogenesis and motor map reorganization occur during late, but not early, phase of motor skill learning</article-title>. <source>Journal of Neuroscience</source> <volume>24</volume>, <fpage>628</fpage>–<lpage>633</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Rapid formation and selective stabilization of synapses for enduring motor memories</article-title>. <source>Nature</source> <volume>462</volume>, <fpage>915</fpage>–<lpage>919</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rioult-Pedotti</surname>, <given-names>M.-S.</given-names></string-name>, <string-name><surname>Friedman</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hess</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Donoghue</surname>, <given-names>J. P.</given-names></string-name></person-group> <article-title>Strengthening of horizontal cortical connections following skill learning</article-title>. <source>Nature Neuroscience</source> <volume>1</volume>, <fpage>230</fpage>–<lpage>234</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wise</surname>, <given-names>S. P.</given-names></string-name> &amp; <string-name><surname>Murray</surname>, <given-names>E. A.</given-names></string-name></person-group> <article-title>Role of the hippocampal system in conditional motor learning: Mapping antecedents to action</article-title>. <source>Hippocampus</source> <volume>9</volume>, <fpage>101</fpage>–<lpage>117</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hikosaka</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Nakamura</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Sakai</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Nakahara</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>Central mechanisms of motor skill learning</article-title>. <source>Current opinion in neurobiology</source> <volume>12</volume>, <fpage>217</fpage>–<lpage>222</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morton</surname>, <given-names>S. M.</given-names></string-name> &amp; <string-name><surname>Bastian</surname>, <given-names>A. J.</given-names></string-name></person-group> <article-title>Cerebellar Contributions to Locomotor Adaptations during Splitbelt Treadmill Walking</article-title>. <source>Journal of Neuroscience</source> <volume>26</volume>, <fpage>9107</fpage>–<lpage>9116</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Imamizu</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Human cerebellar activity reflecting an acquired internal model of a new tool</article-title>. <source>Nature</source> <volume>403</volume>, <fpage>192</fpage>–<lpage>195</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perich</surname>, <given-names>M. G.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>L. E.</given-names></string-name></person-group> <article-title>Altered tuning in primary motor cortex does not account for behavioral adaptation during force field learning</article-title>. <source>Experimental Brain Research</source> <volume>235</volume>, <fpage>2689</fpage>–<lpage>2704</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oby</surname>, <given-names>E. R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>New neural activity patterns emerge with long-term learning</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>116</volume>, <fpage>15210</fpage>–<lpage>15215</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>X.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cortical preparatory activity indexes learned motor memories</article-title>. <source>Nature</source> <volume>602</volume>, <fpage>274</fpage>–<lpage>279</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Komiyama</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Learning-related fine-scale specificity imaged in motor cortex circuits of behaving mice</article-title>. <source>Nature</source> <volume>464</volume>, <fpage>1182</fpage>–<lpage>1186</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Holcomb</surname>, <given-names>H. H.</given-names></string-name></person-group> <article-title>Neural Correlates of Motor Memory Consolidation</article-title>. <source>Science</source> <volume>277</volume>, <fpage>821</fpage>–<lpage>825</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Park</surname>, <given-names>S.-W.</given-names></string-name>, <string-name><surname>Dijkstra</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Sternad</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Learning to never forget—time scales and specificity of long-term memory of a motor skill</article-title>. <source>Frontiers in Computational Neuroscience</source> <volume>7</volume>, <fpage>111</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miyachi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hikosaka</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Lu</surname>, <given-names>X.</given-names></string-name></person-group> <article-title>Differential activation of monkey striatal neurons in the early and late stages of procedural learning</article-title>. <source>Experimental Brain Research</source> <volume>146</volume>, <fpage>122</fpage>–<lpage>126</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vyas</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Neural population dynamics underlying motor learning transfer</article-title>. <source>Neuron</source> <volume>97</volume>, <fpage>1177</fpage>–<lpage>1186.e3</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Hadjiosif</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wong</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Haith</surname>, <given-names>A. M.</given-names></string-name></person-group> <article-title>Motor learning</article-title>. <source>Comprehensive Physiology</source> <volume>9</volume>, <fpage>613</fpage>–<lpage>663</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Losey</surname>, <given-names>D. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Learning leaves a memory trace in motor cortex</article-title>. <source>Current Biology</source> <volume>34</volume>, <fpage>1519</fpage>–<lpage>1531.e4</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name>, <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name></person-group> <article-title>Cortical control of arm movements: A dynamical systems perspective</article-title>. <source>Annual Review of Neuroscience</source> <volume>36</volume>, <fpage>337</fpage>–<lpage>359</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gallego</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Perich</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>L. E.</given-names></string-name> &amp; <string-name><surname>Solla</surname>, <given-names>S. A.</given-names></string-name></person-group> <article-title>Neural manifolds for the control of movement</article-title>. <source>Neuron</source> <volume>94</volume>, <fpage>978</fpage>–<lpage>984</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barack</surname>, <given-names>D. L.</given-names></string-name> &amp; <string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name></person-group> <article-title>Two views on the cognitive brain</article-title>. <source>Nature Reviews Neuroscience</source> <volume>22</volume>, <fpage>359</fpage>–<lpage>371</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sadtler</surname>, <given-names>P. T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Neural constraints on learning</article-title>. <source>Nature</source> <volume>512</volume>, <fpage>423</fpage>–<lpage>426</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marshel</surname>, <given-names>J. H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cortical layer–specific critical dynamics triggering perception</article-title>. <source>Science</source> <volume>365</volume>, <fpage>eaaw5202</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Okun</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Diverse coupling of neurons to populations in sensory cortex</article-title>. <source>Nature</source> <volume>521</volume>, <fpage>511</fpage>–<lpage>515</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Mussa-Ivaldi</surname>, <given-names>F. A.</given-names></string-name></person-group> <article-title>Adaptive representation of dynamics during learning of a motor task</article-title>. <source>Journal of Neuroscience</source> <volume>14</volume>, <fpage>3208</fpage>–<lpage>3224</lpage> (<year>1994</year>).</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feulner</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Perich</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Gallego</surname>, <given-names>J. A.</given-names></string-name></person-group> <article-title>A neural implementation model of feedback-based motor learning</article-title>. <source>Nature Communications</source> <volume>16</volume>, <fpage>1805</fpage> (<year>2025</year>).</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feulner</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Neural manifold under plasticity in a goal driven learning behaviour</article-title>. <source>PLOS Computational Biology</source> <volume>17</volume>, <fpage>e1008621</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sussillo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name> &amp; <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name></person-group> <article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title>. <source>Nature Neuroscience</source> <volume>18</volume>, <fpage>1025</fpage>–<lpage>1033</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hennequin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title>. <source>Neuron</source> <volume>82</volume>, <fpage>1394</fpage>–<lpage>1406</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Michaels</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Schaffelhofer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Agudelo-Toro</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Scherberger</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>A goal-driven mod-ular neural network predicts parietofrontal neural dynamics during grasping</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>117</volume>, <fpage>32124</fpage>–<lpage>32135</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Perich</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Gallego</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>De novo motor learning creates structure in neural activity that shapes adaptation</article-title>. <source>Nature Communications</source> <volume>15</volume>, <fpage>4084</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Gurnani</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Brunton</surname>, <given-names>B. W.</given-names></string-name></person-group> <article-title>Feedback control of recurrent dynamics constrains learning timescales during motor adaptation</article-title> <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2024.05.24.595772</pub-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heald</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Lengyel</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name></person-group> <article-title>Contextual inference underlies the learning of sensorimotor repertoires</article-title>. <source>Nature</source> <volume>600</volume>, <fpage>489</fpage>–<lpage>493</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ogasa</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Decision uncertainty as a context for motor memory</article-title>. <source>Nature Human Behaviour</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hennig</surname>, <given-names>J. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>How learning unfolds in the brain: Toward an optimization view</article-title>. <source>Neuron</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Humphreys</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Daie</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Botvinick</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name></person-group> <article-title>BCI learning phenomena can be explained by gradient-based optimization</article-title> <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2022.12.08.519453</pub-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flesch</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Balaguer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dekker</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Nili</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Comparing continual task learning in minds and machines</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>115</volume>, <fpage>E10313</fpage>–<lpage>E10322</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shea</surname>, <given-names>J. B.</given-names></string-name> &amp; <string-name><surname>Morgan</surname>, <given-names>R. L.</given-names></string-name></person-group> <article-title>Contextual interference effects on the acquisition, retention, and transfer of a motor skill</article-title>. <source>Journal of Experimental Psychology: Human Learning and Memory</source> <volume>5</volume>, <fpage>179</fpage>–<lpage>187</lpage> (<year>1979</year>).</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gréa</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A lesion of the posterior parietal cortex disrupts on-line adjustments during aiming movements</article-title>. <source>Neuropsychologia</source> <volume>40</volume>, <fpage>2471</fpage>–<lpage>2480</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mutha</surname>, <given-names>P. K.</given-names></string-name>, <string-name><surname>Sainburg</surname>, <given-names>R. L.</given-names></string-name> &amp; <string-name><surname>Haaland</surname>, <given-names>K. Y.</given-names></string-name></person-group> <article-title>Left parietal regions are critical for adaptive visuomotor control</article-title>. <source>Journal of Neuroscience</source> <volume>31</volume>, <fpage>6972</fpage>–<lpage>6981</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galea</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Vazquez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pasricha</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Orban de Xivry</surname>, <given-names>J.-J.</given-names></string-name> &amp; <string-name><surname>Celnik</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Dissociating the roles of the cerebellum and motor cortex during adaptive learning: The motor cortex retains what the cerebellum learns</article-title>. <source>Cerebral Cortex</source> <volume>21</volume>, <fpage>1761</fpage>–<lpage>1770</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taylor</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Klemfuss</surname>, <given-names>N. M.</given-names></string-name> &amp; <string-name><surname>Ivry</surname>, <given-names>R. B.</given-names></string-name></person-group> <article-title>An explicit strategy prevails when the cerebellum fails to compute movement errors</article-title>. <source>Cerebellum</source> <volume>9</volume>, <fpage>580</fpage>–<lpage>586</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rabe</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Adaptation to visuomotor rotation and force field perturbation is correlated to different brain areas in patients with cerebellar degeneration</article-title>. <source>Journal of Neurophysiology</source> <volume>101</volume>, <fpage>1961</fpage>–<lpage>1971</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname>, <given-names>S. B. E.</given-names></string-name>, <string-name><surname>Ko</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Ölveczky</surname>, <given-names>B. P.</given-names></string-name></person-group> <article-title>Distinct roles for motor cortical and thalamic inputs to striatum during motor skill learning and execution</article-title>. <source>Science Advances</source> <volume>8</volume>, <fpage>eabk0231</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dudman</surname>, <given-names>J. T.</given-names></string-name> &amp; <string-name><surname>Krakauer</surname>, <given-names>J. W.</given-names></string-name></person-group> <article-title>The basal ganglia: From motor commands to the control of vigor</article-title>. <source>Current Opinion in Neurobiology</source> <volume>37</volume>, <fpage>158</fpage>–<lpage>166</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golub</surname>, <given-names>M. D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Learning by neural reassociation</article-title>. <source>Nature Neuroscience</source> <volume>21</volume>, <fpage>607</fpage>–<lpage>616</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wärnberg</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Kumar</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Perturbing low dimensional activity manifolds in spiking neuronal networks</article-title>. <source>PLOS Computational Biology</source> <volume>15</volume>, <fpage>e1007074</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Fortunato</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Nonlinear manifolds underlie neural population activity during behaviour</article-title> <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.07.18.549575</pub-id> (<year>2023</year>).</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gallego</surname>, <given-names>J. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cortical population activity within a preserved neural manifold underlies multiple motor behaviors</article-title>. <source>Nature Communications</source> <volume>9</volume>, <fpage>4233</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A brain-inspired algorithm that mitigates catastrophic forgetting of artificial and spiking neural networks with low computational cost</article-title>. <source>Science Advances</source> <volume>9</volume>, <fpage>eadi2947</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kirkpatrick</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Overcoming catastrophic forgetting in neural networks</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>114</volume>, <fpage>3521</fpage>–<lpage>3526</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duncker</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Driscoll</surname>, <given-names>L. N.</given-names></string-name>, <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name>, <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Sussillo</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Organizing recurrent network dynamics by task-computation to enable continual learning</article-title>. <source>NeurIPS</source> <volume>11</volume> (<year>2020</year>).</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsuda</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Tye</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Siegelmann</surname>, <given-names>H. T.</given-names></string-name> &amp; <string-name><surname>Sejnowski</surname>, <given-names>T. J.</given-names></string-name></person-group> <article-title>A modeling framework for adaptive lifelong learning with transfer and savings through gating in the prefrontal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>117</volume>, <fpage>29872</fpage>–<lpage>29882</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Cui</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Yu</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Continual learning of context-dependent processing in neural networks</article-title>. <source>Nature Machine Intelligence</source> <volume>1</volume>, <fpage>364</fpage>–<lpage>372</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Masse</surname>, <given-names>N. Y.</given-names></string-name>, <string-name><surname>Grant</surname>, <given-names>G. D.</given-names></string-name> &amp; <string-name><surname>Freedman</surname>, <given-names>D. J.</given-names></string-name></person-group> <article-title>Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>115</volume>, <fpage>E10467</fpage>–<lpage>E10475</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumaran</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hassabis</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>McClelland</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated</article-title>. <source>Trends in Cognitive Sciences</source> <volume>20</volume>, <fpage>512</fpage>–<lpage>534</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feulner</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Small, correlated changes in synaptic connectivity may facilitate rapid motor learning</article-title>. <source>Nature Communications</source> <volume>13</volume>, <fpage>5163</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Björck</surname>, <given-names>Å.</given-names></string-name> &amp; <string-name><surname>Golub</surname>, <given-names>G. H.</given-names></string-name></person-group> <article-title>Numerical Methods for Computing Angles Between Linear Subspaces</article-title>. <source>Mathematics of Computation</source> <volume>27</volume>, <fpage>579</fpage>–<lpage>594</lpage> (<year>1973</year>).</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elsayed</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Lara</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name> &amp; <string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name></person-group> <article-title>Reorganization between preparatory and movement population responses in motor cortex</article-title>. <source>Nature Communications</source> <volume>7</volume>, <fpage>13239</fpage> (<year>2016</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106871.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ponte Costa</surname>
<given-names>Rui</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents <bold>valuable</bold> computational findings on the neural basis of learning new motor memories without interfering with previously learned behaviours using recurrent neural networks. The evidence supporting the claims of the authors is <bold>solid</bold>, but it would benefit from stronger and clearer links with experimental findings. This work will be of interest to computational and experimental neuroscientists working in motor learning.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106871.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work investigates the neural basis of continual motor learning, specifically how brains might accommodate new motor memories without interfering with previously learned behaviours. Mainly drawing inspiration from recent experimental studies in monkeys (Losey et al. and Sun, O'Shea et al.), the authors use recurrent neural networks (RNNs) to model sequential learning and examine the emergence and properties of two proposed neural signatures of motor memory: the &quot;uniform shift&quot; observed in preparatory activity and the &quot;memory trace&quot; observed in execution activity.</p>
<p>Strengths:</p>
<p>The work's main contribution is demonstrating that both uniform shifts and memory traces emerge in RNN models trained on a sequential BCI task, without requiring explicit additional mechanisms. The work explores the relationship between these signatures and behavioural savings, finding that the memory trace correlates with immediate retention savings in networks without context, while the uniform shift does not. The study also investigates how properties of the new task perturbation (within- vs. outside-manifold) and the presence of explicit context cues affect these signatures and their relationship to savings, generally finding that context signals and outside-manifold perturbations reduce savings by decreasing the inherent overlap in the neural strategies used to solve the task.</p>
<p>Weaknesses:</p>
<p>A primary weakness is the lack of clear definitions of the uniform shift and the memory trace, which are quite different metrics. Another primary weakness is that the task modelled is well-matched to the Losey et al. BCI paradigm, but not well-matched to the Sun, O'Shea et al.'s curl field paradigm, which is likely impacting some of the results, primarily the lack of a relationship between the uniform shift and motor memories. While there are improvements that could be made in this work, we think it is a demonstration that modeling learning in neural activity using neural network models continues to be a valuable tool, moving the field forward.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106871.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Chang et al. develop an RNN model of a BCI sequential learning task to examine the emergence of motor memory in the network. They use this system to quantify signatures of memory in continual learning, comparing their model with experimental observations from monkeys in prior publications. They show that the RNN model has signatures of shifts associated with sequential learning without any non-standard learning rules. This convincing study contributes to the knowledge of how motor memories are formed and shaped so that they are flexible in acquiring multiple behaviors.</p>
<p>Strengths:</p>
<p>This paper describes a well-designed numerical experiment that comes to a clear interpretation of a set of neural BCI experiments. The learning signatures the authors describe are interesting and well laid out, and the paper is well written. I find it insightful that the neural signature of motor learning emerges in a trained network without special learning rules.</p>
<p>Weaknesses:</p>
<p>The paper could be stronger if it made a stronger interpretation of how memory traces and uniform shifts are related. These two observations are taken from the BCI sequential learning literature and introduced by two different prior experimental papers on two different tasks, so it seems like there is an opportunity here to use the RNN model to unite these concepts, or define another metric for signatures of learning from a more normative approach.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106871.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors build and analyze recurrent neural network (RNN) models of brain-computer interface (BCI) multi-task learning, developing a valuable theoretical understanding of learning-related neural population phenomena (&quot;memory traces&quot; and &quot;uniform shifts&quot;) that have been reported in recent experimental studies of BCI and motor learning. The authors find that both phenomena emerge in their RNN models, and both correlate in some manner to learning-related behavioral phenomena (&quot;savings&quot; and &quot;forgetting&quot;). The authors also reveal that RNN training details, in particular, incorporating a task-indicating contextual input, can impact these population-level signatures of learning in RNN activity and their relation to those behavioral phenomena.</p>
<p>Strengths:</p>
<p>The text is well written, and the figures are clearly composed to convey the core concepts and findings. The RNN studies are elegant in their ability to recapitulate the memory trace and uniform shift phenomena, and further allow evaluations of novel scenarios that were not tested in the original corpus of the modeled animal experiments. The authors assess the sensitivity of their results to multiple approaches to RNN training, including training connectivity within a model of motor cortex, training only an upstream model that provides inputs to the motor cortex model, and providing task-indicating contextual inputs.</p>
<p>Weaknesses:</p>
<p>(1) It is unclear to what extent these RNN models operate in regimes relevant to biological neural networks (e.g., motor cortex), even at the neural-population level of abstraction studied here. Can the authors speak to how sensitive their results are to details that might speak to these operating regimes (e.g., signal-to-noise ratios or dimensionality of the RNN activities)?</p>
<p>(2) The work could be further strengthened by analyses demonstrating a more direct link between the neural population phenomena (memory trace and uniform shift) and the behavioral phenomena (savings, forgetting, etc). While in animal experiments, it can be exceedingly difficult to demonstrate links beyond correlative effects, the promise of a model is the relative tractability of implementing manipulations that might establish something closer to a causal link between phenomena. Is it the case that the memory trace is a task-dependent, mean-preserving rotation of the across-target task-relevant activity space? And that the uniform shift is a translation (non-mean-preserving) of that space? If so, could the authors design regularization schemes that specifically target each of these effects, enabling a more direct test of the functional role the effects play in driving behavioral phenomena?</p>
<p>Minor Comments:</p>
<p>The current study is based on BCI learning of center-out tasks, analogous to the Losey et al. task that initially reported the memory trace phenomena. However, a rather different behavioral task - involving arm movements through curl force fields - was employed by the Sun, O'Shea, et al. study that originally reported the uniform shift phenomena. How should readers interpret the current study's findings related to the uniform shift? To what extent might the behavioral implications of the uniform shift depend on the demands of the task, e.g., the biomechanics, day-to-day experiencing of different curl-field perturbations, etc.?</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106871.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chang</surname>
<given-names>Joanna C</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7725-0989</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Clopath</surname>
<given-names>Claudia</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4507-8648</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Gallego</surname>
<given-names>Juan A</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2146-0703</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the reviewers for their thoughtful comments, and we plan to implement many of their suggestions to improve the paper. We agree that the paper can benefit from clearer links between the two neural signatures (memory traces and uniform shifts) themselves, and between the neural signatures and behavioral phenomena. We will address these limitations in multiple ways. First, as the reviewers noted, RNN models have the potential to probe these relationships, so we plan to perform further analyses and modeling experiments to uncover any causal relationships. Second, we will also establish clearer definitions of the neural signatures and explore how these signatures can be unified using our models. Finally, we will compare the experimental paradigms between Losey et al and Sun, O’Shea et al, and discuss how differences between the paradigms may have impacted our observations, particularly in the context of other experimental and modeling papers.</p>
</body>
</sub-article>
</article>