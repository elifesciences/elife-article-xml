<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">101277</article-id>
<article-id pub-id-type="doi">10.7554/eLife.101277</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101277.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Endogenous Precision of the Number Sense</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6710-1488</contrib-id>
<name>
<surname>Prat-Carrabin</surname>
<given-names>Arthur</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5485-5280</contrib-id>
<name>
<surname>Woodford</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Economics, Columbia University</institution>, New York, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Zhang</surname>
<given-names>Hang</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>email: <email>arthurpc@fas.harvard.edu</email></corresp>
<fn id="n1" fn-type="present-address"><label>†</label><p>Department of Psychology, Harvard University, Cambridge, USA</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-09-20">
<day>20</day>
<month>09</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP101277</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-25">
<day>25</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-18">
<day>18</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.14.585091"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Prat-Carrabin &amp; Woodford</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Prat-Carrabin &amp; Woodford</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-101277-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>The behavioral variability in psychophysical experiments and the stochasticity of sensory neurons have revealed the inherent imprecision in the brain’s representations of environmental variables<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>. Numerosity studies yield similar results, pointing to an imprecise ‘number sense’ in the brain<sup><xref ref-type="bibr" rid="c7">7</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup>. If the imprecision in representations reflects an optimal allocation of limited cognitive resources, as suggested by efficient-coding models<sup><xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c26">26</xref></sup>, then it should depend on the context in which representations are elicited<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>. Through an estimation task and a discrimination task, both involving numerosities, we show that the scale of subjects’ imprecision increases, but sublinearly, with the width of the prior distribution from which numbers are sampled. This sublinear relation is notably different in the two tasks. The double dependence of the imprecision — both on the prior and on the task — is consistent with the optimization of a tradeoff between the expected reward, different for each task, and a resource cost of the encoding neurons’ activity. Comparing the two tasks allows us to clarify the form of the resource constraint. Our results suggest that perceptual noise is endogenously determined, and that the precision of percepts varies both with the context in which they are elicited, and with the observer’s objective.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<p>Quartz wristwatches gain or lose about half a second every day. Still, they are useful for what one typically needs to know about the time, and they sell for as low as five dollars. The most recent atomic clocks carry an error of less than one second over the age of the Universe, and they are used to detect the effect of Einstein’s theory of general relativity at a millimeter scale<sup><xref ref-type="bibr" rid="c28">28</xref></sup>; but they are much more expensive. Precision comes at a cost, and the kind of cost that one is willing to bear depends on one’s objective. Here we argue that in order to make the many decisions that stipple our daily lives, the brain faces—and rationally solves— similar tradeoff problems, which we describe formally, between an objective that may vary with the context, and a cost on the precision of its internal representations about external information.</p>
<p>As a considerable fraction of our decisions hinges on our appreciation of environmental variables, it is a matter of central interest to understand the brain’s internal representations of these variables—and the factors that determine their precision. An almost invariable behavioral pattern, in more than a century of studies in psychophysics, is that the responses of subjects exhibit variability across repeated trials. This variability has increasingly been thought to reflect the randomness in the brain’s representations of the magnitudes of the experimental stimuli<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref></sup>. Substantiating this view, studies in neuroscience exhibit how many of these representations seem to materialize in the activity of populations of neurons, whose patterns of firing of action potentials (electric signals) are well described by Poisson processes: typically, average firing rates are functions (‘tuning curves’) of the stimulus magnitude, which is therefore ‘encoded’ in an ensemble of action potentials, i.e., in a stochastic, and thus imprecise, fashion<sup><xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>. Similar results have been obtained in studies on the perception of numerical magnitudes. People are imprecise, when asked to estimate the ‘numerosity’ of an array of items, or in tasks involving Arabic numerals<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>; and the tuning curves of number-selective neurons in the brains of humans and monkeys have been exhibited<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. These findings point to the existence of a ‘number sense’ that endows humans (and some animals) with the ability to represent, imprecisely, numerical magnitudes <sup><xref ref-type="bibr" rid="c11">11</xref></sup>.</p>
<p>The quality of neural representations depends on the number of neurons dedicated to the encoding, on the specifics of their tuning curves, and on the duration for which they are probed. Models of <italic>efficient coding</italic> propose, as a guiding principle, that the encoding optimizes some measure of the fidelity of the representation, under a constraint on the available encoding resources<sup><xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c26">26</xref></sup>. While they make several successful predictions (e.g., more frequent stimuli are encoded with higher precision<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>), including in the numerosity domain<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>, several aspects of these models remain subject to debate<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>, although they shape crucial features of the predicted representations. First, in many studies, the encoding is assumed to optimize the mutual information between the external stimulus and the internal representations<sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>, but it is seldom the case that this is actually the objective that an observer needs to optimize. An alternative possibility is that the encoding optimizes the observer’s current objective, which may vary depending on the task at hand<sup><xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>. Second, the nature of the resource that constrains the encoding is also unclear, and several possible limiting quantities are suggested in the literature (e.g., the expected spike rate, the number of neurons<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c21">21</xref></sup>, or a functional on the Fisher information, a statistical measure of the encoding precision<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>). Third, most studies posit that the resource in question is costless, up to a certain bound beyond which the resource becomes depleted. Another possibility is that there is a cost that increases with increasing utilization of the resource (e.g., action potentials come with a metabolic cost<sup><xref ref-type="bibr" rid="c32">32</xref>–<xref ref-type="bibr" rid="c34">34</xref></sup>). Together, these aspects determine how the optimal encoding, and thus the resulting behavior, depend on the task and on the ‘prior’ (the stimulus distribution).</p>
<p>Hence we shed light on all three questions by manipulating, in experiments, the task and the prior. In an estimation task, subjects estimate the numbers of dots in briefly presented arrays. In a discrimination task, subjects see two series of numbers and are asked to choose the one with the highest average. In both tasks, experimental conditions differ by the size of the range of numbers that are presented to subjects (i.e., by the width of the prior). In each case we examine closely the variability of the subjects’ responses. We find that it depends on both the task and the prior. The scale of the subjects’ imprecision increases <italic>sublinearly</italic> with the width of the prior, and this sublinear relation is different in the two tasks. We reject ‘normalization’ accounts of the behavioral variability, and in the estimation task we find no evidence of ‘scalar variability’, whereby the standard deviation of estimates for a number is proportional to the number, as sometimes reported in numerosity studies. The behavioral patterns we exhibit are predicted by a model in which the imprecision in representations is adapted to the observer’s current task, whose expected reward it optimizes under a resource cost on the activity of the encoding neurons. The subjects’ imprecision is thus endogenously determined, through the rational allocation of costly encoding resources.</p>
<p>Our experimental results suggest, at least in the numerosity domain, a behavioral regularity — a task-dependent quantitative law of the scaling of the responses’ variability with the range of the prior — for which we provide a resource-rational account. Below, we present the results pertaining to the estimation task, followed by those of the discrimination task, before turning to our theoretical account of these experimental findings. The results we present here are obtained by pooling together the responses of the subjects; the analysis of individual data further substantiates our conclusions (see Methods).</p>
</sec>
<sec id="s2">
<title>Estimation task</title>
<p>In each trial of a numerosity estimation task, subjects are asked to provide their best estimate of the number of dots contained in an array of dots presented for 500ms on a computer screen (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). In all trials, the number of dots is randomly sampled from a uniform distribution, hereafter called ‘the prior’, but the width of the prior, <italic>w</italic>, is different in three experimental conditions. In the ‘Narrow’ condition, the range of the prior is [50, 70] (thus the width <italic>w</italic> is 20); in the ‘Medium’ condition, the range is [40, 80] (thus <italic>w</italic> = 40); and in the ‘Wide’ condition, the range is [30, 90] (thus <italic>w</italic> = 60; <xref rid="fig1" ref-type="fig">Fig. 1b</xref>). In all three conditions the mean of the prior (which is the middle of the range) is 60. As an incentive, the subjects receive for each trial a financial reward which decreases linearly with the square of their estimation error. Each condition comprises 120 trials, and thus often the same number is presented multiple times, but in these cases the subjects do not always provide the same estimates. We now examine this variability in subjects’ responses.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>Estimation task: the scale of subjects’ imprecision increases sublinearly with the prior width.</title>
<p><bold>a</bold>. Illustration of the estimation task: in each trial, a cloud of dots is presented on screen for 500ms. Subjects are then asked to provide their best estimate of the number of dots shown. <bold>b</bold>. Uniform prior distributions (from which the numbers of dots are sampled) in the three conditions of the task. <bold>c</bold>. Standard deviation of the responses of the subjects (solid lines) and of the best-fitting model (dotted lines), as a function of the number of presented dots, in the three conditions. For each prior, five bins of approximately equal sizes are defined; subjects’ responses to the numbers falling in each bin are pooled together (thick lines) or not (thin lines). <bold>d</bold>. Variance of subjects’ responses, as a function of the width of the prior (purple line) and of the squared width (grey line). Both lines show the same data; only the x-axis scale has been changed. <bold>e</bold>. Subjects’ coefficients of variations, defined as the ratio of the standard deviation of estimates over the mean estimate, as a function of the presented number, in the three conditions. <bold>f</bold>. Absolute error (solid line), defined as the absolute difference between a subject’s estimate and the correct number, and relative error (dashed line), defined as the ratio of the absolute error to the prior width, as a function of the prior width. In panels <bold>c-d</bold>, the responses of all the subjects are pooled together; error bars show twice the standard errors.</p></caption>
<graphic xlink:href="585091v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Studies on numerosity estimation with similar stimuli sometimes report that the standard deviation of estimates increases proportionally to the estimated number. This property, dubbed ‘scalar variability’, has been seen as a signature of numerical-estimation tasks, and more generally, of the ‘number sense’<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. However, looking at the standard deviation of estimates as a function of the presented number, we find that it is not well described by an increasing line. In the three conditions, the standard deviation seems to be maximal near the center of the range (60), and to slightly decrease for numbers closer to the boundaries of the prior (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>). Dividing each prior range in five bins of similar sizes, we compute the variance of estimates in each bin (see Methods). In the three conditions, the variance in the middle (third) bin is greater than the variances in the fourth and fifth bins (which contain larger numbers). These differences are significative (p-values of Levene’s tests of equality of variances: third vs. fifth bin, largest p-v. across the three conditions: 5e-6; third vs. fourth bin, Narrow condition: 0.009, Medium condition: 1.2e-5) except between the third and fourth bin in the Wide condition (p-v.: 0.12). This substantiates the conclusion that the standard deviation of estimates is not an increasing linear function of the number. Moreover, a hallmark of scalar variability is that the ‘coefficient of variation’, defined as the ratio of the standard deviation of estimates to the mean estimate, is constant<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. We find that in our experiment, it is decreasing for most of the numbers, in the three conditions (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>); this is consistent with the results of Ref.<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. We conclude that the scalar-variability property is not verified in our data.</p>
<p>In fact, the most striking feature of the variability of estimates is not how it depends on the number, but how it strongly depends on the width of the prior, <italic>w</italic> (<xref rid="fig1" ref-type="fig">Fig. 1c,d</xref>). For instance, with the numerosity 60, the standard deviation of subjects’ estimates is 4.2 in the Narrow condition, 6.8 in the Medium condition, and 8.4 in the Wide condition, although these estimates were all obtained after the presentations of the same number of dots (60). Testing for the equality of the variances of estimates across the three conditions, for each number contained in all three priors (i.e., all the numbers in the Narrow range,) we find that the three variances are significantly different, for all the numbers (largest Levene’s test p-value, across the numbers: 1e-7, median: 2e-15).</p>
<p>The variability of estimates increases with the width of the prior. This suggests that the imprecision in the internal representation of a number is larger when a larger range of numbers needs to be represented. This would be the case if internal representations relied on a mapping of the range of numbers to a normalized, bounded internal scale, and the estimate of a number resulted from a noisy readout (or a noisy storage) on this scale, as in ‘range-normalization’ models<sup><xref ref-type="bibr" rid="c37">37</xref>–<xref ref-type="bibr" rid="c42">42</xref></sup>. Consider for instance the representation of a number <italic>x</italic>, obtained through its normalization onto the unit range [0, 1], and then read with noise, as
<disp-formula id="eqn1">
<graphic xlink:href="585091v1_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>x</italic><sub><italic>min</italic></sub> is the lowest value of the prior, and <italic>ε</italic> a centered normal random variable with variance <italic>ν</italic><sup>2</sup>. Suppose that the estimate, <inline-formula><inline-graphic xlink:href="585091v1_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is obtained by rescaling the noisy representation back to the original range, i.e., <inline-formula><inline-graphic xlink:href="585091v1_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (we make this assumption for the sake of simplicity, but the argument we develop here is equally relevant for the more elaborate, Bayesian model we present below). The scale of the noise, given by <italic>ν</italic>, is constant in the normalized scale; thus in the space of estimates the noise scales with the prior width, <italic>w</italic>. If we allow, in addition to the noise in estimates, for some amount of independent motor noise of variance <inline-formula><inline-graphic xlink:href="585091v1_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the responses actually chosen by the subject, we obtain a model in which the variance of responses is <inline-formula><inline-graphic xlink:href="585091v1_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, i.e., an affine function of the <italic>square</italic> of the width of the prior.</p>
<p>With the numerosity 60, the variance of subjects’ estimates is 4.2<sup>2</sup> = 17.64 in the Narrow condition (<italic>w</italic> = 20), and 6.8<sup>2</sup> = 46.24 in the Medium condition (<italic>w</italic> = 40): given these two values, the affine relation just mentioned predicts that in the Wide condition (<italic>w</italic> = 60) the variance should be 9.7<sup>2</sup> = 93.91. We find instead that it is 8.4<sup>2</sup> = 70.56, i.e., about 25% lower than predicted, suggesting a sublinear relation between the variance and the square of the prior width. Indeed the variance of estimates does not seem to be an affine function of the square of the prior width (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>, grey line and grey abscissa). Our investigations reveal that instead, the variance is significantly better captured by an affine function of the width — and not of the squared width (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>, purple line and purple abscissa).</p>
<p>As an additional illustration of this result, for each of the five bins mentioned above and defined for the three priors, we compute the predicted variance of estimates in the Wide condition on the basis of the variances in the Narrow and Medium conditions, and resulting either from the hypothesis of an affine function of the squared width, <inline-formula><inline-graphic xlink:href="585091v1_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, or from the hypothesis of an affine function of the width,<inline-formula><inline-graphic xlink:href="585091v1_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The variances predicted with the former hypothesis all overestimate the variances of subjects’ responses (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, orange crosses), but the predictions of the latter hypothesis appear consistent with the behavioral data (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, orange circles).</p>
<p>We further investigate how the imprecision in internal representations depends on the width of the prior through a behavioral model in which responses results from a stochastic encoding of the numerosity, followed by a Bayesian decoding step. Specifically, the presentation of a number <italic>x</italic> results in an internal representation, <italic>r</italic>, drawn from a Gaussian distribution with mean <italic>x</italic> and whose standard deviation, <italic>νw</italic><sup><italic>α</italic></sup>, is proportional to the prior width raised to the power <italic>α</italic>; i.e., <italic>r</italic> |<italic>x</italic> ∼ <italic>N</italic> (<italic>x, ν</italic><sup>2</sup><italic>w</italic><sup>2<italic>α</italic></sup>), where <italic>ν</italic> is a positive parameter that determines the baseline degree of imprecision in the representation, and <italic>α</italic> is a non-negative exponent that governs the dependence of the imprecision on the width of the prior. The observer derives, from the internal representation <italic>r</italic>, the mean of the Bayesian posterior over <italic>x, x</italic><sup><italic>∗</italic></sup>(<italic>r</italic>) ≡ 𝔼[<italic>x</italic> |<italic>r</italic>]. We note that this estimate minimizes the squared-error loss, and thus maximizes the expected reward in the task. The selection of a response includes an amount of motor noise: the response, <inline-formula><inline-graphic xlink:href="585091v1_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is drawn from a Gaussian distribution centered on the Bayesian estimate, <italic>x</italic><sup><italic>∗</italic></sup>(<italic>r</italic>), with variance<inline-formula><inline-graphic xlink:href="585091v1_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, truncated to the prior range, and rounded to the nearest integer. This model has three parameters (<italic>σ</italic><sub>0</sub>, <italic>ν</italic>, and <italic>α</italic>).</p>
<p>The likelihood of the model is maximized for <italic>α</italic> = 0.48, a value close to 1<italic>/</italic>2 (and less close to 1), suggesting that the standard deviation is approximately a linear function of <inline-formula><inline-graphic xlink:href="585091v1_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (and the variance a linear function of <italic>w</italic>). The nested model obtained by fixing <italic>α</italic> = 1<italic>/</italic>2 yields a slightly poorer fit (which is expected for a nested model), but the difference in log-likelihood is small (0.38), and the Bayesian Information Criterion (BIC), a measure of fit that penalizes larger numbers of parameters<sup><xref ref-type="bibr" rid="c43">43</xref></sup>, is lower (i.e., better) by 8.70 for the constrained model with <italic>α</italic> = 1<italic>/</italic>2. This indicates that setting <italic>α</italic> = 1<italic>/</italic>2 provides a parsimonious fit to the data that is not significantly improved by allowing <italic>α</italic> to differ from 1<italic>/</italic>2. A different specification, <italic>α</italic> = 1, corresponds to a normalization model similar to the one described above, but here with a Bayesian decoding of the internal representation. The BIC of this model is higher by 244 than that with <italic>α</italic> = 1<italic>/</italic>2, indicating a much worse fit to the data. (Throughout, we report the models’ BICs even if they have the same number of parameters, so as to compare the values of a single metric). We emphasize that this large difference in BIC implies that the hypothesis <italic>α</italic> = 1 can be confidently rejected, in favor of the hypothesis <italic>α</italic> = 1<italic>/</italic>2 (in informal terms, it is not the case that the grey line in <xref rid="fig1" ref-type="fig">Fig. 1d</xref>, showing the variance vs. the squared width, only appears curved because of some sampling noise, in fact it is indeed <italic>not</italic> a straight line; while it is substantially more probable that the purple one, showing the variance vs. the width, corresponds indeed to a straight line).</p>
<p>The standard deviation of representations thus seems to increase linearly with the square root of the prior width,<inline-formula><inline-graphic xlink:href="585091v1_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The positive dependence results in larger errors when the prior is wider (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>, solid line). But the sublinear relation implies that the subjects in fact make smaller <italic>relative</italic> errors (relatively to the width of the prior), when the prior is wider. In the Narrow condition, the ratio of the average absolute error to the width of the prior, <inline-formula><inline-graphic xlink:href="585091v1_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is 19.7%, i.e., the size of errors is about one fifth of the prior width. This ratio decreases substantially, to 14.5% and 11.6% in the Medium and Wide conditions, respectively, i.e., the size of errors is about one ninth of the prior width in the Wide condition (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>, dashed line). In other words, while the size of the prior is multiplied by 3, the relative size of errors is multiplied by <inline-formula><inline-graphic xlink:href="585091v1_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and thus the absolute size of errors is multiplied by <inline-formula><inline-graphic xlink:href="585091v1_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. If subjects had the same relative sizes of errors in both the Narrow and the Wide conditions, their absolute error would be multiplied by 3; conversely the absolute error would be the same in the two conditions if the relative error was divided by 3. The behavior of subjects falls in between these two scenarios: they adopt smaller relative errors in the Wide condition, although not so much so as to reach the same absolute error as in the Narrow condition. Below, we show how this behavior is accounted for by a tradeoff between the performance in the task and a resource cost on the activity of the mobilized neurons. But first, we ask whether subjects exhibit, in a discrimination task, the same sublinear relation between the imprecision of representations and the width of the prior.</p>
</sec>
<sec id="s3">
<title>Discrimination task</title>
<p>In many decision situations, instead of providing an estimate, one is required to select the better of two options. We thus investigate experimentally the behavior of subjects in a discrimination task. In each trial, subjects are presented with two interleaved series of numbers, five red and five blue numbers, after which they are asked to choose the series that had the higher average (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). Each number is shown for 500ms. Two experimental conditions differ by the width of the uniform prior from which the numbers (both blue and red) are sampled: in the Narrow condition the range of the prior is [35, 65] (the width of the prior is thus <italic>w</italic> = 30) and in the Wide condition the range is [10, 90] (the width is thus <italic>w</italic> = 80; <xref rid="fig2" ref-type="fig">Fig. 2b</xref>). After each decision, subjects receive a number of points equal to the average that they chose. At the end of the experiment, the total sum of their points is converted to a financial reward (through an increasing affine function).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>Discrimination task: the scale of subjects’ imprecision increases with the prior width; the relation is sublinear, but different than in the estimation task.</title>
<p><bold>a</bold>. Illustration of the discrimination task: in each trial, subjects are shown five blue numbers and five red numbers, alternating in color, each for 500ms, after which they are asked to choose the color whose numbers have the higher average. <bold>b</bold>. Uniform prior distributions (from which the numbers of dots are sampled) in the two conditions of the task. <bold>c</bold>. Proportion of choices ‘red’ in the responses of the subjects (solid lines) and of the best-fitting model (dotted lines), as a function of the difference between the two averages, in the two conditions. <bold>d</bold>. Proportion of correct choices in subjects’ responses as a function of the absolute difference between the two averages divided by the square root of the prior width (left), by the prior width raised to the power 3<italic>/</italic>4 (middle), and by the prior width (right). The three subpanels are different representations of the same data. In panels <bold>c</bold> and <bold>d</bold>, the responses of all the subjects are pooled together; error bars show the 95% confidence intervals.</p></caption>
<graphic xlink:href="585091v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Subjects in this experiment sometimes make incorrect choices (i.e., they choose the color whose numbers had the lower average), but they make less incorrect choices when the difference between the two averages is larger, and the proportion of trials in which they choose ‘red’ is a sigmoid function of the difference between the average of the red numbers, <italic>x</italic><sub><italic>R</italic></sub>, and the average of the blue numbers, <italic>x</italic><sub><italic>B</italic></sub> (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). In the Narrow condition, this proportion reaches 60% when the difference in the averages is 1, and 90% when the difference is 7. In the Wide condition, we find that the slope of this psychometric curve is less steep: subjects reach the same two proportions for differences of about 2.4 and 12.6, respectively.</p>
<p>In the Wide condition, it thus requires a larger difference between the red and blue averages for the subjects to reach the same discrimination threshold; put another way, the same difference in the averages results in more incorrect choices in the Wide condition than in the Narrow condition. As with the estimation task, this suggests that the degree of imprecision in representations is larger when the range of numbers that must be represented is larger. To estimate this quantitatively, we turn to the predictions of the model presented above, here considered in the context of the discrimination task: in this model, the average <italic>x</italic><sub><italic>C</italic></sub>, where <italic>C</italic> is ‘blue’ or ‘red’ (denoted by <italic>B</italic> and <italic>R</italic>, respectively), results in an internal representation, <italic>r</italic><sub><italic>C</italic></sub>, drawn from a Gaussian distribution with mean <italic>x</italic><sub><italic>C</italic></sub> and whose variance, <italic>ν</italic><sup>2</sup><italic>w</italic><sup>2<italic>α</italic></sup>, is proportional to the prior width raised to the exponent 2<italic>α</italic>, i.e., <italic>r</italic><sub><italic>C</italic></sub>|<italic>x</italic><sub><italic>C</italic></sub> ∼ <italic>N</italic> (<italic>x</italic><sub><italic>C</italic></sub>, <italic>ν</italic><sup>2</sup><italic>w</italic><sup>2<italic>α</italic></sup>). Given the (independent) representations <italic>r</italic><sub><italic>B</italic></sub> and <italic>r</italic><sub><italic>R</italic></sub>, the subject, optimally, compares the Bayesian estimates for each quantity, <italic>x</italic><sup><italic>∗</italic></sup>(<italic>r</italic><sub><italic>B</italic></sub>) and <italic>x</italic><sup><italic>∗</italic></sup>(<italic>r</italic><sub><italic>R</italic></sub>), and chooses the greater one. As the Bayesian estimate is an increasing function of the representation, the probability that the subject choose ‘red’, conditional on two averages <italic>x</italic><sub><italic>B</italic></sub> and <italic>x</italic><sub><italic>R</italic></sub>, is the probability that <italic>r</italic><sub><italic>R</italic></sub> be larger than <italic>r</italic><sub><italic>B</italic></sub>, i.e.,
<disp-formula id="eqn2">
<graphic xlink:href="585091v1_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where Φ is the cumulative distribution function of the standard normal distribution.The choice probability is thus predicted to be a function of the ratio <inline-formula><inline-graphic xlink:href="585091v1_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of the difference between the two averages over the width of the prior raised to the power <italic>α</italic>, and therefore the same choice probability should be obtained across conditions as long as this ratio is the same. In <xref rid="fig2" ref-type="fig">Figure 2d</xref>, we show for different values of <italic>α</italic> the subjects’ proportions of correct responses as a function of the absolute value of this ratio, so as to be able to examine closely the difference between the resulting choice curves in the two conditions. The case <italic>α</italic> = 1 corresponds, as above, to the hypothesis that the standard deviation of internal representations is a linear function of the width, <italic>w</italic>, i.e., a normalization of the numbers by the width of the prior. But we find that the proportion of correct choices as a function of the ratio |<italic>x</italic><sub><italic>R</italic></sub> −<italic>x</italic><sub><italic>B</italic></sub> |<italic>/w</italic> is greater in the Wide condition than in the Narrow condition (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>, last panel). In other words, in the Wide condition the subjects are more sensitive to the normalized difference than in the Narrow condition. This suggests that between the Narrow and the Wide conditions, the imprecision in representations does not change in the same proportions as does the prior width; specifically, it suggests a sublinear relation between the scale of the imprecision and the width of the prior.</p>
<p>As seen in the previous section, the behavioral data in the estimation task precisely suggest such a sublinear relation, and more precisely point to the exponent <italic>α</italic> = 1<italic>/</italic>2, i.e., to a linear relation between the standard deviation and the square-root of the width,<inline-formula><inline-graphic xlink:href="585091v1_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. But the proportion of correct choices as a function of the corresponding ratio, <inline-formula><inline-graphic xlink:href="585091v1_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is greater in the Narrow condition than in the Wide condition (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>, first panel). The sublinear relation, thus, is not the same in the two tasks; and the data suggest in the case of the discrimination task an exponent <italic>α</italic> greater than 1<italic>/</italic>2, but lower than 1. Indeed, we find that the choice curves in the two conditions match very well with <italic>α</italic> = 3<italic>/</italic>4 (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>, middle panel).</p>
<p>Model fitting substantiates this result. We add to our model (in which the probability of choosing ‘red’ is given by <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref>) the possibility of ‘lapse’ events, in which either response is chosen with probability 50%; an additional parameter, <italic>η</italic>, governs the probability of lapses. (We reach the same conclusions with a model with no lapse, but this model with lapses yields a better fit; see Methods.) The BIC of this model with <italic>α</italic> = 3<italic>/</italic>4 is lower (i.e., better) by 44.1 than that with <italic>α</italic> = 1<italic>/</italic>2, and by 18.3 than that with <italic>α</italic> = 1, indicating strong evidence rejecting the hypotheses <italic>α</italic> = 1<italic>/</italic>2 and <italic>α</italic> = 1, in favor instead of the hypothesis of an exponent <italic>α</italic> equal to 3<italic>/</italic>4. Notwithstanding the theoretical reasons, presented below, that motivate our focus on this specific value of the exponent in addition to the good fit to the data, we can let <italic>α</italic> be a free parameter, in which case its best-fitting value is 0.80 (and thus close to 3<italic>/</italic>4). This model’s BIC is however higher (i.e., worse) by 7.9 than that of the model with <italic>α</italic> fixed at 3<italic>/</italic>4, which indicates strong evidence<sup><xref ref-type="bibr" rid="c44">44</xref></sup> in favor of the equality <italic>α</italic> = 3<italic>/</italic>4. In sum, our best-fitting model is one in which the standard deviation of the internal representations is a linear function of the prior width raised to the power 3/4. As with the estimation task, this sublinear relation implies that subjects are relatively more precise when the prior is wider. This allows them to achieve a significantly better performance in the Wide condition than in the Narrow condition (with 80.2% and 77.4% of correct responses, respectively; p-value of Fisher’s exact test of equality of the proportions: 9.5e-5).</p>
<sec id="s3a">
<title>Task-optimal endogenous precision</title>
<p>The subjects’ behavioral patterns in the estimation task and in the discrimination task suggest that the scale of the imprecision in their internal representations increases sublinearly with the range of numerosities used in a given experimental condition. Specifically, the scale of the imprecision seems to be a linear function of the prior width raised to the power 1<italic>/</italic>2, in the estimation task, and raised to the power 3<italic>/</italic>4, in the discrimination task. We now show that these two exponents, 1<italic>/</italic>2 and 3<italic>/</italic>4, arise naturally if one assumes that the observer optimizes the expected reward in each task, while incurring a cost on the activity of the neurons that encode the numerosities.</p>
<p>Inspired by models of perception in neuroscience<sup><xref ref-type="bibr" rid="c17">17</xref>–<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c21">21</xref>–<xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c45">45</xref>–<xref ref-type="bibr" rid="c47">47</xref></sup>, we consider a two-stage, encoding-decoding model of an observer’s numerosity representation. In the encoding stage, a numerosity <italic>x</italic> elicits in the brain of the observer an imprecise, stochastic representation, <italic>r</italic>, while the decoding stage yields the mean of the Bayesian posterior, which is the optimal decoder in both tasks. The model of Gaussian representations that we use throughout the text is one example of such an encoding-decoding model.</p>
<p>The encoding mechanism is characterized by its Fisher information, <italic>I</italic>(<italic>x</italic>), which reflects the sensitivity of the representation’s probability distribution to changes in the stimulus <italic>x</italic>. The inverse of the square-root of the Fisher information, <inline-formula><inline-graphic xlink:href="585091v1_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, can be understood as the scale of the imprecision of the representation about a numerosity <italic>x</italic>. More precisely, it is approximately — when <italic>I</italic>(<italic>x</italic>) is large — the standard deviation of the Bayesian-mean estimate of <italic>x</italic> derived from the encoded representation. (For smaller <italic>I</italic>(<italic>x</italic>), the standard deviation of the Bayesian-mean estimate increasingly depends on the shape of the prior; with a uniform prior, it decreases near the boundaries.) The variability in subjects’ responses in the estimation task, and their choice probabilities in the discrimination task, reported above, are thus indirect measures of the Fisher information of their encoding process.</p>
<p>Moreover, the expected squared error of the Bayesian-mean estimate of <italic>x</italic> is approximately the inverse of the Fisher information, 1<italic>/I</italic>(<italic>x</italic>). We thus consider the generalized loss function
<disp-formula id="eqn3">
<graphic xlink:href="585091v1_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>π</italic>(<italic>x</italic>) is the prior distribution from which <italic>x</italic> is sampled. With <italic>a</italic> = 1, this quantity approximates the expected quadratic loss that subjects in the estimation task should minimize in order to maximize their reward. And with <italic>a</italic> = 2, minimizing this loss is approximately equivalent to maximizing the reward in the discrimination task<sup><xref ref-type="bibr" rid="c25">25</xref></sup>. (The squared prior, in the expression of <italic>L</italic><sub>2</sub>[<italic>I</italic>], corresponds to the probability of the co-occurrence of two presented numerosities that are close to each other, which is the kind of event most likely to result in errors in discrimination.)</p>
<p>In both cases, a more precise encoding, i.e., a greater Fisher information, results in a smaller loss. This precision, however, comes with a cost. We assume that the encoding results from an accumulation of signals, each entailing an identical cost (e.g., the energy resources consumed by action potentials<sup><xref ref-type="bibr" rid="c32">32</xref>–<xref ref-type="bibr" rid="c34">34</xref></sup>.) The more signals the observer collects, the greater the precision; but also the greater the cost, which is proportional to the number of signals. Formally, we consider a continuum-limit model, in which a representation proceeds from a Wiener process (Brownian motion) with infinitesimal variance <italic>s</italic><sup>2</sup>, observed for a duration <italic>T</italic> (the continuum equivalent of the number of collected signals). The drift of the process, <italic>m</italic>(<italic>x</italic>), encodes the number: it can be, for instance, some normalized value of <italic>x</italic>; but here we only assume that the function <italic>m</italic>(<italic>x</italic>) is increasing and bounded. The resulting representation, <italic>r</italic>, is normally distributed, as <italic>r</italic> |<italic>x</italic> ∼ <italic>N</italic> (<italic>m</italic>(<italic>x</italic>)<italic>T, s</italic><sup>2</sup><italic>T</italic>), and its Fisher information is <italic>T</italic> (<italic>m</italic><sup><italic>′</italic></sup>(<italic>x</italic>))<sup>2</sup><italic>/s</italic><sup>2</sup> and thus it is proportional to <italic>T</italic>. The bound on <italic>m</italic>(<italic>x</italic>) puts a constraint on the Fisher information: specifically, it implies that the quantity
<disp-formula id="eqn4">
<graphic xlink:href="585091v1_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
is bounded by a quantity proportional to the duration, i.e., <italic>C</italic>[<italic>I</italic>] ≤ <italic>KT</italic>, where <italic>K &gt;</italic> 0. Other studies<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup> have posited a bound on the quantity <italic>C</italic>[<italic>I</italic>], but here we emphasize that the bound is a linear function of the duration of observation, and we assume, crucially, that the observer can choose this duration, <italic>T</italic>, but at the expense of a cost that is proportional to <italic>T</italic>. Specifically, we assume that the observer chooses the function <italic>I</italic>(.) and the duration <italic>T</italic> that solve the minimization problem
<disp-formula id="eqn5">
<graphic xlink:href="585091v1_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>λ &gt;</italic> 0. In this problem, any increase of the Fisher information, within the bound, improves the objective function; and thus the solution saturates the bound, i.e., <italic>C</italic>[<italic>I</italic>] = <italic>KT</italic>. Hence the problem reduces to that of choosing the function <italic>I</italic>(.) that solves the minimization problem
<disp-formula id="eqn6">
<graphic xlink:href="585091v1_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>θ</italic> = <italic>λ/K</italic>. The solution is
<disp-formula id="eqn7">
<graphic xlink:href="585091v1_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
This implies that the optimal Fisher information vanishes outside of the support of the prior; and in the case of a uniform prior of width <italic>w, I</italic>(<italic>x</italic>) is constant, as
<disp-formula id="eqn8">
<graphic xlink:href="585091v1_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
for any <italic>x</italic> such that <italic>π</italic>(<italic>x</italic>) ≠ 0.</p>
<p>The scale of the imprecision of internal representations, <inline-formula><inline-graphic xlink:href="585091v1_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is thus predicted to be proportional to the prior width raised to the power 1<italic>/</italic>2, in the estimation task, and raised to the power 3<italic>/</italic>4, in the discrimination task. As shown above, we find indeed that in these tasks, the imprecision of representations not only increases with the prior width, but it does so in a way that is quantitatively consistent with these two exponents. As for the model of Gaussian representations that we have considered throughout the text, it is in fact equivalent to the model just presented, up to a linear transformation of the representation that does not impact its Fisher information (nor the resulting estimates). Its Fisher information is the inverse of the variance, i.e., 1<italic>/</italic> (<italic>ν</italic><sup>2</sup><italic>w</italic><sup>2<italic>α</italic></sup>), and thus <xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref> implies <italic>α</italic> = 1<italic>/</italic>2 for the estimation task, and <italic>α</italic> = 3<italic>/</italic>4 for the discrimination task, i.e., the two values that indeed best fit the data.</p>
<p>Many efficient-coding models in the literature feature a different objective, the maximization of the mutual information<sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup>; but a single objective cannot explain our different findings in the two tasks (namely, the different dependence on the prior width). Many models also feature a different kind of constraint: a <italic>fixed</italic> bound on the quantity in <xref ref-type="disp-formula" rid="eqn4">Eq. 4</xref>, or on a generalization of this quantity<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c24">24</xref></sup>. But here also, as this bound is usually saturated, the optimal Fisher information, which is constant, here, due to the uniform prior, is entirely determined by the constraint—irrespective of the objective of the task. This hypothesis thus cannot account either for the difference that we find between the two tasks. By contrast, we assume that it is the task’s expected reward that is maximized, and that the amount of utilized encoding resources can be endogenously determined: our model is thus able to predict not only that the behavior should depend on the prior, but also that this dependence should change with the task; and it makes quantitative predictions that coincide with our experimental findings.</p>
<p>We compare the responses of the subjects and of the Gaussian-representation model, with <italic>α</italic> = 1<italic>/</italic>2 in the estimation task and <italic>α</italic> = 3<italic>/</italic>4 in the discrimination task. In both cases, the parameter <italic>ν</italic> governs the imprecision in the internal representation, and a second parameter corresponds to additional response noise: the motor noise, parameterized by<inline-formula><inline-graphic xlink:href="585091v1_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, in the estimation task, and the lapse probability, <italic>η</italic>, in the discrimination task. The behavior of the model, across the two tasks and the different priors, reproduces that of the subjects (<xref rid="fig1" ref-type="fig">Figs. 1c</xref> and <xref rid="fig2" ref-type="fig">2c</xref>, dotted lines). In the estimation task, the standard deviation of estimates increases as a function of the prior width, as it does in subjects’ responses. The Fisher information in this model is constant with respect to <italic>x</italic>, and thus the variance of the internal representation, <italic>r</italic>, is also constant; but the Bayesian estimate, <italic>x</italic><sup><italic>∗</italic></sup>(<italic>r</italic>), depends on the prior, and its variability decreases for numerosities closer to the edges of the uniform prior. Hence the standard deviation of the model’s estimates adopts an inverted U-shape similar to that of the subjects (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>). In the discrimination task, the model’s choice-probability curve is steeper in the Narrow condition than in the Wide condition, and the two predicted curves are close to the subjects’ choice probabilities (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). We emphasize that how the internal imprecision scales with the prior width is entirely determined by our theoretical predictions (<xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>); these quantitative predictions allow our model to capture the subjects’ imprecise responses simultaneously across different priors.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In this study, we examine the variability in subjects’ responses in two different tasks and with different priors. We find that the precision of their responses depends both on the task and on the prior. The scale of their imprecision about the presented numbers increases sub-linearly with the width of the prior, and this sublinear relation is different in each task. The two sublinear relations are predicted by a resource-rational account, whereby the allocation of encoding resources optimizes a tradeoff, maximizing each task’s expected reward while incurring a cost on the activity of the encoding neurons. Different formalizations of this tradeoff suggested in several other studies cannot reproduce our experimental findings.</p>
<p>The model and the data suggest a scaling law relating the size of the representations’ imprecision to the width of the prior, with an exponent that depends on the task at hand. An important implication is that the relative precision with which people represent external information can be modulated by their objective and by the manner and the context in which the representations are elicited. In the model, the scaling law results from the solution to the encoding allocation problem (<xref ref-type="disp-formula" rid="eqn6">Eq. 6</xref>) in the special case of a uniform prior, and in the contexts of estimation and discrimination tasks. We surmise that with non-uniform priors and with other tasks (that imply different expected-reward functions), the behavior of subjects should be consistent with the optimal solution to the corresponding resource-allocation problem, provided that subjects are able to learn these other priors and objectives. Further investigations of this conjecture will be crucial in order to understand the extent to which the formalism of optimal resource-allocation that we present here might form a fundamental component in a comprehensive theory of the brain’s internal representations of magnitudes.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L. L.</given-names> <surname>Thurstone</surname></string-name></person-group>. <article-title>Psychophysical Analysis</article-title>. <source>The American Journal of Psychology</source>, <volume>38</volume>(<issue>3</issue>):<fpage>368</fpage>, <month>jul</month> <year>1927</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Wilson P.</given-names> <surname>Tanner</surname></string-name> and <string-name><given-names>John A.</given-names> <surname>Swets</surname></string-name></person-group>. <article-title>A decision-making theory of visual detection</article-title>. <source>Psychological Review</source>, <volume>61</volume>(<issue>6</issue>):<fpage>401</fpage>–<lpage>409</lpage>, <year>1954</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>George A.</given-names> <surname>Gescheider</surname></string-name></person-group>. <source>Psychophysics</source>. <publisher-name>Psychology Press</publisher-name>, <month>jun</month> <year>1997</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. H.</given-names> <surname>Hubel</surname></string-name> and <string-name><given-names>T. N.</given-names> <surname>Wiesel</surname></string-name></person-group>. <article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title>. <source>The Journal of Physiology</source>, <volume>148</volume>(<issue>3</issue>):<fpage>574</fpage>–<lpage>591</lpage>, <month>oct</month> <year>1959</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. H.</given-names> <surname>Henry</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Dreher</surname></string-name>, and <string-name><given-names>P. O.</given-names> <surname>Bishop</surname></string-name></person-group>. <article-title>Orientation specificity of cells in cat striate cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>37</volume>(<issue>6</issue>):<fpage>1394</fpage>–<lpage>1409</lpage>, <year>1974</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>KH</given-names> <surname>Britten</surname></string-name>, <string-name><given-names>MN</given-names> <surname>Shadlen</surname></string-name>, <string-name><given-names>WT</given-names> <surname>Newsome</surname></string-name>, and <string-name><given-names>JA</given-names> <surname>Movshon</surname></string-name></person-group>. <article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title>. <source>The Journal of Neuroscience</source>, <volume>12</volume>(<issue>12</issue>):<fpage>4745</fpage>–<lpage>4765</lpage>, <month>dec</month> <year>1992</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E. L.</given-names> <surname>Kaufman</surname></string-name>, <string-name><given-names>M. W.</given-names> <surname>Lord</surname></string-name>, <string-name><given-names>T. W.</given-names> <surname>Reese</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Volkmann</surname></string-name></person-group>. <article-title>The Discrimination of Visual Number</article-title>. <source>The American Journal of Psychology</source>, <volume>62</volume>(<issue>4</issue>):<fpage>498</fpage>, <month>oct</month> <year>1949</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robert S.</given-names> <surname>Moyer</surname></string-name> and <string-name><given-names>Thomas K.</given-names> <surname>Landauer</surname></string-name></person-group>. <article-title>Time required for Judgements of Numerical Inequality</article-title>. <source>Nature</source>, <volume>215</volume>(<issue>5109</issue>):<fpage>1519</fpage>–<lpage>1520</lpage>, <month>sep</month> <year>1967</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andreas</given-names> <surname>Nieder</surname></string-name> and <string-name><given-names>Earl K.</given-names> <surname>Miller</surname></string-name></person-group>. <article-title>Coding of cognitive magnitude: Compressed scaling of numerical information in the primate prefrontal cortex</article-title>. <source>Neuron</source>, <volume>37</volume>(<issue>1</issue>):<fpage>149</fpage>–<lpage>157</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Esther F.</given-names> <surname>Kutter</surname></string-name>, <string-name><given-names>Jan</given-names> <surname>Bostroem</surname></string-name>, <string-name><given-names>Christian E.</given-names> <surname>Elger</surname></string-name>, <string-name><given-names>Florian</given-names> <surname>Mormann</surname></string-name>, and <string-name><given-names>Andreas</given-names> <surname>Nieder</surname></string-name></person-group>. <article-title>Single Neurons in the Human Brain Encode Numbers</article-title>. <source>Neuron</source>, <volume>100</volume>(<issue>3</issue>):<fpage>753</fpage>– <lpage>761.e4,</lpage> <year>2018</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Stanislas</given-names> <surname>Dehaene</surname></string-name></person-group>. <source>The Number Sense: How the Mind Creates Mathematics</source>. <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>New York</publisher-loc>, <conf-date>ec</conf-date> <year>2011</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Samuel J.</given-names> <surname>Cheyette</surname></string-name> and <string-name><given-names>Steven T.</given-names> <surname>Piantadosi</surname></string-name></person-group>. <article-title>A unified account of numerosity perception</article-title>. <source>Nature Human Behaviour</source>, <year>2020</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Arthur</given-names> <surname>Prat-Carrabin</surname></string-name> and <string-name><given-names>Michael</given-names> <surname>Woodford</surname></string-name></person-group>. <article-title>Efficient coding of numbers explains decision bias and noise</article-title>. <source>Nature Human Behaviour</source>, pages <fpage>845</fpage>–<lpage>848</lpage>, <month>may</month> <year>2022</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>H. B.</given-names> <surname>Barlow</surname></string-name></person-group>. <chapter-title>Possible Principles Underlying the Transformations of Sensory Messages</chapter-title> <person-group person-group-type="editor"><string-name><given-names>W. A.</given-names> <surname>Rosenblith</surname></string-name></person-group>, editor, <source>Sensory Communication</source>, chapter <volume>13</volume>, pages <fpage>217</fpage>–<lpage>234</lpage>. <publisher-name>The MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>, <month>sep</month> <year>1961</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Nicolas</given-names> <surname>Brunel</surname></string-name> and <string-name><given-names>Jean-Pierre</given-names> <surname>Nadal</surname></string-name></person-group>. <article-title>Optimal tuning curves for neurons spiking as a Poisson process</article-title>. <source>Proceedings of the ESANN Conference</source>, <year>1997</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark D.</given-names> <surname>McDonnell</surname></string-name> and <string-name><given-names>Nigel G.</given-names> <surname>Stocks</surname></string-name></person-group>. <article-title>Maximally Informative Stimuli and Tuning Curves for Sigmoidal Rate-Coding Neurons and Populations</article-title>. <source>Physical Review Letters</source>, <volume>101</volume>(<issue>5</issue>):<fpage>058103</fpage>, <month>aug</month> <year>2008</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Deep</given-names> <surname>Ganguli</surname></string-name> and <string-name><given-names>Eero P</given-names> <surname>Simoncelli</surname></string-name></person-group>. <chapter-title>Implicit encoding of prior probabilities in optimal neural populations</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>J. D.</given-names> <surname>Lafferty</surname></string-name>, <string-name><given-names>C. K. I.</given-names> <surname>Williams</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shawe-Taylor</surname></string-name>, <string-name><given-names>R. S.</given-names> <surname>Zemel</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Culotta</surname></string-name></person-group>, editors, <source>Advances in Neural Information Processing Systems</source> <volume>23</volume>, pages <fpage>658</fpage>—-<lpage>666</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2010</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Deep</given-names> <surname>Ganguli</surname></string-name> and <string-name><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name></person-group>. <article-title>Efficient Sensory Encoding and Bayesian Inference with Heterogeneous Neural Populations</article-title>. <source>Neural Computation</source>, <volume>26</volume>(<issue>10</issue>):<fpage>2103</fpage>–<lpage>2134</lpage>, <month>oct</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xue-Xin</given-names> <surname>Wei</surname></string-name> and <string-name><given-names>Alan A.</given-names> <surname>Stocker</surname></string-name></person-group>. <article-title>A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts</article-title>. <source>Nature Neuroscience</source>, <volume>18</volume>(<issue>10</issue>):<fpage>1509</fpage>–<lpage>1517</lpage>, <month>oct</month> <year>2015</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xue-Xin</given-names> <surname>Wei</surname></string-name> and <string-name><given-names>Alan A</given-names> <surname>Stocker</surname></string-name></person-group>. <article-title>Mutual Information, Fisher Information, and Efficient Coding</article-title>. <source>Neural Computation</source>, <volume>326</volume>:<fpage>305</fpage>–<lpage>326</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Deep</given-names> <surname>Ganguli</surname></string-name> and <string-name><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name></person-group>. <article-title>Neural and perceptual signatures of efficient sensory coding</article-title>. <source>arXiv</source>, pages <fpage>1</fpage>–<lpage>24</lpage>, <month>feb</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Zhuo</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Alan A.</given-names> <surname>Stocker</surname></string-name>, and <string-name><given-names>Daniel D.</given-names> <surname>Lee</surname></string-name></person-group>. <article-title>Efficient Neural Codes That Minimize Lp Reconstruction Error</article-title>. <source>Neural Computation</source>, <volume>28</volume>(<issue>12</issue>):<fpage>2656</fpage>–<lpage>2686</lpage>, <conf-date>ec</conf-date> <year>2016</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Il Memming</given-names> <surname>Park</surname></string-name> and <string-name><given-names>Jonathan W.</given-names> <surname>Pillow</surname></string-name></person-group>. <article-title>Bayesian Efficient Coding</article-title>. <source>bioRxiv</source>, <year>2017</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael</given-names> <surname>Morais</surname></string-name> and <string-name><given-names>Jonathan W</given-names> <surname>Pillow</surname></string-name></person-group>. <article-title>Power-law efficient neural codes provide general link between perceptual bias and discriminability</article-title>. <source>Advances in Neural Information Processing Systems 31</source>, <volume>2</volume>(<issue>1</issue>):<fpage>5076</fpage>–<lpage>5085</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Arthur</given-names> <surname>Prat-Carrabin</surname></string-name> and <string-name><given-names>Michael</given-names> <surname>Woodford</surname></string-name></person-group>. <chapter-title>Bias and variance of the Bayesian-mean decoder</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>M</given-names> <surname>Ranzato</surname></string-name>, <string-name><given-names>A</given-names> <surname>Beygelzimer</surname></string-name>, <string-name><given-names>Y</given-names> <surname>Dauphin</surname></string-name>, <string-name><given-names>P S</given-names> <surname>Liang</surname></string-name>, and <string-name><given-names>J Wortman</given-names> <surname>Vaughan</surname></string-name></person-group>, editors, <source>Advances in Neural Information Processing Systems</source>, volume <volume>34</volume>, pages <fpage>23793</fpage>–<lpage>23805</lpage>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2021</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ling Qi</given-names> <surname>Zhang</surname></string-name> and <string-name><given-names>Alan A.</given-names> <surname>Stocker</surname></string-name></person-group>. <article-title>Prior Expectations in Visual Speed Perception Predict Encoding Characteristics of Neurons in Area MT</article-title>. <source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source>, <volume>42</volume>(<issue>14</issue>):<fpage>2951</fpage>–<lpage>2962</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jonathan</given-names> <surname>Schaffner</surname></string-name>, <string-name><given-names>Sherry Dongqi</given-names> <surname>Bao</surname></string-name>, <string-name><given-names>Philippe N.</given-names> <surname>Tobler</surname></string-name>, <string-name><given-names>Todd A.</given-names> <surname>Hare</surname></string-name>, and <string-name><given-names>Rafael</given-names> <surname>Polania</surname></string-name></person-group>. <article-title>Sensory perception relies on fitness-maximizing codes</article-title>. <source>Nature Human Behaviour</source>, <volume>7</volume>(<issue>7</issue>):<fpage>1135</fpage>–<lpage>1151</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Tobias</given-names> <surname>Bothwell</surname></string-name>, <string-name><given-names>Colin J.</given-names> <surname>Kennedy</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Aeppli</surname></string-name>, <string-name><given-names>Dhruv</given-names> <surname>Kedar</surname></string-name>, <string-name><given-names>John M.</given-names> <surname>Robinson</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Oelker</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Staron</surname></string-name>, and <string-name><given-names>Jun</given-names> <surname>Ye</surname></string-name></person-group>. <article-title>Resolving the gravitational redshift across a millimetre-scale atomic sample</article-title>. <source>Nature</source>, <volume>602</volume>(<issue>7897</issue>):<fpage>420</fpage>–<lpage>424</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ahna R</given-names> <surname>Girshick</surname></string-name>, <string-name><given-names>Michael S</given-names> <surname>Landy</surname></string-name>, and <string-name><given-names>Eero P</given-names> <surname>Simoncelli</surname></string-name></person-group>. <article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>(<issue>7</issue>):<fpage>926</fpage>–<lpage>932</lpage>, <month>jul</month> <year>2011</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Falk</given-names> <surname>Lieder</surname></string-name> and <string-name><given-names>Thomas L.</given-names> <surname>Griffiths</surname></string-name></person-group>. <article-title>Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>43</volume>:<fpage>e1</fpage>, <month>feb</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Wei Ji</given-names> <surname>Ma</surname></string-name> and <string-name><given-names>Michael</given-names> <surname>Woodford</surname></string-name></person-group>. <article-title>Multiple conceptions of resource rationality</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>43</volume>:<fpage>e15</fpage>, <month>mar</month> <year>2020</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author">S B <string-name><surname>Laughlin</surname>, <given-names>R R</given-names></string-name> <string-name><given-names>de Ruyter</given-names> <surname>Van Steveninck</surname></string-name>, and <string-name><given-names>J C</given-names> <surname>Anderson</surname></string-name></person-group>. <article-title>The metabolic cost of neural information</article-title>. <source>Nature neuroscience</source>, <volume>1</volume>(<issue>1</issue>):<fpage>36</fpage>–<lpage>41</lpage>, <year>1998</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea</given-names> <surname>Hasenstaub</surname></string-name>, <string-name><given-names>Stephani</given-names> <surname>Otte</surname></string-name>, <string-name><given-names>Edward</given-names> <surname>Callaway</surname></string-name>, and <string-name><given-names>Terrence J.</given-names> <surname>Sejnowski</surname></string-name></person-group>. <article-title>Metabolic cost as a unifying principle governing neuronal biophysics</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>107</volume>(<issue>27</issue>):<fpage>12329</fpage>–<lpage>12334</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Biswa</given-names> <surname>Sengupta</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Stemmler</surname></string-name>, <string-name><given-names>Simon B.</given-names> <surname>Laughlin</surname></string-name>, and <string-name><given-names>Jeremy E.</given-names> <surname>Niven</surname></string-name></person-group>. <article-title>Action potential energy efficiency varies among neuron types in vertebrates and invertebrates</article-title>. <source>PLoS Computational Biology</source>, <volume>6</volume>(<issue>7</issue>):<fpage>35</fpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Véronique</given-names> <surname>Izard</surname></string-name> and <string-name><given-names>Stanislas</given-names> <surname>Dehaene</surname></string-name></person-group>. <article-title>Calibrating the mental number line</article-title>. <source>Cognition</source>, <volume>106</volume>(<issue>3</issue>):<fpage>1221</fpage>–<lpage>1247</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alberto</given-names> <surname>Testolin</surname></string-name> and <string-name><given-names>James L.</given-names> <surname>McClelland</surname></string-name></person-group>. <article-title>Do estimates of numerosity really adhere to Weber’s law? A reexamination of two case studies</article-title>. <source>Psychonomic Bulletin and Review</source>, <volume>28</volume>(<issue>1</issue>):<fpage>158</fpage>–<lpage>168</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Camillo</given-names> <surname>Padoa-Schioppa</surname></string-name></person-group>. <article-title>Range-adapting representation of economic value in the orbitofrontal cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>29</volume>(<issue>44</issue>):<fpage>14004</fpage>–<lpage>14014</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Shunsuke</given-names> <surname>Kobayashi</surname></string-name>, <string-name><given-names>Ofelia Pinto</given-names> <surname>de Carvalho</surname></string-name>, and <string-name><given-names>Wolfram</given-names> <surname>Schultz</surname></string-name></person-group>. <article-title>Adaptation of Reward Sensitivity in Orbitofrontal Neurons</article-title>. <source>The Journal of Neuroscience</source>, <volume>30</volume>(<issue>2</issue>):<fpage>534</fpage>– <lpage>544</lpage>, <month>jan</month> <year>2010</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xinying</given-names> <surname>Cai</surname></string-name> and <string-name><given-names>Camillo</given-names> <surname>Padoa-Schioppa</surname></string-name></person-group>. <article-title>Neuronal encoding of subjective value in dorsal and ventral anterior cingulate cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>(<issue>11</issue>):<fpage>3791</fpage>– <lpage>3808</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alireza</given-names> <surname>Soltani</surname></string-name>, <string-name><given-names>Benedetto</given-names> <surname>De Martino</surname></string-name>, and <string-name><given-names>Colin</given-names> <surname>Camerer</surname></string-name></person-group>. <article-title>A Range-Normalization Model of Context-Dependent Choice: A New Model and Evidence</article-title>. <source>PLoS Computational Biology</source>, <volume>8</volume>(<issue>7</issue>):<fpage>e1002607</fpage>, <month>jul</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Antonio</given-names> <surname>Rangel</surname></string-name> and <string-name><given-names>John A.</given-names> <surname>Clithero</surname></string-name></person-group>. <article-title>Value normalization in decision making: Theory and evidence</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>22</volume>(<issue>6</issue>):<fpage>970</fpage>–<lpage>981</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kenway</given-names> <surname>Louie</surname></string-name> and <string-name><given-names>Paul W.</given-names> <surname>Glimcher</surname></string-name></person-group>. <article-title>Efficient coding and the neural representation of value</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1251</volume>(<issue>1</issue>):<fpage>13</fpage>–<lpage>32</lpage>, <month>mar</month> <year>2012</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gideon</given-names> <surname>Schwarz</surname></string-name></person-group>. <article-title>Estimating the Dimension of a Model</article-title>. <source>The Annals of Statistics</source>, <volume>6</volume>(<issue>2</issue>):<fpage>461</fpage>–<lpage>464</lpage>, <month>mar</month> <year>1978</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robert E.</given-names> <surname>Kass</surname></string-name> and <string-name><given-names>Adrian E.</given-names> <surname>Raftery</surname></string-name></person-group>. <article-title>Bayes Factors</article-title>. <source>Journal of the American Statistical Association</source>, <volume>90</volume>(<issue>430</issue>):<fpage>773</fpage>–<lpage>795</lpage>, <month>jun</month> <year>1995</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Norberto M.</given-names> <surname>Grzywacz</surname></string-name> and <string-name><given-names>Rosario M.</given-names> <surname>Balboa</surname></string-name></person-group>. <article-title>A Bayesian framework for sensory adaptation</article-title>. <source>Neural Computation</source>, <volume>14</volume>(<issue>3</issue>):<fpage>543</fpage>–<lpage>559</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alan A.</given-names> <surname>Stocker</surname></string-name> and <string-name><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name></person-group>. <article-title>Sensory adaptation within a Bayesian frame-work for perception</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>18</volume>:<fpage>1291</fpage>–<lpage>1298</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alan A.</given-names> <surname>Stocker</surname></string-name> and <string-name><given-names>Eero P.</given-names> <surname>Simoncelli</surname></string-name></person-group>. <article-title>Noise characteristics and prior expectations in human visual speed perception</article-title>. <source>Nature Neuroscience</source>, <volume>9</volume>(<issue>4</issue>):<fpage>578</fpage>–<lpage>585</lpage>, <month>apr</month> <year>2006</year>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Estimation task</title>
<sec id="s5a1">
<title>Task and subjects</title>
<p>36 subjects (20 female, 15 male, 1 non-binary) participated in the estimation-task experiment (average age: 21.4, standard deviation: 2.8). The experiment took place at Columbia University, and complied with the relevant ethical regulations; it was approved by the university’s Institutional Review Board (protocol number: IRB-AAAS8409). All subjects experienced the three conditions.</p>
<p>In the experiment, subjects provide their responses using a slider (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>), whose size on screen is proportional to the width of the prior. Each condition comprises three different phases. In all the trials of all three phases the numerosities are randomly sampled from the prior corresponding to the current condition. This prior is explicitly told to the subject when the condition starts. In each of the 15 trials of the first, ‘learning’ phase, the subject is shown a cloud of dots together with the number of dots it contains (i.e., its numerosity represented with Arabic numerals). These elements stay on screen until the subject chooses to move on to the next trial. No response is required from the subject in this phase. Then follow the 30 trials of the ‘feedback’ phase, in which clouds of dots are shown for 500ms without any other information on their numerosities. The subject is then asked to provide an estimate of the numerosity. Once the estimate is submitted, the correct number is shown on screen. The third and last phase is the ‘no-feedback’ phase, which is identical to the ‘feedback’ phase, except that no feedback is provided. In both the ‘feedback’ phase and the ‘no-feedback’ phase, subjects respond at their own pace. All the analyses presented here use the data of the ‘no-feedback’ phase, which comprises 120 trials.</p>
<p>At the end of the experiment, subjects receive a financial reward equal to the sum of a $5 show-up fee (USD) and of a performance bonus. After each submission of an estimate, an amount equal to <inline-formula><inline-graphic xlink:href="585091v1_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>x</italic> is the correct number and <inline-formula><inline-graphic xlink:href="585091v1_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the estimate, is added to the performance bonus. If at the end of the experiment the performance bonus is negative, it is set to zero. The average reward was $11.80 (standard deviation: 6.98).</p>
</sec>
<sec id="s5a2">
<title>Bins defined over the priors, and calculation of the variance</title>
<p>The ranges of the three priors (50-70, 40-80 and 30-90), contain 21, 41, and 61 integers, respectively, and thus none of them can be split in five bins containing the same number of integers. Hence the ranges defining each of the five bins were chosen such that the third bin contains an odd number of integers, with at its middle the middle number of the prior (60 in each case), and such that the second and fourth bins contain the same number of integers as the third one; the first and last bins then contain the remaining integers. In the Narrow condition, the ranges of the five bins are: 50-52, 53-57, 58-62, 63-67, and 68-70. In the Medium condition, the ranges of the five bins are: 40-46, 47-55, 56-64, 65-73, and 74-80. In the Wide condition, the ranges of the five bins are: 30-40, 41-53, 54-66, 67-79, and 80-90.</p>
<p>In our calculation of the variance of estimates, when pooling responses by bins of presented numbers, we do not wish to include the variability stemming from the diversity of numbers in each bin. Thus we subtract from each estimate <inline-formula><inline-graphic xlink:href="585091v1_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of a number the average of all the estimates obtained with the same number, <inline-formula><inline-graphic xlink:href="585091v1_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The calculation of the variance for a bin then makes use of these ‘excursions’ from the mean estimates, <inline-formula><inline-graphic xlink:href="585091v1_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula></p>
</sec>
<sec id="s5a3">
<title>Model fitting and individual subjects analysis</title>
<p>The Gaussian-representation model used throughout the text has three parameters: <italic>α, ν</italic>, and <italic>σ</italic><sub>0</sub>. We fit these parameters to the subjects’ data by maximizing the model’s likelihood. For each parameter, we can either allow for ‘individual’ values of the parameter that may be different for different subjects, or we can fit the responses of all the subjects with the same, ‘shared’ value of the parameter. In the main text we discuss the model with ‘shared’ parameters; the corresponding BICs are shown in the first three lines of <xref rid="tbl1" ref-type="table">Table 1</xref>. The other lines of the Table correspond to specifications of the model in which at least one parameter is allowed to take ‘individual’ values. In both cases the lowest BIC is obtained for models with a fixed exponent <italic>α</italic> = 1<italic>/</italic>2, common to all the subjects, consistently with our prediction (<xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>). Overall, the best-fitting model allows for ‘individual’ values of the parameters <italic>ν</italic> and <italic>σ</italic><sub>0</sub>, and a fixed, shared value for <italic>α</italic>. This suggests that the parameters <italic>ν</italic> and <italic>σ</italic><sub>0</sub>, which govern, respectively, the degrees of “internal” and “external” (motor) imprecision, capture individual traits characteristic of each subject, while the exponent <italic>α</italic> reflects the solution to the optimization problem posed by the task, which is the same for all the subjects.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Estimation task: model fitting supports the hypothesis <italic>α</italic> = 1<italic>/</italic>2, both with pooled and individual responses.</title>
<p>Number of parameters (second-to-last column) and BIC (last column) of the Gaussian-representation model under different specifications regarding whether all subjects share the same values of the three parameters <italic>α, ν</italic>, and <italic>σ</italic><sub>0</sub> (first three columns). ‘Shared’ indicates that the responses of all the subjects are modeled with the same value of the parameter. ‘Indiv.’ indicates that different values of the parameter are allowed for different subjects. For the parameter <italic>α</italic>, ‘Fixed’ indicates that the value of <italic>α</italic> is fixed (thus it is not a free parameter); when the parameter <italic>α</italic> is ‘Shared’, it is a free parameter, and we indicate its best-fitting value in parentheses. In the first three lines of the table, all three parameters are shared across the subjects (the three lines differ only by the specification of <italic>α</italic>); while in the remaining lines at least one parameter is individually fit. In both cases the lowest BIC (indicated by a star) is obtained for a model with a fixed parameter <italic>α</italic> = 1<italic>/</italic>2.</p></caption>
<graphic xlink:href="585091v1_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
<sec id="s5b">
<title>Discrimination task</title>
<sec id="s5b1">
<title>Task and subjects</title>
<p>111 subjects (61 male, 50 female) participated in the discrimination-task experiment (average age: 31.4, standard deviation: 10.2). Due to the COVID crisis, the experiment was run online, and each subject experienced only one condition. 31 subjects participated in the Narrow condition, and 32 subjects participated in the Wide condition. This experiment was approved by Columbia University’s Internal Review Board (protocol number: IRB-AAAR9375).</p>
<p>In this experiment, each condition starts with 20 practice trials. In each of these trials, five red numbers and five blue numbers are shown to the subject, each for 500ms. In the first 10 practice trials, no response is asked from the subject. In the following 10 practice trials, the subject is asked to choose a color; choices in these trials do not impact the reward. Then follow 200 ‘real’ trials in which the averages chosen by the subject are added to a score. At the end of the experiment, the subject receives a financial reward that is the sum of a $1.50 fixed fee (USD) and of a non-negative variable bonus. The variable bonus is equal to max(0, 1.6(AverageScore − 50)), where AverageScore is the score divided by 200. The average reward was $6.80 (standard deviation: 2.15).</p>
</sec>
<sec id="s5b2">
<title>Individual subjects analysis</title>
<p>In the Gaussian-representation model, a numerosity <italic>x</italic> yields a representation that is normally-distributed, as <italic>r</italic> |<italic>x</italic> ∼ <italic>N</italic> (<italic>x, ν</italic><sup>2</sup><italic>w</italic><sup>2<italic>α</italic></sup>). Fitting the model to the pooled data collected in the two conditions has enabled us to identify separately the two parameters <italic>ν</italic> and <italic>α</italic>. But fitting to the responses of individual subjects, who experienced only one of the two conditions, only allows to identify the variance <inline-formula><inline-graphic xlink:href="585091v1_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and not <italic>ν</italic> and <italic>α</italic> separately. However, an important difference between these two parameters is that the baseline variance <italic>ν</italic><sup>2</sup> is idiosyncratic to each subject (and thus we expect inter-subject variability for this parameter), while the exponent <italic>α</italic>, in our theory, is determined by the specifics of the task, and thus it should be the same for all the subjects; in particular, we predict <italic>α</italic> = 3<italic>/</italic>4. Therefore, as subjects were randomly assigned to one of the two conditions, we expect the distribution of <inline-formula><inline-graphic xlink:href="585091v1_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to be identical across the two conditions. We thus look at the empirical distributions of this quantity, with different values of <italic>α</italic>, in the two conditions. We find that the distributions of <inline-formula><inline-graphic xlink:href="585091v1_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="585091v1_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, in the two conditions, do not match well; but the distributions of <inline-formula><inline-graphic xlink:href="585091v1_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the two conditions are close to each other (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). In each of these four cases, we run a Kolmogorov-Smirnov test of the equality of the underlying distributions. With <inline-formula><inline-graphic xlink:href="585091v1_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="585091v1_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the null hypothesis is rejected (p-values: 1e-10, 0.008, and 0.001, respectively), while with <inline-formula><inline-graphic xlink:href="585091v1_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the hypothesis (of equality of the distributions in the two conditions) is not rejected (p-value: 0.79). Thus this analysis, based on the individual model-fitting of the subjects, substantiates our conclusions.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Discrimination task: empirical across-subjects distribution of scaled best-fitting standard-deviation parameter.</title>
<p>The first panel shows the empirical cumulative distribution function (CDF) of the fitted parameter<inline-formula><inline-graphic xlink:href="585091v1_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, unscaled. The second, third, and fourth panels show the empirical CDF of <inline-formula><inline-graphic xlink:href="585091v1_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula> respectively. divided by <italic>w</italic><sup><italic>α</italic></sup>, with <italic>α</italic> = 1<italic>/</italic>2, 3<italic>/</italic>4, and 1, respectively.</p></caption>
<graphic xlink:href="585091v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s5b3">
<title>Models’ BICs</title>
<p>We fit the Gaussian-representation model, with or without lapses, to the subjects’ responses in the discrimination task. In the main text we discuss the model-fitting results of the model with lapses. The corresponding BICs are reported in the last four lines of <xref rid="tbl2" ref-type="table">Table 2</xref>, while the first four lines report the BICs of the model with no lapses. <xref rid="tbl2" ref-type="table">Table 2</xref> shows that including lapses in the model yields lower BICs, but also that in both cases (with or without lapses), the lowest BIC is obtained with the model with a fixed parameter <italic>α</italic> = 3<italic>/</italic>4, consistently with our theoretical prediction (<xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>).</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Discrimination task: model fitting supports the hypothesis <italic>α</italic> = 3<italic>/</italic>4.</title>
<p>Number of parameters (second-to-last column) and BIC (last column) of the Gaussian-representation model under different specifications regarding the parameter <italic>α</italic> (first column) and the absence or presence of lapses (second column). In the bottom four lines the model features lapses, while it does not in the top four lines; in both cases the lowest BIC (indicated with a star) is obtained with the specification <italic>α</italic> = 3<italic>/</italic>4.</p></caption>
<graphic xlink:href="585091v1_tbl2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
</sec>
<sec id="s6">
<title>Data availability statement</title>
<p>Requests for the data can be sent via email to the corresponding author.</p>
</sec>
<sec id="s7">
<title>Code availability statement</title>
<p>Requests for the code used for all analyses can be sent via email to the corresponding author.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Jessica Li and Maggie Lynn for their help as research assistants, Hassan Afrouzi for helpful comments, and the National Science Foundation for research support (grant SES DRMS 1949418).</p>
</ack>
<sec id="s8">
<title>Competing interest declaration</title>
<p>The authors declare no conflict of interest.</p>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101277.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Hang</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This research investigates the precision of numerosity perception in two different tasks and concludes that human performance aligns with an efficient coding model optimized for current environmental statistics and task goals. The findings may have <bold>important</bold> implications for our understanding of numerosity perception as well as the ongoing debate on different efficient coding models. However, the evidence presented in the paper to support the conclusion is still <bold>incomplete</bold> and could be strengthened by further modeling analysis or experimental data that can address potential confounds.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101277.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The &quot;number sense&quot; refers to an imprecise and noisy representation of number. Many researchers propose that the number sense confers a fixed (exogenous) subjective representation of number that adheres to scalar variability, whereby the variance of the representation of number is linear in the number.</p>
<p>This manuscript investigates whether the representation of number is fixed, as usually assumed in the literature, or whether it is endogenous. The two dimensions on which the authors investigate this endogeneity are the subject's prior beliefs about stimuli values and the task objective. Using two experimental tasks, the authors collect data that are shown to violate scalar variability and are instead consistent with a model of optimal encoding and decoding, where the encoding phase depends endogenously on prior and task objectives. I believe the paper asks a critically important question. The literature in cognitive science, psychology, and increasingly in economics, has provided growing empirical evidence of decision-making consistent with efficient coding. However, the precise model mechanics can differ substantially across studies. This point was made forcefully in a paper by Ma and Woodford (2020, Behavioral &amp; Brain Sciences), who argue that different researchers make different assumptions about the objective function and resource constraints across efficient coding models, leading to a proliferation of different models with ad-hoc assumptions. Thus, the possibility that optimal coding depends endogenously on the prior and the objective of the task, opens the door to a more parsimonious framework in which assumptions of the model can be constrained by environmental features. Along these lines, one of the authors' conclusions is that the degree of variability in subjective responses increases sublinearly in the width of the prior. And importantly, the degree of this sublinearity differs across the two tasks, in a manner that is consistent with a unified efficient coding model.</p>
<p>Comments:</p>
<p>(1) Modeling and implementation of estimation task</p>
<p>The biggest concern I have with the paper is about the experimental implementation and theoretical account of the estimation task. The salient features of the experimental data (Figure 1C) are that the standard deviations of subjects' estimated quantities are hump-shaped in the true stimulus x and that the standard deviation, conditional on the true stimulus x, is increasing in prior width. The authors attribute these features to a Bayesian encoding and decoding model in which the internal representation of the quantity is noisy, and the degree of noise depends on the prior - as in models of efficient coding (Wei and Stocker 2015 Nature Neuro; Bhui and Gershman 2018 Psych Review; Hahn and Wei 2024 Nature Neuro).</p>
<p>The concern I have is about the final &quot;step&quot; in the model, where the authors assume there is an additional layer of motor noise in selecting the response. The authors posit that the subject's selection of the response is drawn from a Gaussian with a mean set to the optimally decoded estimate x*(r), and variance set to a free parameter sigma_0^2. However, the authors also assume that the Gaussian distribution is &quot;truncated to the prior range.&quot; This truncation is a nontrivial assumption, and I believe that on its own, it can explain many features of the data.</p>
<p>To see this, assume that there is no noise in the internal representation of x, there is only motor noise. This corresponds to a special case of the authors' model in which υ is set to 0. The model then reduces to a simple account in which responses are drawn from a Gaussian distribution centered at the true value of x, but with asymmetric noise due to the truncation. I simulated such a model with sigma_0=7. The resulting standard deviations of responses for each value of x (based on 1000 draws for each value of x), across the three different priors, reproduce the salient patterns of the standard deviation in Figure 1C: i) within each condition, the standard deviation is hump-shaped and peaks at x=60 and ii) conditional on x, standard deviation increases in prior width. The takeaway is that this simple model with only truncated motor noise - and without any noisy or efficient coding of internal representations - provides an alternative channel through which the prior affects behavior.</p>
<p>Of course, this does not imply that subjects' coding is not described by the efficient encoding and decoding model posited by the authors. However, it does suggest an important alternative mechanism for the authors' theoretical results in the estimation task. Moreover, some of the quantitative conclusions about the differences in behavior with the discrimination task would be greatly affected by the assumption of truncated motor noise.</p>
<p>Turning to the experiment, a basic question is whether such a truncation was actually implemented in the design. That is, was the range of the slider bar set to the range of the prior? (The methods section states that the size on the screen of the slider was proportional to the prior width, but it was unclear whether the bounds of the slider bar changed with the prior). If the slider bar range did depend on the prior, then it becomes difficult to interpret the data. If not, then perhaps one can perform analyses to understand how much the motor noise is responsible for the dependence of the standard deviation on both x and the prior width. Indeed, the authors emphasize that their model is best fit at α=0.48, which would seem to imply that the best fitting value of υ is strictly positive. However, it would be important to clarify whether the estimation procedure allowed for υ=0, or whether this noise parameter was constrained to be positive (i.e., clarify whether the estimation assumed noisy and efficient coding of internal representations).</p>
<p>(2) Differences across tasks</p>
<p>A main takeaway from the paper is that optimal coding depends on the expected reward function in each task. This is the explanation for why the degree of sublinearity between standard deviation and prior width changes across the estimation and discrimination task. But besides the two different reward functions, there are also other differences across the two tasks. For example, the estimation task involves a single array of dots, whereas the discrimination task involves a pair of sequences of Arabic numerals. Related to the discussion above, in the estimation task the response scale is continuous whereas in the discrimination task, responses are binary. Is it possible that these other differences in the task could contribute to the observed different degrees of sublinearity? It is likely beyond the scope of the paper to incorporate these differences into the model, but such differences across the two tasks should be discussed as potential drivers of differences in observed behavior.</p>
<p>If it becomes too difficult to interpret the data from the estimation task due to the slider bar varying with the prior range, then which of the paper's conclusions would still follow when restricting the analysis to the discrimination task?</p>
<p>(3) Placement literature</p>
<p>One closely related experiment to the discrimination task in the current paper can be found in Frydman and Jin (2022 Quarterly Journal of Economics). Those authors also experimentally vary the width of a uniform prior in a discrimination task using Arabic numerals, in order to test principles of efficient coding. Consistent with the current findings, Frydman and Jin find that subjects exhibit greater precision when making judgments about numbers drawn from a narrower distribution. However, what the current manuscript does is it goes beyond Frydman and Jin by modeling and experimentally varying task objectives to understand and test the effects on optimal coding. This contribution should be highlighted and contrasted against the earlier experimental work of Frydman and Jin to better articulate the novelty of the current manuscript.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101277.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper provides an ingenious experimental test of an efficient coding objective based on optimization as a task success. The key idea is that different tasks (estimation vs discrimination) will, under the proposed model, lead to a different scaling between the encoding precision and the width of the prior distribution. Empirical evidence in two tasks involving number perception supports this idea.</p>
<p>Strengths:</p>
<p>- The paper provides an elegant test of a prediction made by a certain class of efficient coding models previously investigated theoretically by the authors.</p>
<p>The results in experiments and modeling suggest that competing efficient coding models, optimizing mutual information alone, may be incomplete by missing the role of the task.</p>
<p>Weaknesses:</p>
<p>- The claims would be more strongly validated if data were present at more than two widths in the discrimination experiment.</p>
<p>- A very strong prediction of the model -- which determines encoding entirely from prior and task -- is that Fisher Information is uniform throughout the range, strongly at odds with the traditional assumption of imprecision increasing with the numerosity (Weber/Fechner law). This prediction should be checked against the data collected. It may not be trivial to determine this in the Estimation experiment, but should be feasible in the Discrimination experiment in the Wide condition: Is there really no difference in discriminability at numbers close to 10 vs numbers close to 90? Figure 2 collapses over those, so it's not evident whether such a difference holds or not. I'd have loved to look into this in reviewing, but the authors have not yet made their data publicly available - I strongly encourage them to do so.</p>
<p>Importantly, the inverse u-shaped pattern in Figure 1 is itself compatible with a Weber's-law-based encoding, as shown by simulation in Figure 5d in Hahn&amp;Wei [1]. This suggests a potential competing variant account, in apparent qualitative agreement with the findings reported: the encoding is compatible with Fisher's law, and only a single scalar, the magnitude of sensory noise, is optimized for the task for the loss function (3). As this account would be substantially more in line with traditional accounts of numerosity perception - while still exhibiting task-dependence of encoding as proposed by the authors - it would be worth investigating if it can be ruled out based on the data gathered for this paper.</p>
<p>References:</p>
<p>[1] Hahn &amp; Wei, A unifying theory explains seemingly contradictory biases in perceptual estimation, Nature Neuroscience 2024</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.101277.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work demonstrates that people's imprecision in numeric perception varies with the stimulus context and task goal. By measuring imprecision across different widths of uniform prior distributions in estimation and discrimination tasks, the authors find that imprecision changes sublinearly with prior width, challenging previous range normalization models. They further show that these changes align with the efficient encoding model, where decision-makers balance expected rewards and encoding costs optimally.</p>
<p>Strengths:</p>
<p>The experimental design is straightforward, controlling the mean of the number distribution while varying the prior width. By assessing estimation errors and discrimination accuracy, the authors effectively highlight how imprecision adjusts across conditions.</p>
<p>The model's predictions align well with the data, with the exponential terms (1/2 and 3/4) of imprecision changes matching the empirical results impressively.</p>
<p>Weaknesses:</p>
<p>Some details in the model section are unclear. Specifically, I'm puzzled by the Wiener process assumption where r∣x∼N(m(x)T,s^2T). Does this imply that both the representation of number x and the noise are nearly zero at the beginning, increasing as observation time progresses? This seems counterintuitive, and a clearer explanation would be helpful.</p>
<p>The authors explore range normalization models with Gaussian representation, but another common approach is the logarithmic representation (Barretto-García et al., 2023; Khaw et al., 2021). Could the logarithmic representation similarly lead to sublinearity in noise and distribution width?</p>
<p>Additionally, Heng et al. (2020) found that subjects did not alter their encoding strategy across different task goals, which seems inconsistent with the fully adaptive representation proposed here. I didn't find the analysis of participants' temporal dynamics of adaptation. The behavioral results in the manuscript seem to imply that the subjects adopted different coding schemes in a very short period of time. Yet in previous studies of adaptation, experimental results seem to be more supportive of a partial adaptive behavior (Bujold et al., 2021; Heng et al., 2020), which might balance experimental and real-world prior distributions. Analyzing temporal dynamics might provide more insight. Noting that the authors informed subjects about the shape of the prior distribution before the experiment, do the results in this manuscript suggest a top-down rapid modulation of number representation?</p>
<p>Barretto-García, M., De Hollander, G., Grueschow, M., Polanía, R., Woodford, M., &amp; Ruff, C. C. (2023). Individual risk attitudes arise from noise in neurocognitive magnitude representations. Nature Human Behaviour, 7(9), 1551-1567. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-023-01643-4">https://doi.org/10.1038/s41562-023-01643-4</ext-link></p>
<p>Bujold, P. M., Ferrari-Toniolo, S., &amp; Schultz, W. (2021). Adaptation of utility functions to reward distribution in rhesus monkeys. Cognition, 214, 104764. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cognition.2021.104764">https://doi.org/10.1016/j.cognition.2021.104764</ext-link></p>
<p>Heng, J. A., Woodford, M., &amp; Polania, R. (2020). Efficient sampling and noisy decisions. eLife, 9, e54962. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.54962">https://doi.org/10.7554/eLife.54962</ext-link></p>
<p>Khaw, M. W., Li, Z., &amp; Woodford, M. (2021). Cognitive Imprecision and Small-Stakes Risk Aversion. The Review of Economic Studies, 88(4), 1979-2013. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/restud/rdaa044">https://doi.org/10.1093/restud/rdaa044</ext-link></p>
</body>
</sub-article>
</article>