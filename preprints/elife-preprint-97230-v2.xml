<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97230</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97230</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97230.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Inverted encoding of neural responses to audiovisual stimuli reveals super-additive multisensory enhancement</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0002-4249-462X</contrib-id>
<name>
<surname>Buhmann</surname>
<given-names>Zak</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>z.buhmann@uq.net.au</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7378-2803</contrib-id>
<name>
<surname>Robinson</surname>
<given-names>Amanda K</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0929-9216</contrib-id>
<name>
<surname>Mattingley</surname>
<given-names>Jason B</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8416-005X</contrib-id>
<name>
<surname>Rideaux</surname>
<given-names>Reuben</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rqy9422</institution-id><institution>Queensland Brain Institute, The University of Queensland</institution></institution-wrap>, <city>Brisbane</city>, <country>Australia</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rqy9422</institution-id><institution>School of Psychology, The University of Queensland</institution></institution-wrap>, <city>Brisbane</city>, <country>Australia</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0384j8v12</institution-id><institution>School of Psychology, University of Sydney</institution></institution-wrap>, <city>Sydney</city>, <country>Australia</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-05-17">
<day>17</day>
<month>05</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-12-06">
<day>06</day>
<month>12</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97230</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-03-03">
<day>03</day>
<month>03</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-04">
<day>04</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.15.580589"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-05-17">
<day>17</day>
<month>05</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97230.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.97230.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.97230.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.97230.1.sa0">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.97230.1.sa3">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Buhmann et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Buhmann et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97230-v2.pdf"/>
<abstract>
<title>Abstract</title><p>A central challenge for the brain is how to combine separate sources of information from different sensory modalities to optimally represent objects and events in the external world, such as combining someone’s speech and lip movements to better understand them in a noisy environment. At the level of individual neurons, audiovisual stimuli often elicit super-additive interactions, where the neural response is greater than the sum of auditory and visual responses. However, investigations using electroencephalography (EEG) to record brain activity have revealed inconsistent interactions, with studies reporting a mix of super- and sub-additive effects. A possible explanation for this inconsistency is that standard univariate analyses obscure multisensory interactions present in EEG responses by overlooking multivariate changes in activity across the scalp. To address this shortcoming, we investigated EEG responses to audiovisual stimuli using inverted encoding, a population tuning approach that uses multivariate information to characterise feature-specific neural activity. Participants (n = 41) completed a spatial localisation task for both unisensory stimuli (auditory clicks, visual flashes) and combined audiovisual stimuli (spatiotemporally congruent clicks and flashes). To assess multivariate changes in EEG activity, we used inverted encoding to recover stimulus location information from event-related potentials (ERPs). Participants localised audiovisual stimuli more accurately than unisensory stimuli alone. For univariate ERP analyses we found an additive multisensory interaction. By contrast, multivariate analyses revealed a super-additive interaction ∼180 ms following stimulus onset, such that the location of audiovisual stimuli was decoded more accurately than that predicted by maximum likelihood estimation. Our results suggest that super-additive integration of audiovisual information is reflected within multivariate patterns of activity rather than univariate evoked responses.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Introduction updated to with more information on super-additivity in EEG.
Discussion updated with revised arguments on the influence of eye-movements.
Restructured manuscript to have Methods after Discussion (figure numbers updated accordingly).
Methods and Results updated to improve clarity on procedure and analyses.
Included Supplemental Materials.
Error on Figure 2 corrected.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/8CDRA">https://doi.org/10.17605/OSF.IO/8CDRA</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>We exist in a complex, dynamically changing sensory environment. Vertebrates, including humans, have evolved sensory organs that transduce relevant sources of physical information, such as light and changes in air pressure, into patterns of neural activity that support perception (vision and audition) and adaptive behaviour. Such activity patterns are noisy, and often ambiguous, due to a combination of external (environmental) and internal (transduction) factors. Critically, information from the different sensory modalities can be highly correlated because it is often elicited by a common external source or event. For example, the sight and sound of a hammer hitting a nail produces a single, unified perceptual experience, as does the sight of a person’s lips moving as we hear their voice. To improve the reliability of neural representations, the brain leverages these sensory relationships by combining information in a process referred to as <italic>multisensory integration</italic>. The existence of such processes heighten perception, e.g., by making it easier to understand a person’s speech in a noisy setting by looking at their lip movements (<xref ref-type="bibr" rid="c60">Sumby &amp; Pollack, 1954</xref>).</p>
<p>Multisensory integration of audiovisual cues improves performance across a range of behavioural outcomes, including detection accuracy (<xref ref-type="bibr" rid="c8">Bolognini et al., 2005</xref>; <xref ref-type="bibr" rid="c24">Frassinetti et al., 2002</xref>; <xref ref-type="bibr" rid="c34">Lovelace et al., 2003</xref>), response speed (<xref ref-type="bibr" rid="c2">Arieh &amp; Marks, 2008</xref>; <xref ref-type="bibr" rid="c15">Cappe et al., 2009</xref>; <xref ref-type="bibr" rid="c17">Colonius &amp; Diederich, 2004</xref>; <xref ref-type="bibr" rid="c43">Rach &amp; Diederich, 2006</xref>; <xref ref-type="bibr" rid="c53">Senkowski et al., 2011</xref>), and saccade speed and accuracy (<xref ref-type="bibr" rid="c18">Corniel et al., 2002</xref>; <xref ref-type="bibr" rid="c64">Van Wanrooij et al., 2009</xref>). Successful integration requires the constituent stimuli to occur at approximately the same place and time (<xref ref-type="bibr" rid="c33">Leone &amp; McCourt, 2015</xref>). The degree to which behavioural performance is improved follows the principles of maximum likelihood estimation (MLE), wherein sensory information from each modality is weighted and integrated according to its relative reliability (<xref ref-type="bibr" rid="c1">Alais &amp; Burr, 2004</xref>; <xref ref-type="bibr" rid="c21">Ernst &amp; Banks, 2002</xref>; although other processing schemes have also been identified; <xref ref-type="bibr" rid="c47">Rideaux &amp; Welchman, 2018</xref>). As such, behavioural performance that matches MLE predictions is often seen as a benchmark of successful, optimal integration of relevant unisensory cues.</p>
<p>The ubiquity of behavioural enhancements for audiovisual stimuli suggests there are fundamental neural mechanisms that facilitate improved precision. Recordings from single multisensory (audiovisual) neurons within cat superior colliculus have revealed the principle of inverse effectiveness, whereby the increased response to audiovisual stimuli is larger when the constituent unisensory stimuli are weakly stimulating (<xref ref-type="bibr" rid="c18">Corniel et al., 2002</xref>; <xref ref-type="bibr" rid="c35">Meredith &amp; Stein, 1983</xref>). Depending on the intensity of the integrated stimuli, the neural response can be either <italic>super-additive,</italic> where the multisensory response is greater than the sum of the unisensory responses, <italic>additive,</italic> equal to the sum of responses, or <italic>sub-additive</italic>, where the combined response is less than the sum of the unisensory responses (see <xref ref-type="bibr" rid="c55">Stein &amp; Stanford, 2008</xref>). Inverse effectiveness has also been observed in human behavioural experiments, with low intensity audiovisual stimuli eliciting greater multisensory enhancements in response precision than those of high intensity (<xref ref-type="bibr" rid="c17">Colonius &amp; Diederich, 2004</xref>; <xref ref-type="bibr" rid="c18">Corniel et al., 2002</xref>; <xref ref-type="bibr" rid="c43">Rach &amp; Diederich, 2006</xref>; <xref ref-type="bibr" rid="c44">Rach et al., 2010</xref>).</p>
<p>Neuroimaging methods, such as electroencephalography (EEG) and functional magnetic resonance imaging (fMRI), have been used to investigate neural population-level audiovisual integration in humans. These studies have typically applied an additive criterion to quantify multisensory integration, wherein successful integration is marked by a non-linear enhancement of audiovisual responses relative to unisensory responses (<xref ref-type="bibr" rid="c7">Besle et al., 2004</xref>). The super- or sub-additive nature of this enhancement, however, is often inconsistent. In fMRI, neural super-additivity in blood-oxygen-level dependent (BOLD) responses to audiovisual stimuli has been found in a variety of regions, primarily the superior temporal sulcus (STS; <xref ref-type="bibr" rid="c12">Calvert et al., 2000</xref>; <xref ref-type="bibr" rid="c13">Calvert et al., 2001</xref>; <xref ref-type="bibr" rid="c57">Stevenson et al., 2007</xref>; <xref ref-type="bibr" rid="c59">Stevenson &amp; James, 2009</xref>; <xref ref-type="bibr" rid="c68">Werner &amp; Noppeney, 2010</xref>, <xref ref-type="bibr" rid="c69">2011</xref>). However, other studies have failed to replicate audiovisual super-additivity in the STS (<xref ref-type="bibr" rid="c29">Joassin et al., 2011</xref>; <xref ref-type="bibr" rid="c41">Porada et al., 2021</xref>; <xref ref-type="bibr" rid="c51">Ross et al., 2022</xref>; <xref ref-type="bibr" rid="c65">Venezia et al., 2015</xref>), or have found sub-additive responses (see <xref ref-type="bibr" rid="c52">Scheliga et al., 2023</xref> for review). As such, some have argued that BOLD responses are not sensitive enough to adequately characterise super-additive audiovisual interactions within populations of neurons (<xref ref-type="bibr" rid="c4">Beauchamp, 2005</xref>; <xref ref-type="bibr" rid="c28">James et al., 2012</xref>; <xref ref-type="bibr" rid="c32">Laurienti et al., 2005</xref>). In EEG, meanwhile, the evoked response to an audiovisual stimulus typically conforms to a sub-additive principle (<xref ref-type="bibr" rid="c16">Cappe et al., 2010</xref>; <xref ref-type="bibr" rid="c23">Fort et al., 2002</xref>; <xref ref-type="bibr" rid="c26">Giard &amp; Peronnet, 1999</xref>; <xref ref-type="bibr" rid="c37">Murray et al., 2016</xref>; <xref ref-type="bibr" rid="c42">Puce et al., 2007</xref>; <xref ref-type="bibr" rid="c56">Stekelenburg &amp; Vroomen, 2007</xref>; <xref ref-type="bibr" rid="c62">Teder-Sälejärvi et al., 2002</xref>; <xref ref-type="bibr" rid="c66">Vroomen &amp; Stekelenburg, 2010</xref>). However, when the principle of inverse effectiveness is considered and relatively weak stimuli are presented together, there has been some evidence for super-additive responses (<xref ref-type="bibr" rid="c53">Senkowski et al., 2011</xref>).</p>
<p>It is important to consider the differences in how super-additivity is classified between neural and behavioural measures. At the level of single neurons, super-additivity is defined as a non-linear response enhancement, with the multisensory response exceeding the sum of the unisensory responses. In behaviour, meanwhile, it has been observed that the performance improvement from combining two senses is close to what is expected from optimal integration of information across the senses (<xref ref-type="bibr" rid="c1">Alais &amp; Burr, 2004</xref>; Stanford &amp; Stein, 2007). Critically, behavioural enhancement of this kind does not require non-linearity in the neural response, but can arise from a reliability-weighted average of sensory information. In short, behavioural performance that conforms to MLE is not necessarily indicative of neural super-additivity, and the MLE model can be considered a linear baseline for multisensory integration.</p>
<p>While behavioural outcomes for multisensory stimuli can be predicted by MLE, and single neuron responses follow the principles of inverse effectiveness and super-additivity, among others (<xref ref-type="bibr" rid="c46">Rideaux et al., 2021</xref>), how audiovisual super-additivity manifests within populations of neurons is comparatively unclear given the mixed findings from relevant fMRI and EEG studies. This uncertainty may be due to biophysical limitations of human neuroimaging techniques, but it may also be related to the analytic approaches used to study these recordings. For instance, super-additive responses to audiovisual stimuli in EEG studies are often reported from very small electrode clusters (<xref ref-type="bibr" rid="c36">Molholm et al., 2002</xref>; <xref ref-type="bibr" rid="c53">Senkowski et al., 2011</xref>; <xref ref-type="bibr" rid="c61">Talsma et al., 2007</xref>), suggesting that neural super-additivity in humans may be highly specific. However, information encoded by the brain can be represented as increased activity in some areas, accompanied by decreased activity in others, so simplifying complex neural responses to the average rise and fall of activity in specific sensors may obscure relevant multivariate patterns of activity evoked by a stimulus.</p>
<p>Inverted encoding is a multivariate analytic method that can reveal how sensory information is encoded within the brain by recovering patterns of neural activity associated with different stimulus features. This method has been successfully used in fMRI, EEG, and magnetoencephalography studies to characterise the neural representations of a range of stimulus features, including colour (<xref ref-type="bibr" rid="c10">Brouwer &amp; Heeger, 2009</xref>), spatial location (<xref ref-type="bibr" rid="c5">Bednar &amp; Lalor, 2020</xref>; <xref ref-type="bibr" rid="c49">Robinson et al., 2021</xref>) and orientation (<xref ref-type="bibr" rid="c11">Brouwer &amp; Heeger, 2011</xref>; <xref ref-type="bibr" rid="c27">Harrison et al., 2023</xref>; <xref ref-type="bibr" rid="c31">Kok et al., 2017</xref>). A multivariate approach may capture potential non-linear enhancements associated with audiovisual responses and thus could reveal super-additive interactions that would otherwise be hidden within the brain’s univariate responses. The sensitivity of inverted encoding analyses to multivariate neural patterns may provide insight into how audiovisual information is processed and integrated at the population level.</p>
<p>In the present study, we investigated neural super-additivity in human audiovisual sensory processing using inverted encoding of EEG responses during a task where participants had to spatially localise visual, auditory, and audiovisual stimuli. In a separate behavioural experiment, we monitored response accuracy to characterise behavioural improvements to audiovisual relative to unisensory stimuli. Although there was no evidence for super-additivity in response to audiovisual stimuli within univariate ERPs, we observed a reliable non-linear enhancement of multivariate decoding performance at ∼180 ms following stimulus onset when auditory and visual stimuli were presented concurrently as opposed to alone. These findings suggest that population-level super-additive multisensory neural responses are present within multivariate patterns of activity rather than univariate evoked responses.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Behavioural performance</title>
<p>Participants performed well in discriminating stimulus location across all conditions in both the behavioural and EEG sessions (<xref rid="fig1" ref-type="fig">Figure 1</xref>). For the behavioural session, the psychometric curves for responses as a function of stimulus location showed stereotypical relationships for the auditory, visual, and audiovisual conditions (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). A quantification of the behavioural sensitivity (i.e., steepness of the curves) revealed significantly higher sensitivity for the audiovisual stimuli (<italic>M</italic> = .04, <italic>SD</italic> = .02) than for the auditory stimuli alone (<italic>M</italic> = .03, <italic>SD</italic> = .01; <italic>Z</italic> = -3.09, <italic>p</italic> = .002), and than for the visual stimuli alone (<italic>M</italic> = .02, <italic>SD</italic> = .01; <italic>Z</italic> = -5.28, <italic>p</italic> = 1.288e-7; <xref rid="fig1" ref-type="fig">Figure 1B</xref>). Sensitivity for auditory stimuli was also significantly higher than sensitivity for visual stimuli (<italic>Z</italic> = 2.02, <italic>p</italic> = .044). To test for successful integration of stimuli in the audiovisual condition, we calculated the predicted MLE sensitivity from the unisensory auditory and visual results. We found no evidence for a significant difference between the predicted and actual audiovisual sensitivity (<italic>Z</italic> = - 1.54, <italic>p</italic> = 0.125).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Behavioural performance is improved for audiovisual stimuli.</title><p><bold>A</bold>) Average accuracy of responses across participants in the behavioural session at each stimulus location for each stimulus condition, fitted to a psychometric curve. Steeper curves indicate greater sensitivity in identifying stimulus location. <bold>B</bold>) Average sensitivity across participants in the behavioural task, estimated from psychometric curves, for each stimulus condition. The red cross indicates estimated performance assuming optimal (MLE) integration of unisensory cues. <bold>C</bold>) Average behavioural sensitivity across participants in the EEG session for each stimulus condition. Error bars indicate ±1 SEM.</p></caption>
<graphic xlink:href="580589v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We repeated these analyses for behavioural performance in the EEG session (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). We found a similar pattern of results to those in the behavioural session; sensitivity for audiovisual stimuli (<italic>M</italic> = .85, <italic>SD</italic> = .33) was significantly higher than for auditory (<italic>M</italic> = .69, <italic>SD</italic> = .41; <italic>Z</italic> = -2.27, <italic>p</italic> = .023) and visual stimuli alone (<italic>M</italic> = .61, <italic>SD</italic> = .29; <italic>Z</italic> = -3.52, <italic>p</italic> = 4.345e-4), but not significantly different from the MLE prediction (<italic>Z</italic> = -1.07, <italic>p</italic> = .285). However, sensitivity for auditory stimuli was not significantly different from sensitivity to visual stimuli (<italic>Z</italic> = 1.12, <italic>p</italic> = .262).</p>
</sec>
<sec id="s2b">
<title>Event-related potentials</title>
<p>We plotted the ERPs for auditory, visual, and audiovisual conditions at each stimulus location from -100ms to 500ms around stimulus presentation (<xref rid="fig2" ref-type="fig">Figure 2</xref>). For each stimulus location, cluster corrected <italic>t</italic>-tests were conducted to assess significant differences in ERP amplitude between the unisensory (auditory and visual) and audiovisual conditions. While auditory ERPs did not significantly differ from the audiovisual, visual ERPs were significantly lower in amplitude than audiovisual ERPs at all stimulus locations (typically from ∼80-130 ms following stimulus presentation).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Audiovisual ERPs follow an additive principle.</title>
<p>Average ERP amplitude for each modality condition. Five plots represent the different stimulus locations, as indicated by the grey inset, and the final plot (bottom-right) shows the difference between the summed auditory and visual responses and the audiovisual response. Shaded error bars indicate ±1 SEM. Orange horizontal bars indicate cluster corrected periods of significant difference between visual and audiovisual ERP amplitudes.</p></caption>
<graphic xlink:href="580589v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To test whether the enhancement in response amplitude to audiovisual stimuli was super-additive, we compared this response with the sum of the response amplitudes for visual and auditory conditions, averaged over stimulus location. We found no significant difference between the additive and audiovisual ERPs (<xref rid="fig2" ref-type="fig">Figure 2</xref>, bottom right). This result suggests that, using univariate analyses, the audiovisual response was additive and did not show any evidence for super- or sub-additivity.</p>
</sec>
<sec id="s2c">
<title>Inverted encoding results</title>
<p>We next used inverted encoding to calculate the spatial decoding accuracy for auditory, visual, and audiovisual stimuli (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). For all conditions, we found that spatial location could be reliably decoded from approximately ∼100-150 ms after stimulus onset. Decoding for all conditions was consistent for most of the epoch, indicating that location information within the neural signal was relatively persistent and robust.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Spatiotemporal representation of audiovisual location.</title><p><bold>A</bold>) Accuracy of locations decoded from neural responses for each stimulus condition. Shaded error bars indicate ±1 SEM. Coloured horizontal bars indicate cluster corrected periods that showed a significant difference from chance (0). <bold>B</bold>) Topographic decoding performance in each condition during critical period (grey inset in (<bold>A</bold>)).</p></caption>
<graphic xlink:href="580589v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To assess the spatial representation of the neural signal containing location-relevant information, we computed decoding accuracy at each electrode from 150-250 ms post-stimulus presentation (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). For auditory stimuli, information was primarily based over bilateral temporal regions, whereas for visual and audiovisual stimuli, the occipital electrodes carried the most information.</p>
</sec>
<sec id="s2d">
<title>Multivariate super-additivity</title>
<p>Although the univariate response did not show evidence for super-additivity, we expected the multivariate measure would be more sensitive to nonlinear audiovisual integration. To test whether a super-additive interaction was present in the multivariate response, we calculated the sensitivity of the decoder in discriminating stimuli presented on the left and right side. The pattern of decoding sensitivity for auditory, visual, and audiovisual stimuli (<xref rid="fig4" ref-type="fig">Figure 4A</xref>) was similar to that in decoding accuracy (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). Notably, audiovisual sensitivity was significantly greater than sensitivity to auditory and visual stimuli alone, particularly ∼180 ms following stimulus onset. To test whether this enhanced sensitivity reflected super-additivity, we compared decoding sensitivity for audiovisual stimuli with two estimates of linearly combined unisensory stimuli: 1) MLE predicted sensitivity based on auditory and visual sensitivity and 2) aggregate responses of auditory and visual stimuli (<xref rid="fig4" ref-type="fig">Figure 4B</xref>). We found that audiovisual sensitivity significantly exceeded both estimates of linear combination (MLE, ∼160-220 ms post-stimulus; aggregate, ∼150-250 ms). These results provide evidence of non-linear audiovisual integration in the multivariate pattern of EEG recordings. Taken together with the ERP results, our findings suggest that super-additive integration of audiovisual information is reflected in multivariate patterns of activity, but not univariate evoked responses.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Super-additive multisensory interaction in multivariate patterns of EEG activity.</title><p><bold>A</bold>) Decoding sensitivity in each stimulus condition across the epoch. Overall trends closely matched decoding accuracy. <bold>B</bold>) Predicted (optimal sensitivity through MLE and aggregate A+V) and actual audiovisual sensitivity across the epoch. Coloured horizontal bars indicate cluster corrected periods where actual sensitivity significantly exceeded that which was predicted. Shaded error bars indicate ±1 SEM.</p></caption>
<graphic xlink:href="580589v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2e">
<title>Neurobehavioural correlations</title>
<p>As behavioural and neural data violated assumptions of normality, we calculated rank-order correlations (Spearman’s rho) between the average decoding sensitivity for each participant from 150-250 ms post-stimulus onset and behavioural performance on the EEG task. As Spearman’s rho is resistant to outliers (<xref ref-type="bibr" rid="c72">Wilcox, 2016</xref>), we did not perform outlier rejection. We found that decoding sensitivity was significantly positively correlated with behavioural sensitivity for audiovisual stimuli (<italic>r</italic> = .43, <italic>p</italic> = .003), but not for auditory (<italic>r</italic> = -.04, <italic>p</italic> = .608) or visual stimuli (<italic>r</italic> = .15, <italic>p</italic> = .170) alone.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Audiovisual decoding sensitivity is significantly positively correlated with behavioural sensitivity.</title>
<p>Correlations (Spearman’s rho) are shown between decoding and behavioural sensitivity from the EEG session (150-250 ms post-stimulus onset) for each stimulus condition, with a line of best fit.</p></caption>
<graphic xlink:href="580589v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We tested for super-additivity in multivariate patterns of EEG responses to audiovisual stimuli. Participants judged the location of auditory, visual, and audiovisual stimuli while their brain activity was measured using EEG. As expected, participants’ behavioural responses to audiovisual stimuli were more precise than that for unisensory auditory and visual stimuli. ERP analyses showed that although audiovisual stimuli elicited larger responses than visual stimuli, the overall response followed an additive principle. Critically, our multivariate analyses revealed that decoding sensitivity for audiovisual stimuli exceeded predictions of both MLE and aggregate auditory and visual information, indicating non-linear multisensory enhancement (i.e., super-additivity).</p>
<p>Participants localised audiovisual stimuli more accurately than unisensory in both the behavioural and EEG sessions. This behavioural facilitation in response to audiovisual stimuli is well-established within the literature (<xref ref-type="bibr" rid="c8">Bolognini et al., 2005</xref>; <xref ref-type="bibr" rid="c24">Frassinetti et al., 2002</xref>; <xref ref-type="bibr" rid="c34">Lovelace et al., 2003</xref>; <xref ref-type="bibr" rid="c35">Meredith &amp; Stein, 1983</xref>; <xref ref-type="bibr" rid="c53">Senkowski et al., 2011</xref>). In accordance with theories of optimal cue integration, we found participants’ performance for audiovisual stimuli in both sessions matched that predicted by MLE (<xref ref-type="bibr" rid="c21">Ernst &amp; Banks, 2002</xref>). Matching this ‘optimal’ prediction of performance indicates that the auditory and visual cues were successfully integrated when presented together in the audiovisual condition (<xref ref-type="bibr" rid="c22">Fetsch et al., 2013</xref>).</p>
<p>Our EEG analyses revealed that for most spatial locations, audiovisual stimuli elicited a significantly greater neural response than exclusively visual stimuli approximately 80-120 ms after stimulus onset. Despite numerically larger ERPs to audiovisual than auditory stimuli, this effect failed to reach significance, most likely due to greater inter-trial variability in the auditory ERPs. Critically, however, the audiovisual ERPs consistently matched the sum of visual and auditory ERPs. Sub- or super-additive interaction effects in neural responses to multisensory stimuli are a hallmark of successful integration of unisensory cues in ERPs (<xref ref-type="bibr" rid="c7">Besle et al., 2004</xref>; <xref ref-type="bibr" rid="c58">Stevenson et al., 2014</xref>). An additive ERP in this context cannot imply successful multisensory integration, as the multisensory ‘enhancement’ may be the result of recording from distinct populations of unisensory neurons responding to the two unisensory sensory modalities (<xref ref-type="bibr" rid="c6">Besle et al., 2009</xref>). This invites the question of why we see evidence for integration at the behavioural level, but not in the amplitude of neural responses. One explanation could be that the signals measured by EEG simply do not contain evidence of non-linear integration because the super-additive responses are highly spatiotemporally localized and filtered out by the skull before reaching the EEG sensors. Another possibility, however, is that evidence for non-linear integration is only observable within the changing pattern of ERPs across sensors. Indeed, <xref ref-type="bibr" rid="c37">Murray et al. (2016)</xref> found that multisensory interactions followed from changes in scalp topography rather than net gains to ERP amplitude.</p>
<p>Our decoding results reveal that not only do audiovisual stimuli elicit more distinguishable patterns of activity than visual and auditory stimuli, but this enhancement exceeds that predicted by both optimal integration and the aggregate combination of auditory and visual responses. Critically, the non-linear enhancement of decoding sensitivity for audiovisual stimuli indicates the presence of an interactive effect for the integration of auditory and visual stimuli that was not evident from the univariate analyses. This indicates super-additive enhancement of the neural representation of integrated audiovisual cues, and supports the interpretation that increased behavioural performance for multisensory stimuli is related to a facilitation of the neural response (<xref ref-type="bibr" rid="c22">Fetsch et al., 2013</xref>). This interaction was absent from univariate analyses (<xref ref-type="bibr" rid="c38">Nikbakht et al., 2018</xref>), suggesting that the neural facilitation of audiovisual processing is more nuanced than net increased excitation, and may be associated with a complex pattern of excitatory and inhibitory neural activity, e.g., divisive normalization (<xref ref-type="bibr" rid="c39">Ohshiro et al., 2017</xref>).</p>
<p>The non-linear neural enhancement in decoding sensitivity for audiovisual stimuli occurred ∼180 ms after stimulus onset, which is later than previously reported audiovisual interactions (&lt;150 ms; <xref ref-type="bibr" rid="c16">Cappe et al., 2010</xref>; <xref ref-type="bibr" rid="c23">Fort et al., 2002</xref>; <xref ref-type="bibr" rid="c26">Giard &amp; Peronnet, 1999</xref>; <xref ref-type="bibr" rid="c36">Molholm et al., 2002</xref>; <xref ref-type="bibr" rid="c37">Murray et al., 2016</xref>; <xref ref-type="bibr" rid="c53">Senkowski et al., 2011</xref>; <xref ref-type="bibr" rid="c61">Talsma et al., 2007</xref>; <xref ref-type="bibr" rid="c62">Teder-Sälejärvi et al., 2002</xref>). As stimulus characteristics and task requirements are likely to have a significant influence over the timing of multisensory interaction effects in EEG activity (<xref ref-type="bibr" rid="c14">Calvert &amp; Thesen, 2004</xref>; <xref ref-type="bibr" rid="c19">De Meo et al., 2015</xref>), our use of peripheral spatial locations (where previous studies only used stimuli centrally) may explain the slightly later timing of our audiovisual effect. Indeed, our finding is consistent with previous multivariate studies which found that location information in EEG data, for both visual (<xref ref-type="bibr" rid="c45">Rideaux, 2024</xref>; <xref ref-type="bibr" rid="c49">Robinson et al., 2021</xref>) and auditory (<xref ref-type="bibr" rid="c5">Bednar &amp; Lalor, 2020</xref>) stimuli, is maximal at ∼190 ms following stimulus presentation.</p>
<p>An interesting aspect of our results is the apparent mismatch between the behavioural and neural responses. While the behavioural results meet the optimal statistical threshold predicted by MLE, the decoding analyses suggest that the neural response exceeds it. Though non-linear neural responses and statistically optimal behavioural responses are reliable phenomena in multisensory integration (<xref ref-type="bibr" rid="c1">Alais &amp; Burr, 2004</xref>; <xref ref-type="bibr" rid="c21">Ernst &amp; Banks, 2002</xref>; Stanford &amp; Stein, 2007), the question remains – if neural super-additivity exists to improve behavioural performance, why is it not reflected in behavioural responses? A possible explanation for this neurobehavioural discrepancy is the large difference in timing between sensory processing and behavioural responses. A motor response would typically occur some time after the neural response to a sensory stimulus (e.g., 70-200 ms), with subsequent neural processes between perception and action that introduce noise (Heekeren et al., 2008) and may obscure super-additive perceptual sensitivity. In the current experiment, participants reported either the distribution of 20 serially presented stimuli (EEG session) or compared the positions of two stimuli (behavioural session), whereas the decoder attempts to recover the location of every presented stimulus. While stimulus location could be represented with higher fidelity in multisensory relative to unisensory conditions, this would not necessarily result in better performance on a binary behavioural task in which multiple temporally separated stimuli are compared. One must also consider the inherent differences in how super-additivity is measured at the neural and behavioural levels. Neural super-additivity should manifest in responses to each individual stimulus. In contrast, behavioural super-additivity is often reported as proportion correct, which can only emerge between conditions after being averaged across multiple trials. The former is a biological phenomenon, while the latter is an analytical construct. In our experiment, we recorded neural responses for every presentation of a stimulus, but behavioural responses were only obtained after multiple stimulus presentations. Thus, the failure to find super-additivity in behavioural responses might be due to their operationalisation, with between-condition comparisons lacking sufficient sensitivity to detect super-additive sensory improvements. Future work should focus on experimental designs that can reveal super-additive responses in behaviour.</p>
<p>We also found a significant positive correlation between participants’ behavioural judgements in the EEG session and decoding sensitivity for audiovisual stimuli. This result suggests that participants who were better at identifying stimulus location also had more reliably distinct patterns of neural activity. The lack of neurobehavioural correlation in the unisensory conditions might suggest a poor correspondence between the different tasks, perhaps indicative of the differences between behavioural and neural measures explained previously. However, multisensory stimuli have consistently been found to elicit stronger neural responses than unisensory stimuli (<xref ref-type="bibr" rid="c35">Meredith &amp; Stein, 1983</xref>; <xref ref-type="bibr" rid="c42">Puce et al., 2007</xref>; <xref ref-type="bibr" rid="c53">Senkowski et al., 2011</xref>; <xref ref-type="bibr" rid="c66">Vroomen &amp; Stekelenburg, 2010</xref>), which has been associated with behavioural performance (<xref ref-type="bibr" rid="c25">Frens &amp; Van Opstal, 1998</xref>; <xref ref-type="bibr" rid="c67">Wang et al., 2008</xref>). Thus, the weaker signal-to-noise ratio in unisensory conditions may prevent correlations from being detected.</p>
<p>Any experimental design that varies stimulus location needs to consider the potential contribution of eye movements. We computed correlations between participants’ average eye position and each stimulus position between the three sensory conditions (auditory, visual and audiovisual; Figure S1) and found evidence that participants made eye movements toward stimuli. A re-analysis of the data with a very strict eye-movement criterion (i.e., removing trials with eye movements &gt;1.875°) revealed that the super-additive enhancement in decoding accuracy no longer survived cluster correction, suggesting that our results may be impacted by the consistent motor activity of saccades towards presented stimuli. Further investigation, however, suggests this is unlikely. Though the correlations were significantly different from 0, they were not significantly different from each other. If consistent saccades to audiovisual stimuli were responsible for the nonlinear multisensory benefit we observed, we would expect to find a higher positive correlation between horizontal eye position and stimulus location in the audiovisual condition than in the auditory or visual conditions. Interestingly, eye movements corresponded more to stimulus location in the auditory and audiovisual conditions than in the visual condition, indicating that it was the presence of a sound, rather than a visual stimulus, that drove small eye movements. This could indicate that participants inadvertently moved their eyes when localising the origin of sounds. We also re-ran our analyses using the activity measured from the frontal electrodes alone (Figure S2). If the source of the nonlinear decoding accuracy in the audiovisual condition was due to muscular activity produced by eye movements, there should be better decoding accuracy from sensors closer to the source. Instead, we found that decoding accuracy of stimulus location from the frontal electrodes (peak d’ = 0.08) was less than half that of decoding accuracy from the more posterior electrodes (peak d’ = 0.18). These results suggest that the source of neural activity containing information about stimulus position was located over occipito-parietal areas, consistent with our topographical analyses (inset of <xref rid="fig3" ref-type="fig">Figure 3</xref>).</p>
<p>In summary, here we have shown a non-linear enhancement in the neural representation of audiovisual stimuli relative to unisensory (visual/auditory) stimuli. This enhancement was obscured within univariate ERP analyses focusing exclusively on response amplitude but was revealed through inverted encoding analyses in feature-space, suggesting that super-additive integration of audiovisual information is reflected within multivariate patterns of activity rather than univariate evoked responses. Further research on the multivariate representation of audiovisual integration may shed light on the neural mechanisms that facilitate this non-linear enhancement. In particular, future work may consider the influence of different stimulus features and task requirements on the timing and magnitude of the audiovisual enhancement. How and when auditory and visual information are integrated to enhance multisensory processing remains an open question, with evidence for a complex combination of top-down and bottom-up interactions (<xref ref-type="bibr" rid="c20">Delong &amp; Noppeney, 2021</xref>; <xref ref-type="bibr" rid="c30">Keil &amp; Senkowski, 2018</xref>; <xref ref-type="bibr" rid="c50">Rohe &amp; Noppeney, 2018</xref>). Our study highlights the importance of considering multivariate analyses in multisensory research, and the potential loss of stimulus-relevant neural information when relying solely on univariate responses.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Seventy-one human adults were recruited in return for payment. The study was approved by The University of Queensland Human Research Ethics Committee, and informed consent was obtained in all cases. Participants were first required to complete a behavioural session with above 60% accuracy in all conditions to qualify for the EEG session (see Behavioural session for details). Twenty-nine participants failed to meet this criterion and were excluded from further participation and analyses, along with one participant who failed to complete the EEG session with above chance behavioural accuracy. This left a total of 41 participants (<italic>M</italic> = 27.21 yrs; min = 20 yrs; max = 64 yrs; 24 females; 41 right-handed). Participants reported no neurological or psychiatric disorders, and had normal visual acuity (assessed using a standard Snellen eye chart).</p>
</sec>
<sec id="s4b">
<title>Materials and procedure</title>
<p>The experiment was split into two separate sessions, with participants first completing a behavioural session followed by an EEG session. Each session had three conditions, in which the presented stimuli were either visual, auditory, or combined audio and visual (audiovisual). Conditions were not interleaved, but the order in which conditions were presented was counterbalanced across participants. Before each task, participants were given instructions and completed two rounds of practice for each condition.</p>
</sec>
<sec id="s4c">
<title>Apparatus</title>
<p>The experiment was conducted in a dark, acoustically and electromagnetically shielded room. For the EEG session, stimuli were presented on a 24-inch ViewPixx monitor (VPixx Technologies Inc., Saint-Bruno, QC) with 1920×1080-pixel resolution and a refresh rate of 144 Hz. Viewing distance was maintained at 54 cm using a chinrest. For the behavioural session, stimuli were presented on a 32-inch Cambridge Research Systems Display++ LCD monitor with a 1920×1080-pixel resolution, hardware gamma correction and a refresh rate of 144Hz. Viewing distance was maintained at 59.5 cm using a chinrest. Stimuli were generated in MATLAB v2021b (<xref ref-type="bibr" rid="c63">The MathWorks Inc., 2021</xref>) using the Psychophysics Toolbox (<xref ref-type="bibr" rid="c9">Brainard, 1997</xref>). Auditory stimuli were played through two loudspeakers placed either side of the display (80 cm apart for the behavioural session, 58cm apart for the EEG session). The loudspeakers had a power handling capacity of 25-75 W and a nominal impedance of 6 Ω. In both experiments, an EyeLink 1000 infrared eye tracker recorded gaze direction (SR Research Ltd., 2009) at a sampling rate of 1000 Hz.</p>
</sec>
<sec id="s4d">
<title>Stimuli</title>
<p>The EEG and behavioural paradigms used the same stimuli within each condition. Visual stimuli were gaussian blobs (0.2 contrast, 16° diameter) presented for 16 ms on a mid-grey background. Auditory stimuli were 100 ms, 850 Hz tones with a decay function (sample rate = 44, 100 Hz; volume = 60 dBA SPL, as measured at the ears). Audiovisual stimuli were spatially and temporally matched combinations of the audio and visual stimuli, with no changes to stimuli properties. To manipulate spatial location, target stimuli were presented from multiple horizontal locations along the display, centred on linearly spaced locations from 15° visual angle to the left and right of the display centre (eight locations for behavioural, five for EEG). Auditory stimuli were played through two speakers placed equidistantly either side of the display. The perceived source location of auditory stimuli was manipulated via changes to interaural level and timing (<xref ref-type="bibr" rid="c70">Whitworth &amp; Jeffress, 1961</xref>; <xref ref-type="bibr" rid="c71">Wightman &amp; Kistler, 1992</xref>). The precise timing of when each speaker delivered an auditory stimulus was calculated from the following formula:
<disp-formula>
<graphic xlink:href="580589v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>x</italic> and <italic>z</italic> are the horizontal and forward distances in metres between the ears and the source of the sound on the display, respectively, <italic>r</italic> is the head radius, and <italic>s</italic> is the speed of sound. We used a constant approximate head radius of 8 cm for all participants. <italic>r</italic> was added to <italic>x</italic> for the left speaker and subtracted for the right speaker to produce the interaural time difference. For ±15° source locations, interaural timing difference was 1.7 ms. To simulate the decrease in sound intensity as a function of distance, we calculated interaural level differences for the left and right speakers by dividing the sounds by the left and right distance vectors. Finally, we resampled the sound using linear interpolation based on the calculations of the interaural level and timing differences. This process was used to calculate the soundwaves played by the left and right speakers for each of the possible stimulus locations on the display. The maximum interaural level difference between speakers was 0.14 <italic>A</italic> for ±15° auditory locations, and 0.07 <italic>A</italic> for ±7.5°.</p>
</sec>
<sec id="s4e">
<title>Behavioural Session</title>
<p>During pilot testing, stimulus features (size, luminance, volume, frequency etc.) were manipulated to make visual and auditory stimuli similarly difficult to spatially localize. These values were held constant in the main experiment. We employed a two-interval forced choice design to measure participants’ audiovisual localization sensitivity. Participants were presented with two consecutive stimuli and tasked with indicating, via button press, whether the first (‘1’ number-pad key) or second (‘2’ number-pad key) interval contained the more leftward stimulus. Each trial consisted of a central reference stimulus, and a target stimulus presented at one of eight locations along the horizontal azimuth on the display. The presentation order of the reference and target stimuli was randomised across trials. Stimulus modality was auditory, visual, or audiovisual, presented in separate blocks with short breaks (∼2 min) between conditions (see <xref rid="fig6" ref-type="fig">Figure 6A</xref> for an example trial). The order of conditions was counterbalanced across participants. Each condition consisted of 384 target presentations across the eight origin locations, leading to 48 presentations at each location.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Experimental design of behavioural and EEG sessions.</title><p><bold>A)</bold> An example trial for the audiovisual condition in the behavioural session. Each trial consisted of a (centred) reference stimulus and a target stimulus presented at one of eight locations along the horizontal meridian of the display. <bold>B)</bold> An example trial for the audiovisual condition in the EEG session. The top row displays the possible locations of stimuli. In each trial, participants were presented with 20 stimuli that were each spatially localised to one of five possible locations along the horizontal meridian. The task was to determine if there were more stimuli presented to the left or right of fixation.</p></caption>
<graphic xlink:href="580589v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s4f">
<title>EEG Session</title>
<p>In this session, the experimental task was changed slightly from the behavioural session to increase the number stimulus presentations required for inverted encoding analyses of EEG data. Participants viewed and/or listened to a sequence of 20 stimuli, each of which were presented at one of five horizontal locations along the display (selected at random). At the end of each sequence, participants were tasked with indicating, via button press, whether more presentations appeared on the right (‘right’ arrow key) or the left (‘left’ arrow key) of the display. To minimize eye movements, participants were asked to fixate on a black dot presented 8° above the display centre (see <xref rid="fig6" ref-type="fig">Figure 6B</xref> for an example trial). The task used in the EEG session included the same blocked conditions as in the behavioural session, i.e., visual, auditory, and (congruent) audiovisual stimuli. As the locations of stimuli were selected at random, some sequences had an equal number of presentations on each side of the display, and thus had no correct “left” or “right” response; these trials were not included in the analysis of behavioural performance. Each block consisted of 10 trials, followed by a feedback display indicating the number of trials participants answered correctly. Each condition consisted of 12 blocks, yielding a total of 2400 presentations for each.</p>
</sec>
<sec id="s4g">
<title>EEG data pre-processing</title>
<p>EEG data were recorded using a 64-channel BioSemi system at a sampling rate of 1024 Hz, which was down-sampled to 512 Hz during preprocessing. Signals were recorded with reference to the CMS/DRL electrode loop, with bipolar electrodes placed above and below the eye, at the temples, and at each mastoid to monitor for eye movements and muscle artifacts. EEG preprocessing was undertaken in MATLAB using custom scripts and the EEGLAB toolbox (Delorme &amp; Makeig, 2004). Data were high-pass filtered at 0.25 Hz to remove baseline drifts, and re-referenced according to the average of all 64 channels. Analyses were stimulus locked, with ERP responses segmented into 600 ms epochs from 100 ms before stimulus presentation to 500 ms after stimulus presentation. We removed trials with substantial eye movements (&gt;3.75 away from fixation) from the analyses. After the removal of eye movements, on average 2365 (<italic>SD</italic> = 56.94), 2346 (<italic>SD</italic> = 152.87) and 2350 (<italic>SD</italic> = 132.47) trials remained for auditory, visual and audiovisual conditions, respectively, from the original 2400 per condition.</p>
</sec>
<sec id="s4h">
<title>Forward model</title>
<p>To describe the neural representations of sensory stimuli, we used an inverted modelling approach to reconstruct the location of stimuli based upon recorded ERPs (<xref ref-type="bibr" rid="c11">Brouwer &amp; Heeger, 2011</xref>; <xref ref-type="bibr" rid="c27">Harrison et al., 2023</xref>). Analyses were performed separately for visual, auditory, and audiovisual stimuli. We first created an encoding model that characterised the patterns of activity in the EEG electrodes given the five locations of the presented stimuli. The encoding model was then used to obtain the inverse decoding model that described the transformation from electrode activity to stimulus location. We used a 10-fold cross-validation approach where 90% of the data were used to obtain the inverse model on which the remaining 10% of the data were decoded. Cross-validation was repeated 10 times such that all the data were decoded. For the purposes of these analyses, we assume that EEG electrode noise is isotropic across locations and additive with the signal.</p>
<p>Prior to the neural decoding analyses, we established the sensors that contained the most location information by treating time as the decoding dimension and obtaining the inverse models from each electrode, using 10-fold cross-validation. This revealed that location was primarily represented in posterior electrodes for visual and audiovisual stimuli, and in central electrodes for auditory stimuli. Thus, for all subsequent analyses we only included signals from the central-temporal, parietal-occipital, occipital and inion sensors for computing the inverse model (see final inset of <xref rid="fig2" ref-type="fig">Figure 2</xref>). As the model was fitted for multiple electrodes, subtle patterns of neural information contained within combinations of sensors could be detected.</p>
<p>The encoding model contained five hypothetical channels, with evenly distributed idealised location preferences between -15° to +15° viewing angles to the left and right of the display centre. Each channel consisted of a half-wave rectified sinusoid raised to the fifth power. The channels were arranged such that an idealised tuning curve of each location preference could be expressed as a weighted sum of the five channels. The observed activity for each presentation can be described by the following linear model:
<disp-formula>
<graphic xlink:href="580589v3_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <bold>B</bold> indicates the EEG data (<italic>m</italic> electrodes x <italic>n</italic> presentations), <bold>W</bold> is a weight matrix (<italic>m</italic> electrodes x 5 channels) that describes the transformation from EEG activity to stimulus location, <bold>C</bold> denotes the hypothesized channel activities (5 channels x <italic>n</italic> presentations), and <bold>E</bold> indicates the residual errors.</p>
<p>To compute the inverse model we estimated the weights that, when applied to the data, would reconstruct the channel activities with the least error. Due to the correlation between neighbouring electrodes, we took noise covariance into account when computing the model to optimize it for EEG data (<xref ref-type="bibr" rid="c27">Harrison et al., 2023</xref>; <xref ref-type="bibr" rid="c31">Kok et al., 2017</xref>; <xref ref-type="bibr" rid="c48">Rideaux et al., 2023</xref>). We then used the inverse model to reconstruct the stimulus location from the recorded ERP responses.</p>
<p>To assess how well the forward model captured location information in the neural signal per modality, two measures of performance were analysed. First, decoding accuracy was calculated as the similarity of the decoded location to the presented location, represented in arbitrary units. To test whether a super-additive interaction was present in the multivariate response, an additive threshold against which to compare the audiovisual response was required. However, it is unclear how the arbitrary units used to represent decoding accuracy translate to a measure of the linear summation of auditory and visual accuracy. As used for the behavioural analyses, MLE provides a framework for calculating the estimated optimal sensitivity of the combination of two sensory signals, according to signal detection theory principles. To compute decoding sensitivity (d’), required to apply MLE, we omitted trials where stimuli appeared in the centre of the display. The decoder’s reconstructions of stimulus location were grouped for stimuli appearing on the left and right side of the display, respectively. The proportion of hits and misses was derived by comparing the decoded side to the presented side, which was then used to calculate d’ for each condition (<xref ref-type="bibr" rid="c54">Stanislaw &amp; Todorov, 1999</xref>). The d’ from the auditory and visual conditions can be used to estimate the predicted ‘optimal’ sensitivity to audiovisual signals as calculated through the following formula:
<disp-formula>
<graphic xlink:href="580589v3_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
We can then compare actual audiovisual sensitivity to this auditory + visual sensitivity and test for super-additivity in the audiovisual condition as evidenced by the presence of a nonlinear combination of auditory and visual stimuli. A similar method was previously employed to investigate depth estimation from motion and binocular disparity cues, decoded from BOLD responses (<xref ref-type="bibr" rid="c3">Ban et al., 2012</xref>).</p>
<p>To represent an additional ‘additive’ multivariate signal with which to compare the decoding sensitivity derived through MLE, we first matched the EEG data between unisensory conditions such that the order of presented stimulus locations was the same for the auditory and visual conditions. The auditory and visual condition data were then concatenated across sensors, and inverted encoding analyses were performed on the resulting ‘additive’ audiovisual dataset. This additive condition was designed to represent neural activity evoked by both the auditory and visual conditions, without any non-linear neural interaction, and served as a baseline for the audiovisual condition.</p>
</sec>
<sec id="s4i">
<title>Statistical analyses</title>
<p>Statistical analyses were performed in MATLAB v2021b. Two metrics of accuracy were calculated to assess behavioural performance. For the behavioural session we calculated participants’ sensitivity separately for each modality condition by fitting psychometric functions to the proportion of rightward responses per stimulus location. In the EEG session participants responded to multiple stimuli rather than individual presentations, so behavioural performance was assessed via d’. We derived d’ in each condition from the average proportion of hits and misses for each participant’s performance in discriminating the side of the display on which more stimuli were presented (<xref ref-type="bibr" rid="c54">Stanislaw &amp; Todorov, 1999</xref>). A one-sample Kolmogorov-Smirnov test for each condition revealed all conditions in both sessions violated assumptions of normality. A non-parametric two-sided Wilcoxon signed-rank test was therefore used to test for significant differences in behavioural accuracy between all conditions.</p>
<p>For the neural data, univariate ERPs were calculated by averaging EEG activity across presentations and channels for each stimulus location from -100 to 500 ms around stimulus onset. A conservative mass-based cluster correction was applied to account for spurious differences across time (<xref ref-type="bibr" rid="c40">Pernet et al., 2015</xref>). To test for significant differences between conditions, a paired-samples <italic>t</italic>-test was conducted between each condition at each time point. A one-sample <italic>t-</italic>test was used when comparing decoding accuracy against chance (i.e., zero). Next, the summed value of computed <italic>t</italic> statistics associated with each comparison (separately for positive and negative values) was calculated within contiguous temporal clusters of significant values. We then simulated the null distribution of the maximum summed cluster values using permutation (<italic>n</italic> = 5000) of the location labels, from which we derived the 95% percentile threshold value. Clusters identified in the data with a summed effect-size value less than the threshold were considered spurious and removed.</p>
</sec>
<sec id="s4j">
<title>Data availability</title>
<p>The behavioural and EEG data, and the scripts used for analysis and figure creation, are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/8CDRA">https://doi.org/10.17605/OSF.IO/8CDRA</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank R. West for data collection, and D. Lloyd for technical assistance. This work was supported by Australian Research Council (ARC) Discovery Early Career Researcher Awards awarded to RR (DE210100790) and AKR (DE200101159). RR was also supported by a National Health and Medical Research Council (Australia) Investigator Grant (2026318).</p>
</ack>
<sec id="d1e1246" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1217">
<label>Supplemental Figures 1, 2 &amp; 3</label>
<media xlink:href="supplements/580589_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Burr</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2004</year>). <article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title>. <source>Current Biology</source>, <volume>14</volume>(<issue>3</issue>), <fpage>257</fpage>–<lpage>262</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arieh</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Marks</surname>, <given-names>L. E.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Cross-modal interaction between vision and hearing: a speed-accuracy analysis. <italic>Attention, Perception</italic></article-title>, <source>&amp; Psychophysics</source>, <volume>70</volume>(<issue>3</issue>), <fpage>412</fpage>–<lpage>421</lpage>. <pub-id pub-id-type="doi">10.3758/pp.70.3.412</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ban</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Preston</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Meeson</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Welchman</surname>, <given-names>A. E.</given-names></string-name></person-group> (<year>2012</year>). <article-title>The integration of motion and disparity cues to depth in dorsal visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>15</volume>(<issue>4</issue>), <fpage>636</fpage>–<lpage>643</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3046</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beauchamp</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Statistical criteria in fMRI studies of multisensory integration</article-title>. <source>Neuroinformatics</source>, <volume>3</volume>(<issue>2</issue>), <fpage>93</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1385/NI:3:2:093</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bednar</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Where is the cocktail party? Decoding locations of attended and unattended moving sound sources using EEG</article-title>. <source>Neuroimage</source>, <volume>205</volume>, <fpage>116283</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116283</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Besle</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Giard</surname>, <given-names>M. H.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Electrophysiological (EEG, sEEG, MEG) evidence for multiple audiovisual interactions in the human auditory cortex</article-title><source>. Hearing Research</source>, <volume>258</volume>(<issue>1</issue>), <fpage>143</fpage>–<lpage>151</lpage>. <pub-id pub-id-type="doi">10.1016/j.heares.2009.06.016</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Besle</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fort</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Giard</surname>, <given-names>M. H.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Interest and validity of the additive model in electrophysiological studies of multisensory interactions</article-title>. <source>Cognitive Processing</source>, <volume>5</volume>(<issue>3</issue>), <fpage>189</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1007/s10339-004-0026-y</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bolognini</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Frassinetti</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Serino</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Ladavas</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2005</year>). <article-title>“Acoustical vision” of below threshold stimuli: interaction among spatially converging audiovisual inputs</article-title>. <source>Experimental Brain Research</source>, <volume>160</volume>(<issue>3</issue>), <fpage>273</fpage>–<lpage>282</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-004-2005-z</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>–<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brouwer</surname>, <given-names>G. J.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Decoding and Reconstructing Color from Responses in Human Visual Cortex</article-title>. <source>The Journal of Neuroscience</source>, <volume>29</volume>(<issue>44</issue>), <fpage>13992</fpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3577-09.2009</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brouwer</surname>, <given-names>G. J.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Cross-orientation suppression in human visual cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>106</volume>(<issue>5</issue>), <fpage>2108</fpage>–<lpage>2119</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00540.2011</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calvert</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Campbell</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Brammer</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex</article-title>. <source>Current Biology</source>, <volume>10</volume>(<issue>11</issue>), <fpage>649</fpage>–<lpage>657</lpage>. <pub-id pub-id-type="doi">10.1016/S0960-9822(00)00513-3</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calvert</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Hansen</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Iversen</surname>, <given-names>S. D.</given-names></string-name>, &amp; <string-name><surname>Brammer</surname>, <given-names>M. J.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Detection of Audio-Visual Integration Sites in Humans by Application of Electrophysiological Criteria to the BOLD Effect</article-title>. <source>Neuroimage</source>, <volume>14</volume>(<issue>2</issue>), <fpage>427</fpage>–<lpage>438</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.2001.0812</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calvert</surname>, <given-names>G. A.</given-names></string-name>, &amp; <string-name><surname>Thesen</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Multisensory integration: methodological approaches and emerging principles in the human brain</article-title>. <source>Journal of Physiology-Paris</source>, <volume>98</volume>(<issue>1-3</issue>), <fpage>191</fpage>–<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1016/j.jphysparis.2004.03.018</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cappe</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Romei</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Selective integration of auditory-visual looming cues by humans</article-title>. <source>Neuropsychologia</source>, <volume>47</volume>(<issue>4</issue>), <fpage>1045</fpage>–<lpage>1052</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2008.11.003</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cappe</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Romei</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Auditory–Visual Multisensory Interactions in Humans: Timing, Topography, Directionality, and Sources</article-title>. <source>The Journal of Neuroscience</source>, <volume>30</volume>(<issue>38</issue>), <fpage>12572</fpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1099-10.2010</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Colonius</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Diederich</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Multisensory Interaction in Saccadic Reaction Time: A Time-Window-of-Integration Model</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>16</volume>(<issue>6</issue>), <fpage>1000</fpage>–<lpage>1009</lpage>. <pub-id pub-id-type="doi">10.1162/0898929041502733</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corniel</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>van Wanrooij</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Munoz</surname>, <given-names>D. P.</given-names></string-name>, &amp; <string-name><surname>van Opstal</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Auditory-Visual Interactions Subserving Goal-Directed Saccades in a Complex Scene</article-title>. <source>Journal of Neurophysiology</source>, <volume>88</volume>(<issue>1</issue>), <fpage>438</fpage>–<lpage>454</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00699.2001</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Meo</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Clarke</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Matusz</surname>, <given-names>P. J.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Top-down control and early multisensory processes: chicken vs. egg</article-title>. <source>Frontiers in integrative neuroscience</source>, <volume>9</volume>(<fpage>17</fpage>). <pub-id pub-id-type="doi">10.3389/fnint.2015.00017</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delong</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Semantic and spatial congruency mould audiovisual integration depending on perceptual awareness</article-title>. <source>Scientific Reports</source>, <volume>11</volume>(<issue>1</issue>), <fpage>10832</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-90183-w</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name>, &amp; <string-name><surname>Banks</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>, <volume>415</volume>(<issue>6870</issue>), <fpage>429</fpage>–<lpage>433</lpage>. <pub-id pub-id-type="doi">10.1038/415429a</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fetsch</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>DeAngelis</surname>, <given-names>G. C.</given-names></string-name>, &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Bridging the gap between theories of sensory cue integration and the physiology of multisensory neurons</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>14</volume>(<issue>6</issue>), <fpage>429</fpage>–<lpage>442</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3503</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Delpuech</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pernier</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Giard</surname>, <given-names>M.-H.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Dynamics of Cortico-subcortical Cross-modal Operations Involved in Audio-visual Object Detection in Humans</article-title>. <source>Cerebral Cortex</source>, <volume>12</volume>(<issue>10</issue>), <fpage>1031</fpage>–<lpage>1039</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/12.10.1031</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frassinetti</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bolognini</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Ladavas</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Enhancement of visual perception by crossmodal visuo-auditory interaction</article-title>. <source>Experimental Brain Research</source>, <volume>147</volume>(<issue>3</issue>), <fpage>332</fpage>–<lpage>343</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-002-1262-y</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frens</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Van Opstal</surname>, <given-names>A. J.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Visual-auditory interactions modulate saccade-related activity in monkey superior colliculus</article-title>. <source>Brain Research Bulletin</source>, <volume>46</volume>(<issue>3</issue>), <fpage>211</fpage>–<lpage>224</lpage>. <pub-id pub-id-type="doi">10.1016/S0361-9230(98)00007-0</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giard</surname>, <given-names>M. H.</given-names></string-name>, &amp; <string-name><surname>Peronnet</surname>, <given-names>F.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Auditory-Visual Integration during Multimodal Object Recognition in Humans: A Behavioral and Electrophysiological Study</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>11</volume>(<issue>5</issue>), <fpage>473</fpage>–<lpage>490</lpage>. <pub-id pub-id-type="doi">10.1162/089892999563544</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harrison</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Bays</surname>, <given-names>P. M.</given-names></string-name>, &amp; <string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Neural tuning instantiates prior expectations in the human visual system</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>), <fpage>5320</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-023-41027-w</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>James</surname>, <given-names>T. W.</given-names></string-name>, <string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Kim</surname>, <given-names>S.</given-names></string-name> (). . In <string-name><given-names>B. E.</given-names> <surname>Stein</surname></string-name></person-group><year>2012</year>). <article-title>Inverse Effectiveness and BOLD fMRI</article-title>. In  (Ed.), <source>The New Handbook of Multisensory Processing</source> (pp. <fpage>207</fpage>). <pub-id pub-id-type="doi">10.7551/mitpress/8466.003.0020</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joassin</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Maurage</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Campanella</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2011</year>). <article-title>The neural network sustaining the crossmodal processing of human gender from faces and voices: An fMRI study</article-title>. <source>Neuroimage</source>, <volume>54</volume>(<issue>2</issue>), <fpage>1654</fpage>–<lpage>1661</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.08.073</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keil</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Senkowski</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Neural Oscillations Orchestrate Multisensory Processing</article-title>. <source>The Neuroscientist</source>, <volume>24</volume>(<issue>6</issue>), <fpage>609</fpage>–<lpage>626</lpage>. <pub-id pub-id-type="doi">10.1177/1073858418755352</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Prior expectations induce prestimulus sensory templates</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>39</issue>), <fpage>10473</fpage>–<lpage>10478</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1705652114</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laurienti</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Perrault</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Stanford</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Wallace</surname>, <given-names>M. T.</given-names></string-name>, &amp; <string-name><surname>Stein</surname>, <given-names>B. E.</given-names></string-name></person-group> (<year>2005</year>). <article-title>On the use of superadditivity as a metric for characterizing multisensory integration in functional neuroimaging studies</article-title>. <source>Experimental Brain Research</source>, <volume>166</volume>(<issue>3</issue>), <fpage>289</fpage>–<lpage>297</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-005-2370-2</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leone</surname>, <given-names>L. M.</given-names></string-name>, &amp; <string-name><surname>McCourt</surname>, <given-names>M. E.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Dissociation of perception and action in audiovisual multisensory integration</article-title>. <source>European Journal of Neuroscience</source>, <volume>42</volume>(<issue>11</issue>), <fpage>2915</fpage>–<lpage>2922</lpage>. <pub-id pub-id-type="doi">10.1111/ejn.13087</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lovelace</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Stein</surname>, <given-names>B. E.</given-names></string-name>, &amp; <string-name><surname>Wallace</surname>, <given-names>M. T.</given-names></string-name></person-group> (<year>2003</year>). <article-title>An irrelevant light enhances auditory detection in humans: a psychophysical analysis of multisensory integration in stimulus detection</article-title>. <source>Cognitive Brain Research</source>, <volume>17</volume>(<issue>2</issue>), <fpage>447</fpage>–<lpage>453</lpage>. <pub-id pub-id-type="doi">10.1016/s0926-6410(03)00160-5</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meredith</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Stein</surname>, <given-names>B. E.</given-names></string-name></person-group> (<year>1983</year>). <article-title>Interactions Among Converging Sensory Inputs in the Superior Colliculus</article-title>. <source>Science</source>, <volume>221</volume>(<issue>4608</issue>), <fpage>389</fpage>–<lpage>391</lpage>. <pub-id pub-id-type="doi">10.1126/science.6867718</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Molholm</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ritter</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Javitt</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name>, &amp; <string-name><surname>Foxe</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Multisensory auditory–visual interactions during early sensory processing in humans: a high-density electrical mapping study</article-title>. <source>Cognitive Brain Research</source>, <volume>14</volume>(<issue>1</issue>), <fpage>115</fpage>–<lpage>128</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(02)00066-6</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Thelen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Romei</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Martuzzi</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Matusz</surname>, <given-names>P. J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The multisensory function of the human primary visual cortex</article-title>. <source>Neuropsychologia</source>, <volume>83</volume>, <fpage>161</fpage>–<lpage>169</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.08.011</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nikbakht</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Tafreshiha</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Diamond</surname>, <given-names>M. E.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Supralinear and Supramodal Integration of Visual and Tactile Signals in Rats: Psychophysics and Neuronal Mechanisms</article-title>. <source>Neuron</source>, <volume>97</volume>(<issue>3</issue>), <fpage>626</fpage>–<lpage>639</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.003</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ohshiro</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Angelaki</surname>, <given-names>D. E.</given-names></string-name>, &amp; <string-name><surname>DeAngelis</surname>, <given-names>G. C.</given-names></string-name></person-group> (<year>2017</year>). <article-title>A Neural Signature of Divisive Normalization at the Level of Multisensory Integration in Primate Cortex</article-title>. <source>Neuron</source>, <volume>95</volume>(<issue>2</issue>), <fpage>399</fpage>–<lpage>411</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.043</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pernet</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Latinus</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, &amp; <string-name><surname>Rousselet</surname>, <given-names>G. A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Cluster-based computational methods for mass univariate analyses of event-related brain potentials/fields: A simulation study</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>250</volume>, <fpage>85</fpage>–<lpage>93</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.08.003</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Porada</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Regenbogen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Freiherr</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Seubert</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Lundström</surname>, <given-names>J. N.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Trimodal processing of complex stimuli in inferior parietal cortex is modality-independent</article-title>. <source>Cortex</source>, <volume>139</volume>, <fpage>198</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2021.03.008</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Puce</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Epling</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>J. C.</given-names></string-name>, &amp; <string-name><surname>Carrick</surname>, <given-names>O. K.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Neural responses elicited to face motion and vocalization pairings</article-title>. <source>Neuropsychologia</source>, <volume>45</volume>(<issue>1</issue>), <fpage>93</fpage>–<lpage>106</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.04.017</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rach</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Diederich</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Visual-tactile integration: does stimulus duration influence the relative amount of response enhancement?</article-title> <source>Experimental Brain Research</source>, <volume>173</volume>(<issue>3</issue>), <fpage>514</fpage>–<lpage>520</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-006-0452-4</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rach</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Diederich</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Colonius</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2010</year>). <article-title>On quantifying multisensory interaction effects in reaction time and detection rate</article-title>. <source>Psychological Research</source>, <volume>75</volume>(<issue>2</issue>), <fpage>77</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1007/s00426-010-0289-0</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Task-related modulation of event-related potentials does not reflect changes to sensory representations</article-title>. <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2024.01.20.576485</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Storrs</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Maiello</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Welchman</surname>, <given-names>A. E.</given-names></string-name></person-group> (<year>2021</year>). <article-title>How multisensory neurons solve causal inference</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>32</issue>), <fpage>e2106235118</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2106235118</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Welchman</surname>, <given-names>A. E.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Proscription supports robust perceptual integration by suppression in human visual cortex</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>), <fpage>1502</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-03400-y</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>West</surname>, <given-names>R. K.</given-names></string-name>, <string-name><surname>Rangelov</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Mattingley</surname>, <given-names>J. B.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Distinct early and late neural mechanisms regulate feature-specific sensory adaptation in the human visual system</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>120</volume>(<issue>6</issue>), <fpage>e2216192120</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2216192120</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robinson</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Shatek</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Gerboni</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Holcombe</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Overlapping neural representations for the position of visible and imagined objects</article-title>. <source>Neurons, behavior, data analysis, and theory</source>, <volume>4</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.51628/001c.19129</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rohe</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Reliability-Weighted Integration of Audiovisual Signals Can Be Modulated by Top-down Attention</article-title>. <source>eneuro</source>, <volume>5</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1523/ENEURO.0315-17.2018</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ross</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Molholm</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Butler</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Bene</surname>, <given-names>V. A. D.</given-names></string-name>, &amp; <string-name><surname>Foxe</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Neural correlates of multisensory enhancement in audiovisual narrative speech perception: A fMRI investigation</article-title>. <source>Neuroimage</source>, <volume>263</volume>, <fpage>119598</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119598</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scheliga</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kellermann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lampert</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rolke</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Spehr</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Habel</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Neural correlates of multisensory integration in the human brain: an ALE meta-analysis</article-title>. <source>Reviews in the Neurosciences</source>, <volume>34</volume>(<issue>2</issue>), <fpage>223</fpage>–<lpage>245</lpage>. <pub-id pub-id-type="doi">10.1515/revneuro-2022-0065</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Senkowski</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Saint-Amour</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hofle</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Foxe</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Multisensory interactions in early evoked brain activity follow the principle of inverse effectiveness</article-title>. <source>Neuroimage</source>, <volume>56</volume>(<issue>4</issue>), <fpage>2200</fpage>–<lpage>2208</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.075</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stanislaw</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Todorov</surname>, <given-names>N.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Calculation of signal detection theory measures</article-title>. <source>Behavior Research Methods, Instruments, &amp; Computers</source>, <volume>31</volume>(<issue>1</issue>), <fpage>137</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.3758/BF03207704</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stein</surname>, <given-names>B. E.</given-names></string-name>, &amp; <string-name><surname>Stanford</surname>, <given-names>T. R.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Multisensory integration: current issues from the perspective of the single neuron</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>9</volume>(<issue>4</issue>), <fpage>255</fpage>–<lpage>266</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2331</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stekelenburg</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Neural Correlates of Multisensory Integration of Ecologically Valid Audiovisual Events</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>19</volume>(<issue>12</issue>), <fpage>1964</fpage>–<lpage>1973</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.12.1964</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Geoghegan</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>James</surname>, <given-names>T. W.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Superadditive BOLD activation in superior temporal sulcus with threshold non-speech objects</article-title>. <source>Experimental Brain Research</source>, <volume>179</volume>(<issue>1</issue>), <fpage>85</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-006-0770-6</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Ghose</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Fister</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Sarko</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Altieri</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Nidiffer</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Kurela</surname>, <given-names>L. R.</given-names></string-name>, <string-name><surname>Siemann</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>James</surname>, <given-names>T. W.</given-names></string-name>, &amp; <string-name><surname>Wallace</surname>, <given-names>M. T.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Identifying and quantifying multisensory integration: a tutorial review</article-title>. <source>Brain Topography</source>, <volume>27</volume>(<issue>6</issue>), <fpage>707</fpage>–<lpage>730</lpage>. <pub-id pub-id-type="doi">10.1007/s10548-014-0365-7</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>James</surname>, <given-names>T. W.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Audiovisual integration in human superior temporal sulcus: Inverse effectiveness and the neural processing of speech and object recognition</article-title>. <source>Neuroimage</source>, <volume>44</volume>(<issue>3</issue>), <fpage>1210</fpage>–<lpage>1223</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.034</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sumby</surname>, <given-names>W. H.</given-names></string-name>, &amp; <string-name><surname>Pollack</surname>, <given-names>I.</given-names></string-name></person-group> (<year>1954</year>). <article-title>Visual Contribution to Speech Intelligibility in Noise</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>26</volume>(<issue>2</issue>), <fpage>212</fpage>–<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1121/1.1907309</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Talsma</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Doty</surname>, <given-names>T. J.</given-names></string-name>, &amp; <string-name><surname>Woldorff</surname>, <given-names>M. G.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Selective Attention and Audiovisual Integration: Is Attending to Both Modalities a Prerequisite for Early Integration?</article-title> <source>Cerebral Cortex</source>, <volume>17</volume>(<issue>3</issue>), <fpage>679</fpage>–<lpage>690</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhk016</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teder-Sälejärvi</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>McDonald</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Di Russo</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name></person-group> (<year>2002</year>). <article-title>An analysis of audio-visual crossmodal integration by means of event-related potential (ERP) recordings</article-title>. <source>Cognitive Brain Research</source>, <volume>14</volume>(<issue>1</issue>), <fpage>106</fpage>–<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(02)00065-4</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>The MathWorks Inc</collab></person-group>. (<year>2021</year>). <article-title>MATLAB version: 9.11.0 (R2021b), Natick, Massachusetts</article-title>. <source>The MathWorks Inc</source>. <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Wanrooij</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Bell</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Munoz</surname>, <given-names>D. P.</given-names></string-name>, &amp; <string-name><surname>Van Opstal</surname>, <given-names>A. J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>The effect of spatial-temporal audiovisual disparities on saccades in a complex scene</article-title>. <source>Experimental Brain Research</source>, <volume>198</volume>(<issue>2-3</issue>), <fpage>425</fpage>–<lpage>437</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-009-1815-4</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Venezia</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Matchin</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name> (). . In <string-name><given-names>A. W.</given-names> <surname>Toga</surname></string-name></person-group><year>2015</year>). <chapter-title>Multisensory Integration and Audiovisual Speech Perception</chapter-title>. In  (Ed.), <source>Brain Mapping</source> (pp. <fpage>565</fpage>–<lpage>572</lpage>). <publisher-name>Academic Press</publisher-name>. <pub-id pub-id-type="doi">10.1016/B978-0-12-397025-1.00047-6</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Stekelenburg</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Visual Anticipatory Information Modulates Multisensory Interactions of Artificial Audiovisual Stimuli</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>22</volume>(<issue>7</issue>), <fpage>1583</fpage>–<lpage>1596</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2009.21308</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Celebrini</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Trotter</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Barone</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Visuo-auditory interactions in the primary visual cortex of the behaving monkey: Electrophysiological evidence</article-title>. <source>BMC neuroscience</source>, <volume>9</volume>(<issue>1</issue>), <fpage>79</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2202-9-79</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Werner</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Superadditive Responses in Superior Temporal Sulcus Predict Audiovisual Benefits in Object Categorization</article-title>. <source>Cerebral Cortex</source>, <volume>20</volume>(<issue>8</issue>), <fpage>1829</fpage>–<lpage>1842</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhp248</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Werner</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2011</year>). <article-title>The Contributions of Transient and Sustained Response Codes to Audiovisual Integration</article-title>. <source>Cerebral Cortex</source>, <volume>21</volume>(<issue>4</issue>), <fpage>920</fpage>–<lpage>931</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhq161</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whitworth</surname>, <given-names>R. H.</given-names></string-name>, &amp; <string-name><surname>Jeffress</surname>, <given-names>L. A.</given-names></string-name></person-group> (<year>1961</year>). <article-title>Time vs Intensity in the Localization of Tones</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>33</volume>(<issue>7</issue>), <fpage>925</fpage>–<lpage>929</lpage>. <pub-id pub-id-type="doi">10.1121/1.1908849</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wightman</surname>, <given-names>F. L.</given-names></string-name>, &amp; <string-name><surname>Kistler</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>1992</year>). <article-title>The dominant role of low-frequency interaural time differences in sound localization</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>91</volume>(<issue>3</issue>), <fpage>1648</fpage>–<lpage>1661</lpage>. <pub-id pub-id-type="doi">10.1121/1.402445</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilcox</surname>, <given-names>R.R.</given-names></string-name></person-group> (<year>2016</year>), <article-title>Comparing dependent robust correlations</article-title>. <source>British Journal of Mathematical &amp; Statistical Psychology</source>, <volume>69</volume>(<issue>3</issue>), <fpage>215</fpage>–<lpage>224</lpage>. <pub-id pub-id-type="doi">10.1111/bmsp.12069</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97230.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>Despite the well-established facilitatory effects of multisensory integration on behavioural measures, standard neuroimaging approaches have yet to reliably and precisely identify the corresponding neural correlates. In this <bold>valuable</bold> paper, Buhmann et al. leverage EEG decoding methods, moving beyond traditional univariate analyses, to capture these correlates. They present <bold>solid</bold> evidence that this approach can effectively estimate multisensory integration in humans across a broad range of contexts.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97230.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study presents a novel application of inverted encoding (i.e., decoding) to detect non-linear correlates of crossmodal integration in human neural activity, using EEG (electroencephalography). The method is successfully applied to data from a group of 41 participants, performing a spatial localization task on auditory, visual, and audio-visual events. The analyses clearly show a behavioural superiority for audio-visual localization. Like previous studies, the results when using traditional univariate ERP analyses were inconclusive, showing once more the need for alternative, more sophisticated approaches. The inverted encoding approach of this study, harnessing on the multivariate nature of the signal, captured clear signs of super-additive responses, considered by many as the hallmark of multisensory integration. Despite the removal of eye-movement artefacts from the signal eliminated the significant decoding effect, the author's control analyses showed that decoding is more effective from parietal, compared to frontal electrodes, thereby ruling out ocular contamination as the sole origin of the relevant signal.</p>
<p>This significant finding can bear important advances in the many fields where multisensory integration has been shown to play an important role, by providing a way to bring much needed coherence across levels of analysis, from behaviour to single-cell electrophysiology. To achieve this, it would be ideal to contrast whether the pattern of super-additive effects in other scenarios where clear behavioural signs of multisensory integration are also observed. One could also try to further support the posterior origin of the super-additive effects by source localization.</p>
<p>Comments on revised version:</p>
<p>All my previous concerns have been addressed. I congratulate the authors on a very nice paper.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97230.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript seeks to reconcile observations in multisensory perception - from behavior and from neural responses. It is intuitively obvious that perceiving a stimulus via two senses results in better performance than one alone. However, the nature of this interaction is complicated and relating different measures (behavioural, neural) is challenging.</p>
<p>It is not uncommon to observe that for a perceptual task the percentage of correct responses seen with two senses is higher than the sum of the percentage correct obtained with each modality individually. i.e. the gains are &quot;superadditive&quot;. The gains of adding a second sense are typically larger when the performance with the first sense is relatively poor - this effect is often called the principle inverse effectiveness. More generally, what this tells us is that performance in a multisensory perceptual task is a non-linear sum of performance for each sensory modality alone. In invasive recordings from single neurons, a wide range of non-linear interactions is observed - some superadditive, and some sub-additive.</p>
<p>Despite this abundance evidence of non-linearity in some measures of multisensory integration, evoked responses (EEG) to such sensory stimuli often show little evidence of it - and this is the problem this manuscript tackles. The key assertion made is that a univariate analysis of the EEG signal is likely to average out non-linear effects of integration. This is a reasonable assertion, and their analysis does indeed provide evidence that a multivariate approach can reveal non-linear interactions in the evoked responses.</p>
<p>Strengths:</p>
<p>It is of great value to understand how the process of multisensory integration occurs, and despite a wealth of observations of the benefits of perceiving the world with multiple senses, we still lack a reasonable understanding of how the brain integrates information. For example - what underlies the large individual differences in the benefits of two senses over one? One way to tackle this is via brain imaging, but this is problematic if important features of the processing - such as non-linear interactions are obscured by the lack of specificity of the measurements. The approach they take to analysis of the EEG data allows the authors to look in more detail at the variation in activity across EEG electrodes, which averaging across electrodes cannot.</p>
<p>This version of the manuscript is well written and for the most part clear and the report of non-linear summation of neural responses is convincing. A particular strength of the paper is their use of a statistical model of multisensory integration as their &quot;null&quot; model of neural responses, and the &quot;inverted-encoder&quot; which infers an internal representation of the stimulus which can explain the EEG responses. This encoder generates a prediction of decoding performance, which can be used to generate predictions of multisensory decoding from unisensory decoding, or from a sum of the unisensory internal representations.</p>
<p>In behavioural performance, it is frequently observed that the performance increase from two senses is close to what is expected from the optimal integration of information across the senses, in a statistical sense. It can be plausibly explained by assuming that people are able to weight sensory inputs according to their reliability - and somewhat optimally. Critically the apparent &quot;superadditive&quot; effect on performance described above does not require any non-linearity in the sum of information across the senses, but can arise from correctly weighting the information according to reliability.</p>
<p>The authors apply a similar model to predict the neural responses expected to audiovisual stimuli from the neural responses to audio and visual stimuli alone, assuming optimal statistical integration of information. The neural responses to audiovisual stimuli exceed the predictions of this model and this is the main evidence supporting their conclusion, and it is convincing.</p>
<p>Weaknesses:</p>
<p>The main weakness of the manuscript is that their behavioural data show no evidence of performance that exceeds the predictions of these statistical models. In fact, the models predict multisensory performance from unisensory performance pretty well. This makes it hard to interpret their results, as surely if these nonlinear neural interactions underlie the behaviour, then we should be able to see evidence of it in the behaviour. I cannot offer an easy explanation for this.</p>
<p>Overall, therefore, I applaud the motivation and the sophistication of the analysis method and think it shows great promise for tackling these problems.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97230.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Buhmann</surname>
<given-names>Zak</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0002-4249-462X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Robinson</surname>
<given-names>Amanda K</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7378-2803</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mattingley</surname>
<given-names>Jason B</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0929-9216</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Rideaux</surname>
<given-names>Reuben</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8416-005X</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1:</bold></p>
</disp-quote>
<p>We thank Reviewer 1 for their helpful comments and hope that the changes made to the revised manuscript have addressed their points.</p>
<disp-quote content-type="editor-comment">
<p>This study presents a novel application of the inverted encoding (i.e., decoding) approach to detect the correlates of crossmodal integration in the human EEG (electrophysiological) signal. The method is successfully applied to data from a group of 41 participants, performing a spatial localization task on auditory, visual, and audiovisual events. The analyses clearly show a behavioural superiority for audio-visual localization. Like previous studies, the results when using traditional univariate ERP analyses were inconclusive, showing once more the need for alternative, more sophisticated approaches. Instead, the principal approach of this study, harnessing the multivariate nature of the signal, captured clear signs of super-additive responses, considered by many as the hallmark of multisensory integration. Unfortunately, the manuscript lacks many important details in the descriptions of the methodology and analytical pipeline. Although some of these details can eventually be retrieved from the scripts that accompany this paper, the main text should be self-contained and sufficient to gain a clear understanding of what was done. (A list of some of these is included in the comments to the authors). Nevertheless, I believe the main weakness of this work is that the positive results obtained and reported in the results section are conditioned upon eye movements. When artifacts due to eye movements are removed, then the outcomes are no longer significant.</p>
<p>Therefore, whether the authors finally achieved the aims and showed that this method of analysis is truly a reliable way to assess crossmodal integration, does not stand on firm ground. The worst-case scenario is that the results are entirely accounted for by patterns of eye movements in the different conditions. In the best-case scenario, the method might truly work, but further experiments (and/or analyses) would be required to confirm the claims in a conclusive fashion.</p>
<p>One first step toward this goal would be, perhaps, to facilitate the understanding of results in context by reporting both the uncorrected and corrected analyses in the main results section. Second, one could try to support the argument given in the discussion, pointing out the origin of the super-additive effects in posterior electrode sites, by also modelling frontal electrode clusters and showing they aren't informative as to the effect of interest.</p>
</disp-quote>
<p>We performed several additional analyses to address concerns that our main result was caused by different eye movement patterns between conditions. We re-ran our key analyses using activity exclusively from frontal electrodes, which revealed poorer decoding performance than that from posterior electrodes. If eye movements were driving the non-linear enhancement in the audiovisual condition, we would expect stronger decoding using sensors closer to the source, i.e., the extraocular muscles. We also computed the correlations between average eye position and stimulus position for each condition to evaluate whether participants made larger eye movements in the audiovisual condition, which might have contributed to better decoding results. Though we did find evidence for eye movements toward stimuli, the degree of movement did not significantly differ between conditions.</p>
<p>Furthermore, we note that the analysis using a stricter eye movement criterion, acknowledged in the Discussion section of the original manuscript, resulted in very similar results to the original analysis. There was significantly better decoding in the AV condition (as measured by d') than the MLE prediction, but this difference did not survive cluster correction. The most likely explanation for this is that the strict eye movement criterion combined with our conservative measure of (mass-based) cluster correction led to reduced power to detect true differences between conditions. Taken together with the additional analyses described in the revised manuscript and supplementary materials, the results show that eye movements are unlikely to account for differences between the multisensory and unisensory conditions. Instead, our decoding results likely reflect nonlinear neural integration between audio and visual sensory information.</p>
<p>“Any experimental design that varies stimulus location needs to consider the potential contribution of eye movements. We computed correlations between participants’ average eye position and each stimulus position between the three sensory conditions (auditory, visual and audiovisual; Figure S1) and found evidence that participants made eye movements toward stimuli. A re-analysis of the data with a very strict eye-movement criterion (i.e., removing trials with eye movements &gt;1.875º) revealed that the super-additive enhancement in decoding accuracy no longer survived cluster correction, suggesting that our results may be impacted by the consistent motor activity of saccades towards presented stimuli. Further investigation, however, suggests this is unlikely. Though the correlations were significantly different from 0, they were not significantly different from each other. If consistent saccades to audiovisual stimuli were responsible for the nonlinear multisensory benefit we observed, we would expect to find a higher positive correlation between horizontal eye position and stimulus location in the audiovisual condition than in the auditory or visual conditions. Interestingly, eye movements corresponded more to stimulus location in the auditory and audiovisual conditions than in the visual condition, indicating that it was the presence of a sound, rather than a visual stimulus, that drove small eye movements. This could indicate that participants inadvertently moved their eyes when localising the origin of sounds. We also re-ran our analyses using the activity measured from the frontal electrodes alone (Figure S2). If the source of the nonlinear decoding accuracy in the audiovisual condition was due to muscular activity produced by eye movements, there should be better decoding accuracy from sensors closer to the source. Instead, we found that decoding accuracy of stimulus location from the frontal electrodes (peak d' = 0.08) was less than half that of decoding accuracy from the more posterior electrodes (peak d' = 0.18). These results suggest that the source of neural activity containing information about stimulus position was located over occipito-parietal areas, consistent with our topographical analyses (inset of Figure 3).”</p>
<disp-quote content-type="editor-comment">
<p>The univariate ERP analyses an outdated contrast, AV &lt;&gt; A + V to capture multisensory integration. A number of authors have pointed out the potential problem of double baseline subtraction when using this contrast, and have recommended a number of solutions, experimental and analytical. See for example: [1] and [2].</p>
<p>(1) Teder-Salejarvi, W. A., McDonald, J. J., Di Russo, F., &amp; Hillyard, S. A. (2002). Cognitive Brain Research, 14, 106-114.</p>
<p>(2) Talsma, D., &amp; Woldorff, M. G. (2005). Journal of cognitive neuroscience, 17(7), 1098-1114.</p>
</disp-quote>
<p>We thank the reviewer for raising this point. Comparing ERPs across different sensory conditions requires careful analytic choices to discern genuine sensory interactions within the signal. The AV &lt;&gt; (A +V) contrast has often been used to detect multisensory integration, though any non-signal related activity (i.e. anticipatory waves; Taslma &amp; Woldorff, 2005) or pre-processing manipulation (e.g. baseline subtraction; Teder-Sälejärvi et al., 2002) will be doubled in (A + V) but not in AV. Critically, we did not apply a baseline correction during preprocessing and thus our results are not at risk of double-baseline subtraction in (A + V). Additionally, we temporally jittered the presentation of our stimuli to mitigate the potential influence of consistent overlapping ERP waves (Talsma &amp; Woldorff, 2005).</p>
<disp-quote content-type="editor-comment">
<p>The results section should provide the neurometric curve/s used to extract the slopes of the sensitivity plot (Figure 2B).</p>
</disp-quote>
<p>We thank the reviewer for raising this point of clarification. The sensitivity plots for Figures 2B and 2C were extracted from the behavioural performance of the behavioural and EEG tasks, respectively. The sensitivity plot for Figure 2B was extracted from individual psychometric curves, whereas the d’ values for Figure 2C were calculated from the behavioural data for the EEG task. This information has been clarified in the manuscript.</p>
<p>“Figure 1. Behavioural performance is improved for audiovisual stimuli. A) Average accuracy of responses across participants in the behavioural session at each stimulus location for each stimulus condition, fitted to a psychometric curve. Steeper curves indicate greater sensitivity in identifying stimulus location. B) Average sensitivity across participants in the behavioural task, estimated from psychometric curves, for each stimulus condition. The red cross indicates estimated performance assuming optimal (MLE) integration of unisensory cues. C) Average behavioural sensitivity across participants in the EEG session for each stimulus condition. Error bars indicate ±1 SEM.”</p>
<disp-quote content-type="editor-comment">
<p>The encoding model was fitted for each electrode individually; I wonder if important information contained as combinations of (individually non-significant) electrodes was then lost in this process and if the authors consider that this is relevant.</p>
</disp-quote>
<p>Although the encoding model was fitted for each electrode individually for the topographic maps (Figure 4B), in all other analyses the encoding model was fitted across a selection of electrodes (see final inset of Figure 3). As this electrode set was used for all other neural analyses, our model would allow for the detection of important information contained in the neural patterns across electrodes. This information has been clarified in the manuscript.</p>
<p>“Thus, for all subsequent analyses we only included signals from the central-temporal, parietal-occipital, occipital and inion sensors for computing the inverse model (see final inset of Figure 2). As the model was fitted for multiple electrodes, subtle patterns of neural information contained within combinations of sensors could be detected.”</p>
<disp-quote content-type="editor-comment">
<p>Neurobehavioral correlations could benefit from outlier rejection and the use of robust correlation statistics.</p>
</disp-quote>
<p>We thank the reviewer for raising this issue. Note, however, that the correlations we report are resistant to the influence of outliers because we used Spearman’s rho1 (as opposed to Pearson’s). This information has been communicated in the manuscript.</p>
<p>(1) Wilcox, R.R. (2016), Comparing dependent robust correlations. <italic>British Journal of Mathematical &amp; Statistical Psychology, 69</italic>(3), 215-224. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/bmsp.12069">https://doi.org/10.1111/bmsp.12069</ext-link></p>
<p><italic>“Neurobehavioural correlations.</italic> As behavioural and neural data violated assumptions of normality, we calculated rank-order correlations (Spearman’s rho) between the average decoding sensitivity for each participant from 150-250 ms poststimulus onset and behavioural performance on the EEG task. As Spearman’s rho is resistant to outliers (Wilcox, 2016), we did not perform outlier rejection.”</p>
<p>“Wilcox, R.R. (2016), Comparing dependent robust correlations. <italic>British Journal of Mathematical &amp; Statistical Psychology, 69</italic>(3), 215-224. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/bmsp.12069">https://doi.org/10.1111/bmsp.12069</ext-link>”</p>
<disp-quote content-type="editor-comment">
<p>Many details that are important for the reader to evaluate the evidence and to understand the methods and analyses aren't given; this is a non-exhaustive list:</p>
</disp-quote>
<p>We thank the reviewer for highlighting these missing details. We have updated the manuscript where necessary to ensure the methods and analyses are fully detailed and replicable.</p>
<disp-quote content-type="editor-comment">
<p>- specific parameters of the stimuli and performance levels. Just saying &quot;similarly difficult&quot; or &quot;marginally higher volume&quot; is not enough to understand exactly what was done.</p>
</disp-quote>
<p>“The perceived source location of auditory stimuli was manipulated via changes to interaural level and timing (Whitworth &amp; Jeffress, 1961; Wightman &amp; Kistler, 1992). The precise timing of when each speaker delivered an auditory stimulus was calculated from the following formula:</p>
<disp-formula id="sa3equ1">
<graphic mime-subtype="jpg" xlink:href="elife-97230-sa3-equ1.jpg" mimetype="image"/>
</disp-formula>
<p>where <italic>x</italic> and <italic>z</italic> are the horizontal and forward distances in metres between the ears and the source of the sound on the display, respectively, <italic>r</italic> is the head radius, and <italic>s</italic> is the speed of sound. We used a constant approximate head radius of 8 cm for all participants. <italic>r</italic> was added to <italic>x</italic> for the left speaker and subtracted for the right speaker to produce the interaural time difference. For ±15° source locations, interaural timing difference was 1.7 ms. To simulate the decrease in sound intensity as a function of distance, we calculated interaural level differences for the left and right speakers by dividing the sounds by the left and right distance vectors. Finally, we resampled the sound using linear interpolation based on the calculations of the interaural level and timing differences. This process was used to calculate the soundwaves played by the left and right speakers for each of the possible stimulus locations on the display. The maximum interaural level difference between speakers was 0.14 <italic>A</italic> for ±15° auditory locations, and 0.07 <italic>A</italic> for ±7.5°.”</p>
<disp-quote content-type="editor-comment">
<p>- where are stimulus parameters adjusted individually or as a group? Which method was followed?</p>
</disp-quote>
<p>To clarify, stimulus parameters (frequency, size, luminance, volume, location, etc.) were manipulated throughout pilot testing <italic>only</italic>. Parameters were adjusted to achieve similar pilot behavioural results between the auditory and visual conditions. For the experiment proper, parameters remained constant for both tasks and were the same for all participants.</p>
<p>“During pilot testing, stimulus features (size, luminance, volume, frequency etc.) were manipulated to make visual and auditory stimuli similarly difficult to spatially localize. These values were held constant in the main experiment.”</p>
<disp-quote content-type="editor-comment">
<p>- specify which response buttons were used.</p>
</disp-quote>
<p>“Participants were presented with two consecutive stimuli and tasked with indicating, via button press, whether the first (‘1’ number-pad key) or second (‘2’ number-pad key) interval contained the more leftward stimulus.”</p>
<p>“At the end of each sequence, participants were tasked with indicating, via button press, whether more presentations appeared on the right (‘right’ arrow key) or the left (‘left’ arrow key) of the display.”</p>
<disp-quote content-type="editor-comment">
<p>- no information is given as to how many trials per condition remained on average, for analysis.</p>
</disp-quote>
<p>The average number of remaining trials per condition after eye-movement analysis is now included in the Methods section of the revised manuscript.</p>
<p>“We removed trials with substantial eye movements (&gt;3.75 away from fixation) from the analyses. After the removal of eye movements, on average 2365 (<italic>SD</italic> = 56.94), 2346 (<italic>SD</italic> = 152.87) and 2350 (<italic>SD</italic> = 132.47) trials remained for auditory, visual and audiovisual conditions, respectively, from the original 2400 per condition.”</p>
<disp-quote content-type="editor-comment">
<p>- no information is given on the specifics of participant exclusion criteria. (even if the attrition rate was surprisingly high, for such an easy task).</p>
</disp-quote>
<p>The behavioural session also served as a screening task. Although the task instructions were straightforward, perceptual discrimination was not easy due to the ambiguity of the stimuli. Auditory localization is not very precise, and the visual stimuli were brief, dim, and diffuse. The behavioural results reflect the difficulty of the task. Attrition rate was high as participants who scored below 60% correct in any condition were deemed unable to accurately perform the task, were not invited to complete the subsequent EEG session, and omitted from the analyses. We have included the specific criteria in the manuscript.</p>
<p>“Participants were first required to complete a behavioural session with above 60% accuracy in all conditions to qualify for the EEG session (see <italic>Behavioural session</italic> for details).”</p>
<disp-quote content-type="editor-comment">
<p>- EEG pre-processing: what filter was used? How was artifact rejection done? (no parameters are reported); How were bad channels interpolated?</p>
</disp-quote>
<p>We used a 0.25 Hz high-pass filter to remove baseline drifts, but no low-pass filter. In line with recent studies on the undesirable influence of EEG preprocessing on ERPs1, we opted to avoid channel interpolation and artifact rejection. This was erroneously reported in the manuscript and has now been clarified. For the sake of clarity, here we demonstrate that a reanalysis of data using channel interpolation and artifact rejection returned the same pattern of results.</p>
<p>(1) Delorme, A. (2023). EEG is better left alone. <italic>Scientific Reports</italic>, 13, 2372. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-023-27528-0">https://doi.org/10.1038/s41598-023-27528-0</ext-link></p>
<disp-quote content-type="editor-comment">
<p>- specific electrode locations must be given or shown in a plot (just &quot;primarily represented in posterior electrodes&quot; is not sufficiently informative).</p>
</disp-quote>
<p>A diagram of the electrodes used in all analyses is included within Figure 3, and we have drawn readers’ attention to this in the revised manuscript.</p>
<p>“Thus, for all subsequent analyses we only included signals from the central-temporal, parietal-occipital, occipital and inion sensors for computing the inverse model (see final inset of Figure 2).”</p>
<disp-quote content-type="editor-comment">
<p>- ERP analysis: which channels were used? What is the specific cluster correction method?</p>
</disp-quote>
<p>We used a conservative mass-based cluster correction from Pernet et al. (2015) - this information has been clarified in the manuscript.</p>
<p>“A conservative mass-based cluster correction was applied to account for spurious differences across time (Pernet et al., 2015).”</p>
<p>“Pernet, C. R., Latinus, M., Nichols, T. E., &amp; Rousselet, G. A. (2015). Cluster-based computational methods for mass univariate analyses of event-related brain potentials/fields: A simulation study. <italic>Journal of Neuroscience Methods</italic>, <italic>250</italic>, 85-93. <ext-link ext-link-type="uri" xlink:href="https://doi.org/https://doi.org/10.1016/j.jneumeth.2014.08.003">https://doi.org/https://doi.org/10.1016/j.jneumeth.2014.08.003</ext-link>”</p>
<disp-quote content-type="editor-comment">
<p>- results: descriptive stats on performance must be given (instead of saying &quot;participants performed well&quot;).</p>
</disp-quote>
<p>The mean and standard deviation of participants’ performance for each condition in the behavioural and EEG experiments are now explicitly mentioned in the manuscript.</p>
<p>“A quantification of the behavioural sensitivity (i.e., steepness of the curves) revealed significantly higher sensitivity for the audiovisual stimuli (<italic>M</italic> = .04, <italic>SD</italic> = .02) than for the auditory stimuli alone (<italic>M</italic> = .03, <italic>SD</italic> = .01; Z = -3.09, <italic>p</italic> = .002), and than for the visual stimuli alone (<italic>M</italic> = .02, <italic>SD</italic> = .01; Z = -5.28, <italic>p</italic> = 1.288e-7; Figure 1B). Sensitivity for auditory stimuli was also significantly higher than sensitivity for visual stimuli (<italic>Z</italic> = 2.02, <italic>p</italic> = .044).”</p>
<p>“We found a similar pattern of results to those in the behavioural session; sensitivity for audiovisual stimuli (<italic>M</italic> = .85, <italic>SD</italic> = .33) was significantly higher than for auditory (<italic>M</italic> = .69, <italic>SD</italic> = .41; <italic>Z</italic> = -2.27, <italic>p</italic> = .023) and visual stimuli alone (<italic>M</italic> = .61, <italic>SD</italic> = .29; <italic>Z</italic> = -3.52, <italic>p</italic> = 4.345e-4), but not significantly different from the MLE prediction (<italic>Z</italic> = -1.07, <italic>p</italic> = .285).”</p>
<disp-quote content-type="editor-comment">
<p>- sensitivity in the behavioural and EEG sessions is said to be different, but no comparison is given. It is not even the same stimulus set across the two tasks...</p>
</disp-quote>
<p>This relationship was noted as a potential explanation for the higher sensitivities obtained in the EEG task, and was not intended to stand up to statistical scrutiny. We agree it makes little sense to compare statistically between the EEG and behavioural results as they were obtained from different tasks. We would like to clarify, however, that the stimuli used in the two tasks were the same, with the exception that in the EEG task the stimuli were presented from 5 locations versus 8 in the behavioural task. To avoid potential confusion, we have removed the offending sentence from the manuscript:</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2:</bold></p>
<p>Their measure of neural responses is derived from the decoder responses, and this takes account of the reliability of the sensory representations - the d' statistics - which is an excellent thing. It also means if I understand their analysis correctly (it could bear clarifying - see below), that they can generate from it a prediction of the performance expected if an optimal decision is made combining the neural signals from the individual modalities. I believe this is the familiar root sum of squares d' calculation (or very similar). Their decoding of the audiovisual responses comfortably exceeds this prediction and forms part of the evidence for their claims.</p>
<p>Yet, superadditivity - including that in evidence in the principle of inverse effectiveness more typically quantifies the excess over the sum of proportions correct in each modality. Their MLE d' statistic can already predict this form of superadditivity. Therefore, the superadditivity they report here is not the same form of superadditivity that is usually referred to in behavioural studies. It is in fact a stiffer definition. What their analysis tests is that decoding performance exceeds what would be expected from an optimally weighted linear integration of the unisensory information. As this is not the common definition it is difficult to relate to behavioral superadditivity reported in much literature (of percentage correct). This distinction is not at all clear from the manuscript.</p>
<p>But the real puzzle is here: The behavioural data or this task do not exceed the optimal statistical decision predicted by signal detection theory (the MLE d'). Yet, the EEG data would suggest that the neural processing is exceeding it. So why, if the neural processing is there to yield better performance is it not reflected in the behaviour? I cannot explain this, but it strikes me that the behaviour and neural signals are for some reason not reflecting the same processing.</p>
<p>Be explicit and discuss this mismatch they observe between behaviour and neural responses.</p>
</disp-quote>
<p>Thank you, we agree that it is worth expanding on the observed disconnect between MSI in behaviour and neural signals. We have included an additional paragraph in the Discussion of the revised manuscript. Despite the mismatch, we believe the behavioural and neural responses still reflect the same underlying processing, but at different levels of sensitivity. The behavioural result likely reflects a coarse down-sampling of the precision in location representation, and thus less likely to reflect subtle MSI enhancements.</p>
<p>“An interesting aspect of our results is the apparent mismatch between the behavioural and neural responses. While the behavioural results meet the optimal statistical threshold predicted by MLE, the decoding analyses suggest that the neural response exceeds it. Though non-linear neural responses and statistically optimal behavioural responses are reliable phenomena in multisensory integration (Alais &amp; Burr, 2004; Ernst &amp; Banks, 2002; Stanford &amp; Stein, 2007), the question remains – if neural super-additivity exists to improve behavioural performance, why is it not reflected in behavioural responses? A possible explanation for this neurobehavioural discrepancy is the large difference in timing between sensory processing and behavioural responses. A motor response would typically occur some time after the neural response to a sensory stimulus (e.g., 70-200 ms), with subsequent neural processes between perception and action that introduce noise (Heekeren et al., 2008) and may obscure super-additive perceptual sensitivity. In the current experiment, participants reported either the distribution of 20 serially presented stimuli (EEG session) or compared the positions of two stimuli (behavioural session), whereas the decoder attempts to recover the location of every presented stimulus. While stimulus location could be represented with higher fidelity in multisensory relative to unisensory conditions, this would not necessarily result in better performance on a binary behavioural task in which multiple temporally separated stimuli are compared. One must also consider the inherent differences in how super-additivity is measured at the neural and behavioural levels. Neural super-additivity should manifest in responses to each individual stimulus. In contrast, behavioural super-additivity is often reported as proportion correct, which can only emerge between conditions after being averaged across multiple trials. The former is a biological phenomenon, while the latter is an analytical construct. In our experiment, we recorded neural responses for every presentation of a stimulus, but behavioural responses were only obtained after multiple stimulus presentations. Thus, the failure to find super-additivity in behavioural responses might be due to their operationalisation, with between-condition comparisons lacking sufficient sensitivity to detect super-additive sensory improvements. Future work should focus on experimental designs that can reveal super-additive responses in behaviour.”</p>
<disp-quote content-type="editor-comment">
<p>Re-work the introduction to explain more clearly the relationship between the behavioural superadditivities they review, the MLE model, and the superadditivity it actually tests.</p>
</disp-quote>
<p>We agree it is worth discussing how super-additivity is operationalised across neural and behavioural measures. However, we do not believe the behavioural studies we reviewed claimed super-additive behavioural enhancements. While MLE is often used as a behavioural marker of successful integration, it is not necessarily used as evidence for super-additivity within the behavioural response, as it relies on linear operations.</p>
<p>“It is important to consider the differences in how super-additivity is classified between neural and behavioural measures. At the level of single neurons, superadditivity is defined as a non-linear response enhancement, with the multisensory response exceeding the sum of the unisensory responses. In behaviour, meanwhile, it has been observed that the performance improvement from combining two senses is close to what is expected from optimal integration of information across the senses (Alais &amp; Burr, 2004; Stanford &amp; Stein, 2007). Critically, behavioural enhancement of this kind does not require non-linearity in the neural response, but can arise from a reliability-weighted average of sensory information. In short, behavioural performance that conforms to MLE is not necessarily indicative of neural super-additivity, and the MLE model can be considered a linear baseline for multisensory integration.”</p>
<disp-quote content-type="editor-comment">
<p>Regarding the auditory stimulus, this reviewer notes that interaural time differences are unlikely to survive free field presentation.</p>
</disp-quote>
<p>Despite the free field presentation, in both the pilot test and the study proper participants were able to localize auditory stimuli significantly above chance.</p>
<disp-quote content-type="editor-comment">
<p>&quot;However, other studies have found super-additive enhancements to the amplitude of sensory event-related potentials (ERPs) for audiovisual stimuli (Molholm et al., 2002; Talsma et al., 2007), especially when considering the influence of stimulus intensity (Senkowski et al., 2011).&quot; - this makes it obvious that there are some studies which show superadditivity. It would have been good to provide a little more depth here - as to what distinguished those studies that reported positive effects from those that did not.</p>
</disp-quote>
<p>We have provided further detail on how super-additivity appears to manifest in neural measures.</p>
<p>“In EEG, meanwhile, the evoked response to an audiovisual stimulus typically conforms to a sub-additive principle (Cappe et al., 2010; Fort et al., 2002; Giard &amp; Peronnet, 1999; Murray et al., 2016; Puce et al., 2007; Stekelenburg &amp; Vroomen, 2007; Teder- Sälejärvi et al., 2002; Vroomen &amp; Stekelenburg, 2010). However, when the principle of inverse effectiveness is considered and relatively weak stimuli are presented together, there has been some evidence for super-additive responses (Senkowski et al., 2011).”</p>
<p>“While behavioural outcomes for multisensory stimuli can be predicted by MLE, and single neuron responses follow the principles of inverse effectiveness and super- additivity, among others (Rideaux et al., 2021), how audiovisual super-additivity manifests within populations of neurons is comparatively unclear given the mixed findings from relevant fMRI and EEG studies. This uncertainty may be due to biophysical limitations of human neuroimaging techniques, but it may also be related to the analytic approaches used to study these recordings. For instance, superadditive responses to audiovisual stimuli in EEG studies are often reported from very small electrode clusters (Molholm et al., 2002; Senkowski et al., 2011; Talsma et al., 2007), suggesting that neural super-additivity in humans may be highly specific. However, information encoded by the brain can be represented as increased activity in some areas, accompanied by decreased activity in others, so simplifying complex neural responses to the average rise and fall of activity in specific sensors may obscure relevant multivariate patterns of activity evoked by a stimulus.”</p>
<disp-quote content-type="editor-comment">
<p>P9. &quot;(25-75 W, 6 Ω).&quot; This is not important, but it is a strange way to cite the power handling of a loudspeaker.</p>
</disp-quote>
<p>“The loudspeakers had a power handling capacity of 25-75 W and a nominal impedance of 6 Ω.”</p>
<disp-quote content-type="editor-comment">
<p>I am struggling to understand the auditory stimulus:</p>
<p>&quot;Auditory stimuli were 100 ms clicks&quot;. Is this a 100-ms long train of clicks? A single pulse which is 100ms long would not sound like a click, but two clicks once filtered by the loudspeaker. Perhaps they mean 100us.</p>
<p>&quot;..with a flat 850 Hz tone embedded within a decay envelope&quot;. Does this mean the tone is gated - i.e. turns on and off slowly? Or is it constant?</p>
</disp-quote>
<p>We thank the reviewer for catching this. ‘Click’ may not be the most apt way of defining the auditory stimulus. It was a 100 ms square wave tone with decay, i.e., with an onset at maximal volume before fading gradually. Given that the length of the stimulus was 100 ms, the decay occurs quickly and provides a more ‘click-like’ percept than a pure tone. We have provided a representation of the sound below for further clarification. This represents the amplitude from the L and R speakers for maximally-left and maximally-right stimuli. We have added this clarification in the revised manuscript.</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-97230-sa3-fig1.jpg" mimetype="image"/>
</fig>
<p>“Auditory stimuli were 100 ms, 850 Hz tones with a decay function (sample rate = 44, 100 Hz; volume = 60 dBA SPL, as measured at the ears).”</p>
<disp-quote content-type="editor-comment">
<p>P10. &quot;Stimulus modality was either auditory, visual, or audiovisual. Trials were blocked with short (~2 min) breaks between conditions&quot;.</p>
<p>Presumably the blocks were randomised across participants.</p>
</disp-quote>
<p>Condition order was not randomised across participants, but counterbalanced. This has been clarified in the manuscript.</p>
<p>“Stimulus modality was auditory, visual or audiovisual, presented in separate blocks with short breaks (~2 min) between conditions (see Figure 6A for an example trial). The order of conditions was counterbalanced across participants.”</p>
<disp-quote content-type="editor-comment">
<p>P15. Feels like there is a step not described here: &quot;The d' of the auditory and visual conditions can be used to estimate the predicted 'optimal' sensitivity of audiovisual signals as calculated through MLE.&quot; Do they mean sqrt[ (d'A)^2 + (d'V)^2] ? If it is so simple then it may as well be made explicit here. A quick calculation from eyeballing Figures 2B and 2C suggests this is the case.</p>
</disp-quote>
<p>We thank the reviewer for raising this point of clarification. Yes, the ‘optimal’ audiovisual sensitivity was calculated as the hypotenuse of the auditory and visual sensitivities. This calculation has been made explicit in the revised manuscript.</p>
<p>The d’ from the auditory and visual conditions can be used to estimate the predicted ‘optimal’ sensitivity to audiovisual signals as calculated through the following formula:</p>
<disp-formula id="sa3equ2">
<graphic mime-subtype="jpg" xlink:href="elife-97230-sa3-equ2.jpg" mimetype="image"/>
</disp-formula>
<disp-quote content-type="editor-comment">
<p>&quot;The perceived source location of auditory stimuli was manipulated via changes to interaural intensity and timing (Whitworth &amp; Jeffress, 1961; Wightman &amp; Kistler, 1992).&quot; The stimuli were delivered by a pair of loudspeakers, and the incident sound at each ear would be a product of both speakers. And - if there were a time delay between the two speakers, then both ears could potentially receive separate pulses one after the other at different delays. Did they record this audio stimulus with manikin? If not, it would be very difficult to know what it was at the ears. I don't doubt that if they altered the relative volume of the loudspeakers then some directionality would be perceived but I cannot see how the interaural level and timing differences could be matched - as if the sound were from a single source. I doubt that this invalidates their results, but to present this as if it provided matched spatial and timing cues is wrong, and I cannot work out how they can attribute an azimuthal location to the sound. For replication purposes, it would be useful to know how far apart the loudspeakers were and what the timing and level differences actually were.</p>
</disp-quote>
<p>The behavioural tasks each had evenly distributed ‘source locations’ on the horizontal azimuth of the computer display (8 for the behavioural session, 5 for the EEG session). We manipulated the perceived location of auditory stimuli through interaural time delays and interaural level differences. By first measuring the forward (<italic>z</italic>) and horizontal (<italic>x</italic>) distance of each source location to each ear, the method worked by calculating what the time-course of a sound wave should be at the location of the ear given the sound wave at the source. Then, for each source location, we can calculate the time delay between speakers given the vectors of <italic>x</italic> and <italic>z</italic>, the speed of sound and the width of the head.  As the intensity of sound drops inversely with the square of the distance, we can divide the sound wave by the distance for each source location to provide the interaural level difference. Though we did not record the auditory stimulus with a manikin, our behavioural analyses show that participants were able to detect the directions of auditory stimuli from our manipulations, even to a degree that significantly exceeded the localisation accuracy for visual stimuli (for the behavioural session task). This information has been clarified in the manuscript.</p>
<p>“Auditory stimuli were played through two loudspeakers placed either side of the display (80 cm apart for the behavioural session, 58 cm apart for the EEG session).”</p>
<p>“The perceived source location of auditory stimuli was manipulated via changes to interaural level and timing (Whitworth &amp; Jeffress, 1961; Wightman &amp; Kistler, 1992). The precise timing of when each speaker delivered an auditory stimulus was calculated from the following formula:</p>
<disp-formula id="sa3equ3">
<graphic mime-subtype="jpg" xlink:href="elife-97230-sa3-equ3.jpg" mimetype="image"/>
</disp-formula>
<p>where <italic>x</italic> and <italic>z</italic> are the horizontal and forward distances in metres between the ears and the source of the sound on the display, respectively, <italic>r</italic> is the head radius, and <italic>s</italic> is the speed of sound. We used a constant approximate head radius of 8 cm for all participants. <italic>r</italic> was added to <italic>x</italic> for the left speaker and subtracted for the right speaker to produce the interaural time difference. For ±15° source locations, interaural timing difference was 1.7 ms. To simulate the decrease in sound intensity as a function of distance, we calculated interaural level differences for the left and right speakers by dividing the sounds by the left and right distance vectors. Finally, we resampled the sound using linear interpolation based on the calculations of the interaural level and timing differences. This process was used to calculate the soundwaves played by the left and right speakers for each of the possible stimulus locations on the display. The maximum interaural level difference between speakers was 0.14 <italic>A</italic> for ±15° auditory locations, and 0.07 <italic>A</italic> for ±7.5°.</p>
<disp-quote content-type="editor-comment">
<p>I am confused about this statement: &quot;A quantification of the behavioural sensitivity (i.e., steepness of the curves) revealed significantly greater sensitivity for the audiovisual stimuli than for the auditory stimuli alone (<italic>Z</italic> = -3.09, <italic>p</italic> = .002),&quot; It is not clear from the methods how they attributed sound source angle to the sounds. Conceivably they know the angle of the loudspeakers, and this would provide an outer bound on the perceived location of the sound for extreme interaural level differences (although free field interaural timing cues can create a wider sound field).</p>
</disp-quote>
<p>Our analysis of behavioural sensitivity was dependent on the set ‘source locations’ that were used to calculate the position of auditory and audiovisual stimuli.  In the behavioural task, participants judged the position of the target stimulus relative to a central stimulus. Thus, for each source location, we recorded how often participants correctly discriminated between presentations. The quoted analysis acknowledges that participants were more sensitive to audiovisual stimuli than auditory stimuli in the context of this task. A full explanation of how source location was implemented for auditory stimuli has been clarified in the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>It would be very nice to see some of the &quot;channel&quot; activity - to get a feel for the representation used by the decoder.</p>
</disp-quote>
<p>We have included responses for the five channels as a Supplemental Figure.</p>
<disp-quote content-type="editor-comment">
<p>Figure 6 appears to show that there is some agreement between behaviour and neural responses - for the audiovisual case alone. The positive correlation of behavioural and decoding sensitivity appears to be driven by one outlier - who could not perform the audiovisual task (and indeed presumably any of them). Furthermore, if we were simply Bonferonni correct for the three comparisons, this would become non-significant. It is also puzzling why the unisensory behaviour and EEG do not correlate - which seems to again suggest a poor correspondence between them. Opposite to the claim made.</p>
</disp-quote>
<p>We understand the reviewer’s concern here. We would like to note, however, that each correlation used unique data sets – that is, the behavioural and neural data for each separate condition. In this case, we believe a Bonferroni correction for multiple comparisons is too conservative, as no data set was compared more than once. Neither the behavioural nor the neural data were normally distributed, and both contained outliers. Rather than reduce power through outlier rejection, we opted to test correlations using Spearman’s rho, which is resistant to outliers1. It is also worth noting that, without outlier rejection, the audiovisual correlation (<italic>p</italic> = .003) would survive a Bonferroni correction for 3 comparisons. The nonsignificant correlation in the auditory and visual conditions might be due to the weaker responses elicited by unisensory stimuli, with the reduced signal-to-noise ratio obscuring potential correlations. Audiovisual stimuli elicited more precise responses both behaviourally and neurally, increasing the power to detect a correlation.</p>
<p>(1) Wilcox, R.R. (2016), Comparing dependent robust correlations. <italic>British Journal of Mathematical &amp; Statistical Psychology, 69</italic>(3), 215-224. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/bmsp.12069">https://doi.org/10.1111/bmsp.12069</ext-link></p>
<p>“We also found a significant positive correlation between participants’ behavioural judgements in the EEG session and decoding sensitivity for audiovisual stimuli. This result suggests that participants who were better at identifying stimulus location also had more reliably distinct patterns of neural activity. The lack of neurobehavioural correlation in the unisensory conditions might suggest a poor correspondence between the different tasks, perhaps indicative of the differences between behavioural and neural measures explained previously. However, multisensory stimuli have consistently been found to elicit stronger neural responses than unisensory stimuli (Meredith &amp; Stein, 1983; Puce et al., 2007; Senkowski et al., 2011; Vroomen &amp; Stekelenburg, 2010), which has been associated with behavioural performance (Frens &amp; Van Opstal, 1998; Wang et al., 2008). Thus, the weaker signalto-noise ratio in unisensory conditions may prevent correlations from being detected.”</p>
<p>Further changes:</p>
<p>(1)   To improve clarity, we shifted the Methods section to after the Discussion. This change included updating the figure numbers to match the new order (Figure 1 becomes Figure 6, Figure 2 becomes Figure 1, and so on).</p>
<p>(2)   We also resolved an error on Figure 2 (previously Figure 3). The final graph (Difference between AV and A + V) displayed incorrect values on the Y axis.</p>
<p>This has now been remedied.</p>
</body>
</sub-article>
</article>