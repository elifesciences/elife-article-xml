<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109155</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109155</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109155.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Improved sensory representations as a result of temporal adaptation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-9567-2435</contrib-id>
<name>
<surname>Brands</surname>
<given-names>Amber Marijn</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Oz</surname>
<given-names>Zilan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Vukšić</surname>
<given-names>Nikolina</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ortiz</surname>
<given-names>Paulo</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5536-6128</contrib-id>
<name>
<surname>Groen</surname>
<given-names>Iris Isabelle Anna</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>i.i.a.groen@uva.nl</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dkp9463</institution-id><institution>Informatics Institute, University of Amsterdam</institution></institution-wrap>, <city>Amsterdam</city>, <country country="NL">Netherlands</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dkp9463</institution-id><institution>Department of Psychology, University of Amsterdam</institution></institution-wrap>, <city>Amsterdam</city>, <country country="NL">Netherlands</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2026-02-03">
<day>03</day>
<month>02</month>
<year>2026</year>
</pub-date>
<volume>15</volume>
<elocation-id>RP109155</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-10-27">
<day>27</day>
<month>10</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-10-28">
<day>28</day>
<month>10</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.26.605075"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2026, Brands et al</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>Brands et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109155-v1.pdf"/>
<abstract>
<p>Human perception is robust under challenging conditions, for example when sensory inputs change over time. Temporal adaptation in the form of reduced responses to repeated external stimuli is ubiquitously observed in the brain, yet it remains unclear how repetition suppression aids recognition of novel inputs. To clarify this, we collected behavioural and electrocorticography (EEG) measurements while human participants categorized objects embedded in visual noise patterns after first viewing these patterns in isolation, inducing adaptation to the noise stimulus. We furthermore manipulated the availability of object information in the visual input by varying the contrast of the noise-embedded objects. Our results provide convergent behavioral, neural and computational evidence of a benefit of temporal adaptation on sensory representations. Adapting to a noise pattern resulted in overall faster object recognition and better recognition of objects as object contrast increased. These adaptation-induced behavioral improvements were accompanied by more pronounced contrast-dependent modulation of object-evoked EEG responses, and better decoding of object information from EEG activity. To identify potential neural computations mediating the benefits of temporal adaptation on object recognition, we equipped task-optimized deep convolutional neural networks (DCNNs) with different candidate mechanisms to adjust network activations over time. DCNNs with intrinsic adaptation mechanisms, such as additive suppression, best captured contrast-dependent human performance benefits, whilst also showing improved object decoding as a result of adaptation. Finally, adaptation effects in networks that use temporal divisive normalization, a biologically-plausible canonical neural computation, were most robust to spatial shifts, suggesting that temporal adaptation via divisive normalization aids stable representations of time-varying visual inputs. Overall, our results demonstrate how temporal adaptation improves sensory representations and identify candidate neural computations mediating these effects.</p>
</abstract>
<abstract abstract-type="teaser">
<title>Author summary</title>
<p>Robust perception is essential for the human brain to detect, process, and act upon new sensory inputs. Temporal adaptation is believed to play a key role in robust sensory processing by allowing neurons to continuously adjust their responses to previous inputs in order to optimize the processing of future inputs. Here, we show that temporal adaptation aids visual object recognition by improving neural representations of object contrast and object category. By emulating temporal adaptation in deep convolutional neural network models with different computational mechanisms, we identify candidate neural computations mediating benefits of temporal adaptation on sensory processing.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>This revised manuscript has a corrected panel placement in Figure 5.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>In natural, real-world environments, humans often need to process sensory stimuli in challenging settings, for example when detecting an approaching car on a misty road (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). How does the human brain compute robust sensory representations in such suboptimal, dynamically varying viewing conditions? One neural phenomenon that may aid robust perception is temporal adaptation to previously perceived inputs. Numerous studies have shown that repeated sensory stimulation reduces neural responses, both in the visual modality (<xref ref-type="bibr" rid="c28">Grill-Spector et al. 2006</xref>; <xref ref-type="bibr" rid="c36">Henson 2003</xref>; <xref ref-type="bibr" rid="c31">Groen et al. 2022</xref>), and other senses (<xref ref-type="bibr" rid="c73">Whitmire and Stanley, 2016</xref>). Typically, the brain shows stronger response reductions for more similar inputs (e.g., <xref ref-type="bibr" rid="c55">Sawamura et al. 2006</xref>; <xref ref-type="bibr" rid="c7">Brands et al. 2024</xref>; <xref rid="fig1" ref-type="fig">Fig. 1B</xref>). This form of temporal adaptation, known as repetition suppression, can improve recognition of the repeated stimulus itself, a process known as priming (<xref ref-type="bibr" rid="c16">Desimone, 1996</xref>; <xref ref-type="bibr" rid="c56">Schacter and Buckner, 1998</xref>; <xref ref-type="bibr" rid="c36">Henson, 2003</xref>). However, adaptation is also thought to increase sensitivity to other, novel stimuli (the approaching car), by decreasing the saliency of recently seen stimuli (the misty road), so as to efficiently process changes in the environment (<xref ref-type="bibr" rid="c5">Barlow, 1993</xref>; <xref ref-type="bibr" rid="c71">Vogels, 2016</xref>; <xref ref-type="bibr" rid="c12">Clifford et al., 2007</xref>; <xref ref-type="bibr" rid="c40">Kohn, 2007</xref>). Benefits of adaptation on low-level vision (contrast sensitivity, orientation tuning and motion perception) have been extensively documented (e.g., <xref ref-type="bibr" rid="c59">Solomon and Kohn 2014</xref>), but its influence on higher-level neural representations, and how this aids perceptual performance, is less understood. Here, we examine how temporal adaptation facilitates sensory processing of objects by measuring human neural responses and recognition behavior with and without prior adaptation.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Neural response properties in visual cortex that affect perception under challenging viewing conditions.</title>
<p><bold>A</bold>: Humans are able to recognize temporally varying targets, for example an approaching car, in suboptimal viewing conditions, for example mist. <bold>B</bold>: Temporal adaptation refers to the reduction of neural responses when stimuli are repeated, with more pronounced reductions for similar compared to different repeated inputs. <bold>C</bold>: Contrast modulation is characterized by the contrast response function; neural responses are reduced for low input contrast.</p></caption>
<graphic xlink:href="605075v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Another outstanding question is what neural computations underlie temporal adaptation. Many neural response properties are thought to be governed by canonical computations, arising from intrinsic biophysical mechanisms operating within individual neurons (<xref ref-type="bibr" rid="c73">Whitmire and Stanley, 2016</xref>) or from recurrent interactions (<xref ref-type="bibr" rid="c15">del Mar Quiroga et al., 2016</xref>). A prior study by <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref> investigated the computational mechanisms underlying benefits of adaptation on object recognition by endowing model neurons in a deep convolutional neural network (DCNN) with an additive adaptation state, and found that this simple intrinsic suppression mechanism was sufficient to mimic adaptation-induced object categorization improvements as observed in human participants. An alternative form of intrinsic adaptation, divisive normalization (<xref ref-type="bibr" rid="c34">Heeger, 1992</xref>, <xref ref-type="bibr" rid="c35">1993</xref>), has been proposed as a canonical neural computation underlying a wide range of neural response properties (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>), including repetition suppression in human intracranial recordings (<xref ref-type="bibr" rid="c31">Groen et al., 2022</xref>; <xref ref-type="bibr" rid="c7">Brands et al., 2024</xref>). Here, we investigate whether implementing divisive normalization in a DCNN offers an advantage over additive suppression in simulating improved object recognition after adaptation. Moreover, since prior work has shown that adding recurrent connections to a feedforward DCNN also capture carry-over effects in sequences of visual stimuli well (<xref ref-type="bibr" rid="c65">Tang et al., 2018</xref>; <xref ref-type="bibr" rid="c60">Sörensen et al., 2023</xref>), we also included a third adaptation mechanism based on lateral recurrence.</p>
<p>To elucidate how temporal adaptation to previously seen stimuli can aid novel object perception, human participants categorized objects embedded in repeated noise patterns, while electrocorticography (EEG) was measured. To manipulate task difficulty and better emulate naturalistic viewing conditions, we additionally varied the contrast of the target objects. Contrast reliably modulates visual response magnitudes, as characterized by the contrast response function (CRF) (<xref ref-type="bibr" rid="c3">Albrecht and Hamilton 1982</xref>; <xref ref-type="bibr" rid="c2">Albrecht et al. 2002</xref>; <xref rid="fig1" ref-type="fig">Fig. 1C</xref>). Although such modulations have been most thoroughly characterized in low-level visual areas, they also occur in higher-level areas, resulting in remarkably similar response reductions in intracranial responses as repeated, full-contrast stimuli (<xref ref-type="bibr" rid="c31">Groen et al., 2022</xref>). Here, we asked how these two types of response modulations interact and how they could potentially facilitate or compensate for one another. Based on the results by <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref>, we predicted that adapting to the noise pattern would reduce its saliency by suppressing responses to these stimuli, resulting in stronger responses to the hidden objects, thereby enhancing the quality of the object representations. However, since contrast reduction also suppresses neural responses, we expect temporal adaptation to benefit object representation in a graded fashion - more so for high than for lower object contrasts.</p>
<p>Our EEG analyses focused on characterizing the joint effects of noise adaptation and object contrast on object-evoked responses (ERPs), which are known to be sensitive to both stimulus repetition (<xref ref-type="bibr" rid="c63">Summerfield et al., 2011</xref>; <xref ref-type="bibr" rid="c58">Schweinberger and Neumann, 2016</xref>) and contrast (<xref ref-type="bibr" rid="c69">Vassilev et al., 1994</xref>; <xref ref-type="bibr" rid="c57">Schadow et al., 2007</xref>). Given that divisive normalization can account for both contrast- and repetition-induced response reductions (<xref ref-type="bibr" rid="c31">Groen et al., 2022</xref>), we anticipated that DCNNs with divisive normalization would more accurately capture potential interaction effects of repetition suppression and contrast on object recognition as compared to the additive model introduced by <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref>. We furthermore asked whether adaptation resulted in better decoding of object properties from the EEG signal, as well as convolutional neural network activations.</p>
<p>Consistent with joint effects of adaptation and contrast modulation, we find that temporal adaptation to the noise pattern leads to improved categorization performance for higher but not lower contrast objects. The behavioral improvement of adaptation on object recognition is accompanied by stronger contrast modulation in evoked responses and improved decoding of object category from EEG signals after adaptation. We find that DCNNs with intrinsic adaptation mechanisms more accurately capture human behavior and neural responses than lateral recurrence mechanisms. Moreover, DCNNs that employ intrinsic adaptation via divisive normalization are also more robust to spatial shifts of the adapter, suggesting that this mechanism contributes to temporal stability of adaptation. All together, these results suggest that robust object recognition arises from an interplay between temporal adaptation and contrast modulation and suggest that divisive normalization is not only an effective mechanism for capturing neural responses in sensory cortex, but also leads to perceptual robustness.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>To examine how temporal adaptation improves representations of novel sensory inputs, we collected EEG data while participants categorized test images containing objects with various contrast levels embedded in noise patterns (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>), after inducing distinct forms of adaptation. Specifically, we varied the adapter preceding the test image to be either the same of a different noise pattern (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>), or a uniform control (blank; <xref rid="fig2" ref-type="fig">Fig. 2C</xref>). Below, we first reveal how prior adaptation to the noise pattern impacts object recognition performance, after which we show how it affects the representation of object class and contrast in event-related responses (ERPs) to the test images. Next, we compare several computational mechanisms that could underlie temporal adaptation using deep convolutional neural network (DCNN) modeling (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>), and asses which models most accurately capture human object categorization performance and neural representations. Among the mechanisms tested are those inspired by previous work by <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref>, including an intrinsic suppression mechanism and a mechanism operating across feature maps through lateral recurrence (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>). Additionally, we also examined whether an alternative, biologically-plausible form of temporal adaptation, namely divisive normalization, offers advantages over the other two mechanisms and better aligns with the human data (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>). Lastly, we investigate the robustness of the DCNNs implemented with different adaptation mechanisms, particularly their ability to maintain performance when spatial shifts are applied to the input.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Experimental design and network modeling.</title>
<p><bold>A</bold>: Object recognition task with the test images consisting of objects (digits 3, 6, 8 and 9 from the MNIST dataset) embedded in a pixelised noise pattern. Contrast of the digit image was varied (50%, 60%, 70%, 80% and 90%). <bold>B</bold>: Adaptation trials, consisting of the presentation of same (left) or different (right) noise prior to the test image. <bold>C</bold>: Control trials, consisting of the presentation of the test image in isolation. <bold>D</bold>: DCNNs were trained with a feedforward backbone consisting of three convolutional filter layers, one fully connected layer and a readout layer. After each convolution, history-dependent adaptation was applied (depicted in green), feeding activations from the previous model time step, i.e. previous feedforward pass. Filter sizes are represented within parentheses. <bold>E</bold>: Temporal adaptation mechanisms as implemented by <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref>. <italic>Left</italic>, Additive suppression whereby each unit feeds its activations from the previous timestep. <italic>Right</italic>, Temporal adaptation mechanism which feeds unit activations across feature maps. <bold>F</bold>: Divisive normalization introduced by <xref ref-type="bibr" rid="c34">Heeger (1992</xref>, <xref ref-type="bibr" rid="c35">1993</xref>) which feeds activations from the previous timestep using divisive rather than additive suppression, here implemented using a recursive multiplicative feedback signal (see Methods: DCNN modeling).</p></caption>
<graphic xlink:href="605075v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2a">
<title>Faster and more accurate object recognition after noise adaptation</title>
<p>First, we established that adding noise to the test images impaired object recognition accuracy: whereas subjects readily recognized clean objects (mean categorization performance = 98.15%, SD = 0.30%), performance decreased substantially for objects embedded in noise patterns (mean = 60.61%, SD = 9.39%), with stronger deterioration as object contrast decreased (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>). We analyzed categorization accuracy using a linear mixed effects model (see Materials and Methods, <italic>Statistical testing for behavioral data</italic>) that tested for effects of adapter type (blank, same and different) and object contrast (50%, 60%, 70%, 80% and 90%). We found a main effect of adapter (<italic>F</italic> <sub>(2</sub>,<sub>4)</sub> = 28.24, <italic>p &lt;</italic> 0.001) and object contrast (<italic>F</italic> <sub>(2</sub>,<sub>4)</sub> = 141.81, <italic>p &lt;</italic> 0.001), as well as an interaction (<italic>F</italic> <sub>(2</sub>,<sub>8)</sub> = 2.75, <italic>p</italic> = 0.006), indicating that adaptation differently affects performance across object contrasts. Adapting to noise had no effect for the two lowest contrast levels (50% and 60%), with similar performance across conditions, while for 70% object contrast, performance improved after adapting to both same and different noise, but not to the blank adapter (same, <italic>p &lt;</italic> 0.001; different, <italic>p &lt;</italic> 0.001). For the highest contrast levels (80% and 90%), adapting to the same noise resulted in higher performance compared to both blank (80%, <italic>p &lt;</italic> 0.001; 90%, <italic>p &lt;</italic> 0.001) and different noise adapters (80%, <italic>p</italic> = 0.04; 90%, <italic>p</italic> = 0.0019). These results confirm earlier findings showing a clear benefit of temporal adaptation on novel object recognition: the performance-degrading effect of the spatial noise patterns is diminished by the prior adaptation phase, with same-noise adaptation resulting in the most robust categorization improvement. Notably, adaptation did not improve recognition for the weakest object contrasts, indicating that human behavioral benefits of adaptation are somewhat contrast-dependent.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Effects of temporal adaptation on categorization performance and reaction times for noise-embedded objects.</title>
<p><bold>A</bold>: Accuracy across contrast levels for test images shown after a blank adapter (grey) or a same (blue) or different (yellow) noise adapter as the test image. The solid red line shows performance for objects without noise; the dotted black line shows chance level (25%). Each point depicts an individual subject. <bold>B</bold>: Similar as (A) but for reaction times. Adaptation to same noise most improves recognition performance for higher contrast levels, while adaption to both same and different noise results in faster reaction times overall. Linear mixed effect model, post-hoc Tukey test, <sup>∗</sup> p <italic>&lt;</italic> 0.05, <sup>∗∗</sup> p <italic>&lt;</italic> 0.01, <sup>∗∗∗</sup> p <italic>&lt;</italic> 0.001. This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure3.py">mkFigure3.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We also examined how temporal adaptation affected reaction times (RTs) to the test images. Similar to categorization accuracy, we found a main effect of adapter (F<sub>(2</sub>,<sub>4)</sub> = 105.09, <italic>p &lt;</italic> 0.001) and contrast level (<italic>F</italic> <sub>(2</sub>,<sub>4)</sub> = 7.42, <italic>p &lt;</italic> 0.001) as well as an interaction effect (<italic>F</italic> <sub>(2</sub>,<sub>8)</sub> = 2.81, <italic>p</italic> = 0.005). However, the pattern of RT differences between conditions was different from that observed for accuracy; response times for test images presented with a noise adapter (either the same or different noise) were significantly shorter compared to blank adapters for all object contrasts (pairwise <italic>p &lt;</italic> 0.001), with largest RT effects for the lowest contrasts (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>). In fact, having a noise adapter resulted in RTs similar to those of clean object images, suggesting that temporal adaption relieved the detrimental impact of the added noise on response speed. Together, our behavioral results reveal clear benefits of temporal adaptation on object categorization performance, with improved recognition of noise-embedded objects (given sufficient object contrast) after first adapting to the same preceding noise, accompanied by overall faster responses after adaptation.</p>
</sec>
<sec id="s2b">
<title>Temporal adaptation affects early and late responses in occipito-parietal electrodes</title>
<p>We next examined how visually-evoked ERPs were modulated by temporal adaptation. All test images, regardless of the presented adapter, elicited deflections in occipital and parietal electrodes both early (100-150 ms, <xref rid="fig4" ref-type="fig">Fig. 4A</xref>, <italic>top</italic>) and later in the response (∼ 300 ms, <xref rid="fig4" ref-type="fig">Fig. 4A</xref>, <italic>top</italic>). Direct comparison shows that ERP differences due to adaptation were most pronounced in occipito-parietal electrodes, evident in a clear difference in ERP amplitude between the test images with noise vs. blank adapters (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>). Notably, responses after same-noise adaptation showed the largest deflection difference compared to the blank adapter. To quantify these response differences, we first determined which time points showed significant effects of repetition suppression, by testing when the average ERP to the noise-adapted test images differed from those to test images following a uniform control adapter (see Materials and Methods, <italic>Identification of time windows of interest</italic>). We then computed ERP response magnitudes separately for same- and different-noise adapters in those time windows. For occipital electrodes we found two significant time windows with suppression effects (45-111 ms and 201-271 ms), but these contained no significant difference between same and different noise adapters (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>). For parietal electrodes, we found three time windows, whereby similar to occipital electrodes, the first two (29-80 ms and 158-228 ms) did not differ across adapter types, but during the third time window (341-431 ms), same-noise adapters resulted in larger ERP deflections than different noise adapters (dependent-samples T-test, <italic>t</italic> <sub>(20)</sub> = −3.54, <italic>p</italic> = 0.002, <xref rid="fig4" ref-type="fig">Fig. 4D</xref>). These results show that temporal adaptation to a noise pattern affects evoked activity to test images in both occipital and parietal electrodes, with adapter-specific effects emerging later in time at parietal electrodes only. There, same-noise adaptation results in stronger ERP deflections than different-noise adaptation, possibly reflecting facilitated neural processing of the object due to repetition suppression of the noise adapter.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Temporal adaptation results in enhanced object-evoked activity at parieto-occipital electrodes.</title>
<p><bold>A</bold>: Topomaps showing ERP amplitudes for test images presented without adapter (blank, left) and test images presented after adapting to same (middle) or different noise (right). <bold>B</bold>: Difference in evoked potentials to test images with and without adapters (left, same noise versus blank; right, different noise versus blank). <bold>C</bold>: Left, average ERP per condition for electrodes Iz, Oz, O1 and O2. Middle, differences in ERPs with and without adaptation. Time windows for test images with adaptation for which same- (s) and different-noise (d) conditions are significantly different from blank (b) conditions are depicted by shaded red areas. Right, average ERP amplitudes for same vs. different noise adaptation within the identified time windows. Shaded regions and errorbars depict SEM across subjects. <bold>D</bold>: Same as C, but for electrodes P9 and P10. Adaptation-specific differences arise in parietal electrodes later in the response, with stronger deflections when adapting to same as opposed to different noise. T-test (two-sided), <sup>∗∗</sup> p <italic>&lt;</italic> 0.01. This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure456.py">mkFigure456.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>(Occipito)-parietal electrodes exhibit contrast-dependent response modulation</title>
<p>We have shown that evoked responses are modulated by preceding inputs and that adaptation-specific differences occur later in the response. In addition to inducing temporal adaptation, we also varied the contrast of the noise-embedded objects. Consistent with the fact that the test images differed only marginally in their overall contrast level (which was dominated by the contrast of the noise pattern; see Materials and Methods, <italic>Stimuli</italic>), we found no object contrast-dependent differences in early ERP responses (<italic>&lt;</italic> 200 ms, <xref rid="fig5" ref-type="fig">Fig. 5A</xref>). Interestingly, however, later responses (<italic>&gt;</italic> 200 ms) in both occipito-parietal and parietal, but not occipital channels varied systematically with object contrast (<xref rid="fig5" ref-type="fig">Fig. 5B-D</xref>). To test for an effect of object contrast, we performed a linear fit on the average ERP amplitude in the late time window (see Materials and Methods, <italic>Identification of time windows of interest</italic>) and found a significant non-zero slope at parietal (one-sample T-test, slope of linear fit against 0, <italic>t</italic> <sub>(20)</sub> = −8.93, <italic>p &lt;</italic> 0.001, <xref rid="fig5" ref-type="fig">Fig. 5B</xref>) and occipito-parietal (<italic>t</italic> <sub>(20)</sub> = 6.24, <italic>p &lt;</italic> 0.001, <xref rid="fig5" ref-type="fig">Fig. 5C</xref>), but not occipital electrodes (<italic>t</italic> <sub>(20)</sub> = −0.51, <italic>p</italic> = 0.62, <xref rid="fig5" ref-type="fig">Fig. 5D</xref>). These results show that response amplitudes of parietal electrodes, in addition to being modulated by temporal adaptation, were also modulated by the contrast of the noise-embedded object.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Responses in (occipito-)parietal, but not occipital, electrodes are modulated by the contrast level of noise-embedded objects.</title>
<p><bold>A</bold>: Topomaps representing ERPs during presentation of the test image for three object contrast levels. <bold>B</bold>: Left, ERPs shown separately per contrast level for P9 and P10. Right, response magnitude computed by taking the mean amplitude for the P300 component. <bold>C-D</bold>: Same as B for occipito-parietal (C) electrodes, including Pz, P1, P2, P3 and P4, and occipital electrodes (D), including Iz, Oz, O1 and O2. The P300 component of (occipito-)parietal electrodes is modulated by object contrast. 1-sample T-Test (coefficient of linear curve against 0), <sup>∗∗∗</sup> p <italic>&lt;</italic> 0.001. This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure456.py">mkFigure456.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Increased contrast-dependence after temporal adaptation in parietal electrodes</title>
<p>So far, we found contrast-dependent benefits of temporal adaptation on object categorization performance, as well as independent effects of noise adaptation and contrast on ERPs. To test whether temporal adaptation in fact facilitated neural object processing, we next searched for evidence of enhanced object contrast processing as a consequence of adaptation. Since ERP effects of temporal adaptation and object contrast co-occurred on parietal electrodes (P9 and P10), we specifically focus on these electrodes to test for the interaction, by now separating responses to test images with different object contrast levels by the type of adapter. This more focused comparison indicated that parietal electrodes indeed exhibited more pronounced contrast-dependent modulation after same-noise adapters, compared to blank or different-noise adapters (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>). A linear fit on response amplitudes in late time windows across object contrast levels shows that adapting to the same noise results in significant contrast-dependent modulation (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>), with increasing negative amplitudes for increasing contrast levels (one-sample T-test, slope of linear fit against 0, <italic>t</italic> <sub>(20)</sub> = 3.32, <italic>p</italic> = 0.003). Although ERP responses to test images in the other conditions showed similar trends, the linear fit was not significant (different-noise trials, <italic>t</italic> <sub>(20)</sub> = 0.57, <italic>p</italic> = 0.57); blank trials, <italic>t</italic> <sub>(20)</sub> = 0.62, <italic>p</italic> = 0.54). Moreover, these adapter-specific effects on contrast modulation were not observed in the occipital and occipito-parietal electrode groups. To conclude, our EEG data reveal that temporal adaptation induces specific modulations of EEG responses to noise-embedded objects, resulting in more pronounced object contrast-dependent responses in occipito-parietal electrodes beyond 200 ms of visual processing.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>More pronounced object contrast modulation at parietal electrodes after same-noise adaptation.</title>
<p><bold>A</bold>: ERPs for P9 and P10 shown separately for trials without adaptation (left) and adaptation with same (middle) or different (right) noise compared to the test image. The shaded regions depict the SEM across subjects. <bold>B</bold>: Response magnitude was computed by taking the mean amplitude for the P300 component for the similar experimental conditions as in panel (A). ERP signals exhibit significant object contrast-dependent modulation, evident as increasingly negative response deflections for increasingly higher contrast levels, after adapting to same noise only. 1-sample T-Test (coefficient of linear curve against 0), <sup>∗∗</sup> p <italic>&lt;</italic> 0.01. This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure456.py">mkFigure456.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2e">
<title>Temporal adaptation leads to increased object decoding from ERP signals</title>
<p>The more pronounced neural modulations by object contrast after same-noise adaptation suggests that repetition suppression of the noise pattern resulted in better neural encoding of the noise-embedded object. While this increased contrast-dependence may indicate enhanced processing of the embedded objects, it does not directly demonstrate that same-noise adaptation benefits object recognition performance by improving the object discriminability. To test this, we assessed how well we could decode object class from ERP responses to the test images (see Materials and Methods, <italic>ERP Decoding Analysis</italic>). If temporal adaptation improves behavioral categorization by enhancing the neural representation of the object, we expect more successful decoding after same-noise adapters, compared to blank or different-noise adapters.</p>
<p>The results show that inducing temporal adaptation to the noise pattern indeed results in better object decoding from EEG responses. We first verified that object representations were detectable in the evoked activity by performing the decoding analysis on ‘clean’ objects presented in full contrast without embedded noise, which resulted in strong above-chance decoding accuracy (one-sample T-test, <italic>t</italic><sub>(20)</sub> = 8.48, <italic>p &lt;</italic> 0.001; <xref rid="fig7" ref-type="fig">Fig. 7A</xref>). Unsurprisingly, embedding the objects in noise strongly reduced the quality of object representations, since for test images preceded by blank adapters, decoding accuracy dropped to chance level. Remarkably, however, these object-representations were partly restored after adaptation to noise, resulting in significant above-chance decoding accuracy after both same-noise (one-sample T-test against chance level, <italic>t</italic><sub>(20)</sub> = 3.36, <italic>p</italic> = 0.003) and different-noise adapters (one-sample T-test, <italic>t</italic><sub>(20)</sub> = 2.79, <italic>p</italic> = 0.01), mirroring the improved behavioral object recognition performance as a result of adaptation.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Temporal adaptation improves decoding of objects from EEG responses.</title>
<p><bold>A</bold>: Average decoding accuracy for predicting the presented object class based on evoked potentials for test images without (dark gray) and with embedded noise, including blank (light grey), same- (blue) and different-noise (orange) trials. Decoding accuracy’s are averaged for [0, 1]s time window after stimulus onset. The lower asteriks denote significant differences from chance level (0.25, one-sided T-test). The upper asteriks denote significant differences across trial types (one-way ANOVA, post-hoc Tukey test). <sup>∗</sup> p <italic>&lt;</italic> 0.05, <sup>∗∗</sup> p <italic>&lt;</italic> 0.01, <sup>∗∗∗</sup> p <italic>&lt;</italic> 0.001. <bold>B</bold>: Decoding accuracy for the different trial types across time points, with the number of time points for which decoding accuracy was significantly different from chance level (i.e. 25%) noted on the right. Shaded regions depict the SEM across subjects. This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure7_SFig2.ipynb">mkFigure7_SFig2.ipynb</ext-link>.</p></caption>
<graphic xlink:href="605075v4_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Statistical comparison between decoding accuracies for the different adaptation conditions indicated a main effect of adapter type (one-way ANOVA, <italic>F</italic> <sub>(3)</sub> = 7.02, <italic>p &lt;</italic> 0.001), with a significant difference in decoding performance between clean and blank trials (<italic>p &lt;</italic> 0.001) and between same-noise and blank trials (<italic>p</italic> = 0.036), but not between different-noise and blank trials (<italic>p</italic> = 0.13), suggesting an adapter-specific effect on object-representations in the evoked activity. Indeed, decoding accuracy was significantly above chance for substantial more time-points during same-noise trials compared to different-noise trials (<xref rid="fig7" ref-type="fig">Fig. 7B</xref>), which aligns with the most robust perceptual benefit of same-noise adaptation observed in behavior.</p>
<p>We also assessed whether EEG object decoding accuracy was affected by object contrast. While we did not find a main effect of contrast on average decoding accuracy (<xref rid="figs2" ref-type="fig">Supp. Fig. 2A</xref>, one-way ANOVA, <italic>F</italic> <sub>(4)</sub> = 0.34, <italic>p</italic> = 0.85), we did observe above chance decoding accuracy for substantially more time-points for higher than for lower contrast levels (50% = 10, 60% = 26, 70% = 64, 80% = 43, 90% = 76 significant time-points), consistent with the observed improved categorization accuracy in behavior (<xref rid="figs2" ref-type="fig">Supp. Fig. 2B</xref>). Overall, these results demonstrate that temporal adaptation enhances neural decoding of object information under challenging viewing conditions, suggesting that the behavioral benefit of temporal adaptation on object recognition is mediated by improvements in the underlying neural representation.</p>
</sec>
<sec id="s2f">
<title>DCNNs with a single-unit adaptation mechanism align better with human behavior and neural responses</title>
<p>Our results so far show that temporal adaptation to a noise pattern improves participants’ ability to recognize objects embedded in that same noise pattern, in particular for higher object contrasts (<xref rid="fig8" ref-type="fig">Fig. 8A</xref>), and that these behavioral improvements are accompanied by more distinct neural representations of object contrast and category (<xref rid="fig6" ref-type="fig">Fig. 6</xref>-<xref rid="fig7" ref-type="fig">7</xref>). To discern potential computational mechanisms that could mediate these perceptual and neural effects, we endowed DCNNs with three different adaptation mechanisms (<xref rid="fig2" ref-type="fig">Fig. 2D-F</xref>; see Materials and Methods, <italic>DCNNs: temporal adaptation mechanisms</italic>). Two of these, additive suppression and divisive normalization, operate on the level of the single model unit, whereby responsiveness to network inputs is reduced proportional to previous activations of only the unit itself. The other adaptation mechanism uses lateral recurrence, operating across feature maps from previous timesteps. By training and testing the DCNNs on the same task as our human participants (see Materials and Methods, <italic>DCNN modeling: Neural network training</italic>), we examined which mechanism best emulated the observed object recognition benefits of temporal adaptation in behavior and EEG. Following an earlier implementation (<xref ref-type="bibr" rid="c70">Vinken et al., 2020</xref>), our first batch of networks were trained on short sequences (<italic>t</italic> = 3), with the adapter and test image presented for one model timestep each. We also included a DCNN without any temporal adaptation as a baseline.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8:</label>
<caption><title>DCNNs with extended intrinsic adaptation show human-like benefits on performance and neural object representations.</title>
<p>(<italic>previous page</italic>) <bold>A</bold>: Human object recognition performance for same- (blue) and different- (yellow) noise trials (same data as in <xref rid="fig3" ref-type="fig">Fig. 3A</xref>). <bold>B</bold>: Network classification accuracy for the test set after same- and different-noise adapters for DCNNs without a temporal adaptation mechanism. <bold>C</bold>: Network classification accuracy for DCNNs with one of three temporal adaptation mechanisms (from left to right): additive suppression, lateral recurrence and divisive normalization. Input sequences consisted of three images: an adapter (A), a blank (B) and a test (T) image (<italic>ABT</italic>). <bold>D</bold>: Same as panel C for networks trained on input sequences of 21 images (<italic><named-content content-type="sequence">AAAAAAAAAAAAAAA</named-content>BTTTTT</italic>). Networks with instrinsic adaptation mechanisms better approximate the human behavior, showing increasing benefit of temporal adaptation for higher object contrasts. <bold>E</bold>: Decoding accuracy for the first convolutational layer for the test set for same (blue) and different (yellow) noise adapters for DCNNs with temporal adaptation. Decoding performance for later layers is shown in <xref rid="figs8" ref-type="fig">Supp. Fig. 8</xref>. Adapting to the same noise leads to better object decoding of for all adaptation mechanisms. Panels A-D can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure8ABCD.py">mkFigure8ABCD.py</ext-link> and panels E can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure8E_SFig8.py">mkFigure8E_SFig8.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As expected, the baseline model did not benefit from the presentation of the same noise prior to the test image, as this network processes each image independently (<xref rid="fig8" ref-type="fig">Fig. 8B</xref>). In contrast, all networks with temporal adaptation mechanisms show a benefit of adaptation, evident from higher DCNN classification performance for same compared to different noise trials (<xref rid="fig8" ref-type="fig">Fig. 8C</xref>), in line with human behavior (<xref rid="fig8" ref-type="fig">Fig. 8A</xref>) and previous modeling work (<xref ref-type="bibr" rid="c70">Vinken et al., 2020</xref>). However, all models failed to accurately predict the interaction between adapter type and contrast exhibited by humans, who showed stronger performance benefits for same-noise adaptation for high but not low object contrasts. In fact, the models showed the opposite pattern, namely a larger benefit of adaptation for lower rather than higher contrast levels. We reasoned that this mismatch could be due to the short training sequence we used: in our human experiment, the adaptation phase lasted for several seconds, allowing temporal accumulation of adaptation (<xref ref-type="bibr" rid="c1">Akyürek, 2025</xref>), which may be necessary for the emergence of contrast-dependent modulation of neural responses (<xref ref-type="bibr" rid="c50">Ohzawa et al., 1985</xref>). In contrast, the DCNN models could only process the input images for a single time step each, prohibiting prolonged adaptation. Therefore, we trained a second batch of models on increasingly longer input sequences, allowing the models to fully exploit their adaptation mechanisms and to better approximate the extended adaptation period in the human experiment.</p>
<p>Optimization on longer input sequences revealed that DCNNs with temporal adaptation indeed more closely mimic human behavior, evident by the fact that the benefit of adaptation decreased for lower, but not higher object contrasts (<xref rid="fig8" ref-type="fig">Fig. 8D</xref>). Interestingly, the interaction between contrast and temporal adaptation observed in human behavioral performance was better matched by models with intrinsic adaptation mechanisms (additive suppression and divisive normalization) than models with lateral recurrence, which continued to show quite a large improvement for same-noise adaptation even for low-contrast objects. Moreover, unit activations in DCNNs with lateral recurrence did not reduce, but rather increased during the adaptation period, whereas DCNNs with intrinsic adaptation mechanisms showed clear reductions; and while all three models exhibited repetition suppression and contrast modulations to test images, divisive normalization models showed most stable temporal dynamics across initializations (<xref rid="figs3" ref-type="fig">Supp. Fig. 3</xref>-<xref rid="figs7" ref-type="fig">7</xref>).</p>
<p>Finally, we also tested whether DCNNs with adaptation mechanisms similarly exhibit more separable object representations in their model activations, following adaptation. To test this, we performed an analogous object class decoding analysis as for the EEG responses on the model unit activations, separately for each DCNN model layer (see Materials and Methods, <italic>DCNN modeling: DCNN decoding analysis</italic>). The results show that already in the first model layer (<xref rid="fig8" ref-type="fig">Fig. 8E</xref>; results for other layers are provided in <xref rid="figs8" ref-type="fig">Supp. Fig. 8</xref>), all three types of models indeed demonstrated improved object decoding for same compared to different noise adapters. Moreover, DCNNs with intrinsinc adaptation, but not lateral recurrence, again better mirrored the human data, in the form of more improved object decoding for higher but not lower contrasts, after prolonged adaptation.</p>
<p>Overall, our comparison between DCNNs with human behavioral and EEG response patterns suggests that intrinsic adaptation mechanisms best account for the observed perceptual benefits of temporal adaptation on human object categorization behavior, provided the model has sufficient temporal input for the adaptation mechanism to take effect.</p>
</sec>
<sec id="s2g">
<title>Adaptation in DCNNs with divisive normalization is more robust to spatial shifts</title>
<p>Our computational modeling analysis found that DCNNs with intrinsic adaptation mechanisms yielded highest similarity with human behavior, showing neurally plausible response reductions during the adaptation period. While more similar to human behavior and neural decoding across object contrasts, these networks were however objectively <italic>worse</italic> than networks with lateral recurrence in terms of overall performance. Could temporally extended adaptation via intrinsic mechanisms potentially benefit other aspects of perception, such as robustness or representational stability? Compared to humans, classic DCNNs show poorer robustness and generalisation to distorted images (<xref ref-type="bibr" rid="c25">Geirhos et al., 2018b</xref>), with small image perturbations resulting in large deviations in model behavior, a property that is famously exploited in adversarial network attacks (<xref ref-type="bibr" rid="c64">Szegedy et al., 2013</xref>). To investigate whether temporal adaptation can enhance network robustness, we ran a new version of our adaptation experiment on the DCNN models whereby we introduced minor spatial perturbations on the noise patterns. Specifically, we used the same noise pattern for both adapter and test image, but now spatially shifted the pixel values of the latter, such that the resulting test image has a slight offset. We then assessed to what extent the benefits of adaptation on object recognition persisted across these small perturbations, and to what extent this depended on the length of the adaptation period.</p>
<p>Reflecting DCNNs sensitivity to pixel-level changes, shifting the noise pattern during test indeed resulted in reduced accuracy (compared to no shift) for all the temporal adaptation mechanisms (<xref rid="fig9" ref-type="fig">Fig. 9A</xref>), indicating that spatial perturbations reduce the benefit of adaptation for object recognition in these models. However, robustness of adaptation increased as sequence length increased, in particular so for DCNNs with divisive normalization. In fact, we find a main effect (one-way ANOVA, <italic>F</italic> = 40.99, <italic>p &lt;</italic> 0.001) with DCNNs employing divisive normalization showing significant increases in performance with more prolonger adaptation, unlike DCNNs with additive suppression (one-way ANOVA, <italic>F</italic> = 1.24, <italic>p</italic> = 0.33) and lateral recurrence (one-way ANOVA, <italic>F</italic> = 0.46, <italic>p</italic> = 0.77). Moreover, DCNNs with divisive normalization show the least performance reduction over individual pixel shifts with prolonged adaptation (<xref rid="fig9" ref-type="fig">Fig. 9B</xref>), suggesting this mechanism maintains the adapter-induced suppression most robustly, resulting in better performance at test time. Overall, these results demonstrate that DCNNs with divisive normalization not only accurately capture benefits of temporal adaptation on neural object representations and object recognition behavior as observed in human behavior, but also show more spatially robust adaptation, supporting its role as a biologically plausible adaptation mechanism supporting stable sensory representations.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9:</label>
<caption><title>DCNNs with divisive normalization show higher robustness against spatial shifts of input.</title>
<p><bold>A</bold>: Average accuracy across spatial shifts for DCNNs with temporal adaptation trained on short (n = 3, ABT) or long (n = 21, <italic><named-content content-type="sequence">AAAAAAAAAAAAAAA</named-content>BTTTTT</italic>) input sequences. The accuracy is normalized with respect to no shift. DCNNs with divisive normalization (<italic>DN</italic>) are more robust against spatially shifting noise during test time as the sequence length increases compared to DCNNs with additive suppression (<italic>AS</italic>) and lateral recurrence (<italic>LR</italic>). Error bars show the SEM. Independent T-test, <sup>∗∗∗</sup> p <italic>&lt;</italic> 0.001. <bold>B</bold>: Effect of spatially shifting the noise during the test image on network performance. Depicted is the drop in accuracy compared to 0 pixel shift for the three temporal adaptation mechanisms optimized on the different sequence lengths. This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure9.py">mkFigure9.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Our aim was to examine how temporal adaptation aids object recognition in human visual cortex. Confirming prior work, we find that inducing temporal adaptation to a visual noise pattern leads to improved subsequent recognition of objects embedded in that same noise pattern. Here, we show that this adaptation-induced behavioral improvement is accompanied by more pronounced modulation of high-level EEG responses by object contrast, as well as improved decoding of the object class from EEG. Together, these results show that temporal adaptation improves neural representations of noise-embedded objects by allowing the brain to reduce the representation of the surrounding noise, resulting in more robust representation of the object.</p>
<p>We also examined which computations could potentially explain the benefit of temporal adaptation by implementing several candidate mechanisms in DCNNs. We demonstrate that DCNNs with intrinsic mechanisms operating in individual model units (i.e. lacking lateral interactions) most accurately capture the behavioral and neural effects observed in humans. Moreover, DCNNs that adapt via intrinsic divisive normalization show better robustness to spatial shifts of the adapter. Overall, these findings suggest that improved object recognition due to temporal adaptation may result from relatively simple, intrinsic mechanisms, while pointing to temporal divisive normalization in particular as a promising biologically plausible mechanism to increase temporal stability of object representations in convolutional neural networks.</p>
<sec id="s3a">
<title>The role of temporal adaptation on object recognition behavior</title>
<p>Previous work has suggested that temporal adaptation could serve to decrease salience of recently seen stimuli (<xref ref-type="bibr" rid="c59">Solomon and Kohn, 2014</xref>; <xref ref-type="bibr" rid="c71">Vogels, 2016</xref>). We demonstrated this principle by testing human observers and DCNNs on a task with temporally repeated, but task-irrelevant noise patterns, whereby reducing the salience of recently seen features resulted in perceptual changes evident by increased categorization performance. In addition to temporal adaptation, we studied the effects of contrast-dependent modulation of neural responses, which are characterized by a contrast response function (<xref ref-type="bibr" rid="c3">Albrecht and Hamilton, 1982</xref>). The benefit of temporal adaptation was only apparent for objects with sufficient contrast, suggesting that the saliency reduction of the noise patterns could not fully compensate for lowered contrast of the object. Interestingly, we found faster reaction times when participants were presented with an adapter regardless of object contrast, and regardless of whether the adapter noise was the same or different to the test image. This dissociation between human performance and reaction time suggests that different neural mechanisms may underlie improvements in categorization accuracy versus processing speed. The faster reaction times could potentially be attributed to a phenomenon known as contrast gain, referring to a shift of the dynamic range of the CRF as a result of pre-exposure to our high-contrast adapters (<xref ref-type="bibr" rid="c74">Wilson and Humanski, 1993</xref>; <xref ref-type="bibr" rid="c14">Dao et al., 2006</xref>), which has been previously shown to indeed affect reaction times (<xref ref-type="bibr" rid="c8">Cao and Pokorny, 2010</xref>). Since the contrast of the same- and different noise adapters were identical, however, contrast gain cannot explain the specific benefits of same-noise adaptation on categorization accuracy - this effect can rather be attributed to repetition suppression. Overall, these findings emphasize the importance of studying neural response properties simultaneously, as is the case in more naturalistic settings, to better understand how their combined effects influence different facets of perception, including performance and processing speed.</p>
</sec>
<sec id="s3b">
<title>Signatures of repetition suppression and contrast gain in evoked neural responses</title>
<p>Our findings are consistent with prior EEG results showing effects of temporal adaptation (<xref ref-type="bibr" rid="c23">Garrido et al., 2009</xref>; <xref ref-type="bibr" rid="c20">Engell and McCarthy, 2014</xref>) and contrast-dependent modulation (<xref ref-type="bibr" rid="c57">Schadow et al., 2007</xref>; <xref ref-type="bibr" rid="c30">Groen et al., 2013</xref>; <xref ref-type="bibr" rid="c75">Xi et al., 2020</xref>) on evoked responses. Our EEG measurements show clear evidence of repetition suppression, in the form of reduced responses to the repeated noise pattern early in time, but increased responses later in time after adapting to same compared to different noise, suggestive of enhanced processing of the object stimulus due to repetition suppression. At these late time-points, we also found specific effects of the object contrast, with larger deflections as the object contrast level increases. More interestingly, we also observed a joint effect of temporal adaptation and object contrast, with stronger contrast-dependent modulation when adapting to same compared to different or no noise. This finding suggests that the shift of the CRF due to pre-exposure to the high contrast adapter, in combination with the response reduction induced by adapting to the same noise, together result in a larger gain of the neural response to the object, resulting in better categorization performance. An earlier study by <xref ref-type="bibr" rid="c11">Chaumon and Busch (2014)</xref> showed that a reduction of the gain in EEG responses results in reduced accuracy during a detection task, suggesting a link between the input-output-relationship and visual performance. It is important to note that our observations are correlational: future research should investigate in more detail the possibly causal relationship between the gain in neural response magnitude and categorization performance. Nonetheless, we show that the joint effects of temporal adaptation and contrast gain on categorization performance are reflected in the evoked responses, revealing a link between neural processing and perception during object recognition.</p>
</sec>
<sec id="s3c">
<title>Effective implementation of temporal adaptation in DCNNs</title>
<p>Consistent with the results by <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref>, we found that DCNNs with temporal adaptation operating on the level of the single unit best capture behavioral adaptation in our noise adaptation task. In addition, we show these DCNNs capture the interaction between temporal adaptation and contrast gain on categorization performance, but only when we increased the temporal integration window with which the networks could deploy their adaptation mechanism. Importantly, achieving the better match with human behavior necessitated lower network accuracy for low-contrast objects, thus lowering overall object recognition performance. This could mirror a trade-off in sensory adaptation: while adaptation can help filter out redundant information, it may also reduce sensitivity to weak but behaviorally relevant signals when the system has already attenuated similar input over time. The DCNNs’ ability to mimic this balance suggests that single unit-level mechanisms are sufficient to approximate the trade-off effects of adaptation underlying human perception. This computational framework provides a promising approach to investigate how adaptation shapes object representations in the brain in time-varying environments.</p>
<p>In addition to mechanisms operating on the single-unit level, we implemented one form of recurrence operating across channels. While these networks capture the behavioral benefit of adaptation for higher contrast levels, they were not affected by sequence length and failed to capture the interaction between temporal adaptation and contrast modulation. These results may inform us about which perceptual effects could arise from visual processing within individual neurons and which arise due to recurrent interactions. Given the abundance of recurrence in the visual system (<xref ref-type="bibr" rid="c21">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="c62">Sporns and Zwi, 2004</xref>; <xref ref-type="bibr" rid="c45">Markov et al., 2014</xref>) it is most likely that temporal adaptation is implemented by a combination of intrinsic cell properties and recurrent connections. Future work could include other tasks or extend the current model combining both intrinsic adaptation properties and lateral and/or top-down recurrence (<xref ref-type="bibr" rid="c61">Spoerer et al., 2017</xref>; <xref ref-type="bibr" rid="c67">Thorat et al., 2021</xref>; <xref ref-type="bibr" rid="c43">Lindsay et al., 2022</xref>). Nonetheless, we demonstrate how the role of nonlinear neural response properties in perceptual adaptation and object recognition can be elucidated using image-computable DCNNs, an approach that aligns with the neuroconnectionism program (<xref ref-type="bibr" rid="c19">Doerig et al., 2023</xref>) which uses artificial neural networks to model behavior and neural information processing.</p>
</sec>
<sec id="s3d">
<title>Decoding object representations from EEG responses and DCNN activations</title>
<p>We show that high-contrast objects presented without noise can be decoded from EEG responses, which is consistent with prior studies (<xref ref-type="bibr" rid="c13">Contini et al., 2017</xref>; <xref ref-type="bibr" rid="c32">Grootswagers et al., 2019</xref>). More interestingly, we observe an effect of temporal adaptation, where the drop in decoding accuracy as a result of embedding objects in noise can be recovered by adapting to the same or a different noise pattern. While it has been previously shown that stimulus adaptation can improve decoding of objects from neural responses in monkey IT (<xref ref-type="bibr" rid="c39">Kaliukhovich et al., 2013</xref>), to our knowledge facilitatory effects of prior adaptation on object decoding from EEG signals have not been previously reported. We also show that adaptation to same noise results in higher decoding accuracies of the object class for more timepoints compared to adaptation to different noise, suggesting that the perceptual benefit of temporal adaptation may be mediated by improved discriminability of object-related neural representations. DCNNs with temporal adaptation modulated by object contrast show a similar pattern as the neural data, with higher decoding accuracies from unit activations for same-compared to different-noise trials. This convergence in decoding behaviour between humans and DCNNs further supports the idea that temporal adaptation enhances the separability of object representations under challenging conditions. Overall, we reveal a correspondence between informative features representing object identity in evoked responses and DCNN activations, linking neural activations patterns to perceptual experience.</p>
</sec>
<sec id="s3e">
<title>Temporal divisive normalization enhances temporal stability of adaptation</title>
<p>We offer a potential advantage for divisive normalization over other implementations of temporal adaptation, evident by the fact that DCNNs endowed with this adaptation mechanism are more robust against spatially shifting inputs compared to networks with additive suppression and lateral recurrence. One explanation for this difference in robustness could be the multiplicative manner in which divisive normalization feeds back previous inputs as opposed to the additive computations used by the other two mechanisms. Multiplicative feedback effectively scales the current response based on prior activity, adjusting the gain of neural signals, whereas additive feedback simply shifts the response by adding or subtracting a fixed amount. By scaling responses relative to prior activity, this computation may better preserve the relative strength of feature representations, thereby making object identity more resilient to spatial distortions. Previous work has similarly observed benefits of multiplicative interactions, showing improved object recognition of images consisting of composite images of multiple categories (<xref ref-type="bibr" rid="c41">Konkle and Alvarez, 2024</xref>). It has also been suggested that multiplicative effects play a role in shaping the CRF during attention-related processes (<xref ref-type="bibr" rid="c44">Liu et al., 2021</xref>) and are a useful inductive bias for conditional computations (<xref ref-type="bibr" rid="c37">Jayakumar et al., 2020</xref>). Overall, characterizing the unique role of different adaptation mechanisms in how information flow is conditioned is a promising avenue and valuable addition to recent endeavors taken to explore network robustness (e.g. <xref ref-type="bibr" rid="c24">Geirhos et al. 2018a</xref>).</p>
</sec>
<sec id="s3f">
<title>Limitations and future work</title>
<p>First, during the object recognition task, object contrast was varied within a fixed range across subjects. Previous work however, has shown large heterogeneity in contrast sensitivity across individuals (<xref ref-type="bibr" rid="c4">Baker and Graf, 2009</xref>). Consequently, it is likely the lowest and highest contrast conditions yielded different neural response variations depending on subject-specific CRFs. Future studies could determine more fine-grained contrast ranges on the individual subject level, such that effects of temporal adaptation in dynamical as well as saturating contrast ranges can be investigated. Second, in the current study neural responses were collected with EEG. It is however unclear how CRFs of individual neurons translate to population responses. While in our EEG data analysis we quantify contrast-dependent modulation using a linear fit (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>), previous studies have shown nonlinear IO relationships between visual inputs and neural responses (<xref ref-type="bibr" rid="c10">Chan et al., 2022</xref>). To further investigate how contrast-dependent modulation affects neural responses and how this leads to perceptual experience in humans, other brain measurements could be deployed such as intracranial EEG, which can be measured more locally and whose high-frequency content is more closely related to spiking activity (<xref ref-type="bibr" rid="c48">Miller et al., 2009</xref>; <xref ref-type="bibr" rid="c54">Ray and Maunsell, 2011</xref>) and behavior (<xref ref-type="bibr" rid="c47">Miller et al., 2014</xref>). Lastly, there are several possible extensions to the current modeling framework, including testing on additional tasks (e.g. object occlusion, multiple-object recognition) and computational mechanisms (e.g. power law adaptation,; <xref ref-type="bibr" rid="c60">Sörensen et al. 2023</xref>) to further study the role of various canonical computations and resulting response dynamics in robust object recognition.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Twenty-four subjects participated in this study (age mean ± SD, 21 ± 2 years, 2 males), which was approved by the Ethical Committee of the Psychology Department at the University of Amsterdam. All participants gave written informed consent before participation and were rewarded with study credits or financial compensation (10 euro/h). Two participants were excluded from analysis because of incomplete recordings, and one participant was excluded because of technical issues that arose during the recording session, yielding a total of 21 included participants.</p>
</sec>
<sec id="s4b">
<title>Stimuli</title>
<p>We adapted the object categorization task used in <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref>, who used hand-drawn doodles hidden in a temporally repeated noise pattern. Here, we extended this paradigm by manipulating object contrast, whilst replacing the hand-drawn doodles with digits. Participants were instructed to categorize exemplars of four digit classes (3, 6, 8 and 9) from the MNIST dataset (<xref ref-type="bibr" rid="c42">LeCun et al. 2010</xref>) presented with varying object contrast levels (50%, 60%, 70%, 80% and 90%) (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). The digit was embedded in noise, which consisted of adding a pixelized grey-scale noise pattern which was drawn from a uniform distribution on interval [0, 1) following a pink distribution (such that the power spectral density was inversely proportional to the frequency of the signal). The image consisting of the noise-embedded digit will from here on be referred to as the <italic>test</italic> image. Varying the contrast of the digit resulted in easy (high contrast) and difficult (low contrast) task conditions, while only marginally affecting the overall contrast of the test image, which was mainly determined by the noise pattern (Root Mean Square test image, mean 50% = 0.45, SD = 0.018; mean 60% = 0.46, SD = 0.018; mean 70% = 0.47, SD = 0.019; mean 80% = 0.48, SD = 0.024; mean 90% = 0.49, SD = 0.026). Depending on the trial type (see below), participants were also presented with <italic>adapter</italic> images consisting of noise patterns without embedded digits, as well as uniform grayscale control adapters.</p>
</sec>
<sec id="s4c">
<title>Experimental procedure</title>
<p>Participants completed one EEG recording session lasting approximately 90 minutes, consisting of two blocks with a total of 880 trials. Participants were instructed to fixate on a red cross in the center of a grey screen throughout each trial. Categorization responses were acquired by keyboard presses and inter-trial intervals were sampled from a uniform distribution between 1.25 and 1.75 seconds. After every 50 trials, a break was inserted during which participants were encouraged to take a short rest.</p>
<p>During the first block, participants were presented with three different trial types. Two trial types consisted of the presentation of an adapter (1.5 s), followed by a short grayscale interval and then the test image (0.5 s), with the adapter containing the same noise as that of the subsequent test image or a randomly generated different noise pattern (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>). We refer to these trials where same or different noise was presented prior to the test image as <italic>same-noise</italic> and <italic>different-noise</italic> trials, respectively. The grayscale interval between the adapter and test was fixed (134 ms) and for each trial, a different object exemplar was selected and a unique noise pattern was created, such that all stimuli were trial-unique and there were no repetitions across trials. The third trial type served as a control - referred to as the <italic>blank</italic> trial - which consisted of a blank image followed by the presentation of the test image (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). For the <italic>same-noise</italic>, <italic>different-noise</italic> and <italic>blank</italic> trials eight exemplars per digit were presented for each contrast condition, re-sulting in a total of 480 trials (160 per adapter type, i.e. four targets × five contrast levels × eight exemplars).</p>
<p>In order to conduct a decoding analysis (see below) a second block of trials was presented at the end of the session during which participants categorized the target digits (100% contrast) without any added noise. Here, 100 exemplars were presented for each target digit class, resulting in a total of 400 trials.</p>
</sec>
<sec id="s4d">
<title>EEG acquisition and preprocessing</title>
<p>EEG recordings were made with a Biosemi 64-channel Active TwoEEG system (Biosemi Instrumentation; <ext-link ext-link-type="uri" xlink:href="https://www.biosemi.com">www.biosemi.com</ext-link>), using an extended 10–20 layout modified with two additional occipital electrodes (I1 and I2, while removing electrodes F5 and F6). Eye movements were monitored with electro-oculograms (EOGs). The EEG signal was digitized at 1024 Hz sampling rate. Recording was followed by offline referencing to external electrodes placed on the mastoids. Preprocessing for the purpose of computing event-related responses (ERPs) was done in Python, MNE (<xref ref-type="bibr" rid="c26">Gramfort et al., 2013</xref>), following standard pipelines in our lab (e.g., (<xref ref-type="bibr" rid="c29">Groen et al. 2012</xref>, see preprocessing createEpochs.ipynb): a high-pass filter at 0.01 Hz (6 dB/octave); a low-pass filter at 30 Hz (6 dB/octave) (Butterworth bandpass zero-phase, two-pass forward and reverse, non-causal filter; filter order 16, effective, after forward-backward; cutoffs at 0.01, 30.00 Hz: −6.02, −6.02 dB); a notch filter (zero-phase) at 50 Hz; epoch segmentation in −100 to 500 ms from stimulus onset of the test image; baseline correction between −100 and 0 ms; ocular correction using the EOG electrodes (<xref ref-type="bibr" rid="c27">Gratton et al., 1983</xref>); conversion to Current Source Density responses (<xref ref-type="bibr" rid="c52">Perrin et al., 1987</xref>).</p>
<p>After computing epochs for each of the 880 trials per subject, we removed epochs containing artifacts for individual electrodes and subjects (preprocessing epochSelection.py) as follows: We first computed the maximum response in the [-0.1, 0.5] time-window (voltage time courses were first full-wave rectified), after which the standard deviation (SD) of these maximum values over all trials was computed. Trials were excluded from analysis if the maximum response was <italic>&gt;</italic> 2 SD. Based on the selection methods described above, on average 6.7% (min: 4.4%, max: 8.2%) of the epochs were excluded.</p>
</sec>
<sec id="s4e">
<title>Data analysis</title>
<sec id="s4e1">
<title>Statistical testing for behavioral data</title>
<p>Categorization accuracy and reaction times were computed separately for each subject and trial-type, and subsequently fitted with linear mixed effects models (LMMs) using the <italic>lme4</italic> package implemented in R (<xref ref-type="bibr" rid="c6">Bates and DebRoy, 2004</xref>) (statistics lmer.R). We choose an LLM approach because humans are known to exhibit individual differences in contrast sensitivity (<xref ref-type="bibr" rid="c4">Baker and Graf, 2009</xref>), resulting in substantial variation in categorization performance across subjects. The model comprised two fixed effects: adapter type (blank, same or different) and contrast level (50%, 60%, 70%, 80% and 90%), and one random effect (subject). The estimated coefficients for each of the fixed effects were evaluated with ANOVAs, and the resulting p-values were corrected for multiple comparisons using Tukey method for comparing three estimates (i.e. the adapter types).</p>
</sec>
<sec id="s4e2">
<title>Averaging procedure for ERP analysis</title>
<p>In total, we obtained 880 single ERPs resulting from presentation of the test image, including 480 obtained from the noise-embedded object trials and 400 obtained from clean object trials. For the ERP analysis responses were averaged over trials as follows: For estimating the effect of adapter type, trials were averaged over exemplars, digit classes and contrast levels, yielding 160 ERPs per adaptation condition. For estimating the effect of contrast level, trials were averaged over exemplars, digit classes and adapter types, yielding 96 ERPs per contrast condition. For estimating the interaction effect between adapter type and contrast, trials were averaged over exemplars and digit classes yielding 32 ERPs per condition.</p>
</sec>
<sec id="s4e3">
<title>Identification of electrodes of interest</title>
<p>We focus our analyses on electrodes recording from early visual areas, including Oz, Iz, O1 and O2, and from higher-order visual areas, including P9 and P10, based on prior literature showing object-related processing/repetition suppression effects at these electrodes (<xref ref-type="bibr" rid="c33">Gruber and Müller, 2002</xref>; <xref ref-type="bibr" rid="c58">Schweinberger and Neumann, 2016</xref>; <xref ref-type="bibr" rid="c38">Johnsdorf et al., 2023</xref>). Moreover, for examining the contrast-dependent modulation, we also inspected parietal-occipital electrodes, including Pz, P1, P2, P3, and P4, which have been implicated contrast-related mechanisms (<xref ref-type="bibr" rid="c17">Di Russo et al., 2001</xref>). For the decoding analysis (see Modeling section below), single ERPs of all electrodes were used, motivated by the fact that this required no <italic>a priori</italic> assumptions regarding the spatial distribution of object-related representations.</p>
</sec>
<sec id="s4e4">
<title>Identification of time windows of interest</title>
<p>To study the effect of adapter types, we first determined which time windows were affected by preceding inputs. We achieved this by subtracting the ERPs of trials with blank adapters from the trials with same- or different-noise adapters. Subsequently, we determined the timepoints for which the noise-blank difference was significantly different from 0 using a one-sample t-test. To investigate the effect of contrast, we selected time windows implicated in early and late visual processing. The early time window included the P100, with an onset of 100-150 ms after stimulus onset, which has been linked to a first wave of image-selective neural activity facilitating core object recognition (<xref ref-type="bibr" rid="c68">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="c18">DiCarlo and Maunsell, 2000</xref>). The late time window was centered on the P300 component (280-320 ms), which is known to reflect the engagement of higher-order cognitive functions, such as decision-making behavior during visual processing (<xref ref-type="bibr" rid="c49">Nazari et al., 2010</xref>; <xref ref-type="bibr" rid="c53">Philiastides and Sajda, 2006</xref>).</p>
</sec>
<sec id="s4e5">
<title>ERP decoding analysis</title>
<p>To examine how adaptation and object contrast jointly affect representations of object classes in noise in EEG responses, we ran a series of decoding analyses, which consisted of training a linear classifier to predict the presented digit (i.e. 3, 6, 8 or 9) based on the spatially-distributed ERP responses. We performed a subject-wise decoding analysis separately for the ERPs obtained by presenting either clean or noise-embedded test images (<ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure8AB.ipynb">mkFigure8AB.ipynb</ext-link>). We used the Python sklearn function <monospace>sklearn</monospace>.<monospace>svm</monospace>.<monospace>SVC()</monospace>, with default parameters, which is able to handle binary as well as multi-class classification. To reduce computation time, ERP signals were first downsampled to 128 Hz. To increase the strength of the signal compared to the background noise a sliding window of 62.5 ms was used. For the trials presenting the noise-embedded test images (480 in total), a k-fold cross-validation was applied with 15 folds during which each ERP was used for inference once, with 448 images used for training and 32 images used for testing per fold. Here, the images used for testing within each fold belonged to one contrast level and one adapter type (e.g. same-noise adapter for an object contrast of 50%). For clean test images, the classifier was trained on 360 trials and tested on 40 trials. Similarly to the trials presenting the noise-embedded objects, a k-fold cross-validation was performed with 10 folds such that each trial was used once during inference.</p>
</sec>
</sec>
<sec id="s4f">
<title>DCNN modeling</title>
<p>DCNN models were trained on the same object recognition task as the human participants, using stimulus sequencies containing both same and different noise images preceding the test image. We incorporated temporal dynamics into the networks by feeding the activity from previous timesteps, with one timestep, <italic>t</italic>, defined as one feedforward pass through the network, during which a single image is processed.</p>
</sec>
<sec id="s4g">
<title>Stimuli</title>
<p>In the original paradigm by (<xref ref-type="bibr" rid="c70">Vinken et al., 2020</xref>), a training sample consisted of three timesteps, during which the networks were fed an adapter image (<italic>t</italic><sub>1</sub>), followed by a gray scale image (<italic>t</italic><sub>2</sub>, pixel values of 0.5) and a test image (<italic>t</italic><sub>3</sub>), with input size of 1 × 28 × 28 (channels × height 235 × width) for all timesteps. Since the adapter and test image are here each presented for only a single timestep, this set-up assumes no accumulation of adaptation within the presentation of adapter itself, which we found to reduce the model’s ability to capture the behavioral performance of the human participants in our experiment. To address this, we also presented networks with longer input sequences, both during model training and testing, using a 3:1 adapter-to-test duration ratio which mirrored the temporal structure of the visual stimuli shown to human participants (1500:500 ms, adapter:test). Sequence length was varied such that the test image was presented for a minimum of two timesteps and included sequences of adapter (<italic>A</italic>), blank (<italic>B</italic>) and test (<italic>T</italic>) of either 9 (<italic>AAAAAABTT</italic>), 13 (<italic>AAAAAAAAABTTT</italic>), 17 (<italic><named-content content-type="sequence">AAAAAAAAAAAA</named-content>BTTTT</italic>) and 21 (<italic><named-content content-type="sequence">AAAAAAAAAAAAAAA</named-content>BTTTTT</italic>) images. For all sequences, pixel values were clamped to the range [0, 1] to reduce numerical complexity and increase computational efficiency.</p>
</sec>
<sec id="s4h">
<title>Neural network training</title>
<p>All networks were implemented using Pytorch (<xref ref-type="bibr" rid="c51">Paszke et al., 2019</xref>) with standard convolutions and linear layers. Activations from the last time step were used for the classification, computation of the loss and weight updates. The backbone of all DCNNs consisted of three convolutional layers, one fully connected and a readout layer (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>, for the implementation see cnn feedforward.py). We define a linear response <bold>L</bold><italic><sub>n</sub></italic>(<italic>t</italic>) at time <italic>t</italic> as the output of a convolutional layer <italic>n</italic>:
<disp-formula id="eqn1">
<graphic xlink:href="605075v4_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
given the unit’s current input <bold>x</bold><italic><sub>n</sub></italic><sub>−1</sub>(<italic>t</italic>), bottom-up convolutional weights <bold>W</bold><italic><sub>n</sub></italic> and biases <bold>b</bold><italic><sub>n</sub></italic>, whereby the convolution operation is represented as ∗. We trained five different model architectures. For the baseline model, no adaptation mechanism was added such that all images were processed independently over time. We then implemented and compared three different adaptation mechanisms (see below) in the model convolutional layers, such that responses on a given time step were modulated by responses on the previous time steps. Corresponding parameters were implemented per layer and were optimized simultaneously with regular DCNN parameters. Networks were trained using noise-embedded digits where the contrast was randomly varied between [0.1, 1] for 5 epochs (with 60000 images per epoch). To assess the robustness of our findings (<xref ref-type="bibr" rid="c46">Mehrer et al., 2020</xref>), 5 instances of DCNNs were trained from random initializations for each training length, resulting in a total of 20 trained models (4 model architectures x 5 initializations). Models were trained using the Adam optimizer, a learning rate of 0.001, the cross entropy using stochastic gradient descent with a batch size of 100. After the last convolution, 50% dropout was applied. Code used for model training can be found at model train.py. An overview of the number of parameters for each network architecture and dataset is provided in <xref rid="tbls1" ref-type="table">Supplementary Table 1</xref> and the temporal adaptation parameter values obtained after training are reported in <xref rid="figs1" ref-type="fig">Supplementary Figure 1</xref>.</p>
</sec>
<sec id="s4i">
<title>Temporal adaptation mechanisms</title>
<sec id="s4i1">
<title>Additive suppression</title>
<p>We implemented an additive suppression mechanism that emerges from intrinsic properties of individual units (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>, <italic>left</italic>), as introduced previously by <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref>. Here, each unit <italic>i</italic> in the network has an exponentially decaying adaptation state, <italic>s<sub>t</sub></italic>, which is updated at each time step <italic>t</italic>, based on its previous state <italic>s</italic>(<italic>t</italic> − 1) and the previous response <italic>r</italic>(<italic>t</italic> − 1) as follows:
<disp-formula id="eqn2">
<graphic xlink:href="605075v4_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>α</italic> is a constant determining the time scale of the decay. This adaptation state is then subtracted from the linear response before applying the activation function <italic>ϕ</italic>.
<disp-formula id="eqn3">
<graphic xlink:href="605075v4_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>β</italic> is a constant scaling the amount of suppression. For <italic>β &gt;</italic> 0, these updating rules result in an exponentially decaying response for constant input that recovers in case of no input.</p>
</sec>
<sec id="s4i2">
<title>Recurrent interactions</title>
<p>In addition to temporal adaptation arising from intrinsic biophysical mechanisms, temporal adaptation phenomena have also been proposed to be the result of recurrent interactions. Here, we incorporated temporal adaptation via lateral recurrence using a method adapted from <xref ref-type="bibr" rid="c70">Vinken et al. (2020)</xref>, inspired by computational models which implemented adaptation by changing recurrent interactions between orientation tuned channels (<xref ref-type="bibr" rid="c22">Felsen et al., 2002</xref>; <xref ref-type="bibr" rid="c66">Teich and Qian, 2003</xref>; <xref ref-type="bibr" rid="c72">Westrick et al., 2016</xref>):
<disp-formula id="eqn4">
<graphic xlink:href="605075v4_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="605075v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are the lateral weights (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>, <italic>right</italic>) consisting of 32 kernels of size 1 × 1 × 32 (stride = 1).</p>
</sec>
<sec id="s4i3">
<title>Divisive normalization</title>
<p>Divisive normalization was originally proposed by <xref ref-type="bibr" rid="c34">Heeger (1992)</xref> and has been studied extensively in the spatial domain, where the response <italic>r<sub>i</sub></italic> of neuron <italic>i</italic> is modelled as:
<disp-formula id="eqn5">
<graphic xlink:href="605075v4_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>K</italic> determines the maximum attainable response, <italic>n</italic> is an exponent, <italic>σ</italic> a semi-saturation constant and <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="605075v4_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula> the sum of the activations of neighbouring neurons. Here, we transition to the temporal domain and implement a mathematical framework originally formulated in <xref ref-type="bibr" rid="c34">Heeger (1992</xref>, <xref ref-type="bibr" rid="c35">1993</xref>), which defines a neuron’s response <italic>r<sub>i</sub></italic> recursively over time (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>). For each unit <italic>i</italic> in the network the response at timepoint <italic>t</italic> is updated before applying the rectifier activation function <italic>ϕ</italic> such that:
<disp-formula id="eqn6">
<graphic xlink:href="605075v4_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>G</italic>(<italic>t</italic> − 1) is a temporal feedback signal from the previous time step which is updated based on its previous state and the current response:
<disp-formula id="eqn7">
<graphic xlink:href="605075v4_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>α</italic> determines the time scale of the feedback signal. This multiplicative feedback signal results in divisive suppression (for details, see <xref ref-type="bibr" rid="c35">Heeger 1993</xref>, Appendix A).</p>
</sec>
</sec>
<sec id="s4j">
<title>DCNN decoding analysis</title>
<p>To measure the amount of explicit information about the object present in DCNN activations and to compare with the results obtained from object decoding based on ERP responses, we also performed a decoding analysis on the DCNN unit activations of the last timestep (presentation of the test image) separately for each layer and contrast level. We applied a five-fold cross-validation, where each classifier was trained with 800 images (using the same type of classifier as was used for the decoding analysis of the ERPs, i.e. <monospace>sklearn</monospace>.<monospace>svm</monospace>.<monospace>SVC()</monospace>) and tested on 200 images (all randomly drawn from the test set). This procedure was repeated five times after which accuracies were averaged over DCNN instances.</p>
</sec>
</sec>
</body>
<back>
<sec id="s6" sec-type="supplementary">
<title>Supplementary information</title>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Supplementary Table 1:</label>
<caption><title>Number of trainable parameters per model.</title><p>Shown are the number of parameters for a feedforward model without (baseline) and with a temporal adaptation, arising from intrinsic (int) or recurrent (rec) mechanisms.</p></caption>
<graphic xlink:href="605075v4_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1:</label>
<caption><title>Temporal adaptation parameters for DCNNs with additive suppression and divisive normalization.</title>
<p><bold>A</bold>: Trained parameter values for each convolutional layer for DCNNs with additive suppression, including <italic>α</italic> and <italic>β</italic>. Each row depicts a different sequence length and error bars depict SEM across network initializations (<italic>n</italic> = 5). <bold>B</bold>: Same as panel A for DCNNs with divisive normalization, including the temporal adaptation parameters <italic>α</italic>, <italic>K</italic> and <italic>σ</italic>. <bold>C</bold>: Average parameter values for DCNNs with additive suppression averaged across the convolutional layers. Errors bars depict SEM across network initializations (<italic>n</italic> = 5). <bold>D</bold>: Same as panel C for DCNNs with divisive normalization. This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkSFigure1.py">mkSFigure1.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 2:</label>
<caption><title>Object-related representations in the neural data per object contrast.</title>
<p><bold>A</bold>: Average decoding accuracy for predicting the presented object class based on evoked potentials for test images varying in object contrast level. Decoding accuracy’s are averaged for [0, 1]s time window after stimulus onset. <bold>B</bold>: Decoding accuracy for the different object contrast levels, with the number of time points for which decoding accuracy was significantly different from chance level (i.e. 25%) noted on the top left. Shaded regions depict the SEM across subjects. This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure7_SFig2.ipynb">mkFigure7_SFig2.ipynb</ext-link>.</p></caption>
<graphic xlink:href="605075v4_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 3:</label>
<caption><title>Model activations.</title>
<p><bold>A</bold>: Activations for the first convolutional layer for the test set for same (blue) and different (yellow) noise adapters for the first DCNN initialization without a temporal adaptation mechanism. Results are shown for three different object contrast levels (%), including 10, 50, 90. <bold>B-D</bold>: Same as panel A for a DCNN initialization endowed with a temporal adaptation mechanism, including additive suppression (B), lateral recurrence (C) and divisive normalization (D). This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkSFigure3-7.py">mkSFigure3-7.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 4:</label>
<caption><title>Model activations.</title>
<p><bold>A</bold>: Activations for the first convolutional layer for the test set for same (blue) and different (yellow) noise adapters for the second DCNN initialization without a temporal adaptation mechanism. Results are shown for three different object contrast levels (%), including 10, 50, 90. <bold>B-D</bold>: Same as panel A for a DCNN initialization endowed with a temporal adaptation mechanism, including additive suppression (B), lateral recurrence (C) and divisive normalization (D). This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkSFigure3-7.py">mkSFigure3-7.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 5:</label>
<caption><title>Model activations.</title>
<p><bold>A</bold>: Activations for the first convolutional layer for the test set for same (blue) and different (yellow) noise adapters for the third DCNN initialization without a temporal adaptation mechanism. Results are shown for three different object contrast levels (%), including 10, 50, 90. <bold>B-D</bold>: Same as panel A for a DCNN initialization endowed with a temporal adaptation mechanism, including additive suppression (B), lateral recurrence (C) and divisive normalization (D). This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkSFigure3-7.py">mkSFigure3-7.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 6:</label>
<caption><title>Model activations.</title>
<p><bold>A</bold>: Activations for the first convolutional layer for the test set for same (blue) and different (yellow) noise adapters for the fourth DCNN initialization without a temporal adaptation mechanism. Results are shown for three different object contrast levels (%), including 10, 50, 90. <bold>B-D</bold>: Same as panel A for a DCNN initialization endowed with a temporal adaptation mechanism, including additive suppression (B), lateral recurrence (C) and divisive normalization (D). This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkSFigure3-7.py">mkSFigure3-7.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 7:</label>
<caption><title>Model activations.</title>
<p><bold>A</bold>: Activations for the first convolutional layer for the test set for same (blue) and different (yellow) noise adapters for the fifth DCNN initialization without a temporal adaptation mechanism. Results are shown for three different object contrast levels (%), including 10, 50, 90. <bold>B-D</bold>: Same as panel A for a DCNN initialization endowed with a temporal adaptation mechanism, including additive suppression (B), lateral recurrence (C) and divisive normalization (D). This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkSFigure3-7.py">mkSFigure3-7.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 8:</label>
<caption><title>Decoding accuracy for the convolutional layers across adapter types.</title>
<p><bold>A</bold>: Decoding accuracy for the first convolutional layer for the test set for same (blue) and different (yellow) noise adapters for DCNNs endowed with one of three temporal adaptation mechanisms (from left to right): additive suppression, lateral recurrence and divisive normalization. Input sequences varied in length, including short (<italic>ABT</italic>) and long (<italic><named-content content-type="sequence">AAAAAAAAAAAAAAA</named-content>BTTTTT</italic>) sequences. <bold>B-C</bold>: Same as panel A for the second (B) and third (C) convolutional layer.This figure can be reproduced by <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG/blob/master/mkFigure8E_SFig8.py">mkFigure8E_SFig8.py</ext-link>.</p></caption>
<graphic xlink:href="605075v4_figs8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>The EEG data, behavioral responses, and stimuli are openly available on <ext-link ext-link-type="uri" xlink:href="https://osf.io/ukqhg/">https://osf.io/ukqhg/</ext-link>. All code used for the purpose of this paper can be found at the GitHub repository <ext-link ext-link-type="uri" xlink:href="https://github.com/ABra1993/tAdaptation_EEG.git">https://github.com/ABra1993/tAdaptation_EEG.git</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by a MacGillavry Fellowship to IIAG.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akyürek</surname>, <given-names>E. G.</given-names></string-name></person-group> (<year>2025</year>). <article-title>Temporal integration as an adaptive process in visual perception, attention, and working memory</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, page <fpage>106041</fpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Albrecht</surname>, <given-names>D. G.</given-names></string-name>, <string-name><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name>, <string-name><surname>Frazor</surname>, <given-names>R. A.</given-names></string-name>, and <string-name><surname>Crane</surname>, <given-names>A. M</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Visual cortex neurons of monkeys and cats: temporal dynamics of the contrast response function</article-title>. <source>Journal of neurophysiology</source>, <volume>88</volume>(<issue>2</issue>):<fpage>888</fpage>–<lpage>913</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Albrecht</surname>, <given-names>D. G.</given-names></string-name> and <string-name><surname>Hamilton</surname>, <given-names>D. B</given-names></string-name></person-group>. (<year>1982</year>). <article-title>Striate cortex of monkey and cat: contrast response function</article-title>. <source>Journal of neurophysiology</source>, <volume>48</volume>(<issue>1</issue>):<fpage>217</fpage>–<lpage>237</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baker</surname>, <given-names>D. H.</given-names></string-name> and <string-name><surname>Graf</surname>, <given-names>E. W</given-names></string-name></person-group>. (<year>2009</year>). <article-title>On the relation between dichoptic masking and binocular rivalry</article-title>. <source>Vision research</source>, <volume>49</volume>(<issue>4</issue>):<fpage>451</fpage>–<lpage>459</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barlow</surname>, <given-names>H</given-names></string-name></person-group>. (<year>1993</year>). <article-title>A theory about the functional role and synaptic</article-title>. <source>Vision: Coding and efficiency, page</source> <volume>363</volume>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bates</surname>, <given-names>D. M.</given-names></string-name> and <string-name><surname>DebRoy</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Linear mixed models and penalized least squares</article-title>. <source>Journal of Multi-variate Analysis</source>, <volume>91</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brands</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Devore</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Devinsky</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Doyle</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Flinker</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Friedman</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Dugan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Groen</surname>, <given-names>I. I. A</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Temporal dynamics of short-term neural adaptation across human visual cortex</article-title>. <source>PLOS Computational Biology</source>, <volume>20</volume>(<issue>5</issue>):<fpage>e1012161</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Pokorny</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Rod and cone contrast gains derived from reaction time distribution modeling</article-title>. <source>Journal of vision</source>, <volume>10</volume>(<issue>2</issue>):<fpage>11</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Normalization as a canonical neural computation</article-title>. <source>Nature reviews neuroscience</source>, <volume>13</volume>(<issue>1</issue>):<fpage>51</fpage>–<lpage>62</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chan</surname>, <given-names>Y. M.</given-names></string-name>, <string-name><surname>Glarin</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Moffat</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Bode</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>McKendrick</surname>, <given-names>A. M</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Relating the cortical visual contrast gain response to spectroscopy-measured excitatory and inhibitory metabolites in people who experience migraine</article-title>. <source>Plos one</source>, <volume>17</volume>(<issue>4</issue>):<fpage>e0266130</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chaumon</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Busch</surname>, <given-names>N. A</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Prestimulus neural oscillations inhibit visual perception via modulation of response gain</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>26</volume>(<issue>11</issue>):<fpage>2514</fpage>–<lpage>2529</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clifford</surname>, <given-names>C. W.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Stanley</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Kohn</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sharpee</surname>, <given-names>T. O.</given-names></string-name>, and <string-name><surname>Schwartz</surname>, <given-names>O</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Visual adaptation: Neural, psychological and computational aspects</article-title>. <source>Vision research</source>, <volume>47</volume>(<issue>25</issue>):<fpage>3125</fpage>–<lpage>3131</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Contini</surname>, <given-names>E. W.</given-names></string-name>, <string-name><surname>Wardle</surname>, <given-names>S. G.</given-names></string-name>, and <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions</article-title>. <source>Neuropsychologia</source>, <volume>105</volume>:<fpage>165</fpage>–<lpage>176</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dao</surname>, <given-names>D. Y.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>Z.-L.</given-names></string-name>, and <string-name><surname>Dosher</surname>, <given-names>B. A</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Adaptation to sine-wave gratings selectively reduces the contrast gain of the adapted stimuli</article-title>. <source>Journal of Vision</source>, <volume>6</volume>(<issue>7</issue>):<fpage>6</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>del Mar Quiroga</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Morris</surname>, <given-names>A. P.</given-names></string-name>, and <string-name><surname>Krekelberg</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Adaptation without plasticity</article-title>. <source>Cell reports</source>, <volume>17</volume>(<issue>1</issue>):<fpage>58</fpage>–<lpage>68</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desimone</surname>, <given-names>R</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Neural mechanisms for visual memory and their role in attention</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>93</volume>(<issue>24</issue>):<fpage>13494</fpage>–<lpage>13499</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Russo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Spinelli</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Morrone</surname>, <given-names>M. C.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Automatic gain control contrast mechanisms are modulated by attention in humans: evidence from visual evoked potentials</article-title>. <source>Vision research</source>, <volume>41</volume>(<issue>19</issue>):<fpage>2435</fpage>– <lpage>2447</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name> and <string-name><surname>Maunsell</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>2000</year>). <article-title>Form representation in monkey inferotemporal cortex is virtually unaltered by free viewing</article-title>. <source>Nature neuroscience</source>, <volume>3</volume>(<issue>8</issue>):<fpage>814</fpage>–<lpage>821</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sommers</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Seeliger</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ismael</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lindsay</surname>, <given-names>G. W.</given-names></string-name>, <string-name><surname>Kording</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Van Gerven</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2023</year>). <article-title>The neuroconnectionist research programme</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>24</volume>(<issue>7</issue>):<fpage>431</fpage>–<lpage>450</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engell</surname>, <given-names>A. D.</given-names></string-name> and <string-name><surname>McCarthy</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Repetition suppression of face-selective evoked and induced eeg recorded from human cortex</article-title>. <source>Human brain mapping</source>, <volume>35</volume>(<issue>8</issue>):<fpage>4155</fpage>–<lpage>4162</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felleman</surname>, <given-names>D. J.</given-names></string-name> and <string-name><surname>Van Essen</surname>, <given-names>D. C</given-names></string-name></person-group>. (<year>1991</year>). <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cerebral cortex (New York, NY: 1991)</source>, <volume>1</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felsen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>Y.-s.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Spor</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Dynamic modification of cortical orientation tuning mediated by recurrent connections</article-title>. <source>Neuron</source>, <volume>36</volume>(<issue>5</issue>):<fpage>945</fpage>–<lpage>954</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garrido</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Kilner</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Kiebel</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Stephan</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Baldeweg</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Friston</surname>, <given-names>K. J</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Repetition suppression and plasticity in the human brain</article-title>. <source>Neuroimage</source>, <volume>48</volume>(<issue>1</issue>):<fpage>269</fpage>–<lpage>279</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Geirhos</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Rubisch</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Michaelis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name>, and <string-name><surname>Brendel</surname>, <given-names>W</given-names></string-name></person-group>. (<year>2018a</year>). <article-title>Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geirhos</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Temme</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Rauber</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schütt</surname>, <given-names>H. H.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name></person-group> (<year>2018b</year>). <article-title>Generalisation in humans and deep neural networks</article-title>. <source>Advances in neural information processing systems</source>, <volume>31</volume>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Luessi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Larson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Strohmeier</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Goj</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brooks</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Parkkonen</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Hämäläinen</surname>, <given-names>M. S.</given-names></string-name></person-group> (<year>2013</year>). <article-title>MEG and EEG data analysis with MNE-Python</article-title>. <source>Frontiers in Neuroscience</source>, <volume>7</volume>(<issue>267</issue>):<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gratton</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Coles</surname>, <given-names>M. G.</given-names></string-name>, and <string-name><surname>Donchin</surname>, <given-names>E</given-names></string-name></person-group>. (<year>1983</year>). <article-title>A new method for off-line removal of ocular artifact</article-title>. <source>Electroencephalography and clinical neurophysiology</source>, <volume>55</volume>(<issue>4</issue>):<fpage>468</fpage>–<lpage>484</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Henson</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Martin</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Repetition and the brain: neural models of stimulus-specific effects</article-title>. <source>Trends in cognitive sciences</source>, <volume>10</volume>(<issue>1</issue>):<fpage>14</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Groen</surname>, <given-names>I. I.</given-names></string-name>, <string-name><surname>Ghebreab</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lamme</surname>, <given-names>V. A.</given-names></string-name>, and <string-name><surname>Scholte</surname>, <given-names>H. S.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Spatially pooled contrast responses predict neural and perceptual similarity of naturalistic image categories</article-title>. <source>PLoS Comput Biol</source> <volume>8</volume>:<elocation-id>e1002726</elocation-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Groen</surname>, <given-names>I. I.</given-names></string-name>, <string-name><surname>Ghebreab</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Prins</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lamme</surname>, <given-names>V. A.</given-names></string-name>, and <string-name><surname>Scholte</surname>, <given-names>H. S</given-names></string-name></person-group>. (<year>2013</year>). <article-title>From image statistics to scene gist: evoked neural activity reveals transition from low-level natural image structure to scene category</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>48</issue>):<fpage>18814</fpage>–<lpage>18824</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Groen</surname>, <given-names>I. I.</given-names></string-name>, <string-name><surname>Piantoni</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Montenegro</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Flinker</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Devore</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Devinsky</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Doyle</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Dugan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Friedman</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ramsey</surname>, <given-names>N. F.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2022</year>). <article-title>Temporal dynamics of neural responses in human visual cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>42</volume>(<issue>40</issue>):<fpage>7562</fpage>–<lpage>7580</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>A. K.</given-names></string-name>, and <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The representational dynamics of visual objects in rapid serial visual processing streams</article-title>. <source>NeuroImage</source>, <volume>188</volume>:<fpage>668</fpage>–<lpage>679</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gruber</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Müller</surname>, <given-names>M. M.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Effects of picture repetition on induced gamma band responses, evoked potentials, and phase synchrony in the human eeg</article-title>. <source>Cognitive Brain Research</source>, <volume>13</volume>(<issue>3</issue>):<fpage>377</fpage>–<lpage>392</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>1992</year>). <article-title>Normalization of cell responses in cat striate cortex</article-title>. <source>Visual neuroscience</source>, <volume>9</volume>(<issue>2</issue>):<fpage>181</fpage>– <lpage>197</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name></person-group>. (<year>1993</year>). <article-title>Modeling simple-cell direction selectivity with normalized, half-squared, linear operators</article-title>. <source>Journal of neurophysiology</source>, <volume>70</volume>(<issue>5</issue>):<fpage>1885</fpage>–<lpage>1898</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henson</surname>, <given-names>R. N</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Neuroimaging studies of priming</article-title>. <source>Progress in neurobiology</source>, <volume>70</volume>(<issue>1</issue>):<fpage>53</fpage>–<lpage>81</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Jayakumar</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Czarnecki</surname>, <given-names>W. M.</given-names></string-name>, <string-name><surname>Menick</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schwarz</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rae</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Osindero</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Teh</surname>, <given-names>Y. W.</given-names></string-name>, <string-name><surname>Harley</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Pascanu</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2020</year>). <source>Multiplicative interactions and where to find them</source>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnsdorf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kisker</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gruber</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Schöne</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Comparing encoding mechanisms in realistic virtual reality and conventional 2d laboratory settings: Event-related potentials in a repetition suppression paradigm</article-title>. <source>Frontiers in Psychology</source>, <volume>14</volume>:<fpage>1051938</fpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaliukhovich</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>De Baene</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Vogels</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Effect of adaptation on object representation accuracy in macaque inferior temporal cortex</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>25</volume>(<issue>5</issue>):<fpage>777</fpage>–<lpage>789</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kohn</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Visual adaptation: physiology, mechanisms, and functional benefits</article-title>. <source>Journal of neurophysiology</source>, <volume>97</volume>(<issue>5</issue>):<fpage>3155</fpage>–<lpage>3164</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Alvarez</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Cognitive steering in deep neural networks via long-range modulatory feedback connections</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="data"><person-group person-group-type="author"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Cortes</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Burges</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Mnist handwritten digit database</article-title>. <source>ATT Labs</source>: <ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist">http://yann.lecun.com/exdb/mnist</ext-link>, <fpage>2</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lindsay</surname>, <given-names>G. W.</given-names></string-name>, <string-name><surname>Mrsic-Flogel</surname>, <given-names>T. D.</given-names></string-name>, and <string-name><surname>Sahani</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Bio-inspired neural networks implement different recurrent visual processing strategies than task-trained ones do</article-title>. <source>bioRxiv</source></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Qian</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Zhang</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Layer-dependent multiplicative effects of spatial attention on contrast responses in human early visual cortex</article-title>. <source>Progress in Neurobiology</source>, <volume>207</volume>:<fpage>101897</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markov</surname>, <given-names>N. T.</given-names></string-name>, <string-name><surname>Vezoli</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chameau</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Falchier</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Quilodran</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Huissoud</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lamy</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Misery</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Giroud</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ullman</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2014</year>). <article-title>Anatomy of hierarchy: feedforward and feedback pathways in macaque visual cortex</article-title>. <source>Journal of comparative neurology</source>, <volume>522</volume>(<issue>1</issue>):<fpage>225</fpage>–<lpage>259</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mehrer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Spoerer</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Kietzmann</surname>, <given-names>T. C</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Individual differences among deep neural network models</article-title>. <source>Nature communications</source>, <volume>11</volume>(<issue>1</issue>):<fpage>5725</fpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Honey</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Hermes</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Rao</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Ojemann</surname>, <given-names>J. G.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2014</year>). <article-title>Broadband changes in the cortical surface potential track activation of functionally diverse neuronal populations</article-title>. <source>Neuroimage</source>, <volume>85</volume>:<fpage>711</fpage>–<lpage>720</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Sorensen</surname>, <given-names>L. B.</given-names></string-name>, <string-name><surname>Ojemann</surname>, <given-names>J. G.</given-names></string-name>, and <string-name><surname>Den Nijs</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Power-law scaling in the brain surface electric potential</article-title>. <source>PLoS computational biology</source>, <volume>5</volume>(<issue>12</issue>):<fpage>e1000609</fpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nazari</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Wallois</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Aarabi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nosratabadi</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Berquin</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2010</year>). <article-title>P300 component modulation during a go/nogo task in healthy children</article-title>. <source>Basic and Clinical Neuroscience</source>, <volume>2</volume>(<issue>1</issue>):<fpage>31</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ohzawa</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Sclar</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Freeman</surname>, <given-names>R. D</given-names></string-name></person-group>. (<year>1985</year>). <article-title>Contrast gain control in the cat’s visual system</article-title>. <source>Journal of neurophysiology</source>, <volume>54</volume>(<issue>3</issue>):<fpage>651</fpage>–<lpage>667</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paszke</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Massa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lerer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bradbury</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chanan</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Killeen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Gimelshein</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Antiga</surname>, <given-names>L.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2019</year>). <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>. <source>Advances in neural information processing systems</source>, <volume>32</volume>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrin</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Pernier</surname>, <given-names>J</given-names></string-name></person-group>. (<year>1987</year>). <article-title>Scalp current density mapping: value and estimation from potential data</article-title>. <source>IEEE Transactions on biomedical engineering</source>, (<issue>4</issue>):<fpage>283</fpage>–<lpage>288</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Philiastides</surname>, <given-names>M. G.</given-names></string-name> and <string-name><surname>Sajda</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Temporal characterization of the neural correlates of perceptual decision making in the human brain</article-title>. <source>Cerebral cortex</source>, <volume>16</volume>(<issue>4</issue>):<fpage>509</fpage>–<lpage>518</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ray</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Maunsell</surname>, <given-names>J. H</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title>. <source>PLoS biology</source>, <volume>9</volume>(<issue>4</issue>):<fpage>e1000610</fpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sawamura</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Orban</surname>, <given-names>G. A.</given-names></string-name>, and <string-name><surname>Vogels</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Selectivity of neuronal adaptation does not match response selectivity: a single-cell study of the fmri adaptation paradigm</article-title>. <source>Neuron</source>, <volume>49</volume>(<issue>2</issue>):<fpage>307</fpage>–<lpage>318</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schacter</surname>, <given-names>D. L.</given-names></string-name> and <string-name><surname>Buckner</surname>, <given-names>R. L</given-names></string-name></person-group>. (<year>1998</year>). <article-title>Priming and the brain</article-title>. <source>Neuron</source>, <volume>20</volume>(<issue>2</issue>):<fpage>185</fpage>–<lpage>195</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schadow</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lenz</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Thaerig</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Busch</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Fründ</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Rieger</surname>, <given-names>J. W.</given-names></string-name>, and <string-name><surname>Herrmann</surname>, <given-names>C. S.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Stimulus intensity affects early sensory processing: visual contrast modulates evoked gamma-band activity in human eeg</article-title>. <source>International Journal of Psychophysiology</source>, <volume>66</volume>(<issue>1</issue>):<fpage>28</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schweinberger</surname>, <given-names>S. R.</given-names></string-name> and <string-name><surname>Neumann</surname>, <given-names>M. F</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Repetition effects in human erps to faces</article-title>. <source>Cortex</source>, <volume>80</volume>:<fpage>141</fpage>–<lpage>153</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Solomon</surname>, <given-names>S. G.</given-names></string-name> and <string-name><surname>Kohn</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Moving sensory adaptation beyond suppressive effects in single neurons</article-title>. <source>Current biology</source>, <volume>24</volume>(<issue>20</issue>):<fpage>R1012</fpage>–<lpage>R1022</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sörensen</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Bohté</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>De Jong</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Slagter</surname>, <given-names>H. A.</given-names></string-name>, and <string-name><surname>Scholte</surname>, <given-names>H. S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Mechanisms of human dynamic object recognition revealed by sequential deep neural networks</article-title>. <source>PLOS Computational Biology</source>, <volume>19</volume>(<issue>6</issue>):<fpage>e1011169</fpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spoerer</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>McClure</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Recurrent convolutional neural networks: a better model of biological object recognition</article-title>. <source>Frontiers in psychology</source>, <volume>8</volume>:<fpage>278016</fpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name> and <string-name><surname>Zwi</surname>, <given-names>J. D</given-names></string-name></person-group>. (<year>2004</year>). <article-title>The small world of the cerebral cortex</article-title>. <source>Neuroinformatics</source>, <volume>2</volume>:<fpage>145</fpage>–<lpage>162</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wyart</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Johnen</surname>, <given-names>V. M.</given-names></string-name>, and <string-name><surname>De Gardelle</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Human scalp electroencephalography reveals that repetition suppression varies with expectation</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>5</volume>:<fpage>67</fpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Szegedy</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zaremba</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Bruna</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Erhan</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Goodfellow</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Fergus</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Intriguing properties of neural networks</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Schrimpf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lotter</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Moerman</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Paredes</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ortega Caro</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hardesty</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Cox</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Kreiman</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Recurrent computations for visual pattern completion</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>115</volume>(<issue>35</issue>):<fpage>8835</fpage>–<lpage>8840</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teich</surname>, <given-names>A. F.</given-names></string-name> and <string-name><surname>Qian</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Learning and adaptation in a recurrent model of v1 orientation selectivity</article-title>. <source>Journal of Neurophysiology</source>, <volume>89</volume>(<issue>4</issue>):<fpage>2086</fpage>–<lpage>2100</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Thorat</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Aldegheri</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Kietzmann</surname>, <given-names>T. C</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization</article-title>. <source>arXiv</source>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thorpe</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fize</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Marlot</surname>, <given-names>C</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Speed of processing in the human visual system</article-title>. <source>nature</source>, <volume>381</volume>(<issue>6582</issue>):<fpage>520</fpage>–<lpage>522</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vassilev</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stomonyakov</surname>, <given-names>V.</given-names></string-name>, and <string-name><surname>Manahilov</surname>, <given-names>V</given-names></string-name></person-group>. (<year>1994</year>). <article-title>Spatial-frequency specific contrast gain and flicker masking of human transient vep</article-title>. <source>Vision research</source>, <volume>34</volume>(<issue>7</issue>):<fpage>863</fpage>–<lpage>872</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vinken</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Boix</surname>, <given-names>X.</given-names></string-name>, and <string-name><surname>Kreiman</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Incorporating intrinsic suppression in deep neural networks captures dynamics of adaptation in neurophysiology and perception</article-title>. <source>Science Advances</source>, <volume>6</volume>(<issue>42</issue>):<fpage>eabd4205</fpage>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vogels</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Sources of adaptation of inferior temporal cortical responses</article-title>. <source>Cortex</source>, <volume>80</volume>:<fpage>185</fpage>–<lpage>195</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Westrick</surname>, <given-names>Z. M.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name>, and <string-name><surname>Landy</surname>, <given-names>M. S</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Pattern adaptation and normalization reweighting</article-title>. <source>Journal of Neuroscience</source>, <volume>36</volume>(<issue>38</issue>):<fpage>9805</fpage>–<lpage>9816</lpage>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whitmire</surname>, <given-names>C. J.</given-names></string-name> and <string-name><surname>Stanley</surname>, <given-names>G. B</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Rapid sensory adaptation redux: a circuit perspective</article-title>. <source>Neuron</source>, <volume>92</volume>(<issue>2</issue>):<fpage>298</fpage>–<lpage>315</lpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname>, <given-names>H. R.</given-names></string-name> and <string-name><surname>Humanski</surname>, <given-names>R</given-names></string-name></person-group>. (<year>1993</year>). <article-title>Spatial frequency adaptation and contrast gain control</article-title>. <source>Vision research</source>, <volume>33</volume>(<issue>8</issue>):<fpage>1133</fpage>–<lpage>1149</lpage>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>W.-L.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>G.-T.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Huang</surname>, <given-names>C.-B</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Multi-stage cortical plasticity induced by visual contrast learning</article-title>. <source>Frontiers in Neuroscience</source>, <volume>14</volume>:<fpage>555701</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109155.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gold</surname>
<given-names>Joshua I</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution>
</institution-wrap>
<city>Philadelphia</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study examined how sensory adaptation supports visual perception in the presence of noise. The authors used a combination of human psychophysics, electroencephalography (EEG), and deep neural networks to show that adaptation to noise can improve perception. The results are <bold>solid</bold> but are, at present, weakened by a number of concerns, including some related to the experimental design and some regarding the interpretation of the results in terms of particular mechanisms. With these concerns adequately addressed, the study and conclusions would be likely to be of broad interest to the neuroscience community.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109155.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors sought to investigate the role of adaptation in supporting object recognition. In particular, the extent to which adaptation to noise improves subsequent recognition of objects embedded in the same or similar noise, and how this interacts with target contrast. The authors approach this question using a combination of psychophysics, electroencephalography, and deep neural networks. They find better behavioural performance and multivariate decoding of stimuli preceded by noise, suggesting a beneficial effect of adaptation to noise. The neural network analysis seeks to provide a deeper explanation of the results by comparing how well different adaptation mechanisms capture the empirical behavioural results. The results show that models incorporating intrinsic adaptation mechanisms, such as additive suppression and divisive normalisation, capture the behavioural results better than those that incorporate recurrent interactions. The study has the potential to provide interesting insights into adaptation, but there are alternative (arguably more parsimonious) explanations for the results that have not been refuted (or even recognised) in the manuscript. If these confounds can be compellingly addressed, then I expect the results would be of interest to a broad range of readers.</p>
<p>The study uses a multi-modal approach, which provides a rich characterisation of the phenomenon. The methods are described clearly, and the accompanying code and data are made publicly available. The comparison between univariate and multivariate analyses is interesting, and the application of neural networks to distinguish between different models of adaptation seems quite promising.</p>
<p>There are several concerning confounding factors that need to be addressed before the results can be meaningfully interpreted. In particular, differences in behavioural accuracy may be explained by a simple change detection mechanism in the &quot;same noise&quot; condition, and temporal cuing by the &quot;adaptor&quot; stimulus may explain differences in reaction time. Similarly, interference between event-related potentials may explain the univariate EEG results, and biased decoder training may explain the multivariate results. Thus, it is currently unclear if any of the results reflect adaptation.</p>
<p>My main concerns relate to how adaptation is induced and how differences between conditions are interpreted. The adaptation period is only 1.5 s. Although brief adaptors (~1 s) can produce stimulus history effects, it is unclear whether these reflect the same mechanisms as those observed with standard, longer adaptation durations (e.g., 10-30 s). Prior EEG work on visual adaptation using longer adaptors has shown that feature-specific effects emerge very early (&lt;100 ms) after test onset in both univariate and multivariate responses (Rideaux et al., 2023, PNAS). In contrast, the present study finds no difference between same and different adaptor conditions until much later (&gt;300 ms). These later effects likely reflect cognitive processes such as template matching or decision-making, rather than sensory adaptation. Although early differences appear between blank and adaptor conditions, these could be explained by interactions between ERPs elicited by adaptor onset/offset and those elicited by the test stimulus; therefore, they cannot be attributed to adaptation. This contradicts the statement in the Discussion that &quot;Our EEG measurements show clear evidence of repetition suppression, in the form of reduced responses to the repeated noise pattern early in time.&quot;</p>
<p>A second concern is the brief inter-stimulus interval. The adaptor is shown for 1.5 s, followed by only a 134 ms blank before the target. When the &quot;adaptor&quot; and test noise are identical, improved performance could simply arise from detecting the pixels that change, namely, those forming the target number. Such change detection does not require adaptation; even simple motion detector units would suffice. If the blank period were longer-beyond the temporal window of motion detectors-then improved performance would more convincingly reflect adaptation. Given the very short blank, however, a more parsimonious explanation for the behavioural effect in the same-noise condition is that change detection mechanisms isolate the target.</p>
<p>Differences between the blank and adaptor conditions may also be explained by temporal cueing. In the noise conditions, the noise reliably signals the upcoming target time, whereas the blank condition provides no such cue. Given the variable inter-trial interval and the brief target presentation, this temporal cue would strongly facilitate target perception. This account is consistent with the reaction time results: both adaptor conditions produce faster reaction times than the blank condition, but do not differ from each other.</p>
<p>The decoding analyses are also difficult to interpret, given the training-testing protocol. All trials from the three main conditions (blank, same, different) were used to train the classifier, and then held-out trials - all from one condition-were decoded. Because ERPs in the adaptor conditions differ substantially from those in the blank condition, and because there are twice as many adaptor trials, the classifier is biased toward patterns from the adaptor conditions and will naturally perform worse on blank trials. To compare decoding accuracy meaningfully across conditions, the classifier should be trained on a separate unbiased dataset (e.g., the &quot;clean&quot; data), or each condition should be trained and tested separately using cross-fold validation.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109155.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Neurons adapt to prolonged or repeated sensory inputs. One function of such adaptation may be to save resources to avoid representing the same inputs over and over again. However, it has been hypothesized that adaptation could additionally help improve the representation of sensory stimuli, especially during difficult recognition scenarios. This study sheds light on this question and provides behavioral evidence for such enhancement. The behavioral results are interesting and compelling. The paper also includes scalp electroencephalographic (EEG) data, which are noisy but point toward similar conclusions. The authors finally implement a deep convolutional neural network (DCNN) with adaptation mechanisms, which nicely capture human behavior.</p>
<p>Strengths:</p>
<p>(1) The authors introduce an interesting hypothesis about the role of adaptation in visual recognition.</p>
<p>(2) The authors present interesting and compelling behavioral data consistent with the hypothesis.</p>
<p>(3) The authors introduce a computational model that can capture mechanisms that can lead to adaptation, enhancing visual recognition.</p>
<p>Weaknesses:</p>
<p>(1) The main weakness is the scalp EEG data. As detailed below, the results are minimal at best and do not contribute to understanding the mechanisms of adaptation. The paper would be stronger without the EEG data.</p>
<p>(2) I wonder whether the hypothesis also holds with real-world objects in natural scenes, beyond the confines of MNIST digits.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109155.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Brands and colleagues investigate how temporal adaptation can aid object recognition, and what neural computations may underlie these effects. They employed a previously published experimental paradigm to study how adaptation to temporally constant distractor input facilitates the recognition of a newly appearing target object. Specifically, they studied how this effect is modulated by the contrast of the target object.</p>
<p>They found that adaptation enhances the recognition of high-contrast objects more than that of low-contrast objects. This behavioral effect was mirrored by a larger effect of adaptation on the response to the high-contrast objects in relatively higher visual areas.</p>
<p>To investigate what neural computations can support this interaction, they implement several candidate neural mechanisms in a deep convolutional neural network: additive suppression, divisive suppression, and lateral recurrence. The authors conclude that divisive and additive suppression, which are intrinsic to the neuron, best explain the interaction between contrast and adaptation in the human data. They further show that these mechanisms, and divisive suppression in particular, show increased robustness to spatial shifts of the adaptor stimulus, hinting and potential perceptual benefits.</p>
<p>Strengths:</p>
<p>(1) Overall, this is a well-written paper, supported by thorough analyses and illustrated with clear, well-designed figures that effectively show overall trends as well as data variance. The authors tell a compelling story while responsibly steering away from overreaching conclusions.</p>
<p>(2) What makes this paper stand out is its comprehensive approach to understanding the behavioral benefit of neural adaptation and its mechanistic underpinnings. The authors effectively achieve this through integrating new behavioral and neural data with simulations using neural network models.</p>
<p>(3) The findings convincingly demonstrate that neuronally intrinsic adaptation mechanisms are sufficient to explain the observed interaction between temporal adaptation, contrast, and object recognition. Furthermore, the paper highlights that these intrinsic mechanisms offer superior robustness compared to learned lateral recurrence mechanisms, which, while being more expressive, can also be more brittle.</p>
<p>Weaknesses:</p>
<p>While the results and conclusion are well supported, there were a few major points that need clarification for me.</p>
<p>(1) Divisive normalization</p>
<p>I was confused by the author's classification of divisive normalization as a neuronally intrinsic mechanism, that is, one that operates within a single neuron, independent of interactions with other neurons.</p>
<p>My understanding is that divisive normalization, as originally proposed by Heeger in the early nineties, describes a mechanism where neurons integrate pooled activity from neighboring cells to mutually inhibit one another. In this form, divisive normalization is fundamentally an interneuronal mechanism involving recurrence. Adding to the confusion, the authors highlight in the introduction their interest in divisive normalization for its relation to stimulus contrast, a relation likely linked to neuronal pooling.</p>
<p>However, my reading of the methods section (Equations 6 and 7) suggests the authors implemented only a temporal feedback component, leaving out the pooling across neurons (Equation 5). This distinction should be disambiguated early in the paper. I recommend choosing a less ambiguous term than &quot;divisive normalization&quot;. Even &quot;temporal divisive normalization&quot; is still ambiguous, as lateral neuronal interactions are also inherently temporal.</p>
<p>(2) Parietal electrodes</p>
<p>The paper's adapter-specific effects are centered around the P9/P10 electrodes, which the authors identify as &quot;parietal.&quot; However, it is unclear to me which part of the cortex drives these electrodes, particularly whether it is actually the parietal cortex. I am no expert in EEG, but based on the topomaps in Figures 4 and 5, it appears that these electrodes cover more posterior occipito-temporal regions rather than truly parietal regions. Given the central role of P9/P10 to the main findings, the paper would be significantly improved for non-EEG readers by clarifying which cortical regions are covered by these electrodes.</p>
<p>(3) Interpretation of non-significant statistical results</p>
<p>In some places, the authors attach relatively strong claims to non-significant statistical results. For example, in Figure 5D, they claim that there is no effect of contrast on occipital electrodes, based on a non-significant p-value. P-values do not quantify evidence for the null hypothesis, so the authors should be careful with such claims. In fact, Figure 5D shows such a clear negative slope, with variance comparable to Figure 5A, that I am surprised that the p-value for the slope of Figure 5D was in fact so large. A similar issue arises in the discussion for Figure 6, where the authors claim that the effect of contrast is adapter-specific. However, this claim is based on the observation that is significant for same-noise trials, but not for different-noise or blank trials. To statistically substantiate the claims that there is an adapter-specific effect, the authors should directly compare the slope for same-noise trials with the slope for different-noise/blank trials.</p>
<p>(4) The match between behavior and models</p>
<p>The authors' claim that models with intrinsic adaptation better match the interaction between contrast and temporal adaptation observed in human behavior is not fully substantiated. This conclusion appears to be based on a qualitative assessment of Figure 8, which, in my view, does not unambiguously rule out an interaction for lateral recurrence. Furthermore, a potential confounding factor is the ceiling effect that limits higher accuracy values. Indeed, conditions where the interaction was not/less (i.e., shorter time sequences and lateral inhibition) are also the conditions where accuracy values are closer to this ceiling, which may mask a potential interaction.</p>
</body>
</sub-article>
</article>