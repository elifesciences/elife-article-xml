<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108941</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108941</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108941.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Visuomotor mismatch EEG responses in occipital cortex of freely moving human subjects</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2969-2963</contrib-id>
<name>
<surname>Solyga</surname>
<given-names>Magdalena</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>magdalena.solyga@fmi.ch</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3857-3173</contrib-id>
<name>
<surname>Zelechowski</surname>
<given-names>Marek</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1401-0117</contrib-id>
<name>
<surname>Keller</surname>
<given-names>Georg B</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
    <aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01bmjkv45</institution-id><institution>Friedrich Miescher Institute for Biomedical Research</institution></institution-wrap>, <city>Basel</city>, <country country="CH">Switzerland</country></aff>
    <aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s6k3f65</institution-id><institution>Center for medical Image Analysis &amp; Navigation, Department of Biomedical Engineering, University of Basel</institution></institution-wrap>, <city>Basel</city>, <country country="CH">Switzerland</country></aff>
    <aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s6k3f65</institution-id><institution>Faculty of Science, University of Basel</institution></institution-wrap>, <city>Basel</city>, <country country="CH">Switzerland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5153-4959</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Büchel</surname>
<given-names>Christian</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>University Medical Center Hamburg-Eppendorf</institution>
</institution-wrap>
<city>Hamburg</city>
<country country="DE">Germany</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: Part of the results presented herein have been included in patent application EP25195394.9.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-11-24">
<day>24</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108941</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-09-12">
<day>12</day>
<month>09</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-08-19">
<day>19</day>
<month>08</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.08.14.670295"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Solyga et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Solyga et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108941-v1.pdf"/>
<abstract>
<p>Likely the strongest predictor of visual feedback is self-motion. In mice, the coupling between movement and visual feedback is learned with first visual experience of the world (<xref ref-type="bibr" rid="c1">Attinger et al., 2017</xref>), and brief perturbations of the coupling result in strong visuomotor mismatch responses in visual cortex that possibly reflect prediction errors (<xref ref-type="bibr" rid="c15">Keller et al., 2012</xref>; <xref ref-type="bibr" rid="c43">Zmarz and Keller, 2016</xref>). In humans, predictive coding has primarily been studied using oddball paradigms which rely on violations of stimulus probability based on recent sensory history. It was still unclear, however, whether humans exhibit visuomotor mismatch responses similar to those observed in mice. This question was important for two reasons. First, visuomotor mismatch responses in humans constitute a basis to start translating the mechanistic understanding of the circuit that computes these responses from mouse to human cortex. Second, a paradigm that can trigger strong prediction error responses and consequently requires shorter recording times would simplify experiments in a clinical setting. Here, by combining a wireless EEG recording system with virtual reality headset, we found robust visuomotor mismatch responses in human cortex that were characterized by a reversed polarity relative to visual evoked responses and a greater signal power than both visual responses and oddball mismatch responses.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00yjd3n13</institution-id>
<institution>Swiss National Science Foundation</institution>
</institution-wrap>
</funding-source>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/04f9t1x17</institution-id>
<institution>Novartis Foundation</institution>
</institution-wrap>
</funding-source>
</award-group>
<award-group id="funding-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
    <award-id award-id-type="doi">10.3030/865617</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Predictive processing is a framework for understanding brain function. It proposes that the brain constructs internal models of the world based on regularities in past sensory input and sensorimotor loops to predict upcoming sensory input. The brain does this by minimizing the mismatches between predicted and actual sensory input. These mismatches, known as prediction errors, play a dual role in both updating internal representations and internal models (<xref ref-type="bibr" rid="c10">Friston, 2005</xref>; <xref ref-type="bibr" rid="c18">Keller and Sterzer, 2024</xref>; <xref ref-type="bibr" rid="c26">Rao and Ballard, 1999</xref>).</p>
<p>Predictions about sensory input can be made based on a variety of sources of other information the brain has available. Certain stimuli can be predicted from the preceding sensory input – experimentally this can be used, e.g., in oddball paradigms to trigger stimulus history based prediction errors (<xref ref-type="bibr" rid="c12">Garrido et al., 2009</xref>). Predictions can be crossmodal. Certain sounds are associated with specific types of visual input. Experimentally visual stimuli can be coupled to sounds to then trigger audiovisual prediction errors in mouse visual cortex (<xref ref-type="bibr" rid="c11">Garner and Keller, 2022</xref>). Predictions can also be based on memory. Not seeing a stimulus in a specific spatial location can result in visuospatial prediction errors in mouse cortex (<xref ref-type="bibr" rid="c9">Fiser et al., 2016</xref>). However, the strongest predictor - by consistency of coupling - is self-generated movement. Every movement is directly coupled to sensory feedback throughout life.</p>
<p>An essential ingredient to the computation of visuomotor prediction error responses are motor-related predictions of visual feedback (<xref ref-type="bibr" rid="c17">Keller and Mrsic-Flogel, 2018</xref>). In the mouse, evidence for such predictions have come from the discovery of a strong motor-related modulation of activity in visual cortex (<xref ref-type="bibr" rid="c15">Keller et al., 2012</xref>; <xref ref-type="bibr" rid="c24">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="c27">Saleem et al., 2013</xref>). These motor-related signals are likely in part driven by motor-related predictions arriving from motor areas of cortex (<xref ref-type="bibr" rid="c20">Leinweber et al., 2017</xref>). However, the overall level of this motor-related activity is much higher than one would expect simply from predictions of visual feedback that are compared against visual input.</p>
<p>The more precise the prediction and comparison, the less motor-related activity should be detectable in visual cortex. This is evident in the auditory system of a songbird, which relies on very precise sensorimotor error detection for vocal learning, where there is much less motor modulation of auditory responses (<xref ref-type="bibr" rid="c16">Keller and Hahnloser, 2009</xref>). Similarly, there is very little movement related modulation of activity in visual cortex of non-human primates (<xref ref-type="bibr" rid="c21">Liska et al., 2024</xref>; <xref ref-type="bibr" rid="c37">Talluri et al., 2023</xref>). In humans, movement-related modulation of EEG activity in visual cortex has been recently described (<xref ref-type="bibr" rid="c3">Cheng and Nordin, 2025</xref>), but it is still unclear if visual responses are modulated by these signals. If indeed the precision of visuomotor coupling determines the amount of motor modulation of visual responses, we would expect to find less of it in humans than in the mouse.</p>
<p>Imprecise comparisons of motor related predictions and visual input should manifest as increases in motor-related activity and smaller prediction error response. Based on this, we would expect to find strong visuomotor mismatch responses in humans, similar to – or stronger than - those observed in mice (<xref ref-type="bibr" rid="c15">Keller et al., 2012</xref>). Based on this and prior work demonstrating that there are selective responses in human visual cortex to a mismatch between actual hand position and that of a virtual hand (<xref ref-type="bibr" rid="c34">Stanley and Miall, 2007</xref>), we expected to find strong EEG responses to a break in coupling between locomotion and visual feedback.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Visual responses</title>
<p>To quantify visual responses in freely moving human participants we combined a virtual reality headset with an 8-electrode wireless EEG recording system (<bold>Methods</bold>; <xref rid="fig1" ref-type="fig">Figure 1A</xref>). Participants were shown a virtual environment that consisted of a large empty floor space with a square checkerboard pattern fixed to their viewing angle directly in front of them (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). The checkerboard covered a visual angle of 53° and contained a red fixation dot in the center. Participants were asked to fixate on the red dot throughout all measurements. The checkerboard reversed white-black at intervals sampled from a uniform distribution between 2 and 4 s (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). Recordings were split into a passive session during which participants were seated on a chair, and an active session during which they were instructed to walk around a rectangular floor area of 5 by 7 m. During the entire experiment we tracked the location of participants (<xref rid="fig1" ref-type="fig">Figure 1D</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption>
<title>EEG responses to visual stimulation are modulated by walking.</title>
<p><bold>(A)</bold> Participant wearing the setup for wireless EEG recordings and a virtual reality headset. An Open BCI EEG electrode cap is combined with META QUEST 3. Inset: first person view of the virtual environment. Red fixation dot was enlarged in figure to make it visible. <bold>(B)</bold> A third person view of the virtual scene used to study visual responses to a reversing checkerboard pattern. The black line on the ground indicates the safety boundary within which the participant was instructed to walk around during the movement block of the paradigm. The grid indicates 1 by 1 m squares. <bold>(C)</bold> Participants viewed a reversing checkerboard in two conditions: during half of the trials, they remained seated, and during the other half they walked within a defined safety boundary. Checkerboard reversed colors at random intervals every 2 to 4 s. <bold>(D)</bold> 3D trajectory of an example participant during the visual experiment. Movement was recorded using accelerometers integrated into the VR headset. <bold>(E)</bold> Visually evoked potentials measured in the sitting condition on the occipital electrodes O1 and O2. Responses recorded on other electrodes are presented in <xref ref-type="fig" rid="figS2">Figure S2</xref>. Solid black line represents the mean, and shading indicates the SEM across participants. Dashed vertical red line indicates the time of the checkerboard reversal. P1 - positive peak at around 88 ms. <bold>(F)</bold> As in <bold>E</bold>, but while participants were walking. Inset: comparison of visual evoked potentials measured in the sitting <bold>E</bold> and walking <bold>F</bold> conditions. The horizontal bar above the plot marks time bins in which responses differ significantly (black: p &lt; 0.05) or do not differ (gray: p &gt; 0.05).</p>
</caption>
<graphic xlink:href="670295v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Walking triggered movement related artifacts in the EEG signals (<xref rid="figS1" ref-type="fig">Figure S1</xref>). The strength of these movement artifacts varied as a function of hairstyle and gait of the participants. All participants were encouraged to go ‘gentle’ to reduce this problem. We excluded all data with high movement related artifacts from further analysis (<bold>Methods</bold>; visual paradigm: 19 of 48 recordings, visuomotor mismatch paradigm: 15 of 48 recordings).</p>
<p>We could detect multiphasic responses to the checkerboard inversion in occipital EEG electrodes (<xref rid="fig1" ref-type="fig">Figures 1E</xref> and <xref ref-type="fig" rid="figS2">S2</xref>). These responses were comparable to previous reports (<xref ref-type="bibr" rid="c6">Drislane, 2007</xref>) and exhibited a positive peak at around 88 ms (P1). Interestingly, when participants were walking, the same responses showed an early negative deflection with a peak latency of 48 ms that preceded P1 (<xref rid="fig1" ref-type="fig">Figure 1F</xref>).</p>
<p>While participants were instructed to fixate on the red dot, it is possible that there are systematic differences in eye movements between walking and sitting conditions. Our setup did not allow for concurrent eye tracking; thus we cannot exclude the possibility that differences in eye movements contribute to the differences in EEG responses we observe. However, given the relatively rapid onset of response differences (40 ms), we suspect that stimulus triggered eye movement cannot account for these early differences.</p>
</sec>
<sec id="s2b">
<title>Visuomotor mismatch responses</title>
<p>To measure visuomotor mismatch responses, participants were instructed to walk around in a virtual corridor (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). The corridor was 1 m wide, 2.4 m high and of an oval shape covering approximately 7 by 5 m (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Virtual movement in the corridor was coupled to the movement of participants. We refer to this as closed loop. To trigger visuomotor mismatches, we briefly (0.5 s) halted the coupling between movement of the participants and visual feedback in the virtual corridor at random times (every 10 to 15 s; <bold>Methods</bold>; <xref rid="fig2" ref-type="fig">Figure 2C</xref>). Throughout all experiments, we tracked the virtual position of participants as they were walking around the corridor (<xref rid="fig2" ref-type="fig">Figure 2D</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption>
<title>Visuomotor mismatch elicits strong EEG responses.</title>
<p><bold>(A)</bold> Participant navigating a virtual tunnel. Inset: first person view of the tunnel. <bold>(B)</bold> Third person view of the virtual environment used to study visuomotor mismatch responses. The grid indicates 1 by 1 m squares. <bold>(C)</bold> In closed loop sessions, the walking speed of participants was coupled to movement in the virtual corridor. Visuomotor mismatches were introduced by briefly halting the visual flow for 0.5 s. Following each visuomotor mismatch event, the view was updated to the participants’ current position (brief peaks in visual flow speed after the mismatch event) and visuomotor coupling was resumed. <bold>(D)</bold> Trajectory of an example participant during the visuomotor mismatch paradigm. <bold>(E)</bold> Responses to visuomotor mismatches recorded from occipital electrodes (O1 and O2). Solid black line represents the mean, and shading indicates the SEM across participants. Dashed vertical red lines are onset and offset of the visuomotor mismatch. The gray shaded areas mark the analysis windows used to quantify response strength in <bold>G</bold> and <bold>H</bold>. Analysis windows of 100 ms were centered on the peak of the visuomotor mismatch response and the visual flow re-onset response. <bold>(F)</bold> As in <bold>E</bold>, but for visual flow playback halt responses recorded from occipital electrodes. <bold>(G)</bold> Comparison of the response strength to visuomotor mismatches and playback halts. Here and elsewhere: boxes mark median, quartiles, and range of data not considered outliers. Each circle represents data from one participant. ***: p&lt;0.001. One data point not shown (Mismatch response, at −9.5 μV). See <xref ref-type="table" rid="tblS1">Table S1</xref> for all statistical information. <bold>(H)</bold> Comparison of the response strength to visual flow re-onset events following visuomotor mismatches in the closed loop condition and playback halts in the open loop condition. n.s.: not significant. See <xref ref-type="table" rid="tblS1">Table S1</xref> for all statistical information. <bold>(I)</bold> Average walking speed of participants during visuomotor mismatches. Inset: Temporally expanded view. The horizontal bar above the plot marks time bins where walking speed differs significantly (black: p &lt; 0.05) or does not differ (gray: p &gt; 0.05) from baseline.</p>
</caption>
<graphic xlink:href="670295v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Anecdotally, the perception some participants reported experiencing during these visuomotor mismatches was a very salient sudden movement of the entire visual world. In the words of one participant: “The world suddenly flew forward! – Are you printing this? – Hi Mom!”. This was reminiscent of a case report of a patient with a lesion to the lateral rectus muscle described by von Helmholtz (<xref ref-type="bibr" rid="c42">von Helmholtz, 1867</xref>). The lateral rectus muscle moves the eye laterally. The patient, upon attempting to move the affected eye laterally, reported seeing the world rapidly moving in the direction of the intended eye movement. One interpretation of this is that the combination of movement and absence of resulting visual flow change results in the perception that the world must be moving.</p>
<p>In the EEG recordings of occipital electrodes, we found strong responses triggered by these visuomotor mismatches (<xref rid="fig2" ref-type="fig">Figure 2E</xref>). Responses were dominated by a positive component peaking at 180 ms. Following the 0.5 s halting of the visual stimulus, the virtual location was updated to match the actual location of participants, and the visual flow coupling was resumed. This resulted in a sudden change of visual stimulus combined with a visual flow onset and drove what is likely a visual response in the EEG signal (<xref rid="fig2" ref-type="fig">Figure 2E</xref>).</p>
<p>To quantify how much of the visuomotor mismatch response can be explained by visual input alone, we exposed participants to a replay of the visual flow that was self-generated in the preceding closed loop session, including the visual flow halts that constitute visuomotor mismatch in the closed loop condition. We refer to this as open loop. For these experiments, participants were seated on a chair. To reduce the likelihood of triggering nausea in participants, roll and pitch movements of the head were removed from this replay. We found measurable responses to visual flow playback halts (<xref rid="fig2" ref-type="fig">Figure 2F</xref>), but these were much smaller than visuomotor mismatch responses (<xref rid="fig2" ref-type="fig">Figure 2G</xref>). In the few participants who volunteered to experience playback with pitch and roll movements, playback halt responses were not different from those in the reduced playback condition (<xref rid="figS3" ref-type="fig">Figure S3</xref>). The difference in response amplitude between mismatch and playback response could not be explained by a simple movement related gain of visual responses, as the response to the re-onset of visual flow was similar in both closed and open loop conditions (<xref rid="fig2" ref-type="fig">Figure 2H</xref>). Note that the mismatch responses and playback halt responses shown here do not come from fully overlapping pools of participants (<xref ref-type="table" rid="tblS1">Table S1</xref>). A subset of sessions had to be excluded because of recording noise (visuomotor mismatch: 15 of 48, playback: 4 of 48), while some participants aborted the playback recording due to developing nausea (11 of 48; <bold>Methods</bold>). A participant-wise comparison of mismatch and playback halt responses is shown in <xref rid="figS4" ref-type="fig">Figure S4</xref>.</p>
<p>Although participants reduced their walking speed in response to the visuomotor mismatch, this reduction was merely a trend and did not reach statistical significance (<xref rid="fig2" ref-type="fig">Figure 2I</xref>). However, given the comparably slow time course of reduction in walking speed, these behavioral changes cannot explain visuomotor mismatch responses.</p>
</sec>
<sec id="s2c">
<title>Distribution of visuomotor mismatch response strength</title>
<p>In mice, visuomotor mismatch responses originate in primary visual cortex and propagate to a network of areas across dorsal cortex (<xref ref-type="bibr" rid="c13">Heindorf and Keller, 2023</xref>; <xref ref-type="bibr" rid="c36">Takeuchi et al., 2024</xref>). Thus, we expected to find larger and faster responses in the two occipital electrodes. To test this, we compared the visuomotor mismatch responses across the eight recorded locations in both response strength and timing (<xref rid="fig3" ref-type="fig">Figures 3A</xref> and <xref ref-type="fig" rid="fig3">3B</xref>). We indeed found that the strongest responses were observed at occipital electrodes O1 and O2, though significant responses were also present at frontal (Fp1) and central (C3) sites (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). However, while there was a trend toward faster responses in occipital cortex, we found no evidence of differences in response latency across the different electrodes (<xref rid="fig3" ref-type="fig">Figures 3D</xref> and <xref ref-type="fig" rid="fig3">3E</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption>
<title>Visuomotor mismatch responses are most prominent in occipital cortex.</title>
<p><bold>(A)</bold> Top down view of EEG electrode locations on the head. <bold>(B)</bold> Visuomotor mismatch responses measured on electrodes shown in <bold>A</bold>. Solid black lines represent the mean, and shading indicates the SEM across participants. Dashed vertical red lines are onset and offset of the visuomotor mismatch. <bold>(C)</bold> Comparison of the response strength to visuomotor mismatches measured on electrodes shown in <bold>A</bold>. Average response strength was calculated within a 100 ms window centered on the peak of the average visuomotor mismatch response across all electrodes. Boxes mark median, quartiles, and range of data not considered outliers. Each circle represents data from individual participant. ***: p&lt;0.001, **: p&lt;0.01, *: p&lt;0.05, n.s.: not significant. See <xref ref-type="table" rid="tblS1">Table S1</xref> for all statistical information. <bold>(D)</bold> The responses shown in <bold>B</bold>, averaged over pairs of electrodes and overlaid. Solid black lines represent mean and shading SEM across electrodes. Dashed vertical red lines are onset and offset of the visuomotor mismatch. <bold>(E)</bold> Comparison of the latency to half maximum response for the 4 electrode pairs. n.s.: not significant. See <xref ref-type="table" rid="tblS1">Table S1</xref> for all statistical information.</p>
</caption>
<graphic xlink:href="670295v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<title>Comparison of visual responses and visuomotor mismatch responses</title>
<p>The time course of visual responses (<xref rid="fig1" ref-type="fig">Figure 1E</xref>) and visuomotor mismatch responses (<xref rid="fig2" ref-type="fig">Figure 2E</xref>) differed in that they appeared to exhibit reversed polarity (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Despite this difference in polarity, the initial peaks occurred at similar latencies. The total power of the visuomotor response was higher than that of the visual response (<xref rid="fig4" ref-type="fig">Figure 4B</xref>) but given that we did not optimize either the visuomotor mismatch stimulus nor the visual stimulus for maximum response, this comparison is more qualitative than quantitative in nature. We include it here primarily to illustrate how the visuomotor mismatch response compares to a more conventional visual response.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption>
<title>Visuomotor mismatch responses have reversed polarity and more power compared to visual responses.</title>
<p><bold>(A)</bold> Comparison of visual (<xref ref-type="fig" rid="fig1">Figure 1E</xref>) and visuomotor mismatch (<xref ref-type="fig" rid="fig2">Figure 2E</xref>) responses recorded from occipital electrodes. Solid black line represents the mean, and shading indicates the SEM across participants. Dashed vertical red lines are onset (visual and mismatch) and offset (mismatch) of the stimuli. <bold>(B)</bold> Comparison of the power of visual and visuomotor mismatch responses, calculated within a 0 - 0.5 s time window following stimulus onset. Boxes mark median, quartiles, and range of data not considered outliers. Each circle represents data from individual participant. ***: p&lt;0.001. See <xref ref-type="table" rid="tblS1">Table S1</xref> for all statistical information.</p>
</caption>
<graphic xlink:href="670295v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2e">
<title>Comparison of visuomotor mismatch responses and auditory oddball mismatch responses</title>
<p>To further contrast the strength of visuomotor mismatch to other frequently used prediction error signals, we also recorded EEG responses in an auditory oddball paradigm. Mismatch responses recorded with this paradigm can be thought of as a stimulus history prediction error (<xref ref-type="bibr" rid="c12">Garrido et al., 2009</xref>; <xref ref-type="bibr" rid="c23">Näätänen et al., 2007</xref>). We implemented an oddball paradigm composed of a sequence of identical tones that was occasionally disrupted by a tone of a different frequency (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). We used pure tones of 1 kHz and 1.2 kHz frequency, and alternated in blocks which tone was standard and which was the deviant (<bold>Methods</bold>). To enable direct comparison with visuomotor mismatch responses, recordings were made from occipital electrodes O1 and O2. Participants watched short silent movies in the VR headset while listening to the tone sequences delivered in headphones. To isolate the oddball driven mismatch response, we subtracted the response to the tone when presented as a standard from the response to the same tone when presented as an deviant. The resulting difference waveform closely resembled those reported in previous studies (<xref rid="fig5" ref-type="fig">Figures 5B</xref> and <xref ref-type="fig" rid="fig5">5C</xref>) (<xref ref-type="bibr" rid="c23">Näätänen et al., 2007</xref>). Comparing these oddball mismatch responses to the visuomotor mismatch response, we found that both had similar dynamics with a dominant positive peak at around 180 ms (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). Comparing the total power of the two responses, we found that visuomotor mismatch responses were significantly larger than oddball mismatch responses (<xref rid="fig5" ref-type="fig">Figure 5E</xref>). Also here the comparison of total power is more qualitative than quantitative as neither stimulus was optimized for maximum EEG response, and we include it to illustrate how typical variants of these responses compare.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption>
<title>Visuomotor mismatch responses are larger than auditory oddball mismatch responses but have similar temporal dynamics</title>
<p><bold>(A)</bold> Design of the auditory oddball paradigm with examples of the silent films participants were exposed to. <bold>(B)</bold> Top: Auditory responses to the 1 kHz tone presented as a standard versus as a deviant. Bottom: Oddball mismatch response calculated by subtracting the average response over trials in which the tone was presented as a standard from those when the tone was presented as a deviant. Solid black lines represent the mean, and shading indicates the SEM across participants. Dashed vertical red line is the onset of the auditory stimulus. <bold>(C)</bold> As in <bold>B</bold>, but for the 1.2 kHz tone. <bold>(D)</bold> Comparison of visuomotor mismatch, oddball mismatch (average over data shown in panel B and C) and playback halt responses recorded from occipital electrodes. Solid black lines represent the mean response, with shading indicating SEM across electrodes. Dashed vertical red lines are onset (visual, mismatch) and offset (mismatch) of the stimuli. <bold>(E)</bold> Comparison of the power of visuomotor mismatch, oddball mismatch response (average over data shown in panel B and C) and playback halt responses, calculated within a 0 s - 0.5 s time window following stimulus onset. Boxes mark median, quartiles, and range of data not considered outliers. Each circle represents data from individual participant. ***: p&lt;0.001; *p&lt;0.05. See <xref ref-type="table" rid="tblS1">Table S1</xref> for all statistical information.</p>
</caption>
<graphic xlink:href="670295v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The utility of paradigms for measuring prediction errors in humans primarily comes from their relevance to psychiatric disorders, where disruptions in prediction error processing are widely implicated (Adams et al., 2013; Fletcher and Frith, 2009; Kirihara et al., 2020; Qela et al., 2025; <xref ref-type="bibr" rid="c35">Sterzer et al., 2018</xref>). Various types of prediction error responses are being explored as potential biomarkers in clinical research, serving to monitor disease progression, evaluate symptoms, and provide insight into the underlying neural mechanisms of these conditions.</p>
<p>The most prominent of these prediction errors is mismatch negativity (MMN). MMN is the first component of the oddball mismatch response (<xref ref-type="bibr" rid="c22">Näätänen et al., 1978</xref>). MMN has been shown to be reduced or abnormal in patients with schizophrenia spectrum disorders (<xref ref-type="bibr" rid="c39">Todd et al., 2012</xref>; <xref ref-type="bibr" rid="c41">Umbricht and Krljes, 2005</xref>), autism spectrum disorders (<xref ref-type="bibr" rid="c2">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="c7">Dunn et al., 2008</xref>; <xref ref-type="bibr" rid="c33">Schwartz et al., 2018</xref>), speech disorders (<xref ref-type="bibr" rid="c8">El Hatal de Souza et al., 2020</xref>), depression (<xref ref-type="bibr" rid="c40">Tseng et al., 2021</xref>), and bipolar disorders (<xref ref-type="bibr" rid="c4">Chitty et al., 2013</xref>). More generally, it is thought that many of these disorders can be related to changes in predictive processing. In the case of schizophrenia, for example, the symptoms are thought to arise from an imbalance in the strength of high and low level predictions (priors) (<xref ref-type="bibr" rid="c31">Schmack et al., 2017</xref>, <xref ref-type="bibr" rid="c30">2015a</xref>, <xref ref-type="bibr" rid="c32">2015b</xref>, <xref ref-type="bibr" rid="c29">2013</xref>). Further testing these ideas will require experiments that trigger identified functional responses in humans, for which we have a circuit level understanding based on cell type specific recordings.</p>
<p>We were able to measure robust visuomotor mismatch responses in freely moving humans. These signals cannot be attributed to changes in participants’ behavior or to simple visual offset responses. Notably, the visuomotor mismatch responses exhibited a markedly different temporal profile compared to purely visual responses. What might account for this difference? One possibility is that visuomotor mismatch signals may rely more strongly on input from other cortical areas than visual responses and are thus delayed relative to visual responses. A more interesting interpretation is that visual responses and visuomotor mismatch responses are both prediction errors of different types. A central tenet of the cortical circuit for predictive processing is the split into separate populations of neurons that compute positive and negative prediction errors (<xref ref-type="bibr" rid="c17">Keller and Mrsic-Flogel, 2018</xref>; <xref ref-type="bibr" rid="c26">Rao and Ballard, 1999</xref>). In this interpretation, a visuomotor mismatch response is a negative prediction error, while the response to a visual stimulus is a positive prediction error.</p>
<p>Visuomotor prediction errors are likely computed in layer 2/3 of mouse visual cortex (<xref ref-type="bibr" rid="c14">Jordan and Keller, 2020</xref>), and are also preferentially detectable in superficial layers of human visual cortex (<xref ref-type="bibr" rid="c38">Thomas et al., 2024</xref>). In the mouse, positive and negative prediction errors are likely computed in separate populations of neurons in visual cortex (<xref ref-type="bibr" rid="c17">Keller and Mrsic-Flogel, 2018</xref>; <xref ref-type="bibr" rid="c25">O’Toole et al., 2023</xref>). These two cell types have a different depth distribution, with negative prediction error neurons more superficial and positive prediction error neurons located deeper in cortex (<xref ref-type="bibr" rid="c25">O’Toole et al., 2023</xref>). Given that EEG signals are thought to exhibit positive or negative deflections as a function of depth of the source (<xref ref-type="bibr" rid="c5">Cohen, 2017</xref>; <xref ref-type="bibr" rid="c19">Kirschstein and Köhling, 2009</xref>), the polarity reversal of visual and visuomotor mismatch responses (<xref rid="fig4" ref-type="fig">Figure 4</xref>) may be the consequence of different populations of cells being activated and inhibited. Visuomotor mismatch should activate negative prediction error neurons and inhibit positive prediction error neurons, while the reverse is the case for a visual stimulus.</p>
<p>The fact that there is also a strong visual response to the visual flow re-onset following visuomotor mismatch means that our visuomotor mismatch paradigm might allow us to measure both negative and positive prediction error responses. More intriguingly, there is a differential modulation by walking of the two responses (<xref rid="fig2" ref-type="fig">Figures 2G</xref> and <xref ref-type="fig" rid="fig2">2H</xref>). In theory, visuomotor mismatch drives a combination of two negative prediction errors. One is based on a movement related prediction. The other on a stimulus-history related prediction. Both the self-motion as well as the ongoing visual flow would predict a continuation of visual flow. A visuomotor mismatch violates both predictions. During passive observation there is no movement-related prediction, but the stimulus history-based prediction is still violated.</p>
<p>Intriguing is the similarity in timing and polarity of the playback halt and the difference between oddball and standard response (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). An oddball response is always a combination of both a positive and a negative prediction error. There is a negative prediction error for the absence of the expected standard tone as well as a positive prediction error for the presence of the unexpected oddball tone. Thus, it is conceivable that the subtraction of standard response from the oddball response, isolates the component of the oddball response that corresponds to a negative prediction error.</p>
<p>One of the key challenges in systems neuroscience is translating findings from animal models to humans. Although recent animal studies have provided detailed insights into the circuit level implementation of predictive processing in the cortex (<xref ref-type="bibr" rid="c17">Keller and Mrsic-Flogel, 2018</xref>), and psychological and psychiatric conditions have long been described within this framework (<xref ref-type="bibr" rid="c18">Keller and Sterzer, 2024</xref>; <xref ref-type="bibr" rid="c35">Sterzer et al., 2018</xref>), translation to human research has been limited by a lack of methods to record cell type specific signals in human experiments. With an understanding from animal models of how positive and negative prediction errors are computed, and which cortical layers are involved, an approach based on functionally identified responses might be most promising. One step in this direction is to use experimental paradigms that can cleanly separate e.g. positive and negative prediction error responses. As argued above, we speculate that the visuomotor mismatch paradigm as we have used it here, is one possible way to achieve this.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>The study was approved by Ethikkommission Nordwest- und Zentralschweiz (Project-ID: 2024-02458). Participants signed a written informed consent before participation and were not financially compensated for their participation. 46 healthy adults (19 males, 27 females) participated in the study, ranging in age from 18 to 65 years. The age distribution was as follows: 19 participants aged 18-30, 18 aged 31-40, 5 aged 41-50, and 4 aged 51-65. Two participants went through the recordings twice, resulting in 48 recorded sessions in total. None of the participants reported a prior diagnosis of movement disorders, vestibular dysfunction, or epilepsy. Experience with virtual reality (VR) technology ranged from beginner to advanced, with most participants reporting minimal prior experience.</p>
</sec>
<sec id="s4b">
<title>EEG recordings in humans</title>
<p>We integrated a wireless EEG recording system with a VR headset. The EEG recording system was composed of a wet electrode cap and a Cyton biosensing board from OpenBCI. This allowed us to record 8 EEG channels (45 sessions: FP1, FP2, C3, C4, P3, P4, O1 and O2; 3 sessions: FP1, FP2, T3, T4, T5, T6, O1 and O2) at a 250 Hz sampling rate. Electrode labels follow the international 10-20 system: FP = frontopolar, C = central, P = parietal, O = occipital, and T = temporal; odd numbers indicate the left hemisphere, and even numbers the right (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). All EEG data were wirelessly (via Bluetooth) transmitted to a nearby computer. The VR headset was a Meta Quest 3, with a virtual environment developed in the Unity engine (Unity Technologies). Virtual 3D objects were designed in Fusion 360 (Autodesk). Synchronization between EEG recording and VR headset was performed by connecting an auditory output of the VR headset to the Cyton board to exchange synchronization triggers. This allowed us to synchronize EEG data with VR events offline. Auxiliary signals, including the participant’s position, stimulus trigger timing, and type, were recorded directly on the headset at a sampling rate of approximately 100 Hz.</p>
</sec>
<sec id="s4c">
<title>Visual responses</title>
<p>Visual stimuli were presented using the VR headset. We used a reversing square checkerboard stimulus to drive visually evoked potentials. The checkerboard reversed colors at random intervals (between 2 and 4 s). In virtual space, the checkerboard measured 0.5 by 0.5 m and was positioned 0.5 m in front of participants’ eyes. This resulted in a horizontal and vertical coverage by the checkerboard of approximately 53° of visual angle. Each visual stimulation session lasted 4 min, during which the checkerboard reversed colors between 78 and 82 times. For half of the session, participants viewed the stimulus while seated; for the other half, they were instructed to walk freely within a 7 by 5 m empty floor space. The order of sitting and walking conditions was randomized across participants.</p>
</sec>
<sec id="s4d">
<title>Visuomotor mismatch responses</title>
<p>For the experiments measuring visuomotor mismatch responses, we used a 3D virtual corridor with vertical gratings on the walls. The corridor measured 1 m in width, 2.4 m in height, and had an oval shape of 7 by 5 m. Visuomotor mismatches were introduced at random intervals every 10 to 15 s as participants walked through the corridor. The session lasted 5 min and included between 22 and 26 visuomotor mismatch events. During these events, the coupling between the participant’s movement and the visual feedback in the VR headset was briefly interrupted, i.e. the visual scene was frozen for 0.5 s. Because participants continued to move during this time, they were teleported to their current position in the virtual space following visuomotor mismatch event.</p>
</sec>
<sec id="s4e">
<title>Playback halt responses</title>
<p>To quantify how much of the visuomotor mismatch response could be explained by visual input alone, participants were asked to passively observe a replay of the visual flow they had self-generated during the preceding closed loop session. We quickly learned, however, that watching 5 minutes of playback in the VR headset triggered nausea in most participants. Thus, we started experimenting with changes to the playback to minimize the risk of triggering nausea. One such modification we settled on to use for experiments was a playback version that omitted pitch and roll movements of the head. Full playback involved six degrees of freedom (6DOF): 3D position in space, plus pitch, yaw, and roll angles of the head. The constrained playback included only four degrees of freedom (4DOF): the 3D position in space, but only yaw movements of the head. A subset of participants volunteered to view the full playback (<xref rid="figS3" ref-type="fig">Figure S3</xref>). Playback sessions were conducted while participants were seated and lasted 5 min, matching the duration of the closed loop session. Approximately 23% of participants reported strong nausea during the beginning of the 4DOF playback session, at which point the recordings were terminated.</p>
</sec>
<sec id="s4f">
<title>Mismatch negativity</title>
<p>To measure mismatch negativity we used an auditory oddball paradigm. Auditory stimuli consisted of two pure tones: 1 kHz and 1.2 kHz. They were presented at 60 to 70 dB sound pressure level in a counterbalanced design, in which each tone served as the standard in one condition and as the deviant in the other. Tones were 50 ms in duration, including 5 ms linear rise and fall ramps. Stimuli were delivered in a pseudorandom order, with deviant tones comprising 15% of all trials and preceded by at least five standard tones. The inter-trial interval was selected from a uniform distribution of between 500 and 600 ms. Participants were seated and listened to the tones while watching a silent movie that was not related to the tone sequences.</p>
</sec>
<sec id="s4g">
<title>EEG Signal Analysis</title>
<p>Data analysis was done using custom-written MATLAB scripts. EEG signals were band-pass filtered between 0.2 and 100 Hz. To remove power line noise, a band-stop filter was applied between 49 and 51 Hz. Movement of the participants triggered all varieties of movement related artifacts in the EEG recordings. The strength of these artifacts depended on a variety of factors: impedance of the electrodes, hair style of participants, gait pattern, and likely others. To reduce data contaminated by excessive movement artifacts, trials with a maximum absolute response amplitude exceeding 100 μV were discarded from further analysis (<xref rid="figS5" ref-type="fig">Figure S5</xref>). Data from each electrode were included in the final analysis only if at least 15 triggers remained after exclusion of triggers with excessive movement artifacts (<xref ref-type="table" rid="tblS1">Table S1</xref>). To compare average response strength across conditions, a 100 ms analysis window was used, centered on the peak of the respective responses: the visuomotor mismatch event recorded at occipital electrodes O1 and O2 (<xref rid="fig2" ref-type="fig">Figures 2E, 2F</xref><bold>, and </bold><xref ref-type="fig" rid="figS4">S4</xref>), the mean visuomotor mismatch response across all electrodes (<xref rid="fig3" ref-type="fig">Figure 3C</xref>), the mean visual response across all electrodes (<xref rid="figS2" ref-type="fig">Figure S2C</xref>) or playback halt response in the 6DOF condition (<xref rid="figS3" ref-type="fig">Figure S3B</xref>). Signal power was compared by calculating the mean squared amplitude within a 0 - 0.5 s analysis window following stimulus onset (<xref rid="fig4" ref-type="fig">Figures 4B</xref> and <xref ref-type="fig" rid="fig5">5E</xref>).</p>
</sec>
<sec id="s4h">
<title>Statistical tests</title>
<p>All statistical analyses were conducted using hierarchical bootstrap (<xref ref-type="bibr" rid="c28">Saravanan et al., 2020</xref>). Bootstrap resampling enables statistical comparisons across conditions without assuming a specific distribution of the EEG data. For analysis in <xref rid="fig1" ref-type="fig">Figures 1</xref><bold>, </bold><xref rid="fig2" ref-type="fig">2</xref><bold>, </bold><xref rid="fig4" ref-type="fig">4</xref><bold>, </bold><xref rid="fig5" ref-type="fig">5</xref><bold>, </bold><xref ref-type="fig" rid="figS3">S3</xref><bold>, and </bold><xref ref-type="fig" rid="figS4">S4</xref>, we averaged signals from electrodes O1 and O2, and treated the result as a single data point per participant. For the analysis shown in <xref rid="fig3" ref-type="fig">Figures 3C, 3E</xref><bold>, and </bold><xref ref-type="fig" rid="figS2">S2C</xref> we included signals from all electrodes and used a nested bootstrap to account for multiple data points originating from the same participant. We first resampled the data with replacement at the level of participants, followed by resampling at the level of electrodes. For each resampled population, we computed the mean response and repeated this procedure 10000 times. The p value was estimated as the fraction of bootstrap samples in which the sample mean violated the tested hypothesis. See <xref ref-type="table" rid="tblS1">Table S1</xref> for all information on number of participants or electrodes used for all analyses shown.</p>
</sec>
</sec>
</body>
<back>
<sec sec-type="supplementary" id="supplementary19">
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption>
<title>Movement onsets result in increases in variance in EEG activity.</title>
<p><bold>(A)</bold> We included only data in which the EEG signals remained below an exclusion threshold of 100 μV. Most of the movement related variance in the EEG activity is likely a movement artifact. Example of an EEG signal (black line) at movement onset that reached exclusion threshold (100 μV). Overlaid is the walking speed of the participant (green line). <bold>(B)</bold> As in <bold>A</bold>, but for an example of an EEG signal at movement onset that did not reach exclusion threshold (100 μV). (<bold>C, D</bold>) As in <bold>A</bold>, but for examples of EEG signals at movement onset with minimal movement contamination.</p>
</caption>
<graphic xlink:href="670295v1_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption>
<title>Visual responses are strongest in occipital cortex.</title>
<p><bold>(A)</bold> Top down view of EEG electrode locations on the head. <bold>(B)</bold> Visual evoked responses measured on electrodes shown in <bold>A</bold>. Solid black lines represent the mean, and shading indicates the SEM across participants. Dashed vertical red line is the onset of the checkerboard inversion. <bold>(C)</bold> Comparison of the response strength of visual evoked potentials measured on electrodes shown in <bold>A</bold>. Average response strength was calculated within a 100 ms window centered on the peak of the average visual response across all electrodes. Boxes mark median, quartiles, and range of data not considered outliers. Each circle represents data from an individual participant. ***: p&lt;0.001, **: p&lt;0.01, *: p&lt;0.05, n.s.: not significant. See <xref ref-type="table" rid="tblS1">Table S1</xref> for all statistical information.</p>
</caption>
<graphic xlink:href="670295v1_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption>
<title>Playback halt responses do not depend on whether coupling is full.</title>
<p><bold>(A)</bold> A subset of participants viewed 6DOF playback (<bold>Methods</bold>). Shown are visual flow playback halt responses recorded from occipital electrodes in these participants. Solid black lines represent the mean, and shading indicates the SEM across participants. Dashed vertical red lines are onset and offset of the visuomotor mismatch. <bold>(B)</bold> Comparison of the response strength to 6DOF and 4DOF playback halts. Average response strength was calculated within a 100 ms analysis window centered on the peak of the playback halt response in the 6DOF condition. Data were collected from four participants; one of them participated in two separate recording sessions. Each circle represents data from individual recording session. Boxes mark median, quartiles, and range of data not considered outliers. n.s.: not significant.</p>
</caption>
<graphic xlink:href="670295v1_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption>
<title>Mismatch and playback halt responses obtained from the same participants.</title>
<p><bold>(A)</bold> Responses to visuomotor mismatches recorded from occipital electrodes (O1 and O2). Solid black line represents the mean, and shading indicates the SEM across participants. The gray shaded areas mark the analysis windows used to quantify response strength in <bold>C</bold>. Dashed vertical red lines are onset and offset of the visuomotor mismatch. As in <xref rid="fig2" ref-type="fig">Figure 2E, F</xref>, but only including data from participants for which we have both closed and open loop data. <bold>(B)</bold> As in <bold>A</bold>, but for visual flow playback halt responses recorded from occipital electrodes. <bold>(C)</bold> Comparison of the response strength to visuomotor mismatch and playback halts (23 participants 4DOF and 3 participants 6DOF). Boxes mark median, quartiles, and range of data not considered outliers. Each data point corresponds to one participant and lines connect mismatch and playback halt responses from the same participant. ***: p&lt;0.001. One data point not shown (Mismatch response, at −9.5 μV). See <xref ref-type="table" rid="tblS1">Table S1</xref> for all statistical information.</p>
</caption>
<graphic xlink:href="670295v1_figS4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5.</label>
<caption>
<title>Examples of rejected and valid trials based on maximum signal amplitude in the visuomotor mismatch paradigm.</title>
<p>(<bold>A</bold>) Example of an EEG response contaminated by an eye blink artifact (arrow). Dashed vertical red lines are onset and offset of the visuomotor mismatch. This trial was removed. (<bold>B, C</bold>) As in <bold>A</bold>, but for examples of EEG responses contaminated by walking artifacts. These trials were removed. (<bold>D, E</bold>) As in <bold>A</bold>, but for an example of an EEG response that met the inclusion criteria (amplitude &lt; 100 μV). (<bold>F</bold>) Histogram of maximum trial amplitudes. The red dashed line marks the threshold for inclusion. 46 trials with amplitudes exceeding 1500 μV are not shown.</p>
</caption>
<graphic xlink:href="670295v1_figS5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Statistics All information on statistical tests used in this manuscript.</title>
<p>We used hierarchical bootstrap (<xref ref-type="bibr" rid="c28">Saravanan et al., 2020</xref>) for all comparisons. We had a total of 46 participants, the numbers in the table indicate the subset of these we could include for each analysis. Note, this differs for electrode location and condition. Exclusion reasons were a) recording too noisy, or b) participant aborted the recording (in the case of playback).</p></caption>
<graphic xlink:href="670295v1_tblS1.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="670295v1_tblS1a.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="utbl1" orientation="portrait" position="float">
<caption><title>Key Resources Table</title></caption>
<graphic xlink:href="670295v1_utbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank all participants for taking part in the study. We thank all members of the Keller lab for discussion and support. We thank Philipp Sterzer for feedback on the manuscript. This project has received funding from the Swiss National Science Foundation (GBK), the Novartis Research Foundation (GBK), and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 865617) (GBK).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Attinger</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name></person-group>, <year>2017</year>. <article-title>Visuomotor Coupling Shapes the Functional Development of Mouse Visual Cortex</article-title>. <source>Cell</source> <volume>169</volume>, <fpage>1291</fpage>-<lpage>1302.e14</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2017.05.023</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>T.-C.</given-names></string-name>, <string-name><surname>Hsieh</surname>, <given-names>M.H.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>Y.-T.</given-names></string-name>, <string-name><surname>Chan</surname>, <given-names>P.-Y.S.</given-names></string-name>, <string-name><surname>Cheng</surname>, <given-names>C.-H.</given-names></string-name></person-group>, <year>2020</year>. <article-title>Mismatch negativity to different deviant changes in autism spectrum disorders: A meta-analysis</article-title>. <source>Clin. Neurophysiol</source>. <volume>131</volume>, <fpage>766</fpage>–<lpage>777</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2019.10.031</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheng</surname>, <given-names>Y.-P.</given-names></string-name>, <string-name><surname>Nordin</surname>, <given-names>A.D.</given-names></string-name></person-group>, <year>2025</year>. <article-title>Effects of Matched and Mismatched Visual Flow and Gait Speeds on Human Electrocortical Spectral Power</article-title>. <source>Brain Sci</source>. <volume>15</volume>, <fpage>531</fpage>. <pub-id pub-id-type="doi">10.3390/brainsci15050531</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chitty</surname>, <given-names>K.M.</given-names></string-name>, <string-name><surname>Lagopoulos</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>R.S.C.</given-names></string-name>, <string-name><surname>Hickie</surname>, <given-names>I.B.</given-names></string-name>, <string-name><surname>Hermens</surname>, <given-names>D.F.</given-names></string-name></person-group>, <year>2013</year>. <article-title>A systematic review and meta-analysis of proton magnetic resonance spectroscopy and mismatch negativity in bipolar disorder</article-title>. <source>Eur. Neuropsychopharmacol</source>. <volume>23</volume>, <fpage>1348</fpage>–<lpage>1363</lpage>. <pub-id pub-id-type="doi">10.1016/j.euroneuro.2013.07.007</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>M.X.</given-names></string-name></person-group>, <year>2017</year>. <article-title>Where Does EEG Come From and What Does It Mean?</article-title> <source>Trends Neurosci</source>. <volume>40</volume>, <fpage>208</fpage>–<lpage>218</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2017.02.004</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Drislane</surname>, <given-names>F.W.</given-names></string-name></person-group>, <year>2007</year>. <chapter-title>Visual Evoked Potentials</chapter-title>, in: <person-group person-group-type="editor"><string-name><surname>Blum</surname>, <given-names>A.S.</given-names></string-name>, <string-name><surname>Rutkove</surname>, <given-names>S.B.</given-names></string-name></person-group> (Eds.), <source>The Clinical Neurophysiology Primer</source>. <publisher-name>Humana Press</publisher-name>, <publisher-loc>Totowa, NJ</publisher-loc>, pp. <fpage>461</fpage>–<lpage>473</lpage>. <pub-id pub-id-type="doi">10.1007/978-1-59745-271-7_25</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dunn</surname>, <given-names>M.A.</given-names></string-name>, <string-name><surname>Gomes</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Gravel</surname>, <given-names>J.</given-names></string-name></person-group>, <year>2008</year>. <article-title>Mismatch Negativity in Children with Autism and Typical Development</article-title>. <source>J. Autism Dev. Disord</source>. <volume>38</volume>, <fpage>52</fpage>–<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1007/s10803-007-0359-3</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>El Hatal de Souza</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pinto</surname>, <given-names>J.D.</given-names></string-name>, <string-name><surname>Mezommo</surname>, <given-names>C.L.</given-names></string-name>, <string-name><surname>Vieira Biaggio</surname>, <given-names>E.P.</given-names></string-name></person-group>, <year>2020</year>. <article-title>Mismatch Negativity in children with Phonological Disorders</article-title>. <source>Int. J. Pediatr. Otorhinolaryngol</source>. <volume>139</volume>, <fpage>110445</fpage>. <pub-id pub-id-type="doi">10.1016/j.ijporl.2020.110445</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fiser</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mahringer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Oyibo</surname>, <given-names>H.K.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>A.V.</given-names></string-name>, <string-name><surname>Leinweber</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name></person-group>, <year>2016</year>. <article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title>. <source>Nat. Neurosci</source>. <volume>19</volume>, <fpage>1658</fpage>–<lpage>1664</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4385</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name></person-group>, <year>2005</year>. <article-title>A theory of cortical responses</article-title>. <source>Philos. Trans. R. Soc. Lond. B. Biol. Sci</source>. <volume>360</volume>, <fpage>815</fpage>–<lpage>836</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garner</surname>, <given-names>A.R.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name></person-group>, <year>2022</year>. <article-title>A cortical circuit for audio-visual predictions</article-title>. <source>Nat. Neurosci</source>. <volume>25</volume>, <fpage>98</fpage>–<lpage>105</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-021-00974-7</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garrido</surname>, <given-names>M.I.</given-names></string-name>, <string-name><surname>Kilner</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Stephan</surname>, <given-names>K.E.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K.J.</given-names></string-name></person-group>, <year>2009</year>. <article-title>The mismatch negativity: A review of underlying mechanisms</article-title>. <source>Clin. Neurophysiol</source>. <volume>120</volume>, <fpage>453</fpage>–<lpage>463</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2008.11.029</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heindorf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name></person-group>, <year>2023</year>. <article-title>Antipsychotic drugs selectively decorrelate long-range interactions in deep cortical layers</article-title>. <source>eLife</source> <volume>12</volume>. <pub-id pub-id-type="doi">10.7554/eLife.86805</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jordan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name></person-group>, <year>2020</year>. <article-title>Opposing Influence of Top-down and Bottom-up Input on Excitatory Layer 2/3 Neurons in Mouse Primary Visual Cortex</article-title>. <source>Neuron</source> <volume>108</volume>, <fpage>1194</fpage>-<lpage>1206.e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.024</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name>, <string-name><surname>Bonhoeffer</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hübener</surname>, <given-names>M.</given-names></string-name></person-group>, <year>2012</year>. <article-title>Sensorimotor mismatch signals in primary visual cortex of the behaving mouse</article-title>. <source>Neuron</source> <volume>74</volume>, <fpage>809</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.040</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name>, <string-name><surname>Hahnloser</surname>, <given-names>R.H.R.</given-names></string-name></person-group>, <year>2009</year>. <article-title>Neural processing of auditory feedback during vocal practice in a songbird</article-title>. <source>Nature</source> <volume>457</volume>, <fpage>187</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1038/nature07467</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name>, <string-name><surname>Mrsic-Flogel</surname>, <given-names>T.D.</given-names></string-name></person-group>, <year>2018</year>. <article-title>Predictive Processing: A Canonical Cortical Computation</article-title>. <source>Neuron</source> <volume>100</volume>, <fpage>424</fpage>–<lpage>435</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.003</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name>, <string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name></person-group>, <year>2024</year>. <article-title>Predictive Processing: A Circuit Approach to Psychosis</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>47</volume>, <fpage>85</fpage>– <lpage>101</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-100223-121214</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kirschstein</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Köhling</surname>, <given-names>R.</given-names></string-name></person-group>, <year>2009</year>. <article-title>What is the source of the EEG?</article-title> <source>Clin. EEG Neurosci</source>. <volume>40</volume>, <fpage>146</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.1177/155005940904000305</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leinweber</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ward</surname>, <given-names>D.R.</given-names></string-name>, <string-name><surname>Sobczak</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Attinger</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name></person-group>, <year>2017</year>. <article-title>A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions</article-title>. <source>Neuron</source> <volume>95</volume>, <fpage>1420</fpage>-<lpage>1432.e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.036</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liska</surname>, <given-names>J.P.</given-names></string-name>, <string-name><surname>Rowley</surname>, <given-names>D.P.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T.T.K.</given-names></string-name>, <string-name><surname>Muthmann</surname>, <given-names>J.-O.</given-names></string-name>, <string-name><surname>Butts</surname>, <given-names>D.A.</given-names></string-name>, <string-name><surname>Yates</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Huk</surname>, <given-names>A.C.</given-names></string-name></person-group>, <year>2024</year>. <article-title>Running modulates primate and rodent visual cortex differently</article-title>. <source>eLife</source> <volume>12</volume>, <elocation-id>RP87736</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.87736</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Näätänen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gaillard</surname>, <given-names>A.W.K.</given-names></string-name>, <string-name><surname>Mäntysalo</surname>, <given-names>S.</given-names></string-name></person-group>, <year>1978</year>. <article-title>Early selective-attention effect on evoked potential reinterpreted</article-title>. <source>Acta Psychol (Amst)</source> <volume>42</volume>, <fpage>313</fpage>–<lpage>329</lpage>. <pub-id pub-id-type="doi">10.1016/0001-6918(78)90006-9</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Näätänen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Paavilainen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rinne</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Alho</surname>, <given-names>K.</given-names></string-name></person-group>, <year>2007</year>. <article-title>The mismatch negativity (MMN) in basic research of central auditory processing: A review</article-title>. <source>Clin. Neurophysiol</source>. <volume>118</volume>, <fpage>2544</fpage>–<lpage>2590</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2007.04.026</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niell</surname>, <given-names>C.M.</given-names></string-name>, <string-name><surname>Stryker</surname>, <given-names>M.P.</given-names></string-name></person-group>, <year>2010</year>. <article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title>. <source>Neuron</source> <volume>65</volume>, <fpage>472</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Toole</surname>, <given-names>S.M.</given-names></string-name>, <string-name><surname>Oyibo</surname>, <given-names>H.K.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name></person-group>, <year>2023</year>. <article-title>Molecularly targetable cell types in mouse visual cortex have distinguishable prediction error responses</article-title>. <source>Neuron</source> <volume>111</volume>, <fpage>2918</fpage>-<lpage>2928.e8</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2023.08.015</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R.P.N.</given-names></string-name>, <string-name><surname>Ballard</surname>, <given-names>D.H.</given-names></string-name></person-group>, <year>1999</year>. <article-title>Predictive coding in the visual cortex: a functional interpretation of some extraclassical receptive-field effects</article-title>. <source>Nat. Neurosci</source>. <volume>2</volume>, <fpage>79</fpage>–<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1038/4580</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saleem</surname>, <given-names>A.B.</given-names></string-name>, <string-name><surname>Ayaz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jeffery</surname>, <given-names>K.J.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K.D.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name></person-group>, <year>2013</year>. <article-title>Integration of visual motion and locomotion in mouse visual cortex</article-title>. <source>Nat. Neurosci</source>. <volume>16</volume>, <fpage>1864</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3567</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saravanan</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Berman</surname>, <given-names>G.J.</given-names></string-name>, <string-name><surname>Sober</surname>, <given-names>S.J.</given-names></string-name></person-group>, <year>2020</year>. <article-title>Application of the hierarchical bootstrap to multi-level data in neuroscience</article-title>. <source>Neurons Behav Data Anal Theory</source> <volume>3</volume>, <ext-link ext-link-type="uri" xlink:href="https://nbdt.scholasticahq.com/article/13927-application-of-the-hierarchical-bootstrap-to-multi-level-data-in-neuroscience">https://nbdt.scholasticahq.com/article/13927-application-of-the-hierarchical-bootstrap-to-multi-level-data-in-neuroscience</ext-link>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmack</surname>, <given-names>K.</given-names></string-name>, Castro, <string-name><given-names>A.G.-C.</given-names> <surname>de</surname></string-name>, <string-name><surname>Rothkirch</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sekutowicz</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rössler</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Haynes</surname>, <given-names>J.-D.</given-names></string-name>, <string-name><surname>Heinz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Petrovic</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name></person-group>, <year>2013</year>. <article-title>Delusions and the Role of Beliefs in Perceptual Inference</article-title>. <source>J. Neurosci</source>. <volume>33</volume>, <fpage>13701</fpage>–<lpage>13712</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1778-13.2013</pub-id></mixed-citation></ref>
    <ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmack</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Rössler</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sekutowicz</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brandl</surname>, <given-names>E.J.</given-names></string-name>, <string-name><surname>Müller</surname>, <given-names>D.J.</given-names></string-name>, <string-name><surname>Petrovic</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name></person-group>, <year>2015a</year>. <article-title>Linking unfounded beliefs to genetic dopamine availability</article-title>. <source>Front Hum Neurosci</source>. <volume>9</volume>. <pub-id pub-id-type="doi">10.3389/fnhum.2015.00521</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmack</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Rothkirch</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Priller</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name></person-group>, <year>2017</year>. <article-title>Enhanced predictive signalling in schizophrenia</article-title>. <source>Hum. Brain Mapp</source>. <volume>38</volume>, <fpage>1767</fpage>–<lpage>1779</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.23480</pub-id></mixed-citation></ref>
    <ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmack</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schnack</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Priller</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name></person-group>, <year>2015b</year>. <article-title>Perceptual instability in schizophrenia: Probing predictive coding accounts of delusions with ambiguous stimuli</article-title>. <source>Schizophr Res Cogn</source> <volume>2</volume>, <fpage>72</fpage>–<lpage>77</lpage>. <pub-id pub-id-type="doi">10.1016/j.scog.2015.03.005</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwartz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Shinn-Cunningham</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Tager-Flusberg</surname>, <given-names>H.</given-names></string-name></person-group>, <year>2018</year>. <article-title>Meta-analysis and systematic review of the literature characterizing auditory mismatch negativity in individuals with autism</article-title>. <source>Neurosci. Biobehav. Rev</source>. <volume>87</volume>, <fpage>106</fpage>–<lpage>117</lpage>.<pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.01.008</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stanley</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Miall</surname>, <given-names>R.C.</given-names></string-name></person-group>, <year>2007</year>. <article-title>Functional activation in parieto-premotor and visual areas dependent on congruency between hand movement and visual stimuli during motor-visual priming</article-title>. <source>NeuroImage</source> <volume>34</volume>, <fpage>290</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.08.043</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Adams</surname>, <given-names>R.A.</given-names></string-name>, <string-name><surname>Fletcher</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Frith</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lawrie</surname>, <given-names>S.M.</given-names></string-name>, <string-name><surname>Muckli</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Petrovic</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Uhlhaas</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Voss</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Corlett</surname>, <given-names>P.R.</given-names></string-name></person-group>, <year>2018</year>. <article-title>The Predictive Coding Account of Psychosis</article-title>. <source>Biol. Psychiatry</source> <volume>84</volume>, <fpage>634</fpage>–<lpage>643</lpage>. <pub-id pub-id-type="doi">10.1016/j.biopsych.2018.05.015</pub-id></mixed-citation></ref>
    <ref id="c36"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Takeuchi</surname>, <given-names>R.F.</given-names></string-name>, <string-name><surname>Sato</surname>, <given-names>A.Y.</given-names></string-name>, <string-name><surname>Ito</surname>, <given-names>K.N.</given-names></string-name>, <string-name><surname>Yokoyama</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Miyata</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Ueda</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Kitajima</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kamaguchi</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Suzuki</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Isobe</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Honda</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Osakada</surname>, <given-names>F.</given-names></string-name></person-group>, <year>2024</year>. <article-title>Posteromedial cortical networks encode visuomotor prediction errors</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2022.08.16.504075</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Talluri</surname>, <given-names>B.C.</given-names></string-name>, <string-name><surname>Kang</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Lazere</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Quinn</surname>, <given-names>K.R.</given-names></string-name>, <string-name><surname>Kaliss</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Yates</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Butts</surname>, <given-names>D.A.</given-names></string-name>, <string-name><surname>Nienborg</surname>, <given-names>H.</given-names></string-name></person-group>, <year>2023</year>. <article-title>Activity in primate visual cortex is minimally driven by spontaneous movements</article-title>. <source>Nat. Neurosci</source>. <volume>26</volume>, <fpage>1953</fpage>–<lpage>1959</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-023-01459-5</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thomas</surname>, <given-names>E.R.</given-names></string-name>, <string-name><surname>Haarsma</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Nicholson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yon</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Press</surname>, <given-names>C.</given-names></string-name></person-group>, <year>2024</year>. <article-title>Predictions and errors are distinctly represented across V1 layers</article-title>. <source>Curr. Biol</source>. <volume>34</volume>, <fpage>2265</fpage>-<lpage>2271.e4</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2024.04.036</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todd</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Michie</surname>, <given-names>P.T.</given-names></string-name>, <string-name><surname>Schall</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Ward</surname>, <given-names>P.B.</given-names></string-name>, <string-name><surname>Catts</surname>, <given-names>S.V.</given-names></string-name></person-group>, <year>2012</year>. <article-title>Mismatch negativity (MMN) reduction in schizophrenia—Impaired prediction-error generation, estimation or salience?</article-title> <source>Int. J. Psychophysiol</source> <volume>83</volume>, <fpage>222</fpage>–<lpage>231</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijpsycho.2011.10.003</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tseng</surname>, <given-names>Y.-J.</given-names></string-name>, <string-name><surname>Nouchi</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Cheng</surname>, <given-names>C.-H.</given-names></string-name></person-group>, <year>2021</year>. <article-title>Mismatch negativity in patients with major depressive disorder: A meta-analysis</article-title>. <source>Clin. Neurophysiol</source>. <volume>132</volume>, <fpage>2654</fpage>–<lpage>2665</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2021.06.019</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Umbricht</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Krljes</surname>, <given-names>S.</given-names></string-name></person-group>, <year>2005</year>. <article-title>Mismatch negativity in schizophrenia: a meta-analysis</article-title>. <source>Schizophr Res</source>. <volume>76</volume>, <fpage>1</fpage>– <lpage>23</lpage>. <pub-id pub-id-type="doi">10.1016/j.schres.2004.12.002</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>von Helmholtz</surname>, <given-names>H.</given-names></string-name></person-group>, <year>1867</year>. <source>Handbuch der physiologischen Optik</source>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zmarz</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>G.B.</given-names></string-name></person-group>, <year>2016</year>. <article-title>Mismatch Receptive Fields in Mouse Visual Cortex</article-title>. <source>Neuron</source> <volume>92</volume>, <fpage>766</fpage>–<lpage>772</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.057</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108941.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5153-4959</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study demonstrates that self-motion strongly affects neural responses to visual stimuli, comparing humans moving through a virtual environment to passive viewing. However, evidence that the modulation is due to prediction is <bold>incomplete</bold> as it stands, since participants may come to expect visual freezes over the course of the experiment. This study bridges human and rodent studies on the role of prediction in sensory processing, and is therefore expected to be of interest to a large community of neuroscientists.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108941.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this paper, the authors wished to determine human visuomotor mismatch responses in EEG in a VR setting. Participants were required to walk around a virtual corridor, where a mismatch was created by halting the display for 0.5s. This occurred every 10-15 seconds. They observe an occipital mismatch signal at 180 ms. They determine the specificity of this signal to visuomotor mismatch by subsequently playing back the same recording passively. They also show qualitatively that the mismatch response is larger than one generated in a standard auditory oddball paradigm. They conclude that humans therefore exhibit visuomotor mismatch responses like mice, and that this may provide an especially powerful paradigm for studying prediction error more generally.</p>
<p>Asking about the role of visuomotor prediction in sensory processing is of fundamental importance to understanding perception and action control, but I wasn't entirely sure what to conclude from the present paradigm or findings. Visuomotor prediction did not appear to have been functionally isolated. I hope the comments below are helpful.</p>
<p>(1) First, isolating visuomotor prediction by contrasting against a condition where the same video stream is played back subsequently does not seem to isolate visuomotor prediction. This condition always comes second, and therefore, predictability (rather than specifically visuomotor predictability) differs. Participants can learn to expect these screen freezes every 10-15 s, even precisely where they are in the session, and this will reduce the prediction error across time. Therefore, the smaller response in the passive condition may be partly explained by such learning. It's impossible to fully remove this confound, because the authors currently play back the visual specifics from the visuomotor condition, but given that the visuomotor correspondences are otherwise pretty stable, they could have an additional control condition where someone else's visual trace is played back instead of their own, and order counterbalanced. Learning that the freezes occur every 10-15 s, or even precisely where they occur, therefore, could not explain condition differences. At a minimum, it would be nice to see the traces for the first and second half of each session to see the extent to which the mismatch response gets smaller. This won't control for learning about the specific separations of the freezes, but it's a step up from the current information.</p>
<p>(2) Second, the authors admirably modified their visual-only condition to remove nausea from 6 df of movement (3D position, pitch, yaw, and roll). However, despite the fact it's far from ideal to have nauseous participants, it would appear from the figures that these modifications may have changed the responses (despite some pairwise lack of significance with small N). Specifically, the trace in S3 (6DOF) and 2E look similar - i.e., comparing the visuomotor condition to the visual condition that matches. Mismatch at 4/5 microvolts in both. Do these significantly differ from each other?</p>
<p>(3) It generally seems that if the authors wish to suggest that this paradigm can be used to study prediction error responses, they need to have controlled for the actions performed and the visual events. This logic is outlined in Press, Thomas, and Yon (2023), Neurosci Biobehav Rev, and Press, Kok, and Yon (2020) Trends Cogn Sci ('learning to perceive and perceiving to learn'). For example, always requiring Ps to walk and always concurrently playing similar visual events, but modifying the extent to which the visual events can be anticipated based on action. Otherwise, it seems more accurately described as a paradigm to study the influence of action on perception, which will be generated by a number of intertwined underlying mechanisms.</p>
<p>More minor points:</p>
<p>(1) I was also wondering whether the authors may consider the findings in frontal electrodes more closely. Within the statistical tests of the frontal electrodes against 0, as displayed in Figure 3c, the insignificance of the effect of Fp2 seems attributable to the small included sample size of just 13 participants for this electrode, as listed in Table S1, in combination with a single outlier skewing the result. The small sample size stands out especially in comparison to the sample size at occipital electrodes, which is double and therefore enjoys far more statistical power. It looks like the selected time window is not perfectly aligned for determining a frontal effect, and also the distribution in 3B looks like responses are absent in more central electrodes but present in occipital and frontal ones. I realise the focus of analysis is on visual processing, but there are likely to be researchers who find the frontal effect just as interesting.</p>
<p>(2) It is claimed throughout the manuscript that the 'strongest predictor (of sensory input) - by consistency of coupling - is self-generated movement'. This claim is going to be hard to validate, and I wonder whether it might be received better by the community to be framed as an especially strong predictor rather than necessarily the strongest. If I hear an ambulance siren, this is an especially strong predictor of subsequent visual events. If I see a traffic light turn red, then yellow, I can be pretty certain what will happen next. Etc.</p>
<p>(3) The checkerboard inversion response at 48 ms is incredibly rapid. Can the authors comment more on what may drive this exceptionally fast response? It was my understanding that responses in this time window can only be isolated with human EEG by presenting spatially polarized events (cf. c1, e.g., Alilovic, Timmermans, Reteig, van Gaal, Slagter, 2019, Cerebral Cortex)</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108941.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigates whether visuomotor mismatch responses can be detected in humans. By adapting paradigms from rodent studies, the authors report EEG evidence of mismatch responses during visuomotor conditions and compare them to visual-only stimulation and mismatch responses in other modalities.</p>
<p>Strengths:</p>
<p>(1) The authors use a creative experimental design to elicit visuomotor mismatch responses in humans.</p>
<p>(2) The study provides an initial dataset and analytical framework that could support future research on human visuomotor prediction errors.</p>
<p>Weaknesses:</p>
<p>(1) Methodological issues (e.g., volume conduction, channel selection, lack of control for eye movements) make it difficult to confidently attribute the observed mismatch responses to activity in visual cortical regions.</p>
<p>(2) A very large portion of the data was excluded due to motion artefacts, raising concerns about statistical power and representativeness. The criteria for trial inclusion and the number of accepted trials per participant appear arbitrary and not justified with reference to EEG reliability standards.</p>
<p>(3) The comparison across sensory modalities (e.g., auditory vs. visual mismatch responses) is conceptually interesting, but due to the choice of analyzing auditory mismatch responses over occipital channels, it has limited interpretability.</p>
<p>The authors successfully demonstrate that visuomotor mismatch paradigms can, in principle, be applied in human EEG. However, due to the issues outlined above, the current findings are relatively preliminary. If validated with improved methodology, this approach could significantly advance our understanding of predictive processing in the human visual system and provide a translational bridge between rodent and human work.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108941.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Solyga, Zelechowski, and Keller present a concise report of an innovative study demonstrating clear visuomotor mismatch responses in ambulating humans, using a mobile EEG setup and virtual reality. Human subjects walked around a virtual corridor while EEGs were recorded. Occasionally, motion and visual flow were uncoupled, and this evoked a mismatch response that was strongest in occipitally placed electrodes and had a considerable signal-to-noise ratio. It was robust across participants and could not be explained by the visual stimulus alone.</p>
<p>Strengths:</p>
<p>This is an important extension of their prior work in mice, and represents an elegant translation of those previous findings to humans, where future work can inform theories of e.g., psychiatric diseases that are believed to involve disordered predictive processing. For the most part, the authors are appropriately circumspect in their interpretations and discussions of the implications. I found the discussion of the polarity differences they found in light of separate positive and negative prediction errors, intriguing.</p>
<p>Weaknesses:</p>
<p>The primary weaknesses rest in how the results are sold and interpreted.</p>
<p>Most notably, the interpretation of the results of the comparison of visuomotor mismatches to the passive auditory oddball induced mismatch responses is inappropriate, as suboptimal electrode choices, unclear matching of trial numbers, and other factors. To clarify, regarding the auditory oddball portion in Figure 5, the data quality is a concern for the auditory ERPs, and the choice of Occipital electrodes is a likely culprit. Typically, auditory evoked responses are maximal at Cz or FCz, although these contacts don't seem to be available with this setup. In general, caution is warranted in comparing ERP peaks between two different sensory modalities - especially if attention is directed elsewhere (to a silent movie) during one recording and not during the other. The authors discuss this as a purely &quot;qualitative&quot; comparison in the text, which is appreciated, and do acknowledge the limitations within the results section, but the figure title and, importantly, the abstract set a different tone. At least, for comparisons between auditory mismatch and visuomotor mismatch, trial numbers need to be equated, as ERP magnitude can be augmented by noise (which reduces with increased numbers of trials in the average). And more generally, the size of the mismatch event at the scalp does not scale one-to-one with the size at the level of the neural tissue. One can imagine a number of variables that impact scalp level magnitudes, which are orthogonal to actual cortex-level activation - the size, spread, and polarity variance of the activated source (which all would diminish amplitude at the scalp due to polyphasic summation/cancelation). The variance of phase to a stimulus across trials (cross trial phase locking) vs magnitude of underlying power - the former, in theory, relates to bottom-up activity and the latter can reflect feedback (which has more variability in time across trials; the distance of the scalp electrode from the activated tissue (which, for the auditory system, would be larger (FCz to superior temporal gyrus) than for the visual system (O1 to V1/2)). None of this precludes the inclusion of the auditory mismatch, which is a strength of the study, but interpretations about this supporting a supremacy of sensory-motor mismatch - regardless of validity - are not warranted. I would recommend changing the way this is presented in the abstract.</p>
<p>Otherwise, the data are of adequate quality to derive most of their conclusions.</p>
<p>The authors claim that the mismatch responses emanate from within the occipital cortex, but I would require denser scalp coverage or a demonstration of consistent impedances across electrodes and across subjects to make conclusions about the underlying cortical sources (especially given the latencies of their peaks). In EEG, the distribution of voltage on the scalp is, of course, related to but not directly reflective of the distribution of the underlying sources. The authors are mostly careful in their discussion of this, but I would strongly recommend changing the work choice of &quot;in occipital cortex&quot; to &quot;over occipital cortex&quot; or even &quot;posteriorly distributed&quot;. Even with very dense electrode coverage and co-registration to MRIs for the generation of forward models that constrain solutions, source localization of EEG signals is very challenging and not a simple problem. Given the convoluted and interior nature of human V1, the ability to reliably detect early evoked responses (which show the mismatch in mouse models) at the scalp in ERP peaks is challenging - especially if one is collapsing ERPs across subjects. And - given the latency of the mismatch responses, I'd imagine that many distributed cortical regions contribute to the responses seen at the scalp.</p>
<p>I think that Figure 3C, but as a difference of visual mismatch vs halting flow alone (in the open loop) might be additionally informative, as it clarifies exactly where the pure &quot;mismatch&quot; or prediction error is represented.</p>
<p>As a suggestion, the authors are encouraged to analyse time-frequency power and phase locking for these mismatch responses, as is common in much of the literature (see Roach et al 2008, Schizophrenia Bulletin). This is not to say that doing so will yield insights into oscillations per se, but converting the data to the time-frequency domain provides another perspective that has some advantages. It fosters translations to rodent models, as ERP peaks do not map well between species, but e.g., delta-theta power does (see Lee et al 2018, Neuropsychopharmacology; Javitt et al 2018, Schizophrenia research; Gallimore et al 2023, Cereb Ctx). Further, ERP peaks can be influenced by the actual neuroanatomy of an individual (especially for quantifying V1 responses). Time frequency analyses may aid in interpreting the &quot;early negative deflection with a peak latency of 48 ms &quot; finding as well.</p>
<p>Finally, the sentence in the abstract that this paradigm &quot; can trigger strong prediction error responses and consequently requires shorter recording 20 times would simplify experiments in a clinical setting&quot; is a nice setup to the paper, but the very fact that one third of recordings had to be removed due to movement artifact, and that hairstyle modulates the recording SnR, is reason that this paradigm, using the reported equipment, may have limited clinical utility in its current form. Further, auditory oddball paradigms are of great clinical utility because they do not require explicit attention and can be recorded very quickly with no behavioral involvement of a hospitalized patient. This should be discussed, although it does not detract from the overall scientific importance of the study. The authors should reconsider putting this statement in the abstract.</p>
</body>
</sub-article>
</article>