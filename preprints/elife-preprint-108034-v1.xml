<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108034</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108034</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108034.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>How attention simplifies mental representations for planning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Castanheira</surname>
<given-names>Jason da Silva</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<email xlink:href="mailto:j.castanheira@ucl.ac.uk">j.castanheira@ucl.ac.uk</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shea</surname>
<given-names>Nicholas</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fleming</surname>
<given-names>Stephen M</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A3">3</xref>
<xref ref-type="aff" rid="A4">4</xref>
</contrib>
<aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Department of Experimental Psychology, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04cw6st05</institution-id><institution>Institute of Philosophy, School of Advanced Study, University of London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Max Planck UCL Centre for Computational Psychiatry and Ageing Research, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sdtdd95</institution-id><institution>Canadian Institute for Advanced Research (CIFAR), Brain, Mind and Consciousness Program</institution></institution-wrap>, <city>Toronto</city>, <country country="CA">Canada</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Faivre</surname>
<given-names>Nathan</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6011-4921</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>Centre National de la Recherche Scientifique</institution>
</institution-wrap>
<city>Grenoble</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8451-0523</contrib-id><role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2025-09-09">
<day>09</day>
<month>09</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108034</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2025-06-23">
<day>23</day>
<month>06</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-06-11">
<day>11</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2506.09520"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Castanheira et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Castanheira et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108034-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Human planning is efficient—it frugally deploys limited cognitive resources to accomplish difficult tasks—and flexible—adapting to novel problems and environments. Computational approaches suggest that people construct simplified mental representations of their environment, balancing the complexity of a task representation with its utility. These models imply a nested optimisation in which planning shapes perception, and perception shapes planning - but the perceptual and attentional mechanisms governing how this interaction unfolds remain unknown. Here, we harness virtual maze navigation to characterise how spatial attention controls which aspects of a task representation enter subjective awareness and are available for planning. We find that spatial proximity governs which aspects of a maze are available for planning, and that when task-relevant information follows natural (lateralised) contours of attention, people can more easily construct simplified and useful maze representations. This influence of attention varies considerably across individuals, explaining differences in people’s task representations and behaviour. Inspired by the ‘spotlight of attention<italic>’</italic> analogy, we incorporate the effects of visuospatial attention into existing computational accounts of value-guided construal. Together, our work bridges computational perspectives on perception and decision-making to better understand how individuals represent their environments in aid of planning.</p>
</abstract>
<abstract abstract-type="summary">
<title>Significance statement</title>
<p>Humans have an impressive ability to plan. Theoretical models in computer science propose that instead of using all the available information in a scene, a decision-maker should form a simplified mental representation of their environment over which they plan. However, little is known about how perceptual and attentional processes shape the planning process in humans. We find that people form simplified mental representations in line with the natural contours of spatial attention, whereby information limited to a visual hemifield is more readily available for planning, like a spotlight illuminating a part of the environment. We develop a novel computational model of the effects of attention on planning and characterise systematic variation between individuals in how they simplify their mental representations.</p>
</abstract>
<kwd-group>
<title>Keywords</title>
<kwd>Planning</kwd>
<kwd>Attention</kwd>
<kwd>Computational model</kwd>
<kwd>Cognitive neuroscience</kwd>
<kwd>Psychology</kwd>
<kwd>Visuospatial attention</kwd>
<kwd>Computational psychology</kwd>
</kwd-group>
<funding-group>
<award-group>
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h531d29</institution-id>
<institution>Natural Sciences and Engineering Research Council of Canada</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name>
<surname>Castanheira</surname>
<given-names>Jason da Silva</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group>
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/029chgv08</institution-id>
<institution>Wellcome Trust</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.35802/206648</award-id>
<principal-award-recipient>
<name>
<surname>Fleming</surname>
<given-names>Stephen M</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group>
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03wnrjx87</institution-id>
<institution>Royal Society</institution>
</institution-wrap>
</funding-source>
<award-id>206648/Z/17/Z</award-id>
<principal-award-recipient>
<name>
<surname>Fleming</surname>
<given-names>Stephen M</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group>
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>101043666</award-id>
<principal-award-recipient>
<name>
<surname>Fleming</surname>
<given-names>Stephen M</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p>Humans have an impressive ability to plan. We are able to model the world, simulate potential outcomes, and select among possible courses of action. Take, for example, your first trip to London. You want to visit Buckingham Palace despite being jet-lagged. Looking at a map of the underground, you’re overwhelmed with information but need to make a plan. How do you solve this problem? Even simple decisions like this involve many potential actions and outcomes, making it impossible to systematically evaluate every possible option, especially given limited cognitive resources<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>. Explaining how people plan efficiently and flexibly under these constraints is a long-standing challenge in human and machine intelligence<sup><xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c8">8</xref></sup>.</p>
<p>Theories of human problem-solving conceptualize planning as a search through a ‘<italic>decision tree’</italic> of all potential actions and their outcomes<sup><xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. In our example, an individual may first list all possible tube stations within walking distance and then evaluate which action sequence will get them closer to their destination. Previous work proposes different algorithmic strategies for how an agent efficiently searches over a complex <italic>decision tree</italic>. These strategies include ignoring low-value actions (i.e., pruning)<sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c11">11</xref>–<xref ref-type="bibr" rid="c13">13</xref></sup>, limiting how far in the future one might search (i.e., depth)<sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c14">14</xref>–<xref ref-type="bibr" rid="c16">16</xref></sup>, or relying on previously learnt strategies (i.e., habits)<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>.</p>
<p>This previous work, however, largely assumes that a decision-maker has a fixed representation of the problem. When planning involves constructing and evaluating multiple multi-step trajectories within a <italic>decision tree</italic>, the computational burden increases with the complexity of the representation of the problem space. Consider planning in a two-dimensional spatial grid, for example. A fine-grained grid presents many choice points about which way to turn. A coarser-grained grid presents fewer choice points. Since the number of branches is a multiplicative function of the number of choice points, a simplified representation of the task space, if chosen appropriately, can have a profound effect on reducing the computational demands of planning.</p>
<p>One elegant approach to forming such a simplified representation is to adaptively select the granularity of information required to successfully complete the task<sup><xref ref-type="bibr" rid="c18">18</xref></sup>, known as value-guided construal (VGC). Under VGC, a cognitively limited decision-maker selects a manageable subset of information over which to plan—i.e., a task representation— balancing utility and complexity<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. In our example, the VGC algorithm predicts that an individual would plan over a few relevant tube lines rather than planning over all possible routes.</p>
<p>In previous work, Ho and colleagues discovered that people’s awareness of, and memory for, obstacles in a maze varies in line with the predictions of a VGC model. The VGC model implies two nested optimisations - an outer loop of construal, and an inner loop that runs a plan conditional on a particular task representation. The VGC model is a normative model and remains agnostic as to the cognitive mechanisms controlling the construal. In particular, the perceptual and attentional mechanisms governing <italic>how</italic> information is selected to become part of a task representation remain unknown. Initiating such a nested computation plausibly rests on inductive biases - general principles that a perceptual system can apply to select task-relevant information, before refining it as part of the planning process <sup><xref ref-type="bibr" rid="c19">19</xref></sup>. Selective attention is proposed as one general mechanism by which the brain selects relevant information, either voluntarily (endogenous) or reflexively (exogenous)<sup><xref ref-type="bibr" rid="c20">20</xref>–<xref ref-type="bibr" rid="c25">25</xref></sup>.</p>
<p>Previous studies have demonstrated that attention guides the selection of particular features of the environment to support reinforcement learning<sup><xref ref-type="bibr" rid="c26">26</xref></sup>. However, it remains unknown whether and how attention shapes value-guided construal “on the fly” during planning. For instance, one possibility is that forming a simplified task representation is a “late” passive side-effect of the planning process - a tendency to focus on what we are thinking about. Alternatively, VGC may reflect an “early” selection of perceptual information, perhaps based on a rapid feedforward sweep of perceptual input<sup><xref ref-type="bibr" rid="c27">27</xref></sup>. These alternatives echo classic debates between early and late selection models of attention<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>, but now situated within the broader landscape of computational accounts of planning. More generally, despite the wealth of literature on attention, and pioneering efforts to incorporate attentional constraints into models of decision-making<sup><xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c26">26</xref></sup>, we lack a basic understanding of how attention influences planning.</p>
<p>To make progress on this question, we examined the role of visuospatial attention on how people construct simplified task representations across three experiments in human participants. We build on previous work using maze navigation to provide a rich readout of people’s current task representations. We predicted that if visuospatial attention is guiding the formation of task representations, the construal process will be constrained by inductive biases characteristic of attentional selection. For instance, previous work has illustrated how attentional selection is biased by the spatial context in which information is presented: presenting distractors alongside task-relevant stimuli makes attentional selection more challenging<sup><xref ref-type="bibr" rid="c30">30</xref>–<xref ref-type="bibr" rid="c32">32</xref></sup>. Attention, in this case, spills over to the neighbouring stimuli. These findings align with the metaphorical attentional spotlight, which stipulates that the focus of visual attention can move around the visual field but is limited in spatial extent<sup><xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>. According to this model, individuals can, for example, orient their attention preferentially to a single hemifield—i.e., lateralizing—which is enabled by a hemispheric lateralization of alpha power over posterior cortex<sup><xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c38">38</xref></sup>.</p>
<p>We harness these classical signatures of attentional selection to characterise how attention shapes planning. First, we demonstrate “<italic>attentional overspill</italic>”: participants preferentially incorporate task-irrelevant information into their task representation when it is presented in spatial proximity to task-relevant information. Second, we observe that <italic>attentional overspill</italic> is reduced when task-relevant information is lateralised to a single hemifield, allowing participants to more effectively form optimal task representations. Finally, we extend the VGC model to incorporate visuospatial attention as a key psychological mechanism for constructing simplified task representations. Together, our findings furnish a computational account of how attention and perception guide simplified representations in the service of planning.</p>
</sec>
<sec id="s2" sec-type="results">
<title>Results</title>
<p>To examine the role of visuospatial attention in planning, we relied on a previously developed maze navigation paradigm in which participants solved 2-D mazes<sup><xref ref-type="bibr" rid="c18">18</xref></sup>, avoiding obstacles obstructing their path (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, left panel). On every trial, participants reported their awareness of specific obstacles (see Methods for details). The level of awareness attached to different obstacles provides a read-out of an individual’s task representation while solving a particular maze. We first reanalyzed the data presented by Ho and colleagues (2022)<sup><xref ref-type="bibr" rid="c18">18</xref></sup> to examine the role of spatial attention in building task representations (datasets Ho 1 and 2). In a new experiment (dataset dSC 1), we designed novel mazes to test the effects of lateralization of attention in enabling efficient planning (see Methods &amp; <xref ref-type="table" rid="tbl1">Table S1</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1</label>
<caption><title>Spatial attention shapes task representations.</title>
<p>(a) Schematic of the maze navigation task. Participants fixated at the start of each trial, after which a maze was presented, which they were asked to navigate. Maze stimuli either remained on the screen during navigation (left panel; <italic>concurrent planning experiments</italic>) or were removed before navigation (right panel; <italic>upfront planning experiments</italic>). Once participants finished navigating the maze, they were asked to report their awareness of each obstacle in the maze.</p>
<p>(b) Left panel: schematic of the analysis pipeline. An example maze is shown where seven obstacles (plotted in orange) are presented on every trial according to pre-defined mazes. Participants report their awareness of every obstacle at the end of each trial (middle maze). The VGC model predicts which obstacles in a maze will likely be included in participants’ task representation (right maze). We use participants’ awareness reports to test the influence of neighbouring obstacles on the probe obstacle (presented in green). We compute the influence of neighbouring obstacles (in grey) on participants’ awareness of the probed obstacle (in green). Right panel: Results of the ranked regression model for dataset Ho 1. We observed that obstacles closest to the probed item (rank 1 &amp; 2) positively impact awareness reports. In contrast, obstacles furthest from the probed item negatively impact awareness reports (rank 5 &amp; 6).</p>
<p>(b) Left panel: The effect of neighbouring obstacles on task representations varied across participants (each represented by a point). Right Panel: Inter-individual differences in the attentional effects correlate with the sparsity of participants’ representations. Participants who showed the greatest influence of neighbouring obstacles (more negative slopes), showed the simplest representations (greatest variance in awareness reports).</p></caption>
<graphic xlink:href="2506.09520v1_fig1.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<sec id="s2-1">
<title>A spotlight of attention influences task representations</title>
<p>We hypothesized that spatial attention would control which items are included in a task representation<sup><xref ref-type="bibr" rid="c30">30</xref>–<xref ref-type="bibr" rid="c32">32</xref></sup>. Specifically, we hypothesised that participants would deviate from the predictions of the VGC model and become distracted by task-irrelevant obstacles when they are presented in spatial proximity to task-relevant obstacles. To evaluate these predictions, we first computed the distance between a probe obstacle and every other obstacle in the maze. Second, we ranked the obstacles from the closest to the furthest from the probed item. Using the ranked obstacles, we trained a linear regression model to predict participants’ awareness of the probed obstacle (in green) from their awareness of the remaining obstacles (in grey; <xref ref-type="fig" rid="fig1">Figure 1b</xref>).</p>
<p>Critically, we observed a significant effect of spatial context on task representations - an effect which is not predicted by the normative VGC model. Participants’ awareness of a particular obstacle was positively predicted by the awareness of its close neighbours (Pi = 0.26, SE = 0.01,95% CI [0.25, 0.28]; β<sub>2</sub> = 0.29, SE = 0.01,95% CI [0.27, 0.30]), whereas awareness of its furthest neighbours negatively predicted participant reports (β<sub>5</sub> = −0.13, SE = 0.01, 95% CI [-0.15, −0.12]; β<sub>6</sub> = −0.13, SE = 0.01, 95% CI [-0.15, −0.12]; see <xref ref-type="table" rid="tbl2">Table S2</xref>). In other words, the spatial context of an obstacle predicted whether it would be included in a simplified task representation – akin to a diffuse attentional spotlight which filters which aspects of the maze are available for planning. This effect remained significant for both task-relevant and task-irrelevant obstacles, and after controlling for the predictions of the VGC model (<xref ref-type="fig" rid="fig10">Figure S7</xref> &amp; <xref ref-type="table" rid="tbl3">Table S3</xref>, respectively). We observed the same effect in a separate experiment where participants planned their route upfront before navigating the mazes (i.e., dataset Ho 2, see <xref ref-type="table" rid="tbl4">Table S4</xref> &amp; <xref ref-type="table" rid="tbl5">S5</xref>). Finally, we replicated this pattern of results in our in-person experiment: closest neighbours positively predicted the awareness of an obstacle (β<sub>1</sub> = 0.19, SE = 0.007, 95% CI [0.18, 0.21]), whereas furthest neighbours negatively predicted participants’ reports (β<sub>3</sub> = −0.10, SE = 0.01, 95% CI [-0.11, −0.08]; β<sub>4</sub> = −0.26, SE = 0.007, 95% CI [-0.27, −0.25]; β<sub>5</sub> = −0.29, SE = 0.007, 95% CI [-0.30, −0.27]; see <xref ref-type="table" rid="tbl6">Table S6</xref> &amp; <xref ref-type="table" rid="tbl7">S7</xref> and <xref ref-type="fig" rid="fig14">Figure S11</xref>).</p>
<p>Next, we explored whether the influence of neighbouring obstacles on task representations varied across individuals. To do so, we fit the regression model described above to quantify each participant’s attentional spillover, and quantified the linear slope of the resulting beta coefficients. Negative slopes indicate a significant effect of attentional spillover on task representation. The influence of attention varied considerably across participants: while on average, participants’ task representations were influenced by attention (mean effect = −0.08; s.d. = 0.04), a subset of participants showed minimal influence of attention on their task representation (i.e., flat slopes; <xref ref-type="fig" rid="fig1">Figure 1c</xref>).</p>
<p>We hypothesized that participants with the largest attention effects (i.e., most negative slopes) would also show sparser task representation (i.e., a “spotlight of attention” which is focused only on a subset of obstacles). To test this, we computed the sparsity of participants’ task representations by estimating the variance of their awareness reports, with higher variance indexing those participants who report being very aware of some obstacles and unaware of others. In line with our hypothesis, we observed that participants who were most influenced by neighbouring obstacles also showed sparser task representations (dataset Ho 1: ρ = −0.35, p&lt; 0.001 ; dataset Ho 2: ρ = −0.49, p&lt; 0.001 ; dataset dSC 1: ρ = −0.51, p&lt; 0.01; see <xref ref-type="fig" rid="fig14">Figure S11</xref>). To address concerns of overfitting, we tested whether the spatial attention effects observed in a lateralized set of mazes generalized to task representations of non-lateralized mazes and vice versa (dataset dSC 1). We observed that inter-individual differences in spatial attention effects in one condition predicted the sparsity of task representations in the other (ρ = −0.48, p&lt; 0.01; ρ = −0.42, p&lt; 0.05).</p>
</sec>
<sec id="s2-2">
<title>Attentional limits constrain the optimality of task representations</title>
<p>Prior psychological research indicates that attention can be efficiently allocated to a “hemifield” of visual space - with information being preferentially processed when presented in the attended hemifield<sup><xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c41">41</xref></sup>. Building on this work, we hypothesized that participants would select task-relevant information with greater ease - constructing task representations more closely aligned with the VGC model - when task-relevant information is spatially confined to a visual hemifield (i.e., presented unilaterally).</p>
<p>To test this hypothesis, we derived a measure of task-relevant lateralization inspired by the attention literature<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup> (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Specifically, we separated maze stimuli across the vertical meridian and computed the ratio of task-relevant information presented on the left versus right side. For example, the maze shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref> has twice the amount of task-relevant information presented in the left hemifield than in the right (lat. Index= 1/3). A lateralization index of 0.0 indicates that both hemifields contain equal amounts of taskrelevant information (i.e., non-lateralized). We used this task-relevance lateralization index as a moderator in a hierarchical linear regression model to test whether participants’ awareness reports were better predicted by the original VGC model in mazes showing the greatest lateralization of task-relevant information.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2</label>
<caption><title>Lateralization of task-relevant information affects task representations.</title>
<p>(a) For each maze, we computed a horizontal meridian and a vertical meridian lateralization index. This index reflects whether task-relevant information is lateralized to a hemifield. In the example plotted, there is more task-relevant information presented on the left than on the right of the maze, therefore this would correspond to a moderate level of vertical meridian (i.e., left vs right) lateralization.</p>
<p>(b) Density plots of the reported awareness of obstacles on the basis of whether the value-guided construal (VGC) model predicted them to be task-relevant (≥0.5; in orange) or task-irrelevant (&lt; 0.5; in grey). Participants were more likely to be aware of obstacles predicted as task-relevant. We split maze stimuli based into terciles based on the degree to which task-relevant information was presented preferentially to one hemifield (x-axis). The leftmost plots are mazes where task-relevant information is presented on both hemifields. In contrast, the rightmost plot depicts mazes with the largest lateralization. We observed that the awareness reports of participants become increasingly aligned to the VGC model’s predictions as lateralization increases.</p>
<p>(c) Scatter plot of the effect of maze lateralization on the relationship between the value-guided model and participants’ awareness of obstacles. We observed a significant vertical meridian lateralization effect whereby participants’ awareness reports were more strongly aligned with the VGC model’s predictions when task-relevant information was presented unilaterally in all datasets.</p></caption>
<graphic xlink:href="2506.09520v1_fig2.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>In line with our hypothesis, we observed a significant moderation effect whereby the greater the lateralization of task-relevant information across the vertical meridian, the better the original VGC model was at predicting participants’ awareness reports (β<sub>interaction</sub> = 0.01, SE = 2.65*10<sup>−3</sup>, 95% CI [0.01, 0.02], <italic>ρ</italic><sub>FDR</sub>&lt; 0.001; <xref ref-type="fig" rid="fig2">Figure 2c</xref> left panel &amp; <xref ref-type="table" rid="tbl8">Table S8</xref>). We replicated these findings with the data collected in dataset Ho 2<sup><xref ref-type="bibr" rid="c18">18</xref></sup> (<italic>ρ</italic><sub>FDR</sub>&lt; 0.01; see <xref ref-type="table" rid="tbl9">Table S9</xref>). These results indicate that participants’ task representations are more closely aligned with the ideal observer (i.e., the original-VGC model) when task-relevant information is presented unilaterally.</p>
<p>In our new dataset (dSC 1), we designed novel maze stimuli to validate these lateralised effects of attention while addressing some limitations of previous experiments (see Methods). We again observed that lateralization of task-relevant information impacted participants’ awareness reports. Participants were less aware of task-irrelevant stimuli on trials where the lateralization of task-relevant information was larger (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) and we replicated the moderation effect of information lateralization on the extent to which the original VGC model captured participants’ awareness reports (β<sub>interaction</sub> = 0.01, SE = 2.65*10<sup>−3</sup>, 95% CI [0.01, 0.02], <italic>p</italic>&lt; 0.001; <xref ref-type="fig" rid="fig2">Figure 2c</xref> &amp; <xref ref-type="table" rid="tbl10">Table S10</xref>).</p>
<p>In contrast with our observations of consistent and strong attentional effects relative to the vertical meridian, effects relative to the horizontal meridian (superior vs. inferior) were inconsistent across experiments. Specifically, we observed a significant moderation effect in dataset Ho 2 (α<sub>interaction</sub> = 0.01, SE = 2.85*10<sup>−3</sup>, 95% CI [0.00, 0.01], <italic>p</italic><sup>FDR</sup>&lt; 0.05; see <xref ref-type="table" rid="tbl9">Table S9</xref>), but not in dataset Ho 1, and the moderation effect was negative rather than positive in dataset dSC 1 (β<sub>interaction</sub> = −0.01, SE = 2.22*10<sup>−3</sup>, 95% CI [-0.01, 0.00], <italic>p</italic>&lt; 0.05). Both of these effects became insignificant after accounting for nuisance covariates (see <xref ref-type="table" rid="tbl14">Table S14</xref> &amp; <xref ref-type="table" rid="tbl15">S15</xref>).</p>
</sec>
<sec id="s2-3">
<title>Attentional spotlight model of task representations</title>
<p>Taken together, our results corroborate a critical role for visuospatial attention in constructing task representations. Notably, these filtering effects of attention on value-guided construal are not currently part of the original VGC framework proposed by Ho and colleagues. In what follows we explicitly incorporate the influence of a spotlight of attention into the original VGC model to formulate the spotlight-VGC model<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup>.</p>
<p>To achieve this, we computed the predictions of the existing VGC model for each obstacle’s task relevance in a given maze, and averaged these predictions within an attentional spotlight of 3 squares (<xref ref-type="fig" rid="fig3">Figure 3a</xref> &amp; <xref ref-type="table" rid="tbl8">S8</xref>, see Methods for details). We depict the effects of this spatial spotlight in <xref ref-type="fig" rid="fig3">Figure 3a</xref>: task-irrelevant stimuli (plotted in grey; see middle left obstacle) neighbouring task-relevant obstacles (plotted in orange) become more task-relevant, whereas task-relevant information becomes less relevant when surrounded by task-irrelevant information (see bottom right orange obstacle). We hypothesized that this spotlight-VGC model would predict participants’ reports better than the original VGC model, which does not account for spatial attention.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3</label>
<caption><title>A VGC model augmented with an attentional spotlight model predicts participants’ task representations.</title>
<p>(a) Schematic of the attentional spotlight model. Inspired by the spotlight of attention analogy, we recompute an obstacle’s probability of being included in a task representation as the weighted average of its neighbours. We first search for all neighbours of obstaclei that are w squares away. We then compute <italic>P</italic>(Obstacle<sub><italic>i</italic></sub>) as the weighted average of obstaclei and its neighbours. This generates more graded model predictions (far right panel).</p>
<p>(b) Left panel: Each row represents a different example maze stimulus. The left column depicts the original VGC model prediction <italic>P(Obstacle<sub>i</sub>)</italic> for every obstacle in the example maze. The middle column shows the attentional-spotlight model prediction for every obstacle. Obstacles that were considered task-relevant (deep orange) in the original model become less important when surrounded by task-irrelevant information (grey obstacles). The right column shows the participants’ average awareness of each obstacle in the example mazes. Right panel: Scatter plot of the linear relationship between participants’ awareness reports of obstacles and model predictions (original-VGC in green and the spotlight-VGC model in orange) for dataset Ho 1. The latter fits participants’ reports better than the original VGC model.</p>
<p>(c) Scatter plot of the linear relationship between participants’ awareness reports of obstacles and model predictions (original VGC in green and the spotlight-VGC model in orange) for dataset dSC 1 separately for non-lateralized (left panel) and lateralized mazes (right panel). Although both models fit participants’ awareness reports better for lateralized mazes, the advantage of the spotlight model over the original model (better model fit / lower BIC) was observed only in non-lateralized mazes.</p></caption>
<graphic xlink:href="2506.09520v1_fig3.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<p>In line with this hypothesis, we observed that the spotlight-VGC model predicted participants’ awareness reports better than the original VGC model in all three datasets (dataset Ho 1: ΔBIC= 84.63; Ho 2: ΔBIC= 203.43; dSC 1: ΔBIC= 70.72; see <xref ref-type="fig" rid="fig3">Figure 3b</xref> right panel). For dataset dSC 1, we observed a significant improvement in model fit for non-lateralized maze stimuli (ΔBIC= 161.93) but failed to find any improvement when maze stimuli were lateralized (ΔBIC= −42.02; see <xref ref-type="fig" rid="fig3">Figure 3c</xref>). These findings dovetail with the previously discussed moderation effects, and suggest that the spotlight-VGC model is particularly useful in improving our ability to explain human behaviour in situations when attentional filtering is more complex.</p>
</sec>
<sec id="s2-4">
<title>Sensitivity analyses</title>
<p>We conducted a series of control analyses to verify the robustness of our experimental results. First, we verified that the spatial proximity effect (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) was not driven solely by the spatial smoothness of participants’ awareness reports by conducting null permutation tests (grey line, <xref ref-type="fig" rid="fig1">Figure 1b</xref>). For each maze stimulus, we permuted the rank of the neighbouring obstacles. We then fit the same linear model to assess the effect of spatial context on task representations. This procedure was repeated 1000 times to generate a null distribution of beta coefficients. The resulting null distribution showed no discernible effect of spatial context. Second, we used null permutation tests to verify that the improved fit of the spotlight-VGC model was not driven by greater spatial smoothness of the model predictions (see <xref ref-type="fig" rid="fig15">Supplemental Figure 12</xref>). Third, we assessed whether nuisance covariates could explain the moderation effects we observed. Specifically, we added the distance from the goal, starting location, center walls, and fixation as nuisance covariates in our hierarchical regression models. Maze lateralization remained a significant moderator of the relationship between the original VGC model and participants’ awareness reports after controlling for these covariates (see <xref ref-type="table" rid="tbl11">Table S11</xref>–<xref ref-type="table" rid="tbl13">13</xref>). This was not the case, however, for lateralization effects along the horizontal meridian (see <xref ref-type="table" rid="tbl14">Table S14</xref>–<xref ref-type="table" rid="tbl15">15</xref>).</p>
<p>Finally, we sought to verify that the lateralization effects we observed were not driven by a change in eye movement patterns. For dataset dSC 1, we continuously tracked the position of participants’ gaze. We explicitly instructed participants to maintain central fixation while planning (see Methods for details) and removed the obstacles from the screen after 6 seconds. This allowed us to verify that greater awareness of obstacles was not driven by longer fixation times. We confirmed that participants maintained central fixation on both lateralized and non-lateralized maze stimuli in most trials (see <xref ref-type="fig" rid="fig16">Figure S13</xref>). Excluding trials where participants exhibited excessive eye movements during planning, we continued to observe qualitatively similar lateralization effects (see <xref ref-type="fig" rid="fig17">Figure S14</xref> and <xref ref-type="table" rid="tbl16">Table S16</xref>).</p>
</sec>
</sec>
<sec id="s3" sec-type="discussion">
<title>Discussion</title>
<p>Searching for a solution in a complex multi-step task is challenging. Recent computational work suggests that humans overcome this challenge by constructing simplified perceptual representations of their environment. In the present study, we reveal a role for visuospatial attention in constructing these simplified perceptual representations.</p>
<sec id="s3-1">
<title>Participants’ task-representations are informed by visuospatial attention</title>
<p>We provide several lines of evidence for the critical role of visuospatial attention in constructing task representations. First, we observed a significant effect of the spatial context in which information is presented. Participants were less likely to incorporate taskrelevant information into their construal when it was surrounded by task-irrelevant information. These effects mirror perceptual crowding effects<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup> which reveal that attention spills over to distractors presented alongside task-relevant stimuli when presented in close spatial proximity. Second, we observed that participants incorporated task-relevant information into their task representations more frequently when relevant obstacles were grouped together within the same hemifield (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Participants’ task representations in such settings were more closely aligned with an ideal observer model - suggesting that the natural contours of visuospatial attention interact with the capacity of observers to form efficient task representations.</p>
<p>We also observed significant inter-individual differences in attentional effects across participants (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). While some participants were strongly influenced by the spatial context of neighbouring stimuli, others showed more limited evidence for an attentional effect (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Inter-individual differences in attention predicted the sparsity of participants’ simplified representations: participants with larger attention effects exhibited sparser representations. Future research could explore how these individual differences constrain performance on other tasks that require planning and search in highdimensional spaces.</p>
</sec>
<sec id="s3-2">
<title>Incorporating attention into a model of value-guided construal</title>
<p>The VGC model articulates how an ideal decision-maker should represent an environment while balancing complexity and utility<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. We developed an extension of this model that accounts for the effects of spatial attention on planning. Our model, inspired by the analogy of a spotlight of attention<sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup>, provides a better fit to participants’ awareness reports than the original VGC model (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This improved model fit was most evident for mazes where task-relevant information was presented to both hemifields (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), suggesting the augmented model is helpful in explaining behaviour in contexts where attentional selection is more complex. These deviations from the original VGC model, therefore provide a useful benchmark to compare human performance and offer insights into natural constraints on human cognition. For instance, we demonstrate that spatial context biases whether information is to be included or excluded from a representation of the environment. These effects may reflect inductive biases in humans who have learned and evolved to select information from real-world environments where obstacles tend to be grouped together in visual scenes<sup><xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>. However, it is plausible that these inductive biases on value-guided construal may themselves be learnt, and vary according to other environmental demands and contexts which impose systematic regularities on useful task representations (e.g., attending preferentially to intersections when planning on the Tube). Future research can explore the flexibility of participants’ task representations across environmental contexts, and ask how these inductive biases are acquired.</p>
</sec>
<sec id="s3-3">
<title>Planning and Consciousness</title>
<p>Our experiments investigate the connection between planning and participants’ reports of their awareness of features of the task environment. The results may therefore be relevant to understanding the functions of conscious experience. While an intimate connection between attention and consciousness is widely recognized<sup><xref ref-type="bibr" rid="c46">46</xref>–<xref ref-type="bibr" rid="c49">49</xref></sup>, there is less work explicitly considering the connection between planning and consciousness<sup><xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c51">51</xref></sup>. However, there are several reasons why the kind of planning at work in our experiments is likely to require the task to be represented consciously.</p>
<p>As we mentioned at the outset, simplified representations reduce the computational burden of planning in a branching multi-step task space. The same consideration suggests that planning should be based on conscious, rather than unconscious, representations<sup><xref ref-type="bibr" rid="c52">52</xref></sup>. Initial stages of perceptual processing can carry information about a range of different and incompatible possibilities at once, for example, a probability distribution across a range of possible orientations of a line. The probabilistic representation attaches some probability (or probability density) to many different possibilities. There is, of course, a certain burden in integrating and weighing probabilistic information of this kind, for which the brain is thought to deploy various solutions (e.g. approximate Bayesian inference). These initial stages of perceptual inference are typically thought to be unconscious. However, forward planning from multiple possibilities in a branching task space rapidly becomes intractable as the combinatorial possibilities explode. Consciousness, by contrast, provides a much sharper representation of the current state, from which planning can proceed forward<sup><xref ref-type="bibr" rid="c53">53</xref>,<xref ref-type="bibr" rid="c54">54</xref></sup>.</p>
<p>Given the computational cost of running through and comparing many potential multi-step action sequences, it makes sense to base that process on a reliable estimate of the current world state. While it is doubtless useful to produce some kinds of unlearned and habitual action very rapidly at the first hint of information, for example of the presence of a predator, with multi-step forward planning it makes sense to integrate information from more sensory modalities and across a longer timescale before then committing to using a representation as the basis for planning. This again suggests that conscious representations, which are known to integrate information across modalities and time<sup><xref ref-type="bibr" rid="c55">55</xref>–<xref ref-type="bibr" rid="c59">59</xref></sup>, are perfectly suited to the functional needs of this kind of planning task. Furthermore, planning depends on both facts and values. Potential actions are assessed based on the expected value of outcomes. The role of value was captured, in our studies, by an extension of the VGC (value-guided construal) model. Consciousness is thought to facilitate the integration of different sources of value<sup><xref ref-type="bibr" rid="c60">60</xref>–<xref ref-type="bibr" rid="c62">62</xref></sup>.</p>
<p>The task and associated computational model thus offer a flexible tool for characterising the computations by which conscious representations influence decisions and actions. Future work could tell us more about the way bottom-up attention-driven inputs and taskbased value jointly influence what information reaches conscious experience. This provides a novel ecologically-valid probe of the connections between attention, consciousness, and decision-making that does not require the explicit labelling of taskrelevant stimuli. Neural (e.g. M/EEG) data collected while participants plan could help understand the timescale and computational steps that lead to the formation of a conscious task representation. Modifications of the paradigm would also be suited to exposing the way non-consciously-presented cues do and do not influence the way participants plan.</p>
</sec>
<sec id="s3-4">
<title>Methodological Considerations and Future Directions</title>
<p>The task we used requires participants to report their experience of each maze. Yet, retrospective reports are one step removed from perception and do not shed light on the time course of how perceptual representations are formed. In line with this limitation, the extended spotlight-VGC model accounts for visuospatial attention effects after a simplified representation has been computed (i.e., at the end of the construal process). This allowed us to evaluate the fit of the model to empirical data, but does not provide insight into how attention affects the construal process itself. Future research examining the neural dynamics of task representations will be critical to disentangling the time course of how attention and planning considerations interact in the formation of task representations. It will also be necessary to elaborate on how bottom-up and top-down aspects of attentional selection are combined to guide complex behaviours. Finally, open questions remain about the nature and form of visuospatial attention and how this intersects with planning. For instance, can multiple spatial locations be preferentially selected at once—i.e., are there multiple spotlights<sup><xref ref-type="bibr" rid="c63">63</xref>–<xref ref-type="bibr" rid="c66">66</xref></sup>? There is also discourse on how spatial attention may move from one location to another: are the intervening visual regions between attended locations similarly selected<sup><xref ref-type="bibr" rid="c63">63</xref>,<xref ref-type="bibr" rid="c67">67</xref>–<xref ref-type="bibr" rid="c69">69</xref></sup>? Our findings tentatively suggest that individuals are able to attend to disparate spatial regions to form sparse task representations, yet there is substantial variability in how individuals orient their attention during the task. The present paradigm and computational modelling, in conjunction with carefully designed stimuli, may help resolve these outstanding questions.</p>
<p>While we observed clear lateralization effects along the vertical meridian (i.e., left vs right hemifield), effects along the horizontal meridian were less clear (see <xref ref-type="table" rid="tbl14">Table S14</xref>–<xref ref-type="table" rid="tbl15">15</xref>). A combination of factors may explain this finding. First and foremost, the maze stimuli were not designed to test horizontal-meridian lateralization effects, possibly leading to a lack of power to detect these effects. Second, prior research suggests that distractors produce a larger crowding effect when presented across the horizontal compared to the vertical meridian<sup><xref ref-type="bibr" rid="c30">30</xref></sup>. The retinotopic organization of the cortex is believed to drive this effect: spatially adjacent stimuli can be retinotopically distant if presented on the opposite side of the vertical meridian, which is thought to facilitate distractor inhibition. A more detailed experiment explicitly designed to test these effects may clarify the interpretation of these null results.</p>
</sec>
<sec id="s3-5">
<title>Conclusions</title>
<p>Complex daily decisions require a decision-maker to arbitrate over countless potential multi-step actions and their outcomes, making searching for a solution difficult. We shed light on how this is achieved by clarifying the role of visuospatial attention in forming simplified perceptual representations to aid in planning. We build on previous work on the effect of task relevance and develop a computational model which explicitly incorporates the role of attention in value-guided construal. Our model bridges the literature on perception, attention and computational models of planning to provide a more complete computational account of human cognition. We believe the results of this paper can inform future research on a comprehensive theory of human cognition and inspire novel biologically-informed intelligent algorithms.</p>
</sec>
</sec>
<sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4-1">
<sec id="s4-1-1">
<title>Experimental Task</title>
<p>To test our hypotheses we relied on a previously established mazenavigation task where participants are asked to move a circle avatar from a starting location to a goal using the arrow keys<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. Each maze consisted of an 11x11 grid with blue obstacles (7 obstacles in datasets Ho 1 &amp; 2, and 6 obstacles in dataset dSC 1), and black central walls arranged in the shape of a fixation cross. Each trial began with a fixation cross (center walls), after which participants were prompted to navigate to the goal. Experiments differed in terms of i) the mazes participants navigated, ii) whether the obstacles were presented before or during the execution of the plan, and iii) what the participant reported.</p>
<p>We reanalyzed the data of Ho and colleagues’ experiments 1 and 2 for the present study. In experiment 1 (i.e., dataset Ho 1), participants were presented with the obstacles throughout the trial. At the end of each trial, participants were asked to rate “How aware of the highlighted obstacle were you at any point?” using a nine-point scale. In experiment 2 (i.e., dataset Ho 2), participants were similarly asked to rate their awareness of the various obstacles but were required to plan their solution before they began to solve the maze. See <sup><xref ref-type="bibr" rid="c18">18</xref></sup> for details concerning the experimental procedures.</p>
<p>We did not reanalyze the results of the fourth experiment by Ho and colleagues. In this experiment, participants were not presented with all the information (i.e., obstacles) at once to solve the maze. Instead, they discovered obstacles by hovering over them with a cursor.</p>
<p>To further test the effects of attention on task representations, we designed a novel set of maze stimuli. This consisted of 12 mazes with task-relevant obstacles lateralized to a hemifield (left or right) and 12 non-lateralized stimuli. Each maze consisted of six obstacles, three on each hemifield, none of which crossed the veridical meridian. This ensured that there were an equal number of obstacles for computing the lateralization index (see below). Maze stimuli of both sets were equated on several nuisance covariates (see Supplemental <xref ref-type="table" rid="tbl1">Table S1</xref>). For dataset dSC 1, participants solved each of these 24 mazes four times (i.e., all possible orientations of the maze).</p>
<p>The design of the in-person experiment (i.e., dataset dSC 1) closely followed the second experiment of Ho and colleagues<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. On every trial, participants were presented with a maze stimulus for 6 seconds, over which they were required to plan. The maze stimulus was offset, and participants were required to solve the maze after a one-second delay. On every trial, participants reported on their task representations using a nine-point awareness scale.</p>
</sec>
<sec id="s4-1-2">
<title>Participants</title>
<p>For datasets Ho 1 &amp; 2, participants completed the task online on Prolific. In dataset Ho 1, 194 participants completed submissions, 161 of whom were included in the final sample after exclusions. In dataset Ho 2, 188 participants completed submissions, 162 of whom were included.</p>
<p>For dataset dSC 1, 35 participants (mean age = 23.14, SD = 5.35; 12 male) completed an in-person eye-tracking experiment (see 'Eye-tracking acquisition'). None of the participants were excluded from the data analysis. We excluded trials where participants’ reaction times were longer than 20 seconds, or where participants deviated more than nine moves from the optimal path (which reflected 3SD above the mean).</p>
</sec>
<sec id="s4-1-3" sec-type="ethics-statement">
<title>Ethics</title>
<p>All procedures were approved by the University College London ethics committee and adhered to the Declaration of Helsinki. Informed consent was obtained from each participant prior to each experiment.</p>
</sec>
<sec id="s4-1-4">
<title>VGC model</title>
<p>We fit the previously described VGC model to our maze stimuli<sup><xref ref-type="bibr" rid="c18">18</xref></sup>. Briefly, this model computes the optimal simplified task representation such that it maximizes the utility of the representation while also minimizing the cognitive cost of keeping information in mind. This model assumes that a decision-maker combines a subset of cause-effect relationships to represent their environment in aid of planning. For every possible construal, the model computes the value of a representation:
<disp-formula id="FD1">
<alternatives>
<mml:math id="M1" display="block"><mml:mrow><mml:mtext>VOR</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2506.09520v1_eqn1.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where U(<italic>π<sub>c</sub></italic>) is the utility of a construed plan <italic>π<sub>c</sub></italic>, and <italic>C</italic>(<italic>c</italic>) represents the cost of keeping that information in mind.</p>
<p>A task representation is selected according to a SoftMax decision rule. We then compute a marginalized probability for each obstacle being included within a construal,
<disp-formula id="FD2">
<alternatives>
<mml:math id="M2" display="block"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>Obstacle</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∑</mml:mo><mml:mtext>  </mml:mtext><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mtext>Obstacle</mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math>
<graphic xlink:href="2506.09520v1_eqn2.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where <italic>φ</italic><sub>Obstacle<italic>i</italic></sub> is the cause-effect relationship for obstacle<sub>i</sub>, P(c) is the probability that the task representation is selected, and ‖X‖ is a statement which evaluates to 1 if X is true, and 0 when X is false. We use the values of <italic>P</italic>(Obstacle<sub><italic>i</italic></sub>) for every obstacle in a maze to predict participants’ awareness reports. See <sup><xref ref-type="bibr" rid="c18">18</xref></sup> for a detailed explanation of the computational model.</p>
<p>We focused our analyses on the <italic>static</italic> version of the VGC model (i.e., sVGC), whereby task representations are assumed to remain stable across planning. Our choice was informed by the design of the experiment where participants were required to plan over all obstacles at once.</p>
</sec>
<sec id="s4-1-5">
<title>Spatial proximity effects</title>
<p>To examine how the spatial context of information influences participants’ awareness reports, we ran a hierarchical linear regression model. First, for every obstacle in every maze, we rank-ordered all other obstacles based on spatial proximity. That is, the participant’s awareness report of the closest item to obstaclei on the trial was used as a predictor of the participant’s report of obstaclei in a hierarchical linear regression model. This yielded a regression model with 6 regression coefficients predicting participants’ awareness reports based on spatial proximity:
<disp-formula id="FD3">
<alternatives>
<mml:math id="M3" display="block"><mml:msub><mml:mtext>report</mml:mtext><mml:mrow><mml:mtext>obstacle i</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mtext>β</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mtext>β</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mtext>report</mml:mtext><mml:mrow><mml:mtext>obstacle 1</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mtext>β</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mtext>report</mml:mtext><mml:mrow><mml:mtext>obstacle 2</mml:mtext></mml:mrow></mml:msub><mml:mn>...</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mtext>β</mml:mtext><mml:mn>6</mml:mn></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mtext>report</mml:mtext><mml:mrow><mml:mtext>obstacle 6</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mtext>MazeID</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mtext>ParticipantID</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>ε</mml:mtext></mml:math>
<graphic xlink:href="2506.09520v1_eqn3.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where <italic>(1| MazeID</italic>) and (<italic>1| ParticipantID</italic>) are random intercepts of each maze and participant, respectively, and β<sub>1</sub> reflects the contribution of the closest obstacle to obstaclei. We interpret any significant effects in this model as the influence of neighbouring stimuli on participants’ representations. We also fit the above hierarchical linear regression model for each participant separately. We report these individual beta coefficients in <xref ref-type="fig" rid="fig1">Figure 1b</xref>.</p>
<p>To ensure that the above spatial proximity effects were not driven by the VGC model predictions, we regressed out the effects of VCG model predictions from participants’ awareness reports, and used the residuals of the model as the dependent variable in a second regression where we similarly predicted the effects of neighbouring stimuli on representations.</p>
<p>We verified that these effects were not explained by the spatial smoothness of our data by conducting 1000 spatial null permutations. For every iteration, we permuted the mapping between each obstacle in a maze and their spatial location maintaining the number of neighbouring obstacles for every trial. We fit a hierarchical linear regression model using this permuted data and built a distribution of null beta coefficients to compare to our observed effects.</p>
</sec>
<sec id="s4-1-6">
<title>The sparsity of task representations</title>
<p>We sought to test the relationship between i) interindividual differences in attention effects and ii) the sparsity of task representations. First, we estimated the magnitude of each person’s attention effect by fitting a linear slope to the beta coefficients obtained (see Spatial proximity effects). A participant with a large negative slope, therefore, showed a larger effect of neighbouring obstacles on their representation. Second, we operationalized the sparsity of participants’ simplified representation as the variance of their awareness reports. A participant with a sparse representation shows a high variance in their awareness of different obstacles in a given maze. Last, we tested the linear monotonic relationship between the sparsity of participants’ representations and the attention effects using Spearman correlation.</p>
</sec>
<sec id="s4-1-7">
<title>Lateralization index</title>
<p>To test the effects of lateralization of task-relevant stimuli on participants’ awareness reports, we developed a lateralized index of task-relevance inspired by the alpha-power attention literature<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>. We divided each maze into a right and left hemifield and computed the ratio of task-relevant obstacles on both sides:
<disp-formula id="FD4">
<alternatives>
<mml:math id="M4" display="block"><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:mstyle><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:mstyle><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:mstyle><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:mstyle><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math>
<graphic xlink:href="2506.09520v1_eqn4.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where sVGC is the model’s prediction of each obstacle task-relevance for that maze. Note obstacles only with a majority of its blocks within a single hemifield were considered (3 or more squares). This yielded an index of task-relevance lateralization for each maze stimulus. We repeated the above procedure to obtain an index of task-relevance lateralization for the horizontal meridian (superior vs inferior hemifield).</p>
<p>We tested whether the lateralization index moderated the relationship between the value-guided model predictions (sVGC) and participants’ awareness reports using a hierarchical linear regression model.</p>
<disp-formula id="FD5">
<alternatives>
<mml:math id="M5" display="block"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>*</mml:mo><mml:mi>s</mml:mi><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>*</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>*</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>*</mml:mo><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2506.09520v1_eqn5.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
<p>where β<sub>3</sub> represents the interaction between the VGC model predictions and the lateralization index.</p>
</sec>
<sec id="s4-1-8">
<title>Spotlight-VGC model</title>
<p>Inspired by previous literature comparing visuospatial attention to a spotlight that moves across the visual field, we developed an extension of the VGC model to account for the effects of attentional selection in forming task representations.</p>
<p>To do this, we recomputed the <italic>P</italic>(Obstacle<sub><italic>i</italic></sub>) as a weighted average of its neighbours. We computed the distance between every obstacle in the maze, and searched for obstacles with neighbours within 3 squares (Manhattan distance) away from obstaclei. The distance of 3 squares reflects the ‘width’ of the attentional spotlight, and was chosen based on the median distance between neighbouring obstacles from our ranked analyses (<xref ref-type="fig" rid="fig1">Figure 1b</xref> &amp; <xref ref-type="fig" rid="fig5">Figure S2</xref>). Neighbouring obstacles that fell within the attention spotlight were averaged as follows:
<disp-formula id="FD6">
<alternatives>
<mml:math id="M6" display="block"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>Obstacle</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>Obstacle</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>Obstacle</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:math>
<graphic xlink:href="2506.09520v1_eqn6.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>where n is the number of obstacles that fall within the width of the attentional <italic>spotlight</italic> (i.e., neighbouring items). We repeat this procedure for all obstacles within each maze. If an obstacle did not have any neighbours, then the value of <italic>P</italic>(Obstacle<sub><italic>i</italic></sub>) remained identical to the value of original VGC model.</p>
<p>We used the outputs of the attention spotlight model in a hierarchical linear regression to predict participants’ awareness reports, where we included participant and maze random intercepts:
<disp-formula id="FD7">
<alternatives>
<mml:math id="M7" display="block"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>*</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mo>.</mml:mo><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>
<graphic xlink:href="2506.09520v1_eqn7.tif" mime-subtype="tif" mimetype="image"/></alternatives>
</disp-formula>
</p>
<p>All linear regression models were fit with the <italic>lmer</italic> package in R.</p>
</sec>
<sec id="s4-1-9">
<title>Null permutations</title>
<p>To ensure that the improved model fit of the attentional spotlight model was not driven by the spatial smoothness of our data, we conducted a series of control analyses where we permuted the model predictions within mazes.</p>
<p>To do so, we re-assigned the <italic>P</italic>(Obstacle<italic>i</italic>) of each obstacle in a given maze to a random item such that each obstacle was given a new model prediction. This permutation procedure maintains the distribution of <italic>P</italic>(Obstacle<italic>i</italic>) across obstacles for each maze, while randomizing the location of task-relevant information. We repeated this procedure for each maze separately. We then used these random model predictions to predict participants’ reports using the same hierarchical linear model described in <underline>Spotlight-VGC model</underline>. We repeated this procedure 1000 times to generate a null distribution of beta coefficients. We compared the observed beta value for the spotlight-VGC model against this distribution. We note that averaging neighbouring obstacles before or after the permutation of the model predictions qualitatively yielded the same result.</p>
</sec>
<sec id="s4-1-10">
<title>Eye-tracking acquisition</title>
<p>For dataset dSC1, participants completed the computer task while their eye-position and pupil size were monitored using an EyeLink 1000 Plus eye tracker at 1000Hz (SR Research, Osgoode, ON). Participants were seated comfortably in a dimly lit room in front of a 24-inch monitor set to the resolution of 1,920 x 1,080 pixels at 60 Hz. Participants were positioned 60 centimetres away from the screen and rested their heads on a mount. Stimuli were presented on MATLAB 2019a using psychtoolbox (3.0.16), synchronized with the eye tracker. Before the start of the experiment, participants completed a standard 5-point calibration procedure. Drift correction was applied after every block.</p>
</sec>
<sec id="s4-1-11">
<title>Eye-tracking preprocessing &amp; analysis</title>
<p>Eye-tracking data were preprocessed with the PuPL toolbox in MATLAB<sup><xref ref-type="bibr" rid="c70">70</xref></sup>. Impossible data points (i.e., gaze outside the screen’s bounds) were removed, in addition to data 50ms before and 150 ms after eye blinks (identified by pupillometry noise<sup><xref ref-type="bibr" rid="c71">71</xref></sup>). Segments of missing gaze position, up to 400ms long, were interpolated using cubic splines. We analyzed eye position data between −1000 ms and 6000 ms around the presentation of the maze, which corresponds to the planning window and the one second prior to planning. To verify that participants did not move their eyes more frequently during planning for lateralized mazes, we computed the standard deviation of eye position along the X-axis for each trial. We compared the fluctuations across lateralized and non-lateralized trials with a two-sample t-test. To verify the robustness of our behavioural effects, we identified and removed from further analysis trials where participants’ eye position exceeded two squares away from fixation.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data &amp; code availability</title>
<p>All in-house code used for data analysis and visualization is available on GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/jasondsc/ConsciousDetour">https://github.com/jasondsc/ConsciousDetour</ext-link>. The reanalyzed data presented herein are available from <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41586-022-04743-9">https://www.nature.com/articles/s41586-022-04743-9</ext-link>. The data from experiment 3 are available from <ext-link ext-link-type="uri" xlink:href="https://osf.io/sa6vf/">https://osf.io/sa6vf/</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>J.d.S.C. is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) postdoctoral fellowship program. S.M.F. is a CIFAR Fellow in the Brain, Mind and Consciousness Program and is supported by a Wellcome/Royal Society Sir Henry Dale Fellowship [206648/Z/17/Z] and UKRI under the UK government’s Horizon Europe funding guarantee (selected as ERC Consolidator, grant number 101043666).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Callaway</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Rational use of cognitive resources in human planning</article-title>. <source>Nat. Hum. Behav.</source> <volume>6</volume>, <fpage>1112</fpage>–<lpage>1125</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Newell</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Simon</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>The logic theory machine-A complex information processing system</article-title>. <source>IRE Trans Inf Theory</source> <volume>2</volume>, <fpage>61</fpage>–<lpage>79</lpage> (<year>1956</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huys</surname>, <given-names>Q. J. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Bonsai Trees in Your Head: How the Pavlovian System Sculpts Goal-Directed Choices by Pruning Decision Trees</article-title>. <source>PLOS Comput Biol</source> <volume>8</volume>, <elocation-id>e1002410</elocation-id> (<year>2012</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dezfouli</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Balleine</surname>, <given-names>B. W.</given-names></string-name></person-group> <article-title>Actions, Action Sequences and Habits: Evidence That Goal-Directed and Habitual Action Control Are Hierarchically Organized</article-title>. <source>PLOS Comput Biol</source> <volume>9</volume>, <elocation-id>e1003364</elocation-id> (<year>2013</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Doing more with less: meta-reasoning and meta-learning in humans and machines</article-title>. <source>Curr. Opin. Behav. Sci.</source> <volume>29</volume>, <fpage>24</fpage>–<lpage>30</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daw</surname>, <given-names>N. D.</given-names></string-name>, <string-name><surname>Niv</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nat. Neurosci.</source> <volume>8</volume>, <fpage>1704</fpage>–<lpage>1711</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saxe</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nelli</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>If deep learning is the answer, what is the question?</article-title> <source>Nat. Rev. Neurosci.</source> <volume>22</volume>, <fpage>55</fpage>–<lpage>67</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hassabis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kumaran</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Botvinick</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Neuroscience-Inspired Artificial Intelligence</article-title>. <source>Neuron</source> <volume>95</volume>, <fpage>245</fpage>–<lpage>258</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quinlan</surname>, <given-names>J. R.</given-names></string-name></person-group> <article-title>Induction of decision trees</article-title>. <source>Mach Learn</source> <volume>1</volume>, <fpage>81</fpage>–<lpage>106</lpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breslow</surname>, <given-names>L. A.</given-names></string-name> &amp; <string-name><surname>Aha</surname>, <given-names>D. W.</given-names></string-name></person-group> <article-title>Simplifying decision trees: A survey</article-title>. <source>Knowl Eng Rev</source> <volume>12</volume>, <fpage>1</fpage>–<lpage>40</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huys</surname>, <given-names>Q. J. M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Interplay of approximate planning strategies</article-title>. <source>Proc. Natl. Acad. Sci.</source> <volume>112</volume>, <fpage>3098</fpage>–<lpage>3103</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mingers</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>An Empirical Comparison of Pruning Methods for Decision Tree Induction</article-title>. <source>Mach Learn</source> <volume>4</volume>, <fpage>227</fpage>–<lpage>243</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knuth</surname>, <given-names>D. E.</given-names></string-name> &amp; <string-name><surname>Moore</surname>, <given-names>R. W.</given-names></string-name></person-group> <article-title>An analysis of alpha-beta pruning</article-title>. <source>Artif Intell</source> <volume>6</volume>, <fpage>293326</fpage> (<year>1975</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keramati</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Smittenaar</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Dolan</surname>, <given-names>R. J.</given-names></string-name> &amp; <string-name><surname>Dayan</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Adaptive integration of habits into depth-limited planning defines a habitual-goal-directed spectrum</article-title>. <source>Proc. Natl. Acad. Sci.</source> <volume>113</volume>, <fpage>12868</fpage>–<lpage>12873</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snider</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Poizner</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Gepshtein</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Prospective Optimization with Limited Resources</article-title>. <source>PLOS Comput Biol</source> <volume>11</volume>, <elocation-id>e1004501</elocation-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Korf</surname>, <given-names>R. E.</given-names></string-name></person-group> <article-title>Depth-first iterative-deepening: An optimal admissible tree search</article-title>. <source>Artif Intell</source> <volume>27</volume>, <fpage>97</fpage>–<lpage>109</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kool</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Cushman</surname>, <given-names>F. A.</given-names></string-name> &amp; <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name></person-group> <article-title>When Does Model-Based Control Pay Off?</article-title> <source>PLOS Comput Biol</source> <volume>12</volume>, <elocation-id>e1005090</elocation-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ho</surname>, <given-names>M. K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>People construct simplified mental representations to plan</article-title>. <source>Nature</source> <volume>606</volume>, <fpage>129</fpage>–<lpage>136</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gershman</surname>, <given-names>S.</given-names></string-name></person-group> <source>What Makes Us Smart: The Computational Logic of Human Cognition</source>. <volume>vii</volume>, <fpage>205</fpage> (<publisher-name>Princeton University Press</publisher-name>, <publisher-loc>Princeton, NJ, US</publisher-loc>, <year>2021</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Landry</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>da Silva Castanheira</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Jerbi</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>Differential and Overlapping Effects between Exogenous and Endogenous Attention Shape Perceptual Facilitation during Visual Processing</article-title>. <source>J. Cogn. Neurosci.</source> <volume>35</volume>, <fpage>1279</fpage>–<lpage>1300</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Landry</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Da Silva Castanheira</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sackur</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Raz</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Investigating how the modularity of visuospatial attention shapes conscious perception using type I and type II signal detection theory</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source> <volume>47</volume>, <fpage>402</fpage>–<lpage>422</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chica</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Bartolomeo</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Lupianez</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Two cognitive and neural systems for endogenous and exogenous spatial attention</article-title>. <source>Behav. Brain Res.</source> <volume>237</volume>, <fpage>107</fpage>–<lpage>123</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carrasco</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Visual attention: The past 25 years</article-title>. <source>Vision Res.</source> <volume>51</volume>, <fpage>1484</fpage>–<lpage>1525</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carrasco</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ling</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Read</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Attention alters appearance</article-title>. <source>Nat. Neurosci.</source> <volume>7</volume>, <fpage>308</fpage>–<lpage>313</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Nobre</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name></person-group> <source>The Oxford Handbook of Attention</source>. (<publisher-name>OUP Oxford</publisher-name>, <year>2014</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niv</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>Learning task-state representations</article-title>. <source>Nat. Neurosci.</source> <volume>22</volume>, <fpage>1544</fpage>–<lpage>1553</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lamme</surname>, <given-names>V. A.</given-names></string-name> &amp; <string-name><surname>Roelfsema</surname>, <given-names>P. R.</given-names></string-name></person-group> <article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title>. <source>Trends Neurosci.</source> <volume>23</volume>, <fpage>571</fpage>–<lpage>579</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Broadbent</surname>, <given-names>D. E.</given-names></string-name></person-group> <source>Perception and Communication</source>. (<publisher-name>Elsevier Science &amp; Technology</publisher-name>, <year>1958</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nelson</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Crisostomo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Khericha</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Russo</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Thorne</surname>, <given-names>G. L.</given-names></string-name></person-group> <article-title>Classic Debates in Selective Attention: Early vs Late, Perceptual Load vs Dilution, Mean RT vs Measures of Capacity</article-title>. <source>Perception</source> <volume>41</volume>, <fpage>997</fpage>–<lpage>1000</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>X.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Reduction of the Crowding Effect in Spatially Adjacent but Cortically Remote Visual Stimuli</article-title>. <source>Curr. Biol.</source> <volume>19</volume>, <fpage>127</fpage>–<lpage>132</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cavanagh</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Intriligator</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Attentional resolution and the locus of visual awareness</article-title>. <source>Nature</source> <volume>383</volume>, <fpage>334</fpage>–<lpage>337</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whitney</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Levi</surname>, <given-names>D. M.</given-names></string-name></person-group> <article-title>Visual Crowding: a fundamental limit on conscious perception and object recognition</article-title>. <source>Trends Cogn Sci</source> <volume>15</volume>, <fpage>160</fpage>–<lpage>168</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Norman</surname>, <given-names>D. A.</given-names></string-name></person-group> <article-title>Toward a theory of memory and attention</article-title>. <source>Psychol Rev</source> <volume>75</volume>, <fpage>522</fpage>–<lpage>536</lpage> (<year>1968</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Posner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Davidson</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Attention and the detection of signals</article-title>. <source>J. Exp. Psychol. Gen.</source> <volume>109</volume>, <fpage>160</fpage>–<lpage>174</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keefe</surname>, <given-names>J. M.</given-names></string-name> &amp; <string-name><surname>Stormer</surname>, <given-names>V. S.</given-names></string-name></person-group> <article-title>Lateralized alpha activity and slow potential shifts over visual cortex track the time course of both endogenous and exogenous orienting of attention</article-title>. <source>NeuroImage</source> <volume>225</volume>, <fpage>117495</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bagherzadeh</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Baldauf</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Alpha Synchrony and the Neurofeedback Control of Spatial Attention</article-title>. <source>Neuron</source> <volume>105</volume>, <fpage>577</fpage>–<lpage>587.e5</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Landry</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>da Silva Castanheira</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Raz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Baillet</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Sackur</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>A lateralized alpha-band marker of the interference of exogenous attention over endogenous attention</article-title>. <source>Cereb Cortex</source> <volume>34</volume>, <fpage>bhad457</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name></person-group> <article-title>Distractor inhibition by alpha oscillations is controlled by an indirect mechanism governed by goal-relevant information</article-title>. <source>Commun Psychol</source> <volume>2</volume>, <fpage>1</fpage>–<lpage>11</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Posner</surname>, <given-names>M. I.</given-names></string-name></person-group> <article-title>Orienting of attention</article-title>. <source>Q J Exp Psychol</source> <volume>32</volume>, <fpage>3</fpage>–<lpage>25</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eriksen</surname>, <given-names>C. W.</given-names></string-name> &amp; <string-name><prefix>St.</prefix> <surname>James</surname>, <given-names>J. D.</given-names></string-name></person-group> <article-title>Visual attention within and around the field of focal attention: A zoom lens model</article-title>. <source>Percept Psychophys</source> <volume>40</volume>, <fpage>225</fpage>–<lpage>240</lpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>James</surname>, <given-names>W. W.</given-names></string-name></person-group> <source>The Principles of Psychology, Vol I</source>. <year>1890</year> <publisher-name>Henry Holt and Co</publisher-name> <pub-id pub-id-type="doi">10.1037/10538-000</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vollebregt</surname>, <given-names>M. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Lateralized modulation of posterior alpha oscillations in children</article-title>. <source>NeuroImage</source> <volume>123</volume>, <fpage>245</fpage>–<lpage>252</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ghafari</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Mazzetti</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Garner</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Gutteling</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name></person-group> <article-title>Modulation of alpha oscillations by attention is predicted by hemispheric asymmetry of subcortical regions</article-title>. <source>eLife</source> <volume>12</volume>, <elocation-id>RP91650</elocation-id> (<year>2024</year>). <pub-id pub-id-type="doi">10.7554/eLife.91650</pub-id></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name> &amp; <string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Attention in the real world: toward understanding its neural basis</article-title>. <source>Trends Cogn Sci</source> <volume>18</volume>, <fpage>242</fpage>–<lpage>250</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaiser</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Quek</surname>, <given-names>G. L.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name></person-group> <article-title>Object Vision in a Structured World</article-title>. <source>Trends Cogn Sci</source> <volume>23</volume>, <fpage>672</fpage>–<lpage>685</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Cavanagh</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Chun</surname>, <given-names>M. M.</given-names></string-name> &amp; <string-name><surname>Nakayama</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>The attentional requirements of consciousness</article-title>. <source>Trends Cogn Sci</source> <volume>16</volume>, <fpage>411</fpage>–<lpage>417</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Tsuchiya</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name></person-group> <chapter-title>On the Relationship Between Consciousness and Attention</chapter-title>. in <source>The Cognitive Neurosciences</source> (eds. <person-group person-group-type="editor"><string-name><surname>Gazzaniga</surname>, <given-names>M. S.</given-names></string-name> &amp; <string-name><surname>Mangun</surname>, <given-names>G. R.</given-names></string-name></person-group>) (<publisher-name>The MIT Press</publisher-name>, <year>2014</year>). doi:<pub-id pub-id-type="doi">10.7551/mitpress/9504.003.0092</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Tsuchiya</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>Attention and consciousness: two distinct brain processes</article-title>. <source>Trends Cogn Sci</source> <volume>11</volume>, <fpage>16</fpage>–<lpage>22</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lamme</surname>, <given-names>V. A. F.</given-names></string-name></person-group> <article-title>Why visual attention and awareness are different</article-title>. <source>Trends Cogn Sci</source> <volume>7</volume>, <fpage>12</fpage>–<lpage>18</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>MacIver</surname>, <given-names>M. A.</given-names></string-name> &amp; <string-name><surname>Finlay</surname>, <given-names>B. L.</given-names></string-name></person-group> <article-title>The neuroecology of the water-to-land transition and the evolution of the vertebrate brain</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>377</volume>, <fpage>20200523</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fleming</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Michel</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Sensory Horizons and the Functions of Conscious Vision</article-title>. <source>Behav Brain Sci</source> <volume>21</volume>, <fpage>1</fpage>-<lpage>53</lpage> (<year>2025</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shea</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Frith</surname>, <given-names>C. D.</given-names></string-name></person-group> <article-title>Dual-process theories and consciousness: the case for ‘Type Zero’ cognition</article-title>. <source>Neurosci Conscious</source> <volume>2016</volume>, <fpage>niw005</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stocker</surname>, <given-names>A. A.</given-names></string-name> &amp; <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name></person-group> <article-title>A Bayesian Model of Conditioned Perception</article-title>. <source>Adv Neural Inf Process Syst</source> <volume>2007</volume>, <fpage>1409</fpage>–<lpage>1416</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Block</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>If perception is probabilistic, why does it not seem probabilistic?</article-title> <source>Philos Trans R Soc B Biol Sci</source> <volume>373</volume>, <fpage>20170341</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mudrik</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Faivre</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Information integration without awareness</article-title>. <source>Trends Cogn Sci</source> <volume>18</volume>, <fpage>488</fpage>–<lpage>496</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deroy</surname>, <given-names>O.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The Complex Interplay Between Multisensory Integration and Perceptual Awareness</article-title>. <source>Multisensory Res.</source> <volume>29</volume>, <fpage>585</fpage>–<lpage>606</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bekinschtein</surname>, <given-names>T. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Classical conditioning in the vegetative and minimally conscious state</article-title>. <source>Nat. Neurosci.</source> <volume>12</volume>, <fpage>1343</fpage>–<lpage>1349</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strauss</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Disruption of hierarchical predictive coding during sleep</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A.</source> <volume>112</volume>, <elocation-id>E1353–1362</elocation-id> (<year>2015</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herzog</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Drissi-Daoudi</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>All in Good Time: Long-Lasting Postdictive Effects Reveal Discrete Perception</article-title>. <source>Trends Cogn Sci</source> <volume>24</volume>, <fpage>826</fpage>–<lpage>837</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dickinson</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Balleine</surname>, <given-names>B.</given-names></string-name></person-group> <chapter-title>Hedonics: The cognitive-motivational interface</chapter-title>. in <person-group person-group-type="editor"><string-name><surname>Kringelbach</surname>, <given-names>M. L.</given-names></string-name> &amp; <string-name><surname>Berridge</surname>, <given-names>K. C.</given-names></string-name></person-group> <source>Pleasures of the brain</source> <fpage>74</fpage>–<lpage>84</lpage> (<publisher-name>Oxford University Press</publisher-name>, <publisher-loc>New York, NY, US</publisher-loc>, <year>2010</year>). doi:<pub-id pub-id-type="doi">10.1093/oso/9780195331028.003.0006</pub-id>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Braver</surname>, <given-names>T. S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Mechanisms of motivation-cognition interaction: challenges and opportunities</article-title>. <source>Cogn Affect Behav Neurosci</source> <volume>14</volume>, <fpage>443</fpage>–<lpage>472</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dung</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Assessing tests of animal consciousness</article-title>. <source>Conscious Cogn</source> <volume>105</volume>, <fpage>103410</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McMains</surname>, <given-names>S. A.</given-names></string-name> &amp; <string-name><surname>Somers</surname>, <given-names>D. C.</given-names></string-name></person-group> <article-title>Multiple Spotlights of Attentional Selection in Human Visual Cortex</article-title>. <source>Neuron</source> <volume>42</volume>, <fpage>677</fpage>–<lpage>686</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pylyshyn</surname>, <given-names>Z. W.</given-names></string-name> &amp; <string-name><surname>Storm</surname>, <given-names>R. W.</given-names></string-name></person-group> <article-title>Tracking multiple independent targets: Evidence for a parallel tracking mechanism*</article-title>. <source>Spat Vis</source> <volume>3</volume>, <fpage>179</fpage>–<lpage>197</lpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Awh</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Pashler</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>Evidence for split attentional foci</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source> <volume>26</volume>, <fpage>834</fpage>–<lpage>846</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shaw</surname>, <given-names>M. L.</given-names></string-name> &amp; <string-name><surname>Shaw</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Optimal allocation of cognitive resources to spatial locations</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source> <volume>3</volume>, <fpage>201</fpage>–<lpage>211</lpage> (<year>1977</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McMains</surname>, <given-names>S. A.</given-names></string-name> &amp; <string-name><surname>Somers</surname>, <given-names>D. C.</given-names></string-name></person-group> <article-title>Processing Efficiency of Divided Spatial Attention Mechanisms in Human Visual Cortex</article-title>. <source>J. Neurosci.</source> <volume>25</volume>, <fpage>9444</fpage>–<lpage>9448</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kr</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Np</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Visuospatial attention: beyond a spotlight model</article-title>. <source>Psychon. Bull. Rev.</source> <volume>6</volume>, (<year>1999</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dubois</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hamker</surname>, <given-names>F. H.</given-names></string-name> &amp; <string-name><surname>VanRullen</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Attentional selection of noncontiguous locations: The spotlight is only transiently “split”</article-title>. <source>J. Vis.</source> <volume>9</volume>, <fpage>3</fpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kinley</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Levy</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>PuPl: an open-source tool for processing pupillometry data</article-title>. <source>Behav. Res. Methods</source> <volume>54</volume>, <fpage>2046</fpage>–<lpage>2069</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hershman</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Henik</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Cohen</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>A novel blink detection method based on pupillometry noise</article-title>. <source>Behav. Res. Methods</source> <volume>50</volume>, <fpage>107</fpage>–<lpage>114</lpage> (<year>2018</year>).</mixed-citation></ref>
</ref-list>
<app-group>
<app id="s6">
<title>Supplemental materials</title>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Model predictions and reported awareness on mazes 0 to 5 of reanalysed data.</title></caption>
<graphic xlink:href="2506.09520v1_fig4.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Model predictions and reported awareness on mazes 6 to 11 of reanalysed data.</title></caption>
<graphic xlink:href="2506.09520v1_fig5.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Model predictions and reported awareness on non-lateralized mazes 0 to 5 of the new experiment.</title></caption>
<graphic xlink:href="2506.09520v1_fig6.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Model predictions and reported awareness on non-lateralized mazes 6 to 11 of the new experiment.</title></caption>
<graphic xlink:href="2506.09520v1_fig7.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Model predictions and reported awareness on lateralized mazes 0 to 5 of the new experiment.</title></caption>
<graphic xlink:href="2506.09520v1_fig8.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure S6.</label>
<caption><title>Model predictions and reported awareness on lateralized mazes 6 to 11 of the new experiment.</title></caption>
<graphic xlink:href="2506.09520v1_fig9.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl1" position="float" orientation="portrait">
<label>Table S1.</label>
<caption><title>The lateralized and non-lateralized maze stimuli for experiment three did not differ on nuisance covariates.</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl1.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top"><italic>t-statistic</italic></th>
<th align="center" valign="top"><italic>Uncorrected p-value</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Optimal no. moves</td>
<td align="center" valign="top">−0.53</td>
<td align="center" valign="top">0.65</td>
</tr>
<tr>
<td align="left" valign="top">Distance to goal</td>
<td align="center" valign="top">0.91</td>
<td align="center" valign="top">0.38</td>
</tr>
<tr>
<td align="left" valign="top">Distance to start</td>
<td align="center" valign="top">1.84</td>
<td align="center" valign="top">0.07</td>
</tr>
<tr>
<td align="left" valign="top">Distance to walls</td>
<td align="center" valign="top">0.99</td>
<td align="center" valign="top">0.33</td>
</tr>
<tr>
<td align="left" valign="top">Distance to center</td>
<td align="center" valign="top">0.77</td>
<td align="center" valign="top">0.48</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<sec id="s6-1">
<title>Spatial proximity effects</title>
<table-wrap id="tbl2" position="float" orientation="portrait">
<label>Table S2.</label>
<caption><title>Hierarchical linear regression model predicting the awareness of an obstacle from its neighbours.</title>
<p>Beta coefficients reflect the contribution of neighbouring obstacles to the awareness of the probe item. Beta coefficients reflect the rank order of the closest (1<sup>st</sup>) to furthest (6<sup>th</sup>) obstacle from the probed item.</p></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl2.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness of probe obstacle</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="center" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top">−0.08 – 0.08</td>
<td align="left" valign="top">0.974</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 1 (closest)</td>
<td align="left" valign="top">0.26</td>
<td align="left" valign="top">0.25 – 0.28</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 2</td>
<td align="left" valign="top">0.29</td>
<td align="left" valign="top">0.27 – 0.30</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 3</td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top">−0.01 – 0.02</td>
<td align="left" valign="top">0.699</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 4</td>
<td align="left" valign="top">−0.03</td>
<td align="left" valign="top">−0.04 – −0.01</td>
<td align="left" valign="top"><bold>0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 5</td>
<td align="left" valign="top">−0.13</td>
<td align="left" valign="top">−0.15 – −0.12</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 6 (furthest)</td>
<td align="left" valign="top">−0.13</td>
<td align="left" valign="top">−0.15 – −0.12</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top" colspan="4">Random Effects</td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.58</td>
<td align="center" valign="top"/>
<td align="center" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.11</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.01</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.17</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">12</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">161</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">13342</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.245 / 0.371</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl3" position="float" orientation="portrait">
<label>Table S3.</label>
<caption><title>Effect of neighbouring obstacles on awareness of probed item, after regressing the effect of the VGC model.</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl3.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top" colspan="3">Awareness residuals</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="center" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">−0.00</td>
<td align="center" valign="top">−0.02 – 0.02</td>
<td align="center" valign="top">1.000</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 1 (closest)</td>
<td align="left" valign="top">0.26</td>
<td align="center" valign="top">0.24 – 0.27</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 2</td>
<td align="left" valign="top">0.27</td>
<td align="center" valign="top">0.25 – 0.28</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 3</td>
<td align="left" valign="top">−0.06</td>
<td align="center" valign="top">−0.08 – −0.05</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 4</td>
<td align="left" valign="top">−0.07</td>
<td align="center" valign="top">−0.09 – −0.05</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 5</td>
<td align="left" valign="top">−0.15</td>
<td align="center" valign="top">−0.16 – −0.13</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 6 (furthest)</td>
<td align="left" valign="top">−0.10</td>
<td align="center" valign="top">−0.12 – −0.08</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">13342</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  R<sup>2</sup> / R<sup>2</sup> adjusted</td>
<td align="left" valign="top" colspan="2">0.201 / 0.201</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="fig10" position="float" fig-type="figure">
<label>Fig. S7.</label>
<caption><title>Spatial proximity predicts awareness for both task -relevant and -irrelevant obstacles.</title>
<p>Standardized beta coefficients of the ranked regression model for task-relevant and - irrelevant obstacles separately across datasets Ho 1 &amp; 2. We observed that obstacles closest to the probed item (rank 1 &amp; 2) positively impact awareness reports, regardless of task-relevance. In contrast, obstacles furthest from the probed item negatively impact awareness reports (rank 5 &amp; 6).</p></caption>
<graphic xlink:href="2506.09520v1_fig10.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig11" position="float" fig-type="figure">
<label>Fig. S8.</label>
<caption><title>Distribution of distances between obstacles for each rank in regression.</title>
<p>Boxplots depicting the distance between obstacles. In datasets Ho 1 &amp; 2, obstacles ranked 1<sup>st</sup> (closest) and 2<sup>nd</sup> in proximity were on average 2.14 (median= 2.0, sd= 0.91), and 3.60 squares away (median= 3.0, sd=1.30), respectively. These obstacles positively predict participants’ awareness of items. In dataset dSC 1, obstacles ranked closest were on average 2.76 (median= 3.0, sd= 0.83) squares away. Based on these statistics, we expected an attentional spotlight of width 3 would provide the best fit to the data.</p></caption>
<graphic xlink:href="2506.09520v1_fig11.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl4" position="float" orientation="portrait">
<label>Table S4.</label>
<caption><title>Hierarchical linear regression model predicting the awareness of an obstacle from its neighbours for dataset Ho 2.</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl4.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness of probe obstacle</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="left" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top">−0.05 – 0.05</td>
<td align="left" valign="top">0.960</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 1 (closest)</td>
<td align="left" valign="top">0.34</td>
<td align="left" valign="top">0.33 – 0.36</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 2</td>
<td align="left" valign="top">0.29</td>
<td align="left" valign="top">0.27 – 0.30</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 3</td>
<td align="left" valign="top">0.04</td>
<td align="left" valign="top">0.03 – 0.06</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 4</td>
<td align="left" valign="top">−0.03</td>
<td align="left" valign="top">−0.05 – −0.02</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 5</td>
<td align="left" valign="top">−0.09</td>
<td align="left" valign="top">−0.11 – −0.07</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 6 (furthest)</td>
<td align="left" valign="top">−0.12</td>
<td align="left" valign="top">−0.14 – −0.11</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top" colspan="4"><bold>Random Effects</bold></td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.60</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.04</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.07</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">12</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">162</td>
<td align="center" valign="top"/>
<td align="center" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">13321</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.312 / 0.362</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl5" position="float" orientation="portrait">
<label>Table S5.</label>
<caption><title>Effect of neighbouring obstacles on awareness of probed item, after regressing the effect of the VGC model for dataset Ho 2.</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl5.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top" colspan="3">Awareness residuals</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="center" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.00</td>
<td align="center" valign="top">−0.01 – 0.01</td>
<td align="center" valign="top">1.000</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 1 (closest)</td>
<td align="left" valign="top">0.32</td>
<td align="center" valign="top">0.30 – 0.34</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 2</td>
<td align="left" valign="top">0.24</td>
<td align="center" valign="top">0.23 – 0.26</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 3</td>
<td align="left" valign="top">−0.01</td>
<td align="center" valign="top">−0.03 – 0.01</td>
<td align="center" valign="top">0.236</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 4</td>
<td align="left" valign="top">−0.07</td>
<td align="center" valign="top">−0.09 – −0.05</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 5</td>
<td align="left" valign="top">−0.12</td>
<td align="center" valign="top">−0.13 – −0.10</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 6 (furthest)</td>
<td align="left" valign="top">−0.12</td>
<td align="center" valign="top">−0.13 – −0.10</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">13321</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  R<sup>2</sup> / R<sup>2</sup> adjusted</td>
<td align="left" valign="top">0.242 / 0.241</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We explored inter-individual differences in spatial attention effects for the second experiment where participants were required to plan their route upfront.</p>
<p>Similar to the first experiment, obstacles closest to the probed item positively related to participants’ awareness (1<sup>st</sup> rank: t(161)= 19.53, <italic>p<sub>fdr</sub></italic> &lt; 0.001; 2<sup>nd</sup> rank: t(161)= 13.69, <italic>p<sub>fdr</sub></italic> &lt; 0.001). In contrast, obstacles furthest from the probed item were negatively related to participants’ awareness of the probed item (4<sup>th</sup> rank: t(161)= −5.36, <italic>p<sub>fdr</sub></italic> &lt; 0.001; 5<sup>th</sup> rank: t(161)= −7.32, <italic>p<sub>fdr</sub></italic> &lt; 0.001; 6<sup>th</sup> rank: t(156)= −10.16, <italic>p<sub>fdr</sub></italic> &lt; 0.001; see <xref ref-type="fig" rid="fig12">Figure S9</xref>).</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Fig. S9.</label>
<caption><title>Neighbouring obstacles predict inclusion/ exclusion from task representation.</title>
<p>Right panel: Results of the ranked regression model for the upfront planning experiment (i.e., dataset Ho 2). Obstacles closest to the probed item (rank 1 &amp; 2) positively impact awareness reports, whereas obstacles furthest from the probed item negatively impact awareness reports (rank 4, 5 &amp; 6).</p>
<p>Left panel: Inter-individual differences in spatial attention effects in dataset Ho 2. Each participant is represented by a point and line.</p></caption>
<graphic xlink:href="2506.09520v1_fig12.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig13" position="float" fig-type="figure">
<label>Fig. S10.</label>
<caption><title>Inter-individual differences in simplified representations.</title>
<p>(a) Analysis pipeline to explore the relationship between spatial attention and taskrepresentations. First, we fit a linear model to beta coefficients obtained in <xref ref-type="fig" rid="fig1">Figure 1b</xref> left for each participant. This slope corresponds to a participant’s attention effect, with more negative slopes indicating larger attention effects. We then computed the sparsity of participant’s representations by computing the mean variance of their awareness reports.</p>
<p>(b) Examples of participants’ task representations. The top row depicts the average awareness of each obstacle in three example mazes. The second row depicts the representations of participants with the most negative attention slopes. The bottom row plots the awareness effects of participants with the shallowest slopes (i.e., weakest spatial attention effects).</p>
<p>(c) Participants with stronger spatial attention effects had more sparse task representations. We replicated this effect in two independent samples across datasets Ho 2 and dSC 1.</p></caption>
<graphic xlink:href="2506.09520v1_fig13.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl6" position="float" orientation="portrait">
<label>Table S6.</label>
<caption><title>Hierarchical linear regression model predicting the awareness of an obstacle from its neighbours for dataset dSC 1.</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl6.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top" colspan="3">Awareness of probe obstacle</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="center" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.00</td>
<td align="center" valign="top">−0.14 – 0.14</td>
<td align="center" valign="top">0.995</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 1 (closest)</td>
<td align="left" valign="top">0.19</td>
<td align="center" valign="top">0.18 – 0.21</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 2</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">−0.00 – 0.02</td>
<td align="center" valign="top">0.115</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 3</td>
<td align="left" valign="top">−0.10</td>
<td align="center" valign="top">−0.11 – −0.08</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 4</td>
<td align="left" valign="top">−0.26</td>
<td align="center" valign="top">−0.27 – −0.25</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 5 (furthest)</td>
<td align="left" valign="top">−0.29</td>
<td align="center" valign="top">−0.30 – −0.27</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top" colspan="4"><bold>Random Effects</bold></td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.65</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.15</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.03</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.21</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">24</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">35</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">19140</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">R<sup>2</sup>/ 0.263 / 0.421</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl7" position="float" orientation="portrait">
<label>Table S7.</label>
<caption><title>Effect of neighbouring obstacles on awareness of probed item, after regressing the effect of the VGC model for dataset dSC 1.</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl7.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top" colspan="3">Awareness residuals</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="center" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">−0.00</td>
<td align="center" valign="top">−0.01 – 0.01</td>
<td align="center" valign="top">1.000</td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 1 (closest)</td>
<td align="left" valign="top">0.23</td>
<td align="center" valign="top">0.22 – 0.24</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 2</td>
<td align="left" valign="top">0.09</td>
<td align="center" valign="top">0.07 – 0.10</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 3</td>
<td align="left" valign="top">−0.08</td>
<td align="center" valign="top">−0.09 – −0.06</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 4</td>
<td align="left" valign="top">−0.22</td>
<td align="center" valign="top">−0.23 – −0.21</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Obstacle 5 (furthest)</td>
<td align="left" valign="top">−0.18</td>
<td align="center" valign="top">−0.20 – −0.17</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">19140</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  R<sup>2</sup> / R<sup>2</sup> adjusted</td>
<td align="left" valign="top" colspan="2">0.213 / 0.213</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="fig14" position="float" fig-type="figure">
<label>Fig. S11.</label>
<caption><title>Spatial proximity predicts awareness for dataset dSC 1.</title>
<p>Standardized beta coefficients of ranked regression model for experiment 3. We observed that obstacles closest to the probed item (rank 1 &amp; 2) positively impact awareness reports, regardless of the type of maze (i.e., lateralized vs non-lateralized; right panel). In contrast, obstacles furthest from the probed item negatively impact awareness reports (rank 5). We note that while the 2<sup>nd</sup> closest obstacle positively predicted the awareness reports, this effect was much weaker than the closest obstacle (i.e., rank 1).</p></caption>
<graphic xlink:href="2506.09520v1_fig14.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s6-2">
<title>Task-relevant lateralization effects</title>
<p>Given the observed spatial attention effects and previous attention literature<sup><xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c41">41</xref></sup> we hypothesized that participants would select task-relevant information with greater ease and therefore form a representation of the stimulus that is more aligned with the predictions of the VGC model when task-relevant obstacles are presented to a single hemifield. To test this, we computed a lateralization index per maze, and used this as a moderator in a hierarchical linear regression model. A summary of the moderation effects can be found in <xref ref-type="table" rid="tbl6">Table S6</xref> for both horizontal and vertical lateralization effects across both experiments.</p>
<table-wrap id="tbl8" position="float" orientation="portrait">
<label>Table S8.</label>
<caption><title>The degree to which task-relevant information is lateralized moderates the relationship between the VGC model and awareness (dataset Ho 1).</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl8.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="center" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.50</td>
<td align="center" valign="top">0.47 – 0.54</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  VGC model</td>
<td align="left" valign="top">0.13</td>
<td align="center" valign="top">0.13 – 0.14</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Lateralization index (vert)</td>
<td align="left" valign="top">0.02</td>
<td align="left" valign="top">−0.01 – 0.04</td>
<td align="left" valign="top">0.238</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model * Lateralization index (vert)</td>
<td align="left" valign="top">0.01</td>
<td align="left" valign="top">0.01 – 0.02</td>
<td align="left" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top" colspan="4"><bold>Random Effects</bold></td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.09</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.03</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.24</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">12</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">161</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">13342</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.138 / 0.348</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl9" position="float" orientation="portrait">
<label>Table S9.</label>
<caption><title>Effect of neighbouring obstacles on awareness of probed item, after regressing the effect of the VGC model.</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl9.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top"/>
<th align="center" valign="top"><italic>Moderation Effect</italic></th>
<th align="center" valign="top"><italic>CI</italic></th>
<th align="center" valign="top"><italic>p</italic></th>
<th align="center" valign="top"><italic>P FDR</italic></th>
<th align="center" valign="top">Marginal R<sup>2</sup>/Conditional R<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top" rowspan="2">dataset Ho</td>
<td align="center" valign="top">vertical</td>
<td align="center" valign="top">0.01</td>
<td align="center" valign="top">0.01 – 0.02</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
<td align="center" valign="top">0.138 / 0.348</td>
</tr>
<tr>
<td align="left" valign="top">horizontal</td>
<td align="center" valign="top">−0.00</td>
<td align="center" valign="top">−0.01 – 0.00</td>
<td align="center" valign="top">0.478</td>
<td align="center" valign="top">0.478</td>
<td align="center" valign="top">0.136 / 0.343</td>
</tr>
<tr>
<td align="left" valign="top" rowspan="2">dataset Ho 2</td>
<td align="center" valign="top">vertical</td>
<td align="center" valign="top">0.01</td>
<td align="center" valign="top">0.00 – 0.02</td>
<td align="center" valign="top"><bold>0.001</bold></td>
<td align="center" valign="top"><bold>0.002</bold></td>
<td align="center" valign="top">0.092 / 0.246</td>
</tr>
<tr>
<td align="left" valign="top">horizontal</td>
<td align="center" valign="top">0.01</td>
<td align="center" valign="top">0.00 – 0.01</td>
<td align="center" valign="top"><bold>0.013</bold></td>
<td align="center" valign="top"><bold>0.017</bold></td>
<td align="center" valign="top">0.091 / 0.244</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl10" position="float" orientation="portrait">
<label>Table S10.</label>
<caption><title>The degree to which task-relevant information is lateralized moderates the relationship between the VGC model and awareness (dataset dSC 1).</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl10.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="center" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="center" valign="top">0.45</td>
<td align="center" valign="top">0.41 – 0.48</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  VGC model</td>
<td align="center" valign="top">0.13</td>
<td align="center" valign="top">0.12 – 0.13</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Lateralization index (vert)</td>
<td align="center" valign="top">−0.00</td>
<td align="center" valign="top">−0.02 – 0.01</td>
<td align="center" valign="top">0.602</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model * Lateralization index (vert)</td>
<td align="center" valign="top">0.01</td>
<td align="center" valign="top">0.01 – 0.02</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top" colspan="4"><bold>Random Effects</bold></td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.09</td>
<td align="center" valign="top"/>
<td align="center" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top"/>
<td align="center" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.10</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">24</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">35</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">19140</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.137 / 0.222</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="s6-3">
<title>Attentional spotlight model</title>
<p>To ensure that our attentional spotlight model results were robust, we conducted control analyses where we ran spatial permutations of our model to ensure that the observed results were not simply due to the spatial smoothness.</p>
<fig id="fig15" position="float" fig-type="figure">
<label>Fig. S12.</label>
<caption><title>Robustness of the attentional spotlight model to spatial autocorrelation.</title>
<p>We assessed the robustness of the spotlight model to spatial autocorrelation by performing two sets of different spatial permutations. (a) In the first set of permutations, we permuted the predictions of the spotlight model across all mazes and used these null predictions in a hierarchical linear regression model. We repeated this procedure 1000 times to produce a null distribution of beta coefficients, depicted in the right panel. The observed spotlight model effect (dotted line) was significantly better than the spatial null permutations.</p>
<p>(b) We similarly permuted the spotlight model’s predictions within each maze, assigning each obstacle a random prediction. We used these null predictions in a hierarchical regression model to predict participants’ awareness reports. We repeated this procedure 1000 times to generate a null distribution, depicted in the right panel. The spotlight model predicted participants’ awareness reports beyond the spatial autocorrelation of the data.</p></caption>
<graphic xlink:href="2506.09520v1_fig15.tif" mime-subtype="tif" mimetype="image"/>
</fig>
</sec>
<sec id="s6-4">
<title>Sensitivity analyses</title>
<fig id="fig16" position="float" fig-type="figure">
<label>Fig. S13.</label>
<caption><title>Eye position during planning.</title>
<p>We verified that our behavioural effects were not driven by eye movements during the planning phase in dataset dSC 1. (a) The fluctuations of eye position, measured as the standard deviation of the eye-position time series, did not statistically differ between non-lateralized and lateralized maze stimuli. (b) Time series of the average position of participants’ gaze during planning along the horizontal and vertical axis, left and right panels, respectively. Participants, on average, moved their eyes more toward the left when maze stimuli were lateralized to the left (yellow line) and toward the right when the maze stimuli were lateralized to the right (orange line). These eye movements, however, remained within the bounds of the central square (dotted grey lines). Eye gaze did not differ between maze stimuli along the y-axis. (c) Heat maps depicting where participants looked during the planning phase for non-lateralized, right-lateralized, and left-lateralized maze stimuli.</p></caption>
<graphic xlink:href="2506.09520v1_fig16.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<fig id="fig17" position="float" fig-type="figure">
<label>Fig. S14.</label>
<caption><title>Attention lateralization effects are robust to eye-gaze.</title>
<p>Density plots of the reported awareness of obstacles separated task-relevant (&gt;0.5; in orange) or task-irrelevant (&lt; 0.5; in grey) obstacles as predicted by the VGC model for trials with minimal eye movements. Participants were more likely to be aware of taskrelevant obstacles and unaware of irrelevant obstacles. This effect was moderated by the degree to which task-relevant information was presented preferentially to one hemifield (x-axis). From left to right, we plot the three terciles of maze lateralization. Participants’ awareness reports become increasingly aligned with the VGC model’s predictions—i.e., the overlap between the two density plots decreases with lateralization.</p></caption>
<graphic xlink:href="2506.09520v1_fig17.tif" mime-subtype="tif" mimetype="image"/>
</fig>
<table-wrap id="tbl11" position="float" orientation="portrait">
<label>Table S11.</label>
<caption><title>Robustness of lateralization moderation effect to nuisance covariates (dataset Ho 1).</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl11.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="center" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.40</td>
<td align="center" valign="top">0.26 – 0.55</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  goal distance</td>
<td align="left" valign="top">0.04</td>
<td align="center" valign="top">0.03 – 0.05</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  start distance</td>
<td align="left" valign="top">0.03</td>
<td align="center" valign="top">0.02 – 0.03</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  wall distance</td>
<td align="left" valign="top">−0.02</td>
<td align="center" valign="top">−0.08 – 0.03</td>
<td align="center" valign="top">0.354</td>
</tr>
<tr>
<td align="left" valign="top">  center distance</td>
<td align="left" valign="top">0.02</td>
<td align="center" valign="top">−0.01 – 0.05</td>
<td align="center" valign="top">0.129</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model</td>
<td align="left" valign="top">0.14</td>
<td align="center" valign="top">0.14 – 0.15</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Lateralization index (vert)</td>
<td align="left" valign="top">0.02</td>
<td align="center" valign="top">−0.01 – 0.04</td>
<td align="center" valign="top">0.196</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model * Lateralization index (vert)</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">0.00 – 0.01</td>
<td align="center" valign="top">0.004</td>
</tr>
<tr>
<td align="left" valign="top" colspan="4"><bold>Random Effects</bold></td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.08</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.03</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.24</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">12</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">161</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">13342</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.148 / 0.356</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl12" position="float" orientation="portrait">
<label>Table S12.</label>
<caption><title>Robustness of lateralization moderation effect to nuisance covariates (dataset Ho 2).</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl12.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="left" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.43</td>
<td align="center" valign="top">0.27 – 0.59</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  goal distance</td>
<td align="left" valign="top">0.06</td>
<td align="center" valign="top">0.05 – 0.07</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  start distance</td>
<td align="left" valign="top">0.05</td>
<td align="center" valign="top">0.04 – 0.06</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  wall distance</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">−0.05 – 0.07</td>
<td align="center" valign="top">0.729</td>
</tr>
<tr>
<td align="left" valign="top">  center distance</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">−0.02 – 0.04</td>
<td align="center" valign="top">0.427</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model</td>
<td align="left" valign="top">0.13</td>
<td align="center" valign="top">0.13 – 0.14</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Lateralization index (vert)</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">−0.01 – 0.03</td>
<td align="center" valign="top">0.320</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model * Lateralization index (vert)</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">0.00 – 0.01</td>
<td align="center" valign="top"><bold>0.013</bold></td>
</tr>
<tr>
<td align="left" valign="top" colspan="4"><bold>Random Effects</bold></td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.10</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.02</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.17</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">12</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">162</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">13321</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.113 / 0.267</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl13" position="float" orientation="portrait">
<label>Table S13.</label>
<caption><title>Robustness of lateralization moderation effect to nuisance covariates (dataset dSC 1).</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl13.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="left" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.45</td>
<td align="center" valign="top">0.41 – 0.49</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  goal distance</td>
<td align="left" valign="top">−0.01</td>
<td align="center" valign="top">−0.04 – 0.02</td>
<td align="center" valign="top">0.379</td>
</tr>
<tr>
<td align="left" valign="top">  start distance</td>
<td align="left" valign="top">−0.02</td>
<td align="center" valign="top">−0.04 – 0.01</td>
<td align="center" valign="top">0.258</td>
</tr>
<tr>
<td align="left" valign="top">  wall distance</td>
<td align="left" valign="top">−0.03</td>
<td align="center" valign="top">−0.12 – 0.06</td>
<td align="center" valign="top">0.545</td>
</tr>
<tr>
<td align="left" valign="top">  center distance</td>
<td align="left" valign="top">0.03</td>
<td align="center" valign="top">−0.07 – 0.12</td>
<td align="center" valign="top">0.579</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model</td>
<td align="left" valign="top">0.13</td>
<td align="center" valign="top">0.12 – 0.13</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Lateralization index (vert)</td>
<td align="left" valign="top">−0.00</td>
<td align="center" valign="top">−0.02 – 0.02</td>
<td align="center" valign="top">0.772</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model * Lateralization index (vert)</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">0.01 – 0.02</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top" colspan="4"><bold>Random Effects</bold></td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.09</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.01</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.10</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">24</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">35</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">19140</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.136 / 0.222</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl14" position="float" orientation="portrait">
<label>Table S14.</label>
<caption><title>Horizontal lateralization moderation regression (dataset Ho 2).</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl14.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="left" valign="top" colspan="3">Awareness</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="left" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.43</td>
<td align="center" valign="top">0.28 – 0.59</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  goal distance</td>
<td align="left" valign="top">0.06</td>
<td align="center" valign="top">0.05 – 0.07</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  start distance</td>
<td align="left" valign="top">0.05</td>
<td align="center" valign="top">0.04 – 0.06</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  wall distance</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">−0.05 – 0.07</td>
<td align="center" valign="top">0.714</td>
</tr>
<tr>
<td align="left" valign="top">  center distance</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">−0.02 – 0.04</td>
<td align="center" valign="top">0.462</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model</td>
<td align="left" valign="top">0.13</td>
<td align="center" valign="top">0.13 – 0.14</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Lateralization index (horz)</td>
<td align="left" valign="top">−0.01</td>
<td align="center" valign="top">−0.03 – 0.01</td>
<td align="center" valign="top">0.205</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model * Lateralization index (horz)</td>
<td align="left" valign="top">0.00</td>
<td align="center" valign="top">−0.00 – 0.01</td>
<td align="center" valign="top">0.686</td>
</tr>
<tr>
<td align="left" valign="top" colspan="4">Random Effects</td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.10</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.02</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.17</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>grid</sub></td>
<td align="left" valign="top">12</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">162</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">13321</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.111 / 0.265</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl15" position="float" orientation="portrait">
<label>Table S15.</label>
<caption><title>Horizontal lateralization moderation regression (dataset dSC 1).</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl15.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="left" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.45</td>
<td align="center" valign="top">0.41 – 0.48</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  goal distance</td>
<td align="left" valign="top">−0.01</td>
<td align="center" valign="top">−0.04 – 0.02</td>
<td align="center" valign="top">0.635</td>
</tr>
<tr>
<td align="left" valign="top">  start distance</td>
<td align="left" valign="top">−0.01</td>
<td align="center" valign="top">−0.04 – 0.01</td>
<td align="center" valign="top">0.378</td>
</tr>
<tr>
<td align="left" valign="top">  wall distance</td>
<td align="left" valign="top">−0.06</td>
<td align="center" valign="top">−0.15 – 0.03</td>
<td align="center" valign="top">0.206</td>
</tr>
<tr>
<td align="left" valign="top">  center distance</td>
<td align="left" valign="top">0.06</td>
<td align="center" valign="top">−0.03 – 0.15</td>
<td align="center" valign="top">0.215</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model</td>
<td align="left" valign="top">0.13</td>
<td align="center" valign="top">0.12 – 0.13</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Lateralization index (horz)</td>
<td align="left" valign="top">0.02</td>
<td align="center" valign="top">−0.00 – 0.04</td>
<td align="center" valign="top">0.095</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model * Lateralization index (horz)</td>
<td align="left" valign="top">−0.00</td>
<td align="center" valign="top">−0.01 – 0.00</td>
<td align="center" valign="top">0.349</td>
</tr>
<tr>
<td align="left" valign="top" colspan="4">Random Effects</td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.09</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 SubjectID</sub></td>
<td align="left" valign="top">0.01</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  T<sub>00 MazelD</sub></td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.10</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>MazeID</sub></td>
<td align="left" valign="top">24</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N <sub>SubjectID</sub></td>
<td align="left" valign="top">35</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">19140</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.134 / 0.218</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="tbl16" position="float" orientation="portrait">
<label>Table S16.</label>
<caption><title>Lateralization moderation effect on trials with minimal eye movements (dataset dSC 1).</title></caption>
<alternatives>
<graphic xlink:href="2506.09520v1_tbl16.tif" mime-subtype="tif" mimetype="image"/>
<table frame="box" rules="all">
<thead>
<tr>
<th align="left" valign="top"/>
<th align="center" valign="top" colspan="3">Awareness</th>
</tr>
<tr>
<th align="left" valign="top">Predictors</th>
<th align="left" valign="top">Estimates</th>
<th align="center" valign="top">CI</th>
<th align="center" valign="top">p</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">  (Intercept)</td>
<td align="left" valign="top">0.45</td>
<td align="center" valign="top">0.41 – 0.48</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  goal distance</td>
<td align="left" valign="top">−0.01</td>
<td align="center" valign="top">−0.04 – 0.02</td>
<td align="center" valign="top">0.360</td>
</tr>
<tr>
<td align="left" valign="top">  start distance</td>
<td align="left" valign="top">−0.02</td>
<td align="center" valign="top">−0.04 – 0.01</td>
<td align="center" valign="top">0.270</td>
</tr>
<tr>
<td align="left" valign="top">  wall distance</td>
<td align="left" valign="top">−0.03</td>
<td align="center" valign="top">−0.12 – 0.06</td>
<td align="center" valign="top">0.525</td>
</tr>
<tr>
<td align="left" valign="top">  center distance</td>
<td align="left" valign="top">0.03</td>
<td align="center" valign="top">−0.06 – 0.12</td>
<td align="center" valign="top">0.570</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model</td>
<td align="left" valign="top">0.13</td>
<td align="center" valign="top">0.13 – 0.13</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top">  Lateralization index (vert)</td>
<td align="left" valign="top">−0.00</td>
<td align="center" valign="top">−0.02 – 0.02</td>
<td align="center" valign="top">0.740</td>
</tr>
<tr>
<td align="left" valign="top">  VGC model * Lateralization index (vert)</td>
<td align="left" valign="top">0.01</td>
<td align="center" valign="top">0.01 – 0.02</td>
<td align="center" valign="top"><bold>&lt;0.001</bold></td>
</tr>
<tr>
<td align="left" valign="top" colspan="4">Random Effects</td>
</tr>
<tr>
<td align="left" valign="top">  σ<sup>2</sup></td>
<td align="left" valign="top">0.09</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Too SubjectID</td>
<td align="left" valign="top">0.01</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Too MazelD</td>
<td align="left" valign="top">0.00</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  ICC</td>
<td align="left" valign="top">0.10</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N MazeID</td>
<td align="left" valign="top">24</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  N SubjectID</td>
<td align="left" valign="top">35</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Observations</td>
<td align="left" valign="top">16686</td>
<td align="left" valign="top"/>
<td align="left" valign="top"/>
</tr>
<tr>
<td align="left" valign="top">  Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>
<td align="left" valign="top" colspan="2">0.139 / 0.223</td>
<td align="left" valign="top"/>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108034.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Faivre</surname>
<given-names>Nathan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6011-4921</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>Centre National de la Recherche Scientifique</institution>
</institution-wrap>
<city>Grenoble</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study utilizes behavioral data and computational modeling to show that spatial properties of visual attention affect human planning. The methodology and statistical analyses are <bold>solid</bold>, though the way attention is conceptualized and modeled could be refined. The findings of this study will interest cognitive scientists studying attention, perception, and decision-making.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108034.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:
This study investigated how visuospatial attention influences the way people build simplified mental representations to support planning and decision-making. Using computational modeling and virtual maze navigation, the authors examined whether spatial proximity and the spatial arrangement of obstacles determine which elements are included in participants' internal models of a task. The study developed and tested an extension of the value-guided construal (VGC) model that incorporates features of spatial attention for selecting simpler task mental representation.</p>
<p>Strengths:</p>
<p>(1) Original Perspective:
The study introduces an explicit attentional component to established models of planning, offering an approach that bridges perception, attention, and decision-making.</p>
<p>(2) Methodological Approach:
The combination of computational modeling, behavioral data, and eye-tracking provides converging measures to assess the relationship between attention and planning representations.</p>
<p>(3) Cross-validated data:
The study relies on the analysis of three separate datasets, two already published and an additional novel one. This allows for cross-validation of the findings and enhances the robustness of the evidence.</p>
<p>(4) Focus on Individual Differences:
Reports of how individual variability in attentional &quot;spillover&quot; correlates with the sparsity of task representations and spatial proximity add depth to the analysis.</p>
<p>Weaknesses:</p>
<p>(1) Clarity of the VGC model and behavioral task:
The exposition of the VGC model lacks sufficient detail for non-expert readers. It is not clear how this model infers which maze obstacles are relevant or irrelevant for planning, nor how the maze tasks specifically operationalize &quot;planning&quot; versus other cognitive processes.</p>
<p>The method for classifying obstacles as relevant or irrelevant to the task and connecting metacognitive awareness (i.e., participants' reports of noticing obstacles) to attentional capture is not well justified. The rationale for why awareness serves as a valid attention proxy, as opposed to behavioral or neurophysiological markers, should be clearer.</p>
<p>(2) Attention framework:
The account of attention is largely limited to the &quot;spotlight&quot; model. When solving a maze, participants trace the correct trail, following it mentally with their overt or covert attention. In this perspective, relevant concepts are also rooted in attention literature pertaining to object-based attention using tasks like curve tracing (e.g., Pooresmaeili &amp; Roelfsema, 2014) and to mental maze solving (e.g., Wong &amp; Scholl, 2024), which may be highly relevant and add nuance to the current work. This view of attention may be more pertinent to the task than models of simultaneously tracking multiple objects cited here. Prior work (notably from the Roelfsema group) indicates that attentional engagement in curve-tracing tasks may be a continuous, bottom-up process that progressively spreads along a trajectory, in time and space, rather than a &quot;spotlight&quot; that simply travels along the path. The spread of attention depends on the spatial proximity to distractors - a point that could also be pertinent to the findings here.</p>
<p>Moreover, the tracing of a &quot;solution&quot; trail in a maze may be spontaneous and not only a top-down voluntary operation (Wong &amp; Scholl, 2024), a finding that requires a more careful framing of the link to conscious perception discussed in the manuscript.</p>
<p>Conceptualizing attention as a spatial spotlight may therefore oversimplify its role in navigation and planning. Perhaps the observed attentional modulation reflects a perceptual stage of building the trail in the maze rather than a filter for a later representation for more efficient decision making and planning. A fuller discussion of whether the current model and data can distinguish between these frameworks would benefit readers.</p>
<p>(3) Lateralization of attention:
The analysis considers whether relevant information is distributed bilaterally or unilaterally across the visual display, but does not sufficiently address evidence for attentional asymmetries across the left and right visual fields due to hemispheric specialization (e.g., Bartolomeo &amp; Seidel Malkinson, 2019). Whether effects differ for left versus right hemifield arrangements is not made explicit in the presented findings.</p>
<p>(4) Individual differences:
Individual differences in attentional modulation are a strength of the work, but similar analyses exploring individual variation in lateralization effects could provide further insight, and the lack of such analyses may mask important effects.</p>
<p>(5) Distinction between overt and covert attention:
The current report at times equates eye movement patterns with the locus of attention. However, attention can be covertly shifted without corresponding gaze changes (see, for example, Pooresmaeili &amp; Roelfsema, 2014).</p>
<p>The implications for interpreting the relationship between eye movement, memory, and attention in this setting are not fully addressed. The potential dynamics of attention along a maze trajectory and their impact on lateralization analysis would benefit from further clarification.</p>
<p>Appraisal of Aims and Results:</p>
<p>The study sets out to determine how spatial attention shapes the construction of task representations in planning contexts. The authors provide evidence that spatial proximity and arrangement influence which environmental features are incorporated into internal models used for navigation, and that accounting for these effects improves model predictions. There is clear documentation of individual variation, with some participants showing greater attentional spillover and more sparse awareness profiles.</p>
<p>However, some conceptual and methodological aspects would be clearer with greater engagement with the broader literature on attention dynamics, a more explicit justification of operational choices, and more targeted lateralization analyses.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108034.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Castanheira et al. investigate the role of spatial attention for planning during three maze navigation experiments (one new experiment and two existing datasets). Effective planning in complex situations requires the construction of simplified representations of the task at hand. The authors find that these mental representations (as assessed by conscious awareness) of a given stimulus are influenced by (spatially) surrounding stimuli. Individual participants varied in the degree to which attention influenced their task representations, and this attentional effect correlated with the sparsity of representations (as measured by the range of awareness reports across all stimuli). Spatially grouping task-relevant information on either the left or right side of the maze led to mental representations more similar to optimal representations predicted by the value-guided construal (VGC) model - a normative model describing a theoretical approach to simplifying complex task information. Finally, the authors propose an update to this model, incorporating an attentional spotlight component; the revised descriptive model predicts empirical task representations better than the original (normative) VGC model.</p>
<p>Strengths:</p>
<p>The novelty of this study lies in the proposal and investigation of a cognitive mechanism through which a normative model like value-guided construal can enable human planning. After proposing attention as this mechanism, the authors make concrete hypotheses about mismatches between the VGC predictions and real human behavior, which are experimentally validated. Thus, not only does this study describe a possible mechanism for simplification of task information for planning, but the authors also propose a descriptive model, revising VGC to incorporate this attentional component.</p>
<p>A strength of this paper is the variety of investigative approaches: analysis of existing data, novel experiment, and a computational approach to predict experimental findings from a theoretical model. Analyzing pre-existing datasets increases the size of the participant cohort and strengthens the authors' conclusions. Meanwhile, comparing the predictions of the existing normative model and the authors' own refined model is a clever approach to substantiate their claims. In addition, the authors describe several crucial controls, which are key to the interpretability of their results. In particular, the eye tracking results were critical.</p>
<p>In summary, this paper constitutes an important step toward a more complete understanding of the human ability to plan.</p>
<p>Weaknesses:</p>
<p>(1) There is a critical conceptual gap in the study and its interpretation, mainly due to the reliance on a self-report metric of awareness (rather than an objective measure of behavioral performance).</p>
<p>a. Awareness is tested by a 9-point self-report scale. It is currently unclear why awareness of task-irrelevant obstacles in this task would necessarily compromise optimal planning. There is no indication of whether self-reported awareness affects performance (e.g., navigation path distance, time to complete the maze, number of errors). Such behavioral evidence of planning would be more compelling.</p>
<p>b. Relatedly, it would have been more convincing to have an objective measure of awareness, for instance, how the presence or absence of a &quot;task-irrelevant&quot; obstacle affects performance (e.g., change navigation path distance or time to complete the maze), or whether participants can accurately recall the location of obstacles.</p>
<p>c. Consequently, I'm not sure that we can conclude that the spatial context does impact participants' ability to plan spatial navigation or to &quot;incorporate task-relevant information into their construal&quot;. We know that the spatial context affects subjective (self-reported) awareness, but the authors do not present evidence that spatial context affects behavioral performance.</p>
<p>d. Another concern that may complicate interpretation is the following: Figure 3c shows improved VGC model predictions (steeper slope) for mazes with greater lateralization. However, there are notable outliers in these plots, where a high lateralization index does not correspond to good model performance. There is currently no discussion/explanation of these cases.</p>
<p>(2) I noticed an issue with clarity regarding task-relevance. It is currently not fully clear which obstacles are &quot;task irrelevant&quot;. Also, the term is used inconsistently, sometimes conflating with &quot;awareness&quot;. For example, in the &quot;Attentional spotlight model of task representations&quot; section, the authors state that &quot;task-relevant information becomes less relevant when surrounded by task-irrelevant information&quot;. But they really mean that participants become less aware of those task-relevant obstacles. I assume task-relevance is an objective characteristic related to maze organization, not to a participant's construal. Indeed, the following paragraph provides evidence of model predictions of awareness.</p>
<p>(3) The behavioral paradigm has some distinct disadvantages, and the validity of the task is not backed up by behavioral data.</p>
<p>a. I understand the need for central fixation, but it also makes the task less naturalistic.</p>
<p>b. The task with its top-down grid view does not seem to mimic real human navigation. Though this grid may be similar to mental maps we form for navigation, the sensory stimuli corresponding to possible paths and to spatial context during real-life navigation are very different.</p>
<p>c. Behavioral performance is not reported, so it is unknown whether participants are able to properly complete the task. The task seems pretty difficult to navigate, especially when the obstacles disappear, and in combination with the central fixation.</p>
<p>d. There is no discussion of whether/how this navigation task generalizes to other forms of planning.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108034.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors build on a recent computational model of planning, the &quot;value-guided construal&quot; framework by Ho et al. (2022), which proposes that people plan by constructing simple models of a task, such as by attending to a subset of obstacles in a maze. They analyze both published experimental data and new experimental data from a task in which participants report attention to objects in mazes. The authors find that attention to objects is affected by spatial proximity to other objects (i.e., attentional overspill) as well as whether relevant objects are lateralized to the same hemifield. To account for these results, the authors propose a &quot;spotlight-VGC&quot; model, in which, after calculating attention scores based on the original VGC model, attention to objects is enhanced based on distance. They find that this model better explains participant responses when objects are lateralized to different hemifields. These results demonstrate complex interactions between filtering of task-relevant information and more classical signatures of attentional selection.</p>
<p>Strengths:</p>
<p>(1) The paper builds on existing modeling work in a novel manner and integrates classic results on attention into the computational framework.</p>
<p>(2) The authors report new and extensive analyses of existing data that shed light on additional sources of systematic variability in responses related to attentional spillover effects</p>
<p>(3) They collect new data using new stimuli in the original paradigm that directly test predictions related to the lateralization of task-relevant information, including eye tracking data that allows them to control for possible confounds.</p>
<p>(4) The extended model (spotlight-VGC) provides a formal account of these new results.</p>
<p>Weaknesses:</p>
<p>(1) The spotlight-VGC model has a free parameter - the &quot;width&quot; of the attentional spotlight. This seems to have been fixed to be 3 squares. It would be good if the authors could describe a more principled procedure for selecting the width so that others can use the model in other contexts.</p>
<p>(2) Have the authors considered other ways in which factors such as attentional spillover and lateralization could be incorporated into the model? The spotlight-VGC model, as presented, involves first computing VGC predictions and only afterwards computing spillover. This seems psychologically implausible, since it supposes that the &quot;optimal&quot; representation is first formed and then it gets corrupted. Is there a way to integrate these biases directly into the VGC framework, perhaps as a prior on construals? The authors gesture towards this when they talk about &quot;inductive biases&quot;, but this is not formalized.</p>
<p>(3) Can the authors rule out that the lateralization effects are the result of memory biases since the main measure used is a self-report of attention?</p>
</body>
</sub-article>
</article>