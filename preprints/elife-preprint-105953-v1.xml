<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">105953</article-id>
<article-id pub-id-type="doi">10.7554/eLife.105953</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.105953.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Top-down feedback matters: Functional impact of brainlike connectivity motifs on audiovisual integration</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0007-9213-9366</contrib-id>
<name>
<surname>Tugsbayar</surname>
<given-names>Mashbayar</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<email>mashbayar.tugsbayar@mila.quebec</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Mingze</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4309-8266</contrib-id>
<name>
<surname>Muller</surname>
<given-names>Eilif B</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9662-2151</contrib-id>
<name>
<surname>Richards</surname>
<given-names>Blake</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McGill University</institution></institution-wrap>, <city>Montréal</city>, <country country="CA">Canada</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0161xgx34</institution-id><institution>Department of Neurosciences, Faculty of Medicine, Université de Montréal</institution></institution-wrap>, <city>Montréal</city>, <country country="CA">Canada</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gv74p78</institution-id><institution>Centre de Recherche Azrieli du CHU Sainte-Justine</institution></institution-wrap>, <city>Montréal</city>, <country country="CA">Canada</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05c22rx21</institution-id><institution>Mila Quebec AI Institute</institution></institution-wrap>, <city>Montréal</city>, <country country="CA">Canada</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Gjorgjieva</surname>
<given-names>Julijana</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Technical University of Munich</institution>
</institution-wrap>
<city>Freising</city>
<country>Germany</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-04-09">
<day>09</day>
<month>04</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP105953</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-01-24">
<day>24</day>
<month>01</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-02-13">
<day>13</day>
<month>02</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.10.01.615270"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Tugsbayar et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Tugsbayar et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-105953-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Artificial neural networks (ANNs) are an important tool for studying neural computation, but many features of the brain are not captured by standard ANN architectures. One notable missing feature in most ANN models is top-down feedback, i.e. projections from higher-order layers to lower-order layers in the network. Top-down feedback is ubiquitous in the brain, and it has a unique modulatory impact on activity in neocortical pyramidal neurons. However, we still do not understand its computational role. Here we develop a deep neural network model that captures the core functional properties of top-down feedback in the neocortex, allowing us to construct hierarchical recurrent ANN models that more closely reflect the architecture of the brain. We use this to explore the impact of different hierarchical recurrent architectures on an audiovisual integration task. We find that certain hierarchies, namely those that mimic the architecture of the human brain, impart ANN models with a light visual bias similar to that seen in humans. This bias does not impair performance on the audiovisual tasks. The results further suggest that different configurations of top-down feedback make otherwise identically connected models functionally distinct from each other, and from traditional feedforward-only models. Altogether our findings demonstrate that modulatory top-down feedback is a computationally relevant feature of biological brains, and that incorporating it into ANNs affects their behavior and helps to determine the solutions that the network can discover.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Fixed Fig 3 (was erroneously the same as Fig 2), minor wording changes to abstract</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Artificial neural networks (ANNs) often draw inspiration from the brain and serve in turn as tools with which to model it (<xref ref-type="bibr" rid="c13">Doerig et al., 2023</xref>). For example, deep convolutional neural networks (CNN)– originally inspired by hierarchical feature detection in the mammalian brain (<xref ref-type="bibr" rid="c18">Fukushima, 1980</xref>)— predict behavior and representations in visual cortex with high accuracy (<xref ref-type="bibr" rid="c7">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="c82">Yamins &amp; DiCarlo, 2016</xref>). However, there are still significant gaps in most deep neural networks’ ability to predict behavior, particularly when presented with ambiguous, challenging stimuli (<xref ref-type="bibr" rid="c20">Geirhos et al., 2018</xref>; <xref ref-type="bibr" rid="c32">Kar et al., 2019</xref>). This gap is thought to be a result, in part, of the exclusively feedforward structure of CNNs, as opposed to the biological brain, which consists of feedforward as well as local and top-down recurrent connections (<xref ref-type="bibr" rid="c32">Kar et al., 2019</xref>; <xref ref-type="bibr" rid="c76">van Bergen &amp; Kriegeskorte, 2020</xref>). Local recurrence is modeled in a subfamily of ANNs known as recurrent neural networks (RNNs), and there have been studies using RNNs to model sensory processes in the brain (<xref ref-type="bibr" rid="c36">Kubilius et al., 2018</xref>). In contrast, top-down feedback has largely been neglected in deep models, with a few exceptions (<xref ref-type="bibr" rid="c29">Islah et al., 2023</xref>; <xref ref-type="bibr" rid="c53">Naumann et al., 2022</xref>; <xref ref-type="bibr" rid="c75">Tsai et al., 2024</xref>; <xref ref-type="bibr" rid="c81">Wybo et al., 2022</xref>). It’s unclear whether or how this omission hinders our ability to accurately model the brain.</p>
<p>Importantly, top-down feedback connections are functionally and physiologically distinct from feed-forward connections. They typically connect higher order association areas to lower order sensory areas (<xref ref-type="bibr" rid="c15">Felleman &amp; Van Essen, 1991</xref>). Data suggests that top-down inputs in the neocortex are modulatory (<xref ref-type="bibr" rid="c37">Larkum, 2004</xref>; <xref ref-type="bibr" rid="c72">Sherman &amp; Guillery, 1998</xref>), meaning they alter the magnitude of neural activity, but do not drive it themselves. It would be beneficial for computational neuroscience if ANN models allowed us to explore the potential unique impacts of top-down feedback on computation in the neocortex.</p>
<p>Here, we examine the impact of different architectures with top-down feedback connections on an audiovisual categorization task. We chose audiovisual tasks because feedback connections can also span sensory modalities, and the auditory areas (which project directly to V1 in primates (<xref ref-type="bibr" rid="c10">Clavagnier et al., 2004</xref>)) are speculated to sit higher on the sensory-association hierarchy than primary visual areas (<xref ref-type="bibr" rid="c35">King &amp; Nelken, 2009</xref>). As such, audio modulation of visual processing likely occurs via top-down feedback across multiple cortical sites. Audiovisual interplay produces many well-known functional effects, such as the McGurck effect (<xref ref-type="bibr" rid="c47">McGurck &amp; Macdonald, 1976</xref>) and sound-induced flash illusions (<xref ref-type="bibr" rid="c71">Shams et al., 2000</xref>), making it a particularly interesting testbed for studies of the computational role of top-down feedback.</p>
<p>To study the effect of top-down feedback on such tasks, we built a freely available code base for creating deep neural networks with an algorithmic approximation of top-down feedback. Specifically, top-down feedback was designed to modulate ongoing activity in recurrent, convolutional neural networks. We explored different architectural configurations of connectivity, including a configuration based on the human brain, where all visual areas send feedforward inputs to, and receive top-down feedback from, the auditory areas. The human brain-based model performed well on all audiovisual tasks, but displayed a unique and persistent visual bias compared to feedforward-only models and models with different hierarchies. This qualitatively matches the reported visual bias of humans engaged in audio-visual tasks (<xref ref-type="bibr" rid="c61">Posner et al., 1976</xref>; <xref ref-type="bibr" rid="c74">Stokes &amp; Biggs, 2014</xref>). Our results confirm that distinct configurations of feedforward/feedback connectivity have an important functional impact on a model’s behavior. Therefore, top-down feedback is a relevant feature that should be considered for deep ANN models in computational neuroscience more broadly.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s3">
<label>2.1</label>
<title>Framework for modeling top-down feedback with RNNs</title>
<p>Based on neurophysiological data, we implemented a deep neural network model where each layer corresponds to a brain region and receives two different types of input: feedforward and feedback. Feedforward input drives the activity of the layer as it does in regular ANNs. In contrast, inspired by the physiology of apical dendrites in pyramidal neurons, feedback input to each neuron is integrated and then run through another non-linear function. The result of this is then multiplied with the integrated feedforward activation (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>, left). As such, feedback input cannot activate a neuron that is not receiving feedforward input, nor can it decrease the activity of the neuron, but it can modulate its level of activity (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>, right). This mimics the modulatory role of top-down feedback in the neocortex as observed in neurophysiological experiments (<xref ref-type="bibr" rid="c37">Larkum, 2004</xref>; <xref ref-type="bibr" rid="c46">McAdams &amp; Maunsell, 1999</xref>). But, we note that experimental and modeling research suggests that top-down feedback can sometimes be weakly driving (<xref ref-type="bibr" rid="c37">Larkum, 2004</xref>; <xref ref-type="bibr" rid="c64">Reynolds et al., 2000</xref>; <xref ref-type="bibr" rid="c70">Shai et al., 2015</xref>), i.e. it can reduce the threshold for activation for the neuron, a form of feedback referred to as “composite” feedback (<xref ref-type="bibr" rid="c70">Shai et al., 2015</xref>). We explore the impact of both purely multiplicative feedback and composite feedback below.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Description of models.</title>
<p><bold>a</bold>, Each area of the model receives driving feedforward input and modulatory feedback input. Feedback input alters the gain of a neuron, but it doesn’t affect its threshold of activation (multiplicative feedback). In later experiments, we explore an alternative mechanism, where it weakly affects the threshold of activation (composite feedback). <bold>b</bold>, Modeled regions and their externopyramidisation values (i.e thickness and relative differentiation of supragranular layers, used as proxy measure for sensory-associational hierarchical position). Note higher overall externopyramidisation values in the occipital lobe compared to the temporal lobe. <bold>c</bold>, Using above hierarchical measures, we constructed models where each connection has a direction (i.e regions send either feedforward or feedback connections to other regions). In the brainlike model based on human cytoarchitectural data, where all visual regions send feedforward to and receive feedback from the auditory regions. In the reverse model, all auditory regions send feedforward to and receive feedback from visual regions, while connections within a modality remain the same. <bold>d</bold>, The resulting ANN. Outputs of image identification tasks are read out taken from IT, outputs of audio identification tasks are read out from A4, an auditory associational area. Connections between modules are simplified for illustration.</p></caption>
<graphic xlink:href="615270v4_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We implemented retinotopic and tonotopic organization, as well as local recurrence, by using a Convolutional Gated Recurrent Unit (GRU) to model each brain region (<xref ref-type="bibr" rid="c3">Ballas et al., 2016</xref>; <xref ref-type="bibr" rid="c8">Cho et al., 2014</xref>). These models were end-to-end differentiable, and therefore, we could train them with backpropagation-of-error. Importantly, there is nothing about this modeling framework that demands a specific type of training, meaning that the models created within this framework can undergo supervised, unsupervised, or reinforcement learning, though here we used supervised tasks (see Methods).</p>
<p>For the audiovisual categorization tasks, we constructed a series of ANNs with four layers corresponding to ventral visual regions (V1, V2, V4, and IT) and three layers corresponding to ventral auditory regions (A1, Belt, A4). For one ANN, which we will call the “brainlike” model, we used the externopyramidisation of each cortical area as seen in human histological data to determine the hierarchical position of said area in the network (<xref rid="fig1" ref-type="fig">Fig 1B</xref>). Externopyramidisation refers to the relative differentiation of the supragranular layers in the cortex (<xref ref-type="bibr" rid="c69">Sanides, 1962</xref>). Percentage of supragranular projecting neurons (SLN) is a longstanding measure of hierarchical distance in mammals, based on experimental observations that long-range feedforward connections originate primarily from the supragranular layers, while feedback connection are infragranular in origin (<xref ref-type="bibr" rid="c4">Barone et al., 2000</xref>; <xref ref-type="bibr" rid="c15">Felleman &amp; Van Essen, 1991</xref>; <xref ref-type="bibr" rid="c44">Markov &amp; Kennedy, 2013</xref>). Externopyramidisation is an indirect estimate of supragranular projecting neurons, based on observations in primates that feedforward-projecting sensory areas feature dense, highly differentiated supragranular layers (<xref ref-type="bibr" rid="c21">Gerbella et al., 2007</xref>) while feedback-projecting transmodal areas have less differentiated supragranular layers and a larger infragranular layers (<xref ref-type="bibr" rid="c49">Morecraft et al., 2012</xref>). Researchers thus assume that higher externopyramidisation scores (i.e. thicker and more differentiated supragranular layers) indicates a “lower order” region that sends more feedforward connections (<xref ref-type="bibr" rid="c24">Goulas et al., 2018</xref>). We make the same assumption in-line with previous literature quantifying hierarchical distance from human histological data (<xref rid="c55" ref-type="bibr">Paquola et al., 2019</xref>; <xref ref-type="bibr" rid="c67">Saberi et al., 2023</xref>), but we note that our modeling framework could easily use other metrics for determining position in the cortical hierarchy (<xref ref-type="bibr" rid="c57">Paquola et al., 2020</xref>; <xref ref-type="bibr" rid="c78">Wagstyl et al., 2015</xref>; <xref ref-type="bibr" rid="c84">Zilles &amp; Amunts, 2009</xref>).</p>
<p>This resulted in an architecture wherein visual areas provided feedforward input to all auditory areas, whereas auditory areas provided feedback to visual areas (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>, left), similar to an existing hypothesis on primate audiovisual processing (<xref ref-type="bibr" rid="c35">King &amp; Nelken, 2009</xref>). In another ANN, that we call the “reversed” model, the directionality was reversed, i.e. all auditory areas feed forward to the visual areas whereas visual areas provide feedback to auditory areas. In this model, directionality of connections within a modality were kept the same as the brain-based model (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>, middle). In three additional control ANNs, the directionality of connections between each pair of regions was determined by a coin toss (<xref rid="fig1" ref-type="fig">Fig 1C</xref>, right).</p>
<p>Altogether, the framework we developed allowed us to explore the functional implications of top-down feedback in multi-layer RNNs. As well, this framework allowed us to examine the functional impact of a RNN architecture inspired by the human brain on audiovisual processing, and compare it to other non-brainlike architectures.</p>
</sec>
<sec id="s3a">
<label>2.2</label>
<title>Effect of top-down feedback on visual tasks with auditory cues</title>
<p>In order to examine the functional impact of different top-down feedback architectures, we first trained multiplicative feedback models (i.e. not composite feedback) on an image categorization task with additional auditory stimuli. The task for the ANN was to correctly identify the category based on the image, but the images were sometimes ambiguous (which were created using a VAE, see Methods). In cases where the visual image was ambiguous, the network was provided with a disambiguating auditory cue (we refer to this as Visual-dominant Stimulus case 1, or VS1; <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, top). In addition, we presented the model with situations where the visual input was unambiguous and the auditory input was misleading (VS2; <xref rid="fig2" ref-type="fig">Fig. 2a</xref>, bottom), in order to test whether the models could learn to ignore misleading audio inputs. Crucially, the models were never told which stimulus scenario they were encountering. As such, they had to learn to rely on the auditory inputs when the visual inputs were ambiguous (VS1), and ignore the auditory inputs when they visual inputs were unambiguous (VS2). Models were trained in mini-batches on both VS1 and VS2 stimuli, then tested on held out data (see Methods for details).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Multimodal visual tasks.</title>
<p><bold>a</bold>, Training conditions. Models must identify the visual stimulus given an ambiguous image and a matching audio clue (VS1) or unambiguous image and distracting audio (VS2). <bold>b-c</bold>, Accuracy across epochs for tasks VS1 and VS2 on holdout datasets, <bold>d</bold> Trained models were given an ambiguous visual stimulus and a nonmatching audio stimulus (VS3) to assess which modality they align most closely with. <bold>e</bold>, Alignment of trained models across epochs based on task VS3. <bold>f</bold>, Models were additionally trained and tested on image stimuli only to assess their baseline performance (VS4). <bold>g</bold>, Accuracy of models across epochs on task VS4</p></caption>
<graphic xlink:href="615270v4_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In stimulus condition VS1, all of the models were able to learn to use the auditory clues to dis-ambiguate the images (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). However, the brainlike model learned to rely on auditory inputs more slowly, whereas the reversed model was exceptionally good at integrating auditory information. In comparison, in VS2, we found that the brainlike model learned to ignore distracting audio inputs quickly and consistently compared to the random models, and a bit more rapidly than the auditory information (<xref rid="fig2" ref-type="fig">Fig 2d</xref>). These data show that the brainlike model is biased towards visual stimuli compared to the other models.</p>
<p>We next wanted to probe this visual bias further by examining how the models relied on either visual or auditory inputs when neither was unambiguously relevant to the task. Thus, in another set of test stimuli, we examined what happened when the models were presented with non-matching auditory input and ambiguous visual input (VS3; <xref rid="fig2" ref-type="fig">Fig. 2b</xref>, top). Notably, VS3 was only used as a test; the models had been trained on stimuli from the VS1 and VS2 conditions. In this situation, we were interested in whether the models were more likely to align their answers to the visual or auditory stimuli, given that neither provided a clear and correct answer. We found that the brainlike model aligned with the visual input from the start, while all other models had to learn to do so further along in training (<xref rid="fig2" ref-type="fig">Fig. 2e</xref>). Importantly, if we trained the models purely on unambiguous images (VS4; <xref rid="fig2" ref-type="fig">Fig. 2b</xref>, bottom), all of the models were equally adept at learning (<xref rid="fig2" ref-type="fig">Fig. 2f</xref>). As such, the visual bias shown by the brainlike model did not reflect a more general increased capability with visual inputs, rather, it reflected an inductive bias in audio-visual integration created by the top-down feedback architecture. Altogether, our results demonstrated both the architecture of top-down feedback can impact audio-visual processing, and that the specific architecture inspired by the human brain has a visual bias in these tasks.</p>
</sec>
<sec id="s3b">
<label>2.3</label>
<title>Effect of top-down feedback on auditory tasks with visual cues</title>
<p>To see if the visual bias persisted for a non-visual task, we then trained the models from scratch on an audio recognition task, where the ANNs now had to identify ambiguous sounds using visual stimuli as clues (AS1; <xref rid="fig3" ref-type="fig">Fig. 3a</xref>, top) and ignore distracting images when they’re not needed (AS2; <xref rid="fig3" ref-type="fig">Fig. 3a</xref>, bottom). As before, the models were trained on data from the AS1 and AS2 conditions, then tested on held out data points. Notably, only the brainlike model and one of the random models learned to use the visual stimuli at all, across 10 random seeds (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>). In contrast, all of the models learned to ignore the distractor image (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>). This indicates that the brainlike model is more inclined to use visual stimuli for the task than the other models.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Multimodal auditory tasks.</title>
<p><bold>a</bold>, Training conditions. Models must identify the auditory stimulus given an ambiguous audio and a matching visual clue (AS1) or unambiguous audio and distracting image (AS2). <bold>b-c</bold>, Accuracy across epochs for tasks AS1 and AS2 on holdout datasets, <bold>d</bold>, Trained models were given an ambiguous audio stimulus and a nonmatching visual stimulus (AS3) to assess which modality they align most heavily with. <bold>e</bold>, Alignment of trained models across epochs based on task AS3. <bold>f</bold>, Models were additionally trained and tested on audio stimuli only to assess their baseline performance (VS4), <bold>g</bold>, Accuracy of models across epochs on task AS4</p></caption>
<graphic xlink:href="615270v4_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Next, we examined how the models responded when neither input was unambiguously informative. In a test scenario with non-matching visual stimuli and ambiguous audio (AS3; <xref rid="fig3" ref-type="fig">Fig 3d</xref>), the brainlike model aligned more heavily with the visual input than most other models (<xref rid="fig3" ref-type="fig">Fig 3e</xref>). Thus, despite the task being primarily auditory, the brainlike model retained its clear visual bias. Interestingly, unlike in the case of visual tasks, there was a difference between the models in terms of their ability to learn purely from unambiguous auditory stimuli (AS4; <xref rid="fig3" ref-type="fig">Fig. 3f</xref>). While all models could achieve the same final error rate, the brainlike model took longer to learn from purely auditory inputs than the other models. Our results suggest that the architecture of top-down feedback in the human brain may provide an inductive bias that favours visual inputs over auditory inputs.</p>
</sec>
<sec id="s3c">
<label>2.4</label>
<title>Effects of composite top-down feedback</title>
<p>As noted above, neurophysiological evidence suggests that in addition to modulating activity in a multiplicative manner, top-down feedback in the neocortex can be weakly driving as well, shifting not just the gain but also the threshold of activation for a neuron (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>) (<xref ref-type="bibr" rid="c70">Shai et al., 2015</xref>). As such, we performed the same experiments as above, but now with composite feedback, i.e. feedback that has both a multiplicative and an additive effect. Notably, composite feedback is more akin to a feedforward connection (due to the weakly driving additive component). Therefore, in order to isolate for the impact of composite top-down feedback we added a control of an identically connected network consisting only of feedforward connections (i.e. no top-down feedback anywhere). We then compared the brainlike composite feedback models to the original controls and the new feedforward only control.</p>
<p>Interestingly, we found that composite feedback improved the performance of all models on tasks that they previously had difficulty with, while maintaining performance on the tasks they were already strong on (<xref rid="fig4" ref-type="fig">Fig. 4a-d</xref>). Specifically, in the visual task with ambiguous visual stimuli and informative audio cues (VS1), we observed faster learning and massively improved final accuracy in the brainlike and random models, which lagged behind the reverse model when using multiplicative feedback (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>). Conversely, the reverse model with multiplicative feedback had difficulty using visual clues when identifying ambiguous audio (AS1). Composite feedback improved the performance of the reverse model on this task, but the already high-performing brainlike model remained unaffected by the change (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Composite versus multiplicative feedback in multimodal tasks.</title>
<p><bold>a, c, e</bold>, Test performances of models with composite feedback and feedforward-only models trained on visual tasks (VS1 and VS2). The final epoch accuracy of models with composite feedback (C) is compared to that of models with multiplicative feedback (M) shown in previous figures <bold>b, d, f</bold>, Test performances of models trained on auditory tasks (AS1 and AS2).</p></caption>
<graphic xlink:href="615270v4_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The impact of composite feedback was more mixed in the situation where the visual input was unambiguous and the auditory input was misleading (VS2). The brainlike model and most random models showed better final accuracy (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>), but a few random models only performed well in VS1 and not VS2 (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>, rightmost outliers). The only task where composite feedback had no noticeable effect was when the auditory input was unambiguous and the visual clue was misleading (AS2), likely because all models already performed at a high level with multiplicative feedback (<xref rid="fig4" ref-type="fig">Fig. 4d</xref>). Interestingly, the all feedforward network consistently performed at an accuracy roughly equivalent to that of the brainlike composite feedback model (<xref rid="fig4" ref-type="fig">Fig. 4 a-d</xref>, blue lines). These data imply that composite feedback may be a more general purpose computational mechanism that can be used for efficient multi-modal learning with the correct architecture, but based on accuracy alone, it did not behave noticeably differently from an all feedforward network.</p>
<p>We next wondered whether the difference between composite top-down feedback and the all feed-forward network could be observed in terms of the visual bias of the networks. Therefore, we examined the visual biases of the models in cases where no unambiguous correct input was given (VS3 and AS3). We found that in both the visual tasks (<xref rid="fig4" ref-type="fig">Fig. 4e</xref>) and the auditory tasks (<xref rid="fig4" ref-type="fig">Fig. 4f</xref>) the visual bias of the brainlike model was still present. In contrast, the feedforward-only model showed biases between that of brainlike and reverse models with a larger variability between seeds than the brainlike model, suggesting the feedforward-only model privileged the visual and auditory input based on task demands and initialization. This shows that the inductive bias towards vision in the brainlike model depended on the presence of the multiplicative component of the the feedback, and therefore, the visual bias is a function of the network architecture. This visual bias shows under all conditions for the network while the bias of the feedforward model varies based on task and initialization. Altogether, these results suggest that composite feedback as seen in the neocortex is an effective mechanism for multi-modal integration, and that the architecture of feedback in the human brain provides an inductive bias towards visual stimuli.</p>
</sec>
<sec id="s3d">
<label>2.5</label>
<title>Effects of top-down feedback on task-switching</title>
<p>Our results so far showed that top-down feedback imparts a persistent inductive bias depending on the network architecture. However, top-down feedback is thought to be particularly important for context-dependent task switching (<xref ref-type="bibr" rid="c40">Li et al., 2004</xref>; <xref ref-type="bibr" rid="c42">Liu et al., 2021</xref>). Thus, we wanted to know if the brainlike model’s visual bias lent it any advantage over other networks, particularly in tasks that required flexible responses to context.</p>
<p>As such, we trained the models on all four training tasks (VS1, VS2, AS1, AS2) at the same time. To do so, we augmented all the models with a new output region that takes input from IT and A4. This new region in the model can be thought of as broadly representing higher order multimodal regions in the brain (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>). This multimodal region additionally received a binary attention flag with each pair of inputs indicating the target stimulus, i.e. whether to attend to the visual or auditory streams (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). As in previous tasks, the models had to determine on their own whether the extraneous stimulus was useful for identifying the target stimulus (VS1, AS1) or a distraction to be ignored (VS2, AS2). We tested with both composite feedback (<xref rid="fig5" ref-type="fig">Fig. 5c-e</xref>) and multiplicative-only feedback (Fig. S1).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Audiovisual switching task.</title>
<p><bold>a</bold>, All models were given a new audiovisual output area (AV) connecting to IT and A4, <bold>b</bold>, The output area receives an attention flag telling which stream of information to attend to (visual or auditory). The models with feedback use with composite feedback. <bold>c-d</bold>, Test performance of models with composite feedback and feedforward-only models on all tasks. The models were trained simultaneously on all tasks, <bold>e</bold>, Alignment of models given ambiguous visual and ambiguous audio input with differing labels.</p></caption>
<graphic xlink:href="615270v4_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>All models quickly learned to identify ambiguous visual stimuli using the auditory input (<xref rid="fig5" ref-type="fig">Fig. 5c</xref>, left), but the brainlike model learned to ignore distracting audio quicker than all other models including the feedforward-only model (<xref rid="fig5" ref-type="fig">Fig. 5c</xref>, right). Similarly, the brainlike model quickly learned to use visual clues when identifying ambiguous audio (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>, left). However, it lagged behind the other models when it had to ignore the visual stimuli (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>, right). When presented with two ambiguous stimuli of different labels, the brainlike model again aligned more frequently with the visual stimuli (<xref rid="fig5" ref-type="fig">Fig. 5e</xref>). In contrast, the feedforward network started out more heavily aligned to the auditory stimuli and slowly aligned with the visual stimuli through training, suggesting it conforms to task demands and does not possess the same inductive bias for visual tasks (<xref rid="fig5" ref-type="fig">Fig. 5e</xref>). Interestingly, the reverse model and two random models struggled to learn to ignore auditory stimuli (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). This is likely because the auditory dataset is smaller, less variable and thus easier to learn than the visual dataset, causing certain models to shortcut their training and rely excessively on the audio inputs (see also <xref rid="fig3" ref-type="fig">Fig. 3b</xref> and <xref rid="fig3" ref-type="fig">Fig. 3e</xref>). This data shows that a visual bias helps models ignore this shortcut, whereas an auditory bias makes them more susceptible to it. The detrimental auditory bias is suppressed during the visual only tasks (<xref rid="fig2" ref-type="fig">Fig. 2e</xref>), but when forced to switch between both auditory and visual tasks, the auditory bias was more apparent.</p>
<p>Our results show that the architectural biases imparted by feedback are even easier to observe in tasks that require flexible switching, and that the brainlike model and its visual bias could be advantageous over feedforward-only models in certain scenarios, if for example there is a data imbalance. In contrast, an auditory bias hampered the models for the scenarios studied here.</p>
</sec>
<sec id="s3e">
<label>2.6</label>
<title>Functional specializations of model regions</title>
<p>Different regions of the model are active at different time steps in our simulations, depending on where the region sits in the hierarchy and when feedforward input is received by that region (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>). We therefore wanted to understand the impact of top-down feedback on the temporal dynamics of computation in the model across different regions. As such, we next studied the representations of stimuli across time in different regions of the model, in order to gain insight into how the various regions contribute to the tasks studied here. First, to gain a qualitative understanding of representations over time in the models, we projected the hidden states of all regions of the models with composite feedback at different time points onto two dimensions using t-distributed Stochastic Neighbor Embedding (<xref rid="fig6" ref-type="fig">Fig. 6b</xref>, left). In general, clustered, easily separable representations greatly aid the performance of a neural network in classification tasks like these. We found that different regions showed different clustering of stimuli over time, for example, in the task where the network must classify auditory stimuli and ignore distracting audio stimuli (AS2), the one seed of the brainlike model showed a clear clustering of different stimuli in the IT region, but not in A4, and only moderately in V1 (<xref rid="fig6" ref-type="fig">Fig. 6b</xref>, left). To quantify the clustering pattern observed across models, we calculated the Neighborhood Hit (NH) score of all datapoints in the latent space at each time step in all regions of the model (<xref ref-type="bibr" rid="c58">Paulovich et al., 2008</xref>; <xref ref-type="bibr" rid="c63">Rauber et al., 2017</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Model activity during multimodal tasks.</title>
<p><bold>a</bold>, Information flows through the model from area to area across time. At the first time step, only the primary visual and auditory areas will process information. The areas they feed forward to are activated at the next time step, incorporating topdown information if there is any. <bold>b</bold>, Comparison of t-SNE reduced latent space and clustering metric in three areas of the brainlike model at different time stages on task VS2 (ignore audio stimulus). <bold>c</bold>, Neighborhood Hit scores in all areas of the trained models across time. Trained models were taken from experiments in <xref ref-type="fig" rid="fig4">Fig 4</xref></p></caption>
<graphic xlink:href="615270v4_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For the visual tasks, in the brainlike and reverse models, we observed distinct, clustered representations in the auditory regions when the auditory stimuli were useful for image identification (<xref rid="fig6" ref-type="fig">Fig 6c</xref>, VS1). Conversely, none of the auditory regions in the brainlike and reverse models formed clustered representations in the scenario when the auditory stimuli were irrelevant to the task, outside brief clustering in the A4 in some brainlike models (<xref rid="fig6" ref-type="fig">Fig. 6c</xref>, VS2). Interestingly, this was not the case with the feedforward-only model, which exhibited clustering in all auditory regions outside A1 even when the auditory inputs were irrelevant. This demonstrates that in the models with top-down feedback there is a specialization that occurs, such that the auditory regions only clustered the data when necessary, whereas the absence of top-down feedback mechanisms led to more multimodal responses in general.</p>
<p>A similar delineation of function was again evident in the auditory tasks, where the brainlike model had highly clustered representations in V1 and V2 when the visual clue was useful to the task (<xref rid="fig6" ref-type="fig">Fig. 6c</xref>, AS1), but showed uniformly lower clustering in the visual regions after the first time step if the visual clue had to be ignored (<xref rid="fig6" ref-type="fig">Fig. 6c</xref>, AS2). Notably, the reverse model did not cluster the visual clue effectively resulting in low clustering in the visual regions during AS1 and a drop in performance (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>). In contrast, clustering exhibited by the all-feedforward model was more variable compared to all models with top-down feedback.</p>
<p>It’s important to emphasize that no specific inductive biases were given to any of the regions of the model beyond the inputs they received and whether those inputs were feedforward or feedback. V2 and the auditory belt, for instance, are identical aside from their connectivity in these models. Moreover, all regions were trained end-to-end with the same, unitary loss function. As such, there was no incentive for a non-primary region to develop an exclusive visual or auditory specialization in the absence of the connectivity constraints. An especially illustrative example of this is one of the random models where the V2 region actually developed into an auditory region (<xref rid="fig6" ref-type="fig">Fig. 6c</xref>, grey line). This model performed consistently well in all tasks. Similarly, the consistently well-performing all-feedforward model developed drastically different functional specialization between seeds in the auditory tasks (<xref rid="fig6" ref-type="fig">Fig. 6c</xref>, blue line). Thus, there are multiple different ways to solve these tasks, and different architectures of feedforward and feedback inputs push the models towards these different solutions.</p>
<p>Another illustrative observation was that the brainlike and reverse models developed more predictable specializations in their visual versus auditory regions compared to other models, i.e. visual regions were largely visual and auditory regions were largely auditory (based on their clustering profiles). This is likely because the brainlike and reverse models had visual-to-auditory connections that were either all feedforward or all feedback, respectively. This further confirms that the distinction between feedforward and feedback inputs, as implemented in our models, helps to determine the set of solutions available to the networks and the regional specializations that they develop.</p>
</sec>
</sec>
<sec id="s4">
<label>3</label>
<title>Discussion</title>
<p>Neurophysiological and anatomical data suggest that top-down feedback to pyramidal neurons in the neocortex has a distinct impact on activity from that of bottom-up feedforward inputs. Specifically, top-down feedback has a more modulatory role, changing the gain of the neurons as well as their spike threshold, rather than directly driving activity (<xref ref-type="bibr" rid="c37">Larkum, 2004</xref>; <xref ref-type="bibr" rid="c70">Shai et al., 2015</xref>). In this study, we built what is, to our knowledge, the first hierarchical multi-modal deep neural network architecture based on the brain’s anatomy that takes this distinction into account. We then explored how different architectures of feedforward and feedback inputs impacted the behavior of networks on audiovisual integration tasks. We compared a brainlike model, based on human cytoarchitectural data to other models, including a model that reversed the relationship observed in the brain, random models, and a model with only feedforward connections. We found that even in densely connected, identically sized models, different configurations of feedforward and feedback connectivity gave the models different strengths, weaknesses and inductive biases. In particular, deep models with a human brainlike hierarchy exhibited a distinct visual bias, but nevertheless performed well on all audiovisual tasks, qualitatively mimicking a long-known human bias for visual stimuli (<xref ref-type="bibr" rid="c61">Posner et al., 1976</xref>; <xref ref-type="bibr" rid="c74">Stokes &amp; Biggs, 2014</xref>). While weakly driving composite feedback improved the performance of some of the lagging models, the visual bias of the brainlike model persisted for both composite and multiplicative feedback. Moreover, all regions of the brainlike model developed their expected functional specializations (i.e. visual regions clustered visual stimuli, and auditory regions clustered auditory stimuli), despite us having given no region-specific bias other than connectivity. This suggests that the profile of feedforward and feedback connectivity of a region helps determine its functional specializations. Altogether, our results demonstrate that the distinction between feedforward and feedback inputs has clear computational implications, and that ANN models of the brain should therefore consider top-down feedback as an important biological feature.</p>
<sec id="s4a">
<label>3.1</label>
<title>Computational impact of top-down feedback</title>
<p>Top-down feedback is speculated to play a variety of roles in the cortex. It’s known to suppress neural responses to predictable inputs (<xref ref-type="bibr" rid="c52">Nassi et al., 2013</xref>; <xref ref-type="bibr" rid="c62">Rao &amp; Ballard, 1999</xref>)–an observation that forms the core of the predictive coding framework <xref ref-type="bibr" rid="c17">Friston and Kiebel, 2009</xref>; <xref ref-type="bibr" rid="c51">Mumford, 1992</xref>; <xref ref-type="bibr" rid="c62">Rao and Ballard, 1999</xref>. In addition to its predictive function, it’s crucial for modulating attention (<xref ref-type="bibr" rid="c11">Debes &amp; Dragoi, 2023</xref>), shaping perception (<xref ref-type="bibr" rid="c43">Manita et al., 2015</xref>), and conveying task-specific context (<xref ref-type="bibr" rid="c40">Li et al., 2004</xref>; <xref ref-type="bibr" rid="c42">Liu et al., 2021</xref>). Long-range feedback from the motor (<xref ref-type="bibr" rid="c31">Jordan &amp; Keller, 2020</xref>; <xref ref-type="bibr" rid="c39">Leinweber et al., 2017</xref>) and auditory cortex (<xref ref-type="bibr" rid="c19">Garner &amp; Keller, 2021</xref>) carry important contextual information to the early visual cortex.</p>
<p>The many proposed roles of top-down feedback have been explored by various computational models (<xref ref-type="bibr" rid="c9">Choksi et al., 2020</xref>; <xref ref-type="bibr" rid="c12">Deco &amp; Rolls, 2004</xref>; <xref ref-type="bibr" rid="c27">Huang et al., 2020</xref>; <xref ref-type="bibr" rid="c30">Jiang &amp; Rao, 2024</xref>; <xref ref-type="bibr" rid="c48">Mittal et al., 2020</xref>; <xref ref-type="bibr" rid="c54">Pang et al., 2021</xref>; <xref ref-type="bibr" rid="c80">Wen et al., 2018</xref>), but these previous studies have not generally incorporated the multiplicative, modulatory role of top-down feedback. There are a few important, recent exceptions. First, <xref ref-type="bibr" rid="c53">Naumann et al., 2022</xref> examined the impact of a top-down feedback that modulated the feedforward synapses, showing that modulatory feedback can help to recover source signals from a noisy environment. Second, <xref ref-type="bibr" rid="c81">Wybo et al., 2022</xref> used a composite feedback mechanism and biophysical modeling to show that modulatory feedback can help neurons to flexibly solve multiple linearly inseparable problems through Hebbian plasticity. Next, <xref ref-type="bibr" rid="c29">Islah et al., 2023</xref> showed that multiplicative feedback can provide crucial contextual information to a neural network, allowing it to disambiguate challenging stimuli. Most recently, <xref ref-type="bibr" rid="c75">Tsai et al., 2024</xref> showed that modulatory feedback enhances taskrelevant sensory signals in a computational model of S1 and lOFC. Our study adds to this previous work by incorporating modulatory top-down feedback into deep, convolutional, recurrent networks that can be matched to real brain anatomy. Importantly, using this framework we could demonstrate that the specific architecture of top-down feedback in a neural network has important computational implications, endowing networks with different inductive biases.</p>
<p>One other potential computational role for top-down feedback is to provide a credit assignment signal (<xref ref-type="bibr" rid="c25">Greedy et al., 2022</xref>; <xref ref-type="bibr" rid="c26">Guerguiev et al., 2017</xref>; <xref ref-type="bibr" rid="c38">Lee et al., 2015</xref>; <xref ref-type="bibr" rid="c59">Payeur et al., 2021</xref>; <xref ref-type="bibr" rid="c65">Roelfsema &amp; Ooyen, 2005</xref>; <xref ref-type="bibr" rid="c68">Sacramento et al., 2018</xref>). There is some experimental evidence supporting this role for top-down feedback in the brain (<xref ref-type="bibr" rid="c6">Bittner et al., 2017</xref>; <xref ref-type="bibr" rid="c14">Doron et al., 2020</xref>; <xref ref-type="bibr" rid="c16">Francioni et al., 2023</xref>), but it is not yet a widely accepted function for top-down inputs. Nonetheless, if additional experimental evidence supports this role for top-down feedback, it will be critical to also incorporate the credit assignment function into future models.</p>
</sec>
<sec id="s4b">
<label>3.2</label>
<title>Testable predictions of audiovisual integration</title>
<p>The idea that auditory cortical regions are higher on the computational hierarchy than visual regions in the human brain is a longstanding, if somewhat controversial hypothesis (<xref ref-type="bibr" rid="c35">King &amp; Nelken, 2009</xref>). Neurons in V1 respond strongly to very low-level features, such as oriented gratings (<xref ref-type="bibr" rid="c28">Hubel &amp; Wiesel, 1962</xref>). Meanwhile, neurons in A1 respond more strongly to complex, modulated, and relatively long stimuli compared to pure tones and frequency sweeps (<xref ref-type="bibr" rid="c79">Wang et al., 2005</xref>). One reason for this may be that the subcortical auditory system in mammals is extensive, and the synaptic distance between the cochlea and A1 is consequently greater than that between the retina and V1. In-line with this, previous modeling work with deep networks showed that the primary auditory area matches intermediate regions of trained deep networks most closely (<xref ref-type="bibr" rid="c33">Kell et al., 2018</xref>) while primary visual regions match early layers (<xref ref-type="bibr" rid="c7">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="c34">Khaligh-Razavi &amp; Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="c82">Yamins &amp; DiCarlo, 2016</xref>). Our study suggests that if the hypothesis is true, then there are functional consequences that flow from the architecture of the human brain, most notably, a bias towards relying on visual inputs more readily to resolve ambiguities. This is one clear, experimental prediction that our models make.</p>
<p>However, while humans are generally thought to be a visually-dominant species, some individuals with musical training report a behaviorally and functionally distinct auditory bias (<xref ref-type="bibr" rid="c22">Giard &amp; Peronnet, 1999</xref>). The proportion of feedforward and feedback connectivity between the visual and auditory regions may explain these individual differences. This represents another testable prediction flowing from our study, which could be studied in humans by examining the optical flow (<xref ref-type="bibr" rid="c60">Pines et al., 2023</xref>) between auditory and visual regions during an audiovisual task. If confirmed, the hypothesis provides a concrete, developmentally flexible mechanism for the emergence of human visual bias.</p>
<p>Moreover, the link between connectivity and functional specialization can be studied further within the same framework to produce testable hypotheses about the effects of cortical lesions, implant responses and stroke recovery. Previous large-scale computational models of lesions lend great insight into their effects on brain function, but cannot readily probe their behavioral effects (<xref ref-type="bibr" rid="c1">Alstott et al., 2009</xref>; <xref ref-type="bibr" rid="c45">Martínez-Molina et al., 2024</xref>). In contrast, the ANNs used in our study can be trained on very complex tasks via backpropagation, and thus could provide direct insight into the potential impact of lesions on task performance and behavior. In general, with intelligent selection of architectures guided by anatomy and tasks selected based on real experimental conditions, ANNs with modulatory topdown feedback and brainlike connectivity could be used to generate many predictions about functional connectivity and lesion recovery.</p>
</sec>
<sec id="s4c">
<label>3.3</label>
<title>Limitations and future directions</title>
<p>We made various simplifying assumptions about connectivity in the human brain to build the brainbased model. Feedforward-feedback directionality of a connection is determined in animals using tracer injections, but in human brains, it must be determined using proxy measures. To determine the directionalities in our model, we used externopyramidisation–the relative thickness and differentiation of supragranular layers, known to be highest at the bottom of the sensory hierarchy. While our measure replicates expected hierarchical ordering of brain regions based on previous literature, visual regions exclusively sending feedforward information to auditory regions in the human brain is an extrapolation of available cytoarchitectural data.</p>
<p>While the visual bias of the brainlike model was evident across all scenarios, the reverse model did not display a similar task-agnostic auditory bias (<xref rid="fig2" ref-type="fig">Fig. 2e</xref>). This is likely because the auditory dataset in our study is smaller, less variable and thus easier to learn than the visual dataset, causing certain models to shortcut their training and rely excessively on it (<xref rid="fig3" ref-type="fig">Fig. 3b</xref> and <xref rid="fig3" ref-type="fig">Fig. 3e</xref>). A visual bias helps models ignore this shortcut, whereas an auditory bias makes them more susceptible to it and hampers a model’s ability to ignore audio input on visual tasks (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>, <xref rid="fig5" ref-type="fig">Fig. 5b</xref>). This points to another limitation in our study, namely the use of relatively simple stimuli and small datasets. Ideally, future work would use rich, natural audiovisual inputs and large datasets to train the networks. This may lead to some different behaviors in the models. Moreover, our results relate to other findings that task demands often supersede architectural choices in computational modeling (<xref ref-type="bibr" rid="c41">Lindsay et al., 2022</xref>). Nonetheless, despite these limitations, our model shows clearly that within certain tasks the biases imparted by architectural choices have important implications that interact with the data.</p>
<p>Another important limitation to note is that many aspects of our model are not biologically plausible, most notably the use of end-to-end backpropagation and supervised learning. Importantly, our models are not only applicable in these cases - one could easily train the ANNs we designed with different learning algorithms. As well, the models we developed are agnostic to training method and can easily be trained with self-supervised and reinforcement learning tasks. But, we believe that the ability to train our models with backpropagation is important, because that will allow them to learn a wide range of complicated, more natural tasks in future research. We have released the codebase to construct these models to facilitate such research.</p>
<p>Finally, another key limitation in this study is that we did not compare our models directly to human neural data. Our results show clearly that the models’ internal representations are altered by top-down feedback (<xref rid="fig6" ref-type="fig">Fig. 6</xref>), so we would expect it to also have an impact on the ability of the models to match the representations in real brains. But, we leave this as future work, which is made easier by the release of the codebase.</p>
<p>In summary, our study shows that modulatory top-down feedback and the architectural diversity enabled by it can have important functional implications for computational models of the brain. We believe that future work examining brain function with deep neural networks should therefore consider incorporating top-down modulatory feedback into model architectures when appropriate. More broadly, our work supports the conclusion that both the cellular neurophysiology and structure of feedback inputs have critical functional implications that need to be considered by computational models of brain function.</p>
</sec>
</sec>
<sec id="s5">
<label>4</label>
<title>Methods</title>
<sec id="s5a">
<label>4.1</label>
<title>Model</title>
<p>Each area of the model is a modified Convolutional Gated Recurrent Unit (ConvGRU) cell. A Gated Recurrent Unit is a standard neural network with locally recurrent connections. The processes are identical to that of a typical GRU with the exception of the linear operators, which are replaced by convolutions (represented by the star) as per <xref ref-type="bibr" rid="c3">Ballas et al., 2016</xref>, and the topdown signal (<italic>m</italic>), which modulates the hidden state after it has been reset and combined with the bottom-up input.</p>
<p>The tasks in this paper are non-sequential; the image and the audio stimuli are presented at the same time to V1 and A1 respectively. At each time step, the areas receiving a bottom-up feedforward input (<italic>h</italic><sub><italic>l−</italic>1</sub>) get activated. The feedforward input is combined with the hidden state memory of the area (<italic>h</italic><sub><italic>l</italic></sub>), and modulated by the excitatory topdown signal (<italic>m</italic>), which is derived from the feedback input (<italic>h</italic><sub><italic>l−</italic>1</sub>) if it exists. In case there is no feedback input (i.e the higher order areas have not been active yet), <italic>m</italic> is a matrix of ones. Resets and updates (<xref ref-type="disp-formula" rid="eqn1">Equations 1</xref> and <xref ref-type="disp-formula" rid="eqn2">2</xref>) are features of the GRU that implement local recurrence. The full equations are presented below.
<disp-formula id="eqn1">
<graphic xlink:href="615270v4_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn2">
<graphic xlink:href="615270v4_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="615270v4_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn4">
<graphic xlink:href="615270v4_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="615270v4_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The input size of the regions decreased as they moved up the sensory-association hierarchy, i.e V1 and A1 received 32x32 input, while V2 and the auditory belt received 16x16 input. Model regions were kept small to show greater contrast in performance on a simple object categorization task. All regions of the model had a hidden state channel size of 10, matching the number of classes.</p>
<p>All feedforward inputs to an area were combined and projected into the correct shape by a convolutional projection layer. Feedback inputs were similarly reshaped by a convolutional projection layer to a shape that matches the hidden state of the region.</p>
<sec id="s5a1">
<label>4.1.1</label>
<title>Composite feedback</title>
<p>When using composite feedback, the top-down signal was split into the modulatory signal <italic>m</italic><sub><italic>mod</italic></sub> matching the shape of the feedforward input, and the driving signal <italic>m</italic><sub><italic>d</italic></sub>, which has ten times fewer channels than the feedforward input. In other words, in addition to multiplicative modulation, the feedback signal provided a driving signal at one tenth the strength of the feedforward driving signal. <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref> was then adjusted to incorporate the two different types of top-down signals:
<disp-formula id="eqn6">
<graphic xlink:href="615270v4_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s5a2">
<label>4.1.2</label>
<title>Classification</title>
<p>For supervised classification tasks, the user must designate an output region (IT in visual tasks and A4 in auditory tasks). The hidden states of the output region are fed to two-layer multilayer perceptron (MLP) trained at the same time as the model, which then outputs a classification.</p>
</sec>
</sec>
<sec id="s5b">
<label>4.2</label>
<title>Bases of the human brainlike model</title>
<p>We based the brainlike model on human structural connectivity and histological data. For all human data, regions of interest were determined using the multimodal HCP parcellation (<xref ref-type="bibr" rid="c23">Glasser et al., 2016</xref>). Four classic regions in the ventral visual system was chosen to represent the visual cortex: V1, V2, V4, and PIT. Three auditory areas were chosen to represent the auditory cortex: A1, parabelt (part of the belt bordering A4), and A4, a portion of Brodmann area 22 that activated by language tasks and is thought to correspond to area Te3 (<xref ref-type="bibr" rid="c50">Morosan et al., 2005</xref>).</p>
<p>To determine the overall connectivity between regions, we used the diffusion tractography data of 50 adult subjects from the Microstructure-Informed Connectomics (MICA-MICs) dataset (<xref ref-type="bibr" rid="c66">Royer et al., 2022</xref>). We averaged the connectivity between all pair of regions across subjects and generated a group-average binary connectivity matrix using distance-based thresholding (<xref ref-type="bibr" rid="c5">Betzel et al., 2019</xref>).</p>
<p>To determine the directionality of connectivity between regions, we used the BigBrain quantitative 3D laminar atlas of the human cerebral cortex (<xref ref-type="bibr" rid="c2">Amunts et al., 2013</xref>; <xref ref-type="bibr" rid="c77">Wagstyl et al., 2020</xref>). We used externopyramidisation (<xref ref-type="bibr" rid="c69">Sanides, 1962</xref>), the relative size of supragranular neurons compared to infragranular neurons, as the proxy for the hierachical position of area. In mammals including humans, it is high in regions that send more supragranular feedforward projections and lower in areas that send more infragranular feedback projections (<xref ref-type="bibr" rid="c24">Goulas et al., 2018</xref>). This value can be estimated from histological data using the relative laminar thickness and staining intensity of supragranular layers, a process described by <xref ref-type="bibr" rid="c57">Paquola et al., 2020</xref>. Similar to the method described in that paper, we rescaled supragranular staining intensity (<italic>in</italic>) and thickness values to a range of 0 to 1, and used the normalized peak intensity from 20 sample locations (<italic>n</italic>) and relative thickness of supragranular layers to calculate externopyramidisation.
<disp-formula id="eqn7">
<graphic xlink:href="615270v4_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Regions with higher externopyrmidisation values were configured to send feedforward connections to and receive feedback from regions with lower externopyramidisation values.</p>
</sec>
<sec id="s5c">
<label>4.3</label>
<title>Stimuli generation and preprocessing</title>
<p>We used MNIST handwritten digit database as the unambiguous visual stimuli and the Free Spoken Digit Dataset (FSDD) as the unambiguous auditory stimuli. The visual stimuli were minimally preprocessed, while the audio was Fourier-transformed and converted to log scale to generate uniformly sized mel-spectrograms.</p>
<p>The ambiguous visual stimuli were created by <xref ref-type="bibr" rid="c29">Islah et al., 2023</xref> and used with permission. Details of it can be found in the cited paper. The ambiguous visual stimuli all have two equally possible labels, capping the performance of classifiers at around 50 percent if they are not given additional clues.</p>
<sec id="s5c1">
<label>4.3.1</label>
<title>Ambiguous auditory stimuli</title>
<p>The ambiguous auditory stimuli were generated in a similar manner to the ambiguous visual stimuli. We trained a conditional variational autoencoder (CVAE) (<xref ref-type="bibr" rid="c73">Sohn et al., 2015</xref>) to project melspectrograms of FSDD data into a 32 dimensional latent space. We then sampled the Euclidean mean of two random data in the latent space with differing labels, and used the decoder of the trained CVAE to deconvolve it into a mel-spectrogram. The resulting ambiguous digit often retained the features of one digit, but lost all features of the other it was “mixed” with. As such, while the ambiguous visual stimuli have two equally possible interpretations by design, the ambiguous auditory stimuli have a single possible interpretation that is nevertheless difficult to identify without additional clues. To ensure a balance of recognizability and ambiguity in the auditory data, we trained a separate softmax classifier on mel spectrograms of unambiguous FSDD data. The softmax classifier output a prediction between 0 and 1.0 (full certainty) for each label. We asked the classifier to predict the labels of each newly generated ambiguous mel spectrogram, and kept only the data and labels that caused the classifier to output a prediction between 0.45 and 0.55. We reiterated the process until we had 4000 unique ambiguous auditory stimuli, the size of the holdout portion of original FSDD dataset. Due to the higher power of the ConvGRU-based models used in our experiments compared to the softmax classifier used for quality control, the models used in the experiments are able to decipher the ambiguous audio up to 80 percent accuracy without clues.</p>
</sec>
</sec>
<sec id="s5d">
<label>4.4</label>
<title>Audiovisual task training</title>
<p>We combined the four types of stimuli into fixed audiovisual datasets, where each datapoint consisted of a pair of audio and visual stimuli and their corresponding labels. We held out 15 percent of each dataset for testing before creating the audiovisual datasets, meaning no image or audio during testing was encountered during training. We calculated the gradient in minibatches of 32, and used the Adam optimizer at learning rate 0.0001 to train the model. All models were trained for 50 epochs. For each experimental condition, we ran 10 different seeds.</p>
<sec id="s5d1">
<label>4.4.1</label>
<title>Training tasks</title>
<p>For tasks VS1 and VS2, we trained the models on a shuffled mix of datasets UAM (unambiguous audio, ambiguous image, matching label) and UUN (unambiguous audio, unambiguous image, mismatched label). We set the label of the image as the target and calculated the cross-entropy loss between it and the model prediction.</p>
<p>For tasks AS1 and AS2, we trained the models on a shuffled mix of datasets AUM (unambiguous audio-ambiguous image, matching label) and UUN (unambiguous audio, unambiguous image, mismatched label). We set the audio labels as the target and otherwise trained the models under the same parameters as the visual tasks.</p>
</sec>
<sec id="s5d2">
<label>4.4.2</label>
<title>Control tasks</title>
<p>Datasets AUN (ambiguous audio, unambiguous image, nonmatching label) and UAN (unambiguous audio, ambiguous image, nonmatching label) were used only for testing in scenarios AS3 and VS3 respectively. The test-only datasets were also generated solely from holdout stimuli.</p>
<p>In addition, models were separately trained and evaluated on unambiguous MNIST and FSDD data to establish their baseline performance (scenarios VS4 and AS4).</p>
</sec>
<sec id="s5d3">
<label>4.4.3</label>
<title>Flexible task training</title>
<p>For the flexible task-switching experiments, the models are given a new output region and trained on datasets AUM, UAM, and UUN. Its output region is given a binary attention flag indicating the input stream to attend to.</p>
<p>After training, the image and auditory alignment of the models were tested using the previously unseen AAN (ambiguous audio, ambiguous image, nonmatching label) dataset.</p>
</sec>
<sec id="s5d4">
<label>4.4.4</label>
<title>Process time</title>
<p>All models have a unique hyperparameter called process time, which roughly determines how many computational steps the model can take before it must output an answer. A process time of 1, for instance, will mean that only the primary areas A1 and V1 will have received any feedforward input before the model generates an output. For all audiovisual tasks, we picked a process time equal to the number of areas in the model (7).</p>
</sec>
</sec>
<sec id="s5e">
<label>4.5</label>
<title>Regional activity analysis</title>
<p>We fetched the hidden state representation of all regions of the trained models at 7 different process times from 1-7 while they performed the tasks they were trained on. We then measured the quality of clustering in the latent space of each model region and time point by calculating the Neighborhood Hit (NH) score (<xref ref-type="bibr" rid="c58">Paulovich et al., 2008</xref>; <xref ref-type="bibr" rid="c63">Rauber et al., 2017</xref>). The NH score is the mean percentage of neighbors <italic>r</italic><sub><italic>j</italic></sub> for each datapoint <italic>n</italic> who share the same label <italic>C</italic>[<italic>x</italic><sub><italic>i</italic></sub>]. It’s calculated as below, using k=4 for every task.
<disp-formula id="eqn8">
<graphic xlink:href="615270v4_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s6">
<label>4.6</label>
<title>Statistical analysis</title>
<p>Mean final accuracies for multiplicative and composite feedback were compared using Welch’s t-test. Error bars and bands represent standard error of the mean, unless stated otherwise. *, p <italic>&lt;</italic> 0.05; **, p <italic>&lt;</italic> 0.01; ***, p <italic>&lt;</italic> 0.001; ****, p <italic>&lt;</italic> 0.0001; n.s., not significant.</p>
</sec>
<sec id="s7">
<label>4.7</label>
<title>Code availability</title>
<p>The toolbox used to convert graphs to top-down recurrent neural networks (Connectome-to-Model) is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/masht18/connectome-to-model">https://github.com/masht18/connectome-to-model</ext-link>. The task training scripts and graphs are also available at the same repository.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Nizar Islah for the ambiguous visual dataset as well as discussion regarding the ambiguous auditory dataset. We additionally thank Jessica Royer and Boris Bernhardt for discussions regarding the brain basis of the model, and Colin Bredenberg for helpful comments on the manuscript. This work was supported by NSERC (Discovery Grant: RGPIN-2020-05105; Discovery Accelerator Supplement: RGPAS-2020-00031; Arthur B. McDonald Fellowship: 566355-2022), CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship), and the Canada First Research Excellence Fund (CFREF Competition 2, 2015-2016) awarded to the Healthy Brains, Healthy Lives initiative at McGill University, through the Helmholtz International BigBrain Analytics and Learning Laboratory (HIBALL). E.B.M. was additionally supported by the Institute for Data Valorization (IVADO), the Centre de recherche Azrieli du CHU Sainte-Justine (CRACHUSJ), Fonds de Recherche du Québec–Santé (FRQS), and CIFAR (Canada AI Chair Mila). This research was enabled in part by support provided by Calcul Québec (<ext-link ext-link-type="uri" xlink:href="https://www.calculquebec.ca/en/">https://www.calculquebec.ca/en/</ext-link>) and the Digital Research Alliance of Canada (<ext-link ext-link-type="uri" xlink:href="https://alliancecan.ca/en">https://alliancecan.ca/en</ext-link>). The authors acknowledge the material support of NVIDIA in the form of computational resources. M. T received additional support from the Healthy Lives Healthy Brains Graduate Fellowship and UNIQUE Excellence Fellowship.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alstott</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hagmann</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cammoun</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Modeling the impact of lesions in the human brain</article-title>. <source>PLoS Computational Biology</source>, <volume>5</volume> (<issue>6</issue>), <fpage>e1000408</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000408</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amunts</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Lepage</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Borgeat</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Mohlberg</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Dickscheid</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rousseau</surname>, <given-names>M.-É.</given-names></string-name>, <string-name><surname>Bludau</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bazin</surname>, <given-names>P.-L.</given-names></string-name>, <string-name><surname>Lewis</surname>, <given-names>L. B.</given-names></string-name>, <string-name><surname>Oros-Peusquens</surname>, <given-names>A.-M.</given-names></string-name>, <string-name><surname>Shah</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Lippert</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Zilles</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Evans</surname>, <given-names>A. C.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Bigbrain: An ultrahigh-resolution 3d human brain model</article-title>. <source>Science</source>, <volume>340</volume> (<issue>6139</issue>), <fpage>1472</fpage>– <lpage>1475</lpage>. <pub-id pub-id-type="doi">10.1126/science.1235381</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ballas</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Pal</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Courville</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Delving Deeper into Convolutional Networks for Learning Video Representations</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1511.06432</pub-id> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1511.06432">http://arxiv.org/abs/1511.06432</ext-link> Comment: ICLR 2016.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barone</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Batardiere</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Knoblauch</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Kennedy</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Laminar Distribution of Neurons in Extrastriate Areas Projecting to Visual Areas V1 and V4 Correlates with the Hierarchical Rank and Indicates the Operation of a Distance Rule</article-title>. <source>The Journal of Neuroscience</source>, <volume>20</volume> (<issue>9</issue>), <fpage>3263</fpage>–<lpage>3281</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-09-03263.2000</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Betzel</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Griffa</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hagmann</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Mišić</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Distance-dependent consensus thresholds for generating group-representative structural brain networks</article-title>. <source>Network Neuroscience</source>, <volume>3</volume> (<issue>2</issue>), <fpage>475</fpage>–<lpage>496</lpage>. <pub-id pub-id-type="doi">10.1162/netna00075</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bittner</surname>, <given-names>K. C.</given-names></string-name>, <string-name><surname>Milstein</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Grienberger</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Romani</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Magee</surname>, <given-names>J. C.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Behavioral time scale synaptic plasticity underlies ca1 place fields</article-title>. <source>Science</source>, <volume>357</volume> (<issue>6355</issue>), <fpage>1033</fpage>–<lpage>1036</lpage>. <pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cadena</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Denfield</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Walker</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Gatys</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Deep convolutional models improve predictions of macaque v1 responses to natural images</article-title>. <source>PLOS Computational Biology</source>, <volume>15</volume> (<issue>4</issue>), <fpage>e1006897</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cho</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>van Merrienboer</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Gulcehre</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bahdanau</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bougares</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Schwenk</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Learning phrase representations using rnn encoder–decoder for statistical machine translation</article-title>. <conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</conf-name>. <pub-id pub-id-type="doi">10.3115/v1/d14-1179</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Choksi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Mozafari</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>O’May</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Ador</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Alamia</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>VanRullen</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Brain-inspired predictive coding dynamics improve the robustness of deep neural networks</article-title>. <source>NeurIPS 2020 Workshop SVRHM</source>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=q1o2mWaOssG">https://openreview.net/forum?id=q1o2mWaOssG</ext-link></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clavagnier</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Falchier</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Kennedy</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Long-distance feedback projections to area V1: Implications for multisensory integration, spatial awareness, and visual consciousness</article-title>. <source>Cognitive, Affective, &amp; Behavioral Neuroscience</source>, <volume>4</volume> (<issue>2</issue>), <fpage>117</fpage>–<lpage>126</lpage>. <pub-id pub-id-type="doi">10.3758/CABN.4.2.117</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Debes</surname>, <given-names>S. R.</given-names></string-name>, &amp; <string-name><surname>Dragoi</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Suppressing feedback signals to visual cortex abolishes attentional modulation</article-title>. <source>Science</source>, <volume>379</volume> (<issue>6631</issue>), <fpage>468</fpage>–<lpage>473</lpage>. <pub-id pub-id-type="doi">10.1126/science.ade1855</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Rolls</surname>, <given-names>E. T.</given-names></string-name></person-group> (<year>2004</year>). <article-title>A Neurodynamical cortical model of visual attention and invariant object recognition</article-title>. <source>Vision Research</source>, <volume>44</volume> (<issue>6</issue>), <fpage>621</fpage>–<lpage>642</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2003.09.037</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sommers</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Seeliger</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ismael</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lindsay</surname>, <given-names>G. W.</given-names></string-name>, <string-name><surname>Kording</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>van Gerven</surname>, <given-names>M. A. J.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The neuroconnectionist research programme</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>24</volume> (<issue>7</issue>), <fpage>431</fpage>–<lpage>450</lpage>. <pub-id pub-id-type="doi">10.1038/s41583-023-00705-w</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doron</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Shin</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Takahashi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Drüke</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bocklisch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Skenderi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>de Mont</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Toumazou</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ledderose</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Brecht</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Larkum</surname>, <given-names>M. E.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Perirhinal input to neocortical layer 1 controls learning</article-title>. <source>Science</source>, <volume>370</volume>(<issue>6523</issue>):<elocation-id>eaaz3136</elocation-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felleman</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Distributed Hierarchical Processing in the Primate Visual Cortex</article-title>. <source>Cerebral Cortex</source>, <volume>1</volume> (<issue>1-47</issue>). <pub-id pub-id-type="doi">10.1093/cercor/1.1.1</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Francioni</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>V. D.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Toloza</surname>, <given-names>E. H.</given-names></string-name>, &amp; <string-name><surname>Harnett</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Vectorized instructive signals in cortical dendrites during a brain-computer interface task</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2023.11.03.565534</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Kiebel</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Predictive coding under the free-energy principle</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>364</volume> (<issue>1521</issue>), <fpage>1211</fpage>–<lpage>1221</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2008.0300</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fukushima</surname>, <given-names>K.</given-names></string-name></person-group> (<year>1980</year>). <article-title>Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title>. <source>Biological Cybernetics</source>, <volume>36</volume> (<issue>4</issue>), <fpage>193</fpage>–<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1007/bf00344251</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garner</surname>, <given-names>A. R.</given-names></string-name>, &amp; <string-name><surname>Keller</surname>, <given-names>G. B.</given-names></string-name></person-group> (<year>2021</year>). <article-title>A cortical circuit for audio-visual predictions</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume> (<issue>1</issue>), <fpage>98</fpage>–<lpage>105</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-021-00974-7</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Geirhos</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Temme</surname>, <given-names>C. R. M.</given-names></string-name>, <string-name><surname>Rauber</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schütt</surname>, <given-names>H. H.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Generalisation in humans and deep neural networks</article-title>. In <person-group person-group-type="editor"><string-name><given-names>S.</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Wallach</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Grauman</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Cesa-Bianchi</surname></string-name>, &amp; <string-name><given-names>R.</given-names> <surname>Garnett</surname></string-name></person-group> (Eds.), <conf-name>Advances in neural information processing systems</conf-name> (Vol. <volume>31</volume>). <publisher-name>Curran Associates, Inc</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paperfiles/paper/2018/file/0937fb5864ed06ffb59ae5f9b5ed67a9-Paper.pdf">https://proceedings.neurips.cc/paperfiles/paper/2018/file/0937fb5864ed06ffb59ae5f9b5ed67a9-Paper.pdf</ext-link></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gerbella</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Belmalih</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Borra</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Rozzi</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Luppino</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Multimodal architectonic subdivision of the caudal ventrolateral prefrontal cortex of the macaque monkey</article-title>. <source>Brain Structure and Function</source>, <volume>212</volume> (<issue>3-4</issue>), <fpage>269</fpage>–<lpage>301</lpage>. <pub-id pub-id-type="doi">10.1007/s00429-007-0158-9</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giard</surname>, <given-names>M. H.</given-names></string-name>, &amp; <string-name><surname>Peronnet</surname>, <given-names>F.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Auditory-Visual Integration during Multimodal Object Recognition in Humans: A Behavioral and Electrophysiological Study</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>11</volume> (<issue>5</issue>), <fpage>473</fpage>–<lpage>490</lpage>. <pub-id pub-id-type="doi">10.1162/089892999563544</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Hacker</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Harwell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> (<year>2016</year>). <article-title>A multi-modal parcellation of human cerebral cortex</article-title>. <source>Nature</source>, <volume>536</volume> (<issue>7615</issue>), <fpage>171</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1038/nature18933</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goulas</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zilles</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Hilgetag</surname>, <given-names>C. C.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Cortical gradients and laminar projections in mammals</article-title>. <source>Trends in Neurosciences</source>, <volume>41</volume> (<issue>11</issue>), <fpage>775</fpage>–<lpage>788</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2018.06.003</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Greedy</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>H. W.</given-names></string-name>, <string-name><surname>Pemberton</surname>, <given-names>J. O.</given-names></string-name>, <string-name><surname>Mellor</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Costa</surname>, <given-names>R. P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Single-phase deep learning in cortico-cortical networks</article-title>. In <person-group person-group-type="editor"><string-name><given-names>A. H.</given-names> <surname>Oh</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Agarwal</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Belgrave</surname></string-name>, &amp; <string-name><given-names>K.</given-names> <surname>Cho</surname></string-name></person-group> (Eds.), <source>Advances in neural information processing systems</source>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=szt95rn-ql">https://openreview.net/forum?id=szt95rn-ql</ext-link></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guerguiev</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name>, &amp; <string-name><surname>Richards</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Towards deep learning with segregated dendrites</article-title>. <source>eLife</source>, <volume>6</volume>. <pub-id pub-id-type="doi">10.7554/elife.22901</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Gornet</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tsao</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Anandkumar</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Neural networks with recurrent generative feedback</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>33</volume>, <fpage>535</fpage>–<lpage>545</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname>, <given-names>D. H.</given-names></string-name>, &amp; <string-name><surname>Wiesel</surname>, <given-names>T. N.</given-names></string-name></person-group> (<year>1962</year>). <article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title>. <source>The Journal of Physiology</source>, <volume>160</volume> (<issue>1</issue>), <fpage>106</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Islah</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Etter</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Tugsbayar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gurbuz</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Muller</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Learning to combine top-down context and feed-forward representations under ambiguity with apical and basal dendrites</article-title>. <source>arXiv preprint</source> arXiv: <pub-id pub-id-type="arxiv">2312.05484</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiang</surname>, <given-names>L. P.</given-names></string-name>, &amp; <string-name><surname>Rao</surname>, <given-names>R. P. N.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Dynamic predictive coding: A model of hierarchical sequence learning and prediction in the neocortex</article-title>. <source>PLOS Computational Biology</source>, <volume>20</volume> (<issue>2</issue>), <fpage>e1011801</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1011801</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jordan</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Keller</surname>, <given-names>G. B.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Opposing influence of top-down and bottom-up input on excitatory layer 2/3 neurons in mouse primary visual cortex</article-title>. <source>Neuron</source>, <volume>108</volume> (<issue>6</issue>), <fpage>1194</fpage>–<lpage>1206.e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.024</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kar</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kubilius</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Issa</surname>, <given-names>E. B.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume> (<issue>6</issue>), <fpage>974</fpage>–<lpage>983</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-019-0392-5</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kell</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Yamins</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Shook</surname>, <given-names>E. N.</given-names></string-name>, <string-name><surname>Norman-Haignere</surname>, <given-names>S. V.</given-names></string-name>, &amp; <string-name><surname>McDermott</surname>, <given-names>J. H.</given-names></string-name></person-group> (<year>2018</year>). <article-title>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title>. <source>Neuron</source>, <volume>98</volume> (<issue>3</issue>), <fpage>630</fpage>–<lpage>644.e16</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Deep supervised, but not unsupervised, models may explain it cortical representation</article-title>. <source>PLoS Computational Biology</source>, <volume>10</volume> (<issue>11</issue>), <fpage>e1003915</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>King</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Unraveling the principles of auditory cortical processing: Can we learn from the visual system?</article-title> <source>Nature Neuroscience</source>, <volume>12</volume> (<issue>6</issue>), <fpage>698</fpage>–<lpage>701</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2308</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kubilius</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schrimpf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nayebi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bear</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Cornet: Modeling the neural mechanisms of core object recognition</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/408385</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Larkum</surname>, <given-names>M. E.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Top-down dendritic input increases the gain of layer 5 pyramidal neurons</article-title>. <source>Cerebral Cortex</source>, <volume>14</volume> (<issue>10</issue>), <fpage>1059</fpage>–<lpage>1070</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhh065</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>D.-H.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fischer</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2015</year>). <chapter-title>Difference target propagation</chapter-title>. In <source>Machine Learning and Knowledge Discovery in Databases</source> (pp. <fpage>498</fpage>–<lpage>515</lpage>). <publisher-name>Springer International Publishing</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-3-319-23528-8_31</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leinweber</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ward</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Sobczak</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Attinger</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Keller</surname>, <given-names>G. B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions</article-title>. <source>Neuron</source>, <volume>95</volume> (<issue>6</issue>), <fpage>1420</fpage>–<lpage>1432.e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.036</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Piëch</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Gilbert</surname>, <given-names>C. D.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Perceptual learning and top-down influences in primary visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>7</volume> (<issue>6</issue>), <fpage>651</fpage>–<lpage>657</lpage>. <pub-id pub-id-type="doi">10.1038/nn1255</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Lindsay</surname>, <given-names>G. W.</given-names></string-name>, <string-name><surname>Mrsic-Flogel</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Bio-inspired neural networks implement different recurrent visual processing strategies than task-trained ones do</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2022.03.07.483196</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Xin</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Xu</surname>, <given-names>N.-l.</given-names></string-name></person-group> (<year>2021</year>). <article-title>A cortical circuit mechanism for structural knowledge-based flexible sensorimotor decision-making</article-title>. <source>Neuron</source>, <volume>109</volume> (<issue>12</issue>), <fpage>2009</fpage>–<lpage>2024.e6</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2021.04.014</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manita</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Suzuki</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Homma</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Matsumoto</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Odagawa</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Yamada</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ota</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Matsubara</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Inutsuka</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sato</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ohkura</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Yamanaka</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yanagawa</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Nakai</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hayashi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Larkum</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Murayama</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2015</year>). <article-title>A Top-Down Cortical Circuit for Accurate Sensory Perception</article-title>. <source>Neuron</source>, <volume>86</volume> (<issue>5</issue>), <fpage>1304</fpage>–<lpage>1316</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.006</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markov</surname>, <given-names>N. T.</given-names></string-name>, &amp; <string-name><surname>Kennedy</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2013</year>). <article-title>The importance of being hierarchical</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>23</volume> (<issue>2</issue>), <fpage>187</fpage>–<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2012.12.008</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martínez-Molina</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Escrichs</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sanz-Perl</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sihvonen</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Särkämö</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kringelbach</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2024</year>). <article-title>The evolution of whole-brain turbulent dynamics during recovery from traumatic brain injury</article-title>. <source>Network Neuroscience</source>, <volume>8</volume> (<issue>1</issue>), <fpage>158</fpage>–<lpage>177</lpage>. <pub-id pub-id-type="doi">10.1162/netna00346</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAdams</surname>, <given-names>C. J.</given-names></string-name>, &amp; <string-name><surname>Maunsell</surname>, <given-names>J. H. R.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area v4</article-title>. <source>The Journal of Neuroscience</source>, <volume>19</volume> (<issue>1</issue>), <fpage>431</fpage>–<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.19-01-00431.1999</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGurck</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Macdonald</surname>, <given-names>o.</given-names></string-name></person-group> (<year>1976</year>). <article-title>Hearing lips and seeing voices</article-title>. <source>Nature</source>, <volume>264</volume> (<issue>5588</issue>), <fpage>746</fpage>–<lpage>748</lpage>. <pub-id pub-id-type="doi">10.1038/264746a0</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Mittal</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lamb</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Goyal</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Voleti</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Shanahan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lajoie</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Mozer</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2020</year>, 13–18 Jul). <article-title>Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules</article-title>. In <person-group person-group-type="editor"><string-name><given-names>H.D.</given-names> <surname>Iii</surname></string-name> &amp; <string-name><given-names>A.</given-names> <surname>Singh</surname></string-name></person-group> (Eds.), <conf-name>Proceedings of the 37th international conference on machine learning</conf-name> (pp. <fpage>6972</fpage>–<lpage>6986</lpage>, Vol. <volume>119</volume>). <publisher-name>PMLR</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v119/mittal20a.html">https://proceedings.mlr.press/v119/mittal20a.html</ext-link></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morecraft</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Stilwell-Morecraft</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Cipolloni</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ge</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>McNeal</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Pandya</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Cytoarchitecture and cortical connections of the anterior cingulate and adjacent somatomotor fields in the rhesus monkey</article-title>. <source>Brain Research Bulletin</source>, <volume>87</volume> (<issue>4-5</issue>), <fpage>457</fpage>–<lpage>497</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainresbull.2011.12.005</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morosan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Schleicher</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Amunts</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Zilles</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Multimodal architectonic mapping of human superior temporal gyrus</article-title>. <source>Anatomy and Embryology</source>, <volume>210</volume> (<issue>5–6</issue>), <fpage>401</fpage>–<lpage>406</lpage>. <pub-id pub-id-type="doi">10.1007/s00429-005-0029-1</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mumford</surname>, <given-names>D.</given-names></string-name></person-group> (<year>1992</year>). <article-title>On the computational architecture of the neocortex: Ii the role of cortico-cortical loops</article-title>. <source>Biological cybernetics</source>, <volume>66</volume> (<issue>3</issue>), <fpage>241</fpage>–<lpage>251</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nassi</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Lomber</surname>, <given-names>S. G.</given-names></string-name>, &amp; <string-name><surname>Born</surname>, <given-names>R. T.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Corticocortical feedback contributes to surround suppression in v1 of the alert primate</article-title>. <source>The Journal of Neuroscience</source>, <volume>33</volume> (<issue>19</issue>), <fpage>8504</fpage>–<lpage>8517</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.5124-12.2013</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naumann</surname>, <given-names>L. B.</given-names></string-name>, <string-name><surname>Keijser</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Sprekeler</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Invariant neural subspaces maintained by feedback modulation</article-title>. <source>eLife</source>, <volume>11</volume>, <elocation-id>e76096</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.76096</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>O’May</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Choksi</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>VanRullen</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Predictive coding feedback results in perceived illusory contours in a recurrent neural network</article-title>. <source>Neural Networks</source>, <volume>144</volume>, <fpage>164</fpage>–<lpage>175</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2021.08.024</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paquola</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Vos De Wael</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Wagstyl</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bethlehem</surname>, <given-names>R. A. I.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Seidlitz</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bullmore</surname>, <given-names>E. T.</given-names></string-name>, <string-name><surname>Evans</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Misic</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Margulies</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Smallwood</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Bernhardt</surname>, <given-names>B. C.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Microstructural and functional gradients are increasingly dissociated in transmodal cortices [Edition: 2019/05/21]</article-title>. <source>PLoS Biol</source>, <volume>17</volume> (<issue>5</issue>), <fpage>e3000284</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000284</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paquola</surname>, <given-names>CaseyVos</given-names></string-name> <string-name><given-names>De</given-names> <surname>Wael</surname></string-name>, <string-name><given-names>Reinder</given-names> <surname>Wagstyl</surname></string-name>, <string-name><given-names>Konrad</given-names> <surname>Bethlehem</surname></string-name>, <string-name><given-names>Richard A</given-names> <surname>IHong</surname></string-name>, <string-name><surname>Seok-Jun</surname> <given-names>Seidlitz</given-names></string-name>, <string-name><given-names>Jakob</given-names> <surname>Bullmore</surname></string-name>, <string-name><given-names>Edward</given-names> <surname>TEvans</surname></string-name>, <string-name><given-names>Alan</given-names> <surname>CMisic</surname></string-name>, <string-name><given-names>Bratislav</given-names> <surname>Margulies</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>SSmallwood</surname></string-name>, <string-name><given-names>Jonathan</given-names> <surname>Bernhardt</surname></string-name></person-group>, <article-title>Boris Ceng FDN-154298/CIHR/CanadaResearch Support, Non-U.S. Gov’t</article-title> <source>PLoS Biol</source>. <year>2019</year> <month>May</month> 20;<volume>17</volume>(<issue>5</issue>):<fpage>e3000284</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.3000284</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paquola</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Seidlitz</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Benkarim</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Royer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Klimes</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bethlehem</surname>, <given-names>R. A. I.</given-names></string-name>, <string-name><surname>Lariviére</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vos de Wael</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Rodríguez-Cruces</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Frauscher</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Smallwood</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Bernhardt</surname>, <given-names>B. C.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A multi-scale cortical wiring space links cellular architecture and functional dynamics in the human brain</article-title>. <source>PLOS Biology</source>, <volume>18</volume> (<issue>11</issue>), <fpage>e3000979</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000979</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paulovich</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nonato</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Minghim</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Levkowitz</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Least Square Projection: A Fast High-Precision Multidimensional Projection Technique and Its Application to Document Mapping</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>, <volume>14</volume> (<issue>3</issue>), <fpage>564</fpage>–<lpage>575</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2007.70443</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Payeur</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Guerguiev</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zenke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B. A.</given-names></string-name>, &amp; <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits</article-title>. <source>Nature Neuroscience</source>, <volume>24</volume> (<issue>7</issue>), <fpage>1010</fpage>– <lpage>1019</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-021-00857-x</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pines</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Keller</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Larsen</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bertolero</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ashourvan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Cieslak</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Covitz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fan</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Feczko</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Houghton</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rueter</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Saggar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Shafiei</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Tapera</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Vogel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Weinstein</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Shinohara</surname>, <given-names>R. T.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>L. M.</given-names></string-name>, <etal>…</etal> <string-name><surname>Satterthwaite</surname>, <given-names>T. D.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Development of top-down cortical propagations in youth</article-title>. <source>Neuron</source>, <volume>111</volume> (<issue>8</issue>), <fpage>1316</fpage>–<lpage>1330.e5</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2023.01.014</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Posner</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Nissen</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Klein</surname>, <given-names>R. M.</given-names></string-name></person-group> (<year>1976</year>). <article-title>Visual dominance: An information-processing account of its origins and significance</article-title>. <source>Psychological Review</source>, <volume>83</volume> (<issue>2</issue>), <fpage>157</fpage>–<lpage>171</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295x.83.2.157</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R. P. N.</given-names></string-name>, &amp; <string-name><surname>Ballard</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source>, <volume>2</volume> (<issue>1</issue>), <fpage>79</fpage>–<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1038/4580</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rauber</surname>, <given-names>P. E.</given-names></string-name>, <string-name><surname>Fadel</surname>, <given-names>S. G.</given-names></string-name>, <string-name><surname>Falcao</surname>, <given-names>A. X.</given-names></string-name>, &amp; <string-name><surname>Telea</surname>, <given-names>A. C.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Visualizing the Hidden Activity of Artificial Neural Networks</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>, <volume>23</volume> (<issue>1</issue>), <fpage>101</fpage>–<lpage>110</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2016.2598838</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reynolds</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Pasternak</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Attention increases sensitivity of v4 neurons</article-title>. <source>Neuron</source>, <volume>26</volume> (<issue>3</issue>), <fpage>703</fpage>–<lpage>714</lpage>. <pub-id pub-id-type="doi">10.1016/s0896-6273(00)81206-4</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roelfsema</surname>, <given-names>P. R.</given-names></string-name>, &amp; <string-name><surname>Ooyen</surname>, <given-names>A. v.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Attention-gated reinforcement learning of internal representations for classification</article-title>. <source>Neural Computation</source>, <volume>17</volume> (<issue>10</issue>), <fpage>2176</fpage>–<lpage>2214</lpage>. <pub-id pub-id-type="doi">10.1162/0899766054615699</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Royer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rodríguez-Cruces</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Tavakol</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lariviére</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Herholz</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Vos de Wael</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Paquola</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Benkarim</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Park</surname>, <given-names>B.-y.</given-names></string-name>, <string-name><surname>Lowe</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Margulies</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Smallwood</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bernasconi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bernasconi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Frauscher</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Bernhardt</surname>, <given-names>B. C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>An open mri dataset for multiscale neuroscience</article-title>. <source>Scientific Data</source>, <volume>9</volume> (<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/s41597-022-01682-y</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saberi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Paquola</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Wagstyl</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Hettwer</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Bernhardt</surname>, <given-names>B. C.</given-names></string-name>, <string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name>, &amp; <string-name><surname>Valk</surname>, <given-names>S. L.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The regional variation of laminar thickness in the human isocortex is related to cortical hierarchy and interregional connectivity</article-title>. <source>PLOS Biology</source>, <volume>21</volume> (<issue>11</issue>), <fpage>e3002365</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3002365</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Sacramento</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ponte Costa</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Dendritic cortical microcircuits approximate the backpropagation algorithm</article-title>. In <person-group person-group-type="editor"><string-name><given-names>S.</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Wallach</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Grauman</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Cesa-Bianchi</surname></string-name>, &amp; <string-name><given-names>R.</given-names> <surname>Garnett</surname></string-name></person-group> (Eds.), <conf-name>Advances in neural information processing systems</conf-name> (Vol. <volume>31</volume>). <publisher-name>Curran Associates, Inc</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2018/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf</ext-link></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sanides</surname>, <given-names>F.</given-names></string-name></person-group> (<year>1962</year>). <source>Die architektonik des menschlichen stirnhirns zugleich eine darstellung der prinzipien seiner gestaltung als spiegel der stammesgeschichtlichen differenzierung der grosshirnrinde</source>. <publisher-name>Springer-Verlag</publisher-name>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shai</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Anastassiou</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Larkum</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Physiology of layer 5 pyramidal neurons in mouse primary visual cortex: Coincidence detection through bursting</article-title>. <source>PLOS Computational Biology</source>, <volume>11</volume> (<issue>3</issue>), <fpage>e1004090</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004090</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shams</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Shimojo</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2000</year>). <article-title>What you see is what you hear</article-title>. <source>Nature</source>, <volume>408</volume> (<issue>6814</issue>), <fpage>788</fpage>–<lpage>788</lpage>. <pub-id pub-id-type="doi">10.1038/35048669</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sherman</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Guillery</surname>, <given-names>R. W.</given-names></string-name></person-group> (<year>1998</year>). <article-title>On the actions that one nerve cell can have on another: Distinguishing “drivers” from “modulators”</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>95</volume> (<issue>12</issue>), <fpage>7121</fpage>–<lpage>7126</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.95.12.7121</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Sohn</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Yan</surname>, <given-names>X.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Learning structured output representation using deep conditional generative models</article-title>. In <person-group person-group-type="editor"><string-name><given-names>C.</given-names> <surname>Cortes</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Lawrence</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sugiyama</surname></string-name>, &amp; <string-name><given-names>R.</given-names> <surname>Garnett</surname></string-name></person-group> (Eds.), <conf-name>Advances in neural information processing systems</conf-name> (Vol. <volume>28</volume>). <publisher-name>Curran Associates, Inc</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paperfiles/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf">https://proceedings.neurips.cc/paperfiles/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf</ext-link></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Stokes</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Biggs</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2014</year>). <chapter-title>The dominance of the visual</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>D.</given-names> <surname>Stokes</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Matthen</surname></string-name>, &amp; <string-name><given-names>S.</given-names> <surname>Biggs</surname></string-name></person-group> (Eds.), <source>Perception and its modalities</source>. <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tsai</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Teutsch</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wybo</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>Helmchen</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Banerjee</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2024</year>, <month>October</month>). <article-title>Hierarchy of prediction errors shapes the learning of context-dependent sensory representations</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2024.09.30.615819</pub-id></mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Bergen</surname>, <given-names>R. S.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Going in circles is the way forward: The role of recurrence in visual inference</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>65</volume>, <fpage>176</fpage>–<lpage>193</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2020.11.009</pub-id></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wagstyl</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Larocque</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cucurull</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lepage</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Bludau</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Palomero-Gallagher</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lewis</surname>, <given-names>L. B.</given-names></string-name>, <string-name><surname>Funck</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Spitzer</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Dickscheid</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Fletcher</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Romero</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zilles</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Amunts</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Evans</surname>, <given-names>A. C.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Bigbrain 3d atlas of cortical layers: Cortical and laminar thickness gradients diverge in sensory and motor cortices</article-title>. <source>PLOS Biology</source>, <volume>18</volume> (<issue>4</issue>), <fpage>e3000678</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000678</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wagstyl</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ronan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Goodyer</surname>, <given-names>I. M.</given-names></string-name>, &amp; <string-name><surname>Fletcher</surname>, <given-names>P. C.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Cortical thickness gradients in structural hierarchies</article-title>. <source>NeuroImage</source>, <volume>111</volume>, <fpage>241</fpage>–<lpage>250</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.02.036</pub-id></mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Snider</surname>, <given-names>R. K.</given-names></string-name>, &amp; <string-name><surname>Liang</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Sustained firing in auditory cortex evoked by preferred stimuli</article-title>. <source>Nature</source>, <volume>435</volume> (<issue>7040</issue>), <fpage>341</fpage>–<lpage>346</lpage>. <pub-id pub-id-type="doi">10.1038/nature03565</pub-id></mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Culurciello</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Deep predictive coding network for object recognition</article-title>. <conf-name>International Conference on Machine Learning</conf-name>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Wybo</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>Tsai</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Khoa Tran</surname>, <given-names>V. A.</given-names></string-name>, <string-name><surname>Illing</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Jordan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Morrison</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Senn</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Dendritic modulation enables multitask representation learning in hierarchical sensory processing pathways</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2022.11.25.517941</pub-id></mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>D. L.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Using goal-driven deep learning models to understand sensory cortex [Edition: 2016/02/26]</article-title>. <source>Nat Neurosci</source>, <volume>19</volume> (<issue>3</issue>), <fpage>356</fpage>–<lpage>65</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4244</pub-id></mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname> <given-names>DLK</given-names></string-name> <string-name><given-names>J</given-names> <surname>DiCarlo</surname></string-name></person-group> <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title> <source>Review Nat Neurosci</source>. <year>2016</year> <month>Mar</month>;<volume>19</volume>(<issue>3</issue>):<fpage>356</fpage>–<lpage>65</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn.4244</pub-id>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zilles</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Amunts</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Receptor mapping: Architecture of the human cerebral cortex</article-title>. <source>Current Opinion in Neurology</source>, <volume>22</volume> (<issue>4</issue>), <fpage>331</fpage>–<lpage>339</lpage>. <pub-id pub-id-type="doi">10.1097/wco.0b013e32832d95db</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105953.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Gjorgjieva</surname>
<given-names>Julijana</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Technical University of Munich</institution>
</institution-wrap>
<city>Freising</city>
<country>Germany</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study investigates the computational role of top-down feedback -- a property that is found in biological circuits -- in Artificial Neural Network (ANN) models of the neocortex. Using hierarchical recurrent ANNs in an audiovisual integration task, the authors show a visual bias consistent with that observed in human perception, which mildly improves learning speed. While the study offers a tool that is of value for studying top-down feedback in cortical models, with the potential to inspire other fields (e.g. machine learning), the presented evidence for a general framework of deep learning architectures that predict behavior is <bold>incomplete</bold>, and the methods section lacks sufficient detail in terms of hyperparameter choice and network structures.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105953.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Here, the authors aim to investigate the potential improvements of ANNs when used to explain brain data using top-down feedback connections found in the neocortex. To do so, they use a retinotopic and tonotopic organization to model each subregion of the ventral visual (V1, V2, V4, and IT) and ventral auditory (A1, Belt, A4) regions using Convolutional Gated Recurrent Units. The top-down feedback connections are inspired by the apical tree of pyramidal neurons, modeled either with a multiplicative effect (change of gain of the activation function) or a composite effect (change of gain and threshold of the activation function).</p>
<p>To assess the functional impact of the top-down connections, the authors compare three architectures: a brain-like architecture derived directly from brain data analysis, a reversed architecture where all feedforward connections become feedback connections and vice versa, and a random connectivity architecture. More specifically, in the brain-like model the visual regions provide feedforward input to all auditory areas, whereas auditory areas provide feedback to visual regions.</p>
<p>First, the authors found that top-down feedback influences audiovisual processing and that the brain-like model exhibits a visual bias in multimodal visual and auditory tasks. Second, they discovered that in the brain-like model, the composite integration of top-down feedback, similar to that found in the neocortex, leads to an inductive bias toward visual stimuli, which is not observed in the feedforward-only model. Furthermore, the authors found that the brain-like model learns to utilize relevant stimuli more quickly while ignoring distractors. Finally, by analyzing the activations of all hidden layers (brain regions), they found that the feedforward and feedback connectivity of a region could determine its functional specializations during the given tasks.</p>
<p>Strengths:</p>
<p>The study introduces a novel methodology for designing connectivity between regions in deep learning models. The authors also employ several tasks based on audiovisual stimuli to support their conclusions. Additionally, the model utilizes backpropagation of error as a learning algorithm, making it applicable across a range of tasks, from various supervised learning scenarios to reinforcement learning agents. Conversely, the presented framework offers a valuable tool for studying top-down feedback connections in cortical models. Thus, it is a very nice study that also can give inspiration to other fields (machine learning) to start exploring new architectures.</p>
<p>Weaknesses:</p>
<p>Although the study explores some novel ideas on how to study the feedback connections of the neocortex, the data presented here are not complete in order to propose a concrete theory of the role of top-down feedback inputs in such models of the brain.</p>
<p>(1) The gap in the literature that the paper tries to fill in the ability of DL algorithms to predict behavior: &quot;However, there are still significant gaps in most deep neural networks' ability to predict behavior, particularly when presented with ambiguous, challenging stimuli.&quot; and &quot;[...] to accurately model the brain.&quot;</p>
<p>It is unclear to me how the presented work addresses this gap, as the only facts provided are derived from a simple categorization task that could also be solved by the feedforward-only model (see Figures 4 and 5). In my opinion, this statement is somewhat far-fetched, and there is insufficient data throughout the manuscript to support this claim.</p>
<p>(2) It is not clear what the advantages are between the brain-like model and a feedforward-only model in terms of performance in solving the task. Given Figures 4 and 5, it is evident that the feedforward-only model reaches almost the same performance as the brain-like model (when the latter uses the modulatory feedback with the composite function) on almost all tasks tested. The speed of learning is nearly the same: for some tested tasks the brain-like model learns faster, while for others it learns slower. Thus, it is hard to attribute a functional implication to the feedback connections given the presented figures and therefore the strong claims in the Discussion should be rephrased or toned down.</p>
<p>(3) The Methods section lacks sufficient detail. There is no explanation provided for the choice of hyperparameters nor for the structure of the networks (number of trainable parameters, number of nodes per layer, etc). Clarifying the rationale behind these decisions would enhance understanding. Moreover, since the authors draw conclusions based on the performance of the networks on specific tasks, it is unclear whether the comparisons are fair, particularly concerning the number of trainable parameters. Furthermore, it is not clear if the visual bias observed in the brain-like model is an emerging property of the network or has been created because of the asymmetries in the visual vs. auditory pathway (size of the layer, number of layers, etc).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105953.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work addresses the question of whether artificial deep neural network models of the brain could be improved by incorporating top-down feedback, inspired by the architecture of the neocortex.</p>
<p>In line with known biological features of cortical top-down feedback, the authors model such feedback connections with both, a typical driving effect and a purely modulatory effect on the activation of units in the network.</p>
<p>To assess the functional impact of these top-down connections, they compare different architectures of feedforward and feedback connections in a model that mimics the ventral visual and auditory pathways in the cortex on an audiovisual integration task.</p>
<p>Notably, one architecture is inspired by human anatomical data, where higher visual and auditory layers possess modulatory top-down connections to all lower-level layers of the same modality, and visual areas provide feedforward input to auditory layers, whereas auditory areas provide modulatory feedback to visual areas.</p>
<p>First, the authors find that this brain-like architecture imparts the models with a light visual bias similar to what is seen in human data, which is the opposite in a reversed architecture, where auditory areas provide a feedforward drive to the visual areas.</p>
<p>Second, they find that, in their model, modulatory feedback should be complemented by a driving component to enable effective audiovisual integration, similar to what is observed in neural data.</p>
<p>Last, they find that the brain-like architecture with modulatory feedback learns a bit faster in some audiovisual switching tasks compared to a feedforward-only model.</p>
<p>Overall, the study shows some possible functional implications when adding feedback connections in a deep artificial neural network that mimics some functional aspects of visual perception in humans.</p>
<p>Strengths:</p>
<p>The study contains innovative ideas, such as incorporating an anatomically inspired architecture into a deep ANN, and comparing its impact on a relevant task to alternative architectures.</p>
<p>Moreover, the simplicity of the model allows it to draw conclusions on how features of the architecture and functional aspects of the top-down feedback affect the performance of the network.</p>
<p>This could be a helpful resource for future studies of the impact of top-down connections in deep artificial neural network models of the neocortex.</p>
<p>Weaknesses:</p>
<p>Overall, the study appears to be a bit premature, as several parts need to be worked out more to support the claims of the paper and to increase its impact.</p>
<p>First, the functional implication of modulatory feedback is not really clear. The &quot;only feedforward&quot; model (is a drive-only model meant?) attains the same performance as the composite model (with modulatory feedback) on virtually all tasks tested, it just takes a bit longer to learn for some tasks, but then is also faster at others. It even reproduces the visual bias on the audiovisual switching task. Therefore, the claims &quot;Altogether, our results demonstrate that the distinction between feedforward and feedback inputs has clear computational implications, and that ANN models of the brain should therefore consider top-down feedback as an important biological feature.&quot; and &quot;More broadly, our work supports the conclusion that both the cellular neurophysiology and structure of feed-back inputs have critical functional implications that need to be considered by computational models of brain function&quot; are not sufficiently supported by the results of the study. Moreover, the latter points would require showing that this model describes neural data better, e.g., by comparing representations in the model with and without top-down feedback to recorded neural activity.</p>
<p>Second, the analyses are not supported by supplementary material, hence it is difficult to evaluate parts of the claims. For example, it would be helpful to investigate the impact of the process time after which the output is taken for evaluation of the model. This is especially important because in recurrent and feedback models the convergence should be checked, and if the network does not converge, then it should be discussed why at which point in time the network is evaluated.</p>
<p>Third, the descriptions of the models in the methods are hard to understand, i.e., parameters are not described and equations are explained by referring to multiple other studies. Since the implications of the results heavily rely on the model, a more detailed description of the model seems necessary.</p>
<p>Lastly, the discussion and testable predictions are not very well worked out and need more details. For example, the point &quot;This represents another testable prediction flowing from our study, which could be studied in humans by examining the optical flow (Pines et al., 2023) between auditory and visual regions during an audiovisual task&quot; needs to be made more precise to be useful as a prediction. What did the model predict in terms of &quot;optic flow&quot;, how can modulatory from simple driving effect be distinguished, etc.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.105953.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigates the computational role of top-down feedback in artificial neural networks (ANNs), a feature that is prevalent in the brain but largely absent in standard ANN architectures. The authors construct hierarchical recurrent ANN models that incorporate key properties of top-down feedback in the neocortex. Using these models in an audiovisual integration task, they find that hierarchical structures introduce a mild visual bias, akin to that observed in human perception, not always compromising task performance.</p>
<p>Strengths:</p>
<p>The study investigates a relevant and current topic of considering top-down feedback in deep neural networks. In designing their brain-like model, they use neurophysiological data, such as externopyramidisation and hierarchical connectivity. Their brain-like model exhibits a visual bias that qualitatively matches human perception.</p>
<p>Weaknesses:</p>
<p>While the model is brain-inspired, it has limited bioplausibility. The model assumes a simplified and fixed hierarchy. In the brain with additional neuromodulation, the hierarchy could be more flexible and more task-dependent.</p>
<p>While the brain-like model showed an advantage in ignoring distracting auditory inputs, it struggled when visual information had to be ignored. This suggests that its rigid bias toward visual processing could make it less adaptive in tasks requiring flexible multimodal integration. It hence does not necessarily constitute an improvement over existing ANNs. It is unclear, whether this aspect of the model also matches human data. In general, there is no direct comparison to human data. The study does not evaluate whether the top-down feedback architecture scales well to more complex problems or larger datasets. The model is not well enough specified in the methods and some definitions are missing.</p>
</body>
</sub-article>
</article>