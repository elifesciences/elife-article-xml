<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108357</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108357</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108357.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Regulation of sensorimotor serial learning in speech production by motor compensation rather than sensory error</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Lu</surname>
<given-names>Yuhan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Tang</surname>
<given-names>Xiaowei</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Xiao</surname>
<given-names>Zhenyan</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Xu</surname>
<given-names>Anqi</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Chen</surname>
<given-names>Junxi</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<email>chenjunxi2006@163.com</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1629-6304</contrib-id>
<name>
<surname>Tian</surname>
<given-names>Xing</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
<email>xing.tian@nyu.edu</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02n96ep67</institution-id><institution>Shanghai Key Laboratory of Brain Functional Genomics (Ministry of Education), School of Psychology and Cognitive Science, East China Normal University</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vpsdb40</institution-id><institution>NYU-ECNU Institute of Brain and Cognitive Science, New York University Shanghai</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
<aff id="a3"><label>3</label><institution>Department of Neurosurgery, Guangzhou Sanjiu Brain Hospital</institution>, <city>Guanzhou</city>, <country country="CN">China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vpsdb40</institution-id><institution>Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning; Division of Arts and Sciences, New York University Shanghai</institution></institution-wrap>, <city>Shanghai</city>, <country country="CN">China</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yqg2h08</institution-id><institution>School of Humanities and Social Sciences, Harbin Institute of Technology</institution></institution-wrap>, <city>Shenzhen</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id>
<institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5816-8979</contrib-id>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors have contributed equally to this work</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-11-12">
<day>12</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108357</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-07-17">
<day>17</day>
<month>07</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-07-18">
<day>18</day>
<month>07</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.10.27.620480"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Lu et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Lu et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108357-v1.pdf"/>
<abstract>
<p>Motor control is essential for organisms to efficiently interact with the environment by maintaining accurate action and adjusting to future changes. Speech production, one of the most complex motor behaviors, relies on a feedback control process to detect sensory errors and trigger updates in a feedforward control process that implements compensations. However, the specific contributions of these critical processes in sensorimotor learning during continuous vocal production remain debated. Here, we used two experimental designs in five experiments to dissociate these mechanisms.</p>
<p>First, we employed a serial-dependence design with randomized pitch perturbations, dissociating the influences of sensory errors and motor compensation on subsequent vocalizations on a trial-by-trial basis. We found that motor compensation, rather than sensory errors, predicted the compensatory responses in the subsequent trials, suggesting instantaneous serial learning mediated by updates in the feedforward process. This compensation-driven serial learning was generalized across productions of different vowel categories. Second, we further implemented a serial-dependence adaptation design in a sentential context, where auditory perturbation occurred only on a preceding syllable. Any learning effects in its subsequent syllable without pitch perturbation would reflect changes in the speech motor representation. Our results consistently revealed that compensation in the preceding syllable predicted pitch changes in the subsequent syllable, but only when the two adjacent syllables were embedded within a word boundary. Collectively, the study provides ecological-valid evidence supporting that error-based motor compensation, incorporating cognitive and linguistic constraints, directly regulates the speech motor representation and mediates the instantaneous serial learning in successive actions.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>speech motor control</kwd>
<kwd>feedforward</kwd>
<kwd>feedback</kwd>
<kwd>internal forward model</kwd>
<kwd>inverse model</kwd>
<kwd>sensory error</kwd>
<kwd>motor compensation</kwd>
<kwd>sensorimotor learning</kwd>
<kwd>serial dependence</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>32271101</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution>Program of AI-Driven Initiative to Promote Research Paradigm Reform and Empower Disciplinary Advancement by Shanghai Municipal Education Commission (SMEC)</institution>
</institution-wrap>
</funding-source>
</award-group>
<award-group id="funding-3">
<funding-source>
<institution-wrap>
<institution>Program of Introducing Talents of Discipline to Universities</institution>
</institution-wrap>
</funding-source>
<award-id>Base B16018</award-id>
</award-group>
<award-group id="funding-4">
<funding-source>
<institution-wrap>
<institution>NYU Shanghai Boost Fund</institution>
</institution-wrap>
</funding-source>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have significantly revised Introduction and Results.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The accuracy and precision of motor output, as a result of efficient interplay among motor, sensory and cognitive systems, are crucial for human survival and social interactions.<sup><xref ref-type="bibr" rid="c1">1</xref>-<xref ref-type="bibr" rid="c3">3</xref></sup> In the domain of speaking, for example, the control of articulators relies on real-time integration among the language system, speech motor system and sensory feedback from the external environment.<sup><xref ref-type="bibr" rid="c4">4</xref>-<xref ref-type="bibr" rid="c7">7</xref></sup> Specifically, the brain anticipates the sensory consequences of articulatory movements via internal forward models,<sup><xref ref-type="bibr" rid="c8">8</xref>-<xref ref-type="bibr" rid="c13">13</xref></sup> continuously compares the predictions to sensory feedback in the feedback process (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>, left panel).<sup><xref ref-type="bibr" rid="c14">14</xref>-<xref ref-type="bibr" rid="c16">16</xref></sup> Experimentally introduced auditory perturbations (e.g., pitch shifts) create mismatches between expected and heard feedback, effectively serving as sensory errors that trigger the inverse model and update speech motor representation in the feedforward process to correct articulation in real-time (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>, left panel).<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup> Notably, such online compensatory adjustments can propagate to influence subsequent productions<sup><xref ref-type="bibr" rid="c19">19</xref>-<xref ref-type="bibr" rid="c21">21</xref></sup>, reflecting a form of rapid, trial-by-trial sensorimotor learning.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Study design.</title>
<p><bold>A</bold>, Schematics of a processing model of speech production control. Feedback and feedforward processes interactively mediate the control of speech production. When auditory feedback is perturbed and is inconsistent with the predicted consequence from an internal forward model, sensory error is generated and triggers the inverse model in the feedback control. The output of the feedback control further updates the programming of speech-motor representation to compensate for the auditory perturbation. The auditory perturbation and resultant motor compensation could leave traces in either feedback and/or feedforward control processes that mediate sensorimotor learning, even on a trial-by-trial basis. We separately use two paradigms, firstly to test whether sensorimotor learning arises from the changes in the feedback or feedforward processes, and secondly to test whether the learning can be attributed to the updates in the inverse model or in the speech motor representation. <bold>B</bold>, Real-time pitch perturbation during vowel production. The pitch of auditory feedback is artificially shifted (blue) from the utterance recorded in the microphone (orange). The shifted auditory feedback is sent to earphones, which causes compensation in the opposite direction of the pitch shift (the upward change in the orange line of the utterance after the downward pitch shift in the blue line). <bold>C</bold>, The trial sequence of the experiment. Participants undergo 320 trials each day for three consecutive days. The perturbation amounts and directions are randomly applied to each trial.</p></caption>
<graphic xlink:href="620480v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>It remains heavily debated, however, how these serial adjustments contribute to sensorimotor learning via feedback and feedforward control mechanisms.<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c22">22</xref>-<xref ref-type="bibr" rid="c25">25</xref></sup> One possibility is that serial adjustments reflect changes in the feedback control process, whereby sensory errors encountered in a previous utterance recalibrate the sensory system’s sensitivity to subsequent errors.<sup><xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup> Supporting this hypothesis, recent findings show that exposure to unreliable auditory feedback down-weights auditory error signals and leads to reduced compensatory response in subsequent production.<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup> Alternatively, another possibility posits that serial learning reflects updates to the feedforward process, and motor compensation acts not only as an immediate corrective response, but also leaves short-term traces in the speech sound map or articulator map (as proposed in the DIVA model).<sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref></sup> These traces can persist beyond the removal of perturbation, leading to relatively long-lasting adaptive changes that shape subsequent productions even in the absence of feedback perturbation and sensory errors.<sup><xref ref-type="bibr" rid="c20">20</xref></sup></p>
<p>Yet, disentangling the contribution of feedback and feedforward processes to sensorimotor learning remains a major challenge, as sensory errors and motor compensations are tightly coupled. Standard auditory feedback perturbation and adaptation paradigms struggle to differentiate the two because of their consistent co-occurrence across trials.<sup><xref ref-type="bibr" rid="c31">31</xref></sup> To overcome these obstacles, we employed a serial-dependence paradigm that leveraged trial-by-trial variability to dissociate the effects of past sensory error and motor compensation on future speech behavior, allowing us to test feedback and feedforward hypotheses (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>, right panel).<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup> In Experiments 1 and 2 with a serial-dependency perturbation design, if previous sensory errors did not influence compensation in the current trial, this would suggest that the ‘one-shot’ serial learning occurs either in the subsequent inverse model or within the speech motor representation. In Experiment 3 with a serial-dependency adaptation design, perturbation was introduced in the previous trial but not in the current trial; the inverse model would not operate due to the absence of perturbation. If behavioral changes were still observed despite the absence of perturbation, this would indicate that serial learning effects are due to updates in the speech motor representation within the feedforward process. Our results in five experiments consistently show that prior motor compensation, rather than sensory error, predicts subsequent vocal production. The serial learning effects from past perturbed production adapt to the current production without perturbation. These results provide evidence indicating that motor compensation is not only a reflexive response to sensory perturbation, but also drives updates in speech motor representation, forming an instantaneous plasticity under the linguistic constraints during continuous speaking.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Experiment 1: Trial-to-trial learning in repeated production of identical vowel</title>
<p>In the first experiment, we used the simplest setting where participants repeatedly produced a single vowel continuously for 2.5 sec (<italic>N</italic> = 14). When their auditory feedback (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, blue lines) was unexpectedly downward or upward perturbed, subjects could compensate for the shift in speech output (orange lines). Through a long sequence of trials in which six levels of perturbation amount were randomly applied to each trial (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>), we separately measured how the serial changes of the current trial depended on the sensory error and motor compensation of the immediately preceding trial. Throughout the study, we defined each within-trial perturbation and compensation as C<sub>t</sub> and P<sub>t</sub>, and its preceding-1-trial perturbation and compensation as C<sub>t-1</sub> and P<sub>t-1</sub>.</p>
<p>We first measured motor responses to perturbation in the current trial (i.e., the relationship between C<sub>t</sub> and P<sub>t</sub>). We averaged across trials with the same amount of perturbation. It demonstrated that motor compensations were opposed to the direction of pitch perturbation, started around 150 ms after the onset of the perturbation in the conditions of -300 cents, -180 cents, -60 cents, and 60 cents pitch shift (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>, from left to right, respectively, <italic>ts</italic>(13) = 36.13, 56.36, 109.85, and -92.86, <italic>ps</italic> = 0.043, 0.021, 0.008, and 0.024, cluster-based permutation test), yet this effect was not significant in the condition of 180-cent and 300-cent perturbations (<italic>ps</italic> &gt; 0.203). To compare conditions, we selected a 150∼250 ms window that was commonly used to quantify the amount of motor compensation.<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup> Compensation amounts did not significantly differ across upward and downward perturbations (<italic>p</italic> &gt; 0.067, Friedman’s chi-square test, FDR corrected; <xref rid="fig2" ref-type="fig">Fig. 2B</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Serial changes of the immediately preceding trial on the compensatory responses of the current trial.</title>
<p><bold>A</bold>, Pitch traces of motor compensation in response to upward or downward perturbations. The orange lines denote the averaged pitch trace across participants. The red horizontal bars denote the times of the significant effects of compensation (<italic>p</italic> &lt; 0.05). <bold>B</bold>, Temporal averaged compensation in the periods of interest (150-250 ms after the perturbation onset). In <bold>C</bold> and <bold>D</bold>, we plot compensation of the current trial, C<sub>t</sub>, as a function of compensation in the immediately preceding trial, C<sub>t-1</sub>, and as a function of the perturbation in the immediately preceding trial, P<sub>t-1</sub>, respectively. <bold>C</bold>, In the left plots, each dot represents one trial and each black curve represents one participant. The right plots show the relationship between preceding and current trials after averaging across trials and participants. Color dots projecting to the same sign on the x and y axes indicate that C<sub>t</sub> is in the same direction as C<sub>t-1</sub>. <bold>D</bold>, C<sub>t</sub> as a function of P<sub>t-1</sub> does not show significant effects, suggesting the current compensation is not influenced by the perturbation amount of the immediately preceding trial. <bold>E</bold>, The effects of current perturbation (P<sub>t</sub>), preceding perturbation (P<sub>t-1</sub>) and preceding compensation (C<sub>t-1</sub>) on current compensation (C<sub>t</sub>) revealed by a generalized linear mixed-effect model. Regression coefficients for each predictor are extracted from the model. Current perturbation and preceding compensation exert opposite and attractive influences, respectively, on the current compensation. Error bars denote SEM across trials. In <bold>A</bold>-<bold>D</bold>, shaded areas and error bars denote the standard error of mean (SEM) across participants. *<italic>p</italic> &lt; 0.05, **<italic>p</italic> &lt; 0.005.</p></caption>
<graphic xlink:href="620480v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we tested the feedforward and feedback hypotheses by separately measuring the dependency of preceding-1-trial sensory error or motor compensation on its subsequent trial. Specifically, we characterized whether C<sub>t</sub> (in each trial) would follow or oppose C<sub>t-1</sub> or P<sub>t-1</sub> (in its preceding trial). Therefore, we plotted C<sub>t</sub> as a function of C<sub>t-1</sub> and P<sub>t-1</sub>, respectively (<xref rid="fig2" ref-type="fig">Fig. 2CD</xref>). The averaged data across trials (grey dots) and participants (black lines) demonstrated that functions of C<sub>t</sub> to C<sub>t-1</sub> yielded a derivative-of-Gaussian-shaped (DoG) curve (ps = 0.0001 for both, two-sided bootstrap, FDR corrected) (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). That is, C<sub>t</sub> in the current trial followed C<sub>t-1</sub> in the preceding trial, especially around 0; if C<sub>t-1</sub> were approaching the extreme condition (far from 0), the following effect became weak.</p>
<p>We further combined trials with the same P<sub>t</sub> to isolate the dependency of P<sub>t-1</sub> (in the preceding trial) on C<sub>t</sub> (in the current trial). The main effect of P<sub>t-1</sub> was not significant (χ<sup><italic>2</italic></sup>(5) &lt; 6.65, <italic>p</italic> &gt; 0.247; Friedman’s chi-square test, FDR corrected). We further built a linear mixed-effect model to validate the contribution of P<sub>t</sub>, P<sub>t-1</sub>, and C<sub>t-1</sub> to the C<sub>t</sub>. Consistently, the results revealed a significantly negative coefficient of P<sub>t</sub> (same as <xref rid="fig2" ref-type="fig">Fig. 2A</xref>) and a significantly positive coefficient of C<sub>t-1</sub> (same as <xref rid="fig2" ref-type="fig">Fig. 2C</xref>) (<italic>t</italic>(11,680) = -21.8 and 3.03, p = 0.0001 and 0.004, respectively, one-sample <italic>t</italic>-test, FDR corrected; <xref rid="fig2" ref-type="fig">Fig. 2E</xref>). No effect was found for P<sub>t-1</sub> (same as <xref rid="fig2" ref-type="fig">Fig. 2D</xref>) (<italic>t</italic>(11,680) = 1.55, <italic>p</italic> = 0.160; <xref rid="fig2" ref-type="fig">Fig. 2E</xref>). These results suggested that the preceding sensory error only contributed to the instant computation for compensation, but not directly to the future production. It also suggested that updates in the feedforward control process, not the feedback process, regulated subsequent production.</p>
</sec>
<sec id="s2b">
<title>Experiment 2: Trial-to-trial learning across the production of different vowels</title>
<p>Speech production involved variable vowel categories and thus in Experiment 2, we explored whether the serial effect of C<sub>t-1</sub> remained across different vowels. In Experiment 2a, participants (<italic>N</italic> = 17) were asked to produce four different vowels that were randomly presented in each trial, i.e., 啊 (/a1/), 咿 (/i1/), 呃 (/e1/), and 哦 (/o1/). We observed a similar serial effect of C<sub>t-1</sub> on C<sub>t</sub> (<italic>p</italic> = 0.0001, two-sided bootstrap, FDR corrected; <xref rid="fig3" ref-type="fig">Fig. 3A</xref>), but not for the effect of P<sub>t-1</sub> (<xref ref-type="supplementary-material" rid="supp1">Fig. S1</xref>). When comparing experiments 2a and 1, the amplitude of the DoG curve was significantly lower when participants produced different vowels than when they produced the same vowel (p = 0.036, two-sided bootstrap, FDR corrected; <xref rid="fig3" ref-type="fig">Fig. 3C</xref>). The reduction of serial learning can be caused by different vowels or by the uncertainty of next vowel. To exclude the latter possibility, we conducted Experiment 2b where the four vowels were presented in a fixed order; that is, participants would anticipate the identity of the next vowel. We found a consistent serial effect caused by C<sub>t-1</sub> (<italic>N</italic> = 12; <italic>p</italic> = 0.0001, two-sided bootstrap, FDR corrected; <xref rid="fig3" ref-type="fig">Fig. 3B</xref>). Importantly, the amplitude of the DoG curve during fixed-order vowel production was similar to that during random-order (<italic>p</italic> = 0.307, two-sided bootstrap, FDR corrected; <xref rid="fig3" ref-type="fig">Fig. 3C</xref>), suggesting that serial learning of motor compensation was reduced due to different vowels, instead of uncertainty of next production.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Serial effects across different vowel categories.</title>
<p><bold>A</bold> and <bold>B</bold>, C<sub>t</sub> as a function of C<sub>t-1</sub> when participants produce randomly-presented vowels (<bold>A</bold>) and when participants produce expectedly-presented vowels (<bold>B</bold>). In both experiments, significant serial changes to the current motor compensation are observed. <bold>C</bold>, The amount of compensation, characterized by the amplitude of the DoG curve, shows differences between experiments 1, 2a, and 2b, where participants produce the same vowel, different vowels with uncertainty, and different vowels with certainty, respectively. The learning effects are weaker when producing different vowels, compared with those of the same vowels. Error bars denote one SD of the bootstrap distribution. *<italic>p</italic> &lt; 0.05, **<italic>p</italic> &lt; 0.005. See also Figure S1.</p></caption>
<graphic xlink:href="620480v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>Experiment 3: inter-syllabic learning in sentence production within and across word boundary</title>
<p>Experiments 1 and 2 demonstrated that previous motor compensations drove trial-to-trial updates in current speech production, indicating that experience influenced future productions via the feedforward, rather than feedback, control process. However, it remains unclear whether this compensation-driven learning reflects updates in the inverse model or direct modifications to the speech motor representation itself (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). To resolve this ambiguity, we employed an adaptation paradigm in which auditory perturbation was selectively applied to an initial syllable, while leaving the subsequent syllable unperturbed. If trial-to-trial learning relies on updating the inverse model to correct sensory errors, then no adaptive changes should emerge during the unperturbed syllable since the inverse model would not be activated without sensory error.</p>
<p>Alternatively, if previous compensations directly reshape the speech motor representation, adaptive changes should persist in subsequent unperturbed syllables regardless of sensory error.</p>
<p>We manipulated the auditory feedback during the continuous production of sentences at different positions of syntactic structures in two separate experiments (<italic>N</italic> = 15 and 13 in Experiment 3a and 3b, respectively). During sentence production, auditory pitch perturbation was randomly applied to either the second, third, or fourth syllable in each trial (<xref rid="fig4" ref-type="fig">Fig. 4AC</xref>). We quantified compensation in the perturbed syllable and its serial changes in the next syllable, which could be within or across the word boundaries (shown as grey rectangles in insets; <xref rid="fig4" ref-type="fig">Fig. 4BD</xref>). We showed that upward pitch perturbation drove a significant compensatory response around 200 ms after perturbation onset, compared with the pitch trace without perturbation (<italic>t</italic>(14) &lt; -40.30, <italic>p</italic> &lt; 0.009, cluster-based permutation test; <xref ref-type="supplementary-material" rid="supp1">Fig. S2</xref>). We extracted 200-300 ms mean pitch as compensation amount in the perturbed syllable and as serial-learning amount in its next syllable. Only when the two syllables formed a word, the compensation in the preceding syllable influenced the production of the current syllable (right plot in <xref rid="fig4" ref-type="fig">Fig. 4B</xref>: <italic>r</italic> = 0.09, <italic>p</italic> = 0.003; left plot in <xref rid="fig4" ref-type="fig">Fig. 4D</xref>: <italic>r</italic> = 0.22, <italic>p</italic> = 1 × 10<sup>−11</sup>; Spearman correlation).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Inter-syllabic learning persists only within the word boundary.</title>
<p><bold>A</bold> and <bold>C</bold>, Experiment 3a and 3b. Participants are asked to produce three 5-syllable sentences that have the same syntactic structure in each experiment. The position of the two-syllable noun is either at the end (Exp. 3a) or in the middle of the sentence (Exp. 3b). Each character is presented on the screen for 500 ms, followed by a blank for 500 ms. <bold>B</bold> and <bold>D</bold>, Relations between the compensation amount of the preceding syllable (x-axis) and serial-learning amount of the current syllable (y-axis) in Exp. 3a and 3b. The mean pitch averaged across 200-300 ms is separately extracted from the perturbed syllable (black dots in the insert plots) and the subsequent syllable (the dot pointed by a black arrow). The grey-shaped rectangle denotes the two-syllabic word. Only when the two syllables are within a word boundary, significant inter-syllabic learning is observed. **p &lt; 0.005. See also Figures S2 and S3.</p></caption>
<graphic xlink:href="620480v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The correlation coefficients were larger in the within-word-boundary condition than in the across-word-boundary condition (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>: <italic>p</italic> = 0.180 and 0.026 for position 2 vs 4 and for position 3 vs 4, respectively; <xref rid="fig4" ref-type="fig">Fig. 4D</xref>: <italic>p</italic> = 0.010 and 0.0005 for position 2 vs 3 and for position 2 vs 4, respectively; one-sided bootstrap, FDR corrected). In contrast, auditory perturbation in the preceding syllable did not contribute to inter-syllabic learning (<xref ref-type="supplementary-material" rid="supp1">Fig. S3</xref>). These results strongly suggested that compensation-driven learning persisted in the current vocal production even without any perturbation and compensation, and the serial learning was constrained by word boundary. It also suggested that past compensations directly modified speech motor representations for the instantaneous, trial-to-trial learning.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We investigated how previous sensory error and motor compensation regulated trial-to-trial learning. We showed that motor compensation in the preceding trial influenced speech production in the next trial, whereas sensory errors did not have any effects.</p>
<p>Crucially, this compensation-driven serial learning was modulated by linguistic contexts -- persisting across different vowels in a string of production and across vowels embedded within the word boundary. These results suggested that instantaneous sensorimotor learning was driven by motor compensation and was constrained by the linguistic system in continuous speech.</p>
<p>The study introduces a serial-dependence paradigm in the domain of speech production that dissociates the influence of previous sensory error and motor compensation on the current vocal production. We observed that when the compensation in trial t-1 was around zero -- despite the presence of sensory error -- the compensation in trial t also returned to baseline. Whereas, when the compensation in trial t-1 was positive or negative, compensation in trial t followed the same direction. These observations are consistent in all three experiments (<xref rid="fig2" ref-type="fig">Figs. 2C</xref>, <xref ref-type="fig" rid="fig3">3AB</xref>), and can even be found in individual subjects. Moreover, the compensation-driven serial learning was also observed in the production of syllables without feedback perturbation (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). This evidence suggests that instantaneous plasticity in speech production emerges primarily through immediate updates in the feedforward control process, initiated by preceding compensatory responses. Using a novel serial dependency paradigm, this study extended sensorimotor learning in a relatively longer time scale<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup> to trial-by-trial instantaneous plasticity.</p>
<p>The distinction between sensory error and motor correction on driving adaptation has been discussed in the motor control literature. For example, studies on reaching have shown that behavioral adaptation occurs even when online motor corrections are unavailable, suggesting that sensory errors suffice to update subsequent movements.<sup><xref ref-type="bibr" rid="c37">37</xref></sup> Speaking, however, differs from limb movements in many aspects, including the automatic and involuntary nature of sensorimotor mappings,<sup><xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup> as well as its mandatory interplay with higher-level linguistic processing. It is possible that speech corrective signals are generated in response to sensory errors in the brain and drive adaptive changes in subsequent productions, regardless of overt compensatory behavior.</p>
<p>Moreover, the storage-based hierarchical yet fixed mappings between the linguistic and motor systems may reinforce the feedforward process during control and learning in speaking, as demonstrated in the results of Exp. 3. Rather than removing compensatory behavior entirely, the current study directly tests whether the amount of compensation in one trial influences the next, providing new insight into the trial-by-trial dynamics of sensorimotor learning in speech.</p>
<p>The relationship between within-trial compensation and next-trial learning is not linear; it is more likely a DoG-shaped curve. Specifically, if the previous compensation is large, the serial changes in the current trial become smaller, suggesting a dynamic adjustment process during speech planning.<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup> Furthermore, this process is also constrained by the similarity of production targets between trials. If the target in the next trial changes to a different vowel, the trial-to-trial learning becomes weaker (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>). Rochet-Capellan et al. <sup><xref ref-type="bibr" rid="c41">41</xref></sup> tested whether sensorimotor learning during training transfers to new utterances with different sounds and demonstrated that it is highly dependent on the specific utterances. That is, the experience in the motor domain becomes less useful when the next utterance is different.</p>
<p>Lastly, the extension of compensation-driven serial learning into sentence production reveals that serial learning only exists within word boundary (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). In Experiment 3, participants may simply read out aloud each visually presented word without syntactic processing. If this were true, inter-syllabic learning, as observed in Experiments 1 and 2, would occur between every two syllables. However, the observed significant effects of syntactic structures are inconsistent with the alternative ‘reading-only’ hypothesis.</p>
<p>Moreover, the extensive training on the three 5-syllabic sentences ensured that participants were extremely familiar with the materials and retrieved the simple sentential structure at the visual presentation of the first word. All the experimental settings and positive results support the hypothesis regarding the role of syntactic constraints in compensation-driven serial learning.</p>
<p>The results of within-word-boundary inter-syllabic learning suggest that words serve as a central unit in speech planning. Motor compensation in the preceding syllable can be updated to the next within-word syllable. However, any compensation-based experience is not available for the syllables across words. Psycholinguistic studies, such as those by Levelt <sup><xref ref-type="bibr" rid="c42">42</xref></sup>, propose that during speech production, words are retrieved from the mental lexicon as a whole unit, followed by phonological encoding into syllabic and phonetic representations. The entire course of speech production at the linguistic level would interact with sensorimotor processes via internal forward models and regulate feedforward processes at both featural and temporal domains,<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c43">43</xref>-<xref ref-type="bibr" rid="c47">47</xref></sup> to achieve efficient online control. The observed serial effects within word structure suggest that the programming and regulation of speech production are presumably in the size of the encoding unit within the word structure, which is consistent with the hypothesis of interaction between linguistic and sensorimotor systems in speech production control.<sup><xref ref-type="bibr" rid="c4">4</xref></sup></p>
<p>The study showed that previous motor compensation can modulate the subsequent vocal pitch production; whether this compensation-driven serial learning can be generalized to other speech domains, such as formants<sup><xref ref-type="bibr" rid="c19">19</xref></sup> and intensity<sup><xref ref-type="bibr" rid="c11">11</xref></sup> that are controlled by distinct vocal apparatus and may be mediated by potentially different mechanisms, requires further investigation. Considering a broader linguistic context in speech motor control is challenging and relevant studies are rare.<sup><xref ref-type="bibr" rid="c48">48</xref>,<xref ref-type="bibr" rid="c49">49</xref></sup> In this study, we used pitch as a representative dimension and carefully selected different combinations of consonants and vowels in continuous speech to pioneer the investigation. Future studies should be carried out with caution and careful controls. For example, pitch contours in continuous speech can be potentially interfered with by consonantal perturbations<sup><xref ref-type="bibr" rid="c50">50</xref></sup> or sentence intonation<sup><xref ref-type="bibr" rid="c51">51</xref></sup>. During the production of non-sonorant consonants, the vibration of vocal folds will be affected, leading to a break and a carry-over rising or falling in the pitch contours.<sup><xref ref-type="bibr" rid="c52">52</xref></sup></p>
<p>In summary, by implementing a novel serial-dependence paradigm in the domain of speaking, we found distinct functions of sensory error and motor compensation in sensorimotor learning in continuous speech. Motor compensation drives updates in the feedforward control process and regulates trial-to-trial learning that was constrained by linguistic contexts, while sensory error only contributed to the instant computation of compensation. These results suggest that the history of motor processes significantly contributes to sensorimotor learning and control. Systematic interplay among cognitive, motor, and sensory systems regulates behaviors.</p>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<sec id="s4a">
<title>Participant</title>
<p>In five experiments, i.e., Experiments 1, 2a, 2b, 3a, and 3b, we recruited 14 participants (9 females, 18-27 years old, mean 21 years old), 17 participants (11 females, 19-32 years old, mean 23 years old), 12 participants (8 females, 20-27 years old, mean 22 years old), 15 participants (9 females, 19-25 years old, mean 22 years old), and 13 participants (7 females, 18-26 years old, mean 21 years old), respectively. They were college students with self-reported normal hearing and speech. All participants were right-handed and were native Chinese speakers. Each volunteer only participated in one experiment. Written informed consent was obtained from each participant before the experiment. The experimental procedures were approved by the Research Ethics Committee at the East China Normal University and New York University Shanghai.</p>
</sec>
<sec id="s4b">
<title>Procedure in Experiment 1</title>
<p>Participants were asked to produce the vowel /a/ in a total of 960 trials over three days. Each trial began with a white fixation cross, displayed at the center of the screen for a randomized duration of 0.8–1.2 s (uniform distribution), signaling participants to be prepared for vocalization. When the cross turned green, participants were instructed to vocalize no more than 500 ms and sustain the /a/ sound with a steady and natural tone for 2.5 s until the cross disappeared. A pitch perturbation for a duration of 500 ms was randomly introduced 0.8–1.2 s after vocal onset. The direction (i.e., upward or downward) and magnitude (i.e., 60, 180, or 300 cents) of the pitch shift were randomly assigned across the 960 trials for each participant. At the end of each trial, participants received visual feedback: “Good,” “Too slow,” or “Too short.” Participants completed 95.86% ± 0.02% (s.d.) of trials that met criteria. Each trial was followed by a silent interval with a random duration of 1–2 s (uniform distribution) before the next trial began.</p>
<p>Participants completed 320 trials in 10 blocks each day, with self-paced rest periods of at least 1 minute between blocks. At the beginning of each session, participants underwent a training block of 10 trials. The mean pitch from these trials was used to define the pitch range in the Audapter toolbox (see <italic>Auditory feedback perturbation</italic> below).</p>
</sec>
<sec id="s4c">
<title>Procedure in Experiment 2a</title>
<p>The procedure for Experiment 2a was identical to Experiment 1, except participants were required to produce four different vowels without semantic meaning—啊 (/a1/), 咿(/i1/), 呃 (/e1/), and 哦(/o1/)—randomly assigned in each trial. A white letter corresponding to the syllable was presented at the center of the screen, and upon turning green, participants were instructed to vocalize a vowel. Participants completed 94.82% ± 0.11% of trials that met the criteria.</p>
</sec>
<sec id="s4d">
<title>Procedure in Experiment 2b</title>
<p>The procedure for Experiment 2b was identical to Experiment 2a, except the order of four vowels were presented in a fixed order, i.e., /i1/-/a1/-/e1/-/o1/ in every four trials so that participants would not anticipate the next vowel production. Participants completed 97.55% ± 0.03 of trials that met the criteria.</p>
</sec>
<sec id="s4e">
<title>Procedure in Experiment 3a</title>
<p>The procedure for Experiment 3a followed that of Experiment 1, with two differences. First, participants completed 300 trials in a single day. Second, they were required to produce sentences with the same syntactic structure rather than isolated vowels. In each trial, one of three five-syllable sentences (i.e., ▪▪▪猫咪 [ma1 ma1 lu1 mao1 mi1], 丫邀巫医 [ya1 ya1 yao1 wu1 yi1], 依依摸▪▪ [yi1 yi1 mo1 wu1 ya1]) was randomly selected (uniform distribution), with each syllable appearing sequentially at the center of the screen for 500 ms, followed by a 500 ms blank screen. The whole duration for a sentence production was 5 s. During vocalization, a whole-utterance pitch perturbation (upward or downward perturbation at 60, 180, 300 cents, randomly assigned) was applied to the second, third, or fourth word during vocalization (uniformed distribution). In 20% of the sentences, no perturbation was applied. The speech materials and task were carefully chosen to avoid the effect of consonantal obstruction<sup><xref ref-type="bibr" rid="c53">53</xref></sup> and the effect of articulators’ dynamics on pitch contour, respectively. Participants completed 94.44% ± 0.07% of trials that met the criteria.</p>
<p>Before the main experiment, participants completed a training session in which they produced each of the three sentences four times. They were explicitly instructed on the content and syntactic structure of the sentences. Additionally, they were instructed to vocalize each syllable within 500 ms of its appearance and to avoid coarticulation between words. Participants could repeat the training session upon request.</p>
</sec>
<sec id="s4f">
<title>Procedure in Experiment 3b</title>
<p>The procedure for Experiment 3b was the same as Experiment 3a, except that the three sentences were replaced with another three with a different syntactic structure, i.e., 邀依琳摸 ▪ [yao1 yi1 yi1 mo1 ying1], 拉▪▪腌▪ [la1 ma1 ma1 yan1 ya1], and ▪丫丫溜猫 [yue1 ya1 ya1 liu1 mao1], where the two-syllable noun was in the position of second and third character in the sentences. Participants completed 97.36% ± 0.03% of trials that met the criteria.</p>
</sec>
<sec id="s4g">
<title>Auditory feedback perturbation</title>
<p>Participants’ speech was recorded by a Shure beta 58A microphone that was placed 5 centimeters away from their mouths. The acoustic signals from the microphone were pre-amplified and then provided to an audio interface (MOTU Microbook IIc), collected at a 48k Hz sampling rate. The recorded speech was real-time manipulated (i.e., perturbation) in the audio interface, and the processed signal was split and sent both to the computer and to the headphones. The speech processing in the audio interface was performed using the Audapter toolbox.<sup><xref ref-type="bibr" rid="c54">54</xref></sup> The pitch of speech in each trial was shifted using the time-domain method. A 10-ms linear ramp was added at boundaries to attenuate possible abrupt jumps in the speech. A priori pitch range was set as +/-50 Hz relative to the mean pitch of a vocal /a/ for each participant. The rest of the parameters were set as default including algorithm, frame length, and cepstral liftering.</p>
</sec>
<sec id="s4h">
<title>Acoustic analysis</title>
<p>In all experiments, we analyzed signalIn, the speech output from participants. In experiments 1 and 2, we first removed silence from the recordings of produced vowels in each trial using the <italic>vad</italic> function. Visual inspection was performed to make sure noise did not affect the voice activity detection. Pitch tracks (at a 100-Hz sampling rate) were measured using the Praat-based script ProsodyPro.<sup><xref ref-type="bibr" rid="c55">55</xref></sup> Default parameters were used. Perturbation onset in the pitch trace was obtained by a customized automatic algorithm combined with manual inspection. To convert hertz to cent scale, we defined baseline pitch by taking the mean of pitch trace in the -400 to -200 ms before perturbation onset. The pitch trace was transformed according to the formula: <inline-formula><inline-graphic xlink:href="620480v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. A trial rejection was performed if the pitch trace was varied beyond its perturbation amount minus 100 cents (for downward-perturbation trials) or plus 100 cents (for upward-perturbation trials). To extract perturbation and compensation amounts for DoG-curve fitting and linear mixed effect model, the pitch trace in the 50-80 ms and the 150-250 ms windows were averaged, respectively, for each trial.<sup><xref ref-type="bibr" rid="c21">21</xref></sup> Compensation amounts for all trials and all participants were corrected by subtracting the grand average. This correction ensures that the DoG-curve fitting starts from zero, allowing its amplitude to be accurately measured and compared across experiments.</p>
<p>In Experiment 3, we applied the same pipeline except that only the perturbed syllable and its subsequent syllable were extracted to measure their pitch trace. Based on the visual inspection of pitch contours (<xref ref-type="supplementary-material" rid="supp1">Fig. S2</xref>), Mandarin tone 1 produced in isolation was characterized by a high-level tone and showed a pitch contour that typically involved a slight rising intonation at the onset, lasting around 0-150 ms, before stabilizing into a steady high pitch.<sup><xref ref-type="bibr" rid="c56">56</xref></sup> Therefore, we calculated the compensation amount in the perturbed syllable and serial-learning amount in the following syllable using mean pitch in the 200-250 ms time window, with baseline pitch in the 150-200 ms time window.</p>
</sec>
<sec id="s4i">
<title>Statistical analysis of compensatory responses</title>
<p>For participant-level analysis, trials with the same perturbation condition were averaged, regardless of vowel categories in Experiments 2 and 3. To assess whether there was a compensatory response to perturbation, a two-sided cluster-based permutation test<sup><xref ref-type="bibr" rid="c57">57</xref></sup> was used to test averaged pitch traces against perturbation amount or between perturbed and normal conditions (<xref rid="fig1" ref-type="fig">Fig. 1A</xref> and <bold>S2</bold>). A one-sided one-sample t-test was used to test motor compensation extracted from time of interests against 0 (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>).</p>
</sec>
<sec id="s4j">
<title>Statistical analysis of trial-to-trial learning effect</title>
<p>To explore the serial effect of preceding-1-trial motor compensation amount on the current motor response, C<sub>t</sub> was expressed as a function of C<sub>t-1</sub>. For positive values of this C<sub>t</sub> or C<sub>t-1</sub>, participants compensated upward in response to perturbation in the current or preceding-1-trial. Consequently, data points in the scatter plot (<xref rid="fig2" ref-type="fig">Fig. 2C</xref> and <bold>3AB</bold>) that had x- and y-values in the same sign indicated that C<sub>t</sub> was in the direction of C<sub>t-1</sub>. We assess the trial-to-trial learning effect by polling the compensation amount of all trials and fitting the DoG-shape curve, a well-found psychophysical function in previous serial-dependence studies.<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup> The DoG was given as <inline-formula><inline-graphic xlink:href="620480v4_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>a</italic> was the curve amplitude, <italic>w</italic> was the curve width, and <italic>c</italic> was the constant <inline-formula><inline-graphic xlink:href="620480v4_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. To assess the serial effect of preceding motor compensation, <italic>x</italic> was C<sub>t-1</sub>, and width parameter <italic>w</italic> of the curve was constrained to a range of plausible values (<italic>w</italic> = 0.019-0.071, corresponding to the curve peak between 10 and 36 perturbation pitch difference). The model fitting used the function <italic>fmincon</italic> with default parameters. We employed a bias-corrected and accelerated bootstrap to evaluate the significance of the DoG models and to compare DoG models between conditions. We applied a fitted DoG model to the pooled group data and generated a permutation distribution for the DoG-curve amplitude parameter.<sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup> This permutation process was repeated 2,000 times. For a one-sided comparison of curve amplitude, if the data population in one condition was greater than A% of the data population in the other condition, the significance level was (100A + 1)/2,001. For a two-sided comparison, the significance level was (200A + 1)/2,001. A false discovery rate (FDR) correction was applied.</p>
<p>To explore the serial effect of sensory error on C<sub>t</sub>, C<sub>t</sub> was expressed as a function of P<sub>t-1</sub>. We use a Friedman’s chi-square test for multiple condition comparison, i.e., controlling the current perturbation as the same to explore the effect of preceding perturbation on the current compensation (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>). Spearman correlation was used to find the relation between C<sub>t-1</sub>/P<sub>t-1</sub> and inter-syllabic learning within and across syntactic boundaries (<xref rid="fig4" ref-type="fig">Fig. 4BD</xref> and <bold>S3</bold>). To compare the correlation coefficient in within- and across-boundary conditions, a bias-corrected and accelerated bootstrap was used.<sup><xref ref-type="bibr" rid="c60">60</xref></sup></p>
</sec>
<sec id="s4k">
<title>Generalized linear mixed-effect model</title>
<p>To quantitatively test the relations of C<sub>t</sub> to P<sub>t</sub>, P<sub>t-1</sub> and C<sub>t-1</sub>, we built a generalized linear mixed-effect models (<xref rid="fig2" ref-type="fig">Fig. 2E</xref> and <bold>S1</bold>). The model assumed that P<sub>t</sub>, P<sub>t-1</sub> and C<sub>t-1</sub> together influenced C<sub>t</sub>. The function ‘fitglme’ was performed and accounted for group-level fixed effects and random effects because of vocal pitch variation across participants. The model was specified as <italic>C</italic><sub><italic>t ij</italic></sub>,= <italic>f</italic> (<italic>β</italic><sub>0<italic>j</italic></sub> + <italic>β</italic><sub>1<italic>j</italic></sub><italic>P</italic><sub><italic>t ij</italic></sub> + <italic>β</italic><sub>2<italic>j</italic></sub><italic>P</italic><sub><italic>t</italic>−1<italic>ij</italic></sub> + <italic>β</italic><sub>3<italic>j</italic></sub><italic>C</italic><sub><italic>t</italic>−1<italic>ij</italic></sub>+1|<italic>subjects</italic>), where <italic>i</italic> indicated participant index and <italic>j</italic> indicated <italic>j</italic><sup>th</sup> trial. The regression coefficient of each predictor, i.e., <italic>β</italic>, was accessed by the <italic>t</italic>-statistics against the null hypothesis test that the coefficient was equal to 0. FDR correction was applied.</p>
</sec>
</sec>

</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data and code availability</title>
<p>All data and code that support the findings of this study is available in Mendeley Data: 10.17632/pybdtwxvzz.2.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Yuan Xie and Bao Hong for their thoughtful comments on previous versions of the manuscript, Tingting Wang and Huixuan Liu for assisting in collecting data. This study is supported by the National Natural Science Foundation of China 32271101, Program of AI-Driven Initiative to Promote Research Paradigm Reform and Empower Disciplinary Advancement by Shanghai Municipal Education Commission (SMEC), Program of Introducing Talents of Discipline to Universities, Base B16018, and NYU Shanghai Boost Fund to X.T.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Author contributions</title>
<p>Y.L., X.T., and J.C. designed the experiment. Y.L., W.T., and</p>
<p>Z.X. collected the data, Y.L., Z.X., A.X., and W.T., performed the analyses, Y.L., and X.T. drafted, reviewed, and corrected the manuscript. X.T. and J.C. supervised the project.</p>
</sec>
<sec id="s8">
<title>Declaration of generative AI and AI-assisted technologies in the writing process</title>
<p>During the preparation of this work, the authors used ChatGPT to proofread texts. After using this tool or service, the authors reviewed and edited the content as needed and took full responsibility for the content of the publication.</p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary figures</label>
<media xlink:href="supplements/620480_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rosenbaum</surname>, <given-names>D.A.</given-names></string-name></person-group> (<year>2009</year>). <source>Human motor control</source> (<publisher-name>Academic press</publisher-name>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ivanenko</surname>, <given-names>Y.P.</given-names></string-name>, <string-name><surname>Poppele</surname>, <given-names>R.E.</given-names></string-name>, and <string-name><surname>Lacquaniti</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Motor control programs and walking</article-title>. <source>Neuroscientist</source> <volume>12</volume>, <fpage>339</fpage>–<lpage>348</lpage>. <pub-id pub-id-type="doi">10.1177/1073858406287987</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olivier</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Davare</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Andres</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Fadiga</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Precision grasping in humans: from motor control to cognition</article-title>. <source>Curr Opin Neurobiol</source> <volume>17</volume>, <fpage>644</fpage>–<lpage>648</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2008.01.008</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Computational neuroanatomy of speech production</article-title>. <source>Nat Rev Neurosci</source> <volume>13</volume>, <fpage>135</fpage>–<lpage>145</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3158</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Houde</surname>, <given-names>J.F.</given-names></string-name>, and <string-name><surname>Nagarajan</surname>, <given-names>S.S.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Speech production as state feedback control</article-title>. <source>Front Hum Neurosci</source> <volume>5</volume>, <fpage>82</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2011.00082</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Guenther</surname>, <given-names>F.H.</given-names></string-name></person-group> (<year>2016</year>). <source>Neural control of speech</source> (<publisher-name>Mit Press</publisher-name>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name>, and <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Mental imagery of speech and movement implicates the dynamics of internal forward models</article-title>. <source>Front Psychol</source> <volume>1</volume>, <fpage>166</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2010.00166</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kawato</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Internal models for motor control and trajectory planning</article-title>. <source>Curr Opin Neurobiol</source> <volume>9</volume>, <fpage>718</fpage>–<lpage>727</lpage>. <pub-id pub-id-type="doi">10.1016/s0959-4388(99)00028-8</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolpert</surname>, <given-names>D.M.</given-names></string-name>, and <string-name><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Computational principles of movement neuroscience</article-title>. <source>Nat Neurosci</source> <volume>3</volume> <issue>Suppl</issue>, <fpage>1212</fpage>-<lpage>1217</lpage>. <pub-id pub-id-type="doi">10.1038/81497</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schubotz</surname>, <given-names>R.I.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Prediction of external events with our motor system: towards a new framework</article-title>. <source>Trends in Cognitive Sciences</source> <volume>11</volume>, <fpage>211</fpage>–<lpage>218</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2007.02.006</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Teng</surname>, <given-names>X.B.</given-names></string-name>, <string-name><surname>Bai</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Imagined speech influences perceived loudness of sound</article-title>. <source>Nature Human Behaviour</source> <volume>2</volume>, <fpage>225</fpage>–<lpage>234</lpage>. <pub-id pub-id-type="doi">10.1038/s41562-018-0305-8</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Corollary Discharge Versus Efference Copy: Distinct Neural Signals in Speech Preparation Differentially Modulate Auditory Responses</article-title>. <source>Cereb Cortex</source> <volume>30</volume>, <fpage>5806</fpage>–<lpage>5820</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhaa154</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Mental operations in rhythm: Motor-to-sensory transformation mediates imagined singing</article-title>. <source>PLoS Biol</source> <volume>18</volume>, <fpage>e3000504</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000504</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eliades</surname>, <given-names>S.J.</given-names></string-name>, and <string-name><surname>Tsunada</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Auditory cortical activity drives feedback-dependent vocal control in marmosets</article-title>. <source>Nat Commun</source> <volume>9</volume>, <fpage>2540</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-04961-8</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niziolek</surname>, <given-names>C.A.</given-names></string-name>, <string-name><surname>Nagarajan</surname>, <given-names>S.S.</given-names></string-name>, and <string-name><surname>Houde</surname>, <given-names>J.F.</given-names></string-name></person-group> (<year>2013</year>). <article-title>What does motor efference copy represent? Evidence from speech production</article-title>. <source>J Neurosci</source> <volume>33</volume>, <fpage>16110</fpage>–<lpage>16116</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2137-13.2013</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>E.F.</given-names></string-name>, <string-name><surname>Niziolek</surname>, <given-names>C.A.</given-names></string-name>, <string-name><surname>Knight</surname>, <given-names>R.T.</given-names></string-name>, <string-name><surname>Nagarajan</surname>, <given-names>S.S.</given-names></string-name>, and <string-name><surname>Houde</surname>, <given-names>J.F.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Human cortical sensorimotor network underlying feedback control of vocal pitch</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>110</volume>, <fpage>2653</fpage>–<lpage>2658</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1216827110</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lametti</surname>, <given-names>D.R.</given-names></string-name>, <string-name><surname>Nasir</surname>, <given-names>S.M.</given-names></string-name>, and <string-name><surname>Ostry</surname>, <given-names>D.J.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Sensory preference in speech production revealed by simultaneous alteration of auditory and somatosensory feedback</article-title>. <source>J Neurosci</source> <volume>32</volume>, <fpage>9351</fpage>–<lpage>9358</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0404-12.2012</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Larson</surname>, <given-names>C.R.</given-names></string-name>, <string-name><surname>Altman</surname>, <given-names>K.W.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Hain</surname>, <given-names>T.C.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Interactions between auditory and somatosensory feedback for voice F0 control</article-title>. <source>Exp Brain Res</source> <volume>187</volume>, <fpage>613</fpage>–<lpage>621</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-008-1330-z</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>D.L.</given-names></string-name>, <string-name><surname>Parrell</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Niziolek</surname>, <given-names>C.A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Movement variability can be modulated in speech production</article-title>. <source>J Neurophysiol</source> <volume>128</volume>, <fpage>1469</fpage>–<lpage>1482</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00095.2022</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Houde</surname>, <given-names>J.F.</given-names></string-name>, and <string-name><surname>Jordan</surname>, <given-names>M.I.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Sensorimotor adaptation in speech production</article-title>. <source>Science</source> <volume>279</volume>, <fpage>1213</fpage>–<lpage>1216</lpage>. <pub-id pub-id-type="doi">10.1126/science.279.5354.1213</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hantzsch</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Parrell</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Niziolek</surname>, <given-names>C.A.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A single exposure to altered auditory feedback causes observable sensorimotor adaptation in speech</article-title>. <source>eLife</source> <volume>11</volume>. <pub-id pub-id-type="doi">10.7554/eLife.73694</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tourville</surname>, <given-names>J.A.</given-names></string-name>, and <string-name><surname>Guenther</surname>, <given-names>F.H.</given-names></string-name></person-group> (<year>2011</year>). <article-title>The DIVA model: A neural theory of speech acquisition and production</article-title>. <source>Lang Cogn Process</source> <volume>26</volume>, <fpage>952</fpage>–<lpage>981</lpage>. <pub-id pub-id-type="doi">10.1080/01690960903498424</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parrell</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Houde</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Modeling the Role of Sensory Feedback in Speech Motor Control and Learning</article-title>. <source>J Speech Lang Hear Res</source> <volume>62</volume>, <fpage>2963</fpage>–<lpage>2985</lpage>. <pub-id pub-id-type="doi">10.1044/2019_JSLHR-S-CSMC7-18-0127</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>K.S.</given-names></string-name>, <string-name><surname>Gaines</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Parrell</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ramanarayanan</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Nagarajan</surname>, <given-names>S.S.</given-names></string-name>, and <string-name><surname>Houde</surname>, <given-names>J.F.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Mechanisms of sensorimotor adaptation in a hierarchical state feedback control model of speech</article-title>. <source>PLoS Comput Biol</source> <volume>19</volume>, <fpage>e1011244</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1011244</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manes</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Bullock</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Meier</surname>, <given-names>A.M.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>R.S.</given-names></string-name>, <string-name><surname>Richardson</surname>, <given-names>R.M.</given-names></string-name>, and <string-name><surname>Guenther</surname>, <given-names>F.H.</given-names></string-name></person-group> (<year>2024</year>). <article-title>A neurocomputational view of the effects of Parkinson’s disease on speech production</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>18</volume>. <pub-id pub-id-type="doi">10.3389/fnhum.2024.1383714</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname>, <given-names>D.-l.</given-names></string-name>, <string-name><surname>Parrell</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Beach</surname>, <given-names>S.D.</given-names></string-name>, and <string-name><surname>Niziolek</surname>, <given-names>C.A.</given-names></string-name></person-group> (<year>2025</year>). <article-title>The Brain’s Sensitivity to Sensory Error Can Be Modulated by Altering Perceived Variability</article-title>. <source>The Journal of Neuroscience</source> <volume>45</volume>, <fpage>e0024242024</fpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.0024-24.2024</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sitek</surname>, <given-names>K.R.</given-names></string-name>, <string-name><surname>Mathalon</surname>, <given-names>D.H.</given-names></string-name>, <string-name><surname>Roach</surname>, <given-names>B.J.</given-names></string-name>, <string-name><surname>Houde</surname>, <given-names>J.F.</given-names></string-name>, <string-name><surname>Niziolek</surname>, <given-names>C.A.</given-names></string-name>, and <string-name><surname>Ford</surname>, <given-names>J.M.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Auditory cortex processes variation in our own speech</article-title>. <source>PLoS One</source> <volume>8</volume>, <fpage>e82925</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0082925</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niziolek</surname>, <given-names>C.A.</given-names></string-name>, and <string-name><surname>Parrell</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Responses to Auditory Feedback Manipulations in Speech May Be Affected by Previous Exposure to Auditory Errors</article-title>. <source>J Speech Lang Hear Res</source> <volume>64</volume>, <fpage>2169</fpage>–<lpage>2181</lpage>. <pub-id pub-id-type="doi">10.1044/2020_JSLHR-20-00263</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kawato</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Furukawa</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Suzuki</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1987</year>). <article-title>A hierarchical neural-network model for control and learning of voluntary movement</article-title>. <source>Biol Cybern</source> <volume>57</volume>, <fpage>169</fpage>–<lpage>185</lpage>. <pub-id pub-id-type="doi">10.1007/BF00364149</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guenther</surname>, <given-names>F.H.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Cortical interactions underlying the production of speech sounds</article-title>. <source>J Commun Disord</source> <volume>39</volume>, <fpage>350</fpage>–<lpage>365</lpage>. <pub-id pub-id-type="doi">10.1016/j.jcomdis.2006.06.013</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raharjo</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kothare</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Nagarajan</surname>, <given-names>S.S.</given-names></string-name>, and <string-name><surname>Houde</surname>, <given-names>J.F.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Speech compensation responses and sensorimotor adaptation to formant feedback perturbations</article-title>. <source>J Acoust Soc Am</source> <volume>149</volume>, <fpage>1147</fpage>. <pub-id pub-id-type="doi">10.1121/10.0003440</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischer</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Whitney</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Serial dependence in visual perception</article-title>. <source>Nat Neurosci</source> <volume>17</volume>, <fpage>738</fpage>–<lpage>743</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3689</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cicchini</surname>, <given-names>G.M.</given-names></string-name>, <string-name><surname>Mikellidou</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Burr</surname>, <given-names>D.C.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Serial Dependence in Perception</article-title>. <source>Annu Rev Psychol</source> <volume>75</volume>, <fpage>129</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-psych-021523-104939</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burnett</surname>, <given-names>T.A.</given-names></string-name>, <string-name><surname>Freedland</surname>, <given-names>M.B.</given-names></string-name>, <string-name><surname>Larson</surname>, <given-names>C.R.</given-names></string-name>, and <string-name><surname>Hain</surname>, <given-names>T.C.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Voice F0 responses to manipulations in pitch feedback</article-title>. <source>J Acoust Soc Am</source> <volume>103</volume>, <fpage>3153</fpage>–<lpage>3161</lpage>. <pub-id pub-id-type="doi">10.1121/1.423073</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fritsche</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Spaak</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>de Lange</surname>, <given-names>F.P.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A Bayesian and efficient observer model explains concurrent attractive and repulsive history biases in visual perception</article-title>. <source>eLife</source> <volume>9</volume>. <pub-id pub-id-type="doi">10.7554/eLife.55389</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liberman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fischer</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Whitney</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Serial dependence in the perception of faces</article-title>. <source>Curr Biol</source> <volume>24</volume>, <fpage>2569</fpage>–<lpage>2574</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2014.09.025</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tseng</surname>, <given-names>Y.W.</given-names></string-name>, <string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Krakauer</surname>, <given-names>J.W.</given-names></string-name>, <string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Bastian</surname>, <given-names>A.J.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Sensory prediction errors drive cerebellum-dependent adaptation of reaching</article-title>. <source>J Neurophysiol</source> <volume>98</volume>, <fpage>54</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00266.2007</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Assaneo</surname>, <given-names>M.F.</given-names></string-name>, <string-name><surname>Ripolles</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Orpella</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>W.M.</given-names></string-name>, <string-name><surname>de Diego-Balaguer</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Spontaneous synchronization to speech reveals neural mechanisms facilitating language learning</article-title>. <source>Nat Neurosci</source> <volume>22</volume>, <fpage>627</fpage>–<lpage>632</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-019-0353-z</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krakauer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Naber</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Niziolek</surname>, <given-names>C.A.</given-names></string-name>, and <string-name><surname>Parrell</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Divided Attention Has Limited Effects on Speech Sensorimotor Control</article-title>. <source>Journal of Speech, Language, and Hearing Research</source> <volume>67</volume>, <fpage>4358</fpage>–<lpage>4368</lpage>. doi:<pub-id pub-id-type="doi">10.1044/2024_JSLHR-24-00098</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Houde</surname>, <given-names>J.F.</given-names></string-name>, and <string-name><surname>Chang</surname>, <given-names>E.F.</given-names></string-name></person-group> (<year>2015</year>). <article-title>The cortical computations underlying feedback control in vocal production</article-title>. <source>Curr Opin Neurobiol</source> <volume>33</volume>, <fpage>174</fpage>–<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2015.04.006</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rochet-Capellan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Richer</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Ostry</surname>, <given-names>D.J.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Nonhomogeneous transfer reveals specificity in speech motor learning</article-title>. <source>J Neurophysiol</source> <volume>107</volume>, <fpage>1711</fpage>–<lpage>1717</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00773.2011</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Levelt</surname>, <given-names>W.J.M.</given-names></string-name></person-group> (<year>1989</year>). <source>Speaking</source> (<publisher-name>MIT press</publisher-name>). <pub-id pub-id-type="doi">10.7551/mitpress/6393.001.0001</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name>, and <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Dynamics of self-monitoring and error detection in speech production: evidence from mental imagery and MEG</article-title>. <source>J Cogn Neurosci</source> <volume>27</volume>, <fpage>352</fpage>–<lpage>364</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00692</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name>, and <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2013</year>). <article-title>The effect of imagination on stimulation: the functional specificity of efference copies in speech processing</article-title>. <source>J Cogn Neurosci</source> <volume>25</volume>, <fpage>1020</fpage>–<lpage>1036</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00381</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Indefrey</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Levelt</surname>, <given-names>W.J.</given-names></string-name></person-group> (<year>2004</year>). <article-title>The spatial and temporal signatures of word production components</article-title>. <source>Cognition</source> <volume>92</volume>, <fpage>101</fpage>–<lpage>144</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2002.06.001</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartsuiker</surname>, <given-names>R.J.</given-names></string-name>, and <string-name><surname>Kolk</surname>, <given-names>H.H.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Error monitoring in speech production: a computational test of the perceptual loop theory</article-title>. <source>Cogn Psychol</source> <volume>42</volume>, <fpage>113</fpage>–<lpage>157</lpage>. <pub-id pub-id-type="doi">10.1006/cogp.2000.0744</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nozari</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Novick</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Monitoring and Control in Language Production</article-title>. <source>Current Directions in Psychological Science</source> <volume>26</volume>, <fpage>403</fpage>–<lpage>410</lpage>. <pub-id pub-id-type="doi">10.1177/0963721417702419</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Niziolek</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Reilly</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Guenther</surname>, <given-names>F.H.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Prosodic adaptations to pitch perturbation in running speech</article-title>. <source>J Speech Lang Hear Res</source> <volume>54</volume>, <fpage>1051</fpage>–<lpage>1059</lpage>. <pub-id pub-id-type="doi">10.1044/1092-4388(2010/10-0162</pub-id>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Reilly</surname>, <given-names>K.J.</given-names></string-name>, <string-name><surname>Archibald</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Cai</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Guenther</surname>, <given-names>F.H.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Responses to Intensity-Shifted Auditory Feedback During Running Speech</article-title>. <source>J Speech Lang Hear Res</source> <volume>58</volume>, <fpage>1687</fpage>–<lpage>1694</lpage>. <pub-id pub-id-type="doi">10.1044/2015_JSLHR-S-15-0164</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ohala</surname>, <given-names>J.J.</given-names></string-name></person-group> (<year>1974</year>). <article-title>A mathematical model of speech aerodynamics</article-title>. <source>Annual Report of the Institute of Phonetics University of Copenhagen</source> <volume>8</volume>, <fpage>11</fpage>–<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Daneš</surname>, <given-names>F.</given-names></string-name></person-group> (<year>1960</year>). <article-title>Sentence intonation from a functional point of view</article-title>. <source>Word</source> <volume>16</volume>, <fpage>34</fpage>–<lpage>54</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Contextual tonal variations in Mandarin</article-title>. <source>Journal of Phonetics</source> <volume>25</volume>, <fpage>61</fpage>–<lpage>83</lpage>. DOI <pub-id pub-id-type="doi">10.1006/jpho.1996.0034</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hanson</surname>, <given-names>H.M.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Effects of obstruent consonants on fundamental frequency at vowel onset in English</article-title>. <source>J Acoust Soc Am</source> <volume>125</volume>, <fpage>425</fpage>–<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1121/1.3021306</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Cai</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Boucek</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ghosh</surname>, <given-names>S.S.</given-names></string-name>, <string-name><surname>Guenther</surname>, <given-names>F.H.</given-names></string-name>, and <string-name><surname>Perkell</surname>, <given-names>J.S.</given-names></string-name></person-group> (<year>2008</year>). <article-title>A system for online dynamic perturbation of formant trajectories and results from perturbations of the Mandarin triphthong/iau</article-title>. <conf-name>Proceedings of the 8th ISSP</conf-name>, <fpage>65</fpage>-<lpage>68</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2013</year>). <article-title>ProsodyPro—A tool for large-scale systematic prosody analysis</article-title>. <source>(Laboratoire Parole et Langage, France)</source>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Tseng</surname>, <given-names>C.-y.</given-names></string-name></person-group> (<year>1981</year>). <source>An acoustic phonetic study on tones in Mandarin Chinese</source> (<publisher-name>Brown University</publisher-name>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Nonparametric statistical testing of EEG-and MEG-data</article-title>. <source>J Neurosci Methods</source> <volume>164</volume>, <fpage>177</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname>, <given-names>Y.H.</given-names></string-name>, <string-name><surname>Jin</surname>, <given-names>P.Q.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Delta-band neural tracking primarily reflects rule-based chunking instead of semantic relatedness between words</article-title>. <source>Cerebral Cortex</source> <volume>33</volume>, <fpage>4448</fpage>–<lpage>4458</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhac354</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Jin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pan</surname>, <given-names>X.</given-names></string-name>, and <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Delta-band neural activity primarily tracks sentences instead of semantic properties of words</article-title>. <source>Neuroimage</source> <volume>251</volume>, <fpage>118979</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.118979</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Efron</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Tibshirani</surname>, <given-names>R.J.</given-names></string-name></person-group> (<year>1994</year>). <source>An Introduction to the Bootstrap</source> (<publisher-name>CRC press</publisher-name>). <pub-id pub-id-type="doi">10.1201/9780429246593</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108357.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study investigates how people adapt their speech when auditory feedback is altered. The analyses are rigorous and the work makes a <bold>valuable</bold> contribution by extending methods from limb motor control to speech. However, because the paradigm does not directly measure sensory error, the evidence for the proposed mechanism of sensorimotor learning is <bold>incomplete</bold>. The findings are best viewed as evidence for how prior motor adjustments influence subsequent behaviour, highlighting the need for future studies to more precisely separate sensory and motor contributions to adaptation.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108357.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this submitted manuscript, Lu, Tang, and colleagues implement a novel serial perturbation paradigm during speech to isolate the effects of sensory and motor processes on compensation. They perform three main studies: in the first study, they validate their method by randomly perturbing pitch in a series of produced vowels. They demonstrate that the amount of perturbation is driven (in part) by the previous trial's amount of motor compensation applied as opposed to the sensory perturbation. In the second experiment, they found that this effect carries over to single vowel words, but the effect was much weaker when different words were produced. Thirdly, the authors reproduce these findings in a more linguistically relevant way (during sentences) and show that the previously shown compensation effect only occurs within syntactic structures and not across them, suggesting an interplay between sensorimotor systems and linguistic structure processing.</p>
<p>Strengths:</p>
<p>Overall, this is a very unique study and strikes me as being potentially quite impactful. The authors have performed a large number of experiments to validate their findings that provide novel insights into the processes underlying compensation during speech production. These findings are also likely to produce new avenues for studying the neural mechanisms that support these processes.</p>
<p>Weaknesses:</p>
<p>While the authors go to great lengths to disassociate the serial effects of sensory and motor compensation, which is commendable, one weakness is that they are intrinsically linked (motor actions produce sensory consequences). Therefore, there is no obvious way to decouple them for the purposes of investigation. It would be beneficial to discuss future research that could further disentangle these factors.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108357.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study aims to disentangle the contribution of sensory and motor processes (mapped onto the inverse and forward components of speech motor control models like DIVA) to production changes as a result of altered auditory feedback. After five experiments, the authors conclude that it is the motor compensation on the previous trial, and not the sensory error, that drives compensatory responses in subsequent trials.</p>
<p>Assessment:</p>
<p>The goal of this paper is great, and the question is timely. Quite a bit of work has gone into the study, and the technical aspects are sound. That said, I just don't understand how the current design can accomplish what the authors have set as their goal. This may, of course, be a misunderstanding on my part, so I'll try to explain my confusion below. If it is indeed my mistake, then I encourage the authors to dedicate some space to unpacking the logic in the Introduction, which is currently barely over a page long. They should take some time to lay out the logic of the experimental design and the dependent and independent variables, and how this design disentangles sensory and motor influences. Then clearly discuss the opposing predictions supporting sensory-driven vs. motor-driven changes. Given that I currently don't understand the logic and, consequently, the claims, I will focus my review on major points for now.</p>
<p>Main issues</p>
<p>(1) Measuring sensory change. As acknowledged by the authors, making a motor correction as a function of altered auditory feedback is an interactive process between sensory and motor systems. However, one could still ask whether it is primarily a change to perception vs. a change to production that is driving the motor correction. But to do this, one has to have two sets of measurements: (a) perceptual change, and (b) motor change. As far as I understand, the study has the latter (i.e., C), but not the former. Instead, the magnitude of perceptual change is estimated through the proxy of the magnitude of perturbation (P), but the two are not the same; P is a physical manipulation; perceptual change is a psychological response to that physical manipulation. It is theoretically possible that a physical change does not cause a psychological change, or that the magnitude of the two does not match. So my first confusion centers on the absence of any measure of sensory change in this study.</p>
<p>To give an explicit example of what I mean, consider a study like Murphy, Nozari, and Holt (2024; Psychonomic Bulletin &amp; Review). This work is about changes to production as a function of exposure to other talkers' acoustic properties - rather than your own altered feedback - but the idea is that the same sensory-motor loop is involved in both. When changing the acoustic properties of the input, the authors obtain two separate measures: (a) how listeners' perception changes as a function of this physical change in the acoustics of the auditory signal, and (b) how their production changes. This allows the authors to identify motor changes above and beyond perceptual changes. Perhaps making a direct comparison with this study would help the reader understand the parallels better.</p>
<p>(2) A more fundamental issue for me is a theoretical one: Isn't a compensatory motor change ALWAYS a consequence of a perceptual change? I think it makes sense to ask, &quot;Does a motor compensation hinge on a previous motor action or is sensory change enough to drive motor compensation?&quot; This question has been asked for changed acoustics for self-produced speech (e.g., Hantzsch, Parrell, &amp; Niziolek, 2022) and other-produced speech (Murphy, Holt, &amp; Nozari, 2025), and in both cases, the answer has been that sensory changes alone are, in fact, sufficient to drive motor changes. A similar finding has been reported for the role of cerebellum in limb movements (Tseng et al., 2007), with a similar answer (note that in that study, the authors explicitly talk about &quot;the addition&quot; of motor corrections to sensory error, not one vs. the other as two independent factors. So I don't understand a sentence like &quot;We found that motor compensation, rather than sensory errors, predicted the compensatory responses in the subsequent trials&quot;, which views motor compensations and sensory errors as orthogonal variables affecting future motor adjustments.</p>
<p>In other words, there is a certain degree of seriality to the compensation process, with sensory changes preceding motor corrections. If the authors disagree with this, they should explain how an alternative is possible. If they mean something else, a comparison with the above studies and explaining the differences in positions would greatly help.</p>
<p>(3) Clash with previous findings. I used the examples in point 2 to bring up a theoretical issue, but those examples are also important in that all three of them reach a conclusion compatible with one another and different from the current study. The authors do discuss Tseng et al.'s findings, which oppose their own, but dismiss the opposition based on limb vs. articulator differences. I don't find the authors reasoning theoretically convincing here, but more importantly, the current claims also oppose findings from speech motor studies (see citations in point 2), to which the authors' arguments simply don't apply. Strangely, Hantzsch et al.'s study has been cited a few times, but never in its most important capacity, which is to show that speech motor adaptation can take place after a single exposure to auditory error. Murphy et al. report a similar finding in the context of exposure to other talkers' speech.</p>
<p>If the authors can convincingly justify their theoretical position in 2, the next step would be to present a thorough comparison with the results of the three studies above. If indeed there is no discrepancy, this comparison would help clarify it.</p>
<p>References</p>
<p>Hantzsch, L., Parrell, B., &amp; Niziolek, C. A. (2022). A single exposure to altered auditory feedback causes observable sensorimotor adaptation in speech. eLife, 11, e73694.</p>
<p>Murphy, T. K., Nozari, N., &amp; Holt, L. L. (2024). Transfer of statistical learning from passive speech perception to speech production. Psychonomic Bulletin &amp; Review, 31(3), 1193-1205.</p>
<p>Murphy, T. K., Holt, L. L. &amp; Nozari, N. (2025). Exposure to an Accent Transfers to Speech Production in a Single Shot. Preprint available at: <ext-link ext-link-type="uri" xlink:href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5196109">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5196109</ext-link>.</p>
<p>Tseng, Y. W., Diedrichsen, J., Krakauer, J. W., Shadmehr, R., &amp; Bastian, A. J. (2007). Sensory prediction errors drive cerebellum-dependent adaptation of reaching. Journal of neurophysiology, 98(1), 54-62.</p>
</body>
</sub-article>
</article>