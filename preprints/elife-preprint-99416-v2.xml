<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99416</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99416</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99416.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Progressive neural engagement within the IFG-pMTG circuit as gesture and speech entropy and MI advances</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6979-940X</contrib-id>
<name>
<surname>Zhao</surname>
<given-names>Wanying</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>zhaowy@psych.ac.cn</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Zhouyi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Xiang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4512-5221</contrib-id>
<name>
<surname>Du</surname>
<given-names>Yi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>duyi@psych.ac.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>State Key Laboratory of Cognitive Science and Mental Health, Institute of Psychology, Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>Department of Psychology, University of Chinese Academy of Sciences</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/029819q61</institution-id><institution>Chinese Institute for Brain Research</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03x1jna21</institution-id><institution>School of Psychology, Central China Normal University</institution></institution-wrap>, <city>Wuhan</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-12-13">
<day>13</day>
<month>12</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-05-19">
<day>19</day>
<month>05</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99416</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-28">
<day>28</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-04">
<day>04</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.11.23.517759"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-12-13">
<day>13</day>
<month>12</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99416.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa4">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa3">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa2">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa1">Reviewer #3 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.99416.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Zhao et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Zhao et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99416-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Semantic representation emerges from distributed multisensory modalities, yet a comprehensive understanding of the functional changing pattern within convergence zones or hubs integrating multisensory semantic information remains elusive. In this study, employing information-theoretic metrics, we quantified gesture and speech information, alongside their interaction, utilizing entropy and mutual information (MI). Neural activities were assessed via interruption effects induced by High-Definition transcranial direct current stimulation (HD-tDCS). Additionally, chronometric double-pulse transcranial magnetic stimulation (TMS) and high-temporal event-related potentials were utilized to decipher dynamic neural changes resulting from various information contributors. Results showed gradual inhibition of both inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG) as degree of gesture-speech integration, indexed by MI, increased. Moreover, a time-sensitive and staged progression of neural engagement was observed, evidenced by distinct correlations between neural activity patterns and entropy measures of speech and gesture, as well as MI, across early sensory and lexico-semantic processing stages. These findings illuminate the gradual nature of neural activity during multisensory gesture-speech semantic processing, shaped by dynamic gesture constraints and speech encoding, thereby offering insights into the neural mechanisms underlying multisensory language processing.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>gesture-speech integration</kwd>
<kwd>pMTG-IFG circuit</kwd>
<kwd>information theory</kwd>
<kwd>multisensory</kwd>
<kwd>semantic</kwd>
<kwd>dual-stage modal</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Introduction is reorganized to clarify the rationale and relationship of three experiments; additional partial correlation results are added for all three experiments to account for the influence of the candidate numbers; Figure 1 and 5 are revised; discussion is revised to clarify the potential semantic control process.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Semantic representation, distinguished by its cohesive conceptual nature, emerges from distributed modality-specific regions. Consensus acknowledges the presence of ‘convergence zones’ within the temporal and inferior parietal areas <sup><xref ref-type="bibr" rid="c1">1</xref></sup>, or the ‘semantic hub’ located in the anterior temporal lobe<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, pivotal for integrating, converging, or distilling multimodal inputs. Contemporary theories frame the semantic processing as a dynamic sequence of neural states<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, shaped by systems that are finely tuned to the statistical regularities inherent in sensory inputs<sup><xref ref-type="bibr" rid="c4">4</xref></sup>. These regularities enable the brain to evaluate, weight, and integrate multisensory information, optimizing the reliability of individual sensory signals<sup><xref ref-type="bibr" rid="c5">5</xref></sup>. However, sensory inputs available to the brain are often incomplete and uncertain, necessitating adaptive neural adjustments to resolve these ambiguities<sup><xref ref-type="bibr" rid="c6">6</xref></sup>. In this context, neuronal activity is thought to be linked to the probability density of sensory information, with higher levels of uncertainty resulting in the engagement of a broader population of neurons, thereby reflecting the brain’s adaptive capacity to handle diverse possible interpretations<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>. Although the role of ‘convergence zones’ and ‘semantic hubs’ in integrating multimodal inputs is well established, the precise functional patterns of neural activity in response to the distribution of unified multisensory information—along with the influence of unisensory signals —remain poorly understood.</p>
<p>To this end, we developed an analytic approach to directly probe the cortical engagement during multisensory gesture-speech semantic integration. Even though gestures convey information in a global-synthetic way, while speech conveys information in a linear segmented way, there exists a bidirectional semantic influence between the two modalities<sup><xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup>. Gesture is regarded as ‘part of language’<sup><xref ref-type="bibr" rid="c11">11</xref></sup> or functional equivalents of lexical units that alternate and integrate with speech into a ‘single unification space’ to convey a coherent meaning<sup><xref ref-type="bibr" rid="c12">12</xref>–<xref ref-type="bibr" rid="c14">14</xref></sup>. Empirical studies have investigated the semantic integration between gesture and speech by manipulating their semantic relationship<sup><xref ref-type="bibr" rid="c15">15</xref>–<xref ref-type="bibr" rid="c18">18</xref></sup> and revealed a mutual interaction between them<sup><xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref></sup> as reflected by the N400 latency and amplitude<sup><xref ref-type="bibr" rid="c14">14</xref></sup> as well as common neural underpinnings in the left inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG)<sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>.</p>
<p>Building on these insights, the present study quantified the amount of information from both sources and their interaction adopting the information-theoretic complexity metrics of <italic>entropy</italic> and <italic>mutual information</italic> (MI). Unisensory Entropy measures the disorder or randomness of information and serves as an index of the uncertainty in modality-specific representations of gesture or speech activated by an event<sup><xref ref-type="bibr" rid="c24">24</xref></sup>. MI assesses share information between modalities<sup><xref ref-type="bibr" rid="c25">25</xref></sup>, indicating multisensory convergence and acting as an index of gesture-speech integration.</p>
<p>To investigate the neural mechanisms underlying gesture-speech integration, we conducted three experiments to assess how neural activity correlates with distributed multisensory integration, quantified using information-theoretic measures of MI. Additionally, we examined the contributions of unisensory signals in this process, quantified through unisensory entropy. <bold>Experiment 1</bold> employed high-definition transcranial direct current stimulation (HD-tDCS) to administer Anodal, Cathodal and Sham stimulation to either the IFG or the pMTG. HD-tDCS induces membrane depolarization with anodal stimulation and membrane hyperpolarization with cathodal stimulation<sup><xref ref-type="bibr" rid="c26">26</xref></sup>, thereby increasing or decreasing cortical excitability in the targeted brain area, respectively. This experiment aimed to determine whether the overall facilitation (Anodal-tDCS minus Sham-tDCS) and/or inhibitory (Cathodal-tDCS minus Sham-tDCS) of these integration hubs is modulated by the degree of gesture-speech integration, as measure by MI.</p>
<p>Given the differential involvement of the IFG and pMTG in gesture-speech integration, shaped by top-down gesture predictions and bottom-up speech processing <sup><xref ref-type="bibr" rid="c23">23</xref></sup>, <bold>Experiment 2</bold> was designed to further assess whether the activity of these regions was associated with relevant informational matrices. Specifically, we applied inhibitory chronometric double-pulse transcranial magnetic stimulation (TMS) to specific temporal windows associated with integration processes in these regions<sup><xref ref-type="bibr" rid="c23">23</xref></sup>, assessing whether the inhibitory effects of TMS were correlated with unisensory entropy or the multisensory convergence index (MI).</p>
<p><bold>Experiment 3</bold> complemented these investigations by focusing on the temporal dynamics of neural responses during semantic processing, leveraging high-temporal event-related potentials (ERPs). This experiment investigated how distinct information contributors modulated specific ERP components associated with semantic processing. These components included the early sensory effects as P1 and N1–P2<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>, the N400 semantic conflict effect<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>, and the late positive component (LPC) reconstruction effect<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>. By integrating these ERP findings with results from Experiments 1 and 2, Experiment 3 aimed to provide a more comprehensive understanding of how gesture-speech integration is modulated by neural dynamics.</p>
</sec>
<sec id="s2">
<title>Material and methods</title>
<sec id="s2a">
<title>Participants</title>
<p>Ninety-eight young Chinese participants signed written informed consent forms and took part in the present study (Experiment 1: 29 females, 23 males, age = 20 ± 3.40 years; Experiment 2: 11 females, 13 males, age = 23 ± 4.88 years; Experiment 3: 12 females, 10 males, age = 21 ± 3.53 years). All of the participants were right-handed (Experiment 1: laterality quotient (LQ)<sup><xref ref-type="bibr" rid="c32">32</xref></sup> = 88.71 ± 13.14; Experiment 2: LQ = 89.02 ± 13.25; Experiment 3: LQ = 88.49 ± 12.65), had normal or corrected-to-normal vision and were paid ¥100 per hour for their participation. All experiments were approved by the Ethics Committee of the Institute of Psychology, Chinese Academy of Sciences.</p>
</sec>
<sec id="s2b">
<title>Stimuli</title>
<p>Twenty gestures (<xref ref-type="supplementary-material" rid="supp1">Appendix Table 1</xref>) with 20 semantically congruent speech signals taken from previous study<sup><xref ref-type="bibr" rid="c23">23</xref></sup> were used. The stimuli set were recorded from two native Chinese speakers (1 male, 1 female). To validate the stimuli, 30 participants were recruited to replicate the multisensory index of semantic congruency effect, hypothesizing that reaction times for semantically incongruent gesture-speech pairs would be significantly longer than those for congruent pairs. The results confirmed this hypothesis, with a significantly (<italic>t</italic>(29) = 7.16, <italic>p</italic> &lt; 0.001) larger reaction time when participants were asked to judge the gender of the speaker if gesture contained incongruent semantic information with speech (a ‘cut’ gesture paired with speech word ‘喷 pen1 (spray)’: mean = 554.51 ms, SE = 11.65) relative to when they were semantically congruent (a ‘cut’ gesture paired with ‘剪 jian3 (cut)’ word: mean = 533.90 ms, SE = 12.02)<sup><xref ref-type="bibr" rid="c23">23</xref></sup>.</p>
<p>Additionally, two separate pre-tests with 30 subjects in each (pre-test 1: 16 females, 14 males, age = 24 ± 4.37 years; pre-test 2: 15 females, 15 males, age = 22 ± 3.26 years) were conducted to determine the comprehensive values of gesture and speech. Participants were presented with segments of increasing duration, beginning at 40 ms, and were prompted to provide a single verb to describe either the isolated gesture they observed (pre-test 1) or the isolated speech they heard (pre-test 2). For each gesture or speech, the action verb consistently provided by participants across four to six consecutive repetitions—with the number of repetitions varied to mitigate learning effects—was considered the comprehensive response for the gesture or speech. The initial instance duration was marked as the discrimination point (DP) for gesture (mean = 183.78 ± 84.82ms) or the identification point (IP) for speech (mean = 176.40 ± 66.21ms) (<xref rid="fig1" ref-type="fig">Figure 1A top</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Experimental design, and stimulus characteristics.</title><p>(A) <bold>Experimental stimuli</bold>. Twenty gestures were paired with 20 relevant speech stimuli. Two separate Pre-tests were executed to define the minimal length of each gesture and speech required for semantic identification, namely, the discrimination point (DP) of gesture and the identification point (IP) of speech. Overall, a mean of 183.78 ms (SD = 84.82) was found for the DP of gestures and the IP of speech was 176.40 ms (SD = 66.21). The onset of speech was set at the gesture DP. Responses for each item were assessed utilizing information-theoretic complexity metrics to quantify the information content of both gesture and speech during integration, employing entropy and MI.</p><p>(B) <bold>Procedure of Experiment 1</bold>. HD-tDCS, including Anodal, Cathodal, or Sham conditions, was administered to the IFG or pMTG) using a 4 * 1 ring-based electrode montage. Electrode F7 targeted the IFG, with return electrodes placed on AF7, FC5, F9, and FT9. For pMTG stimulation, TP7 was targeted, with return electrodes positioned on C5, P5, T9, and P9. Sessions lasted 20 minutes, with a 5-second fade-in and fade-out, while the Sham condition involved only 30 seconds of stimulation.</p><p>(C) <bold>Procedure of Experiment 2</bold>. Eight time windows (TWs, duration = 40 ms) were segmented in relative to the speech IP. Among the eight TWs, five (TW1, TW2, TW3, TW6, and TW7) were chosen based on the significant results in our prior study<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. Double-pulse TMS was delivered over each of the TW of either the pMTG or the IFG.</p><p>(D) <bold>Procedure of Experiment 3</bold>. Semantically congruent gesture-speech pairs were presented randomly with Electroencephalogram (EEG) recorded simultaneously. Epochs were time locked to the onset of speech and lasted for 1000 ms. A 200 ms pre-stimulus baseline correction was applied before the onset of gesture stoke. Various elicited components were hypothesized.</p></caption>
<graphic xlink:href="517759v4_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To quantify information content, comprehensive responses for each item were converted into Shannon’s entropy (H) as a measure of information richness (<xref rid="fig1" ref-type="fig">Figure 1A bottom</xref>). With no significant gender differences observed in both gesture (<italic>t</italic>(20) = 0.21, <italic>p</italic> = 0.84) and speech (<italic>t</italic>(20) = 0.52, <italic>p</italic> = 0.61), responses were aggregated across genders, resulting in 60 answers per item (<xref ref-type="supplementary-material" rid="supp1">Appendix Table 2</xref>). Here, p(xi) and p(yi) represent the distribution of 60 answers for a given gesture (<xref ref-type="supplementary-material" rid="supp1">Appendix Table 2B</xref>) and speech (<xref ref-type="supplementary-material" rid="supp1">Appendix Table 2A</xref>), respectively. High entropy indicates diverse answers, reflecting broad representation, while low entropy suggests focused lexical recognition for a specific item (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). MI was used to measure the overlap between gesture and speech information, calculated by subtracting the entropy of the combined gesture-speech dataset (Entropy(gesture + speech)) from the sum of their individual entropies (Entropy(gesture) + Entropy(speech)) (see <xref ref-type="supplementary-material" rid="supp1">Appendix Table 2C</xref>). For specific gesture-speech combinations, equivalence between the combined entropy and the sum of individual entropies (gesture or speech) indicates absence of overlap in response sets. Conversely, significant overlap, denoted by a considerable number of shared responses between gesture and speech datasets, leads to a noticeable discrepancy between combined entropy and the sum of gesture and speech entropies. Elevated MI values thus signify substantial overlap, indicative of a robust mutual interaction between gesture and speech.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Quantification formulas (A) and distributions of each stimulus in Shannon’s entropy (B).</title><p>Two separate pre-tests (N = 30) were conducted to assign a single verb for describing each of the isolated 20 gestures and 20 speech items. Responses provided for each item were transformed into Shannon’s entropy using a relative quantification formula. Gesture (<bold>B left</bold>) and speech (<bold>B right</bold>) entropy quantify the randomness of gestural or speech information, representing the uncertainty of probabilistic representation activated when a specific stimulus occurs. Joint entropy (<bold>B middle</bold>) captures the widespread nature of the two sources of information combined. Mutual information (MI) was calculated as the difference between joint entropy with gesture entropy and speech entropy combined (<bold>A</bold>), thereby capturing the overlap of gesture and speech and representing semantic integration.</p></caption>
<graphic xlink:href="517759v4_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Additionally, the number of responses provided for each gesture and speech, as well as the total number of combined responses, were also recorded. The quantitative data for each stimulus, including gesture entropy, speech entropy, joint entropy, MI, and the respective counts, are presented in <xref ref-type="supplementary-material" rid="supp1">Appendix Table 3</xref>.</p>
<p>To determine whether entropy or MI values corresponds to distinct neural changes, the current study first aggregated neural responses (including inhibition effects of tDCS and TMS or ERP amplitudes) that shared identical entropy or MI values, prior to conducting correlational analyses.</p>
</sec>
<sec id="s3">
<title>Experimental procedure</title>
<p>Given that gestures induce a semantic priming effect on concurrent speech<sup><xref ref-type="bibr" rid="c33">33</xref></sup>, this study utilized a semantic priming paradigm in which speech onset was aligned with the DP of each gesture<sup><xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>, the point at which the gesture transitions into a lexical form<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. The gesture itself began at the stroke phase, a critical moment when the gesture conveys its primary semantic content<sup><xref ref-type="bibr" rid="c34">34</xref></sup>.</p>
<p>An irrelevant factor of gender congruency (e.g., a man making a gesture combined with a female voice) was created<sup><xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>. This involved aligning the gender of the voice with the corresponding gender of the gesture in either a congruent (e.g., male voice paired with a male gesture) or incongruent (e.g., male voice paired with a female gesture) manner. This approach served as a direct control mechanism, facilitating the investigation of the automatic and implicit semantic interplay between gesture and speech<sup><xref ref-type="bibr" rid="c35">35</xref></sup>. In light of previous findings indicating a distinct TMS-disruption effect on the semantic congruency of gesture-speech interactions<sup><xref ref-type="bibr" rid="c23">23</xref></sup>, both semantically congruent and incongruent pairs were included in Experiment 1 and Experiment 2. Experiment 3, conversely, exclusively utilized semantically congruent pairs to elucidate ERP metrics indicative of nuanced semantic progression.</p>
<p>Gesture–speech pairs were presented randomly using Presentation software (<ext-link ext-link-type="uri" xlink:href="https://www.neurobs.com">www.neurobs.com</ext-link>). Participants were asked to look at the screen but respond with both hands as quickly and accurately as possible merely to the gender of the voice they heard. The RT and the button being pressed were recorded. The experiment started with a fixation cross presented on the center of the screen, which lasted for 0.5-1.5 sec.</p>
</sec>
<sec id="s4">
<title>Experiment 1: HD-tDCS protocol and data analysis</title>
<p>Participants were divided into two groups, with each group undergoing HD-tDCS stimulation at different target sites (IFG or pMTG). Each participant completed three experimental sessions, spaced one week apart, during which 480 gesture-speech pairs were presented across various conditions. In each session, participants received one of three types of HD-tDCS stimulation: Anodal, Cathodal, or Sham. The order of stimulation site and type was counterbalanced using a Latin square design to control for potential order effects.</p>
<p>HD-tDCS protocol employed a constant current stimulator (The Starstim 8 system) delivering stimulation at an intensity of 2000mA. A 4 * 1 ring-based electrode montage was utilized, comprising a central electrode (stimulation) positioned directly over the target cortical area and four return electrodes encircling it to provide focused stimulation. Building on a meta-analysis of prior fMRI studies examining gesture-speech integration<sup><xref ref-type="bibr" rid="c22">22</xref></sup>, we targeted Montreal Neurological Institute (MNI) coordinates for the left IFG at (-62, 16, 22) and the pMTG at (-50, -56, 10). In the stimulation protocol for HD-tDCS, the IFG was targeted using electrode F7 as the optimal cortical projection site<sup><xref ref-type="bibr" rid="c36">36</xref></sup>, with four return electrodes placed at AF7, FC5, F9, and FT9. For the pMTG, TP7 was selected as the cortical projection site<sup><xref ref-type="bibr" rid="c36">36</xref></sup>, with return electrodes positioned at C5, P5, T9, and P9. The stimulation parameters included a 20-minute duration with a 5-second fade-in and fade-out for both Anodal and Cathodal conditions. The Sham condition involved a 5-second fade-in followed by only 30 seconds of stimulation, then 19’20 minutes of no stimulation, and finally a 5-second fade-out (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). Stimulation was controlled using NIC software, with participants blinded to the stimulation conditions.</p>
<p>All incorrect responses (702 out of the total number of 24960, 2.81% of trials) were excluded. To eliminate the influence of outliers, a 2SD trimmed mean for every participant in each session was also calculated. To examine the relationship between the degree of information and neural responses, we conducted Pearson correlation analyses using a sample of 20 sets. Neural responses were quantified based on the effects of HD-tDCS (active tDCS minus sham tDCS) on the semantic congruency effect, defined as the difference in reaction times between semantic incongruent and congruent conditions (Rt(incongruent) - Rt(congruent)). This effect served as an index of multisensory integration<sup><xref ref-type="bibr" rid="c35">35</xref></sup> within the left IFG and pMTG. The variation in information was assessed using three information-theoretic metrics. To account for potential confounds related to multiple candidate representations, we conducted partial correlation analyses between the tDCS effects and gesture entropy, speech entropy, and MI, controlling for the number of responses provided for each gesture and speech, as well as the total number of combined responses. Given that HD-tDCS induces overall disruption at the targeted brain regions, we hypothesized that the neural activity within the left IFG and pMTG would be progressively affected by varying levels of multisensory convergence, as indexed by MI. Moreover, we hypothesized that the modulation of neural activity by MI would differ between the left IFG and pMTG, as reflected in the differential modulation of response numbers in the partial correlations, highlighting their distinct roles in semantic processing<sup><xref ref-type="bibr" rid="c37">37</xref></sup>. False discovery rate (FDR) correction was applied for multiple comparisons.</p>
</sec>
<sec id="s5">
<title>Experiment 2: TMS protocol and data analysis</title>
<p>Experiment 2 involved 800 gesture-speech pairs, presented across 15 blocks over three days, with one week between sessions. Stimulation was administered at three different sites (IFG, pMTG, or Vertex). Within the time windows (TWs) spanning the gesture-speech integration period, five TWs that exhibited selective disruption of integration were selected: TW1 (-120 to -80 ms relative to the speech identification point), TW2 (-80 to -40 ms), TW3 (-40 to 0 ms), TW6 (80 to 120 ms), and TW7 (120 to 160 ms)<sup><xref ref-type="bibr" rid="c23">23</xref></sup> (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). The order of stimulation site and TW was counterbalanced using a Latin square design.</p>
<p>At an intensity of 50% of the maximum stimulator output, double-pulse TMS was delivered via a 70 mm figure-eight coil using a Magstim Rapid² stimulator (Magstim, UK). High-resolution (1 × 1 × 0.6 mm) T1-weighted MRI scans were obtained using a Siemens 3T Trio/Tim Scanner for image-guided TMS navigation. Frameless stereotaxic procedures (BrainSight 2; Rogue Research) allowed real-time stimulation monitoring. To ensure precision, individual anatomical images were manually registered by identifying the anterior and posterior commissures. Subject-specific target regions were defined using trajectory markers in the MNI coordinate system. Vertex was used as control.</p>
<p>All incorrect responses (922 out of the total number of 19200, 4.8% of trials) were excluded. We focused our analysis on Pearson correlations of the TMS interruption effects (active TMS minus vertex TMS) of the semantic congruency effect with the gesture entropy, speech entropy or MI. To control for potential confounds, partial correlations were also performed between the TMS effects and gesture entropy, speech entropy, and MI, controlling for the number of responses for each gesture and speech, as well as the total number of combined responses. By doing this, we can determine how the time-sensitive contribution of the left IFG and pMTG to gesture–speech integration was affected by gesture and speech information distribution. FDR correction was applied for multiple comparisons.</p>
</sec>
<sec id="s6">
<title>Experiment 3: Electroencephalogram (EEG) recording and data analysis</title>
<p>Experiment 3, comprising a total of 1760 gesture-speech pairs, was completed in a single-day session. EEG were recorded from 48 Ag/AgCl electrodes mounted in a cap according to the 10-20 system<sup><xref ref-type="bibr" rid="c38">38</xref></sup>, amplified with a PORTI-32/MREFA amplifier (TMS International B.V., Enschede, NL) and digitized online at 500 Hz (bandpass, 0.01-70 Hz). EEGLAB, a MATLAB toolbox, was used to analyze the EEG data<sup><xref ref-type="bibr" rid="c39">39</xref></sup>. Vertical and horizontal eye movements were measured with 4 electrodes placed above the left eyebrow, below the left orbital ridge and at bilateral external canthus. All electrodes were referenced online to the left mastoid. Electrode impedance was maintained below 5 KΩ. The average of the left and right mastoids was used for re-referencing. A high-pass filter with a cutoff of 0.05 Hz and a low-pass filter with a cutoff of 30 Hz were applied. Semi-automated artifact removal, including independent component analysis (ICA) for identifying components of eye blinks and muscle activity, was performed (<xref rid="fig1" ref-type="fig">Figure 1D</xref>). Participants with rejected trials exceeding 30% of their total were excluded from further analysis.</p>
<p>All incorrect responses were excluded (147 out of 1760, 8.35% of trials). To eliminate the influence of outliers, a 2 SD trimmed mean was calculated for every participant in each condition. Data were epoched from the onset of speech and lasted for 1000 ms. To ensure a clean baseline with no stimulus presented, a 200 ms pre-stimulus baseline correction was applied before gesture onset.</p>
<p>To consolidate the data, we conducted both a traditional region-of-interest (ROI) analysis, with ROIs defined based on a well-established work<sup><xref ref-type="bibr" rid="c40">40</xref></sup>, and a cluster-based permutation approach, which utilizes data-driven permutations to enhance robustness and address multiple comparisons.</p>
<p>For the traditional ROI analysis, grand-average ERPs at electrode Cz were compared between the higher (≥50%) and lower (&lt;50%) halves for gesture entropy (<xref rid="fig5" ref-type="fig">Figure 5A1</xref>), speech entropy (<xref rid="fig5" ref-type="fig">Figure 5B1</xref>), and MI (<xref rid="fig5" ref-type="fig">Figure 5C1</xref>). Consequently, four ERP components were determined: the P1 effect observed within the time window of 0-100 ms<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>, the N1-P2 effect observed between 150-250ms<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>, the N400 within the interval of 250-450ms<sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>, and the LPC spanning from 550-1000ms<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>. Additionally, seven regions-of-interest (ROIs) were defined in order to locate the modulation effect on each ERP component: left anterior (LA): F1, F3, F5, FC1, FC3, and FC5; left central (LC): C1, C3, C5, CP1, CP3, and CP5; left posterior (LP): P1, P3, P5, PO3, PO5, and O1; right anterior (RA): F2, F4, F6, FC2, FC4, and FC6; right central (RC): C2, C4, C6, CP2, CP4, and CP6; right posterior (RP): P2, P4, P6, PO4, PO6, and O2; and midline electrodes (ML): Fz, FCz, Cz, Pz, Oz, and CPz<sup><xref ref-type="bibr" rid="c40">40</xref></sup>.</p>
<p>Subsequently, cluster-based permutation tests<sup><xref ref-type="bibr" rid="c41">41</xref></sup> in Fieldtrip was further used to determine the significant clusters of adjacent time points and electrodes of ERP amplitude between the higher and lower halves of gesture entropy, speech entropy and MI, respectively. The electrode-level type I error threshold was set to 0.025. Cluster-level statistic was estimated through 5000 Monte Carlo simulations, where the cluster-level statistic is the sum of T-values for each stimulus within a cluster. The cluster-level type I error threshold was set to 0.05. Clusters with a p-value less than the critical alpha-level are considered to be conditionally different.</p>
<p>Paired t-tests were conducted to compare the lower and upper halves of each information model for the averaged amplitude within each ROI or cluster across the four ERP time windows, separately. Pearson correlations were computed between each model value and the averaged ERP amplitudes in each ROI or cluster. Additionally, partial correlations were conducted, accounting for the number of responses for each respective metric. FDR correction was applied for multiple comparisons.</p>
</sec>
</sec>
<sec id="s7">
<title>Results</title>
<sec id="s7a">
<title>Experiment 1: Modulation of left pMTG and IFG engagement by gradual changes in gesture-speech semantic information</title>
<p>In the IFG, one-way ANOVA examining the effects of three tDCS conditions (Anodal, Cathodal, or Sham) on semantic congruency (RT (semantic incongruent) – RT (semantic congruent)) demonstrated a significant main effect of stimulation condition (<italic>F</italic>(2, 75) = 3.673, <italic>p</italic> = 0.030, ηp2 = 0.089). Post hoc paired t-tests indicated a significantly reduced semantic congruency effect between the Cathodal condition and the Sham condition (<italic>t</italic>(26) = -3.296, <italic>p</italic> = 0.003, 95% CI = [-11.488, 4.896]) (<xref rid="fig3" ref-type="fig">Figure 3A left</xref>). Subsequent Pearson correlation analysis revealed that the reduced semantic congruency effect was progressively associated with the MI, evidenced by a significant correlation between the Cathodal-tDCS effect (Cathodal-tDCS minus Sham-tDCS) and MI (<italic>r</italic> = -0.595, <italic>p</italic> = 0.007, 95% CI = [-0.995, -0.195]) (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Additionally, a similar correlation was observed between the Cathodal-tDCS effect and the total response number (<italic>r</italic> = -0.543, <italic>p</italic> = 0.016, 95% CI = [-0.961, -0.125]).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>tDCS effect over semantic congruency.</title>
<p><bold>(A)</bold> tDCS effect was defined as active-tDCS minus sham-tDCS. The semantic congruency effect was calculated as the reaction time (RT) difference between semantically incongruent and semantically congruent pairs (Rt(incongruent) - Rt(congruent)).</p><p><bold>(B)</bold> Correlations of the tDCS effect over the semantic congruency effect with three information models (gesture entropy, speech entropy and MI) are displayed with best-fitting regression lines. Significant correlations are marked in red. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01 after FDR correction.</p></caption>
<graphic xlink:href="517759v4_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>However, partial correlation analysis, controlling for the total response number, revealed that the initially significant correlation between the Cathodal-tDCS effect and MI was no longer significant (<italic>r</italic> = -0.303, <italic>p</italic> = 0.222, 95% CI = [-0.770, 0.164]). This suggests that the observed relationship between Cathodal-tDCS and MI may be confounded by semantic control difficulty, as reflected by the total number of responses. Specifically, the reduced activity in the IFG under Cathodal-tDCS may be driven by variations in the difficulty of semantic control rather than a direct modulation of MI.</p>
<p>In the pMTG, a one-way ANOVA assessing the effects of three tDCS conditions on semantic congruency also revealed a significant main effect of stimulation condition (<italic>F</italic>(2, 75) = 3.250, <italic>p</italic> = 0.044, ηp2 = 0.080). Subsequent paired t-tests identified a significantly reduced semantic congruency effect between the Cathodal condition and the Sham condition (<italic>t</italic>(25) = - 2.740, <italic>p</italic> = 0.011, 95% CI = [-11.915, 6.435]) (<xref rid="fig3" ref-type="fig">Figure 3A right</xref>). Moreover, a significant correlation was observed between the Cathodal-tDCS effect and MI (<italic>r</italic> = -0.457, <italic>p</italic> = 0.049, 95% CI = [-0.900, -0.014]) (<xref rid="fig3" ref-type="fig">Figure 3B</xref>).</p>
<p>Importantly, the reduced activity in the pMTG under Cathodal-tDCS was not influenced by the total response number, as indicated by the non-significant correlation (<italic>r</italic> = -0.253, <italic>p</italic> = 0.295, 95% CI = [-0.735, 0.229]). This finding was further corroborated by the unchanged significance in the partial correlation between Cathodal-tDCS and MI, when controlling for the total response number (<italic>r</italic> = -0.472, <italic>p</italic> = 0.048, 95% CI = [-0.903, -0.041]).</p>
<p>RTs of congruent and incongruent trials of IFG and pMTG in each of the stimulation conditions were shown in <xref ref-type="supplementary-material" rid="supp1">Appendix Table 4A</xref>.</p>
</sec>
<sec id="s7b">
<title>Experiment 2: Time-sensitive modulation of left pMTG and IFG engagements by gradual changes in gesture-speech semantic information</title>
<p>A 2 (TMS effect: active - Vertex) × 5 (TW) ANOVA on semantic congruency revealed a significant interaction between TMS effect and TW (<italic>F</italic>(3.589, 82.538) = 3.273, <italic>p</italic> = 0.019, ηp2 = 0.125). Further t-tests identified a significant TMS effect over the pMTG in TW1 (<italic>t</italic>(23) = - 3.068, <italic>p</italic> = 0.005, 95% CI = [-6.838, 0.702]), TW2 (<italic>t</italic>(23) = -2.923, <italic>p</italic> = 0.008, 95% CI = [-6.490, 0.644]), and TW7 (<italic>t</italic>(23) = -2.005, <italic>p</italic> = 0.047, 95% CI = [-5.628, 1.618]). In contrast, a significant TMS effect over the IFG was found in TW3 (<italic>t</italic>(23) = -2.335, <italic>p</italic> = 0.029, 95% CI = [- 5.928, 1.258]), and TW6 (<italic>t</italic>(23) = -4.839, <italic>p</italic> &lt; 0.001, 95% CI = [-7.617, -2.061]) (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). Raw RTs of congruent and incongruent trials were shown in <xref ref-type="supplementary-material" rid="supp1">Appendix Table 4B</xref>.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>TMS impacts on semantic congruency effect across various time windows (TW).</title>
<p><bold>(A)</bold> Five time windows (TWs) showing selective disruption of gesture-speech integration were chosen: TW1 (-120 to -80 ms relative to speech identification point), TW2 (-80 to -40 ms), TW3 (-40 to 0 ms), TW6 (80 to 120 ms), and TW7 (120 to 160 ms). TMS effect was defined as active-TMS minus vertex-TMS. The semantic congruency effect was calculated as the reaction time (RT) difference between semantically incongruent and semantically congruent pairs.</p><p><bold>(B)</bold> Correlations of the TMS effect over the semantic congruency effect with three information models (gesture entropy, speech entropy and MI) are displayed with best-fitting regression lines. Significant correlations are marked in red. * <italic>p</italic> &lt; 0.05, ** <italic>p</italic> &lt; 0.01, *** <italic>p</italic> &lt; 0.001 after FDR correction.</p></caption>
<graphic xlink:href="517759v4_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Additionally, a significant negative correlation was found between the TMS effect (a larger negative TMS effect signifies a greater disruption of the integration process) and speech entropy when the pMTG was inhibited in TW2 (<italic>r</italic> = -0.792, <italic>p</italic> = 0.004, 95% CI = [-1.252, - 0.331]). Meanwhile, when the IFG activity was interrupted in TW6, a significant negative correlation was found between the TMS effect and gesture entropy (<italic>r</italic> = -0.539, <italic>p</italic> = 0.014, 95% CI = [-0.956, -0.122]), speech entropy (<italic>r</italic> = -0.664, <italic>p</italic> = 0.026, 95% CI = [-1.255, -0.073]), and MI (<italic>r</italic> = -0.677, <italic>p</italic> = 0.001, 95% CI = [-1.054, -0.300]) (<xref rid="fig4" ref-type="fig">Figure 4B</xref>).</p>
<p>Notably, inhibition of pMTG activity in TW2 was not influenced by the number of speech responses (<italic>r</italic> = -0.539, <italic>p</italic> = 0.087, 95% CI = [-1.145, 0.067]). However, the number of speech responses did affect the modulation of speech entropy on the pMTG inhibition effect in TW2. This was evidenced by the non-significant partial correlation between pMTG inhibition and speech entropy when controlling for speech response number (<italic>r</italic> = -0.218, <italic>p</italic> = 0.545, 95% CI = [-0.563, 0.127]).</p>
<p>In contrast, the interrupted IFG activity in TW6 appeared to be consistently influenced by the confound of semantic control difficulty. This was reflected in the significant correlation with both gesture response number (<italic>r</italic> = -0.480, <italic>p</italic> = 0.032, 95% CI = [-904, -0.056]), speech response number (<italic>r</italic> = -0.729, <italic>p</italic> = 0.011, 95% CI = [-1.221, -0.237]), and total response number (<italic>r</italic> = -0.591, <italic>p</italic> = 0.008, 95% CI = [-0.993, -0.189]). Additionally, partial correlation analyses revealed non-significant relationship between interrupted IFG activity in TW6 and gesture entropy (<italic>r</italic> = -0.369, <italic>p</italic> = 0.120, 95% CI = [-0.810, -0.072]), speech entropy (<italic>r</italic> = -0.455, <italic>p</italic> = 0.187, 95% CI = [-1.072, 0.162]), and MI (<italic>r</italic> = -0.410, <italic>p</italic> = 0.091, 95% CI = [-0.856, -0.036]) when controlling for response numbers.</p>
</sec>
<sec id="s7c">
<title>Experiment 3: Temporal modulation of P1, N1-P2, N400 and LPC components by gradual changes in gesture-speech semantic information</title>
<p>Topographical maps illustrating amplitude differences between the lower and higher halves of speech entropy demonstrate a central-posterior P1 amplitude (0-100 ms, <xref rid="fig5" ref-type="fig">Figure 5B</xref>). Aligning with prior findings<sup><xref ref-type="bibr" rid="c27">27</xref></sup>, the paired t-tests demonstrated a significantly larger P1 amplitude within the ML ROI (<italic>t</italic>(22) = 2.510, <italic>p</italic> = 0.020, 95% confidence interval (CI) = [1.66, 3.36]) when contrasting stimuli with higher 50% speech entropy against those with lower 50% speech entropy (<xref rid="fig5" ref-type="fig">Figure 5D1 left</xref>). Subsequent correlation analyses unveiled a significant increase in the P1 amplitude with the rise in speech entropy within the ML ROI (<italic>r</italic> = 0.609, <italic>p</italic> = 0.047, 95% CI = [0.039, 1.179], <xref rid="fig5" ref-type="fig">Figure 5D1 right</xref>). Furthermore, a cluster of neighboring time-electrode samples exhibited a significant contrast between the lower 50% and higher 50% of speech entropy, revealing a P1 effect spanning 16 to 78 ms at specific electrodes (FC2, FCz, C1, C2, Cz, and CPz, <xref rid="fig5" ref-type="fig">Figure 5D2 middle</xref>) (<italic>t</italic>(22) = 2.754, <italic>p</italic> = 0.004, 95% confidence interval (CI) = [1.65, 3.86], <xref rid="fig5" ref-type="fig">Figure 5D2 left</xref>), with a significant correlation with speech entropy (<italic>r</italic> = 0.636, <italic>p</italic> = 0.035, 95% CI = [0.081, 1.191], <xref rid="fig5" ref-type="fig">Figure 5D2 right</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>ERP results of gesture entropy (A), speech entropy (B) or MI (C).</title>
<p>Four ERP components were identified from grand-average ERPs at the Cz electrode, contrasting trials with the lower 50% (red lines) and the higher 50% (blue lines) of gesture entropy, speech entropy or MI. Clusters of adjacent time points and electrodes were subsequently identified within each component using a cluster-based permutation test. Topographical maps depict amplitude differences between the lower and higher halves of each information model, with significant ROIs (<bold>D1-H1 middle</bold>) or electrode clusters (<bold>D2-H2 middle</bold>) highlighted in black. Solid rectangles delineating the ROIs that exhibited the maximal correlation and paired t-values (<bold>D1-H1 middle</bold>). T-test comparisons with normal distribution lines (<bold>left</bold>) and correlations with best-fitting regression lines (<bold>right</bold>) are calculated and illustrated between the average ERP amplitude within the rectangular ROI (<bold>D1-H1</bold>) or the elicited clusters (<bold>D2-H2</bold>) and the three information models individually. * p &lt; 0.05, ** p &lt; 0.01 after FDR correction.</p></caption>
<graphic xlink:href="517759v4_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Additionally, topographical maps comparing the lower 50% and higher 50% gesture entropy revealed a frontal N1-P2 amplitude (150-250 ms, <xref rid="fig5" ref-type="fig">Figure 5A</xref>). In accordance with previous findings on bilateral frontal N1-P2 amplitude<sup><xref ref-type="bibr" rid="c27">27</xref></sup>, paired t-tests displayed a significantly larger amplitude for stimuli with lower 50% gesture entropy than with higher 50% entropy in both ROIs of LA (<italic>t</italic>(22) = 2.820, <italic>p</italic> = 0.011, 95% CI = [2.21, 3.43]) and RA (<italic>t</italic>(22) = 2.223, <italic>p</italic> = 0.038, 95% CI = [1.56, 2.89]) (<xref rid="fig5" ref-type="fig">Figure 5E1 left</xref>). Moreover, a negative correlation was found between N1-P2 amplitude and gesture entropy in both ROIs of LA (<italic>r</italic> = -0.465, <italic>p</italic> = 0.039, 95% CI = [-0.87, -0.06]) and RA (<italic>r</italic> = -0.465, <italic>p</italic> = 0.039, 95% CI = [-0.88, -0.05]) (<xref rid="fig5" ref-type="fig">Figure 5E1 right</xref>). Additionally, through a cluster-permutation test, the N1-P2 effect was identified between 184 to 202 ms at electrodes FC4, FC6, C2, C4, C6, and CP4 (<xref rid="fig5" ref-type="fig">Figure 5E2 middle</xref>) (<italic>t</italic>(22) = 2.638, <italic>p</italic> = 0.015, 95% CI = [1.79, 3.48], (<xref rid="fig5" ref-type="fig">Figure 5E2 left</xref>)), exhibiting a significant correlation with gesture entropy (<italic>r</italic> = -0.485, <italic>p</italic> = 0.030, 95% CI = [-0.91, -0.06], <xref rid="fig5" ref-type="fig">Figure 5E2 right</xref>).</p>
<p>Furthermore, in line with prior research<sup><xref ref-type="bibr" rid="c42">42</xref></sup>, a left-frontal N400 amplitude (250-450 ms) was discerned from topographical maps of gesture entropy (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). Specifically, stimuli with lower 50% values of gesture entropy elicited a larger N400 amplitude in the LA ROI compared to those with higher 50% values (<italic>t</italic>(22) = 2.455, <italic>p</italic> = 0.023, 95% CI = [1.95, 2.96], <xref rid="fig5" ref-type="fig">Figure 5F1 left</xref>). Concurrently, a negative correlation was noted between the N400 amplitude and gesture entropy (<italic>r</italic> = -0.480, <italic>p</italic> = 0.032, 95% CI = [-0.94, -0.03], <xref rid="fig5" ref-type="fig">Figure 5F1 right</xref>) within the LA ROI. The identified clusters showing the N400 effect for gesture entropy (282 – 318 ms at electrodes FC1, FCz, C1, and Cz, <xref rid="fig5" ref-type="fig">Figure 5F2 middle</xref>) (<italic>t</italic>(22) = 2.828, <italic>p</italic> = 0.010, 95% CI = [2.02, 3.64], <xref rid="fig5" ref-type="fig">Figure 5F2 left</xref>) also exhibited significant correlation between the N400 amplitude and gesture entropy (<italic>r</italic> = -0.445, <italic>p</italic> = 0.049, 95% CI = [-0.88, -0.01], <xref rid="fig5" ref-type="fig">Figure 5F2 right</xref>).</p>
<p>Similarly, a left-frontal N400 amplitude (250-450 ms) <sup><xref ref-type="bibr" rid="c42">42</xref></sup> was discerned from topographical maps for MI (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). A larger N400 amplitude in the LA ROI was observed for stimuli with lower 50% values of MI compared to those with higher 50% values (<italic>t</italic>(22) = 3.00, <italic>p</italic> = 0.007, 95% CI = [2.54, 3.46], <xref rid="fig5" ref-type="fig">Figure 5G1 left</xref>). This was accompanied by a significant negative correlation between N400 amplitude and MI (<italic>r</italic> = -0.504, <italic>p</italic> = 0.028, 95% CI = [-0.97, -0.04], <xref rid="fig5" ref-type="fig">Figure 5G1 right</xref>) within the LA ROI. The N400 effect for MI, observed in the 294–306 ms window at electrodes F1, F3, Fz, FC1, FC3, FCz, and C1 (<xref rid="fig5" ref-type="fig">Figure 5G2 middle</xref>) (<italic>t</italic>(22) = 2.461, <italic>p</italic> = 0.023, 95% CI = [1.62, 3.30], <xref rid="fig5" ref-type="fig">Figure 5G2 left</xref>), also showed a significant negative correlation with MI (<italic>r</italic> = -0.569, <italic>p</italic> = 0.011, 95% CI = [-0.98, -0.16], <xref rid="fig5" ref-type="fig">Figure 5G2 right</xref>).</p>
<p>Finally, consistent with previous findings<sup><xref ref-type="bibr" rid="c30">30</xref></sup>, an anterior LPC effect (550-1000 ms) was observed in topographical maps comparing stimuli with lower and higher 50% speech entropy (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). The reduced LPC amplitude was evident in the paired t-tests conducted in ROIs of LA (<italic>t</italic>(22) = 2.614, <italic>p</italic> = 0.016, 95% CI = [1.88, 3.35]); LC (<italic>t</italic>(22) = 2.592, <italic>p</italic> = 0.017, 95% CI = [1.83, 3.35]); RA (<italic>t</italic>(22) = 2.520, <italic>p</italic> = 0.020, 95% CI = [1.84, 3.24]); and ML (<italic>t</italic>(22) = 2.267, <italic>p</italic> = 0.034, 95% CI = [1.44, 3.10]) (<xref rid="fig5" ref-type="fig">Figure 5H1 left</xref>). Simultaneously, a marked negative correlation with speech entropy was evidenced in ROIs of LA (<italic>r</italic> = -0.836, <italic>p</italic> = 0.001, 95% CI = [-1.26, -0.42]); LC (<italic>r</italic> = -0.762, <italic>p</italic> = 0.006, 95% CI = [-1.23, -0.30]); RA (<italic>r</italic> = -0.774, <italic>p</italic> = 0.005, 95% CI = [-1.23, -0.32]) and ML (<italic>r</italic> = -0.730, <italic>p</italic> = 0.011, 95% CI = [-1.22, -0.24]) (<xref rid="fig5" ref-type="fig">Figure 5H1 right</xref>). Additionally, a cluster with the LPC effect (644 - 688 ms at electrodes Cz, CPz, P1, and Pz, <xref rid="fig5" ref-type="fig">Figure 5H2 middle</xref>) (<italic>t</italic>(22) = 2.754, <italic>p</italic> = 0.012, 95% CI = [1.50, 4.01], <xref rid="fig5" ref-type="fig">Figure 5H2 left</xref>) displayed a significant correlation with speech entropy (<italic>r</italic> = -0.699, <italic>p</italic> = 0.017, 95% CI = [-1.24, -0.16], <xref rid="fig5" ref-type="fig">Figure 5H2 right</xref>).</p>
<p>To clarify potential confounds of semantic control difficulty, partial correlation analyses were conducted to examine the relationship between the elicited ERP components and the relevant information matrices, controlling for response numbers. Results consistently indicated modulation by response numbers in the relationship of ERP components with the information matrix, as evidenced by the non-significant partial correlations between the P1 amplitude (P1 component over ML: <italic>r</italic> = -0.574, <italic>p</italic> = 0.082, 95% CI = [-1.141, -0.007]) and the P1 cluster (<italic>r</italic> = -0.503, <italic>p</italic> = 0.138, 95% CI = [-1.102, 0.096]) with speech entropy; the N1-P2 amplitude (N1-P2 component over LA: <italic>r</italic> = -0.080, <italic>p</italic> = 0.746, 95% CI = [-0.554, 0.394]) and N1-P2 cluster (<italic>r</italic> = -0.179, <italic>p</italic> = 0.464, 95% CI = [-0.647, 0.289]) with gesture entropy; the N400 amplitude (N400 component over LA: <italic>r</italic> = 0.264, <italic>p</italic> = 0.247, 95% CI = [-0.195,0.723]) and N400 cluster (<italic>r</italic> = 0.394, <italic>p</italic> = 0.095, 95% CI = [-0.043, 0.831]) with gesture entropy; the N400 amplitude (N400 component over LA: <italic>r</italic> = -0.134, <italic>p</italic> = 0.595, 95% CI = [-0.620, 0.352]) and N400 cluster (<italic>r</italic> = -0.034, <italic>p</italic> = 0.894, 95% CI = [-0.524,0.456]) with MI; and the LPC amplitude (LPC component over LA: <italic>r</italic> = -0.428, <italic>p</italic> = 0.217, 95% CI = [-1.054, 0.198]) and LPC cluster (<italic>r</italic> = -0.202, <italic>p</italic> = 0.575, 95% CI = [-0.881, 0.477]) with speech entropy.</p>
</sec>
</sec>
<sec id="s8">
<title>Discussion</title>
<p>Through mathematical quantification of gesture and speech information using entropy and mutual information (MI), we examined the functional pattern and dynamic neural structure underlying multisensory semantic integration. Our results, for the first time, revealed that the inhibition effect of cathodal-tDCS on the pMTG and IFG correlated with the degree of gesture-speech multisensory convergence, as indexed by MI (<bold>Experiment 1</bold>). Moreover, the gradual neural engagement was found to be time-sensitive and staged, as evidenced by the selectively interrupted time windows (<bold>Experiment 2</bold>) and the distinct correlated ERP components (<bold>Experiment 3</bold>), which were modulated by different information contributors, including unisensory entropy or multisensory MI. These findings significantly expand our understanding of the cortical foundations of statistically regularized multisensory semantic information.</p>
<p>It is widely acknowledged that a single, amodal system mediates the interactions among perceptual representations of different modalities<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup>. Moreover, observations have suggested that semantic dementia patients experience increasing overregularization of their conceptual knowledge due to the progressive deterioration of this amodal system<sup><xref ref-type="bibr" rid="c45">45</xref></sup>. Consistent with this, the present study provides robust evidence, through the application of HD-tDCS and TMS, that the integration hubs for gesture and speech—the pMTG and IFG— operate in an incremental manner. This is supported by the progressive inhibition effect observed in these brain areas as the entropy and mutual information of gesture and speech advances.</p>
<p>Moreover, by dividing the potential integration period into eight time windows (TW) relative to the speech identification point (IP) and administering inhibitory double-pulse TMS across each TW, the current study attributed the gradual TMS-selective regional inhibition to distinct information sources. In TW2 of gesture-speech integration, which precedes the speech identification point<sup><xref ref-type="bibr" rid="c23">23</xref></sup> and represents a pre-lexical stage, the suppression effect observed in the pMTG was correlated with speech entropy. Conversely, during TW6, which follows the speech identification point<sup><xref ref-type="bibr" rid="c23">23</xref></sup> and represents a post-lexical stage, the IFG interruption effect was influenced by both gesture entropy, speech entropy, and their MI. A dual-stage pMTG-IFG-pMTG neurocircuit loop during gesture-speech integration has been proposed previous<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. As an extension, the present study unveils a staged accumulation of engagement within the neurocircuit linking the transmodal regions of pMTG and IFG, arising from distinct contributors of information.</p>
<p>Furthermore, we disentangled the sub-processes of integration with high-temporal ERPs, when representations of gesture and speech were variously presented. Early P1-N1 and P2 sensory effects linked to perception and attentional processes<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c46">46</xref></sup> was comprehended as a reflection of the early audiovisual gesture-speech integration in the sensory-perceptual processing chain<sup><xref ref-type="bibr" rid="c47">47</xref></sup>. Note that a semantic priming paradigm was adopted here to create a top-down prediction of gesture over speech. The observed positive correlation of the P1 effect with speech entropy and the negative correlation of the N1-P2 effect with gesture entropy suggest that the early interaction of gesture-speech information was modulated by both top-down gesture prediction and bottom-up speech processing. Additionally, the lexico-semantic effect of the N400 and the LPC were differentially mediated by top-down gesture prediction, bottom-up speech encoding and their interaction: the N400 was negatively correlated with both the gesture entropy and MI, but the LPC was negatively correlated only with the speech entropy.</p>
<p>The varying contributions of unisensory gesture-speech information and the convergence of multisensory inputs, as reflected in the correlation between distinct ERP components and TMS time windows (TMS TWs), are consistent with recent models suggesting that multisensory processing involves parallel detection of modality-specific information and hierarchical integration across multiple neural levels<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup>. These processes are further characterized by coordination across multiple temporal scales<sup><xref ref-type="bibr" rid="c49">49</xref></sup>. Building on this, the present study offers additional evidence that the multi-level nature of gesture-speech processing is statistically structured, as measured by information matrix of unisensory entropy and multisensory convergence index of MI, the input of either source would activate a distributed representation, resulting in progressively functioning neural responses.</p>
<p>Given that control processes are intrinsically integrated with semantic processing<sup><xref ref-type="bibr" rid="c50">50</xref></sup>, a distributed semantic representation enables dynamic modulation of access to and manipulation of meaningful information, thereby facilitating flexible control over the diverse possibilities inherent in a concept. Accordingly, an increased number of candidate responses amplifies the control demands necessary to resolve competing semantic representations. This effect was observed in the present study, where the association of the information matrix with the tDCS effect in IFG, the inhibition of pMTG activity in TW2, disruption of IFG activity in TW6, and modulation of four distinct ERP components collectively demonstrated that response quantity modulates neural activity. These results underscore the intricate interplay between the difficulty of semantic representation and the control pressures that shape the resulting neural responses.</p>
<p>The IFG and pMTG, central components of the semantic control network, have been extensively implicated in previous research <sup><xref ref-type="bibr" rid="c50">50</xref>–<xref ref-type="bibr" rid="c52">52</xref></sup>. While the role of the IFG in managing both unisensory information and multisensory convergence remains consistent, as evidenced by the confounding difficulty results across Experiments 1 and 2, the current study highlights a more context-dependent function for the pMTG. Specifically, although the pMTG is well-established in the processing of distributed speech information, the multisensory convergence, indexed by MI, did not evoke the same control-related modulation in pMTG activity. These findings suggest that, while the pMTG is critical to semantic processing, its engagement in control processes is likely modulated by the specific nature of the sensory inputs involved.</p>
<p>Considering the close alignment of the ERP components with the TWs of TMS effect, it is reasonable to speculate the ERP components with the cortical involvements (<xref rid="fig6" ref-type="fig">Figure 6</xref>). Consequently, referencing the recurrent neurocircuit connecting the left IFG and pMTG for semantic unification<sup><xref ref-type="bibr" rid="c53">53</xref></sup>, we extended the previously proposed two-stage gesture-speech integration circuit<sup><xref ref-type="bibr" rid="c23">23</xref></sup> into sequential steps. First, bottom-up speech processing mapping acoustic signal to its lexical representation was performed to the pMTG. The larger speech entropy was, the greater effort was made during the matching of the acoustic input with its stored lexical representation, thus leading to a larger involvement of the pMTG at pre-lexical stage (TW2) and a larger P1 effect (<xref rid="fig6" ref-type="fig">Figure 6①</xref>). Second, the gesture representation was activated in the pMTG and further exerted a top-down modulation over the phonological processing of speech <sup><xref ref-type="bibr" rid="c54">54</xref></sup>. The higher the certainty of gesture is, a larger modulation of gesture would be made upon speech, as indexed by a smaller gesture entropy with an enhanced N1-P2 amplitude (<xref rid="fig6" ref-type="fig">Figure 6②</xref>). Third, information was relayed from the pMTG to the IFG for sustained activation, during which a semantic constraint from gesture has been made on the semantic retrieval of speech. Greater TMS inhibitory effect over the IFG at post-lexical stage (TW6) accompanying with a reduced N400 amplitude were found with the increase of gesture entropy, when the representation of gesture was wildly distributed and the constrain over the following speech was weak (<xref rid="fig6" ref-type="fig">Figure 6③</xref>). Fourth, the activated speech representation was compared with that of the gesture in the IFG. At this stage, the larger, overlapped neural populations activated by gesture and speech as indexed by a larger MI is, a greater TMS disruption effect of the IFG and a reduced N400 amplitude indexing easier integration and less semantic conflict were observed (<xref rid="fig6" ref-type="fig">Figure 6④</xref>). Last, the activated speech representation would disambiguate and reanalyze the semantic information and further unify into a coherent comprehension in the pMTG<sup><xref ref-type="bibr" rid="c12">12</xref>,<xref ref-type="bibr" rid="c37">37</xref></sup>. As speech entropy increases, indicating greater uncertainty in the information provided by speech, more cognitive effort is directed towards selecting the targeted semantic representation. This leads to enhanced involvement of the IFG and a corresponding reduction in LPC amplitude (<xref rid="fig6" ref-type="fig">Figure 6⑤</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Progressive processing stages of gesture–speech information within the pMTG-IFG loop.</title>
<p>Correlations between the TMS disruption effect of pMTG and IFG with three information models are represented by the orange line and the green lines, respectively. Black lines denote the strongest correlations of ROI averaged ERP components with three information models. * p &lt; 0.05, ** p &lt; 0.01 after FDR correction.</p></caption>
<graphic xlink:href="517759v4_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Note that the sequential cortical involvement and ERP components discussed above are derived from a deliberate alignment of speech onset with gesture DP, creating an artificial priming effect with gesture semantically preceding speech. Caution is advised when generalizing these findings to the spontaneous gesture-speech relationships, although gestures naturally precede speech<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. Furthermore, MI quantifies overlap in gesture-speech integration, primarily when gestures convey redundant meaning. Consequently, the conclusions drawn in this study are constrained to contexts in which gestures serve to reinforce the meaning of the speech. Future research should aim to explore the neural responses in cases where gestures convey supplementary, rather than redundant, semantic information.</p>
<p>Limitations exist. ERP components and cortical engagements were linked through intermediary variables of entropy and MI. Dissociations were observed between ERP components and cortical engagement. Importantly, there is no direct evidence of the brain structures underpinning the corresponding ERPs, necessitating clarification in future studies. Additionally, not all influenced TWs exhibited significant associations with entropy and MI. While HD-tDCS and TMS may impact functionally and anatomically connected brain regions<sup><xref ref-type="bibr" rid="c55">55</xref>,<xref ref-type="bibr" rid="c56">56</xref></sup>, whether the absence of influence in certain TWs can be attributed to compensation by other connected brain areas, such as angular gyrus<sup><xref ref-type="bibr" rid="c57">57</xref></sup> or anterior temporal lobe<sup><xref ref-type="bibr" rid="c58">58</xref></sup>, warrants further investigation. Therefore, caution is needed when interpreting the causal relationship between inhibition effects of brain stimulation and information-theoretic metrics (entropy and MI). Finally, the current study incorporated a restricted set of entropy and MI measures. The generalizability of the findings should be assessed in future studies using a more extensive range of matrices.</p>
<p>In summary, utilizing information-theoretic complexity metrics such as entropy and mutual information (MI), our study demonstrates that multisensory semantic processing, involving gesture and speech, gives rise to dynamically evolving representations through the interplay between gesture-primed prediction and speech presentation. This process correlates with the progressive engagement of the pMTG-IFG-pMTG circuit and various ERP components. These findings significantly advancing our understanding of the neural mechanisms underlying multisensory semantic integration.</p>
</sec>

</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This research was supported by grants from the STI 2030—Major Projects 2021ZD0201500, the National Natural Science Foundation of China (31822024, 31800964), the Scientific Foundation of Institute of Psychology, Chinese Academy of Sciences (E2CX3625CX), and the Strategic Priority Research Program of Chinese Academy of Sciences (XDB32010300).</p>
</ack>
<sec id="d1e1734" sec-type="additional-information">
<title>Additional information</title>
<sec id="s9">
<title>Author contributions</title>
<p>Conceptualization, W.Y.Z. and Y.D.; Investigation, W.Y.Z. and Z.Y.L.; Formal Analysis, W.Y.Z. and Z.Y.L.; Methodology, W.Y.Z. and Z.Y.L.; Validation, Z.Y.L. and X.L.; Visualization, W.Y.Z. and Z.Y.L. and X.L.; Funding Acquisition, W.Y.Z. and Y.D.; Supervision, Y.D.; Project administration, Y.D.; Writing – Original Draft, W.Y.Z.; Writing – Review &amp; Editing, W.Y.Z., Z.Y.L., X.L., and Y.D.</p>
</sec>
</sec>
<sec id="suppd1e1734" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Appendix Table 1, Appendix Table 2, Appendix Table 3</label>
<media xlink:href="supplements/517759_file03.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Damasio</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Grabowski</surname>, <given-names>T.J.</given-names></string-name>, <string-name><surname>Tranel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hichwa</surname>, <given-names>R.D.</given-names></string-name>, and <string-name><surname>Damasio</surname>, <given-names>A.R</given-names></string-name></person-group>. (<year>1996</year>). <article-title>A neural basis for lexical retrieval</article-title>. <source>Nature</source> <volume>380</volume>, <fpage>499</fpage>–<lpage>505</lpage>. DOI <pub-id pub-id-type="doi">10.1038/380499a0</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Nestor</surname>, <given-names>P.J.</given-names></string-name>, and <string-name><surname>Rogers</surname>, <given-names>T.T</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Where do you know what you know? The representation of semantic knowledge in the human brain</article-title>. <source>Nature Reviews Neuroscience</source> <volume>8</volume>, <fpage>976</fpage>–<lpage>987</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2277</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brennan</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Stabler</surname>, <given-names>E.P.</given-names></string-name>, <string-name><surname>Van Wagenen</surname>, <given-names>S.E.</given-names></string-name>, <string-name><surname>Luh</surname>, <given-names>W.M.</given-names></string-name>, and <string-name><surname>Hale</surname>, <given-names>J.T.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Abstract linguistic structure correlates with temporal activity during naturalistic comprehension</article-title>. <source>Brain and Language</source> <volume>157</volume>, <fpage>81</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2016.04.008</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benetti</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Pavani</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Multimodal processing in face-to-face interactions: A bridging link between psycholinguistics and sensory neuroscience</article-title>. <source>Front Hum Neurosci</source> <volume>17</volume>, <fpage>1108354</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2023.1108354</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noppeney</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Perceptual Inference, Learning, and Attention in a Multisensory World</article-title>. <source>Annual Review of Neuroscience, Vol 44, 2021</source> <volume>44</volume>, <fpage>449</fpage>–<lpage>473</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-100120-085519</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname>, <given-names>W.J.</given-names></string-name>, and <string-name><surname>Jazayeri</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Neural coding of uncertainty and probability</article-title>. <source>Annu Rev Neurosci</source> <volume>37</volume>, <fpage>205</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-014017</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischer</surname>, <given-names>B.J.</given-names></string-name>, and <string-name><surname>Pena</surname>, <given-names>J.L</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Owl’s behavior and neural representation predicted by Bayesian inference</article-title>. <source>Nat Neurosci</source> <volume>14</volume>, <fpage>1061</fpage>–<lpage>1066</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2872</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ganguli</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Simoncelli</surname>, <given-names>E.P</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Efficient sensory encoding and Bayesian inference with heterogeneous neural populations</article-title>. <source>Neural Comput</source> <volume>26</volume>, <fpage>2103</fpage>–<lpage>2134</lpage>. <pub-id pub-id-type="doi">10.1162/NECO_a_00638</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hostetter</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Mainela-Arnold</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Gestures occur with spatial and Motoric knowledge: It’s more than just coincidence</article-title>. <source>Perspectives on Language Learning and Education</source> <volume>22</volume>, <fpage>42</fpage>–<lpage>49</lpage>. doi:<pub-id pub-id-type="doi">10.1044/lle22.2.42</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McNeill</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2005</year>). <source>Gesture and though</source> (<publisher-name>University of Chicago Press</publisher-name>). <pub-id pub-id-type="doi">10.7208/chicago/9780226514642.001.0001</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kendon</surname>, <given-names>A</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Gesture</article-title>. <source>Annu Rev Anthropol</source> <volume>26</volume>, <fpage>109</fpage>–<lpage>128</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.anthro.26.1.109</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2005</year>). <article-title>On broca, brain, and binding: a new framework</article-title>. <source>Trends in Cognitive Sciences</source> <volume>9</volume>, <fpage>416</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2005.07.004</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hald</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bastiaansen</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Petersson</surname>, <given-names>K.M</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Integration of word meaning and world knowledge in language comprehension</article-title>. <source>Science</source> <volume>304</volume>, <fpage>438</fpage>–<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1126/science.1095455</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Willems</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2007</year>). <article-title>On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials</article-title>. <source>J Cognitive Neurosci</source> <volume>19</volume>, <fpage>605</fpage>–<lpage>616</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.4.605</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willems</surname>, <given-names>R.M.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Differential roles for left inferior frontal and superior temporal cortex in multimodal integration of action and language</article-title>. <source>Neuroimage</source> <volume>47</volume>, <fpage>1992</fpage>–<lpage>2004</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.05.066</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Spaak</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Rapid invisible frequency tagging reveals nonlinear integration of auditory and visual information</article-title>. <source>Human Brain Mapping</source> <volume>42</volume>, <fpage>1138</fpage>–<lpage>1152</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25282</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Ozyurek</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Native language status of the listener modulates the neural integration of speech and iconic gestures in clear and adverse listening conditions</article-title>. <source>Brain and Language</source> <volume>177</volume>, <fpage>7</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandl.2018.01.003</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drijvers</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>van der Plas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Native and non-native listeners show similar yet distinct oscillatory dynamics when using gestures to access speech in noise</article-title>. <source>Neuroimage</source> <volume>194</volume>, <fpage>55</fpage>–<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.032</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holle</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Gunter</surname>, <given-names>T.C</given-names></string-name></person-group>. (<year>2007</year>). <article-title>The role of iconic gestures in speech disambiguation: ERP evidence</article-title>. <source>J Cognitive Neurosci</source> <volume>19</volume>, <fpage>1175</fpage>–<lpage>1192</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.7.1175</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Ozyurek</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2003</year>). <article-title>What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking</article-title>. <source>J Mem Lang</source> <volume>48</volume>, <fpage>16</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1016/S0749-596x(02)00505-3</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernardis</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Gentilucci</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Speech and gesture share the same communication system</article-title>. <source>Neuropsychologia</source> <volume>44</volume>, <fpage>178</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.05.007</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>W.Y.</given-names></string-name>, <string-name><surname>Riggs</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schindler</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Holle</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Transcranial magnetic stimulation over left inferior frontal and posterior temporal cortex disrupts gesture-speech integration</article-title>. <source>Journal of Neuroscience</source> <volume>38</volume>, <fpage>1891</fpage>–<lpage>1900</lpage>. <pub-id pub-id-type="doi">10.1523/Jneurosci.1748-17.2017</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Du</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2021</year>). <article-title>TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration</article-title>. <source>The Journal of Neuroscience</source>, <fpage>10356</fpage>–<lpage>10364</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.1355-21.2021</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shannon</surname>, <given-names>C.E</given-names></string-name></person-group>. (<year>1948</year>). <article-title>A mathematical theory of communication</article-title>. <source>Bell Syst Tech J</source> <volume>27</volume>, <fpage>379</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tremblay</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Deschamps</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Baroni</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Hasson</surname>, <given-names>U</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Neural sensitivity to syllable frequency and mutual information in speech perception and production</article-title>. <source>Neuroimage</source> <volume>136</volume>, <fpage>106</fpage>–<lpage>121</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.05.018</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bikson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Inoue</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Akiyama</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Deans</surname>, <given-names>J.K.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>J.E.</given-names></string-name>, <string-name><surname>Miyakawa</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Jefferys</surname>, <given-names>J.G.R</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Effects of uniform extracellular DC electric fields on excitability in rat hippocampal slices</article-title>. <source>J Physiol-London</source> <volume>557</volume>, <fpage>175</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.2003.055772</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Federmeier</surname>, <given-names>K.D.</given-names></string-name>, <string-name><surname>Mai</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Kutas</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Both sides get the point: hemispheric sensitivities to sentential constraint</article-title>. <source>Memory &amp; Cognition</source> <volume>33</volume>, <fpage>871</fpage>–<lpage>886</lpage>. <pub-id pub-id-type="doi">10.3758/bf03193082</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname>, <given-names>S.D.</given-names></string-name>, <string-name><surname>Kravitz</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Hopkins</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Neural correlates of bimodal speech and gesture comprehension</article-title>. <source>Brain and Language</source> <volume>89</volume>, <fpage>253</fpage>–<lpage>260</lpage>. <pub-id pub-id-type="doi">10.1016/s0093-934x(03)00335-3</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname>, <given-names>Y.C.</given-names></string-name>, and <string-name><surname>Coulson</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Meaningful gestures: Electrophysiological indices of iconic gesture comprehension</article-title>. <source>Psychophysiology</source> <volume>42</volume>, <fpage>654</fpage>–<lpage>667</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.2005.00356.x</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fritz</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Littlemore</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Krott</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Multimodal language processing: How preceding discourse constrains gesture interpretation and affects gesture integration when gestures do not synchronise with semantic affiliates</article-title>. <source>J Mem Lang</source> <volume>117</volume>, <fpage>104191</fpage>. <pub-id pub-id-type="doi">10.1016/j.jml.2020.104191</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gunter</surname>, <given-names>T.C.</given-names></string-name>, and <string-name><surname>Weinbrenner</surname>, <given-names>J.E.D</given-names></string-name></person-group>. (<year>2017</year>). <article-title>When to take a gesture seriously: On how we use and prioritize communicative cues</article-title>. <source>J Cognitive Neurosci</source> <volume>29</volume>, <fpage>1355</fpage>–<lpage>1367</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01125</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oldfield</surname>, <given-names>R.C</given-names></string-name></person-group>. (<year>1971</year>). <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source> <volume>9</volume>, <fpage>97</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>W</given-names></string-name></person-group>. (<year>2023</year>). <article-title>TMS reveals a two-stage priming circuit of gesture-speech integration</article-title>. <source>Front Psychol</source> <volume>14</volume>, <fpage>1156087</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2023.1156087</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McNeill</surname>, <given-names>D.</given-names></string-name></person-group> (<year>1992</year>). <source>Hand and mind: what gestures reveal about thought</source> (<publisher-name>University of Chicago Press</publisher-name>). <pub-id pub-id-type="doi">10.2307/1576015</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname>, <given-names>S.D.</given-names></string-name>, <string-name><surname>Creigh</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Bartolotti</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Integrating speech and iconic gestures in a Stroop-like task: Evidence for automatic processing</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>22</volume>, <fpage>683</fpage>–<lpage>694</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2009.21254</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koessler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Maillard</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Benhadid</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Vignal</surname>, <given-names>J.P.</given-names></string-name>, <string-name><surname>Felblinger</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Vespignani</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Braun</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Automated cortical projection of EEG sensors: Anatomical correlation via the international 10-10 system</article-title>. <source>Neuroimage</source> <volume>46</volume>, <fpage>64</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.006</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tesink</surname>, <given-names>C.M.J.Y.</given-names></string-name>, <string-name><surname>Petersson</surname>, <given-names>K.M.</given-names></string-name>, <string-name><surname>van Berkum</surname>, <given-names>J.J.A.</given-names></string-name>, <string-name><surname>van den Brink</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Buitelaar</surname>, <given-names>J.K.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Unification of speaker and meaning in language comprehension: An fMRI study</article-title>. <source>J Cognitive Neurosci</source> <volume>21</volume>, <fpage>2085</fpage>–<lpage>2099</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2008.21161</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nuwer</surname>, <given-names>M.R.</given-names></string-name>, <string-name><surname>Comi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Emerson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fuglsang-Frederiksen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Guerit</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Hinrichs</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ikeda</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Luccas</surname>, <given-names>F.J.</given-names></string-name>, and <string-name><surname>Rappelsberger</surname>, <given-names>P</given-names></string-name></person-group>. (<year>1999</year>). <article-title>IFCN standards for digital recording of clinical EEG. The International Federation of Clinical Neurophysiology</article-title>. <source>Electroencephalogr Clin Neurophysiol Suppl</source> <volume>52</volume>, <fpage>11</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1016/S0013-4694(97)00106-5</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delorme</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Makeig</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2004</year>). <article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>J Neurosci Methods</source> <volume>134</volume>, <fpage>9</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Habets</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Shao</surname>, <given-names>Z.S.</given-names></string-name>, <string-name><surname>Ozyurek</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2011</year>). <article-title>The Role of Synchrony and Ambiguity in Speech-Gesture Integration during Comprehension</article-title>. <source>J Cognitive Neurosci</source> <volume>23</volume>, <fpage>1845</fpage>–<lpage>1854</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2010.21462</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oostenveld</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fries</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Maris</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Schoffelen</surname>, <given-names>J.-M</given-names></string-name></person-group>. (<year>2011</year>). <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Computational Intelligence and Neuroscience</source> <volume>2011</volume>, <fpage>156869</fpage>. <pub-id pub-id-type="doi">10.1155/2011/156869</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kutas</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Federmeier</surname>, <given-names>K.D</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Thirty Years and Counting: Finding Meaning in the N400 Component of the Event-Related Brain Potential (ERP)</article-title>. <source>Annual Review of Psychology, Vol 62</source> <volume>62</volume>, <fpage>621</fpage>–<lpage>647</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname>, <given-names>T.T.</given-names></string-name>, <string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, <string-name><surname>Garrard</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bozeat</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>McClelland</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Hodges</surname>, <given-names>J.R.</given-names></string-name>, and <string-name><surname>Patterson</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Structure and deterioration of semantic memory: A neuropsychological and computational investigation</article-title>. <source>Psychological Review</source> <volume>111</volume>, <fpage>205</fpage>–<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295x.111.1.205</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, <string-name><surname>Jefferies</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Patterson</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Rogers</surname>, <given-names>T.T</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The neural and computational bases of semantic cognition</article-title>. <source>Nature Reviews Neuroscience</source> <volume>18</volume>, <fpage>42</fpage>–<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1038/nrn.2016.150</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname>, <given-names>T.T.</given-names></string-name>, <string-name><surname>Hodges</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Ralph</surname>, <given-names>M.A.L.</given-names></string-name>, and <string-name><surname>Patterson</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Object recognition under semantic impairment: The effects of conceptual regularities on perceptual decisions</article-title>. <source>Lang Cognitive Proc</source> <volume>18</volume>, <fpage>625</fpage>–<lpage>662</lpage>. <pub-id pub-id-type="doi">10.1080/01690960344000053</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fadiga</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Craighero</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Olivier</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Human motor cortex excitability during the perception of others’ action</article-title>. <source>Current Opinion in Neurobiology</source> <volume>15</volume>, <fpage>213</fpage>–<lpage>218</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2005.03.013</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giard</surname>, <given-names>M.H.</given-names></string-name>, and <string-name><surname>Peronnet</surname>, <given-names>F</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Auditory-visual integration during multimodal object recognition in humans: A behavioral and electrophysiological study</article-title>. <source>J Cognitive Neurosci</source> <volume>11</volume>, <fpage>473</fpage>–<lpage>490</lpage>. <pub-id pub-id-type="doi">10.1162/089892999563544</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meijer</surname>, <given-names>G.T.</given-names></string-name>, <string-name><surname>Mertens</surname>, <given-names>P.E.C.</given-names></string-name>, <string-name><surname>Pennartz</surname>, <given-names>C.M.A.</given-names></string-name>, <string-name><surname>Olcese</surname>, <given-names>U.</given-names></string-name>, and <string-name><surname>Lansink</surname>, <given-names>C.S</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The circuit architecture of cortical multisensory processing: Distinct functions jointly operating within a common anatomical network</article-title>. <source>Prog Neurobiol</source> <volume>174</volume>, <fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1016/j.pneurobio.2019.01.004</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Senkowski</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Engel</surname>, <given-names>A.K</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Multi-timescale neural dynamics for multisensory integration</article-title>. <source>Nat Rev Neurosci</source> <volume>25</volume>, <fpage>625</fpage>–<lpage>642</lpage>. <pub-id pub-id-type="doi">10.1038/s41583-024-00845-7</pub-id>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jackson</surname>, <given-names>R.L</given-names></string-name></person-group>. (<year>2021</year>). <article-title>The neural correlates of semantic control revisited</article-title>. <source>Neuroimage</source> <volume>224</volume>, <fpage>117444</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117444</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jefferies</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2013</year>). <article-title>The neural basis of semantic cognition: converging evidence from neuropsychology, neuroimaging and TMS</article-title>. <source>Cortex</source> <volume>49</volume>, <fpage>611</fpage>–<lpage>625</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2012.10.008</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noonan</surname>, <given-names>K.A.</given-names></string-name>, <string-name><surname>Jefferies</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Visser</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Lambon Ralph</surname>, <given-names>M.A</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Going beyond inferior prefrontal involvement in semantic control: evidence for the additional contribution of dorsal angular gyrus and posterior middle temporal cortex</article-title>. <source>J Cogn Neurosci</source> <volume>25</volume>, <fpage>1824</fpage>–<lpage>1850</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00442</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hagoort</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2013</year>). <article-title>MUC (Memory, Unification, Control) and beyond</article-title>. <source>Frontiers in Psychology</source> <volume>4</volume>, <fpage>416</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00416</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bizley</surname>, <given-names>J.K.</given-names></string-name>, <string-name><surname>Maddox</surname>, <given-names>R.K.</given-names></string-name>, and <string-name><surname>Lee</surname>, <given-names>A.K.C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Defining auditory-visual objects: Behavioral tests and physiological mechanisms</article-title>. <source>Trends in Neurosciences</source> <volume>39</volume>, <fpage>74</fpage>–<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2015.12.007</pub-id>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartwigsen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bzdok</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wawrzyniak</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Stockert</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wrede</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Classen</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Saur</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Rapid short-term reorganization in the language network</article-title>. <source>eLife</source> <volume>6</volume>. <pub-id pub-id-type="doi">10.7554/eLife.25964</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jackson</surname>, <given-names>R.L.</given-names></string-name>, <string-name><surname>Hoffman</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pobric</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Ralph</surname>, <given-names>M.A.L</given-names></string-name></person-group>. (<year>2016</year>). <article-title>The semantic network at work and rest: Differential connectivity of anterior temporal lobe subregions</article-title>. <source>Journal of Neuroscience</source> <volume>36</volume>, <fpage>1490</fpage>–<lpage>1501</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2999-15.2016</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Humphreys</surname>, <given-names>G. F.</given-names></string-name>, <string-name><surname>Lambon Ralph</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Simons</surname>, <given-names>J. S</given-names></string-name></person-group>. (<year>2021</year>). <article-title>A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition</article-title>. <source>Trends in neurosciences</source>, <volume>44</volume>(<issue>6</issue>), <fpage>452</fpage>–<lpage>463</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2021.01.006</pub-id></mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonner</surname>, <given-names>M. F.</given-names></string-name>, &amp; <string-name><surname>Price</surname>, <given-names>A. R</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Where is the anterior temporal lobe and what does it do?</article-title>. <source>The Journal of neuroscience: the official journal of the Society for Neuroscience</source>, <volume>33</volume>(<issue>10</issue>), <fpage>4213</fpage>–<lpage>4215</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0041-13.2013</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>useful</bold> study uses brain stimulation and electroencephalography to study speech-gesture integration. It investigates the role of frontotemporal regions in integrating linguistic and extra-linguistic information during communication, focusing on the inferior frontal gyrus and posterior middle temporal gyrus. Reliance on activation patterns of tightly-coupled brain regions over short timescales leads to <bold>incomplete</bold> support for the study's conclusions due to conceptual and methodological limitations.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors quantified information in gesture and speech, and investigated the neural processing of speech and gestures in pMTG and LIFG, depending on their informational content, in 8 different time-windows, and using three different methods (EEG, HD-tDCS and TMS). They found that there is a time-sensitive and staged progression of neural engagement that is correlated with the informational content of the signal (speech/gesture).</p>
<p>Strengths:</p>
<p>A strength of the paper is that the authors attempted to combine three different methods to investigate speech-gesture processing.</p>
<p>Comments on revisions:</p>
<p>I thank the authors for their careful responses to my comments. However, I remain not convinced by their argumentation regarding the specificity of their spatial targeting and the time-windows that they used.</p>
<p>The authors write that since they included a sham TMS condition, that the TMS selectively disrupted the IFG-pMTG interaction during specific time windows of the task related to gesture-speech semantic congruency. This to me does not show anything about the specificity of the time-windows itself, nor the selectivity of targeting in the TMS condition.</p>
<p>It could still equally well be the case that other regions or networks relevant for gesture-speech integration are targeted, and it can still be the case that these timewindows are not specific, and effects bleed into other time periods. There seems to be no experimental evidence here that this is not the case.</p>
<p>To be more specific, the authors write that double-pulse TMS has been widely used in previous studies (as found in their table). However, the studies cited in the table do not necessarily demonstrate the level of spatial and temporal specificity required to disentangle the contributions of tightly-coupled brain regions like the IFG and pMTG during the speech-gesture integration process. pMTG and IFG are located in very close proximity, and are known to be functionally and structurally interconnected, something that is not necessarily the case for the relatively large and/or anatomically distinct areas that the authors mention in their table.</p>
<p>But also more in general: The mere fact that these methods have been used in other contexts does not necessarily mean they are appropriate or sufficient for investigating the current research question. Likewise, the cognitive processes involved in these studies are quite different from the complex, multimodal integration of gesture and speech. The authors have not provided a strong theoretical justification for why the temporal dynamics observed in these previous studies should generalize to the specific mechanisms of gesture-speech integration.</p>
<p>Moreover, the studies cited in the table provided by the authors have used a wide range of interpulse intervals, from 20 ms to 100 ms, suggesting that the temporal precision required to capture the dynamics of gesture-speech integration (which is believed to occur within 200-300 ms; Obermeier &amp; Gunter, 2015) may not even be achievable with their 40 ms time windows.</p>
<p>I do appreciate the extra analyses that the authors mention. However, my 5th comment is still unanswered: why not use entropy scores as a continous measure?</p>
<p>In light of these concerns, I do not believe the authors have adequately demonstrated the spatial and temporal specificity required to disentangle the contributions of the IFG and pMTG during the gesture-speech integration process. While the authors have made a sincere effort to address the concerns raised by the reviewers, and have done so with a lot of new analyses, I remain doubtful that the current methodological approach is sufficient to draw conclusions about the causal roles of the IFG and pMTG in gesture-speech integration.</p>
<p>Reference:</p>
<p>
Obermeier, C., &amp; Gunter, T. C. (2015). Multisensory Integration: The Case of a Time Window of Gesture-Speech Integration. Journal of Cognitive Neuroscience, 27(2), 292-307. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00688">https://doi.org/10.1162/jocn_a_00688</ext-link></p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>The study is an innovative and fundamental study that clarified important aspects of brain processes for integration of information from speech and iconic gesture (i.e., gesture that depicts action, movement, and shape), based on tDCS, TMS and EEG experiments. They evaluated their speech and gesture stimuli in information-theoretic ways and calculated how informative speech is (i.e., entropy), how informative gesture is, and how much shared information speech and gesture encode. The tDCS and TMS studies found that the left IFG and pMTG, the two areas that were activated in fMRI studies on speech-gesture integration in the previous literature, are causally implicated in speech-gesture integration. The size of tDC and TMS effects are correlated with entropy of the stimuli or mutual information, which indicates that the effects stems from the modulation of information decoding/integration processes. The EEG study showed that various ERP (event-related potential, e.g., N1-P2, N400, LPC) effects that have been observed in speech-gesture integration experiments in the previous literature are modulated by the entropy of speech/gesture and mutual information. This makes it clear that these effects are related to information decoding processes. The authors propose a model of how speech-gesture integration process unfolds in time, and how IFG and pMTG interact with each other in that process.</p>
<p>Strengths</p>
<p>The key strength of this study is that the authors used information-theoretic measures of their stimuli (i.e., entropy and mutual information between speech and gesture) in all of their analyses. This made it clear that the neuro-modulation (tDCS, TMS) affected information decoding/integration and ERP effects reflect information decoding/integration. This study used tDCS and TMS methods to demonstrate that left IFG and pMTG are causally involved in speech-gesture integration. The size of tDCS and TMS effects are correlated with information-theoretic measures of the stimuli, which indicate that the effects indeed stem from disruption/facilitation of information decoding/integration process (rather than generic excitation/inhibition). The authors' results also showed correlation between information-theoretic measures of stimuli with various ERP effects. This indicates that these ERP effects reflect the information decoding/integration process.</p>
<p>Weakness</p>
<p>The &quot;mutual information&quot; cannot capture all types of interplay of the meaning of speech and gesture. The mutual information is calculated based on what information can be decoded from speech alone and what information can be decoded from gesture alone. However, when speech and gesture are combined, a novel meaning can emerge, which cannot be decoded from a single modality alone. When example, a person produce a gesture of writing something with a pen, while saying &quot;He paid&quot;. The speech-gesture combination can be interpreted as &quot;paying by signing a cheque&quot;. It is highly unlikely that this meaning is decoded when people hear speech only or see gestures only. The current study cannot address how such speech-gesture integration occur in the brain, and what ERP effects may reflect such a process. The future studies can classify different types of speech-gesture integration and investigate neural processes that underlie each type. Another important topic for future studies is to investigate how the neural processes of speech-gesture integration change when the relative timing between the speech stimulus and the gesture stimulus changes.</p>
<p>Comments on revisions: The authors addressed my concerns well.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99416.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhao</surname>
<given-names>Wanying</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6979-940X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Zhouyi</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Xiang</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Du</surname>
<given-names>Yi</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4512-5221</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the previous reviews</p>
<p><bold>Responses to Editors:</bold></p>
<p>We appreciate the editors’ concern regarding the difficulty of disentangling the contributions of tightly-coupled brain regions to the speech-gesture integration process—particularly due to the close temporal and spatial proximity of the stimulation windows and the potential for prolonged disruption. While we agree with that stimulation techniques, such as transcranial magnetic stimulation (TMS), can evoke or modulate neuronal activity both locally within the target region and in remote connected areas of the network. This complex interaction makes drawing clear conclusions about the causal relationship between stimulation and cognitive function more challenging. However, we believe that cause-and-effect relationships in cognitive neuroscience studies using non-invasive brain stimulation (NIBS) can still be robustly established if key assumptions are explicitly tested and confounding factors are rigorously controlled (Bergmann &amp; Hartwigsen et al., 2021, J Cogn Neurosci).</p>
<p>In our experiment, we addressed these concerns by including a sham TMS condition, an irrelevant control task, and multiple control time points. The results showed that TMS selectively disrupted the IFG-pMTG interaction during specific time windows of the task related to gesture-speech semantic congruency, but not in the sham TMS condition or the control task (gender congruency effect) (Zhao et al., 2021, JN). This selective disruption provides strong evidence for a causal link between IFG-pMTG connectivity and gesture-speech integration in the targeted time window.</p>
<p>Regarding the potential for transient artifacts from TMS, we acknowledge that previous research has demonstrated that single-pulse TMS induces brief artifacts (0–10 ms) due to direct depolarization of cortical neurons, which momentarily disrupts electrical activity in the stimulated area (Romero et al., 2019, NC). However, in the case of paired-pulse TMS (ppTMS), the interaction between the first and second pulses is more complex. The first pulse increases membrane conductance in the target neurons via shunting inhibition mediated by GABAergic interneurons. This effectively lowers neuronal membrane resistance, “leaking” excitatory current and diminishing the depolarization induced by the second pulse, leading to a reduction in excitability during the paired-pulse interval. This mechanism suppresses the excitatory response to the second pulse, which is reflected in a reduced motor evoked potential (MEP) (Paulus &amp; Rothwell, 2016, J Physiol).</p>
<p>Furthermore, ppTMS has been widely used in previous studies to infer causal temporal relationships and explore the neural contributions of both structurally and functionally connected brain regions, across timescales as brief as 3–60 ms. We have reviewed several studies that employed paired-pulse TMS to investigate neural dynamics in regions such as the tongue and lip areas of the primary motor cortex (M1), as well as high-level semantic regions like the pMTG, PFC, and ATL (Table 1). These studies consistently demonstrate the methodological rigor and precision of double-pulse TMS in elucidating the temporal dynamics between different brain regions within short temporal windows.</p>
<p>Given these precedents and the evidence provided, we respectfully assert the validity of the methods employed in our study. We therefore kindly request the editors to reconsider the assessment that “the methods are insufficient for studying tightly-coupled brain regions over short timescales.” We hope that the editors’ concerns about the complexities of TMS-induced effects have been adequately addressed, and that our study’s design and results provide a clear and convincing causal argument for the role of IFG-pMTG in gesture-speech integration.</p>
<table-wrap id="sa3table1">
<label>Author response table 1.</label>
<caption>
<title>Double-pulse TMS studies on brain regions over 3-60 ms time interval</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table1.jpg" mimetype="image"/>
</table-wrap>
<disp-formula id="sa3equ1">
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-equ1.jpg" mimetype="image"/>
</disp-formula>
<p>Reference</p>
<p>Teige, C., Mollo, G., Millman, R., Savill, N., Smallwood, J., Cornelissen, P. L., &amp; Jefferies, E. (2018). Dynamic semantic cognition: Characterising coherent and controlled conceptual retrieval through time using magnetoencephalography and chronometric transcranial magnetic stimulation. Cortex, 103, 329-349.</p>
<p>Amemiya, T., Beck, B., Walsh, V., Gomi, H., &amp; Haggard, P. (2017). Visual area V5/hMT+ contributes to perception of tactile motion direction: a TMS study. Scientific reports, 7(1), 40937.</p>
<p>Muessgens, D., Thirugnanasambandam, N., Shitara, H., Popa, T., &amp; Hallett, M. (2016). Dissociable roles of preSMA in motor sequence chunking and hand switching—a TMS study. Journal of Neurophysiology, 116(6), 2637-2646.</p>
<p>Vernet, M., Brem, A. K., Farzan, F., &amp; Pascual-Leone, A. (2015). Synchronous and opposite roles of the parietal and prefrontal cortices in bistable perception: a double-coil TMS–EEG study. Cortex, 64, 78-88.</p>
<p>Pitcher, D. (2014). Facial expression recognition takes longer in the posterior superior temporal sulcus than in the occipital face area. Journal of Neuroscience, 34(27), 9173-9177.</p>
<p>Bardi, L., Kanai, R., Mapelli, D., &amp; Walsh, V. (2012). TMS of the FEF interferes with spatial conflict. Journal of cognitive neuroscience, 24(6), 1305-1313.</p>
<p>D’Ausilio, A., Bufalari, I., Salmas, P., &amp; Fadiga, L. (2012). The role of the motor system in discriminating normal and degraded speech sounds. Cortex, 48(7), 882-887.</p>
<p>Pitcher, D., Duchaine, B., Walsh, V., &amp; Kanwisher, N. (2010). TMS evidence for feedforward and feedback mechanisms of face and body perception. Journal of Vision, 10(7), 671-671.</p>
<p>Gagnon, G., Blanchet, S., Grondin, S., &amp; Schneider, C. (2010). Paired-pulse transcranial magnetic stimulation over the dorsolateral prefrontal cortex interferes with episodic encoding and retrieval for both verbal and non-verbal materials. Brain Research, 1344, 148-158.</p>
<p>Kalla, R., Muggleton, N. G., Juan, C. H., Cowey, A., &amp; Walsh, V. (2008). The timing of the involvement of the frontal eye fields and posterior parietal cortex in visual search. Neuroreport, 19(10), 1067-1071.</p>
<p>Pitcher, D., Garrido, L., Walsh, V., &amp; Duchaine, B. C. (2008). Transcranial magnetic stimulation disrupts the perception and embodiment of facial expressions. Journal of Neuroscience, 28(36), 8929-8933.</p>
<p>Til Ole Bergmann, Gesa Hartwigsen; Inferring Causality from Noninvasive Brain Stimulation in Cognitive Neuroscience. J Cogn Neurosci 2021; 33 (2): 195–225. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_01591">https://doi.org/10.1162/jocn_a_01591</ext-link></p>
<p>Romero, M.C., Davare, M., Armendariz, M. et al. Neural effects of transcranial magnetic stimulation at the single-cell level. Nat Commun 10, 2642 (2019). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-019-10638-7">https://doi.org/10.1038/s41467-019-10638-7</ext-link></p>
<p>Paulus W, Rothwell JC. Membrane resistance and shunting inhibition: where biophysics meets state-dependent human neurophysiology. J Physiol. 2016 May 15;594(10):2719-28. doi: 10.1113/JP271452. PMID: 26940751; PMCID: PMC4865581.</p>
<p>Staat, C., Gattinger, N., &amp; Gleich, B. (2022). PLUSPULS: A transcranial magnetic stimulator with extended pulse protocols. HardwareX, 13. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.ohx.2022.e00380">https://doi.org/10.1016/j.ohx.2022.e00380</ext-link></p>
<p>Zhao, W., Li, Y., and Du, Y. (2021). TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration. The Journal of Neuroscience, 10356-10364. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/jneurosci.1355-21.2021">https://doi.org/10.1523/jneurosci.1355-21.2021</ext-link>.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>The authors quantified information in gesture and speech, and investigated the neural processing of speech and gestures in pMTG and LIFG, depending on their informational content, in 8 different time-windows, and using three different methods (EEG, HD-tDCS and TMS). They found that there is a time-sensitive and staged progression of neural engagement that is correlated with the informational content of the signal (speech/gesture).</p>
<p>Strengths:</p>
<p>A strength of the paper is that the authors attempted to combine three different methods to investigate speech-gesture processing.</p>
</disp-quote>
<p>We sincerely thank the reviewer for recognizing our efforts in conducting three experiments to explore the neural activity linked to the amount of information processed during multisensory gesture-speech integration. In Experiment 1, we observed that the extent of inhibition in the pMTG and LIFG was closely linked to the overlapping gesture-speech responses, as quantified by mutual information. Building on the established roles of the pMTG and LIFG in our previous study (Zhao et al., 2021, JN), we then expanded our investigation to determine whether the dynamic neural engagement between the pMTG and LIFG during gesture-speech processing was also associated with the quality of the information. This hypothesis was further validated through high-temporal resolution EEG, where we examined ERP components related to varying information contents. Notably, we observed a close time alignment between the ERP components and the time windows of the TMS effects, which were associated with the same informational matrices in gesture-speech processing.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) One major issue is that there is a tight anatomical coupling between pMTG and LIFG. Stimulating one area could therefore also result in stimulation of the other area (see Silvanto and Pascual-Leone, 2008). I therefore think it is very difficult to tease apart the contribution of these areas to the speech-gesture integration process, especially considering that the authors stimulate these regions in time windows that are very close to each other in both time and space (and the disruption might last longer over time).</p>
</disp-quote>
<p>Response 1: We greatly appreciate the reviewer’s careful consideration. We trust that the explanation provided above has clarified this issue (see Response to Editors for detail).</p>
<disp-quote content-type="editor-comment">
<p>(2) Related to this point, it is unclear to me why the HD-TDCS/TMS is delivered in set time windows for each region. How did the authors determine this, and how do the results for TMS compare to their previous work from 2018 and 2023 (which describes a similar dataset+design)? How can they ensure they are only targeting their intended region since they are so anatomically close to each other?</p>
</disp-quote>
<p>Response 2: The current study builds on a series of investigations that systematically examined the temporal and spatial dynamics of gesture-speech integration. In our earlier work (Zhao et al., 2018, J. Neurosci), we demonstrated that interrupting neural activity in the IFG or pMTG using TMS selectively disrupted the semantic congruency effect (reaction time costs due to semantic incongruence), without affecting the gender congruency effect (reaction time costs due to gender incongruence). These findings identified the IFG and pMTG as critical hubs for gesture-speech integration. This informed the brain regions selected for subsequent studies.</p>
<p>In Zhao et al. (2021, J. Neurosci), we employed a double-pulse TMS protocol, delivering stimulation within one of eight 40-ms time windows, to further examine the temporal involvement of the IFG and pMTG. The results revealed time-window-selective disruptions of the semantic congruency effect, confirming the dynamic and temporally staged roles of these regions during gesture-speech integration.</p>
<p>In Zhao et al. (2023, Frontiers in Psychology), we investigated the semantic predictive role of gestures relative to speech by comparing two experimental conditions: (1) gestures preceding speech by a fixed interval of 200 ms, and (2) gestures preceding speech at its semantic identification point. We observed time-window-selective disruptions of the semantic congruency effect in the IFG and pMTG only in the second condition, leading to the conclusion that gestures exert a semantic priming effect on co-occurring speech. These findings underscored the semantic advantage of gesture in facilitating speech integration, further refining our understanding of the temporal and functional interplay between these modalities.</p>
<p>The design of the current study—including the choice of brain regions and time windows—was directly informed by these prior findings. Experiment 1 (HD-tDCS) targeted the entire gesture-speech integration process in the IFG and pMTG to assess whether neural activity in these regions, previously identified as integration hubs, is modulated by changes in informativeness from both modalities (i.e., entropy) and their interactions (mutual information, MI). The results revealed a gradual inhibition of neural activity in both areas as MI increased, evidenced by a negative correlation between MI and the tDCS inhibition effect in both regions. Building on this, Experiments 2 and 3 employed double-pulse TMS and ERPs to further assess whether the engaged neural activity was both time-sensitive and staged. These experiments also evaluated the contributions of various sources of information, revealing correlations between information-theoretic metrics and time-locked brain activity, providing insights into the ‘gradual’ nature of gesture-speech integration.</p>
<p>We acknowledge that the rationale for the design of the current study was not fully articulated in the original manuscript. In the revised version, we provided a more comprehensive and coherent explanation of the logic behind the three experiments, as well as the alignment with our previous findings in Lines 75-102:</p>
<p>‘To investigate the neural mechanisms underlying gesture-speech integration, we conducted three experiments to assess how neural activity correlates with distributed multisensory integration, quantified using information-theoretic measures of MI. Additionally, we examined the contributions of unisensory signals in this process, quantified through unisensory entropy. Experiment 1 employed high-definition transcranial direct current stimulation (HD-tDCS) to administer Anodal, Cathodal and Sham stimulation to either the IFG or the pMTG. HD-tDCS induces membrane depolarization with anodal stimulation and membrane hyperpolarization with cathodal stimulation[26], thereby increasing or decreasing cortical excitability in the targeted brain area, respectively. This experiment aimed to determine whether the overall facilitation (Anodal-tDCS minus Sham-tDCS) and/or inhibitory (Cathodal-tDCS minus Sham-tDCS) of these integration hubs is modulated by the degree of gesture-speech integration, as measure by MI.</p>
<p>Given the differential involvement of the IFG and pMTG in gesture-speech integration, shaped by top-down gesture predictions and bottom-up speech processing [23], Experiment 2 was designed to further assess whether the activity of these regions was associated with relevant informational matrices. Specifically, we applied inhibitory chronometric double-pulse transcranial magnetic stimulation (TMS) to specific temporal windows associated with integration processes in these regions[23], assessing whether the inhibitory effects of TMS were correlated with unisensory entropy or the multisensory convergence index (MI).</p>
<p>Experiment 3 complemented these investigations by focusing on the temporal dynamics of neural responses during semantic processing, leveraging high-temporal event-related potentials (ERPs). This experiment investigated how distinct information contributors modulated specific ERP components associated with semantic processing. These components included the early sensory effects as P1 and N1–P2[27,28], the N400 semantic conflict effect[14,28,29], and the late positive component (LPC) reconstruction effect[30,31]. By integrating these ERP findings with results from Experiments 1 and 2, Experiment 3 aimed to provide a more comprehensive understanding of how gesture-speech integration is modulated by neural dynamics.’</p>
<p>Although the IFG and pMTG are anatomically close, the consistent differentiation of their respective roles, as evidenced by our experiment across various time windows (TWs) and supported by previous research (see Response to editors for details), reinforces the validity of the stimulation effect observed in our study.</p>
<p>References</p>
<p>Zhao, W.Y., Riggs, K., Schindler, I., and Holle, H. (2018). Transcranial magnetic stimulation over left inferior frontal and posterior temporal cortex disrupts gesture-speech integration. Journal of Neuroscience 38, 1891-1900. 10.1523/Jneurosci.1748-17.2017.</p>
<p>Zhao, W., Li, Y., and Du, Y. (2021). TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration. The Journal of Neuroscience, 10356-10364. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/jneurosci.1355-21.2021">https://doi.org/10.1523/jneurosci.1355-21.2021</ext-link>.</p>
<p>Zhao, W. (2023). TMS reveals a two-stage priming circuit of gesture-speech integration. Front Psychol 14, 1156087. 10.3389/fpsyg.2023.1156087.</p>
<p>Bikson, M., Inoue, M., Akiyama, H., Deans, J.K., Fox, J.E., Miyakawa, H., and Jefferys, J.G.R. (2004). Effects of uniform extracellular DC electric fields on excitability in rat hippocampal slices. J Physiol-London 557, 175-190. 10.1113/jphysiol.2003.055772.</p>
<p>Federmeier, K.D., Mai, H., and Kutas, M. (2005). Both sides get the point: hemispheric sensitivities to sentential constraint. Memory &amp; Cognition 33, 871-886. 10.3758/bf03193082.</p>
<p>Kelly, S.D., Kravitz, C., and Hopkins, M. (2004). Neural correlates of bimodal speech and gesture comprehension. Brain and Language 89, 253-260. 10.1016/s0093-934x(03)00335-3.</p>
<p>Wu, Y.C., and Coulson, S. (2005). Meaningful gestures: Electrophysiological indices of iconic gesture comprehension. Psychophysiology 42, 654-667. 10.1111/j.1469-8986.2005.00356.x.</p>
<p>Fritz, I., Kita, S., Littlemore, J., and Krott, A. (2021). Multimodal language processing: How preceding discourse constrains gesture interpretation and affects gesture integration when gestures do not synchronise with semantic affiliates. J Mem Lang 117, 104191. 10.1016/j.jml.2020.104191.</p>
<p>Gunter, T.C., and Weinbrenner, J.E.D. (2017). When to take a gesture seriously: On how we use and prioritize communicative cues. J Cognitive Neurosci 29, 1355-1367. 10.1162/jocn_a_01125.</p>
<p>Ozyurek, A., Willems, R.M., Kita, S., and Hagoort, P. (2007). On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials. J Cognitive Neurosci 19, 605-616. 10.1162/jocn.2007.19.4.605.</p>
<disp-quote content-type="editor-comment">
<p>(3) As the EEG signal is often not normally distributed, I was wondering whether the authors checked the assumptions for their Pearson correlations. The authors could perhaps better choose to model the different variables to see whether MI/entropy could predict the neural responses. How did they correct the many correlational analyses that they have performed?</p>
</disp-quote>
<p>Response 3: We greatly appreciate the reviewer’s thoughtful comments.</p>
<p>(1) Regarding the questioning of normal distribution of EEG signals and the use of Pearson correlation, in Figure 5 of the manuscript, we have already included normal distribution curves to illustrate the relationships between average ERP amplitudes across each ROI or elicited cluster and the three information models.</p>
<p>Additionally, we performed the Shapiro-Wilk test, a widely accepted method for assessing bivariate normality, on both the MI/entropy and averaged ERP data. The p-values for all three combinations were greater than 0.05, indicating that the sample data from all bivariate combinations were normally distributed (Author response table 2).</p>
<table-wrap id="sa3table2">
<label>Author response table 2.</label>
<caption>
<title>Shapiro-Wilk results of bivariable normality test</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table2.jpg" mimetype="image"/>
</table-wrap>
<p>To further consolidate the relationship between entropy/MI and various ERP components, we also conducted a Spearman rank correlation analysis (Author response table 3-5). While the correlation between speech entropy and ERP amplitude in the P1 component yielded a p-value of 0.061, all other results were consistent with those obtained from the Pearson correlation analysis across the three experiments. Therefore, our conclusion that progressive neural responses reflected the degree of information remains robust. Although the Spearman rank and Pearson correlation analyses yielded similar results, we opted to report the Pearson correlation coefficients throughout the manuscript to maintain consistency.</p>
<table-wrap id="sa3table3">
<label>Author response table 3.</label>
<caption>
<title>Comparison of Pearson and Spearman results in Experiment 1</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table3.jpg" mimetype="image"/>
</table-wrap>
<table-wrap id="sa3table4">
<label>Author response table 4.</label>
<caption>
<title>Comparison of Pearson and Spearman results in Experiment 2</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table4.jpg" mimetype="image"/>
</table-wrap>
<table-wrap id="sa3table5">
<label>Author response table 5.</label>
<caption>
<title>Comparison of Pearson and Spearman results in Experiment 3</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table5.jpg" mimetype="image"/>
</table-wrap>
<p>(2) Regarding the reviewer’s comment ‘choose to model the different variables to see whether MI/entropy could predict the neural responses’, we employed Representational Similarity Analysis (RSA) (Popal et.al, 2019) with MI and entropy as continuous variables. This analysis aimed to build a model to predict neural responses based on these feature metrics.</p>
<p>To capture dynamic temporal features indicative of different stages of multisensory integration, we segmented the EEG data into overlapping time windows (40 ms in duration with a 10 ms step size). The 40 ms window was chosen based on the TMS protocol used in Experiment 2, which also employed a 40 ms time window. The 10 ms step size (equivalent to 5 time points) was used to detect subtle shifts in neural responses that might not be captured by larger time windows, allowing for a more granular analysis of the temporal dynamics of neural activity.</p>
<p>Following segmentation, the EEG data were reshaped into a four-dimensional matrix (42 channels × 20 time points × 97 time windows × 20 features). To construct a neural similarity matrix, we averaged the EEG data across time points within each channel and each time window. The resulting matrix was then processed using the pdist function to compute pairwise distances between adjacent data points. This allowed us to calculate correlations between the neural matrix and three feature similarity matrices, which were constructed in a similar manner. These three matrices corresponded to (1) gesture entropy, (2) speech entropy, and (3) mutual information (MI). This approach enabled us to quantify how well the neural responses corresponded to the semantic dimensions of gesture and speech stimuli at each time window.</p>
<p>To determine the significance of the correlations between neural activity and feature matrices, we conducted 1000 permutation tests. In this procedure, we randomized the data or feature matrices and recalculated the correlations repeatedly, generating a null distribution against which the observed correlation values were compared. Statistical significance was determined if the observed correlation exceeded the null distribution threshold (p &lt; 0.05). This permutation approach helps mitigate the risk of spurious correlations, ensuring that the relationships between the neural data and feature matrices are both robust and meaningful.</p>
<p>Finally, significant correlations were subjected to clustering analysis, which grouped similar neural response patterns across time windows and channels. This clustering allowed us to identify temporal and spatial patterns in the neural data that consistently aligned with the semantic features of gesture and speech stimuli, thus revealing the dynamic integration of these multisensory modalities across time. Results are as follows:</p>
<p>(1) Two significant clusters were identified for gesture entropy (Author response image 1 left). The first cluster was observed between 60-110 ms (channels F1 and F3), with correlation coefficients (r) ranging from 0.207 to 0.236 (<italic>p</italic> &lt; 0.001). The second cluster was found between 210-280 ms (channel O1), with r-values ranging from 0.244 to 0.313 (<italic>p</italic> &lt; 0.001).</p>
<p>(2) For speech entropy (Author response image 1 middle), significant clusters were detected in both early and late time windows. In the early time windows, the largest significant cluster was found between 10-170 ms (channels F2, F4, F6, FC2, FC4, FC6, C4, C6, CP4, and CP6), with r-values ranging from 0.151 to 0.340 (p = 0.013), corresponding to the P1 component (0-100 ms). In the late time windows, the largest significant cluster was observed between 560-920 ms (across the whole brain, all channels), with r-values ranging from 0.152 to 0.619 (p = 0.013).</p>
<p>(3) For mutual information (MI) (Author response image 1 right), a significant cluster was found between 270-380 ms (channels FC1, FC2, FC3, FC5, C1, C2, C3, C5, CP1, CP2, CP3, CP5, FCz, Cz, and CPz), with r-values ranging from 0.198 to 0.372 (p = 0.001).</p>
<fig id="sa3fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-fig1.jpg" mimetype="image"/>
</fig>
<p>Results of RSA analysis.</p>
<p>These additional findings suggest that even using a different modeling approach, neural responses, as indexed by feature metrics of entropy and mutual information, are temporally aligned with distinct ERP components and ERP clusters, as reported in the current manuscript. This alignment serves to further consolidate the results, reinforcing the conclusion we draw. Considering the length of the manuscript, we did not include these results in the current manuscript.</p>
<p>(3) In terms of the correction of multiple comparisons, in Experiment 1, two separate participant groups were recruited for HD-tDCS applied over either the IFG or pMTG. FDR correction was performed separately for each group, resulting in six comparisons for each brain region (three information matrices × two tDCS effects: anodal-sham or cathodal-sham). In Experiment 2, six comparisons (three information matrices × two sites: IFG or pMTG) were submitted for FDR correction. In Experiment 3, FDR correction was applied to the seven regions of interest (ROIs) within each component, resulting in five comparisons.</p>
<p>Reference:</p>
<p>Wilk, M.B. (2015). The Shapiro Wilk And Related Tests For Normality.</p>
<p>Popal, H., Wang, Y., &amp; Olson, I. R. (2019). A guide to representational similarity analysis for social neuroscience. Social cognitive and affective neuroscience, 14(11), 1243-1253.</p>
<disp-quote content-type="editor-comment">
<p>(4) The authors use ROIs for their different analyses, but it is unclear why and on the basis of what these regions are defined. Why not consider all channels without making them part of an ROI, by using a method like the one described in my previous comment?</p>
</disp-quote>
<p>Response 4: For the EEG data, we conducted both a traditional ROI analysis and a cluster-based permutation approach. The ROIs were defined based on a well-established work (Habets et al., 2011), allowing for hypothesis-driven testing of specific regions. In addition, we employed a cluster-based permutation methods, which is data-driven and helps enhance robustness while addressing multiple comparisons. This method serves as a complement to the hypothesis-driven ROI analysis, offering an exploratory, unbiased perspective. Notably, the results from both approaches were consistent, reinforcing the reliability of our findings.</p>
<p>To make the methods more accessible to a broader audience, we clarified the relationship between these approaches in the revised manuscript in Lines 267-270: ‘To consolidate the data, we conducted both a traditional region-of-interest (ROI) analysis, with ROIs defined based on a well-established work40, and a cluster-based permutation approach, which utilizes data-driven permutations to enhance robustness and address multiple comparisons’</p>
<p>Additionally, we conducted an RSA analysis without defining specific ROIs, considering all channels in the analysis. This approach yielded consistent results, further validating the robustness of our findings across different analysis methods. See Response 3 for detail.</p>
<p>Reference:</p>
<p>Habets, B., Kita, S., Shao, Z.S., Ozyurek, A., and Hagoort, P. (2011). The Role of Synchrony and Ambiguity in Speech-Gesture Integration during Comprehension. J Cognitive Neurosci 23, 1845-1854. 10.1162/jocn.2010.21462</p>
<disp-quote content-type="editor-comment">
<p>(5) The authors describe that they have divided their EEG data into a &quot;lower half&quot; and a &quot;higher half&quot; (lines 234-236), based on entropy scores. It is unclear why this is necessary, and I would suggest just using the entropy scores as a continuous measure.</p>
</disp-quote>
<p>Response 5: To identify ERP components or spatiotemporal clusters that demonstrated significant semantic differences, we split each model into higher and lower halves based on entropy scores. This division allowed us to capture distinct levels of information processing and explore how different levels of entropy or mutual information (MI) related to neural activity. Specifically, the goal was to highlight the gradual activation process of these components and clusters as they correlate with changes in information content. Remarkably, consistent results were observed between the ERP components and clusters, providing robust evidence that semantic information conveyed through gestures and speech significantly influenced the amplitude of these components or clusters. Moreover, the semantic information was shown to be highly sensitive, varying in tandem with these amplitude changes.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Comment:</p>
<p>Summary:</p>
<p>The study is an innovative and fundamental study that clarified important aspects of brain processes for integration of information from speech and iconic gesture (i.e., gesture that depicts action, movement, and shape), based on tDCS, TMS, and EEG experiments. They evaluated their speech and gesture stimuli in information-theoretic ways and calculated how informative speech is (i.e., entropy), how informative gesture is, and how much shared information speech and gesture encode. The tDCS and TMS studies found that the left IFG and pMTG, the two areas that were activated in fMRI studies on speech-gesture integration in the previous literature, are causally implicated in speech-gesture integration. The size of tDC and TMS effects are correlated with the entropy of the stimuli or mutual information, which indicates that the effects stem from the modulation of information decoding/integration processes. The EEG study showed that various ERP (event-related potential, e.g., N1-P2, N400, LPC) effects that have been observed in speech-gesture integration experiments in the previous literature, are modulated by the entropy of speech/gesture and mutual information. This makes it clear that these effects are related to information decoding processes. The authors propose a model of how the speech-gesture integration process unfolds in time, and how IFG and pMTG interact with each other in that process.</p>
<p>Strengths:</p>
<p>The key strength of this study is that the authors used information theoretic measures of their stimuli (i.e., entropy and mutual information between speech and gesture) in all of their analyses. This made it clear that the neuro-modulation (tDCS, TMS) affected information decoding/integration and ERP effects reflect information decoding/integration. This study used tDCS and TMS methods to demonstrate that left IFG and pMTG are causally involved in speech-gesture integration. The size of tDCS and TMS effects are correlated with information-theoretic measures of the stimuli, which indicate that the effects indeed stem from disruption/facilitation of the information decoding/integration process (rather than generic excitation/inhibition). The authors' results also showed a correlation between information-theoretic measures of stimuli with various ERP effects. This indicates that these ERP effects reflect the information decoding/integration process.</p>
</disp-quote>
<p>We sincerely thank the reviewer for recognizing our efforts and the innovation of employing information-theoretic measures to elucidate the brain processes underlying the multisensory integration of gesture and speech.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>The &quot;mutual information&quot; cannot fully capture the interplay of the meaning of speech and gesture. The mutual information is calculated based on what information can be decoded from speech alone and what information can be decoded from gesture alone. However, when speech and gesture are combined, a novel meaning can emerge, which cannot be decoded from a single modality alone. When example, a person produces a gesture of writing something with a pen, while saying &quot;He paid&quot;. The speech-gesture combination can be interpreted as &quot;paying by signing a cheque&quot;. It is highly unlikely that this meaning is decoded when people hear speech only or see gestures only. The current study cannot address how such speech-gesture integration occurs in the brain, and what ERP effects may reflect such a process. Future studies can classify different types of speech-gesture integration and investigate neural processes that underlie each type. Another important topic for future studies is to investigate how the neural processes of speech-gesture integration change when the relative timing between the speech stimulus and the gesture stimulus changes.</p>
</disp-quote>
<p>We greatly appreciate Reviewer2 ’s thoughtful concern regarding whether &quot;mutual information&quot; adequately captures the interplay between the meanings of speech and gesture. We would like to clarify that the materials used in the present study involved gestures that were performed without actual objects, paired with verbs that precisely describe the corresponding actions. For example, a hammering gesture was paired with the verb “hammer”, and a cutting gesture was paired with the verb “cut”. In this design, all gestures conveyed redundant information relative to the co-occurring speech, creating significant overlap between the information derived from speech alone and that from gesture alone.</p>
<p>We understand the reviewer’s concern about cases where gestures and speech might provide complementary, rather than redundant, information. To address this, we have developed an alternative metric for quantifying information gains contributed by supplementary multisensory cues, which will be explored in a subsequent study. However, for the present study, we believe that the observed overlap in information serves as a key indicator of multisensory convergence, a central focus of our investigation.</p>
<p>Regarding the reviewer’s concern about how neural processes of speech-gesture integration may change with varying relative timing between speech and gesture stimuli, we would like to highlight findings from our previous study (Zhao, 2023, Frontiers in Psychology). In that study, we explored the semantic predictive role of gestures relative to speech under two timing conditions: (1) gestures preceding speech by a fixed interval of 200 ms, and (2) gestures preceding speech at its semantic identification point. Interestingly, only in the second condition did we observe time-window-selective disruptions of the semantic congruency effect in the IFG and pMTG. This led us to conclude that gestures play a semantic priming role for co-occurring speech. Building on this, we designed the present study with gestures deliberately preceding speech at its semantic identification point to reflect this semantic priming relationship. Additionally, ongoing research in our lab is exploring gesture and speech interactions in natural conversational settings to investigate whether the neural processes identified here remain consistent across varying contexts.</p>
<p>To address potential concerns and ensure clarity regarding the limitations of the MI measurement, we have included a discussion of tthis in the revised manuscript in Lines 543-547: ‘Furthermore, MI quantifies overlap in gesture-speech integration, primarily when gestures convey redundant meaning. Consequently, the conclusions drawn in this study are constrained to contexts in which gestures serve to reinforce the meaning of the speech. Future research should aim to explore the neural responses in cases where gestures convey supplementary, rather than redundant, semantic information.’ This is followed by a clarification of the timing relationship between gesture and speech: ‘Note that the sequential cortical involvement and ERP components discussed above are derived from a deliberate alignment of speech onset with gesture DP, creating an artificial priming effect with gesture semantically preceding speech. Caution is advised when generalizing these findings to the spontaneous gesture-speech relationships, although gestures naturally precede speech[34].’ (Lines 539-543).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>In this useful study, Zhao et al. try to extend the evidence for their previously described two-step model of speech-gesture integration in the posterior Middle Temporal Gyrus (pMTG) and Inferior Frontal Gyrus (IFG). They repeat some of their previous experimental paradigms, but this time quantifying Information-Theoretical (IT) metrics of the stimuli in a stroop-like paradigm purported to engage speech-gesture integration. They then correlate these metrics with the disruption of what they claim to be an integration effect observable in reaction times during the tasks following brain stimulation, as well as documenting the ERP components in response to the variability in these metrics.</p>
<p>The integration of multiple methods, like tDCS, TMS, and ERPs to provide converging evidence renders the results solid. However, their interpretation of the results should be taken with care, as some critical confounds, like difficulty, were not accounted for, and the conceptual link between the IT metrics and what the authors claim they index is tenuous and in need of more evidence. In some cases, the difficulty making this link seems to arise from conceptual equivocation (e.g., their claims regarding 'graded' evidence), whilst in some others it might arise from the usage of unclear wording in the writing of the manuscript (e.g. the sentence 'quantitatively functional mental states defined by a specific parser unified by statistical regularities'). Having said that, the authors' aim is valuable, and addressing these issues would render the work a very useful approach to improve our understanding of integration during semantic processing, being of interest to scientists working in cognitive neuroscience and neuroimaging.</p>
<p>The main hurdle to achieving the aims set by the authors is the presence of the confound of difficulty in their IT metrics. Their measure of entropy, for example, being derived from the distribution of responses of the participants to the stimuli, will tend to be high for words or gestures with multiple competing candidate representations (this is what would presumptively give rise to the diversity of responses in high-entropy items). There is ample evidence implicating IFG and pMTG as key regions of the semantic control network, which is critical during difficult semantic processing when, for example, semantic processing must resolve competition between multiple candidate representations, or when there are increased selection pressures (Jackson et al., 2021). Thus, the authors' interpretation of Mutual Information (MI) as an index of integration is inextricably contaminated with difficulty arising from multiple candidate representations. This casts doubt on the claims of the role of pMTG and IFG as regions carrying out gesture-speech integration as the observed pattern of results could also be interpreted in terms of brain stimulation interrupting the semantic control network's ability to select the best candidate for a given context or respond to more demanding semantic processing.</p>
</disp-quote>
<p>Response 1: We sincerely thank the reviewer for pointing out the confound of difficulty. The primary aim of this study is to investigate whether the degree of activity in the established integration hubs, IFG and pMTG, is influenced by the information provided by gesture-speech modalities and/or their interactions. While we provided evidence for the differential involvement of the IFG and pMTG by delineating their dynamic engagement across distinct time windows of gesture-speech integration and associating these patterns with unisensory information and their interaction, we acknowledge that the mechanisms underlying these dynamics remain open to interpretation. Specifically, whether the observed effects stem from difficulties in semantic control processes, as suggested by the reviewer, or from resolving information uncertainty, as quantified by entropy, falls outside the scope of the current study. Importantly, we view these two interpretations as complementary rather than mutually exclusive, as both may be contributing factors. Nonetheless, we agree that addressing this question is a compelling avenue for future research.</p>
<p>In the revised manuscript, we have included an additional analysis to assess whether the confounding effects of lexical or semantic control difficulty—specifically, the number of available responses—affect the neural outcomes. To address this, we performed partial correlation analyses, controlling for the number of responses.</p>
<p>We would like to clarify an important distinction between the measure of entropy derived from the distribution of responses and the concept of response diversity. Entropy, in our analysis, is computed based on the probability distribution of each response, as captured by the information entropy formula. In contrast, response diversity refers to the simple count of different responses provided. Mutual Information (MI), by its nature, is also an entropy measure, quantifying the overlap in responses. For reference, although we observed a high correlation between the three information matrices and the number of responses (gesture entropy &amp; gesture response number: <italic>r</italic> = 0.976, <italic>p</italic> &lt; 0.001; speech entropy &amp; speech response number: <italic>r</italic> = 0.961, <italic>p</italic> &lt; 0.001; MI &amp; total response number: <italic>r</italic> = 0.818, <italic>p</italic> &lt; 0.001), it is crucial to emphasize that these metrics capture different aspects of the semantic information represented. In the revised manuscript, we have provided a table detailing both entropy and response numbers for each stimulus, to allow for greater transparency and clarity.</p>
<p>Furthermore, we have added a comprehensive description of the partial correlation analysis conducted across all three experiments in the methodology section: for Experiment 1, please refer to Lines 213–222: ‘To account for potential confounds related to multiple candidate representations, we conducted partial correlation analyses between the tDCS effects and gesture entropy, speech entropy, and MI, controlling for the number of responses provided for each gesture and speech, as well as the total number of combined responses. Given that HD-tDCS induces overall disruption at the targeted brain regions, we hypothesized that the neural activity within the left IFG and pMTG would be progressively affected by varying levels of multisensory convergence, as indexed by MI. Moreover, we hypothesized that the modulation of neural activity by MI would differ between the left IFG and pMTG, as reflected in the differential modulation of response numbers in the partial correlations, highlighting their distinct roles in semantic processing[37].’</p>
<p>Experiment 2: ‘To control for potential confounds, partial correlations were also performed between the TMS effects and gesture entropy, speech entropy, and MI, controlling for the number of responses for each gesture and speech, as well as the total number of combined responses. By doing this, we can determine how the time-sensitive contribution of the left IFG and pMTG to gesture–speech integration was affected by gesture and speech information distribution.’ (Lines 242–246).</p>
<p>Experiment 3: ‘Additionally, partial correlations were conducted, accounting for the number of responses for each respective metric’ (Lines 292–293).</p>
<p>As anticipated by the reviewer, we observed a consistent modulation of response numbers across both regions as well as across the four ERP components and associated clusters. The detailed results are presented below:</p>
<p>Experiment 1: ‘However, partial correlation analysis, controlling for the total response number, revealed that the initially significant correlation between the Cathodal-tDCS effect and MI was no longer significant (<italic>r</italic> = -0.303, <italic>p</italic> = 0.222, 95% CI = [-0.770, 0.164]). This suggests that the observed relationship between Cathodal-tDCS and MI may be confounded by semantic control difficulty, as reflected by the total number of responses. Specifically, the reduced activity in the IFG under Cathodal-tDCS may be driven by variations in the difficulty of semantic control rather than a direct modulation of MI.’ (Lines 310-316) and ‘’Importantly, the reduced activity in the pMTG under Cathodal-tDCS was not influenced by the total response number, as indicated by the non-significant correlation (<italic>r</italic> = -0.253, <italic>p</italic> = 0.295, 95% CI = [-0.735, 0.229]). This finding was further corroborated by the unchanged significance in the partial correlation between Cathodal-tDCS and MI, when controlling for the total response number (<italic>r</italic> = -0.472, <italic>p</italic> = 0.048, 95% CI = [-0.903, -0.041]). (Lines 324-328).</p>
<p>Experiment 2:’ Notably, inhibition of pMTG activity in TW2 was not influenced by the number of speech responses (<italic>r</italic> = -0.539, <italic>p</italic> = 0.087, 95% CI = [-1.145, 0.067]). However, the number of speech responses did affect the modulation of speech entropy on the pMTG inhibition effect in TW2. This was evidenced by the non-significant partial correlation between pMTG inhibition and speech entropy when controlling for speech response number (<italic>r</italic> = -0.218, <italic>p</italic> = 0.545, 95% CI = [-0.563, 0.127]).</p>
<p>In contrast, the interrupted IFG activity in TW6 appeared to be consistently influenced by the confound of semantic control difficulty. This was reflected in the significant correlation with both gesture response number (<italic>r</italic> = -0.480, <italic>p</italic> = 0.032, 95% CI = [-904, -0.056]), speech response number (<italic>r</italic> = -0.729, <italic>p</italic> = 0.011, 95% CI = [-1.221, -0.237]), and total response number (<italic>r</italic> = -0.591, <italic>p</italic> = 0.008, 95% CI = [-0.993, -0.189]). Additionally, partial correlation analyses revealed non-significant relationship between interrupted IFG activity in TW6 and gesture entropy (<italic>r</italic> = -0.369, <italic>p</italic> = 0.120, 95% CI = [-0.810, -0.072]), speech entropy (<italic>r</italic> = -0.455, <italic>p</italic> = 0.187, 95% CI = [-1.072, 0.162]), and MI (<italic>r</italic> = -0.410, <italic>p</italic> = 0.091, 95% CI = [-0.856, -0.036]) when controlling for response numbers.’ (Lines 349-363)</p>
<p>Experiment 3: ‘To clarify potential confounds of semantic control difficulty, partial correlation analyses were conducted to examine the relationship between the elicited ERP components and the relevant information matrices, controlling for response numbers. Results consistently indicated modulation by response numbers in the relationship of ERP components with the information matrix, as evidenced by the non-significant partial correlations between the P1 amplitude (P1 component over ML: <italic>r</italic> = -0.574, <italic>p</italic> = 0.082, 95% CI = [-1.141, -0.007]) and the P1 cluster (<italic>r</italic> = -0.503, <italic>p</italic> = 0.138, 95% CI = [-1.102, 0.096]) with speech entropy; the N1-P2 amplitude (N1-P2 component over LA: <italic>r</italic> = -0.080, <italic>p</italic> = 0.746, 95% CI = [-0.554, 0.394]) and N1-P2 cluster (<italic>r</italic> = -0.179, <italic>p</italic> = 0.464, 95% CI = [-0.647, 0.289]) with gesture entropy; the N400 amplitude (N400 component over LA: <italic>r</italic> = 0.264, <italic>p</italic> = 0.247, 95% CI = [-0.195,0.723]) and N400 cluster (<italic>r</italic> = 0.394, <italic>p</italic> = 0.095, 95% CI = [-0.043, 0.831]) with gesture entropy; the N400 amplitude (N400 component over LA: <italic>r</italic> = -0.134, <italic>p</italic> = 0.595, 95% CI = [-0.620, 0.352]) and N400 cluster (<italic>r</italic> = -0.034, <italic>p</italic> = 0.894, 95% CI = [-0.524,0.456]) with MI; and the LPC amplitude (LPC component over LA: <italic>r</italic> = -0.428, <italic>p</italic> = 0.217, 95% CI = [-1.054, 0.198]) and LPC cluster (<italic>r</italic> = -0.202, <italic>p</italic> = 0.575, 95% CI = [-0.881, 0.477]) with speech entropy.’ (Lines 424-438)</p>
<p>Based on the above results, we conclude that there is a dynamic interplay between the difficulty of semantic representation and the control pressures that shape the resulting neural responses. Furthermore, while the role of the IFG in control processes remains consistent, the present study reveals a more segmented role for the pMTG. Specifically, although the pMTG is well-established in the processing of distributed speech information, the integration of multisensory convergence, as indexed by MI, did not elicit the same control-related modulation in pMTG activity. A comprehensive discussion of the control process in shaping neural responses, as well as the specific roles of the IFG and pMTG in this process, is provided in the Discussion section in Lines (493-511): ‘Given that control processes are intrinsically integrated with semantic processing50, a distributed semantic representation enables dynamic modulation of access to and manipulation of meaningful information, thereby facilitating flexible control over the diverse possibilities inherent in a concept. Accordingly, an increased number of candidate responses amplifies the control demands necessary to resolve competing semantic representations. This effect was observed in the present study, where the association of the information matrix with the tDCS effect in IFG, the inhibition of pMTG activity in TW2, disruption of IFG activity in TW6, and modulation of four distinct ERP components collectively demonstrated that response quantity modulated neural activity. These results underscore the intricate interplay between the difficulty of semantic representation and the control pressures that shape the resulting neural responses.</p>
<p>The IFG and pMTG, central components of the semantic control network, have been extensively implicated in previous research 50-52. While the role of the IFG in managing both unisensory information and multisensory convergence remains consistent, as evidenced by the confounding difficulty results across Experiments 1 and 2, the current study highlights a more context-dependent function for the pMTG. Specifically, although the pMTG is well-established in the processing of distributed speech information, the multisensory convergence, indexed by MI, did not evoke the same control-related modulation in pMTG activity. These findings suggest that, while the pMTG is critical to semantic processing, its engagement in control processes is likely modulated by the specific nature of the sensory inputs involved’</p>
<p>Reference:</p>
<p>Tesink, C.M.J.Y., Petersson, K.M., van Berkum, J.J.A., van den Brink, D., Buitelaar, J.K., and Hagoort, P. (2009). Unification of speaker and meaning in language comprehension: An fMRI study. J Cognitive Neurosci <italic>21</italic>, 2085-2099. 10.1162/jocn.2008.21161</p>
<p>Jackson, R.L. (2021). The neural correlates of semantic control revisited. Neuroimage 224, 117444. 10.1016/j.neuroimage.2020.117444.</p>
<p>Jefferies, E. (2013). The neural basis of semantic cognition: converging evidence from neuropsychology, neuroimaging and TMS. Cortex 49, 611-625. 10.1016/j.cortex.2012.10.008.</p>
<p>Noonan, K.A., Jefferies, E., Visser, M., and Lambon Ralph, M.A. (2013). Going beyond inferior prefrontal involvement in semantic control: evidence for the additional contribution of dorsal angular gyrus and posterior middle temporal cortex. J Cogn Neurosci 25, 1824-1850. 10.1162/jocn_a_00442.</p>
<disp-quote content-type="editor-comment">
<p>In terms of conceptual equivocation, the use of the term 'graded' by the authors seems to be different from the usage commonly employed in the semantic cognition literature (e.g., the 'graded hub hypothesis', Rice et al., 2015). The idea of a graded hub in the controlled semantic cognition framework (i.e., the anterior temporal lobe) refers to a progressive degree of abstraction or heteromodal information as you progress through the anatomy of the region (i.e., along the dorsal-to-ventral axis). The authors, on the other hand, seem to refer to 'graded manner' in the context of a correlation of entropy or MI and the change in the difference between Reaction Times (RTs) of semantically congruent vs incongruent gesture-speech. The issue is that the discourse through parts of the introduction and discussion seems to conflate both interpretations, and the ideas in the main text do not correspond to the references they cite. This is not overall very convincing. What is it exactly the authors are arguing about the correlation between RTs and MI indexes? As stated above, their measure of entropy captures the spread of responses, which could also be a measure of item difficulty (more diverse responses imply fewer correct responses, a classic index of difficulty). Capturing the diversity of responses means that items with high entropy scores are also likely to have multiple candidate representations, leading to increased selection pressures. Regions like pMTG and IFG have been widely implicated in difficult semantic processing and increased selection pressures (Jackson et al., 2021). How is this MI correlation evidence of integration that proceeds in a 'graded manner'? The conceptual links between these concepts must be made clearer for the interpretation to be convincing.</p>
</disp-quote>
<p>Response 2: Regarding the concern of conceptual equivocation, we would like to emphasize that this study represents the first attempt to focus on the relationship between information quantity and neural engagement, a question addressed in three experiments. Experiment 1 (HD-tDCS) targeted the entire gesture-speech integration process in the IFG and pMTG to assess whether neural activity in these regions, previously identified as integration hubs, is modulated by changes in informativeness from both modalities (i.e., entropy) and their interactions (MI). The results revealed a gradual inhibition of neural activity in both areas as MI increased, evidenced by a negative correlation between MI and the tDCS inhibition effect in both regions. Building on this, Experiments 2 and 3 employed double-pulse TMS and ERPs to further assess whether the engaged neural activity was both time-sensitive and staged. These experiments also evaluated the contributions of various sources of information, revealing correlations between information-theoretic metrics and time-locked brain activity, providing insights into the ‘gradual’ nature of gesture-speech integration.</p>
<p>Therefore, the incremental engagement of the integration hub of IFG and pMTG along with the informativeness of gesture and speech during multisensory integration is different from the &quot;graded hub,&quot; which refers to anatomical distribution. We sincerely apologize for this oversight. In the revised manuscript, we have changed the relevant conceptual equivocation in Lines 44-60: ‘Consensus acknowledges the presence of 'convergence zones' within the temporal and inferior parietal areas [1], or the 'semantic hub' located in the anterior temporal lobe[2], pivotal for integrating, converging, or distilling multimodal inputs. Contemporary theories frame the semantic processing as a dynamic sequence of neural states[3], shaped by systems that are finely tuned to the statistical regularities inherent in sensory inputs[4]. These regularities enable the brain to evaluate, weight, and integrate multisensory information, optimizing the reliability of individual sensory signals[5]. However, sensory inputs available to the brain are often incomplete and uncertain, necessitating adaptive neural adjustments to resolve these ambiguities [6]. In this context, neuronal activity is thought to be linked to the probability density of sensory information, with higher levels of uncertainty resulting in the engagement of a broader population of neurons, thereby reflecting the brain’s adaptive capacity to handle diverse possible interpretations[7,8]. Although the role of 'convergence zones' and 'semantic hubs' in integrating multimodal inputs is well established, the precise functional patterns of neural activity in response to the distribution of unified multisensory information—along with the influence of unisensory signals—remain poorly understood.</p>
<p>To this end, we developed an analytic approach to directly probe the cortical engagement during multisensory gesture-speech semantic integration.’</p>
<p>Furthermore, in the Discussion section, we have replaced the term 'graded' with 'incremental' (Line 456,). Additionally, we have included a discussion on the progressive nature of neural engagement, as evidenced by the correlation between RTs and MI indices in Lines 483-492: ‘The varying contributions of unisensory gesture-speech information and the convergence of multisensory inputs, as reflected in the correlation between distinct ERP components and TMS time windows (TMS TWs), are consistent with recent models suggesting that multisensory processing involves parallel detection of modality-specific information and hierarchical integration across multiple neural levels[4,48]. These processes are further characterized by coordination across multiple temporal scales[49]. Building on this, the present study offers additional evidence that the multi-level nature of gesture-speech processing is statistically structured, as measured by information matrix of unisensory entropy and multisensory convergence index of MI, the input of either source would activate a distributed representation, resulting in progressively functioning neural responses.’</p>
<p>Reference:</p>
<p>Damasio, H., Grabowski, T.J., Tranel, D., Hichwa, R.D., and Damasio, A.R. (1996). A neural basis for lexical retrieval. Nature 380, 499-505. DOI 10.1038/380499a0.</p>
<p>Patterson, K., Nestor, P.J., and Rogers, T.T. (2007). Where do you know what you know? The representation of semantic knowledge in the human brain. Nature Reviews Neuroscience 8, 976-987. 10.1038/nrn2277.</p>
<p>Brennan, J.R., Stabler, E.P., Van Wagenen, S.E., Luh, W.M., and Hale, J.T. (2016). Abstract linguistic structure correlates with temporal activity during naturalistic comprehension. Brain and Language 157, 81-94. 10.1016/j.bandl.2016.04.008.</p>
<p>Benetti, S., Ferrari, A., and Pavani, F. (2023). Multimodal processing in face-to-face interactions: A bridging link between psycholinguistics and sensory neuroscience. Front Hum Neurosci 17, 1108354. 10.3389/fnhum.2023.1108354.</p>
<p>Noppeney, U. (2021). Perceptual Inference, Learning, and Attention in a Multisensory World. Annual Review of Neuroscience, Vol 44, 2021 44, 449-473. 10.1146/annurev-neuro-100120-085519.</p>
<p>Ma, W.J., and Jazayeri, M. (2014). Neural coding of uncertainty and probability. Annu Rev Neurosci 37, 205-220. 10.1146/annurev-neuro-071013-014017.</p>
<p>Fischer, B.J., and Pena, J.L. (2011). Owl's behavior and neural representation predicted by Bayesian inference. Nat Neurosci 14, 1061-1066. 10.1038/nn.2872.</p>
<p>Ganguli, D., and Simoncelli, E.P. (2014). Efficient sensory encoding and Bayesian inference with heterogeneous neural populations. Neural Comput 26, 2103-2134. 10.1162/NECO_a_00638.</p>
<p>Meijer, G.T., Mertens, P.E.C., Pennartz, C.M.A., Olcese, U., and Lansink, C.S. (2019). The circuit architecture of cortical multisensory processing: Distinct functions jointly operating within a common anatomical network. Prog Neurobiol 174, 1-15. 10.1016/j.pneurobio.2019.01.004.</p>
<p>Senkowski, D., and Engel, A.K. (2024). Multi-timescale neural dynamics for multisensory integration. Nat Rev Neurosci 25, 625-642. 10.1038/s41583-024-00845-7.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>I have a number of small suggestions to make the paper more easy to understand.</p>
</disp-quote>
<p>We sincerely thank the reviewer for their careful reading and thoughtful consideration. All suggestions have been thoroughly addressed and incorporated into the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(1) Lines 86-87, please clarify whether &quot;chronometric double-pulse TMS&quot; should lead to either excitation or inhibition of neural activities</p>
</disp-quote>
<p>Double-pulse TMS elicits inhibition of neural activities (see responses to editors), which has been clarified in the revised manuscript in Lines 90-93: ‘we applied inhibitory chronometric double-pulse transcranial magnetic stimulation (TMS) to specific temporal windows associated with integration processes in these regions[23], assessing whether the inhibitory effects of TMS were correlated with unisensory entropy or the multisensory convergence index (MI)’</p>
<disp-quote content-type="editor-comment">
<p>(2) Line 106 &quot;validated by replicating the semantic congruencey effect&quot;. Please specify what the task was in the validation study.</p>
</disp-quote>
<p>The description of the validation task has been added in Lines 116-119: ‘To validate the stimuli, 30 participants were recruited to replicate the multisensory index of semantic congruency effect, hypothesizing that reaction times for semantically incongruent gesture-speech pairs would be significantly longer than those for congruent pairs.’</p>
<disp-quote content-type="editor-comment">
<p>(3) Line 112. &quot;30 subjects&quot;. Are they Chinese speakers?</p>
</disp-quote>
<p>Yes, all participants in the present study, including those in the pre-tests, are native Chinese speakers.</p>
<disp-quote content-type="editor-comment">
<p>(4) Line 122, &quot;responses for each item&quot; Please specify whether you mean here &quot;the comprehensive answer&quot; as you defined in 118-119.</p>
</disp-quote>
<p>Yes, and this information has been added in Lines 136-137: ‘comprehensive responses for each item were converted into Shannon's entropy (H)’</p>
<disp-quote content-type="editor-comment">
<p>(5) Line 163 &quot;one of three stimulus types (Anodal, Cathodal or Sham)&quot;. Please specify whether the order of the three conditions was counterbalanced across participants. Or, whether the order was fixed for all participants.</p>
</disp-quote>
<p>The order of the three conditions was counterbalanced across participants, a clearer description has been added in the revised manuscript in Lines 184-189: ‘Participants were divided into two groups, with each group undergoing HD-tDCS stimulation at different target sites (IFG or pMTG). Each participant completed three experimental sessions, spaced one week apart, during which 480 gesture-speech pairs were presented across various conditions. In each session, participants received one of three types of HD-tDCS stimulation: Anodal, Cathodal, or Sham. The order of stimulation site and type was counterbalanced using a Latin square design to control for potential order effects.’</p>
<disp-quote content-type="editor-comment">
<p>(6) Line 191-192, &quot;difference in reaction time between semantic incongruence and semantic congruent pairs)&quot; Here, please specify which reaction time was subtracted from which one. This information is very crucial; without it, you cannot interpret your graphs.</p>
<p>(17) Figure 3. Figure caption for (A). &quot;The semantic congruence effect was calculated as the reaction time difference between...&quot;. You need to specify which condition was subtracted from what condition; otherwise, you cannot interpret this figure. &quot;difference&quot; is too ambiguous.</p>
</disp-quote>
<p>Corrections have been made in the revised manuscript in Lines 208-211: ‘Neural responses were quantified based on the effects of HD-tDCS (active tDCS minus sham tDCS) on the semantic congruency effect, defined as the difference in reaction times between semantic incongruent and congruent conditions (Rt(incongruent) - Rt(congruent))’ and Line 796-798: ‘The semantic congruency effect was calculated as the reaction time (RT) difference between semantically incongruent and semantically congruent pairs (Rt(incongruent) - Rt(congruent))’.</p>
<disp-quote content-type="editor-comment">
<p>(7) Line 363 &quot;progressive inhibition of IFG and pMTG by HD-tDCS as the degree of gesture-speech interaction, indexed by MI, advanced.&quot; This sentence is very hard to follow. I don't understand what part of the data in Figure 3 speaks to &quot;inhibition of IFG&quot;. And what is &quot;HD-tDCS&quot;? I think it is easier to read if you talk about correlation (not &quot;progressive&quot; and &quot;advanced&quot;).</p>
</disp-quote>
<p>High-Definition transcranial direct current stimulation (HD-tDCS) was applied to modulate the activity of pMTG and IFG, with cathodal stimulation inducing inhibitory effects and anodal stimulation facilitating neural activity. In Figure 3, we examined the relationship between the tDCS effects on pMTG and IFG and the three information matrices (entropy and MI). Our results revealed significant correlations between MI and the cathodal-tDCS effects in both regions. We acknowledge that the original phrasing may have been unclear, and in the revised manuscript, we have provided a more explicit explanation to enhance clarity in Lines 443-445: ‘Our results, for the first time, revealed that the inhibition effect of cathodal-tDCS on the pMTG and IFG correlated with the degree of gesture-speech multisensory convergence, as indexed by MI’.</p>
<disp-quote content-type="editor-comment">
<p>(8) Lines 367-368 I don't understand why gesture is top down and speech is bottom up. Is that because gesture precedes speech (gesture is interpretable at the point of speech onset)?</p>
</disp-quote>
<p>Yes, since we employed a semantic priming paradigm by aligning speech onset with the gesture comprehension point, we interpret the gesture-speech integration process as an interaction between the top-down prediction from gestures and the bottom-up processing of speech. In the revised manuscript, we have provided a clearer and more coherent description that aligns with the results. Lines 445-449: ‘Moreover, the gradual neural engagement was found to be time-sensitive and staged, as evidenced by the selectively interrupted time windows (Experiment 2) and the distinct correlated ERP components (Experiment 3), which were modulated by different information contributors, including unisensory entropy or multisensory MI’</p>
<disp-quote content-type="editor-comment">
<p>(9) Line 380 - 381. Can you spell out &quot;TW&quot; and &quot;IP&quot;?</p>
<p>(16) Line 448, NIBS, Please spell out &quot;NIBS&quot;.</p>
</disp-quote>
<p>&quot;TW&quot; have been spelled out in Lines 459: ‘time windows (TW)’,&quot;IP&quot; in Line 460: ‘identification point (IP)’. The term &quot;NIBS&quot; was replaced with &quot;HD-tDCS and TMS&quot; to provide clearer specification of the techniques employed: ‘Consistent with this, the present study provides robust evidence, through the application of HD-tDCS and TMS, that the integration hubs for gesture and speech—the pMTG and IFG—operate in an incremental manner.’ (Lines 454-457).</p>
<disp-quote content-type="editor-comment">
<p>(10) Line 419, The higher certainty of gesture =&gt; The higher the certainty of gesture is</p>
<p>(13) Line 428, &quot;a larger MI&quot; =&gt; &quot;a larger MI is&quot;</p>
<p>(12) Line 427-428, &quot;the larger overlapped neural populations&quot; =&gt; &quot;the larger, the overlapped neural populations&quot;</p>
</disp-quote>
<p>Changes have been made in Line 522 ‘The higher the certainty of gesture is’ , Line 531: ‘a larger MI is’ and Line 530 ‘the larger, overlapped neural populations’</p>
<disp-quote content-type="editor-comment">
<p>(11) Line 423 &quot;Greater TMS effect over the IFG&quot; Can you describe the TMS effect?</p>
</disp-quote>
<p>TMS effect has been described as ‘Greater TMS inhibitory effect’ (Line 526)</p>
<disp-quote content-type="editor-comment">
<p>(14) Line 423 &quot;reweighting effect&quot; What is this? Please describe (and say which experiment it is about).</p>
</disp-quote>
<p>Clearer description has been provided in Lines 535-538: ‘As speech entropy increases, indicating greater uncertainty in the information provided by speech, more cognitive effort is directed towards selecting the targeted semantic representation. This leads to enhanced involvement of the IFG and a corresponding reduction in LPC amplitude’.</p>
<disp-quote content-type="editor-comment">
<p>(15) Line 437 &quot;the graded functionality of every disturbed period is not guaranteed&quot; (I don't understand this sentence).</p>
</disp-quote>
<p>Clearer description has been provided in Lines 552-557: ‘Additionally, not all influenced TWs exhibited significant associations with entropy and MI. While HD-tDCS and TMS may impact functionally and anatomically connected brain regions[55,56], whether the absence of influence in certain TWs can be attributed to compensation by other connected brain areas, such as angular gyrus[57] or anterior temporal lobe[58], warrants further investigation. Therefore, caution is needed when interpreting the causal relationship between inhibition effects of brain stimulation and information-theoretic metrics (entropy and MI).<italic>’</italic></p>
<p>References:</p>
<p>Humphreys, G. F., Lambon Ralph, M. A., &amp; Simons, J. S. (2021). A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition. Trends in neurosciences, 44(6), 452–463. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2021.01.006">https://doi.org/10.1016/j.tins.2021.01.006</ext-link></p>
<p>Bonner, M. F., &amp; Price, A. R. (2013). Where is the anterior temporal lobe and what does it do?. The Journal of neuroscience : the official journal of the Society for Neuroscience, 33(10), 4213–4215. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0041-13.2013">https://doi.org/10.1523/JNEUROSCI.0041-13.2013</ext-link></p>
<disp-quote content-type="editor-comment">
<p>(18) Figure 4. &quot;TW1&quot;, &quot;TW2&quot;, etc. are not informative. Either replace them with the actual manuscript or add manuscript information (either in the graph itself or in the figure title).</p>
</disp-quote>
<p>Information was added into the figure title ‘Figure 4. TMS impacts on semantic congruency effect across various time windows (TW).’ (Line 804), included a detailed description of each time window in Lines 805-807: ‘(A) Five time windows (TWs) showing selective disruption of gesture-speech integration were chosen: TW1 (-120 to -80 ms relative to speech identification point), TW2 (-80 to -40 ms), TW3 (-40 to 0 ms), TW6 (80 to 120 ms), and TW7 (120 to 160 ms).’</p>
<disp-quote content-type="editor-comment">
<p>(19) Table 2C.</p>
<p>The last column is titled &quot;p(xi, yi)&quot;. I don't understand why the authors use this label for this column.</p>
<p>In the formula, at the very end, there is &quot;p(xi|yi). I wonder why it is p(xi|yi), as opposed to p(yi|xi).</p>
</disp-quote>
<p>Mutual Information (MI) was calculated by subtracting the entropy of the combined gesture-speech dataset (Entropy(gesture + speech)) from the sum of the individual entropies of gesture and speech (Entropy(gesture) + Entropy(speech)). Thus, the p(xi,yi) aimed to describe the entropy of the combined dataset. We acknowledge the potential ambiguity in the original description, and in the revised manuscript, we have changed the formula of p(xi,yi) into ‘p(xi+yi)’ (Line 848) in Table 2C, and the relevant equation of MI ‘<inline-formula id="sa3equ2"><inline-graphic xlink:href="elife-99416-sa3-equ2.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>’. Also we provided a clear MI calculation process in Lines 143-146: ‘MI was used to measure the overlap between gesture and speech information, calculated by subtracting the entropy of the combined gesture-speech dataset (Entropy(gesture + speech)) from the sum of their individual entropies (Entropy(gesture) + Entropy(speech)) (see Appendix Table 2C)’.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations for the authors):</bold></p>
<p>(1) The authors should try and produce data showing that the confound of difficulty due to the number of lexical or semantic representations is not underlying high-entropy items if they wish to improve the credibility of their claim that the disruption of the congruency effect is due to speech-gesture integration. Additionally, they should provide more evidence either in the form of experiments or references to better justify why mutual information is an index for integration in the first place.</p>
</disp-quote>
<p>Response 1: An additional analysis has been conducted to assess whether the number of lexical or semantic representations affect the neural outcomes, please see details in the Responses to Reviewer 3 (public review) response 1.</p>
<p>Mutual information (MI), a concept rooted in information theory, quantifies the reduction in uncertainty about one signal when the other is known, thereby capturing the statistical dependence between them. MI is calculated as the difference between the individual entropies of each signal and their joint entropy, which reflects the total uncertainty when both signals are considered together. This metric aligns with the core principle of multisensory integration: different modalities reduce uncertainty about each other by providing complementary, predictive information. Higher MI values signify that the integration of sensory signals results in a more coherent and unified representation, while lower MI values indicate less integration or greater divergence between the modalities. As such, MI serves as a robust and natural index for assessing the degree of multisensory integration.</p>
<p>To date, the use of MI as an index of integration has been limited, with one notable study by Tremblay et al. (2016), cited in the manuscript, using pointwise MI to quantify the extent to which two syllables mutually constrain each other. While MI has been extensively applied in natural language processing to measure the co-occurrence strength between words (e.g., Lin et al., 2012), its application as an index of multisensory convergence—particularly in the context of gesture-speech integration as employed in this study—is novel. In the revised manuscript, we have clarified the relationship between MI and multisensory convergence: ‘MI assesses share information between modalities[25],indicating multisensory convergence and acting as an index of gesture-speech integration’ (Lines 73-74).</p>
<p>Also, in our study, we calculated MI as per its original definition, by subtracting the entropy of summed dataset of gesture-speech from the combined entropies of gesture and speech. The detailed calculation method is provided in Lines 136-152: ‘To quantify information content, comprehensive responses for each item were converted into Shannon's entropy (H) as a measure of information richness (Figure 1A bottom). With no significant gender differences observed in both gesture (<italic>t</italic>(20) = 0.21, <italic>p</italic> = 0.84) and speech (<italic>t</italic>(20) = 0.52, <italic>p</italic> = 0.61), responses were aggregated across genders, resulting in 60 answers per item (Appendix Table 2). Here, p(xi) and p(yi) represent the distribution of 60 answers for a given gesture (Appendix Table 2B) and speech (Appendix Table 2A), respectively. High entropy indicates diverse answers, reflecting broad representation, while low entropy suggests focused lexical recognition for a specific item (Figure 2B). MI was used to measure the overlap between gesture and speech information, calculated by subtracting the entropy of the combined gesture-speech dataset (Entropy(gesture + speech)) from the sum of their individual entropies (Entropy(gesture) + Entropy(speech)) (see Appendix Table 2C). For specific gesture-speech combinations, equivalence between the combined entropy and the sum of individual entropies (gesture or speech) indicates absence of overlap in response sets. Conversely, significant overlap, denoted by a considerable number of shared responses between gesture and speech datasets, leads to a noticeable discrepancy between combined entropy and the sum of gesture and speech entropies. Elevated MI values thus signify substantial overlap, indicative of a robust mutual interaction between gesture and speech.’</p>
<p>Additional examples outlined in Appendix Table 2 in Lines 841-848:</p>
<p>This novel application of MI as a multisensory convergence index offers new insights into how different sensory modalities interact and integrate to shape semantic processing.</p>
<p>Reference:</p>
<p>Tremblay, P., Deschamps, I., Baroni, M., and Hasson, U. (2016). Neural sensitivity to syllable frequency and mutual information in speech perception and production. Neuroimage 136, 106-121. 10.1016/j.neuroimage.2016.05.018</p>
<p>Lin, W., Wu, Y., &amp; Yu, L. (2012). Online Computation of Mutual Information and Word Context Entropy. International Journal of Future Computer and Communication, 167-169.</p>
<disp-quote content-type="editor-comment">
<p>(2) Finally, if the authors wish to address the graded hub hypothesis as posited by the controlled semantic cognition framework (e.g., Rice et al., 2015), they would have to stimulate a series of ROIs progressing gradually through the anatomy of their candidate regions showing the effects grow along this spline, more than simply correlate MI with RT differences.</p>
</disp-quote>
<p>Response 2: We appreciate the reviewer’s thoughtful consideration. The incremental engagement of the integration hub of IFG and pMTG along with the informativeness of gesture and speech during multisensory integration is different from the concept of &quot;graded hub,&quot; which refers to anatomical distribution. See Responses to reviewer 3 (public review) response 2 for details.</p>
<disp-quote content-type="editor-comment">
<p>(3) The authors report significant effects with p values as close to the threshold as p=0.49 for the pMTG correlation in Experiment 1, for example. How confident are the authors these results are reliable and not merely their 'statistical luck'? Especially in view of sample sizes that hover around 22-24 participants, which have been called into question in the field of non-invasive brain stimulation (e.g., Mitra et al, 2021)?</p>
</disp-quote>
<p>Response 3: In Experiment 1, a total of 52 participants were assigned to two groups, each undergoing HD-tDCS stimulation over either the inferior frontal gyrus (IFG) or posterior middle temporal gyrus (pMTG), yielding 26 participants per group for correlation analysis. Power analysis, conducted using G*Power, indicated that a sample size of 26 participants per group would provide sufficient power (0.8) to detect a large effect size (0.5) at an alpha level of 0.05, justifying the chosen sample size. To control for potential statistical artifacts, we compared the results to those from the unaffected control condition.</p>
<p>In the Experiment 1, participants were tasked with a gender categorization task, where they responded as accurately and quickly as possible to the gender of the voice they saw, while gender congruency (e.g., a male gesture paired with a male voice or a female gesture with a male voice) was manipulated. This manipulation served as direct control, enabling the investigation of automatic and implicit semantic interactions between gesture and speech. This relevant information was provided in the manuscript in Lines 167-172:‘An irrelevant factor of gender congruency (e.g., a man making a gesture combined with a female voice) was created[22,23,35]. This involved aligning the gender of the voice with the corresponding gender of the gesture in either a congruent (e.g., male voice paired with a male gesture) or incongruent (e.g., male voice paired with a female gesture) manner. This approach served as a direct control mechanism, facilitating the investigation of the automatic and implicit semantic interplay between gesture and speech[35]’. Correlation analyses were conducted to examine the TMS disruption effects on gender congruency, comparing reaction times for gender-incongruent versus congruent trials. No significant correlations were found between TMS disruption effects on either the IFG (Cathodal-tDCS effect with MI: <italic>r</italic> = 0.102, <italic>p</italic> = 0.677; Anodal-tDCS effect with MI: <italic>r</italic> = 0.178, <italic>p</italic> = 0.466) or pMTG (Cathodal-tDCS effect with MI: <italic>r</italic> = -0.201, <italic>p</italic> = 0.410; Anodal-tDCS effect with MI: <italic>r</italic> = -0.232, <italic>p</italic> = 0.338).</p>
<p>Moreover, correlations between the TMS disruption effect on semantic congruency and both gesture entropy, speech entropy, and mutual information (MI) were examined. P-values of 0.290, 0.725, and 0.049 were observed, respectively.</p>
<p>The absence of a TMS effect on gender congruency, coupled with the lack of significance when correlated with the other information matrices, highlights the robustness of the significant finding at p = 0.049.</p>
<disp-quote content-type="editor-comment">
<p>(4) The distributions of entropy for gestures and speech are very unequal. Whilst entropy for gestures has high variability, (.12-4.3), that of speech is very low (ceiling effect?) with low variance. Can the authors comment on whether they think this might have affected their analyses or results in any way? For example, do they think this could be a problem when calculating MI, which integrates both measures? L130-131.'</p>
</disp-quote>
<p>Response 4: We sincerely thank the reviewer for raising this insightful question. The core premise of the current study is that brain activity is modulated by the degree of information provided. Accordingly, the 20 entropy values for gesture and speech represent a subset of the overall entropy distribution, with the degree of entropy correlating with a distributed pattern of neural activity, regardless of the scale of variation. This hypothesis aligns with previous studies suggesting that neuronal activity is linked to the probability density of sensory information, with higher levels of uncertainty resulting in the engagement of a broader population of neurons, thereby reflecting the brain’s adaptive capacity to handle diverse possible interpretations (Fischer &amp; Pena, 2011; Ganguli &amp; Simoncelli, 2014).</p>
<p>Importantly, we conducted another EEG experiment with 30 subjects. Given the inherent differences between gesture and speech, it is important to note that speech, being more structurally distinct, tends to exhibit lower variability than gesture. To prevent an imbalance in the distribution of gesture and speech, we manipulated the information content of each modality. Specifically, we created three conditions for both gesture and speech (i.e., 0.75, 1, and 1.25 times the identification threshold), thereby ensuring comparable variance between the two modalities: gesture (mean entropy = 2.91 ± 1.01) and speech (mean entropy = 1.82 ± 0.71) (Author response table 6).</p>
<p>Full-factorial RSA analysis revealed an early P1 effect (0-100 ms) for gesture and a late LPC effect (734-780 ms) for speech (Author response image 2b). Crucially, the identified clusters showed significant correlations with both gesture (Author response image 2c1) and speech entropy (Author response image 2c3), respectively. These findings replicate the results of the present study, demonstrating that, irrespective of the variance in gesture and speech entropy, both modalities elicited ERP amplitude responses in a progressive manner that aligned with their respective information distributions.</p>
<p>Regarding the influence on MI values, since MI was calculated based on the overlapping responses between gesture and speech, a reduction in uncertainty during speech comprehension would naturally result in a smaller contribution to the MI value. However, as hypothesized above, the MI values were also assumed to represent a subset of the overall distribution, where the contributions of both gesture and speech are expected to follow a normal distribution. This hypothesis was further supported by our replication experiment. When the contributions of gesture and speech were balanced, a correlation between MI values and N400 amplitude was observed (Author response image 2c2), consistent with the results reported in the present manuscript. These findings not only support the idea that the correlation between MI and ERP components is unaffected by the subset of MI values but also confirm the replicability of our results.</p>
<table-wrap id="sa3table6">
<label>Author response table 6.</label>
<caption>
<title>Quantitative entropy for each gesture stimulus (BD: before discrimination point; DP: discrimination point; AD: after discrimination point) and speech stimulus (BI: before identification point; IP: identification point; AI: after identification point).</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table6.jpg" mimetype="image"/>
</table-wrap>
<fig id="sa3fig2">
<label>Author response image 2.</label>
<caption>
<title>Results of group-level analysis and full-factorial RSA.</title>
<p>a: The full-factorial representational similarity analysis (RSA) framework is illustrated schematically. Within the general linear model (GLM), the light green matrix denotes the representational dissimilarity matrix (RDM) for gesture semantic states, while light blue matrix represents speech semantic states, and the light red matrix illustrates the semantic congruency effect. The symbol ‘e’ indicates the random error term. All matrices, including the neural dissimilarity matrix, are structured as 18 * 18 matrices, corresponding to 18 conditions (comprising 3 gesture semantic states, 3 speech semantic states, and 2 congruency conditions). b: Coding strength for gesture states, speech states and congruency effect. Shaded clusters represent regions where each factor exhibited significant effects. Clusters with lower opacity correspond to areas where the grand-mean ERP amplitudes across conditions showed the highest correlation with unimodal entropy or MI. c1-c6: Topographical correlation maps illustrate the four significant RSA clusters (top), accompanied by the highest correlations between ERP amplitudes within the significant RSA clusters and the information matrices (bottom). Black dots represent electrodes exhibiting significant correlations, while black stars highlight the electrode with the highest correlation coefficient.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-fig2.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>(5) L383: Why are the authors calling TW2 pre-lexical and TW6 post-lexical? I believe they must provide evidence or references justifying calling these periods pre- and post-lexical. This seems critical given the argument they're trying to make in this paragraph.</p>
</disp-quote>
<p>Response 5: The time windows (TWs) selected for the current study were based on our previous work (Zhao et al., 2021, J. Neurosci). In that study, we employed a double-pulse TMS protocol, delivering stimulation across eight 40-ms time windows: three windows preceding the speech identification point (TWs 1-3) and five windows following it (TWs 4-8). The pre-lexical time windows (TWs 1-3) occur before speech identification, while the post-lexical time windows (TWs 4-8) occur after this point. in the revised manuscript, we have made that clear in Lines 462-466:</p>
<p>“In TW2 of gesture-speech integration, which precedes the speech identification point23 and represents a pre-lexical stage, the suppression effect observed in the pMTG was correlated with speech entropy. Conversely, during TW6, which follows the speech identification point23 and represents a post-lexical stage, the IFG interruption effect was influenced by both gesture entropy, speech entropy, and their MI”</p>
<p>Reference:</p>
<p>Zhao, W., Li, Y., and Du, Y. (2021). TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration. The Journal of Neuroscience, 10356-10364. 10.1523/jneurosci.1355-21.2021.</p>
<disp-quote content-type="editor-comment">
<p>(6) Below, I recommend the authors improve their description of the criteria employed to select ROIs. This is important for several reasons. For example, the lack of a control ROI presumably not implicated in integration makes the interpretation of the specificity of the results difficult. Additionally, other regions have been proposed more consistently by recent evidence as multimodal integrators, like for example, the angular gyrus (Humphreys, 2021), or the anterior temporal lobe. The inclusion of IFG as a key region for integration and the oversight of angular gyrus seems to me unjustified in the light of recent evidence.</p>
</disp-quote>
<p>Response 6: We appreciate the reviewer’s thoughtful consideration. The selection of IFG and pMTG as ROIs was based on a meta-analysis of multiple fMRI studies on gesture-speech integration, in which these two locations were consistently identified as activated. See Table 2 for details of the studies and coordinates of brain locations reported.</p>
<table-wrap id="sa3table7">
<label>Author response table 7.</label>
<caption>
<title>Meta-analysis of previous studies on gesture-speech integration.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table7.jpg" mimetype="image"/>
</table-wrap>
<disp-formula id="sa3equ3">
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-equ3.jpg" mimetype="image"/>
</disp-formula>
<p>Based on the meta-analysis of previous studies, we selected the IFG and pMTG as ROIs for gesture-speech integration. The rationale for selecting these brain regions is outlined in the introduction in Lines 65-68: ‘Empirical studies have investigated the semantic integration between gesture and speech by manipulating their semantic relationship[15-18] and revealed a mutual interaction between them[19-21] as reflected by the N400 latency and amplitude[14] as well as common neural underpinnings in the left inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG)[15,22,23]’.</p>
<p>And further described in Lines 79-80: <italic>‘<italic>Experiment 1 employed high-definition transcranial direct current stimulation (HD-tDCS) to administer Anodal, Cathodal and Sham stimulation to either the IFG or the pMTG ’</italic>.</italic> And Lines 87-90: ‘Given the differential involvement of the IFG and pMTG in gesture-speech integration, shaped by top-down gesture predictions and bottom-up speech processing [23], Experiment 2 was designed to assess whether the activity of these regions was associated with relevant informational matrices’.</p>
<p>In the Methods section, we clarified the selection of coordinates in Lines 193-199: ‘Building on a meta-analysis of prior fMRI studies examining gesture-speech integration[22], we targeted Montreal Neurological Institute (MNI) coordinates for the left IFG at (-62, 16, 22) and the pMTG at (-50, -56, 10). In the stimulation protocol for HD-tDCS, the IFG was targeted using electrode F7 as the optimal cortical projection site[36], with four return electrodes placed at AF7, FC5, F9, and FT9. For the pMTG, TP7 was selected as the cortical projection site36, with return electrodes positioned at C5, P5, T9, and P9.’</p>
<p>The selection of IFG or pMTG as integration hubs for gesture and speech has also been validated in our previous studies. Specifically, Zhao et al. (2018, J. Neurosci) applied TMS to both areas. Results demonstrated that disrupting neural activity in the IFG or pMTG via TMS selectively impaired the semantic congruency effect (reaction time costs due to semantic incongruence), while leaving the gender congruency effect unaffected. These findings identified the IFG and pMTG as crucial hubs for gesture-speech integration, guiding the selection of brain regions for our subsequent studies.</p>
<p>In addition, Zhao et al. (2021, J. Neurosci) employed a double-pulse TMS protocol across eight 40-ms time windows to explore the temporal dynamics of the IFG and pMTG. The results revealed time-window-selective disruptions of the semantic congruency effect, further supporting the dynamic and temporally staged involvement of these regions in gesture-speech integration.</p>
<p>While we have solid rationale for selecting the IFG and pMTG as key regions, we acknowledge the reviewer's point that the involvement of additional functionally and anatomically brain areas, cannot be excluded. We have included in the discussion as limitations in Lines 552-557: ‘Additionally, not all influenced TWs exhibited significant associations with entropy and MI. While HD-tDCS and TMS may impact functionally and anatomically connected brain regions[55,56], whether the absence of influence in certain TWs can be attributed to compensation by other connected brain areas, such as angular gyrus[57] or anterior temporal lobe[58], warrants further investigation. Therefore, caution is needed when interpreting the causal relationship between inhibition effects of brain stimulation and information-theoretic metrics (entropy and MI).<italic>’</italic></p>
<p>References:</p>
<p>Willems, R.M., Ozyurek, A., and Hagoort, P. (2009). Differential roles for left inferior frontal and superior temporal cortex in multimodal integration of action and language. Neuroimage <italic>47</italic>, 1992-2004. 10.1016/j.neuroimage.2009.05.066.</p>
<p>Drijvers, L., Jensen, O., and Spaak, E. (2021). Rapid invisible frequency tagging reveals nonlinear integration of auditory and visual information. Human Brain Mapping <italic>42</italic>, 1138-1152. 10.1002/hbm.25282.</p>
<p>Drijvers, L., and Ozyurek, A. (2018). Native language status of the listener modulates the neural integration of speech and iconic gestures in clear and adverse listening conditions. Brain and Language <italic>177</italic>, 7-17. 10.1016/j.bandl.2018.01.003.</p>
<p>Drijvers, L., van der Plas, M., Ozyurek, A., and Jensen, O. (2019). Native and non-native listeners show similar yet distinct oscillatory dynamics when using gestures to access speech in noise. Neuroimage <italic>194</italic>, 55-67. 10.1016/j.neuroimage.2019.03.032.</p>
<p>Holle, H., and Gunter, T.C. (2007). The role of iconic gestures in speech disambiguation: ERP evidence. J Cognitive Neurosci <italic>19</italic>, 1175-1192. 10.1162/jocn.2007.19.7.1175.</p>
<p>Kita, S., and Ozyurek, A. (2003). What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking. J Mem Lang <italic>48</italic>, 16-32. 10.1016/S0749-596x(02)00505-3.</p>
<p>Bernardis, P., and Gentilucci, M. (2006). Speech and gesture share the same communication system. Neuropsychologia <italic>44</italic>, 178-190. 10.1016/j.neuropsychologia.2005.05.007.</p>
<p>Zhao, W.Y., Riggs, K., Schindler, I., and Holle, H. (2018). Transcranial magnetic stimulation over left inferior frontal and posterior temporal cortex disrupts gesture-speech integration. Journal of Neuroscience <italic>38</italic>, 1891-1900. 10.1523/Jneurosci.1748-17.2017.</p>
<p>Zhao, W., Li, Y., and Du, Y. (2021). TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration. The Journal of Neuroscience, 10356-10364. 10.1523/jneurosci.1355-21.2021.</p>
<p>Hartwigsen, G., Bzdok, D., Klein, M., Wawrzyniak, M., Stockert, A., Wrede, K., Classen, J., and Saur, D. (2017). Rapid short-term reorganization in the language network. Elife <italic>6</italic>. 10.7554/eLife.25964.</p>
<p>Jackson, R.L., Hoffman, P., Pobric, G., and Ralph, M.A.L. (2016). The semantic network at work and rest: Differential connectivity of anterior temporal lobe subregions. Journal of Neuroscience <italic>36</italic>, 1490-1501. 10.1523/JNEUROSCI.2999-15.2016.</p>
<p>Humphreys, G. F., Lambon Ralph, M. A., &amp; Simons, J. S. (2021). A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition. Trends in neurosciences, 44(6), 452–463. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2021.01.006">https://doi.org/10.1016/j.tins.2021.01.006</ext-link></p>
<p>Bonner, M. F., &amp; Price, A. R. (2013). Where is the anterior temporal lobe and what does it do?. The Journal of neuroscience : the official journal of the Society for Neuroscience, 33(10), 4213–4215. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0041-13.2013">https://doi.org/10.1523/JNEUROSCI.0041-13.2013</ext-link></p>
<disp-quote content-type="editor-comment">
<p>(7) Some writing is obscure or unclear, in part due to superfluous words like 'intricate neural processes' on L74. Or the sentence in L47 - 48 about 'quantitatively functional mental states defined by a specific parser unified by statistical regularities' which, even read in context, fails to provide clarity about what a quantitatively functional mental state is, or how it is defined by specific parsers (or what these are), and what is the link to statistical regularities. In some cases, this lack of clarity leads to difficulties assessing the appropriateness of the methods, or the exact nature of the claims. For example, do they mean degree of comprehension instead of comprehensive value? I provide some more examples below:</p>
</disp-quote>
<p>Response 7: We appreciate the reviewer’s thoughtful consideration. The revised manuscript now includes a clear description and a detailed explanation of the association with the statistical logic, addressing the concerns raised in Lines 47-55: ‘Contemporary theories frame the semantic processing as a dynamic sequence of neural states[3], shaped by systems that are finely tuned to the statistical regularities inherent in sensory inputs[4]. These regularities enable the brain to evaluate, weight, and integrate multisensory information, optimizing the reliability of individual sensory signals [5]. However, sensory inputs available to the brain are often incomplete and uncertain, necessitating adaptive neural adjustments to resolve these ambiguities[6]. In this context, neuronal activity is thought to be linked to the probability density of sensory information, with higher levels of uncertainty resulting in the engagement of a broader population of neurons, thereby reflecting the brain’s adaptive capacity to handle diverse possible interpretations[7,8].’</p>
<p>References:</p>
<p>Brennan, J.R., Stabler, E.P., Van Wagenen, S.E., Luh, W.M., and Hale, J.T. (2016). Abstract linguistic structure correlates with temporal activity during naturalistic comprehension. Brain and Language 157, 81-94. 10.1016/j.bandl.2016.04.008.</p>
<p>Benetti, S., Ferrari, A., and Pavani, F. (2023). Multimodal processing in face-to-face interactions: A bridging link between psycholinguistics and sensory neuroscience. Front Hum Neurosci 17, 1108354. 10.3389/fnhum.2023.1108354.</p>
<p>Noppeney, U. (2021). Perceptual Inference, Learning, and Attention in a Multisensory World. Annual Review of Neuroscience, Vol 44, 2021 44, 449-473. 10.1146/annurev-neuro-100120-085519.</p>
<p>Ma, W.J., and Jazayeri, M. (2014). Neural coding of uncertainty and probability. Annu Rev Neurosci 37, 205-220. 10.1146/annurev-neuro-071013-014017.</p>
<p>Fischer, B.J., and Pena, J.L. (2011). Owl's behavior and neural representation predicted by Bayesian inference. Nat Neurosci 14, 1061-1066. 10.1038/nn.2872.</p>
<p>Ganguli, D., and Simoncelli, E.P. (2014). Efficient sensory encoding and Bayesian inference with heterogeneous neural populations. Neural Comput 26, 2103-2134. 10.1162/NECO_a_00638.</p>
<disp-quote content-type="editor-comment">
<p>Comment 7.1: a) I am not too sure what they mean by 'response consistently provided by participants for four to six consecutive instances' [L117-118]. They should be clearer with the description of these 'pre-test' study methods.</p>
</disp-quote>
<p>Response 7.1: Thank you for this insightful question. An example of a participant's response to the gesture 'an' is provided below (Table 3). Initially, within 240 ms, the participant provided the answer &quot;an,&quot; which could potentially be a guess. To ensure that the participant truly comprehends the gesture, we repeatedly present it until the participant’s response stabilizes, meaning the same answer is given consistently over several trials. While one might consider fixing the number of repetitions (e.g., six trials), this could lead to participants predicting the rule and providing the same answer out of habit. To mitigate this potential bias, we allow the number of repetitions to vary flexibly between four and six trials.</p>
<p>We understand that the initial phrase might be ambiguous, in the revised manuscript, we have changed the phrase into: ‘For each gesture or speech, the action verb consistently provided by participants across four to six consecutive repetitions—with the number of repetitions varied to mitigate learning effects—was considered the comprehensive response for the gesture or speech.’ (Lines 130-133)</p>
<table-wrap id="sa3table8">
<label>Author response table 8.</label>
<caption>
<title>Example of participant's response to the gesture 'an'</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-table8.jpg" mimetype="image"/>
</table-wrap>
<disp-quote content-type="editor-comment">
<p>Comment 7.2: b) I do not understand the paragraph in L143 - 146. This is important to rephrase for clarification. What are 'stepped' neural changes? What is the purpose of 'aggregating' neural responses with identical entropy / MI values?</p>
</disp-quote>
<p>Response 7.2: It is important to note that the 20 stimuli exhibit 20 increments of gesture entropy values, 11 increments of speech entropy values, and 19 increments of mutual information values (Appendix Table 3). This discrepancy arises from the calculation of entropy and mutual information, where the distributions were derived from the comprehensive set of responses contributed by all 30 participants. As a result, these values were impacted not only by the distinct nameabilities of the stimuli but also by the entirety of responses provided. Consequently, in the context of speech entropy, 9 items demonstrate the nameability of 1, signifying unanimous comprehension among all 30 participants, resulting in an entropy of 0. Moreover, stimuli 'ning' and 'jiao' share an identical distribution, leading to an entropy of 0.63. Regarding MI, a value of 0.66 is computed for the combinations of stimuli 'sao' (gesture entropy: 4.01, speech entropy: 1.12, Author response image 32) and 'tui' (gesture entropy: 1.62, speech entropy: 0, Author response image 4). This indicates that these two sets of stimuli manifest an equivalent degree of integration.</p>
<fig id="sa3fig3">
<label>Author response image 3.</label>
<caption>
<title>Example of gesture answers (gesture sao), speech answers (speech sao), and mutual information (MI) for the ‘sao’ item</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-fig3.jpg" mimetype="image"/>
</fig>
<fig id="sa3fig4">
<label>Author response image 4.</label>
<caption>
<title>Example of gesture answers (gesture tui), speech answers (speech tui), and mutual information (MI) for the ‘tui’ item</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-fig4.jpg" mimetype="image"/>
</fig>
<p>To precisely assess whether lower entropy/MI corresponds to a smaller or larger neural response, neural responses (ERP amplitude or TMS inhibition effect) with identical entropy or MI values were averaged before undergoing correlational analysis. We understand that the phrasing might be ambiguous. Clear description has been changed in the revised manuscript in Lines 157-160: ‘To determine whether entropy or MI values corresponds to distinct neural changes, the current study first aggregated neural responses (including inhibition effects of tDCS and TMS or ERP amplitudes) that shared identical entropy or MI values, prior to conducting correlational analyses.’</p>
<disp-quote content-type="editor-comment">
<p>Comment 7.3: c) The paragraph in L160-171 is confusing. Is it an attempt to give an overview of all three experiments? If so, consider moving to the end or summarising what each experiment is at the beginning of the paragraph giving it a name (i.e., TMS). Without that, it is unclear what each experiment is counterbalancing or what 'stimulation site' refers to, for example, leading to a significant lack of clarity.</p>
</disp-quote>
<p>Response 7.3: We are sorry for the ambiguity, in the revised manuscript, we have moved the relevant phrasing to the beginning of each experiment.</p>
<p>‘Experiment 1: HD-tDCS protocol and data analysis</p>
<p>Participants were divided into two groups, with each group undergoing HD-tDCS stimulation at different target sites (IFG or pMTG). Each participant completed three experimental sessions, spaced one week apart, during which 480 gesture-speech pairs were presented across various conditions. In each session, participants received one of three types of HD-tDCS stimulation: Anodal, Cathodal, or Sham. The order of stimulation site and type was counterbalanced using a Latin square design to control for potential order effects’ (Lines 183-189)</p>
<p>‘Experiment 2: TMS protocol and data analysis</p>
<p>Experiment 2 involved 800 gesture-speech pairs, presented across 15 blocks over three days, with one week between sessions. Stimulation was administered at three different sites (IFG, pMTG, or Vertex). Within the time windows (TWs) spanning the gesture-speech integration period, five TWs that exhibited selective disruption of integration were selected: TW1 (-120 to -80 ms relative to the speech identification point), TW2 (-80 to -40 ms), TW3 (-40 to 0 ms), TW6 (80 to 120 ms), and TW7 (120 to 160 ms)23 (Figure 1C). The order of stimulation site and TW was counterbalanced using a Latin square design.’ (Lines 223-230)</p>
<p>‘Experiment 3: Electroencephalogram (EEG) recording and data analysis</p>
<p>Experiment 3, comprising a total of 1760 gesture-speech pairs, was completed in a single-day session.’ (Lines 249-250)</p>
<disp-quote content-type="editor-comment">
<p>Comment 7.4: d) L402-406: This sentence is not clear. What do the authors mean by 'the state of [the neural landscape] constructs gradually as measured by entropy and MI'? How does this construct a neural landscape? The authors must rephrase this paragraph using clearer language since in its current state it is very difficult to assess whether it is supported by the evidence they present.</p>
</disp-quote>
<p>Response 7.4: We are sorry for the ambiguity, in the revised manuscript we have provided clear description in Lines 483-492: ‘The varying contributions of unisensory gesture-speech information and the convergence of multisensory inputs, as reflected in the correlation between distinct ERP components and TMS time windows (TMS TWs), are consistent with recent models suggesting that multisensory processing involves parallel detection of modality-specific information and hierarchical integration across multiple neural levels[4,48]. These processes are further characterized by coordination across multiple temporal scales[49]. Building on this, the present study offers additional evidence that the multi-level nature of gesture-speech processing is statistically structured, as measured by information matrix of unisensory entropy and multisensory convergence index of MI, the input of either source would activate a distributed representation, resulting in progressively functioning neural responses’</p>
<p>References:</p>
<p>Benetti, S., Ferrari, A., and Pavani, F. (2023). Multimodal processing in face-to-face interactions: A bridging link between psycholinguistics and sensory neuroscience. Front Hum Neurosci 17, 1108354. 10.3389/fnhum.2023.1108354.</p>
<p>Meijer, G.T., Mertens, P.E.C., Pennartz, C.M.A., Olcese, U., and Lansink, C.S. (2019). The circuit architecture of cortical multisensory processing: Distinct functions jointly operating within a common anatomical network. Prog Neurobiol 174, 1-15. 10.1016/j.pneurobio.2019.01.004.</p>
<p>Senkowski, D., and Engel, A.K. (2024). Multi-timescale neural dynamics for multisensory integration. Nat Rev Neurosci 25, 625-642. 10.1038/s41583-024-00845-7.</p>
<disp-quote content-type="editor-comment">
<p>(8) Some writing suffers from conceptual equivocation. For example, the link between 'multimodal representation' and gesture as a type of multimodal extralinguistic information is not straightforward. What 'multimodal representations' usually refer to in semantic cognition is not the co-occurrence of gesture and speech, but the different sources or modalities that inform the structure of a semantic representation or concept (not the fact we use another modality vision to perceive gestures that enrich the linguistic auditory communication of said concepts). See also my comment in the public review regarding the conceptual conflation of the graded hub hypothesis.</p>
</disp-quote>
<p>Response 8: We aimed to clarify that the integration of gesture and speech, along with the unified representation it entails, is not merely a process whereby perceived gestures enhance speech comprehension. Rather, there exists a bidirectional influence between these two modalities, affecting both their external forms (Bernaidis et al., 2006) and their semantic content (Kita et al., 2003; Kelly et al., 2010). Given that multisensory processing is recognized as an interplay of both top-down and bottom-up mechanisms, we hypothesize that this bidirectional semantic influence between gesture and speech operates similarly. Consequently, we recorded neural responses—specifically the inhibitory effects observed through TMS/tDCS or ERP components—beginning at the onset of speech, which marks the moment when both modalities are accessible.</p>
<p>We prioritize gesture for two primary reasons. Firstly, from a naturalistic perspective, speech and gesture are temporally aligned; gestures typically precede their corresponding speech segments by less than one second (Morrelsamuls et al., 1992). This temporal alignment has prompted extensive research aimed at identifying the time windows during which integration occurs (Obermeier et al., 2011, 2015). Results indicate that local integration of gesture and speech occurs within a time frame extending from -200 ms to +120 ms relative to gesture-speech alignment, where -200 ms indicates that gestures occur 200 ms before speech onset, and +120 ms signifies gestures occurring after the identification point of speech.</p>
<p>Secondly, in our previous study (Zhao, 2023), we investigated this phenomenon by manipulating gesture-speech alignment across two conditions: (1) gestures preceding speech by a fixed interval of 200 ms, and (2) gestures preceding speech at its semantic identification point. Notably, only in the second condition did we observe time-window-selective disruptions of the semantic congruency effect in the IFG and pMTG. This led us to conclude that gestures serve a semantic priming function for co-occurring speech.</p>
<p>We recognize that our previous use of the term &quot;co-occurring speech&quot; may have led to ambiguity. Therefore, in the revised manuscript, we have replaced those sentences with a detailed description of the properties of each modality in Lines 60-62: ‘Even though gestures convey information in a global-synthetic way, while speech conveys information in a linear segmented way, there exists a bidirectional semantic influence between the two modalities[9,10]’</p>
<p>Conceptual conflation of the graded hub hypothesis has been clarified in the Response to Reviewer 3 (public review) response 2.</p>
<p>References:</p>
<p>Bernardis, P., &amp; Gentilucci, M. (2006). Speech and gesture share the same communication system. Neuropsychologia, 44(2), 178-190</p>
<p>Kelly, S. D., Ozyurek, A., &amp; Maris, E. (2010b). Two sides of the same coin: speech and gesture mutually interact to enhance comprehension. Psychological Science, 21(2), 260-267. doi:10.1177/0956797609357327</p>
<p>Kita, S., &amp; Ozyurek, A. (2003). What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking. Journal of Memory and Language, 48(1), 16-32. doi:10.1016/s0749-596x(02)00505-3</p>
<p>Obermeier, C., &amp; Gunter, T. C. (2015). Multisensory Integration: The Case of a Time Window of Gesture-Speech Integration. Journal of Cognitive Neuroscience, 27(2), 292-307. doi:10.1162/jocn_a_00688</p>
<p>Obermeier, C., Holle, H., &amp; Gunter, T. C. (2011). What Iconic Gesture Fragments Reveal about Gesture-Speech Integration: When Synchrony Is Lost, Memory Can Help. Journal of Cognitive Neuroscience, 23(7), 1648-1663. doi:10.1162/jocn.2010.21498</p>
<p>Morrelsamuels, P., &amp; Krauss, R. M. (1992). WORD FAMILIARITY PREDICTS TEMPORAL ASYNCHRONY OF HAND GESTURES AND SPEECH. Journal of Experimental Psychology-Learning Memory and Cognition, 18(3), 615-622. doi:10.1037/0278-7393.18.3.615</p>
<p>Hostetter, A., and Mainela-Arnold, E. (2015). Gestures occur with spatial and Motoric knowledge: It's more than just coincidence. Perspectives on Language Learning and Education 22, 42-49. doi:10.1044/lle22.2.42.</p>
<p>McNeill, D. (2005). Gesture and though (University of Chicago Press). 10.7208/chicago/9780226514642.001.0001.</p>
<p>Zhao, W. (2023). TMS reveals a two-stage priming circuit of gesture-speech integration. Front Psychol 14, 1156087. 10.3389/fpsyg.2023.1156087.</p>
<disp-quote content-type="editor-comment">
<p>(9) The last paragraph of the introduction lacks a conductive thread. The authors describe three experiments without guiding the reader through a connecting thread underlying the experiments. Feels more like three disconnected studies than a targeted multi-experiment approach to solve a problem. What is each experiment contributing to? What is the 'grand question' or thread unifying these?</p>
</disp-quote>
<p>Response 9: The present study introduced three experiments to explore the neural activity linked to the amount of information processed during multisensory gesture-speech integration. In Experiment 1, we observed that the extent of inhibition in the pMTG and LIFG was closely linked to the overlapping gesture-speech responses, as quantified by mutual information. Building on the established roles of the pMTG and LIFG in our previous study (Zhao et al., 2021, JN), we then expanded our investigation to determine whether the dynamic neural engagement between the pMTG and LIFG during gesture-speech processing was also associated with the quality of the information. This hypothesis was further validated through high-temporal resolution EEG, where we examined ERP components related to varying information qualities. Notably, we observed a close time alignment between the ERP components and the time windows of the TMS effects, which were associated with the same informational matrices in gesture-speech processing.</p>
<p>Linkage of the three experiments has been clarified in the introduction in Lines 75-102: ‘</p>
<p>To investigate the neural mechanisms underlying gesture-speech integration, we conducted three experiments to assess how neural activity correlates with distributed multisensory integration, quantified using information-theoretic measures of MI. Additionally, we examined the contributions of unisensory signals in this process, quantified through unisensory entropy. Experiment 1 employed high-definition transcranial direct current stimulation (HD-tDCS) to administer Anodal, Cathodal and Sham stimulation to either the IFG or the pMTG. HD-tDCS induces membrane depolarization with anodal stimulation and membrane hyperpolarization with cathodal stimulation[26], thereby increasing or decreasing cortical excitability in the targeted brain area, respectively. This experiment aimed to determine whether the overall facilitation (Anodal-tDCS minus Sham-tDCS) and/or inhibitory (Cathodal-tDCS minus Sham-tDCS) of these integration hubs is modulated by the degree of gesture-speech integration, as measure by MI.</p>
<p>Given the differential involvement of the IFG and pMTG in gesture-speech integration, shaped by top-down gesture predictions and bottom-up speech processing [23], Experiment 2 was designed to further assess whether the activity of these regions was associated with relevant informational matrices. Specifically, we applied inhibitory chronometric double-pulse transcranial magnetic stimulation (TMS) to specific temporal windows associated with integration processes in these regions[23], assessing whether the inhibitory effects of TMS were correlated with unisensory entropy or the multisensory convergence index (MI).</p>
<p>Experiment 3 complemented these investigations by focusing on the temporal dynamics of neural responses during semantic processing, leveraging high-temporal event-related potentials (ERPs). This experiment investigated how distinct information contributors modulated specific ERP components associated with semantic processing. These components included the early sensory effects as P1 and N1–P2[27,28], the N400 semantic conflict effect[14,28,29], and the late positive component (LPC) reconstruction effect[30,31]. By integrating these ERP findings with results from Experiments 1 and 2, Experiment 3 aimed to provide a more comprehensive understanding of how gesture-speech integration is modulated by neural dynamics’</p>
<p>References:</p>
<p>Bikson, M., Inoue, M., Akiyama, H., Deans, J.K., Fox, J.E., Miyakawa, H., and Jefferys, J.G.R. (2004). Effects of uniform extracellular DC electric fields on excitability in rat hippocampal slices. J Physiol-London <italic>557</italic>, 175-190. 10.1113/jphysiol.2003.055772.</p>
<p>Federmeier, K.D., Mai, H., and Kutas, M. (2005). Both sides get the point: hemispheric sensitivities to sentential constraint. Memory &amp; Cognition <italic>33</italic>, 871-886. 10.3758/bf03193082.</p>
<p>Kelly, S.D., Kravitz, C., and Hopkins, M. (2004). Neural correlates of bimodal speech and gesture comprehension. Brain and Language <italic>89</italic>, 253-260. 10.1016/s0093-934x(03)00335-3.</p>
<p>Wu, Y.C., and Coulson, S. (2005). Meaningful gestures: Electrophysiological indices of iconic gesture comprehension. Psychophysiology <italic>42</italic>, 654-667. 10.1111/j.1469-8986.2005.00356.x.</p>
<p>Fritz, I., Kita, S., Littlemore, J., and Krott, A. (2021). Multimodal language processing: How preceding discourse constrains gesture interpretation and affects gesture integration when gestures do not synchronise with semantic affiliates. J Mem Lang <italic>117</italic>, 104191. 10.1016/j.jml.2020.104191.</p>
<p>Gunter, T.C., and Weinbrenner, J.E.D. (2017). When to take a gesture seriously: On how we use and prioritize communicative cues. J Cognitive Neurosci <italic>29</italic>, 1355-1367. 10.1162/jocn_a_01125.</p>
<p>Ozyurek, A., Willems, R.M., Kita, S., and Hagoort, P. (2007). On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials. J Cognitive Neurosci <italic>19</italic>, 605-616. 10.1162/jocn.2007.19.4.605.</p>
<p>Zhao, W., Li, Y., and Du, Y. (2021). TMS reveals dynamic interaction between inferior frontal gyrus and posterior middle temporal gyrus in gesture-speech semantic integration. The Journal of Neuroscience, 10356-10364. 10.1523/jneurosci.1355-21.2021.</p>
<disp-quote content-type="editor-comment">
<p>(10) The authors should provide a clearer figure to appreciate their paradigm, illustrating clearly the stimulus presentation (gesture and speech).</p>
</disp-quote>
<p>Response 10: To reduce ambiguity, unnecessary arrows were deleted from Figure 1.</p>
<disp-quote content-type="editor-comment">
<p>Comment 11.1: (11) Required methodological clarifications to better assess the strength of the evidence presented:</p>
<p>a) Were the exclusion criteria only handedness and vision? Did the authors exclude based on neurological and psychiatric disorders? Psychoactive drugs? If not, do they think the lack of these exclusion criteria might have influenced their results?</p>
</disp-quote>
<p>Response 11.1: Upon registration, each participant is required to complete a questionnaire alongside the consent form and handedness questionnaire. This procedure is designed to exclude individuals with potential neurological or psychiatric disorders, as well as other factors that may affect their mental state or reaction times. Consequently, all participants reported in the manuscript do not have any of the aforementioned neurological or psychiatric disorders. The questionnaire is attached below:</p>
<fig id="sa3fig5">
<label>Author response image 4.</label>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-fig5.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Comment 11.2: b) Are the subjects from the pre-tests (L112-113) and the replication study (L107) a separate sample or did they take part in Experiments 1-3?</p>
</disp-quote>
<p>Response 11.2: The participants in each pre-test and experiment were independent, resulting in a total of 188 subjects. Since the stimuli utilized in this study were previously validated and reported (Zhao et al., 2021), the 90 subjects who participated in the three pre-tests are not included in the final count for the current study, leaving a total of 98 participants reported in the manuscript in Lines 103-104: ‘Ninety-eight young Chinese participants signed written informed consent forms and took part in the present study’.</p>
<disp-quote content-type="editor-comment">
<p>Comment 11.3: c) L176. The authors should explain how they selected ROIs. This is very important for the reasons outlined above.</p>
</disp-quote>
<p>Response 11.3: Please see Response to Comment 6 for details.</p>
<disp-quote content-type="editor-comment">
<p>Comment 11.4: d) The rationale for Experiment 1 and its analysis approach should be explicitly described. Why perform Pearson correlations? What is the conceptual explanation of the semantic congruency effect and why should it be expected to correlate with the three information-theoretic metrics? What effects could the authors expect to find and what would they mean? There is a brief description in L187-195 but it is unclear.</p>
</disp-quote>
<p>Response 11.4: We thank the reviewer for their rigorous consideration. The semantic congruency effect is widely used as an index of multisensory integration. Therefore, the effects of HD-tDCS on the IFG and pMTG, as measured by changes in the semantic congruency effect, serve as an indicator of altered neural responses to multisensory integration. In correlating these changes with behavioral indices of information degree, we aimed to assess whether the integration hubs (IFG and pMTG) function progressively during multisensory gesture-speech integration. The rationale for using Pearson correlations is based on the hypothesis that the 20 sets of stimuli used in this study represent a sample from a normally distributed population. Thus, even with changes in the sample (e.g., using another 20 values), the gradual relationship between neural responses and the degree of information would remain unchanged. This hypothesis is supported by the findings from another experiment (see details in Response to Comment 4).</p>
<p>In the revised manuscript, we have provided a clear description of the rationale for Experiment 1 in Lines 206-219: ‘To examine the relationship between the degree of information and neural responses, we conducted Pearson correlation analyses using a sample of 20 sets. Neural responses were quantified based on the effects of HD-tDCS (active tDCS minus sham tDCS) on the semantic congruency effect, defined as the difference in reaction times between semantic incongruent and congruent conditions (Rt(incongruent) - Rt(congruent)). This effect served as an index of multisensory integration[35] within the left IFG and pMTG. The variation in information was assessed using three information-theoretic metrics. To account for potential confounds related to multiple candidate representations, we conducted partial correlation analyses between the tDCS effects and gesture entropy, speech entropy, and MI, controlling for the number of responses provided for each gesture and speech, as well as the total number of combined responses. Given that HD-tDCS induces overall disruption at the targeted brain regions, we hypothesized that the neural activity within the left IFG and pMTG would be progressively affected by varying levels of multisensory convergence, as indexed by MI.’</p>
<p>Additionally, in the introduction, we have rephrased the relevant rationale in Lines 75-86: _‘_To investigate the neural mechanisms underlying gesture-speech integration, we conducted three experiments to assess how neural activity correlates with distributed multisensory integration, quantified using information-theoretic measures of MI. Additionally, we examined the contributions of unisensory signals in this process, quantified through unisensory entropy. Experiment 1 employed high-definition transcranial direct current stimulation (HD-tDCS) to administer Anodal, Cathodal and Sham stimulation to either the IFG or the pMTG. HD-tDCS induces membrane depolarization with anodal stimulation and membrane hyperpolarization with cathodal stimulation[26], thereby increasing or decreasing cortical excitability in the targeted brain area, respectively. This experiment aimed to determine whether the overall facilitation (Anodal-tDCS minus Sham-tDCS) and/or inhibitory (Cathodal-tDCS minus Sham-tDCS) of these integration hubs is modulated by the degree of gesture-speech integration, as measure by MI</p>
<p>Reference:</p>
<p>Kelly, S.D., Creigh, P., and Bartolotti, J. (2010). Integrating speech and iconic gestures in a Stroop-like task: Evidence for automatic processing. Journal of Cognitive Neuroscience <italic>22</italic>, 683-694. 10.1162/jocn.2009.21254.</p>
<disp-quote content-type="editor-comment">
<p>Comment 11.5: e) The authors do not mention in the methods if FDR correction was applied to the Pearson correlations in Experiment 1. There is a mention in the Results Figure, but it is unclear if it was applied consistently. Can the authors confirm, and explicitly state the way they carried out FDR correction for this family of tests in Experiment 1? This is especially important in the light of some of their results having a p-value of p=.049.</p>
</disp-quote>
<p>Response 11.5: FDR correction was applied to Experiment 1, and all reported p-values were corrected using this method. In the revised manuscript, we have included a reference to FDR correction in Lines 221-222: ‘False discovery rate (FDR) correction was applied for multiple comparisons.’</p>
<p>In Experiment 1, since two separate participant groups (each N = 26) were recruited for the HD-tDCS over either the IFG or pMTG, FDR correction was performed separately for each group. Therefore, for each brain region, six comparisons (three information matrices × two tDCS effects: anodal-sham or cathodal-sham) were submitted for FDR correction.</p>
<p>In Experiment 2, six comparisons (three information matrices × two sites: IFG or pMTG) were submitted for FDR correction. In Experiment 3, FDR correction was applied to the seven regions of interest (ROIs) within each component, resulting in five comparisons</p>
<p>The confidence of a p-value of 0.049 was clarified in Response to Comment 3.</p>
<disp-quote content-type="editor-comment">
<p>Comment 11.6: f) L200. What does the abbreviation 'TW' stands for in this paragraph? When was it introduced in the main text? The description is in the Figure, but it should be moved to the main text.]</p>
<p>Comment 11.7: g) How were the TWs chosen? Is it the criterion in L201-203? If so, it should be moved to the start of the paragraph. What does the word 'selected' refer to in that description? Selected for what? The explanation seems to be in the Figure, but it should be in the main text. It is still not a complete explanation. What were the criteria for assigning TWs to the IFG or pMTG?</p>
</disp-quote>
<p>Response 11.6&amp; 11.7: Since the two comments are related, we will provide a synthesized response. 'TW' refers to time window, the selection of which was based on our previous study (Zhao et al., 2021, J. Neurosci). In Zhao et al. (2021), we employed the same experimental protocol—using inhibitory double-pulse transcranial magnetic stimulation (TMS) over the IFG and pMTG in one of eight 40-ms time windows relative to the speech identification point (IP; the minimal length of lexical speech), with three time windows before the speech IP and five after. Based on this previous work, we believe that these time windows encompass the potential gesture-speech integration process. Results demonstrated a time-window-selective disruption of the semantic congruency effect (i.e., reaction time costs driven by semantic conflict), with no significant modulation of the gender congruency effect (i.e., reaction time costs due to gender conflict), when stimulating the left pMTG in TW1, TW2, and TW7, and when stimulating the left IFG in TW3 and TW6. Based on these findings, the present study selected the five time windows that showed a selective disruption effect during gesture-speech integration.</p>
<p>Note that in the present study, we applied stimulation to both the IFG and pMTG across all five time windows, and further correlated the TMS disruption effects with the three information matrices.</p>
<p>We recognize that the rationale for the choice of time windows was not sufficiently explained in the original manuscript. In the revised manuscript, we have added the relevant description in Lines 223-228: ‘Stimulation was administered at three different sites (IFG, pMTG, or Vertex). Within the time windows (TWs) spanning the gesture-speech integration period, five TWs that exhibited selective disruption of integration were selected: TW1 (-120 to -80 ms relative to the speech identification point), TW2 (-80 to -40 ms), TW3 (-40 to 0 ms), TW6 (80 to 120 ms), and TW7 (120 to 160 ms)[23] (Figure 1C). The order of stimulation site and TW was counterbalanced using a Latin square design.’</p>
<disp-quote content-type="editor-comment">
<p>Comment 11.8: h) Again, the rationale for the Pearson correlations of semantic congruency with information-theoretic metrics should be explicitly outlined. What is this conceptually?</p>
</disp-quote>
<p>Response 11.8: Given that the rationale behind Experiment 1 and Experiment 2 is similar—both investigating the correlation between interrupted neural effects and the degree of information—we believe that the introduction of the Pearson correlation between semantic congruency and information-theoretic metrics, as presented in Experiment 1 (see Response to Comment 11.4 for details), is sufficient for both experiments.</p>
<disp-quote content-type="editor-comment">
<p>Comment 11.9: i)What does 'gesture stoke' mean in the Figure referring to Experiment 3? Figure 1D is not clear. What are the arrows referring to?</p>
</disp-quote>
<p>Response 11.9: According to McNeill (1992), gesture phases differ based on whether the gesture depicts imagery. Iconic and metaphoric gestures are imagistic and typically consist of three phases: a preparation phase, a stroke phase, and a retraction phrase. Figure 4 provides an example of these three phases using the gesture ‘break’. In the preparation phase, the hand and arm move away from their resting position to a location in gesture space where the stroke begins. As illustrated in the first row of Figure 4, during the preparation phase of the ‘break’ gesture, the hands, initially in a fist and positioned downward, rise to a center-front position. In the stroke phase, the meaning of the gesture is conveyed. This phase occurs in the central gesture space and is synchronized with the linguistic segments it co-expresses. For example, in the stroke phase of the ‘break’ gesture (second row of Figure 4), the two fists move 90 degrees outward before returning to a face-down position. The retraction phase involves the return of the hand from the stroke position to the rest position. In the case of the ‘break’ gesture, this involves moving the fists from the center front back into the resting position (see third row of Figure 4).</p>
<p>Therefore, in studies examining gesture-speech integration, gestures are typically analyzed starting from the stroke phase (Habets et al., 2011; Kelly et al., 2010), a convention also adopted in our previous studies (Zhao et al., 2018, 2021, 2023). We acknowledge that this should be explained explicitly, and in the revised manuscript, we have added the following clarification in Lines 162-166: ‘Given that gestures induce a semantic priming effect on concurrent speech[33], this study utilized a semantic priming paradigm in which speech onset was aligned with the DP of each gesture[23,33], the point at which the gesture transitions into a lexical form[34]. The gesture itself began at the stroke phase, a critical moment when the gesture conveys its primary semantic content[34].’</p>
<p>Additionally, Figure 1 has been revised in the manuscript to eliminate ambiguous arrows. (see Response 10 for detail).</p>
<fig id="sa3fig6">
<label>Author response image 5.</label>
<caption>
<title>An illustration of the gesture phases of the 'break' gesture.</title>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-99416-sa3-fig6.jpg" mimetype="image"/>
</fig>
<p>References：</p>
<p>Habets, B., Kita, S., Shao, Z. S., Ozyurek, A., &amp; Hagoort, P. (2011). The Role of Synchrony and Ambiguity in Speech-Gesture Integration during Comprehension. <italic>Journal of Cognitive Neuroscience, 23</italic>(8), 1845-1854. doi:10.1162/jocn.2010.21462</p>
<p>Kelly, S. D., Creigh, P., &amp; Bartolotti, J. (2010). Integrating Speech and Iconic Gestures in a Stroop-like Task: Evidence for Automatic Processing. <italic>Journal of Cognitive Neuroscience, 22</italic>(4), 683-694. doi:DOI 10.1162/jocn.2009.21254</p>
<disp-quote content-type="editor-comment">
<p>Comment 11.10: j) L236-237: &quot;Consequently, four ERP components were predetermined&quot; is very confusing. Were these components predetermined? Or were they determined as a consequence of the comparison between the higher and lower halves for the IT metrics described above in the same paragraph? The description of the methods is not clear.</p>
</disp-quote>
<p>Response 11.10: The components selected were based on a comparison between the higher and lower halves of the information metrics. By stating that these components were predetermined, we aimed to emphasize that the components used in our study are consistent with those identified in previous research on semantic processing. We acknowledge that the phrasing may have been unclear, and in the revised manuscript, we have provided a more explicit description in Lines 267-276: ‘To consolidate the data, we conducted both a traditional region-of-interest (ROI) analysis, with ROIs defined based on a well-established work[40], and a cluster-based permutation approach, which utilizes data-driven permutations to enhance robustness and address multiple comparisons.</p>
<p>For the traditional ROI analysis, grand-average ERPs at electrode Cz were compared between the higher (≥50%) and lower (&lt;50%) halves for gesture entropy (Figure 5A1), speech entropy (Figure 5B1), and MI (Figure 5C1). Consequently, four ERP components were determined: the P1 effect observed within the time window of 0-100 ms[27,28], the N1-P2 effect observed between 150-250ms[27,28], the N400 within the interval of 250-450ms[14,28,29], and the LPC spanning from 550-1000ms[30,31].’</p>
<p>Reference: Habets, B., Kita, S., Shao, Z.S., Ozyurek, A., and Hagoort, P. (2011). The Role of Synchrony and Ambiguity in Speech-Gesture Integration during Comprehension. J Cognitive Neurosci 23, 1845-1854. 10.1162/jocn.2010.21462.</p>
<disp-quote content-type="editor-comment">
<p>(12) In the Results section for Experiment 2 (L292-295), it is not clear what the authors mean when they mention that a more negative TMS effect represents a stronger interruption of the integration effect. If I understand correctly, the correlation reported for pMTG was for speech entropy, which does not represent integration (that would be MI).</p>
</disp-quote>
<p>Response 12: Since the TMS effect was defined as active TMS minus Vertex TMS, the inhibitory TMS effect is inherently negative. A greater inhibitory TMS effect corresponds to a larger negative value, such that a more negative TMS effect indicates a stronger disruption of the integration process. We acknowledge that the previous phrasing was somewhat ambiguous. In the revised manuscript, we have rephrased the sentence as follows: ‘a larger negative TMS effect signifies a greater disruption of the integration process’ (Lines 342-343)</p>
<p>Multisensory integration transcends simple data amalgamation, encompassing complex interactions at various hierarchical neural levels and the parallel detection and discrimination of raw data from each modality (Benetti et al., 2023; Meijer et al., 2019). Therefore, we regard the process of gesture-speech integration as involving both unisensory processing and multisensory convergence. The correlation of gesture and speech entropy reflects contributions from unisensory processing, while the mutual information (MI) index indicates the contribution of multisensory convergence during gesture-speech integration. The distinction between these various source contributions will be the focus of Experiment 2 and Experiment 3, as described in the revised manuscript Lines 87-102: ‘Given the differential involvement of the IFG and pMTG in gesture-speech integration, shaped by top-down gesture predictions and bottom-up speech processing [23], Experiment 2 was designed to further assess whether the activity of these regions was associated with relevant informational matrices. Specifically, we applied inhibitory chronometric double-pulse transcranial magnetic stimulation (TMS) to specific temporal windows associated with integration processes in these regions[23], assessing whether the inhibitory effects of TMS were correlated with unisensory entropy or the multisensory convergence index (MI).</p>
<p>Experiment 3 complemented these investigations by focusing on the temporal dynamics of neural responses during semantic processing, leveraging high-temporal event-related potentials (ERPs). This experiment investigated how distinct information contributors modulated specific ERP components associated with semantic processing. These components included the early sensory effects as P1 and N1–P2[27,28], the N400 semantic conflict effect[14,28,29], and the late positive component (LPC) reconstruction effect[30,31]. By integrating these ERP findings with results from Experiments 1 and 2, Experiment 3 aimed to provide a more comprehensive understanding of how gesture-speech integration is modulated by neural dynamics’.</p>
<p>References:</p>
<p>Benetti, S., Ferrari, A., and Pavani, F. (2023). Multimodal processing in face-to-face interactions: A bridging link between psycholinguistics and sensory neuroscience. Front Hum Neurosci <italic>17</italic>, 1108354. 10.3389/fnhum.2023.1108354.</p>
<p>Meijer, G.T., Mertens, P.E.C., Pennartz, C.M.A., Olcese, U., and Lansink, C.S. (2019). The circuit architecture of cortical multisensory processing: Distinct functions jointly operating within a common anatomical network. Prog Neurobiol <italic>174</italic>, 1-15. 10.1016/j.pneurobio.2019.01.004.</p>
<disp-quote content-type="editor-comment">
<p>(13) I find the description of the results for Experiment 3 very hard to follow. Perhaps if the authors have decided to organise the main text by describing the components from earliest to latest, the Figure organisation should follow suit (i.e., organise the Figure from the earliest to the latest component, instead of gesture entropy/speech entropy / mutual information). This might make the description of the results easier to follow.</p>
</disp-quote>
<p>Response 13: As suggested, we have reorganized the results of experiment 3 based on components from earliest to latest, together with an updated Figure 5.</p>
<p>The results are detailed in Lines 367-423: ‘Topographical maps illustrating amplitude differences between the lower and higher halves of speech entropy demonstrate a central-posterior P1 amplitude (0-100 ms, Figure 5B). Aligning with prior findings[27], the paired t-tests demonstrated a significantly larger P1 amplitude within the ML ROI (<italic>t</italic>(22) = 2.510, <italic>p</italic> = 0.020, 95% confidence interval (CI) = [1.66, 3.36]) when contrasting stimuli with higher 50% speech entropy against those with lower 50% speech entropy (Figure 5D1 left). Subsequent correlation analyses unveiled a significant increase in the P1 amplitude with the rise in speech entropy within the ML ROI (<italic>r</italic> = 0.609, <italic>p</italic> = 0.047, 95% CI = [0.039, 1.179], Figure 5D1 right). Furthermore, a cluster of neighboring time-electrode samples exhibited a significant contrast between the lower 50% and higher 50% of speech entropy, revealing a P1 effect spanning 16 to 78 ms at specific electrodes (FC2, FCz, C1, C2, Cz, and CPz, Figure 5D2 middle) (<italic>t</italic>(22) = 2.754, <italic>p</italic> = 0.004, 95% confidence interval (CI) = [1.65, 3.86], Figure 5D2 left), with a significant correlation with speech entropy (<italic>r</italic> = 0.636, <italic>p</italic> = 0.035, 95% CI = [0.081, 1.191], Figure 5D2 right).</p>
<p>Additionally, topographical maps comparing the lower 50% and higher 50% gesture entropy revealed a frontal N1-P2 amplitude (150-250 ms, Figure 5A). In accordance with previous findings on bilateral frontal N1-P2 amplitude[27], paired t-tests displayed a significantly larger amplitude for stimuli with lower 50% gesture entropy than with higher 50% entropy in both ROIs of LA (<italic>t</italic>(22) = 2.820, <italic>p</italic> = 0.011, 95% CI = [2.21, 3.43]) and RA (<italic>t</italic>(22) = 2.223, <italic>p</italic> = 0.038, 95% CI = [1.56, 2.89]) (Figure 5E1 left).  Moreover, a negative correlation was found between N1-P2 amplitude and gesture entropy in both ROIs of LA (<italic>r</italic> = -0.465, <italic>p</italic> = 0.039, 95% CI = [-0.87, -0.06]) and RA (<italic>r</italic> = -0.465, <italic>p</italic> = 0.039, 95% CI = [-0.88, -0.05]) (Figure 5E1 right). Additionally, through a cluster-permutation test, the N1-P2 effect was identified between 184 to 202 ms at electrodes FC4, FC6, C2, C4, C6, and CP4 (Figure 5E2 middle) (<italic>t</italic>(22) = 2.638, <italic>p</italic> = 0.015, 95% CI = [1.79, 3.48], (Figure 5E2 left)), exhibiting a significant correlation with gesture entropy (<italic>r</italic> = -0.485, <italic>p</italic> = 0.030, 95% CI = [-0.91, -0.06], Figure 5E2 right).</p>
<p>Furthermore, in line with prior research[42], a left-frontal N400 amplitude (250-450 ms) was discerned from topographical maps of gesture entropy (Figure 5A). Specifically, stimuli with lower 50% values of gesture entropy elicited a larger N400 amplitude in the LA ROI compared to those with higher 50% values  (<italic>t</italic>(22) = 2.455, <italic>p</italic> = 0.023, 95% CI = [1.95, 2.96], Figure 5F1 left). Concurrently, a negative correlation was noted between the N400 amplitude and gesture entropy (<italic>r</italic> = -0.480, <italic>p</italic> = 0.032, 95% CI = [-0.94, -0.03], Figure 5F1 right) within the LA ROI. The identified clusters showing the N400 effect for gesture entropy (282 – 318 ms at electrodes FC1, FCz, C1, and Cz, Figure 5F2 middle) (<italic>t</italic>(22) = 2.828, <italic>p</italic> = 0.010, 95% CI = [2.02, 3.64], Figure 5F2 left) also exhibited significant correlation between the N400 amplitude and gesture entropy (<italic>r</italic> = -0.445, <italic>p</italic> = 0.049, 95% CI = [-0.88, -0.01], Figure 5F2 right).</p>
<p>Similarly, a left-frontal N400 amplitude (250-450 ms) [42] was discerned from topographical maps for MI (Figure 5C). A larger N400 amplitude in the LA ROI was observed for stimuli with lower 50% values of MI compared to those with higher 50% values (<italic>t</italic>(22) = 3.00, <italic>p</italic> = 0.007, 95% CI = [2.54, 3.46], Figure 5G1 left). This was accompanied by a significant negative correlation between N400 amplitude and MI (<italic>r</italic> = -0.504, <italic>p</italic> = 0.028, 95% CI = [-0.97, -0.04], Figure 5G1 right) within the LA ROI. The N400 effect for MI, observed in the 294–306 ms window at electrodes F1, F3, Fz, FC1, FC3, FCz, and C1 (Figure 5G2 middle) (<italic>t</italic>(22) = 2.461, <italic>p</italic> = 0.023, 95% CI = [1.62, 3.30], Figure 5G2 left), also showed a significant negative correlation with MI (<italic>r</italic> = -0.569, <italic>p</italic> = 0.011, 95% CI = [-0.98, -0.16], Figure 5G2 right).</p>
<p>Finally, consistent with previous findings[30], an anterior LPC effect (550-1000 ms) was observed in topographical maps comparing stimuli with lower and higher 50% speech entropy (Figure 5B). The reduced LPC amplitude was evident in the paired t-tests conducted in ROIs of LA (<italic>t</italic>(22) = 2.614, <italic>p</italic> = 0.016, 95% CI = [1.88, 3.35]); LC (<italic>t</italic>(22) = 2.592, <italic>p</italic> = 0.017, 95% CI = [1.83, 3.35]); RA (<italic>t</italic>(22) = 2.520, <italic>p</italic> = 0.020, 95% CI = [1.84, 3.24]); and ML (<italic>t</italic>(22) = 2.267, <italic>p</italic> = 0.034, 95% CI = [1.44, 3.10]) (Figure 5H1 left). Simultaneously, a marked negative correlation with speech entropy was evidenced in ROIs of LA (<italic>r</italic> = -0.836, <italic>p</italic> =   0.001, 95% CI = [-1.26, -0.42]); LC (<italic>r</italic> = -0.762, <italic>p</italic> = 0.006, 95% CI = [-1.23, -0.30]); RA (<italic>r</italic> = -0.774, <italic>p</italic> = 0.005, 95% CI = [-1.23, -0.32]) and ML (<italic>r</italic> = -0.730, <italic>p</italic> = 0.011, 95% CI = [-1.22, -0.24]) (Figure 5H1 right). Additionally, a cluster with the LPC effect (644 - 688 ms at electrodes Cz, CPz, P1, and Pz, Figure 5H2 middle) (<italic>t</italic>(22) = 2.754, <italic>p</italic> = 0.012, 95% CI = [1.50, 4.01], Figure 5H2 left) displayed a significant correlation with speech entropy (<italic>r</italic> = -0.699, <italic>p</italic> = 0.017, 95% CI = [-1.24, -0.16], Figure 5H2 right).’</p>
<disp-quote content-type="editor-comment">
<p>(14) In the Discussion (L394 - 395) the authors mention for the first time their task being a semantic priming paradigm. This idea of the task as a semantic priming paradigm allowing top-down prediction of gesture over speech should be presented earlier in the paper, perhaps during the final paragraph of the introduction (as part of the rationale) or during the explanation of the task. The authors mention top-down influences earlier and this is impossible to understand before this information about the paradigm is presented. It would also make the reading of the paper significantly clearer. Critically, an appropriate description of the paradigm is missing in the Methods (what are the subjects asked to do? It states that it replicates an effect in Ref 28, but this manuscript does not contain a clear description of the task). To further complicate things, the 'Experimental Procedure' section of the methods states this is a semantic priming paradigm of gestures onto speech (L148) and proceeds to provide two seemingly irrelevant references (for example, the Pitcher reference is to a study that employed faces and houses as stimuli). How is this a semantic priming paradigm? The study where I found the first mention of this paradigm seems to clearly classify it as a Stroop-like task (Kelly et al, 2010).</p>
</disp-quote>
<p>We appreciate the reviewer’s thorough consideration. The experimental paradigm employed in the current study differs from the Stroop-like task utilized by Kelly et al. (2010). In their study, the video presentation started with the stroke phase of the gesture, while speech occurred 200 ms after the gesture onset.</p>
<p>As detailed in our previous study (Zhao et al., 2023, Frontiers in Psychology), we confirmed the semantic predictive role of gestures in relation to speech by contrasting two experimental conditions: (1) gestures preceding speech by a fixed 200 ms interval, and (2) gestures preceding speech at the semantic identification point of the gesture. Our findings revealed time-window-selective disruptions in the semantic congruency effect in the IFG and pMTG, but only in the second condition, suggesting that gestures exert a semantic priming effect on concurrent speech.</p>
<p>This work highlighted the semantic priming role of gestures in the integration of speech found in Zhao et al. (2021, Journal of Neuroscience). In the study, a comparable approach was adopted by segmenting speech into eight 40-ms time windows based on the speech discrimination point, while manipulating the speech onset to align with the gesture identification point. The results revealed time-window-selective disruptions in the semantic congruency effect, providing support for the dynamic and temporally staged roles of the IFG and pMTG in gesture-speech integration.</p>
<p>Given that the present study follows the same experimental procedure as our prior work (Zhao et al., 2021, Journal of Neuroscience; Zhao et al., 2023, Frontiers in Psychology), we refer to this design as a &quot;semantic priming&quot; of gesture upon speech. We agree with the reviewer that a detailed description should be clarified earlier in the manuscript. To address this, we have added a more explicit description of the semantic priming paradigm in the methods section of the revised manuscript in Lines 162-166: ‘Given that gestures induce a semantic priming effect on concurrent speech[33], this study utilized a semantic priming paradigm in which speech onset was aligned with the DP of each gesture[23,33], the point at which the gesture transitions into a lexical form[34]. The gesture itself began at the stroke phase, a critical moment when the gesture conveys its primary semantic content [34].’</p>
<p>The task participants completed was outlined immediately following the explanation of the experimental paradigm: ‘Gesture–speech pairs were presented randomly using Presentation software (<ext-link ext-link-type="uri" xlink:href="http://www.neurobs.com">www.neurobs.com</ext-link>). Participants were asked to look at the screen but respond with both hands as quickly and accurately as possible merely to the gender of the voice they heard’ (Lines:177-180).</p>
<p>Wrongly cited references have been corrected.</p>
<disp-quote content-type="editor-comment">
<p>(15) L413-417: How do the authors explain that they observe this earlier ERP component and TMS effect over speech and a later one over gesture in pMTG when in their task they first presented gesture and then speech? Why mention STG/S when they didn't assess this?</p>
<p>(19) L436-440: This paragraph yields the timing of the findings represented in Figure 6 even more confusing. If gesture precedes speech in the paradigm, why are the first TMS and ERP results observed in speech?</p>
</disp-quote>
<p>Response 15 &amp;19: Since these two aspects are closely related, we offer a comprehensive explanation. Although gestures were presented before speech, the integration process occurs once both modalities are available. Consequently, ERP and TMS measurements were taken after speech onset to capture the integration of the two modalities. Neural responses were used as the dependent variable to reflect the degree of integration—specifically, gesture-speech semantic congruency in the TMS study and high-low semantic variance in the ERP study. Therefore, the observed early effect can be interpreted as an interaction between the top-down influence of gesture and the bottom-up processing of speech.</p>
<p>To isolate the pure effect of gesture, neural activity would need to be recorded from gesture onset. However, if one aims to associate the strength of neural activity with the degree of gesture information, recording from the visual processing areas would be more appropriate.</p>
<p>To avoid unnecessary ambiguity, the phrase &quot;involved STG/S&quot; has been removed from the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(16) L427-428: I find it hard to believe that MI, a behavioural metric, indexes the size of overlapped neural populations activated by gesture and speech. The authors should be careful with this claim or provide evidence in favour.</p>
</disp-quote>
<p>Response 16: Mutual information (MI) is a behavioral metric that indexes the distribution of overlapping responses between gesture and speech (for further details, please see the Response to Comment 1). In the present study, MI was correlated with neural responses evoked by gesture and speech, with the goal of demonstrating that neural activity progressively reflects the degree of information conveyed, as indexed by MI.</p>
<disp-quote content-type="editor-comment">
<p>(17) Why would you have easier integration (reduced N400) with larger gesture entropy in IFG (Figure 6(3))? Wouldn't you expect more difficult processing if entropy is larger?</p>
<p>(18) L431-432: The claim that IFG stores semantic information is controversial. The authors provide two references from the early 2000s that do not offer support for this claim (the IFG's purported involvement according to these is in semantic unification, not storage).</p>
</disp-quote>
<p>Response 17 &amp;18: As outlined in the Responses to Comment 1 of the public review, we have provided a re-explanation of the IFG as a semantic control region. Additionally, we have clarified the role of the IFG in relation to the various stages of gesture-speech integration in Lines 533-538: ‘Last, the activated speech representation would disambiguate and reanalyze the semantic information and further unify into a coherent comprehension in the pMTG[12,37]. As speech entropy increases, indicating greater uncertainty in the information provided by speech, more cognitive effort is directed towards selecting the targeted semantic representation. This leads to enhanced involvement of the IFG and a corresponding reduction in LPC amplitude’</p>
<disp-quote content-type="editor-comment">
<p>(20) Overall, the grammar makes some parts of the discussion hard to follow (e.g. the limitation in L446-447: 'While HD tDCS and TMS may impact functionally and anatomically connected brain regions, the graded functionality of every disturbed period is not guaranteed')</p>
</disp-quote>
<p>Response 20: Clear description has been provided in the revised manuscript in Lines 552-557: ‘Additionally, not all influenced TWs exhibited significant associations with entropy and MI. While HD-tDCS and TMS may impact functionally and anatomically connected brain regions[55,56],  whether the absence of influence in certain TWs can be attributed to compensation by other connected brain areas, such as angular gyrus[57] or anterior temporal lobe[58], warrants further investigation. Therefore, caution is needed when interpreting the causal relationship between inhibition effects of brain stimulation and information-theoretic metrics (entropy and MI).’</p>
<p>References:</p>
<p>Hartwigsen, G., Bzdok, D., Klein, M., Wawrzyniak, M., Stockert, A., Wrede, K., Classen, J., and Saur, D. (2017). Rapid short-term reorganization in the language network. Elife <italic>6</italic>. 10.7554/eLife.25964.</p>
<p>Jackson, R.L., Hoffman, P., Pobric, G., and Ralph, M.A.L. (2016). The semantic network at work and rest: Differential connectivity of anterior temporal lobe subregions. Journal of Neuroscience <italic>36</italic>, 1490-1501. 10.1523/JNEUROSCI.2999-15.2016</p>
<p>Humphreys, G. F., Lambon Ralph, M. A., &amp; Simons, J. S. (2021). A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition. Trends in neurosciences, 44(6), 452–463. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2021.01.006">https://doi.org/10.1016/j.tins.2021.01.006</ext-link></p>
<p>Bonner, M. F., &amp; Price, A. R. (2013). Where is the anterior temporal lobe and what does it do?. The Journal of neuroscience : the official journal of the Society for Neuroscience, 33(10), 4213–4215. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0041-13.2013">https://doi.org/10.1523/JNEUROSCI.0041-13.2013</ext-link></p>
<disp-quote content-type="editor-comment">
<p>(21) Inconsistencies between terminology employed in Figures and main text (e.g., pre-test study in text, gating study in Figure?)</p>
</disp-quote>
<p>Response 21: Consistence has been made by changing the ‘gating study’ into ‘pre-tests’ in Figure 1 (Lines 758).</p>
</body>
</sub-article>
</article>