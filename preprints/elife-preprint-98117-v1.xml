<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">98117</article-id>
<article-id pub-id-type="doi">10.7554/eLife.98117</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98117.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.5</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Human EEG and artificial neural networks reveal disentangled representations of object real-world size in natural images</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7953-6742</contrib-id>
<name>
<surname>Lu</surname>
<given-names>Zitong</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3489-0702</contrib-id>
<name>
<surname>Golomb</surname>
<given-names>Julie D</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, The Ohio State University</institution>, Columbus, OH 43212 <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Please address correspondence to: Zitong Lu Department of Psychology The Ohio State University Columbus, OH, 43210 Email: <email>lu.2637@osu.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-11">
<day>11</day>
<month>07</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP98117</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-13">
<day>13</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-21">
<day>21</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.19.553999"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Lu &amp; Golomb</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Lu &amp; Golomb</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-98117-v1.pdf"/>
<abstract>
<p>Remarkably, human brains have the ability to accurately perceive and process the real-world size of objects, despite vast differences in distance and perspective. While previous studies have delved into this phenomenon, distinguishing this ability from other visual perceptions, like depth, has been challenging. Using the THINGS EEG2 dataset with high time-resolution human brain recordings and more ecologically valid naturalistic stimuli, our study uses an innovative approach to disentangle neural representations of object real-world size from retinal size and perceived real-world depth in a way that was not previously possible. Leveraging this state-of-the-art dataset, our EEG representational similarity results reveal a pure representation of object real-world size in human brains. We report a representational timeline of visual object processing: object real-world depth appeared first, then retinal size, and finally, real-world size. Additionally, we input both these naturalistic images and object-only images without natural background into artificial neural networks. Consistent with the human EEG findings, we also successfully disentangled representation of object real-world size from retinal size and real-world depth in all three types of artificial neural networks (visual-only ResNet, visual-language CLIP, and language-only Word2Vec). Moreover, our multi-modal representational comparison framework across human EEG and artificial neural networks reveals real-world size as a stable and higher-level dimension in object space incorporating both visual and semantic information. Our research provides a detailed and clear characterization of the object processing process, which offers further advances and insights into our understanding of object space and the construction of more brain-like visual models.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>real-world size</kwd>
<kwd>depth perception</kwd>
<kwd>RSA</kwd>
<kwd>artificial neural networks</kwd>
<kwd>object recognition</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>revised the whole manuscript from abstract to discussion</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Imagine you are viewing an apple tree while walking around an orchard: as you change your perspective and distance, the retinal size of the apple you plan to pick varies, but you still perceive the apple as having a constant real-world size. How do our brains extract object real-world size information during object recognition to allow us to understand the complex world? Behavioral studies have demonstrated that perceived real-world size is represented as an object physical property, revealing same-size priming effects (<xref ref-type="bibr" rid="c48">Setti et al., 2008</xref>), familiar-size stroop effects (<xref ref-type="bibr" rid="c31">Konkle &amp; Oliva, 2012a</xref>; <xref ref-type="bibr" rid="c35">Long &amp; Konkle, 2017</xref>), and canonical visual size effects (<xref ref-type="bibr" rid="c6">Chen et al., 2022</xref>; <xref ref-type="bibr" rid="c30">Konkle &amp; Oliva, 2011</xref>). Human neuroimaging studies have also found evidence of object real-world size representation (<xref ref-type="bibr" rid="c22">Huang et al., 2022</xref>; S.-M. <xref ref-type="bibr" rid="c26">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="c29">Konkle &amp; Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c32">Konkle &amp; Oliva, 2012b</xref>; <xref ref-type="bibr" rid="c40">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="c45">Quek et al., 2023</xref>; R. <xref ref-type="bibr" rid="c51">Wang et al., 2022</xref>). These findings suggest real-world size is a fundamental dimension of object representation.</p>
<p>However, previous studies on object real-world size have faced several challenges. Firstly, the perception of an object’s real-world size is closely related to the perception of its real-world distance in depth. For instance, imagine you are looking at photos of an apple and a basketball: if the two photos were zoomed in such that the apple and the basketball filled the same exact retinal (image) size, you could still easily perceive that the apple is the physically smaller real-world object. But, you would simultaneously infer that the apple is thus located closer to you (or the camera) than the basketball. In previous neuroimaging studies of perceived real-world size (<xref ref-type="bibr" rid="c22">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="c29">Konkle &amp; Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c32">Konkle &amp; Oliva, 2012b</xref>), researchers presented images of familiar objects zoomed and cropped such that they occupied the same retinal size, finding that neural responses in ventral temporal cortex reflected the perceived real-world size (e.g. an apple smaller than a car). However, while they controlled the retinal size of objects, the intrinsic correlation between real-world size and real-world depth in these images meant that the influence of perceived real-world depth could not be entirely isolated when examining the effects of real-world size. This makes it difficult to ascertain whether their results were driven by neural representations of perceived real-world size and/or perceived real-world depth. MEG and EEG studies focused on temporal processing of object size representations (S.-M. <xref ref-type="bibr" rid="c26">Khaligh-Razavi et al., 2018</xref>; R. <xref ref-type="bibr" rid="c52">Wang et al., 2022</xref>) have been similarly susceptible to this limitation. Indeed, one recent behavioral study (<xref ref-type="bibr" rid="c45">Quek et al., 2023</xref>) provided evidence that perceived real-world depth could influence real-world size representations, further illustrating the necessity of investigating pure real-world size representations in the brain. Secondly, the stimuli used in these studies were cropped object stimuli against a plain white or grey background, which are not particularly naturalistic. More and more studies and datasets have highlighted the important role of naturalistic context in object recognition (<xref ref-type="bibr" rid="c1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="c16">Gifford et al., 2022</xref>; <xref ref-type="bibr" rid="c18">Grootswagers et al., 2022</xref>; <xref ref-type="bibr" rid="c21">Hebart et al., 2019</xref>; <xref ref-type="bibr" rid="c49">Stoinski et al., 2023</xref>). In ecological contexts, inferring the real-world size/distance of an object likely relies on a combination of bottom-up visual information and top-down knowledge about canonical object sizes for familiar objects. Incorporating naturalistic background context in experimental stimuli may produce more accurate assessments of the relative influences of visual shape representations (<xref ref-type="bibr" rid="c4">Bracci et al., 2017</xref>; <xref ref-type="bibr" rid="c5">Bracci &amp; Op de Beeck, 2016</xref>; <xref ref-type="bibr" rid="c44">Proklova et al., 2016</xref>) and higher-level semantic information (<xref ref-type="bibr" rid="c13">Doerig et al., 2022</xref>; <xref ref-type="bibr" rid="c23">Huth et al., 2012</xref>; A. Y. <xref ref-type="bibr" rid="c51">Wang et al., 2022</xref>). Furthermore, most previous studies have tended to categorize size rather broadly, such as merely differentiating between big and small objects (S.-M. <xref ref-type="bibr" rid="c26">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="c32">Konkle &amp; Oliva, 2012b</xref>; R. <xref ref-type="bibr" rid="c52">Wang et al., 2022</xref>) or dividing object size into seven levels from small to big. To more finely investigate the representation of object size in the brain, it may be necessary to obtain a more continuous measure of size for a more detailed characterization.</p>
<p>Certainly, a minority of fMRI studies have attempted to utilize natural images and also engaged in more detailed size measurements to more precisely explore the encoding of object real-world size in different brain areas (<xref ref-type="bibr" rid="c40">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="c50">Troiani et al., 2014</xref>). However, no study has yet comprehensively overcome all the challenges and unfolded a clear processing timeline for object retinal size, real-world size, and real-world depth in human visual perception.</p>
<p>In the current study, we overcome all these challenges by combining high temporal-resolution EEG, naturalistic images, artificial neural networks, and novel computational methods to distinguish the neural representations of object real-world size, retinal size, and real-world depth. We applied our novel computational approach to an open EEG dataset, THINGS EEG2 (<xref ref-type="bibr" rid="c16">Gifford et al., 2022</xref>). Firstly, the visual image stimuli used in this dataset are more naturalistic and include objects that vary in real-world size, depth, and retinal size. This allows us to employ a multi-model representational similarity analysis to investigate pure representations of object real-world size, partialing out – and simultaneously exploring – these confounding features.</p>
<p>Secondly, we are able to explore the neural dynamics of object feature processing in a more ecological context based on natural images in human object recognition. Thirdly, instead of dividing object size into several levels, we applied more detailed behavioral measurements from an online size rating task to obtain a more continuous measure to more finely decode the representation of object size in the brain.</p>
<p>We first focus on unfolding the neural dynamics of pure object real-world size representations. The temporal resolution of EEG allows us the opportunity to investigate the representational timecourse of visual object processing, asking whether processing of perceived object real-world size precedes or follows processing of perceived depth, if these two properties are in fact processed independently.</p>
<p>We then attempt to further explore the underlying mechanisms of how human brains process object size and depth in natural images by integrating artificial neural networks (ANNs). In the domain of cognitive computational neuroscience, ANNs offer a complementary tool to study visual object recognition, and an increasing number of studies support that ANNs exhibit representations similar to human visual systems (<xref ref-type="bibr" rid="c10">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="c56">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="c55">Yamins &amp; DiCarlo, 2016</xref>). Indeed, a recent study found that ANNs also represent real-world size (<xref ref-type="bibr" rid="c22">Huang et al., 2022</xref>); however, their use of a fixed retinal size image dataset with the same cropped objects as described above makes it similarly challenging to ascertain whether the results reflected real-world size and/or depth. Additionally, some recent work indicates that artificial neural networks incorporating semantic embedding and multimodal neural components might more accurately reflect human visual representations within visual areas and even the hippocampus, compared to vision-only networks (<xref ref-type="bibr" rid="c7">Choksi, Mozafari, et al., 2022</xref>; <xref ref-type="bibr" rid="c8">Choksi, Vanrullen, et al., 2022</xref>; <xref ref-type="bibr" rid="c12">Conwell et al., 2022</xref>; <xref ref-type="bibr" rid="c13">Doerig et al., 2022</xref>; <xref ref-type="bibr" rid="c24">Jozwik et al., 2023</xref>; A. Y. <xref ref-type="bibr" rid="c51">Wang et al., 2022</xref>). Given that perception of real-world size may incorporate both bottom-up visual and top-down semantic knowledge about familiar objects, these models offer yet another novel opportunity to investigate this question. Utilizing both visual and visual-semantic models, as well as different layers within these models, ANNs provide us the approach to extract various image features, low-level visual information from early layers and higher-level information including both visual and semantic features from late layers.</p>
<p>The novel computational approach by cross-modal representational comparisons we take with the current study allows us to compare how representations of perceived real-world size and depth emerge in both human brains and artificial neural networks. Unraveling the internal representations of object size and depth features in both human brains and ANNs provides us a deeper approach to not only explore whether both biological and artificial systems represent object real-world size, along with retinal size and real-world depth features, but also investigate possible mechanisms of object real-world size representations.</p>
</sec>
<sec id="s2">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Experimental design, stimuli images and EEG data</title>
<p>We utilized the open dataset from THINGS EEG2 (<xref ref-type="bibr" rid="c16">Gifford et al., 2022</xref>), which includes EEG data from 10 healthy human subjects (age=28.5±4, 8 female and 2 male) in a rapid serial visual presentation (RSVP) paradigm with an orthogonal target detection task to ensure participants paid attention to the visual stimuli. For each trial, subjects viewed one image (sized 500 × 500 pixels) for 100ms. Each subject viewed 16740 images of objects on a natural background for 1854 object concepts from THINGS dataset (<xref ref-type="bibr" rid="c21">Hebart et al., 2019</xref>). For the current study, we used the ‘test’ dataset portion, which includes 16000 trials per subject corresponding to 200 images (200 object concepts, one image per concept) with 80 trials per image. Before inputting the images to the ANNs, we reshaped image sizes to 224 x 224 pixels and normalized the pixel values of images to ImageNet statistics.</p>
<p>EEG data were collected using a 64-channel EASYCAP and a BrainVision actiCHamp amplifier. We used already pre-processed data from 17 channels (O1, Oz, O2, PO7, PO3, POz, PO4, PO8, P7, P5, P3, P1, Pz, P2) overlying occipital and parietal cortex. We re-epoched EEG data ranging from 100ms before stimulus onset to 300ms after onset with a sample frequency of 100Hz. Thus the shape of our EEG data matrix for each trial was 17 channels × 40 time points.</p>
</sec>
<sec id="s2b">
<title>ANN models</title>
<p>We applied two pre-trained ANN models: one visual model (ResNet-101 (<xref ref-type="bibr" rid="c20">He et al., 2016</xref>) pretrained on ImageNet), and one multi-modal (visual+semantic) model (CLIP with a ResNet-101 backbone (<xref ref-type="bibr" rid="c46">Radford et al., 2021</xref>) pretrained on YFCC-15M). We used THINGSvision (<xref ref-type="bibr" rid="c42">Muttenthaler &amp; Hebart, 2021</xref>) to obtain low-and high-level feature vectors of ANN activations from early and late layers (early layer: second convolutional layer; late layer: last visual layer) for the images.</p>
</sec>
<sec id="s2c">
<title>Word2Vec model</title>
<p>To approximate the non-visual, pure semantic space of objects, we also applied a Word2Vec model, a natural language processing model for word embedding, pretrained on Google News corpus (<xref ref-type="bibr" rid="c41">Mikolov et al., 2013</xref>), which contains 300-dimensional vectors for 3 million words and phrases. We input the words for each image’s object concept (pre-labeled in THINGS dataset: <xref ref-type="bibr" rid="c21">Hebart et al., 2019</xref>), instead of the visual images themselves. We used Gensim (<xref ref-type="bibr" rid="c47">Řehůřek &amp; Sojka, 2010</xref>) to obtain Word2Vec feature vectors for the objects in images.</p>
</sec>
<sec id="s2d">
<title>Representational dissimilarity matrices (RDMs)</title>
<p>To conduct RSA across human EEG, artificial models, and our hypotheses corresponding to different visual features, we first computed representational dissimilarity matrices (RDMs) for different modalities (<xref rid="fig2" ref-type="fig">Figure 2</xref>). The shape of each RDM was 200 × 200, corresponding to pairwise dissimilarity between the 200 images. We extracted the 19900 cells from the upper half of the diagonal of each RDM for subsequent analyses.</p>
<sec id="s2d1">
<title>Neural RDMs</title>
<p>From the EEG signal, we constructed timepoint-by-timepoint neural RDMs for each subject with decoding accuracy as the dissimilarity index (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). We first conducted timepoint-by-timepoint classification-based decoding for each subject and each pair of images (200 images, 19900 pairs in total). We applied linear Support Vector Machine (SVM) to train and test a two-class classifier, employing a 5-time 5-fold cross-validation method, to obtain an independent decoding accuracy for each image pair and each timepoint. Therefore, we ultimately acquired 40 (1 per timepoint) EEG RDMs for each subject.</p>
</sec>
<sec id="s2d2">
<title>Hypothesis-based (HYP) RDMs</title>
<p>We constructed three hypothesis-based RDMs reflecting the different types of visual object properties in the naturalistic images (<xref rid="fig2" ref-type="fig">Figure 2B</xref>): Real-World Size RDM, Retinal Size RDM, and Real-World Depth RDM. We constructed these RDMs as follows:
<list list-type="simple">
<list-item><p>(1) For Real-World Size RDM, we obtained human behavioral real-world size ratings of each object concept from the THINGS+ dataset (Stoinski et al., 2022). In the THINGS+ dataset, 2010 participants (different from the subjects in THINGS EEG2) did an online size rating task and completed a total of 13024 trials corresponding to 1854 object concepts. The range of possible size ratings was from 0 to 519 in their online size rating task, with the actual mean ratings across subjects ranging from 100.03 (‘sand’) to 423.09 (‘subway’). We used these ratings as the perceived real-world size measure of the object concept pre-labeled in THINGS dataset (<xref ref-type="bibr" rid="c21">Hebart et al., 2019</xref>) for each image. We then constructed the representational dissimilarity matrix by calculating the absolute difference between perceived real-world size ratings for each pair of images.</p></list-item>
<list-item><p>(2) For Retinal Size RDM, we applied Adobe Photoshop (Adobe Inc., 2019) to crop objects corresponding to object labels from images manually, obtaining a rectangular region that precisely contains a single object, then measured the diagonal length of the segmented object in pixels as the retinal size measure (<xref ref-type="bibr" rid="c30">Konkle &amp; Oliva, 2011</xref>). Due to our calculations being at the object level, if there were more than one same objects in an image, we cropped the most complete one to get more accurate retinal size. We then constructed the RDM by calculating the absolute difference between measured retinal size for each pair of images.</p></list-item>
<list-item><p>(3) For Real-World Depth RDM, we calculated the perceived depth based on the measured retinal size index and behavioral real-world size ratings, such that real-world depth / visual image depth = real-world size / retinal size. Since visual image depth (viewing distance) is held constant across images in the task, real-world depth is proportional to real-world size / retinal size. We then constructed the RDM by calculating the absolute difference between real-world depth index for each pair of images.</p></list-item>
</list>
</p>
</sec>
<sec id="s2d3">
<title>ANN (and Word2Vec) model RDMs</title>
<p>We constructed a total of five model-based RDMs (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Our primary analyses used four ANN RDMs, corresponding to the early and late layers for both ResNet and CLIP (<xref rid="figs1" ref-type="fig">Figure S1</xref>). We also calculated a single Word2Vec RDM for the pure semantic analysis (<xref rid="figs2" ref-type="fig">Figure S2</xref>). For each RDM, we got the dissimilarities by calculating 1 – Pearson correlation coefficient between each pair of two vectors of the model features corresponding to two input images.</p>
</sec>
</sec>
<sec id="s2e">
<title>Representational similarity analyses (RSA) and statistical analyses</title>
<p>We conducted cross-modal representational similarity analyses between the three types of RDMs (<xref rid="fig2" ref-type="fig">Figure 2</xref>). All decoding and RSA analyses were implemented using NeuroRA (<xref ref-type="bibr" rid="c38">Lu &amp; Ku, 2020</xref>).</p>
<sec id="s2e1">
<title>EEG × ANN (or W2V) RSA</title>
<p>To measure the representational similarity between human brains and ANNs and confirm that ANNs have significantly similar representations to human brains, we calculated the Spearman correlation between the 40 timepoint-by-timepoint EEG neural RDMs and the 4 ANN RDMs corresponding to the representations of ResNet early layer, ResNet late layer, CLIP early layer, CLIP late layer, respectively. We also calculated temporal representational similarity between human brains (EEG RDMs) and the Word2Vec model RDM. Cluster-based permutation tests were conducted to determine the time windows of significant representational similarity. First, we performed one-sample t-tests (one-tailed testing) against zero to get the t-value for each timepoint, and extracted significant clusters. We computed the clustering statistic as the sum of t-values in each cluster. Then we conducted 1000 permutations of each subject’s timepoint-by-timepoint similarities to calculate a null distribution of maximum clustering statistics. Finally, we assigned cluster-level p-values to each cluster of the actual representational timecourse by comparing its cluster statistic with the null distribution. Time-windows were determined to be significant if the p-value of the corresponding cluster was &lt;0.05.</p>
</sec>
<sec id="s2e2">
<title>EEG × HYP RSA<italic/></title>
<p>To evaluate how human brains temporally represent different visual features, we calculated the timecourse of representational similarity between the timepoint-by-timepoint EEG neural RDMs and the three hypothesis-based RDMs. To avoid correlations between hypothesis-based RDMs (<xref rid="fig3" ref-type="fig">Figure 3A</xref>) influencing comparison results, we calculated partial correlations with one-tailed test against the alternative hypothesis that the partial correlation was positive (greater than zero). Cluster-based permutation tests were performed as described above to determine the time windows of significant representational similarity. In addition, we conducted peak latency analysis to determine the latency of peak representational similarity for each type of visual information with the EEG signal. We restricted the time-window to the significant (partial) correlation time-window for real-world size, retinal size, and real-world depth, and got the individual peak timepoint corresponding to the highest partial correlation. Paired t-tests (two-tailed) were conducted to assess the statistical differences in peak latencies between different visual features.</p>
</sec>
<sec id="s2e3">
<title>ANN (or W2V) × HYP RSA</title>
<p>To evaluate how different visual information is represented in ANNs, we calculated representational similarity between the ANN RDMs and hypothesis-based RDMs. As in the EEG × HYP RSA, we calculated partial correlations to avoid correlations between hypothesis-based RDMs. We also calculated the partial correlations between hypothesis-based RDMs and the Word2Vec RDM. To determine statistical significance, we conducted a bootstrap test. We shuffled the order of the cells above the diagonal in each ANN (or Word2Vec) RDM 1000 times. For each iteration, we calculated partial correlations corresponding to the three hypothesis-based RDMs. This produced a 1000-sample null distribution for each HYP x ANN (or W2V) RSA. We hypothesized that if the real similarity was higher than the 95% confidence interval of the null distribution, it indicated that ANN (or W2V) features validly encoded the corresponding visual feature.</p>
<p>Additionally, to explore how the naturalistic background present in the images might influence object real-world size, retinal size, and real-world depth representations, we conducted another version of the analysis by inputting cropped object images without background into ANN models to obtain object-only ANN RDMs (<xref rid="figs3" ref-type="fig">Figure S3</xref>). Then we performed the same ANN x HYP similarity analysis to calculate partial correlations between the hypothesis-based RDMs and object-only ANN RDM. (We didn’t conduct the similarity analysis between timepoint-by-timepoint EEG neural RDMs with subjects viewing natural images and object-only ANN RDMs due to the input differences.)</p>
</sec>
</sec>
<sec id="s2f">
<title>Data and code accessibility</title>
<p>EEG data and images from THINGS EEG2 data are publicly available on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/3jk45/">https://osf.io/3jk45/</ext-link>). All Python analysis scripts will be available post-publication on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ZitongLu1996/RWsize">https://github.com/ZitongLu1996/RWsize</ext-link>).</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>We conducted a cross-modal representational similarity analysis (<xref rid="fig1" ref-type="fig">Figures 1</xref>-<xref rid="fig2" ref-type="fig">2</xref>, see Method section for details) comparing the patterns of human brain activation (timepoint-by-timepoint decoding of EEG data) while participants viewed naturalistic object images, the output of different layers of artificial neural networks and semantic language models fed the same stimuli (ANN and Word2Vec models), and hypothetical patterns of representational similarity based on behavioral and mathematical measurements of different visual image properties (perceived real-world object size, displayed retinal object size, and perceived real-world object depth).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1</label>
<caption><p>Overview of our analysis pipeline including constructing three types of RDMs and conducting comparisons between them.</p></caption>
<graphic xlink:href="553999v5_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption><p>Methods for calculating neural (EEG), hypothesis-based (HYP), and artificial neural network (ANN) &amp; semantic language processing (Word2Vec, W2V) model-based representational dissimilarity matrices (RDMs). (A) Steps of computing the neural RDMs from EEG data. EEG analyses were performed in a time-resolved manner on 17 channels as features. For each time t, we conducted pairwise cross-validated SVM classification. The classification accuracy values across different image pairs resulted in each 200 × 200 RDM for each time point. (B) Calculating the three hypothesis-based RDMs: Real-World Size RDM, Retinal Size RDM, and Real-World Depth RDM. Real-world size, retinal size, and real-world depth were calculated for the object in each of the 200 stimulus images. The number in the bracket represents the rank (out of 200, in ascending order) based on each feature corresponding to the object in each stimulus image (e.g. “ferry” ranks 197<sup>th</sup> in real-world size from small to big out of 200 objects). The connection graph to the right of each RDM represents the relative representational distance of three stimuli in the corresponding feature space. (C) Steps of computing the ANN and Word2Vec RDMs. For ANNs, the inputs were the resized images, and for Word2Vec, the inputs were the words of object concepts. For clearer visualization, the shown RDMs were separately histogram-equalized (percentile units).</p></caption>
<graphic xlink:href="553999v5_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s3a">
<title>Dynamic representations of object size and depth in human brains</title>
<p>To explore if and when human brains contain distinct representations of perceived real-world size, retinal size, and real-world depth, we constructed timepoint-by-timepoint EEG neural RDMs (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), and compared these to three hypothesis-based RDMs corresponding to different visual image properties (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Firstly, we confirmed that the hypothesis-based RDMs were indeed correlated with each other (<xref rid="fig3" ref-type="fig">Figure 3A</xref>), and without accounting for the confounding variables, Spearman correlations between the EEG and each hypothesis-based RDM revealed overlapping periods of representational similarity (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). In particular, representational similarity with real-world size (from 90 to 120ms and from 170 to 240ms) overlapped with the significant time-windows of other features, including retinal size from 70 to 210ms, and real-world depth from 60 to 130ms and from 180 to 230ms. But critically, with the partial correlations, we isolated their independent representations. The partial correlation results reveal a pure representation of object real-world size in the human brain from 170 to 240ms after stimulus onset, independent from retinal size and real-world depth, which showed significant representational similarity at different time windows (retinal size from 90 to 200ms, and real-world depth from 60 to 130ms and 270 to 300ms) (<xref rid="fig3" ref-type="fig">Figure 3D</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3</label>
<caption><p>Cross-modal RSA results. (A) Similarities (Spearman correlations) between three hypothesis-based RDMs. Asterisks indicate a significant similarity, p&lt;.05. (B) Representational similarity time courses (full Spearman correlations) between EEG neural RDMs and hypothesis-based RDMs. (C) Temporal latencies for peak similarity (partial Spearman correlations) between EEG and the 3 types of object information. Error bars indicate ±SEM. Asterisks indicate significant differences across conditions (p&lt;.05); (D) Representational similarity time courses (partial Spearman correlations) between EEG neural RDMs and hypothesis-based RDMs. (E) Representational similarities (partial Spearman correlations) between the four ANN RDMs and hypothesis-based RDMs of real-world depth, retinal size, and real-world size. Asterisks indicate significant partial correlations (bootstrap test, p&lt;.05). (F) Representational similarity time courses (Spearman correlations) between EEG neural RDMs and ANN RDMs. Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;.05). Shaded area reflects ±SEM.</p></caption>
<graphic xlink:href="553999v5_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Peak latency results showed that neural representations of real-world size, retinal size and real-world depth reached their peaks at different latencies after stimulus onset (real-world depth: ∼87ms, retinal size: ∼138ms, real-world size: ∼206ms, <xref rid="fig3" ref-type="fig">Figure 3C</xref>). The representation of real-word size had a significantly later peak latency than that of both retinal size (t=4.2950, p=.0020) and real-world depth (t=18.5847, p&lt;.001). And retinal size representation had a significantly later peak latency than real-world depth (t=3.7229, p=.0047). These varying peak latencies imply an encoding order for distinct visual features, transitioning from real-world depth through retinal size, and then to real-world size.</p>
</sec>
<sec id="s3b">
<title>Artificial neural networks also reflect distinct representations of object size and depth</title>
<p>To test how ANNs process these visual properties, we input the same stimulus images into ANN models and got their latent features from early and late layers (<xref rid="fig2" ref-type="fig">Figure 2C</xref>), and then conducted comparisons between the ANN RDMs and hypothesis-based RDMs. Parallel to our findings of dissociable representations of real-world size, retinal size, and real-world depth in the human brain signal, we also found dissociable representations of these visual features in ANNs (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). Our partial correlation RSA analysis showed that early layers of both ResNet and CLIP had significant real-world depth and retinal size representations, whereas the late layers of both ANNs were dominated by real-world size representations, though there was also weaker retinal size representation in the late layer of ResNet and real-world depth representation in the late layer of CLIP. The detailed statistical results are shown in <xref rid="tbls1" ref-type="table">Table S1</xref>.</p>
<p>Thus, ANNs provide another approach to understand the formation of different visual features, offering convergent results with the EEG representational analysis, where retinal size was reflected most in the early layers of ANNs, while object real-world size representations didn’t emerge until late layers of ANNs, consistent with a potential role of higher-level visual information, such as the semantic information of object concepts.</p>
<p>Finally, we directly compared the timepoint-by-timepoint EEG neural RDMs and the ANN RDMs (<xref rid="fig3" ref-type="fig">Figure 3F</xref>). The early layer representations of both ResNet and CLIP were significantly correlated with early representations in the human brain (early layer of ResNet: 40-280ms, early layer of CLIP: 50-130ms and 160-260ms), while the late layer representations of two ANNs were significantly correlated with later representations in the human brain (late layer of ResNet: 80-300ms, late layer of CLIP: 70-300ms). This pattern of early-to-late correspondence aligns with previous findings that convolutional neural networks exhibit similar hierarchical representations to those in the brain visual cortex (<xref ref-type="bibr" rid="c10">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="c28">Kietzmann et al., 2019</xref>; <xref ref-type="bibr" rid="c55">Yamins &amp; DiCarlo, 2016</xref>): that both the early stage of brain processing and the early layer of the ANN encode lower-level visual information, while the late stage of the brain and the late layer of the ANN encode higher-level visual information. Also, human brain representations showed a higher similarity to the early layer representation of the visual model (ResNet) than to the visual-semantic model (CLIP) at an early stage. Conversely, human brain representations showed a higher similarity to the late layer representation of the visual-semantic model (CLIP) than the visual model (ResNet) at a late stage. Interestingly, the peaks of significant time windows for the EEG × HYP RSA also correspond with the peaks of the EEG × ANN RSA timecourse (<xref rid="fig3" ref-type="fig">Figure 3D,F</xref>).</p>
</sec>
<sec id="s3c">
<title>Real-world size as a stable and higher-level dimension in object space</title>
<p>An important aspect of the current study is the use of naturalistic visual images as stimuli, in which objects were presented in their natural contexts, as opposed to cropped images of objects without backgrounds. In natural images, background can play an important role in object perception. How dependent are the above results on the presence of naturalistic background context? To investigate how image context influences object size and depth representations, we next applied a reverse engineering method, feeding the ANNs with modified versions of the stimulus images containing cropped objects without background, and evaluating the ensuing ANN representations compared to the same original hypothesis-based RDMs. If the background significantly contributes to the formation of certain feature representations, we may see some encoding patterns in ANNs disappear when the input only includes the pure object but no background.</p>
<p>Compared to results based on images with background, the ANNs based on cropped-object modified images showed weaker overall representational similarity for all features (<xref rid="fig4" ref-type="fig">Figure 4</xref>). In the early layers of both ANNs, we now only observed significantly preserved retinal size representations (which is a nice validity check, since retinal size measurements were based purely on the physical object dimensions in the image, independent of the background). Real-world depth representations were almost totally eliminated, with only a small effect in the late layer of ResNet. However, we still observed a preserved pattern of real-world size representations, with significant representational similarity in the late layers of both ResNet and CLIP, and not in the early layers. The detailed statistical results are shown in <xref rid="tbls2" ref-type="table">Table S2</xref>. Even though the magnitude of representational similarity for object real-world size decreased when we removed the background, this high-level representation was not entirely eliminated. This finding suggests that background information does indeed influence object processing, but the representation of real-world size seems to be a relatively stable higher-level feature. On the other hand, representational formats of real-world depth changed when the input lacked background information. The deficiency of real-world depth representations in early layers, compared to when using full-background images, might suggest that the human brain typically uses background information to estimate object depth, though the significant effect in the late layer of ResNet in background-absent condition might also suggest that the brain (or at least ANN) has additional ability to integrate size information to infer depth when there is no background.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4</label>
<caption><p>Contribution of image backgrounds to object size and depth representations. Representational similarity results (partial Spearman correlations) between ANNs fed inputs of cropped object images without backgrounds and the hypothesis-based RDMs. Stars above bars indicate significant partial correlations (bootstrap test, p&lt;.05).</p></caption>
<graphic xlink:href="553999v5_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The above results reveal that real-world size emerges with later peak neural latencies and in the later layers of ANNs, regardless of image background information. Is this because real-world size is a more conceptual-level dimension in object semantic space? If so, we might expect it to be driven not only by higher-level visual information, but also potentially by purely semantic information about familiar objects. To test this, we extracted object names from each image and input the object names into a Word2Vec model to obtain a Word2Vec RDM (<xref rid="figs2" ref-type="fig">Figure S2</xref>), and then conducted a partial correlation RSA comparing the Word2Vec representations with the hypothesis-based RDMs (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). The results showed a significant real-world size representation (r=0.1871, p&lt;0.001) but no representation of retinal size (r=-0.0064, p=0.8148) or real-world depth (r=-0.0040, p=.7151) from Word2Vec. Also, the significant time-window (90-300ms) of similarity between Word2Vec RDM and EEG RDMs (<xref rid="fig5" ref-type="fig">Figure 5B</xref>) contained the significant time-window of EEG x real-world size representational similarity (<xref rid="fig3" ref-type="fig">Figure 3B</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5</label>
<caption><p>Representation similarity with a non-visual semantic language processing model (Word2Vec) fed word inputs corresponding to the images’ object concepts. (A) Representational similarity results (partial Spearman correlations) between Word2Vec RDM and hypothesis-based RDMs. Stars above bars indicate significant partial correlations (bootstrap test, p&lt;.05). (B) Representational similarity time course (Spearman correlations) between EEG RDMs (neural activity while viewing images) and Word2Vec RDM (fed corresponding word inputs). Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;.05). Line width reflects ±SEM.</p></caption>
<graphic xlink:href="553999v5_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Both the reverse engineering manipulation and Word2Vec findings corroborate that object real-world size representation, unlike retinal size and real-world depth, emerges in both image- and semantic-level in object space.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>Our study applied computational methods to distinguish the representations of objects’ perceived real-world size, retinal size, and perceived real-world depth features in both human brains and ANNs. Consistent with prior studies reporting real-world size representations (<xref ref-type="bibr" rid="c22">Huang et al., 2022</xref>; S.-M. <xref ref-type="bibr" rid="c26">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="c29">Konkle &amp; Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c32">Konkle &amp; Oliva, 2012b</xref>; <xref ref-type="bibr" rid="c40">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="c45">Quek et al., 2023</xref>; R. <xref ref-type="bibr" rid="c52">Wang et al., 2022</xref>), we found that both human brains and ANNs contain significant information about real-world size. Critically, compared to the prior studies, our study offers several important theoretical and methodological advances: (a) we eliminated the confounding impact of perceived real-world depth (in addition to retinal size) on the real-world size representation; (b) we conducted analyses based on more ecologically valid naturalistic images; (c) we obtained precise feature values for each object in every image, instead of simply dividing objects into 2 or 7 coarse categories; and (d) we utilized a multi-modal, partial correlation RSA that combines EEG, hypothesis-based models, and ANNs. This novel approach allowed us to investigate representational time courses and reverse engineering manipulations in unparalleled detail. By integrating EEG data with hypothesis-based models and ANNs, this method offers a powerful tool for dissecting the neural underpinnings of object size and depth perception in more ecological contexts, which enriches our comprehension of the brain’s representational mechanisms.</p>
<p>Using EEG we uncovered a representational timeline for visual object processing, with object real-world depth information represented first, followed by retinal size, and finally real-world size. While size and depth are highly correlated to each other, our results suggest that the human brain indeed has dissociated time courses and mechanisms to process them. The later representation time-window for object real-world size may suggest that the brain requires more sophisticated, higher-level information to form this representation, perhaps incorporating semantic and/or memory information about familiar objects, which was corroborated by our ANN and Word2Vec analyses. These findings also align with a recent fMRI study (<xref ref-type="bibr" rid="c40">Luo et al., 2023</xref>) using natural images to explore the neural selectivity for real-world size, finding that low-level visual information could hardly account for neural size preferences, although that study did not consider covariables like retinal size and real-world depth.</p>
<p>In contrast to the later-emerging real-world size representations, it makes sense that retinal size representations could be processed more quickly based on more fundamental, lower-level information such as shape and edge discrimination. The intermediate latency for real-world depth processing suggests that this feature may precede real-world size processing. Additionally, there was a secondary, albeit substantially later, significant depth representation time-window, which might indicate that our brains also have the ability to integrate object retinal size and higher-level real-size information to form the final representation of real-world depth. Our comparisons between human brains and artificial models and explorations on ANNs and Word2Vec offer further insights and suggest that although real-world object size and depth are closely related, object real-world size appears to be a more stable and higher-level dimension.</p>
<p>The concept of ‘object space’ in cognitive neuroscience research is crucial for understanding how various visual features of objects are represented. Historically, various visual features have been considered important dimensions in constructing object space, including animate-inanimate (<xref ref-type="bibr" rid="c33">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="c43">Naselaris et al., 2012</xref>), spikiness (<xref ref-type="bibr" rid="c3">Bao et al., 2020</xref>; <xref ref-type="bibr" rid="c11">Coggan &amp; Tong, 2023</xref>), and physical appearance (<xref ref-type="bibr" rid="c15">Edelman et al., 1998</xref>). In this study, we focus on one particular dimension, real-world size (<xref ref-type="bibr" rid="c22">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="c29">Konkle &amp; Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c32">Konkle &amp; Oliva, 2012b</xref>). How we generate neural distinctions of different object real-world size and where this ability comes from remain uncertain. Some previous studies found that object shape rather than texture information could trigger neural size representations (<xref ref-type="bibr" rid="c22">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="c36">Long et al., 2016</xref>, <xref ref-type="bibr" rid="c37">2018</xref>; R. <xref ref-type="bibr" rid="c51">Wang et al., 2022</xref>). Our results attempt to further advance their findings, that object real-world size is a stable and higher-level dimension substantially driven by object semantics in object space.</p>
<p>Increasingly, research has begun to use ANNs to study the mechanisms of object recognition (<xref ref-type="bibr" rid="c2">Ayzenberg et al., 2023</xref>; <xref ref-type="bibr" rid="c9">Cichy &amp; Kaiser, 2019</xref>; <xref ref-type="bibr" rid="c14">Doerig et al., 2023</xref>; <xref ref-type="bibr" rid="c25">Kanwisher et al., 2023</xref>). We can explore how the human brain processes information at different levels by comparing brain activity with models (<xref ref-type="bibr" rid="c10">Cichy et al., 2016</xref>; S. M. <xref ref-type="bibr" rid="c27">Khaligh-Razavi &amp; Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="c34">Kuzovkin et al., 2018</xref>; <xref ref-type="bibr" rid="c53">Xie et al., 2020</xref>), and we can also analyze the representation patterns of the models with some specific manipulations and infer potential processing mechanisms in the brain (<xref ref-type="bibr" rid="c17">Golan et al., 2020</xref>; <xref ref-type="bibr" rid="c22">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="c39">Lu &amp; Ku, 2023</xref>; <xref ref-type="bibr" rid="c54">Xu et al., 2021</xref>). In current study, our comparisons result between EEG signals and different ANNs showed that the visual model’s early layer had a higher similarity to the brain in the early stage, while the visual-semantic model’s late layer had a higher similarity to the brain in the late stage. However, for the representation of objects, partial correlation results for different ANNs didn’t demonstrate the superiority of the multi-modal model at late layers. This might be due to models like CLIP, which contain semantic information, learning more complex image descriptive information (like the relationship between object and the background in the image). Real-world size might be a semantic dimension of the object itself, and its representation does not require overall semantic descriptive information of the image. In contrast, retinal size and real-world depth could rely on image background information for estimation, thus their representations in the CLIP late layer disappeared when input images had only pure object but no background.</p>
<p>Building on the promising findings of our study, future work may further delve into the detailed processes of object processing and object space. One important problem to solve is how real-world size interacts with other object dimensions in object space. In addition, our approach could be used with future studies investigating other influences on object processing, such as how different task conditions impact and modulate the processing of various visual features.</p>
<p>Moreover, we must also emphasize that in this study, we were concerned with perceived real-world size and depth reflecting a perceptual estimation of our world, which are slightly different from absolute physical size and depth. The differences in brain encoding between perceived and absolute physical size and depth require more comprehensive measurements of an object’s physical attributes for further exploration. Also, we focused on perceiving depth and size from 2D images in this study, which might have some differences in brain mechanism compared to physically exploring the 3D world. Nevertheless, we believe our study offers a valuable contribution to object recognition, especially the encoding process of object real-world size in natural images.</p>
<p>In conclusion, we used computational methods to distinguish the representations of real-world size, retinal size, and real-world depth features of objects in ecologically natural images in both human brains and ANNs. We found an unconfounded representation of object real-world size, which emerged at later time windows in the human EEG signal and at later layers of artificial neural networks compared to real-world depth, and which also appeared to be preserved as a stable dimension in object space. Thus, although size and depth properties are closely correlated, the processing of perceived object size and depth may arise through dissociated time courses and mechanisms. Our research provides a detailed and clear characterization of the object processing process, which offers further advances and insights into our understanding of object space and the construction of more brain-like visual models.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by research grants from the National Institutes of Health (R01-EY025648) and from the National Science Foundation (NSF 1848939) to JDG. The authors declare no competing financial interests.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Allen</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>St-Yves</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Breedlove</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Prince</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Dowdle</surname>, <given-names>L. T.</given-names></string-name>, <string-name><surname>Nau</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Caron</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Pestilli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Hutchinson</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Kay</surname>, <given-names>K</given-names></string-name>. (<year>2022</year>). <article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>1</issue>), <fpage>116</fpage>–<lpage>126</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="preprint"><string-name><surname>Ayzenberg</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Blauch</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Behrmann</surname>, <given-names>M.</given-names></string-name> (<year>2023</year>). <article-title>Using deep neural networks to address the how of object recognition</article-title>. <source>PsyArXiv</source>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Bao</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>She</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>McGill</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Tsao</surname>, <given-names>D. Y</given-names></string-name>. (<year>2020</year>). <article-title>A map of object space in primate inferotemporal cortex</article-title>. <source>Nature</source>, <volume>583</volume>(<issue>7814</issue>), <fpage>103</fpage>–<lpage>108</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bracci</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Daniels</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Op de Beeck</surname>, <given-names>H.</given-names></string-name> (<year>2017</year>). <article-title>Task Context Overrules Object- and Category-Related Representational Content in the Human Parietal Cortex</article-title>. <source>Cerebral Cortex</source>, <volume>27</volume>(<issue>1</issue>), <fpage>310</fpage>–<lpage>321</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bracci</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Op de Beeck</surname>, <given-names>H.</given-names></string-name> (<year>2016</year>). <article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title>. <source>Journal of Neuroscience</source>, <volume>36</volume>(<issue>2</issue>), <fpage>432</fpage>– <lpage>444</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Chen</surname>, <given-names>Y. C.</given-names></string-name>, <string-name><surname>Deza</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Konkle</surname>, <given-names>T</given-names></string-name>. (<year>2022</year>). <article-title>How big should this object be? Perceptual influences on viewing-size preferences</article-title>. <source>Cognition</source>, <volume>225</volume>, <fpage>105114</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Choksi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Mozafari</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>VanRullen</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Reddy</surname>, <given-names>L</given-names></string-name>. (<year>2022</year>). <article-title>Multimodal neural networks better explain multivoxel patterns in the hippocampus</article-title>. <source>Neural Networks</source>, <volume>154</volume>, <fpage>538</fpage>–<lpage>542</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="confproc"><string-name><surname>Choksi</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Vanrullen</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Reddy</surname>, <given-names>L</given-names></string-name>. (<year>2022</year>, August 25). <article-title>Do multimodal neural networks better explain human visual representations than vision-only networks?</article-title> <source>Conference on Cognitive Computational Neuroscience</source>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, &amp; <string-name><surname>Kaiser</surname>, <given-names>D</given-names></string-name>. (<year>2019</year>). <article-title>Deep Neural Networks as Scientific Models</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>23</volume>(<issue>4</issue>), <fpage>305</fpage>–<lpage>317</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Khosla</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Torralba</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name>. (<year>2016</year>). <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Scientific Reports</source>, <volume>6</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Coggan</surname>, <given-names>D. D.</given-names></string-name>, &amp; <string-name><surname>Tong</surname>, <given-names>F</given-names></string-name>. (<year>2023</year>). <article-title>Spikiness and animacy as potential organizing principles of human ventral visual cortex</article-title>. <source>Cerebral Cortex</source>, <volume>33</volume>(<issue>13</issue>), <fpage>8194</fpage>–<lpage>8217</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="preprint"><string-name><surname>Conwell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Prince</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name>, &amp; <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name> (<year>2022</year>). <article-title>Large-Scale Benchmarking of Diverse Artificial Vision Models in Prediction of 7T Human Neuroimaging Data</article-title>. <source>BioRxiv</source>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="preprint"><string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name> (<year>2022</year>). <article-title>Semantic scene descriptions as an objective of human vision</article-title>. <source>ArXiv</source>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sommers</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Seeliger</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ismael</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lindsay</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kording</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Van Gerven</surname>, <given-names>M. A. J.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name> (<year>2023</year>). <article-title>The neuroconnectionist research programme</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>24</volume>, <fpage>431</fpage>–<lpage>450</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Edelman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kushnir</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Malach</surname>, <given-names>R</given-names></string-name>. (<year>1998</year>). <article-title>Toward direct visualization of the internal shape representation space by fMRI</article-title>. <source>Psychobiology</source>, <volume>26</volume>(<issue>4</issue>), <fpage>309</fpage>–<lpage>321</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Gifford</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Dwivedi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Roig</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Cichy</surname>, <given-names>R. M</given-names></string-name>. (<year>2022</year>). <article-title>A large and rich EEG dataset for modeling human visual object recognition</article-title>. <source>NeuroImage</source>, <volume>264</volume>, <fpage>119754</fpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Golan</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Raju</surname>, <given-names>P. C.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name>. (<year>2020</year>). <article-title>Controversial stimuli: Pitting neural networks against each other as models of human cognition</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>117</volume>(<issue>47</issue>), <fpage>29330</fpage>–<lpage>29337</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, &amp; <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name>. (<year>2022</year>). <article-title>Human EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams</article-title>. <source>Scientific Data</source>, <volume>9</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Güçlü</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>van Gerven</surname>, <given-names>M. A. J.</given-names></string-name> (<year>2015</year>). <article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>27</issue>), <fpage>10005</fpage>–<lpage>10014</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="confproc"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>J</given-names></string-name>. (<year>2016</year>). <article-title>Deep Residual Learning for Image Recognition</article-title>. <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>, <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Dickter</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Kidder</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kwok</surname>, <given-names>W. Y.</given-names></string-name>, <string-name><surname>Corriveau</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Van Wicklin</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2019</year>). <article-title>THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title>. <source>PLoS ONE</source>, <volume>14</volume>(<issue>10</issue>), <fpage>1</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Huang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>J</given-names></string-name>. (<year>2022</year>). <article-title>Real-world size of objects serves as an axis of object space</article-title>. <source>Communications Biology</source>, <volume>5</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vu</surname>, <given-names>A. T.</given-names></string-name>, &amp; <string-name><surname>Gallant</surname>, <given-names>J. L</given-names></string-name>. (<year>2012</year>). <article-title>A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain</article-title>. <source>Neuron</source>, <volume>76</volume>(<issue>6</issue>), <fpage>1210</fpage>–<lpage>1224</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Jozwik</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Mur</surname>, <given-names>M</given-names></string-name>. (<year>2023</year>). <article-title>Deep Neural Networks and Visuo-Semantic Models Explain Complementary Components of Human Ventral-Stream Representational Dynamics</article-title>. <source>Journal of Neuroscience</source>, <volume>43</volume>(<issue>10</issue>), <fpage>1731</fpage>–<lpage>1741</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Khosla</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Dobs</surname>, <given-names>K</given-names></string-name>. (<year>2023</year>). <article-title>Using artificial neural networks to ask ‘why’ questions of minds and brains</article-title>. <source>Trends in Neurosciences</source>, <volume>46</volume>(<issue>3</issue>), <fpage>240</fpage>–<lpage>254</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name>. (<year>2018</year>). <article-title>Tracking the Spatiotemporal Neural Dynamics of Real-world Object Size and Animacy in the Human Brain</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>30</volume>(<issue>11</issue>), <fpage>1559</fpage>–<lpage>1576</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Khaligh-Razavi</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name>. (<year>2014</year>). <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Computational Biology</source>, <volume>10</volume>(<issue>11</issue>), <fpage>e1003915</fpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name>, <string-name><surname>Spoerer</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Sörensen</surname>, <given-names>L. K. A.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Hauk</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name>. (<year>2019</year>). <article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>116</volume>(<issue>43</issue>), <fpage>21854</fpage>–<lpage>21863</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Caramazza</surname>, <given-names>A</given-names></string-name>. (<year>2013</year>). <article-title>Tripartite Organization of the Ventral Stream by Animacy and Object Size</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>25</issue>), <fpage>10235</fpage>–<lpage>10242</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name>. (<year>2011</year>). <article-title>Canonical Visual Size for Real-World Objects</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>37</volume>(<issue>1</issue>), <fpage>23</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name>. (<year>2012a</year>). <article-title>A familiar-size Stroop effect: Real-world size is an automatic property of object representation</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>38</volume>(<issue>3</issue>), <fpage>561</fpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name>. (<year>2012b</year>). <article-title>A Real-World Size Organization of Object Responses in Occipitotemporal Cortex</article-title>. <source>Neuron</source>, <volume>74</volume>(<issue>6</issue>), <fpage>1114</fpage>–<lpage>1124</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ruff</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Kiani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bodurka</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Esteky</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tanaka</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A</given-names></string-name>. (<year>2008</year>). <article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title>. <source>Neuron</source>, <volume>60</volume>(<issue>6</issue>), <fpage>1126</fpage>–<lpage>1141</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Kuzovkin</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Vicente</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Petton</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lachaux</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Baciu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kahane</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rheims</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vidal</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Aru</surname>, <given-names>J</given-names></string-name>. (<year>2018</year>). <article-title>Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex</article-title>. <source>Communications Biology</source>, <volume>1</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Long</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Konkle</surname>, <given-names>T</given-names></string-name>. (<year>2017</year>). <article-title>A familiar-size Stroop effect in the absence of basic-level recognition</article-title>. <source>Cognition</source>, <volume>168</volume>, <fpage>234</fpage>–<lpage>242</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Long</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Alvarez</surname>, <given-names>G. A</given-names></string-name>. (<year>2016</year>). <article-title>Mid-level perceptual features distinguish objects of different real-world sizes</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>145</volume>(<issue>1</issue>), <fpage>95</fpage>–<lpage>109</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Long</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>C. P.</given-names></string-name>, &amp; <string-name><surname>Konkle</surname>, <given-names>T</given-names></string-name>. (<year>2018</year>). <article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>115</volume>(<issue>38</issue>), <fpage>E9015</fpage>–<lpage>E9024</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Lu</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Ku</surname>, <given-names>Y</given-names></string-name>. (<year>2020</year>). <article-title>NeuroRA: A Python Toolbox of Representational Analysis From Multi-Modal Neural Data</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>14</volume>, <fpage>61</fpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Lu</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Ku</surname>, <given-names>Y</given-names></string-name>. (<year>2023</year>). <article-title>Bridging the gap between EEG and DCNNs reveals a fatigue mechanism of facial repetition suppression</article-title>. <source>IScience</source>, <volume>26</volume>(<issue>12</issue>), <fpage>108501</fpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="preprint"><string-name><surname>Luo</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Wehbe</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Henderson</surname>, <given-names>M. M.</given-names></string-name> (<year>2023</year>). <article-title>Neural Selectivity for Real-World Object Size In Natural Images Abbreviated title : Neural Selectivity for Real-World Size</article-title>. <source>BioRxiv</source>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="confproc"><string-name><surname>Mikolov</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Corrado</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Dean</surname>, <given-names>J</given-names></string-name>. (<year>2013</year>). <article-title>Efficient Estimation of Word Representations in Vector Space</article-title>. <source>Proceedings of the International Conference on Learning Representations (ICLR)</source>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Muttenthaler</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Hebart</surname>, <given-names>M. N</given-names></string-name>. (<year>2021</year>). <article-title>THINGSvision: A Python Toolbox for Streamlining the Extraction of Activations From Deep Neural Networks</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>15</volume>, <fpage>45</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Stansbury</surname>, <given-names>D. E.</given-names></string-name>, &amp; <string-name><surname>Gallant</surname>, <given-names>J. L</given-names></string-name>. (<year>2012</year>). <article-title>Cortical representation of animate and inanimate objects in complex natural scenes</article-title>. <source>Journal of Physiology Paris</source>, <volume>106</volume>(<issue>5–6</issue>), <fpage>239</fpage>– <lpage>249</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Proklova</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kaiser</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Peelen</surname>, <given-names>M. V</given-names></string-name>. (<year>2016</year>). <article-title>Disentangling Representations of Object Shape and Object Category in Human Visual Cortex: The Animate–Inanimate Distinction</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>28</volume>(<issue>5</issue>), <fpage>680</fpage>–<lpage>692</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="preprint"><string-name><surname>Quek</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Theodorou</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name> (<year>2023</year>). <article-title>Better together : Objects in familiar constellations evoke high-level representations of real-world size</article-title>. <source>BioRxiv</source>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="confproc"><string-name><surname>Radford</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Hallacy</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ramesh</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Goh</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Agarwal</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sastry</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Askell</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mishkin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Clark</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Krueger</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Sutskever</surname>, <given-names>I</given-names></string-name>. (<year>2021</year>). <article-title>Learning Transferable Visual Models From Natural Language Supervision</article-title>. <source>Proceedings of the International Conference on Machine Learning (ICML)</source>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Řehůřek</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Sojka</surname>, <given-names>P.</given-names></string-name> (<year>2010</year>). <article-title>Software framework for topic modelling with large corpora</article-title>. <source>Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</source>, <fpage>45</fpage>–<lpage>50</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Setti</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Caramelli</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Borghi</surname>, <given-names>A. M</given-names></string-name>. (<year>2008</year>). <article-title>Conceptual information about size of objects in nouns</article-title>. <source>European Journal of Cognitive Psychology</source>, <volume>21</volume>(<issue>7</issue>), <fpage>1022</fpage>–<lpage>1044</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Stoinski</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Perkuhn</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Hebart</surname>, <given-names>M. N</given-names></string-name>. (<year>2023</year>). <article-title>THINGSplus: New norms and metadata for the THINGS database of 1854 object concepts and 26,107 natural object images</article-title>. <source>Behavior Research Methods</source>, <fpage>1</fpage>–<lpage>21</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Troiani</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Stigliani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Epstein</surname>, <given-names>R. A</given-names></string-name>. (<year>2014</year>). <article-title>Multiple object properties drive scene-selective regions</article-title>. <source>Cerebral Cortex</source>, <volume>24</volume>(<issue>4</issue>), <fpage>883</fpage>–<lpage>897</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="preprint"><string-name><surname>Wang</surname>, <given-names>A. Y.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Wehbe</surname>, <given-names>L.</given-names></string-name> (<year>2022</year>). <article-title>Incorporating natural language into vision models improves prediction and understanding of higher visual cortex</article-title>. <source>BioRxiv</source>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Janini</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Konkle</surname>, <given-names>T</given-names></string-name>. (<year>2022</year>). <article-title>Mid-level feature differences support early animacy and object size distinctions: Evidence from electroencephalography decoding</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>34</volume>(<issue>9</issue>), <fpage>1670</fpage>–<lpage>1680</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Xie</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kaiser</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Cichy</surname>, <given-names>R. M</given-names></string-name>. (<year>2020</year>). <article-title>Visual Imagery and Perception Share Neural Representations in the Alpha Frequency Band</article-title>. <source>Current Biology</source>, <volume>30</volume>(<issue>13</issue>), <fpage>2621</fpage>–<lpage>2627</lpage>.e5.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Xu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhen</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>J</given-names></string-name>. (<year>2021</year>). <article-title>The Face Module Emerged in a Deep Convolutional Neural Network Selectively Deprived of Face Experience</article-title>. <source>Frontiers in Computational Neuroscience</source>, <volume>15</volume>, <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J</given-names></string-name>. (<year>2016</year>). <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>3</issue>), <fpage>356</fpage>–<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Cadieu</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Solomon</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Seibert</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>DiCarlo</surname>, <given-names>J. J</given-names></string-name>. (<year>2014</year>). <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>111</volume>(<issue>23</issue>), <fpage>8619</fpage>–<lpage>8624</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Supplementary materials</title>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><p>Four ANN RDMs of ResNet early layer, ResNet late layer, CLIP early layer, and CLIP late later.</p></caption>
<graphic xlink:href="553999v5_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><p>Word2Vec RDMs.</p></caption>
<graphic xlink:href="553999v5_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><p>Four ANN RDMs with inputs of cropped object images without background of ResNet early layer, ResNet late layer, CLIP early layer, and CLIP late later.</p></caption>
<graphic xlink:href="553999v5_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1</label><caption><title>Statistical results of similarities (partial Spearman correlations) between four ANN RDMs and three hypothesis-based RDMs.</title></caption>
<graphic xlink:href="553999v5_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table S2</label><caption><title>Statistical results of similarities (partial Spearman correlations) between four ANN RDMs with inputs of cropped object images without background and three hypothesis-based RDMs.</title></caption>
<graphic xlink:href="553999v5_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98117.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>SP</surname>
<given-names>Arun</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Indian Institute of Science Bangalore</institution>
</institution-wrap>
<city>Bangalore</city>
<country>India</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>useful</bold> study measured how information about object categories varies with time in EEG responses to object images in human participants and found that real-world size, retinal size, and real-world depth are represented at different time points in the response. The evidence presented is <bold>incomplete</bold> and can be further strengthened by removing confounds related to other covarying properties such as semantic categories, and by clarifying the partial correlations that are used to support the conclusions.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98117.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Lu &amp; Golomb combined EEG, artificial neural networks, and multivariate pattern analyses to examine how different visual variables are processed in the brain. The conclusions of the paper are mostly well supported, but some aspects of methods and data analysis would benefit from clarification and potential extensions.</p>
<p>The authors find that not only real-world size is represented in the brain (which was known), but both retinal size and real-world depth are represented, at different time points or latencies, which may reflect different stages of processing. Prior work has not been able to answer the question of real-world depth due to the stimuli used. The authors made this possible by assessing real-world depth and testing it with appropriate methodology, accounting for retinal and real-world size. The methodological approach combining behavior, RSA, and ANNs is creative and well thought out to appropriately assess the research questions, and the findings may be very compelling if backed up with some clarifications and further analyses.</p>
<p>The work will be of interest to experimental and computational vision scientists, as well as the broader computational cognitive neuroscience community as the methodology is of interest and the code is or will be made available. The work is important as it is currently not clear what the correspondence between many deep neural network models and the brain is, and this work pushes our knowledge forward on this front. Furthermore, the availability of methods and data will be useful for the scientific community.</p>
<p>Some analyses are incomplete, which would be improved if the authors showed analyses with other layers of the networks and various additional partial correlation analyses.</p>
<p>Clarity</p>
<p>(1) Partial correlations methods incomplete - it is not clear what is being partialled out in each analysis. It is possible to guess sometimes, but it is not entirely clear for each analysis. This is important as it is difficult to assess if the partial correlations are sensible/correct in each case. Also, the Figure 1 caption is short and unclear.</p>
<p>For example, ANN-EEG partial correlations - &quot;Finally, we directly compared the timepoint-by-timepoint EEG neural RDMs and the ANN RDMs (Figure 3F). The early layer representations of both ResNet and CLIP were significantly correlated with early representations in the human brain&quot; What is being partialled out? Figure 3F says partial correlation</p>
<p>Issues / open questions</p>
<p>(2) Semantic representations vs hypothesized (hyp) RDMs (real-world size, etc) - are the representations explained by variables in hyp RDMs or are there semantic representations over and above these? E.g., For ANN correlation with the brain, you could partial out hyp RDMs - and assess whether there is still semantic information left over, or is the variance explained by the hyp RDMs?</p>
<p>(3) Why only early and late layers? I can see how it's clearer to present the EEG results. However, the many layers in these networks are an opportunity - we can see how simple/complex linear/non-linear the transformation is over layers in these models. It would be very interesting and informative to see if the correlations do in fact linearly increase from early to later layers, or if the story is a bit more complex. If not in the main text, then at least in the supplement.</p>
<p>(4) Peak latency analysis - Estimating peaks per ppt is presumably noisy, so it seems important to show how reliable this is. One option is to find the bootstrapped mean latencies per subject.</p>
<p>(5) &quot;Due to our calculations being at the object level, if there were more than one of the same objects in an image, we cropped the most complete one to get a more accurate retinal size. &quot; Did EEG experimenters make sure everyone sat the same distance from the screen? and remain the same distance? This would also affect real-world depth measures.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98117.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper aims to test if neural representations of images of objects in the human brain contain a 'pure' dimension of real-world size that is independent of retinal size or perceived depth. To this end, they apply representational similarity analysis on EEG responses in 10 human subjects to a set of 200 images from a publicly available database (THINGS-EEG2), correlating pairwise distinctions in evoked activity between images with pairwise differences in human ratings of real-world size (from THINGS+). By partialling out correlations with metrics of retinal size and perceived depth from the resulting EEG correlation time courses, the paper claims to identify an independent representation of real-world size starting at 170 ms in the EEG signal. Further comparisons with artificial neural networks and language embeddings lead the authors to claim this correlation reflects a relatively 'high-level' and 'stable' neural representation.</p>
<p>Strengths:</p>
<p>- The paper features insightful figures/illustrations and clear figures.</p>
<p>- The limitations of prior work motivating the current study are clearly explained and seem reasonable (although the rationale for why using 'ecological' stimuli with backgrounds matters when studying real-world size could be made clearer; one could also argue the opposite, that to get a 'pure' representation of the real-world size of an 'object concept', one should actually show objects in isolation).</p>
<p>- The partial correlation analysis convincingly demonstrates how correlations between feature spaces can affect their correlations with EEG responses (and how taking into account these correlations can disentangle them better).</p>
<p>- The RSA analysis and associated statistical methods appear solid.</p>
<p>Weaknesses:</p>
<p>- The claim of methodological novelty is overblown. Comparing image metrics, behavioral measurements, and ANN activations against EEG using RSA is a commonly used approach to study neural object representations. The dataset size (200 test images from THINGS) is not particularly large, and neither is comparing pre-trained DNNs and language models, or using partial correlations.</p>
<p>- The claims also seem too broad given the fairly small set of RDMs that are used here (3 size metrics, 4 ANN layers, 1 Word2Vec RDM): there are many aspects of object processing not studied here, so it's not correct to say this study provides a 'detailed and clear characterization of the object processing process'.</p>
<p>- The paper lacks an analysis demonstrating the validity of the real-world depth measure, which is here computed from the other two metrics by simply dividing them. The rationale and logic of this metric is not clearly explained. Is it intended to reflect the hypothesized egocentric distance to the object in the image if the person had in fact been 'inside' the image? How do we know this is valid? It would be helpful if the authors provided a validation of this metric.</p>
<p>- Given that there is only 1 image/concept here, the factor of real-world size may be confounded with other things, such as semantic category (e.g. buildings vs. tools). While the comparison of the real-world size metric appears to be effectively disentangled from retinal size and (the author's metric of) depth here, there are still many other object properties that are likely correlated with real-world size and therefore will confound identifying a 'pure' representation of real-world size in EEG. This could be addressed by adding more hypothesis RDMs reflecting different aspects of the images that may correlate with real-world size.</p>
<p>- The choice of ANNs lacks a clear motivation. Why these two particular networks? Why pick only 2 somewhat arbitrary layers? If the goal is to identify more semantic representations using CLIP, the comparison between CLIP and vision-only ResNet should be done with models trained on the same training datasets (to exclude the effect of training dataset size &amp; quality; cf Wang et al., 2023). This is necessary to substantiate the claims on page 19 which attributed the differences between models in terms of their EEG correlations to one of them being a 'visual model' vs. 'visual-semantic model'.</p>
<p>- The first part of the claim on page 22 based on Figure 4 'The above results reveal that real-world size emerges with later peak neural latencies and in the later layers of ANNs, regardless of image background information' is not valid since no EEG results for images without backgrounds are shown (only ANNs).</p>
<p>Appraisal of claims:</p>
<p>While the method shows useful and interesting patterns of results can be obtained by combining contrasting behavioral/image metrics, the lack of additional control models makes the evidence for the claimed unconfounded representation of real-world size in EEG responses incomplete.</p>
<p>Discussion of likely impact:</p>
<p>The paper is likely to impact the field by showcasing how using partial correlations in RSA is useful, rather than providing conclusive evidence regarding neural representations of objects and their sizes.</p>
<p>Additional context important to consider when interpreting this work:</p>
<p>- Page 20, the authors point out similarities of peak correlations between models ('Interestingly, the peaks of significant time windows for the EEG × HYP RSA also correspond with the peaks of the EEG × ANN RSA timecourse (Figure 3D,F)'. Although not explicitly stated, this seems to imply that they infer from this that the ANN-EEG correlation might be driven by their representation of the hypothesized feature spaces. However this does not follow: in EEG-image metric model comparisons it is very typical to see multiple peaks, for any type of model, this simply reflects specific time points in EEG at which visual inputs (images) yield distinctive EEG amplitudes (perhaps due to stereotypical waves of neural processing?), but one cannot infer the information being processed is the same. To investigate this, one could for example conduct variance partitioning or commonality analysis to see if there is variance at these specific time-points that is shared by a specific combination of the hypothesis and ANN feature spaces.</p>
<p>- Page 22 mentions 'The significant time-window (90-300ms) of similarity between Word2Vec RDM and EEG RDMs (Figure 5B) contained the significant time-window of EEG x real-world size representational similarity (Figure 3B)'. This is not particularly meaningful given that the Word2Vec correlation is significant for the entire EEG epoch (from the time-point of the signal 'arriving' in visual cortex around ~90 ms) and is thus much less temporally specific than the real-world size EEG correlation. Again a stronger test of whether Word2Vec indeed captures neural representations of real-world size could be to identify EEG time-points at which there are unique Word2Vec correlations that are not explained by either ResNet or CLIP, and see if those time-points share variance with the real-world size hypothesized RDM.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98117.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors used an open EEG dataset of observers viewing real-world objects. Each object had a real-world size value (from human rankings), a retinal size value (measured from each image), and a scene depth value (inferred from the above). The authors combined the EEG and object measurements with extant, pre-trained models (a deep convolutional neural network, a multimodal ANN, and Word2vec) to assess the time course of processing object size (retinal and real-world) and depth. They found that depth was processed first, followed by retinal size, and then real-world size. The depth time course roughly corresponded to the visual ANNs, while the real-world size time course roughly corresponded to the more semantic models.</p>
<p>The time course result for the three object attributes is very clear and a novel contribution to the literature. However, the motivations for the ANNs could be better developed, the manuscript could better link to existing theories and literature, and the ANN analysis could be modernized. I have some suggestions for improving specific methods.</p>
<p>(1) Manuscript motivations</p>
<p>
The authors motivate the paper in several places by asking &quot; whether biological and artificial systems represent object real-world size&quot;. This seems odd for a couple of reasons. Firstly, the brain must represent real-world size somehow, given that we can reason about this question. Second, given the large behavioral and fMRI literature on the topic, combined with the growing ANN literature, this seems like a foregone conclusion and undermines the novelty of this contribution.</p>
<p>While the introduction further promises to &quot;also investigate possible mechanisms of object real-world size representations.&quot;, I was left wishing for more in this department. The authors report correlations between neural activity and object attributes, as well as between neural activity and ANNs. It would be nice to link the results to theories of object processing (e.g., a feedforward sweep, such as DiCarlo and colleagues have suggested, versus a reverse hierarchy, such as suggested by Hochstein, among others). What is semantic about real-world size, and where might this information come from? (Although you may have to expand beyond the posterior electrodes to do this analysis).</p>
<p>Finally, several places in the manuscript tout the &quot;novel computational approach&quot;. This seems odd because the computational framework and pipeline have been the most common approach in cognitive computational neuroscience in the past 5-10 years.</p>
<p>(2) Suggestion: modernize the approach</p>
<p>
I was surprised that the computational models used in this manuscript were all 8-10 years old. Specifically, because there are now deep nets that more explicitly model the human brain (e.g., Cornet) as well as more sophisticated models of semantics (e.g., LLMs), I was left hoping that the authors had used more state-of-the-art models in the work. Moreover, the use of a single dCNN, a single multi-modal model, and a single word embedding model makes it difficult to generalize about visual, multimodal, and semantic features in general.</p>
<p>(3) Methodological considerations</p>
<p>
a) Validity of the real-world size measurement</p>
<p>
I was concerned about a few aspects of the real-world size rankings. First, I am trying to understand why the scale goes from 100-519. This seems very arbitrary; please clarify. Second, are we to assume that this scale is linear? Is this appropriate when real-world object size is best expressed on a log scale? Third, the authors provide &quot;sand&quot; as an example of the smallest real-world object. This is tricky because sand is more &quot;stuff&quot; than &quot;thing&quot;, so I imagine it leaves observers wondering whether the experimenter intends a grain of sand or a sandy scene region. What is the variability in real-world size ratings? Might the variability also provide additional insights in this experiment?</p>
<p>
b) This work has no noise ceiling to establish how strong the model fits are, relative to the intrinsic noise of the data. I strongly suggest that these are included.</p>
</body>
</sub-article>
</article>