<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106103</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106103</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106103.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Semantic representations in the visual cortex of blind and sighted humans</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Paczyńska</surname>
<given-names>Małgorzata</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Urbaniak</surname>
<given-names>Marta</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dębecka</surname>
<given-names>Marta</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3774-8664</contrib-id>
<name>
<surname>Bola</surname>
<given-names>Łukasz</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>lbola@psych.pan.pl</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01dr6c206</institution-id><institution>Institute of Psychology, Polish Academy of Sciences</institution></institution-wrap>, <city>Warsaw</city>, <country country="PL">Poland</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0407f1r36</institution-id><institution>SWPS University</institution></institution-wrap>, <city>Warsaw</city>, <country country="PL">Poland</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01dr6c206</institution-id><institution>Graduate School for Social Research, Polish Academy of Sciences</institution></institution-wrap>, <city>Warsaw</city>, <country country="PL">Poland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-04-17">
<day>17</day>
<month>04</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106103</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-01-28">
<day>28</day>
<month>01</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-01-30">
<day>30</day>
<month>01</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.01.28.635293"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Paczyńska et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Paczyńska et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106103-v1.pdf"/>
<abstract>
<title>Abstract</title><p>In blind humans, the “visual” cortex responds to linguistic stimuli, such as words and sentences. This is sometimes taken as evidence that this brain region supports starkly different computations in blind and sighted individuals. Here, we challenge this view and show that, during word processing, the visual areas in these two populations represent the same semantic dimension – the knowledge about physical properties of word referents. Using analysis of fMRI activation patterns, we found that the visual cortex in both congenitally blind and sighted participants represented differences between individual words. In both groups, the activation patterns for words in the visual cortex reflected physical, but not conceptual similarity between word referents. Furthermore, the between-group correlations in these activation patterns were comparable to within-group correlations. Finally, during word processing, the visual areas in both groups showed greatest “representational connectivity” to the occipitotemporal areas. Overall, our findings suggest that responses to linguistic stimuli in the visual cortex of blind individuals are driven by representational mechanisms that are functional also in the sighted adult brain. In sighted individuals, information about physical properties of word referents might be backprojected to visual areas, from the occipitotemporal cortex, to support visual predictions, imagery, and visuospatial thinking. In blind individuals, this mechanism might be preserved and, combined with increased excitability of the blind visual cortex, drive strong responses of this region to linguistic stimuli.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Blindness</kwd>
<kwd>fMRI</kwd>
<kwd>language</kwd>
<kwd>MVPA</kwd>
<kwd>plasticity</kwd>
<kwd>RSA</kwd>
<kwd>semantic processing</kwd>
<kwd>visual cortex</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>

</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Language processing involves comparable brain regions across languages and cultures (<xref ref-type="bibr" rid="c38">Malik-Moraleda et al., 2022</xref>). These are primarily areas clustered around the left sylvian fissure, such as the inferior frontal gyrus, the superior temporal lobe, and the anterior temporal lobe (Fedorenko et al., 2024).</p>
<p>Intriguingly, in blind individuals, language processing activates not only the canonical language network, but also the “visual” cortex. (<xref ref-type="bibr" rid="c59">Sadato et al., 1996</xref>; <xref ref-type="bibr" rid="c13">Burton et al., 2002</xref>; <xref ref-type="bibr" rid="c57">Röder et al., 2002</xref>; <xref ref-type="bibr" rid="c6">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="c7">Bedny et al., 2015</xref>; <xref ref-type="bibr" rid="c28">Lane et al., 2015</xref>). The magnitude of these activations in the visual areas of blind individuals increases with increasing complexity of linguistic stimuli (<xref ref-type="bibr" rid="c57">Röder et al., 2002</xref>; <xref ref-type="bibr" rid="c6">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="c28">Lane et al., 2015</xref>), and is greater for semantic tasks than for phonological tasks (<xref ref-type="bibr" rid="c14">Burton et al., 2003</xref>). Transient disruption of the visual cortex activity in blind individuals interferes with certain operations involving linguistic stimuli, such as Braille reading (<xref ref-type="bibr" rid="c16">Cohen et al., 1997</xref>, <xref ref-type="bibr" rid="c27">Kupers et al., 2007</xref>) or word generation (<xref ref-type="bibr" rid="c3">Amedi et al., 2004</xref>).</p>
<p>An increasingly popular interpretation of these findings is that they signify a profound change in the implementation of cognitive functions in the brain, induced by the absence of visual experience (<xref ref-type="bibr" rid="c5">Bedny, 2017</xref>; <xref ref-type="bibr" rid="c58">Saccone et al., 2024</xref>). As a result of this process the visual cortex in blind humans might potentially support high-level cognitive functions, which are not supported by visual areas of the sighted.</p>
<p>However, an alternative possibility is that responses to linguistic stimuli in the blind visual cortex are driven by neural mechanisms that are functional also in the sighted adult brain. The sighted visual cortex receives information from the higher-level brain regions (<xref ref-type="bibr" rid="c56">Roelfsema and de Lange, 2016</xref>). Thus, signals from the language network can presumably modulate the activity of the visual cortex also in sighted individuals. Certain visual areas in the sighted are activated by spoken words and sentences (<xref ref-type="bibr" rid="c60">Seydell-Greenwald et al., 2023</xref>) and represent physical properties of word referents (<xref ref-type="bibr" rid="c12">Borghesani et al., 2016</xref>; <xref ref-type="bibr" rid="c40">Martin et al., 2018</xref>). At least some of these effects are unlikely to be driven by visual imagery (<xref ref-type="bibr" rid="c60">Seydell-Greenwald et al., 2023</xref>). Based on these results one may suppose that, following language comprehension, the knowledge about physical properties of word referents is backprojected to the visual system in an attempt to predict incoming visual signals (<xref ref-type="bibr" rid="c54">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="c31">Lee and Mumford. 2003</xref>; <xref ref-type="bibr" rid="c22">Friston, 2005</xref>). In this study, we hypothesized that this mechanism develops even in blind individuals and, in the absence of visual inputs, drives strong responses of the blind visual cortex to linguistic stimuli. The comparable representations of physical properties of word referents has been already documented in the occipitotemporal areas of blind and sighted individuals (<xref ref-type="bibr" rid="c36">Mahon et al., 2009</xref>; <xref ref-type="bibr" rid="c23">He et al., 2013</xref>; Peleen et al., 2013, 2014; <xref ref-type="bibr" rid="c65">Xu et al., 2023</xref>). Here, we asked whether such representations can drive responses to linguistic stimuli in the occipital lobe, particularly in the early visual areas, which has been suggested to be a hotspot of functional plasticity in the blind brain (<xref ref-type="bibr" rid="c2">Amedi et al., 2003</xref>).</p>
<p>To investigate this issue, we performed an fMRI study in which sighted and congenitally blind individuals were presented with 20 concrete words. The words were presented in either the spoken or the written modality (the visual alphabet in the sighted group and the Braille alphabet in the blind group), and the participants were asked to answer questions about either physical (e.g., is this round?) or conceptual (e.g., is this living?) properties of word referents (<xref ref-type="bibr" rid="c40">Martin et al., 2018</xref>; <xref ref-type="bibr" rid="c65">Xu et al., 2023</xref>). Furthermore, in a separate behavioral study, we collected the ratings of physical and conceptual similarity between word referents for all words used in the fMRI study.</p>
<p>We used the analysis of fMRI activation patterns to investigate neural representations induced in the visual areas of blind and sighted participants. If responses to linguistic stimuli in the blind visual cortex are driven by a change in cognitive functions implemented in this region, then the visual areas in both groups should represent different word properties. One can particularly expect that the visual cortex in the blind, but not in the sighted individuals, represents abstract, conceptual aspects of word meaning. Conversely, if the responses to linguistic stimuli in the blind visual cortex are driven by mechanisms that are present also in the sighted adult brain, one should find the same type of representations in the visual areas in both groups. Based on our hypothesis, this should be primarily the representation of the physical properties of word referents, which, in sighted individuals, can be particularly useful for visual predictions.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Activation patterns in the visual cortex</title>
<p>We first asked whether the visual areas in blind and sighted individuals represent differences between specific concrete words. To address this question, we tried to decode individual words used in the study from the activation patterns of three occipital Brodmann areas (BAs): area 17 (primary visual cortex), area 18 (secondary visual cortex), and area 19 (associative visual cortex) (<xref rid="fig1" ref-type="fig">Figs. 1</xref>-<xref rid="fig2" ref-type="fig">2</xref>). To attenuate the impact of word sensory properties on the results, the decoding was performed across the modalities of word presentation, that is, the algorithm was always trained on one sensory modality and tested on the other. Generalization across the presentation modalities is one of the hallmarks of semantic representations in the higher-level brain regions (<xref ref-type="bibr" rid="c34">Liuzzi et al., 2017</xref>; <xref ref-type="bibr" rid="c18">Deniz et al., 2019</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>The masks of visual areas used in the study.</title>
<p>The masks of Brodmann areas 17 (primary visual cortex, red), 18 (secondary visual cortex, green), and 19 (associative visual cortex, blue) are presented on the inflated reconstruction of participants’ averaged cortical anatomy.</p></caption>
<graphic xlink:href="635293v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>The visual cortex in blind and sighted individuals represents differences between individual concrete words.</title>
<p>The results of cross-modal classification of activation patterns for individual words, presented in the spoken and the written modality, in the visual areas of blind and sighted participants. * p &lt; 0.05, ** p &lt; 0.01, corrected for multiple comparisons using the false discovery rate. Error bars represent the standard error of the mean. The black line indicates the chance classification level.</p></caption>
<graphic xlink:href="635293v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In both blind and sighted participants, the accuracy of crossmodal word decoding was significantly above chance level in the visual cortex (BAs 17, 18, and 19 combined; both p values &lt; 0.01) and in all specific visual areas (all p values &lt; 0.05). The 2 (group) x 3 (visual area) ANOVA produced only a significant main effect of the visual area (F(2,90) = 13.21, p &lt; 0.001, partial eta squared = 0.23), with post-hoc tests indicating that the word decoding was more accurate in BA 19 than in the other two visual areas (both p values &lt; 0.01). Neither the main effect of the group nor the interaction between the group and the visual area were significant (both p &gt; 0.15). These results show that the visual cortex in both blind and sighted individuals represents differences between individual concrete words in a format that is independent of word presentation modality.</p>
<p>We then averaged the activity patterns for each word across the presentation modalities, and correlated similarity in these average activity patterns in the visual cortex with the behavioral ratings of physical and conceptual similarity between the word referents (<xref rid="fig3" ref-type="fig">Fig. 3</xref>; see <xref rid="figS1" ref-type="fig">Fig. S1</xref> for the visualization of the behavioral similarity matrices). Since the two types of ratings were correlated (Spearman’s rho = 0.41, p &lt; 0.001), we used a partial correlation approach, in which the results were calculated after accounting for this common variance.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>The activation patterns in the visual cortex of blind and sighted individuals reflect physical, but not conceptual similarity between word referents.</title>
<p>Correlations between the neural similarity matrices, reflecting similarity between the activation patterns for individuals words in the visual areas of blind and sighted participants, and the behavioral ratings of physical and conceptual similarity between the word referents. * p &lt; 0.05, ** p &lt; 0.01, corrected for multiple comparisons using the false discovery rate. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In both blind and sighted participants, the similarity in activity patterns for words in the visual cortex (BAs 17, 18, and 19 combined) was correlated with ratings of physical similarity of word referents (both p values &lt; 0.01), but not with ratings of more abstract, conceptual similarity (both p values &gt; 0.25). Significant correlations with ratings of physical similarity were found in all visual areas in the blind participants (all p values &lt; 0.01) and in BAs 18 and 19 in the sighted participants (both p values &lt; 0.05). In contrast, correlations with ratings of conceptual similarity were not observed in any visual area in either group (all p values &gt; 0.25). In line with these results, the 2 (group) x 2 (similarity metric) x 3 (visual area) ANOVA produced a significant main effect of similarity metric (F(1,45) = 7.23, p = 0.01, partial eta squared = 0.14), which did not interact with the group or the visual area (both p values &gt; 0.12). Additionally, the ANOVA also indicated a significant group by visual area interaction (F(2,90) = 6.79, p = 0.005, partial eta squared = 0.13), with post-hoc tests indicating that the correlation values were generally higher in BA 19 than in the other visual areas in the sighted participants (both p values &lt; 0.01), but not in the blind participants (both p values &gt; 0.25). No other main effects or interactions were significant. In summary, our results show that, in both blind and sighted individuals, the activity patterns for words in the visual cortex reflect the physical similarity of word referents, but not the more abstract, conceptual similarity.</p>
<p>While the three-way ANOVA interaction (group x similarity metric x visual area) on correlation scores was not significant (F &lt; 1, p &gt; 0.25), the activity patterns in BA 17 correlated with the ratings of physical similarity of word referents only in the blind group. We performed additional pairwise comparisons to further investigate this effect. The direct between-group comparisons confirmed greater correlation between the activity patterns in BA 17 and the ratings of physical similarity of word referents in the blind participants, compared to the sighted participants (t(45) = 2.28, p = 0.028, Cohen’s d = 0.67). No between-group differences were observed in other visual areas (both p &gt; 0.05). The within-group comparisons showed that, in the sighted participants, the correlation with the ratings of physical similarity of word referents was greater in BA 19 than in BA 17 (t(26) = 2.62, p = 0.045, Cohen’s d = 0.5), with no significant effects in other comparisons (both p values &gt; 0.1). In the blind participants, no differences across the visual areas were found (all p values &gt; 0.25). In line with the results of the main analysis, none of the above-described tests produced significant effects when the correlations with the ratings of conceptual similarity of word referents were considered (all p values &gt; 0.15). This further suggests that activity patterns in the visual areas do not reflect the conceptual similarity between word referents in either sighted or blind individuals.</p>
<p>As a control analysis, we searched for the representation of more abstract, conceptual similarity between word referents in the visual areas in the left hemisphere only, instead of using bilateral masks. While the responses to linguistic stimuli in the blind brain are less lateralized than in the sighted brain (<xref ref-type="bibr" rid="c29">Lane et al., 2017</xref>; <xref ref-type="bibr" rid="c19">Dzięgiel-Fivet and Jednoróg, 2024</xref>), one can perhaps still argue that the representational plasticity for language in blind individuals should be more prominent in the left hemisphere. Contrary to this argument, our analysis in the left hemisphere produced results that are virtually identical to those described above (<xref rid="figS2" ref-type="fig">Fig. S2</xref>).</p>
<p>Furthermore, we asked whether conceptual similarity between word referents is computed in the left anterior temporal lobe (ATL), which is involved in the processing of abstract knowledge in both sighted and blind individuals (<xref ref-type="bibr" rid="c62">Striem-Amit et al., 2018</xref>; <xref ref-type="bibr" rid="c64">Wang et al., 2020</xref>). Indeed, we observed significant correlations between similarity in activity patterns for words in this region and the ratings of conceptual similarity of word referents in both groups (both p values &lt; 0.05) (<xref rid="figS3" ref-type="fig">Fig. S3</xref>). The 2 (group) x 2 (similarity metric) x 2 (brain area) ANOVA, which compared the correlation scores for the visual cortex and the ATL, indicated a significant interaction between the similarity metric and the brain area (F(1,45) = 7.85, p = 0.007, partial eta squared = 0.15). No other main effects or interactions were significant. The post-hoc tests showed that the correlation with conceptual similarity ratings were higher in the ATL than in the visual cortex (p = 0.038), whereas the correlation with physical similarity ratings were higher in the visual cortex than in the ATL (p = 0.008). Furthermore, the correlation with the physical similarity ratings was significantly higher than the correlation with the conceptual similarity ratings in the visual cortex (p = 0.006), but not in the ATL (p &gt; 0.25).</p>
<p>Finally, we directly compared the neural similarity matrices reflecting the similarity in activity patterns for specific words in the visual areas across the blind and the sighted participants (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). To this aim, we correlated such a neural similarity matrix of each participant with the analogous neural similarity matrix of every participant from the other group (sighted-blind, blind-sighted). The obtained correlation scores were then compared to the within-group correlations (sighted-sighted, blind-blind) produced in the analogous way.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Between-group similarity in activity patterns for words in the visual areas of blind and sighted participants is comparable to within-group similarities.</title>
<p>The between-group similarity scores were calculated by correlating the neural similarity matrices of each participant, reflecting similarity between the activity patterns for specific words in the visual areas, with the neural similarity matrices of every participant in the other group (blind-sighted, sighted-blind). The within-group similarity scores were calculated by correlating the neural similarity matrices of each participant with the neural similarity matrices of every other participant from a given group (blind-blind, sighted-sighted). ** p &lt; 0.01, corrected for multiple comparisons using the false discovery rate. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>If word processing induces different types of neural representations in the visual areas of sighted and blind individuals, then the between-group correlations should be either non-significant or markedly lower than the within-group correlations. However, we observed robust between-group correlations in the visual cortex (BAs 17, 18, and 19 combined; p &lt; 0.01) and in all specific visual areas (all p values &lt; 0.01). Moreover, we did not find significant differences in the between-group and the within-group correlation scores in any of the visual regions (all p values &gt; 0.1). The 2 (type of correlation) x 3 (visual area) ANOVA produced only a significant main effect of the visual area (F(2,92) = 25.41, p &lt; 0.001, partial eta squared = 0.37), indicating that both types of correlations linearly increased from BA 17 to BA 19 (all p values &lt; 0.001, linear contrast: p &lt; 0.001). Neither the main effect of the type of correlation nor the interaction were significant (both p values &gt; 0.15). These results further support our hypothesis and suggest that representations activated by words in the visual areas of blind and sighted individuals are related.</p>
</sec>
<sec id="s2b">
<title>Representational connectivity of the visual cortex</title>
<p>Next, we investigated the “representational connectivity” (<xref ref-type="bibr" rid="c26">Kriegeskorte et al., 2008</xref>) of the visual cortex in blind and sighted participants during word processing (<xref rid="fig5" ref-type="fig">Fig. 5</xref>). That is, we searched for brain regions in which words induce similar activity patterns to those induced in the visual cortex. The aim of this analysis was to investigate whether the position of the visual cortex in the cortical processing hierarchy during word processing is comparable in blind and sighted individuals. To study this issue, we correlated the neural similarity matrices reflecting the similarity in activity patterns for specific words in the visual cortex with the analogous neural similarity matrices obtained for every other Brodmann area found in the BrainVoyager cortical atlas (33 target areas in total; see Methods). To account for effects that are present across the whole cortex, we calculated the average correlation between the visual cortex and the target areas and subtracted this score from the results. This allowed us to identify brain regions in which the word-induced representations are more or less similar than the cortical average to the representations computed in the visual cortex.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>During word processing, the visual cortex in both blind and sighted participants shows the greatest “representational connectivity” to the occipitotemporal cortex.</title>
<p>For each brain region, the representational connectivity to the visual cortex was calculated by correlating neural similarity matrices, reflecting similarity between the activity patterns for specific words, obtained for the visual cortex and this region. In each participant group, the average correlation between the visual cortex and the target brain regions was subtracted from the results, to visualize regions that show higher-than-average and lower-than-average representational connectivity to the visual cortex during the word processing. The results for such regions are marked in blue (comparison against average correlation, p &lt; 0.05, corrected for multiple comparisons using the false discovery rate). The target brain regions were sorted by the obtained representational connectivity scores. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In both participant groups, the visual cortex showed the greatest representational connectivity to the occipitotemporal cortex (BA 37). The representational connectivity between these two adjacent regions was higher than the representational connectivity between the visual cortex and its every other adjacent region in the blind participants (BAs 7, 30, 31, and 39; all p values &lt; 0.002), and every other adjacent region, except for BA 7, in the sighted participant (all p values &lt; 0.001). Thus, the preferential representational link between the visual cortex and the occipitotemporal cortex cannot be explained merely by the topographical proximity of these two regions. Conversely, the visual areas in both groups showed lower-than-average representational connectivity to primary auditory and somatosensory cortices (BAs 41-42 and BAs 1-3, respectively). Finally, the representational connectivity scores obtained in both participant groups were highly correlated (Spearman’s rho = 0.91, p &lt; 0.001), which further suggests a similar position of the visual cortex in the cortical processing hierarchy for word processing in these two groups. Virtually the same results were observed when only the lower-level visual areas (BA 17 or BA 18) were included in the analysis (<xref rid="figS4" ref-type="fig">Figs. S4</xref>-<xref rid="figS5" ref-type="fig">5</xref>).</p>
</sec>
<sec id="s2c">
<title>Searchlight analysis</title>
<p>We performed a searchlight analysis to investigate what brain regions, beyond the visual cortex, are involved in representing physical and conceptual similarity between word referents (<xref rid="fig6" ref-type="fig">Fig. 6</xref>). First, we ran the analysis with participants from both groups combined in one sample, to achieve the greatest statistical power to reveal the common effects (the results for each group separately are presented in <xref rid="figS6" ref-type="fig">Fig. S6</xref>). Second, we compared the results across groups to visualize differential effects. As in the analysis in the visual areas, the partial correlation approach was used to account for the common variance in the ratings of physical and conceptual similarity.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Searchlight analysis.</title>
<p>(A-B) The results of correlation between the neural similarity matrices, reflecting similarity between the activity patterns for specific words, and the behavioral ratings of (A) physical and (B) conceptual similarity between the word referents. (C) Brain regions in which the correlations with physical similarity ratings were significantly higher than the correlations with conceptual similarity ratings. The white arrow points to the effect in the occipitotemporal cortex. The analyses combined blind and sighted participants in one sample, to achieve the greatest statistical power to detect common effects. Between-group comparisons did not produce significant results. In all analyses, The statistical significance was determined using threshold-free cluster enhancement (TFCE) maps and Monte Carlo simulation. The statistical threshold was set at p &lt; 0.05, corrected for multiple comparisons. The T-values for the effects that survived the threshold are presented for the visualization purposes.</p></caption>
<graphic xlink:href="635293v1_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>When all participants were combined in one sample, the ratings of physical similarity of word referents correlated with similarity in activity patterns for words in the canonical language and semantic regions, such as the frontal cortex, the anterior temporal lobe, and the precuneus (<xref rid="fig6" ref-type="fig">Fig 6A</xref>). Furthermore, significant effects were found throughout the visual cortex and in the lateral and ventral occipitotemporal regions. The analogous analysis including the ratings of conceptual similarity of word referents produced more constrained, but robust effects in the left anterior temporal lobe, the superior temporal sulci, the left middle temporal gyrus, the left frontal operculum, and the left insula (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>). No significant effects were detected in the visual areas. The direct comparison between the results of these two analyses showed significantly stronger correlation with the physical similarity ratings, compared to the conceptual similarity ratings, in the left ventral occipitotemporal cortex, at the border of BAs 19 and 37 (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>). No differences between groups and no interaction between the group and the similarity metric were observed, even at the level of statistical trend (tested at p &lt; 0.1, corrected for multiple comparisons).</p>
</sec>
<sec id="s2d">
<title>Univariate analysis</title>
<p>We also performed the univariate analysis, to investigate the magnitude of activations for spoken and written words in the visual areas in both groups (<xref rid="fig7" ref-type="fig">Fig. 7</xref>). We particularly asked whether the processing of spoken and Braille words activated the visual cortex in the blind participants, as was reported previously.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Univariate analysis of activations in the visual cortex.</title>
<p>The average activations for spoken and written words, compared to rest periods, in the visual areas of blind and sighted participants. * p &lt; 0.05, ** p &lt; 0.01, corrected for multiple comparisons using the false discovery rate. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Indeed, we found that, compared to rest periods, both spoken and written (Braille) words robustly activated the visual cortex and all specific visual areas in the blind participants (all p values &lt; 0.05) (<xref rid="fig7" ref-type="fig">Fig. 7A</xref>). In the sighted participants, all visual areas were activated by written (visual) words (all p values &lt; 0.01). The spoken words activated the primary visual cortex (BA 17; p = 0.002), but not the higher-level visual areas (p &gt; 0.25), in this group (see also: <xref ref-type="bibr" rid="c60">Seydell-Greenwald et al., 2023</xref>). The 2 (group) x 2 (word presentation modality) x 3 (visual area) ANOVA produced a significant three-way interaction (F(2,90) = 15.73, p &lt; 0.001, partial eta squared = 0.26). The comparison across the groups confirmed that, in all visual areas tested, activations for spoken and written (Braille) words in the blind participants were stronger than the activations for spoken words in the sighted participants (all p values &lt; 0.05). The comparisons across the modalities showed that, in the blind participants, spoken and written word presentations induced comparable activations in all visual areas (all p values &gt; 0.25). As could be expected, in the sighted participants, activations for written (visual) words were stronger than the activations for spoken words (all p values &lt; 0.05). Finally, the comparison across the visual areas revealed that, in the blind group, activations for both spoken and written words were stronger in lower-level visual areas (BAs 17 and 18) than in the higher-level visual areas (BA 19) (all p values &lt; 0.059). In the sighted participants, the activation in the primary visual cortex (BA 17) was stronger than the activation in the higher-level visual areas (BAs 18 and 19), again for both modalities of word presentation (all p values &lt; 0.001).</p>
<p>The whole-brain analysis (<xref rid="figS7" ref-type="fig">Fig. S7</xref>-<xref rid="figS8" ref-type="fig">S8</xref>) confirmed the pattern of results described above. Additionally, this analysis revealed that, in both groups, both modalities of word presentation induced expected activations in the canonical language network. Apart from decreased activation for Braille words in the cuneus and the precuneus in the blind groups, we did not find any significant differences between groups or the modalities of word presentation in the canonical language or semantic regions.</p>
</sec>
<sec id="s2e">
<title>Influence of word presentation modality on the results</title>
<p>Previous studies with blind participants reported the representation of physical properties of word referents in the occipitotemporal cortex (<xref ref-type="bibr" rid="c36">Mahon et al., 2009</xref>; <xref ref-type="bibr" rid="c23">He et al., 2013</xref>; Peleen et al., 2013, 2014; <xref ref-type="bibr" rid="c65">Xu et al., 2023</xref>). Why did these studies not find the effects in the occipital lobe, which we robustly detected in our study? One explanation is that these studies used only the spoken word presentation. The activity of the visual areas is modulated by auditory stimulation (<xref ref-type="bibr" rid="c30">Laurienti et al., 2002</xref>; <xref ref-type="bibr" rid="c25">Iurilli et al., 2012</xref>; <xref ref-type="bibr" rid="c47">Murray et al., 2016</xref>; <xref ref-type="bibr" rid="c4">Anurova et al., 2019</xref>; <xref ref-type="bibr" rid="c20">Ferraro et al., 2020</xref>). Sounds with different acoustic properties induce different activity patterns in the low-level visual areas (<xref ref-type="bibr" rid="c41">Martinelli et al., 2020</xref>). These modulations of activity in the visual cortex, by the auditory cortex, could make effects related to the processing of word meaning less discernible. In line with this hypothesis, studies that found the representation of physical properties of word referents in the occipital lobe of sighted individuals used visual rather than auditory word presentation (<xref ref-type="bibr" rid="c12">Borghesani et al., 2016</xref>; <xref ref-type="bibr" rid="c40">Martin et al., 2018</xref>).</p>
<p>To explore this possibility in our data, we divided them into runs with spoken word presentation and written word presentation. We then compared the correlations between the similarity in activity patterns for words and the ratings of physical and conceptual similarity of word referents in these two datasets (<xref rid="figS9" ref-type="fig">Fig. S9</xref>). Indeed, the results for written words were stronger in both groups. The 2 (group) x 2 (similarity metric) x 3 (visual area) x 2 (modality of word presentation) ANOVA produced a significant main effect of modality of word presentation (F(1,45) = 5.21, p = 0.027, partial eta squared = 0.1), indicating that the correlation scores in the visual cortex were generally higher for written words. Furthermore, we also found the main effect of similarity metric (F(1,45) = 4.3, p = 0.044, partial eta squared = 0.09), indicating that our key finding - significant correlation of the activity patterns for words in the visual cortex with physical, but not conceptual similarity between the word referents in both groups - replicated also in this additional analysis.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study, we found that visual areas of both congenitally blind and sighted participants represented differences between individual concrete words in a format independent of the word presentation modality. We further observed that, in both participant groups, the activation patterns for words in the visual cortex reflected physical, but not more abstract, conceptual similarity between word referents. Subsequently, we directly compared activation patterns induced by words in the visual areas of blind and sighted participants. We found that the between-group correlations in these activity patterns were comparable to the within-group correlations. Finally, we demonstrated that, during word processing, the visual cortex in both blind and sighted participants showed the greatest “representational connectivity” to the same cortical region, the occipitotemporal cortex. All these between-group similarities were observed despite the fact that, as in previous studies, the processing of spoken and Braille words by the blind participants induced markedly stronger responses in the visual cortex than the processing of spoken words by the sighted participants.</p>
<p>Our study shows that, during word processing, the activation patterns in visual areas of congenitally blind and sighted individuals are related and reflect a specific semantic dimension - the physical similarity between word referents. Previous studies have repeatedly demonstrated that processing of linguistic stimuli, such as words and sentences, strongly activates the visual areas in blind individuals, in the absence of comparable results in sighted participants (<xref ref-type="bibr" rid="c59">Sadato et al., 1996</xref>; <xref ref-type="bibr" rid="c13">Burton et al., 2002</xref>; <xref ref-type="bibr" rid="c6">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="c7">Bedny et al., 2015</xref>; <xref ref-type="bibr" rid="c28">Lane et al., 2015</xref>). An increasingly popular interpretation of these findings is that, in the absence of visual inputs, the blind visual cortex assumes higher cognitive functions, such as processing of abstract semantic or grammatical information, which are not processed by the visual areas of the sighted (<xref ref-type="bibr" rid="c5">Bedny, 2017</xref>; <xref ref-type="bibr" rid="c58">Saccone et al., 2024</xref>). Our results suggest a more parsimonious explanation: responses to linguistic stimuli in the blind visual cortex can be driven by representational mechanisms present also in the visual areas of sighted adults. These mechanisms might produce stronger activations in the blind visual cortex because, in the absence of visual inputs, this region develops weaker inhibitory pathways (<xref ref-type="bibr" rid="c8">Benevento et al., 1995</xref>; <xref ref-type="bibr" rid="c45">Morales et al., 2002</xref>) and supranormal sensitivity to stimulation (<xref ref-type="bibr" rid="c42">Merabet et al., 2008</xref>; <xref ref-type="bibr" rid="c11">Binda et al., 2018</xref>). In other words, our results suggest that linguistic stimuli, such as words, activate comparable neural representations in the visual areas of blind and sighted individuals, but this process results in stronger activation of the blind visual cortex due to its enhanced excitability.</p>
<p>Consequently, our findings suggest that responses to language in the blind visual cortex can be driven by representational mechanisms that support visual perception in the sighted brain. In the visual areas of sighted individuals, the processing of visual feedforward signals is modulated by predictions generated in the higher-level brain regions (<xref ref-type="bibr" rid="c54">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="c31">Lee and Mumford. 2003</xref>; <xref ref-type="bibr" rid="c22">Friston, 2005</xref>). Our results suggest that such modulatory and potentially predictive signals can be generated even based on symbolic stimuli, such as words (see also: <xref ref-type="bibr" rid="c60">Seydell-Greenwald et al., 2023</xref>). Based on our results, one might hypothesize that language comprehension activates a top-down informational cascade, in which the knowledge about word referents is first backprojected from the canonical language regions to richly multimodal shape- and size-processing areas in the occipitotemporal cortex (<xref ref-type="bibr" rid="c55">Ricciardi et al., 2014</xref>; <xref ref-type="bibr" rid="c10">Bi et al., 2016</xref>). These areas, in turn, might communicate the physical properties of word referents, most useful for visual predictions, to the occipital lobe. Our findings suggest that such a neural architecture of backprojections can develop and function even in the absence of visual experience.</p>
<p>Previous studies documented the representation of the physical properties of word referents in the visual areas of sighted individuals (<xref ref-type="bibr" rid="c12">Borghesani et al., 2016</xref>; <xref ref-type="bibr" rid="c40">Martin et al., 2018</xref>). In blind individuals, such representations were found primarily in the occipitotemporal cortex (<xref ref-type="bibr" rid="c36">Mahon et al., 2009</xref>; <xref ref-type="bibr" rid="c23">He et al., 2013</xref>; Peleen et al., 2013, 2014; <xref ref-type="bibr" rid="c65">Xu et al., 2023</xref>), with one recent study showing that, in the lateral occipitotemporal cortex, the representation of physical knowledge is not accompanied by representations of more abstract properties in either blind or sighted individuals (<xref ref-type="bibr" rid="c65">Xu et al., 2023</xref>). Here, we document the representation of physical, but not abstract properties of word referents throughout the visual cortex of blind individuals. Interestingly, in sighted participants, correlations with the ratings of physical similarity of word referents were significant in the associative and secondary visual areas, but not in the primary visual cortex (BA 17). In line with this result, the correlation between the activations patterns in the primary visual cortex and the ratings of physical similarity of word referents was significantly lower in the sighted group than in the blind group. This between-group difference was observed even though the other analyses showed that the representation of differences between words in the primary visual cortex is of comparable precision in both groups (<xref rid="fig2" ref-type="fig">Fig. 2</xref>), and that activation patterns induced by words in this area are related across groups (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). One explanation of these results is that the behavioral ratings of physical similarity might predominantly capture the global physical properties of word referents, such as overall shape or size. One can speculate that, in sighted individuals, the primary visual cortex is better suited to represent lower-level features, such as spatial frequencies, contrasts, or orientations (<xref ref-type="bibr" rid="c46">Morgan et al., 2019</xref>). In blind macaques, the projections from high-level ventral visual areas to the primary visual cortex are strengthened, and the average length of intrinsic connectivity in parts of the primary visual cortex is increased (<xref ref-type="bibr" rid="c35">Magrou et al., 2018</xref>). Potentially, similar anatomical plasticity might support representations of more global physical properties in the primary visual cortex of blind humans. Such representation might still be related to lower-level representations in the sighted early visual areas, but, at the same time, might be better described by the behavioral ratings of physical similarity. Overall, our study shows that word processing activates related representations of physical knowledge in the visual cortex of both congenitally blind and sighted individuals. This, however, does not preclude a certain degree of plastic changes in these representations in the blind visual cortex.</p>
<p>The semantic mechanism reported in our study can explain a number of effects found in the blind visual cortex for linguistic stimuli. The gradual increase in activation magnitude for pseudowords, words, and sentences (<xref ref-type="bibr" rid="c6">Bedny et al., 2011</xref>) can be explained by progressively richer information about the physical environment conveyed by these stimuli (compare “an allb” vs. “a ball” vs. “Mary threw a ball to the dog”). Decoding of word semantic categories (<xref ref-type="bibr" rid="c1">Abboud et al., 2019</xref>) might rely on systematically different physical properties of word referents across these categories. Stronger activation of the visual cortex during a semantic task, compared to a phonological task (<xref ref-type="bibr" rid="c14">Burton et al., 2003</xref>), might indicate increased salience of the physical representations of word referents when one thinks about word meaning. Arguably, however, the proposed mechanism is unlikely to be the only source of language-related activations in the visual areas of blind individuals. Importantly, our study directly compared the activation patterns induced by words in the visual areas of sighted and blind individuals (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). This analysis was unconstrained by any theoretical model, therefore, it should capture the influence of every major input to the visual cortex on the representations that emerge in this region during word processing. The results showed that these representations, computed in the visual cortex of sighted and blind individuals, are related. This further suggests that, in both populations, the emergence of these representations is driven by similar mechanisms.</p>
<p>Our findings contribute to a better understanding of principles of functional plasticity in the human brain. One way to think about activations for language in the blind visual cortex is that, in the absence of visual signals, this region takes on new cognitive functions, which are radically different from those computed in the sighted visual cortex (Bedny et al., 2017; <xref ref-type="bibr" rid="c58">Saccone et al., 2024</xref>). Our study provides a different perspective and suggests that responses to language in the blind visual cortex can be driven by representational mechanisms that are present in the sighted adult brain and are uncovered in the absence of visual inputs (<xref ref-type="bibr" rid="c50">Pascual-Leone and Hamilton, 2001</xref>; <xref ref-type="bibr" rid="c37">Makin and Krakauer, 2023</xref>). We have already made that argument in our recent work, in which we demonstrated that the motion-sensitive area V5/MT in blind individuals shows different activity patterns for concrete nouns and verbs, in the absence of significant results for abstract and pseudo nouns and verbs (<xref ref-type="bibr" rid="c63">Urbaniak et al., 2024</xref>). Based on these results, we suggested that this region represents motion connotations of noun and verb referents, more salient in the concrete word category, rather than more abstract grammatical or conceptual distinctions, present in all word categories. This suggests that, in blind individuals, this visual region might represent linguistic stimuli through physical features of word referents, retrieved from semantic representations. Here, we show that this principle may apply throughout the visual cortex of blind individuals, even in the early visual areas, which have been proposed to be a hotspot of functional plasticity in the blind brain (<xref ref-type="bibr" rid="c2">Amedi et al., 2003</xref>). Moreover, we directly demonstrated that processing of words induces the same type of representations in the blind and the sighted visual cortex, a result that validates a critical prediction of our hypothesis.</p>
<p>Furthermore, our results provide insight into mechanisms that underpin top-down modulations of the activity in the visual cortex. The nature of such top-down modulatory signals - particularly their relationship to visual imagery - is still debated. Our study documents the emergence of similar representations of word referents in visual areas of sighted individuals and in visual areas of congenitally blind individuals, who could not develop visual imagery. Previous studies have already shown compelling parallels in the functional organization of the occipital lobe in sighted and blind individuals (<xref ref-type="bibr" rid="c50">Pascual-Leone and Hamilton, 2001</xref>; <xref ref-type="bibr" rid="c55">Ricciardi et al., 2014</xref>; <xref ref-type="bibr" rid="c24">Heimler et al., 2015</xref>). These studies, however, mostly presented blind participants with sounds or shapes, which makes the effects related to lateral projections from the other sensory systems and the backprojections from the higher-level brain regions difficult to disentangle. Our work, in contrast, documents the emergence of semantic representation in the visual areas of both populations, a process that cannot be explained by lateral interactions between sensory systems. This finding strongly suggests that the functional modulation of the visual cortex by backprojections from the higher-level regions is a process that is independent of visual imagery (see also: <xref ref-type="bibr" rid="c60">Seydell-Greenwald et al., 2023</xref>). An intriguing hypothesis is that the functional modulations of the visual cortex, akin to those observed in our study, constitute a mechanism that is evolutionarily older than the conscious, internally generated visual imagery. One can suppose that backprojections to the visual cortex, which originally emerged in the brain to support visual perception, were then utilized for more general visuospatial processing, such as visual imagery or visual working memory, ultimately becoming the brain’s “spatial blackboard” (<xref ref-type="bibr" rid="c56">Roelfsema and de Lange, 2016</xref>; <xref ref-type="bibr" rid="c33">Linton, 2021</xref>). It remains to be tested whether this architecture can causally support spatial thinking or spatial memory in blind individuals.</p>
<p>The idea that functional plasticity in the visual cortex of blind individuals “unmasks” processes and computations present also in the sighted visual cortex has already received considerable support from studies of auditory and tactile processing (<xref ref-type="bibr" rid="c50">Pascual-Leone and Hamilton, 2001</xref>; <xref ref-type="bibr" rid="c24">Heimler et al., 2015</xref>). However, the discovery of activations for words and sentences in the blind visual cortex has been sometimes treated as prime evidence against this hypothesis (Bedny et al., 2017). Our study counters this argument and suggests that enhanced activations of the blind visual cortex during auditory, tactile, and semantic processing may be driven by similar mechanisms.</p>
<p>In summary, we found that, during word processing, the activation patterns in visual areas of congenitally blind and sighted individuals are related and reflect a specific semantic dimension - the physical similarity between word referents. This shows that responses to linguistic stimuli in the blind visual cortex can be driven by representational mechanisms that are present in the sighted adult brain and are uncovered in the absence of visual inputs. Our study suggests that the type of neural representations computed in the visual cortex cannot be easily changed, even by dramatic changes in visual experience.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>Twenty congenitally blind subjects (9 males, 11 females, mean age = 36.4 y, SD = 7.61 y, average length of education = 15.2 y, range = 12 - 17 y) and 27 sighted subjects (12 males, 15 females, mean age = 35.7 y, SD = 8.6 y, average length of education = 15.56 y, range = 12 - 17 y) participated in the study. Two participants in the blind group and three in the sighted group were left-handed, while the remaining participants were right-handed. The blind and the sighted groups were matched for age, gender, handedness, and length of education (Mann-Whitney and Chi-square tests, all p values &gt; 0.25). In the blind group, blindness had a variety of causes, including the retinopathy of prematurity, retinal detachment, atrophy of the optic nerve, or unknown causes. Nine blind participants reported to have some light perception, but no object or contour vision. Additionally, two blind participants reported to have some form of contour vision, which, however, was not precise enough to be functional. All blind participants were proficient Braille readers. All blind and sighted participants were native Polish speakers, had normal hearing, and had no history of neurological disorders, apart from the cause of blindness in the blind group. All participants had no contraindications to the MRI, gave written informed consent and were paid for participation. The study was approved by the ethics committee of Institute of Psychology, Polish Academy of Sciences.</p>
</sec>
<sec id="s4b">
<title>Stimuli</title>
<p>Twenty Polish nouns referring to concrete entities (e.g., “a cup” or “a snake”; see <xref rid="figS1" ref-type="fig">Fig. S1</xref> for a complete list) were used as stimuli. The words had from 3 to 5 letters. The length restriction was introduced because the Braille display used in the fMRI study (see below) can display up to 5 letters at the same time. The words’ frequency of occurrence in Polish language was moderate to high (range of Zipf frequency values = 2.99 - 4.76, as indicated by the Subtlex-pl database; <xref ref-type="bibr" rid="c39">Mandera et al., 2015</xref>). The word list was inspired by the words used by <xref ref-type="bibr" rid="c40">Martin et al. (2018)</xref>. Their list was shortened and adjusted to meet requirements listed above.</p>
</sec>
<sec id="s4c">
<title>fMRI experiment</title>
<p>In the fMRI experiment, the participants were asked to either listen to or read the words and answer the questions about either physical or conceptual properties of word referents. In half of the fMRI experiment, the words were presented in the spoken modality, whereas, in the other half, in the written modality (in the visual alphabet in the sighted group and in the Braille alphabet in the blind group). The modality of word presentation was alternated across fMRI runs in an odd / even scheme. There were 12 runs (6 spoken and 6 written runs).</p>
<p>In each of the runs, there were two experimental blocks - in one of them the participants were asked to think about physical properties of word referents, whereas, in the other, about their more abstract, conceptual properties. Each block started with a 2.5-s presentation of a question about a specific physical (e.g., is this elongated?) or conceptual (e.g., is this animate?) property of word referents. Then, after 12-s break, all 20 words were presented, one after another. The participants’ task was to answer the question for each word referent by providing a “yes” or “no” answer. The blocks were separated by 12-s breaks, and there were 4 s of rest at the beginning and at least 16 s of rest at the end of each run.</p>
<p>In the sighted participants, the questions were presented in either the spoken or the written modality - the modality of question and word presentation were matched in each run. In the blind participants, such a solution was not practical because of the maximal capacity of the Braille display we used during fMRI and a relatively slower pace of Braille reading, compared to visual reading. Thus, in the blind group, the questions were always presented in the spoken modality. Furthermore, the duration of word presentation was also adjusted to the needs of each presentation modality - each word was presented for 1 s in the spoken modality (both groups) and in the visual alphabet (the sighted group only), and for 2.5 s in the Braille modality (blind group only). In each presentation modality, every word presentation was followed by a rest period lasting for either 2 s (67% probability) or 6 s (33% probability). Such a pace was sufficient to ensure word comprehension in all sighted and blind pilot participants. The post-fMRI interviews confirmed that blind and sighted individuals participating in the actual experiment found this pace comfortable.</p>
<p>In the sighted group, the spoken and the written runs had the same length (approx. 4 mins and 4 s) and the modality of a first run was randomly assigned to each participant. In the blind group, the spoken runs were of the same length, but written runs were longer (approx. 4 mins and 43 s) because of longer word presentations in the Braille modality. Because of different run lengths, the modality of the first run had to be fixed for all participants, in order to avoid errors during the data collection - thus, in the blind group, all participants started from the spoken run.</p>
<p>There were 6 different questions about physical properties of word referents and 6 different questions about their conceptual properties (<xref rid="tbl1" ref-type="table">Table 1</xref>). Each question was presented twice during the experiment - once in the spoken run and once in the written run. The order of blocks with physical questions and conceptual questions was counterbalanced within each participant and each word presentation modality - that is, in the first half of the experiment, the runs started from one attentional condition, whereas, in the second half, from the other attentional condition. What condition was first in the first and the second half of the experiment was randomly drawn for each participant.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>English translations of the questions used in the fMRI experiment.</title></caption>
<graphic xlink:href="635293v1_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>The stimuli presentation was controlled by a program written in PsychoPy v2022.2.3 (<xref ref-type="bibr" rid="c51">Peirce, 2007</xref>). The sounds were presented through MRI-compatible headphones. The visual alphabet was presented on a screen (white letters on a gray background), which participants watched in a mirror attached to the MRI head coil. The Braille alphabet was presented through a custom-made, MRI-compatible Braille display (<xref ref-type="bibr" rid="c17">Debowska, 2013</xref>), which was strapped to the left or right thigh of a participant so that it could be comfortably reached by the hand that this participant usually uses for Braille reading. The responses were provided by a two-button response pad. Before starting the experiment, each participant completed a short training session, to ensure that the written words are visible / reachable. Furthermore, the volume of spoken word presentation was individually adjusted to a loud, but comfortable level.</p>
</sec>
<sec id="s4d">
<title>Behavioral rating experiment</title>
<p>The ratings of physical and conceptual similarity between the word referents were obtained through an online experiment. Four hundred and two native Polish speakers (200 males, 202 females, mean age = 28.9 y, SD = 9 y) participated in this study. Each participant was visually presented with 30 word pairs that were randomly drawn from the set of 190 word pairs that could be created from the 20 words used as stimuli. For each pair, the participants were instructed to rate physical similarity of word referents, while ignoring conceptual similarity, and conceptual similarity, while ignoring physical features. In rating the physical similarity, the participants were instructed to focus on properties such as shape, size, and color. They were given an example of “a ball” and “an apple” as words naming physically similar objects. In rating the conceptual similarity, the participants were instructed to focus on their knowledge about word referents, their function, where they can be found, and similar properties. They were given an example of “an apple” and “a banana” as words naming conceptually similar objects. The participants saw a word pair, rated the physical and conceptual similarity of word referents on 5-point scales, and confirmed their answers with a keyboard press in order to see a next word pair.</p>
<p>The experiment was prepared in Psychopy v2022.2.3 and uploaded to Pavlovia online repository (<ext-link ext-link-type="uri" xlink:href="https://pavlovia.org">https://pavlovia.org</ext-link>). The participants were recruited through the Prolific platform (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.com">https://www.prolific.com</ext-link>) and were paid for the participation. The results obtained for each word pair were averaged across the participants. The obtained matrices of average ratings were then used in the analysis of neuroimaging data.</p>
</sec>
<sec id="s4e">
<title>Neuroimaging parameters</title>
<p>The neuroimaging data were acquired on a 3T Siemens Magnetom Prisma Fit MRI scanner using a 32-channel head coil at the Laboratory of Brain Imaging in the Nencki Institute of Experimental Biology in Warsaw. Functional data were acquired using a multiband sequence with the following parameters: 60 slices, phase encoding direction from posterior to anterior; voxel size: 2,5 mm<sup>3</sup>; TR = 1.41 s; TE: 30.4 ms; multiband factor: 3. Before the start of the first functional run, T1-weighted anatomic scans were acquired using MPRAGE sequence with the following parameters: 208 slices, phase encoding direction from anterior to posterior; voxel size: 0,8 mm<sup>3</sup>; TR = 2.5 s; TE: 21.7 ms.</p>
</sec>
<sec id="s4f">
<title>Neuroimaging data analysis</title>
<sec id="s4f1">
<title>Preprocessing</title>
<p>The neuroimaging data were converted from the DICOM to the NIFTI format using the dcm2niix (<xref ref-type="bibr" rid="c32">Li et al., 2016</xref>). Then, the data were preprocessed in BrainVoyager 22.4 (BrainInnovation). Standard routines were used to preprocess the functional data, including slice scan time correction, 3D rigid body motion correction, and temporal high-pass filter (GLM with Fourier basis set, 2 cycles per run). For each participant, the functional data were mapped onto an individual reconstruction of the cortical surface, created based on the collected anatomical image. All subsequent preprocessing steps and analyses were performed in the surface space. The functional data were spatially smoothed for the univariate analysis (the nearest neighbors approach, repeat value: 4). No spatial smoothing was performed for the multi-voxel pattern analysis.</p>
<p>Two first-level statistical models were created for each subject. For the multi-voxel pattern analysis, the data were modeled at the level of single words (20 words x 2 experimental blocks in each run, 40 predictors per run). For the univariate analysis, the data were modeled at the level of word blocks, understood as time between the onset of the first word in a given experimental block and the offset of the last word in this block (2 predictors per run). In both models, questions that were presented before the word blocks were additionally modeled as conditions of no interest (2 predictors per run). Signal time course was modeled using a general linear model (<xref ref-type="bibr" rid="c21">Friston et al., 1995</xref>) by convolving a canonical hemodynamic response function with the time series of predictors.</p>
</sec>
<sec id="s4f2">
<title>Multi-voxel pattern analysis</title>
<p>All multi-voxel pattern analyses were performed in CosmoMVPA (v.1.1.0; <xref ref-type="bibr" rid="c49">Oosterhof et al., 2016</xref>), running on Matlab R2022b (MathWorks). The NeuroElf toolbox (<ext-link ext-link-type="uri" xlink:href="https://neuroelf.net/">https://neuroelf.net/</ext-link>) was used to import BrainVoyager files into CosmoMVPA. All analyses were performed on T-values reflecting activation for each word presentation relative to average activation during rest periods in a given run (<xref ref-type="bibr" rid="c44">Misaki et al., 2010</xref>). Thus, 480 T-maps (20 words x 2 experimental blocks x 12 runs) were calculated and included in the analysis for each participant.</p>
<p>The multi-voxel pattern classification analyses were performed using a linear support vector machine classification algorithm, as implemented in the LIBSVM toolbox (v. 3.23; Chang and Lin, 2001). A standard LIBSVM data normalization procedure (i.e., Z-scoring beta estimates for each voxel in the training set and applying output values to the test set) was applied to the data before classification. The classification was performed in an odd-even cross validation scheme, that is, the classifier was trained on the odd runs and tested on the even runs, and vice versa. As a result, the algorithm was always trained on one sensory modality of the word presentation and tested on the other.</p>
<p>The representational similarity analyses were performed on brain activations (quantified as T-values, see above) that were averaged across runs and experimental blocks for each word. For each participant, the neural similarity matrices were calculated by correlating activity patterns for words in a given area with each other using Pearson correlation. Then, these matrices were correlated with the behavioral ratings of physical and conceptual similarity between word referents using Spearman’s correlation. Partial correlation approach, as implemented in CosmoMVPA, was used to account for common variance in the physical and conceptual similarity ratings.</p>
<p>The neural similarity matrices obtained in the visual cortex and in specific visual areas were correlated across the participants, to obtain the between-group and the within group similarity scores. To calculate the between-group similarity score, the neural similarity matrices of each participant were correlated with the neural similarity matrices of every participant from the other group (sighted-blind, blind-sighted). These values were then averaged to obtain one score for each participant. These scores were compared with within- group similarity scores, which were obtained by correlating the neural similarity matrices of each participant with the neural similarity matrices of every other participant from the same group (sighted-sighted, blind-blind). Spearman’s correlation was used to obtain all these correlation scores.</p>
<p>Finally, to obtain the “representational connectivity” scores (<xref ref-type="bibr" rid="c26">Kriegeskorte et al., 2008</xref>), the neural similarity matrices obtained in the visual areas of each participant was correlated with neural similarity matrices obtained for this participant in all other Brodmann areas found in the BrainVoyager cortical atlas (see below; 33 target areas in total). To account for effects present across the whole cortex, we calculated the average correlation across all target areas and subtracted this score from the results. This allowed us to identify brain regions in which the word-induced representation is more or less similar than the cortical average to the representation computed in the visual cortex. Spearman’s correlation was again used to obtain the correlation scores.</p>
<p>All analyses described above were performed in patches of interest (POIs) taken from the BrainVoyager cortical atlas of Brodmann areas. The bilateral areas 17, 18, and 19 were used as the visual cortex POIs (the analysis presented in <xref rid="figS2" ref-type="fig">Fig. S2</xref> was constrained to the left hemisphere). Analogously, the target areas in the representational connectivity analysis were also defined bilaterally. The left area 38 was used as the anterior temporal lobe POI in the analysis presented in <xref rid="figS3" ref-type="fig">Fig. S3</xref>. To obtain POIs that were adjusted to individual patterns of cortical folding, the participants’ cortical surface reconstructions were aligned to the atlas template in a cortex-based alignment procedure. Then, the inverse transformation matrices were used to project the atlas areas onto cortex reconstructions of each participant. The POIs were minimally dilated (one iteration of the BrainVoyager “dilate” POI function) to account for potential alignment imperfections. The analysis in POIs that were not dilated produced virtually identical results.</p>
<p>At the group level, the results were tested against chance levels using null distributions that were empirically derived in a permutation procedure. Specifically, each analysis was rerun 1000 times for each participant with word labels randomly assigned to this participant’s brain activity patterns in each iteration. Null distributions created in this procedure were averaged across participants and compared with the actual average results. The obtained p values (minimal value: p = 0.001) were corrected for multiple comparisons using the false discovery rate (<xref ref-type="bibr" rid="c9">Benjamini and Hochberg, 1995</xref>). The correction was jointly applied to all tests against chance level performed in a given analysis, in both groups. The results of correlation of neural activity patterns with the physical similarity ratings and the conceptual similarity ratings were treated as separate analyses and, consequently, corrected separately. A review of null distributions confirmed that, in all analyses, the empirically-derived chance levels were indistinguishable from a priori chance levels (i.e., accuracy = 5% in the classification analyses, Spearman’s rho = 0 in the representational similarity analyses). Thus, for simplicity, the a priori chance level is presented in the figures.</p>
<p>Testing for main effects and interactions were performed with ANOVAs, as implemented in SPSS 25 (IBM Corp, Armonk, NY). The Bonferroni correction, as implemented in SPSS, was used to correct the results of post hoc tests for multiple comparisons. Additional, planned comparisons testing for between-group and within-group differences were performed with two-sample and paired t tests, respectively. The Bonferroni correction was applied to the results of these tests, when appropriate, consistently with ANOVA post-hoc comparisons.</p>
</sec>
<sec id="s4f3">
<title>Searchlight analysis</title>
<p>The searchlight analysis was performed in surface patches of 500 vertices, using the CosmoMVPA and the Surfing toolbox (<xref ref-type="bibr" rid="c48">Oosterhof et al., 2011</xref>). The neural similarity matrices were created using Pearson correlation, and correlated with ratings of physical and conceptual similarity between word referents using the Spearman’s partial correlation, as was described above. The results obtained for each participant were aligned to the BrainVoyager atlas template, using forward transformation matrices obtained in the cortex-based alignment procedure. Then, the results were entered into group statistical models. One-sample t tests were used to test the results against chance level, in each group separately and when the participants from both groups were combined in one sample. Two-sample t tests and paired t tests were used to test for the differences between groups and conditions, respectively. Two-sample t tests performed on between-condition difference scores were used to test for interaction between the participant group and the condition.</p>
<p>CosmoMVPA was used to perform these tests. The resulting group T-maps were then converted into a threshold-free cluster enhancement (TFCE) map (<xref ref-type="bibr" rid="c61">Smith and Nichols 2009</xref>), calculated with standard parameters (E = 0.5, H = 2). The statistical significance of the results were determined by comparing the obtained TFCE values with null distributions obtained in the Monte Carlo simulation procedure (10 000 iterations) implemented in CosmoMVPA. The analyses were thresholded at p &lt; 0.05, corrected for multiple comparisons (Z &lt; 1.65). The T-values obtained in the group models were overlaid on the vertices in which the effects survived the threshold, in order to better visualize the statistical strength of the results.</p>
</sec>
<sec id="s4f4">
<title>Univariate analysis</title>
<p>We first performed the univariate POI analysis in the visual cortex. The analysis of contrast estimates for words presented in each modality, relative to rest periods, was performed in the same visual POIs that were used in the multi-voxel pattern analysis.</p>
<p>BrainVoyager was used to obtain these contrast estimates for each participant from the first-level models. At the group level, one-sample t-tests were used to test for significant effects, relative to rest. The false discovery rate was used to correct the results across all tests performed. The ANOVA, calculated in SPSS 25, was used to test for the main effects and interactions. The Bonferroni correction, as implemented in SPSS, was used to correct the results of post hoc tests for multiple comparisons. Additional, planned comparisons testing for between-group differences were performed with two-sample t tests. The Bonferroni correction was applied to the results, when appropriate.</p>
<p>Subsequently, the whole-brain univariate analysis was performed. The contrast estimates obtained for each participant were aligned to the BrainVoyager atlas template, using forward transformation matrices obtained in the cortex-based alignment procedure. Then, the aligned contrast estimates were entered into 2 (group) x 2 (word presentation modality) ANOVA, as implemented in BrainVoyager. All results were thresholded at p &lt; 0.001 voxel-wise, corrected for multiple comparisons using a surface patch extent, as determined by BrainVoyager.</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s7">
<title>Supplementary Figures</title>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><title>The behavioral ratings of physical and conceptual similarity between the referents of words used in the study.</title>
<p>The matrices visualize ratings for all 190 word pairs that can be created from the 20 words used in the study. The participants rated the similarity between word referents on scales from 1 to 5. The scores were averaged across the participants (there were no average scores greater than 4). The English translations of Polish words that were actually used in the study are presented.</p></caption>
<graphic xlink:href="635293v1_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><title>The activity patterns in the visual cortex of blind and sighted individuals reflect physical, but not conceptual similarity between word referents – analysis in the left hemisphere only.</title>
<p>The results of correlation between the neural similarity matrices, reflecting similarity between the activity patterns for specific words in the visual areas of blind and sighted participants, and the behavioral ratings of physical and conceptual similarity between the word referents. Only activity patterns in the left hemisphere were analyzed. * p &lt; 0.05, ** p &lt; 0.01, corrected for multiple comparisons using the false discovery rate. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><title>The activity patterns in the left anterior lobe reflect conceptual similarity between word referents in both participant groups.</title>
<p>The results of correlation between the neural similarity matrices, reflecting similarity between the activity patterns for specific words in the left anterior temporal lobe (ATL, marked in purple) of blind and sighted participants, and the behavioral ratings of physical and conceptual similarity between the word referents. * p &lt; 0.05, corrected for multiple comparisons using the false discovery rate. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4.</label>
<caption><title>During the word processing, the visual cortex in both blind and sighted participants shows the greatest “representational connectivity” to the occipitotemporal cortex - the analysis constrained to the primary visual cortex (BA 17).</title>
<p>For each brain region, the representational connectivity to the BA 17 was calculated by correlating neural similarity matrices, reflecting similarity between the activity patterns for specific words, obtained for the BA 17 and this region. In each participant group, the average correlation between the BA 17 and the target brain regions was subtracted from the results, to visualize regions that show higher-than-average and lower-than-average representational connectivity to the BA 17 during the word processing. The results for such regions are marked in blue (comparison against average correlation, p &lt; 0.05, corrected for multiple comparisons using the false discovery rate). The target brain regions were sorted by the obtained representational connectivity scores. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5.</label>
<caption><title>During the word processing, the visual cortex in both blind and sighted participants shows the greatest “representational connectivity” to the occipitotemporal cortex - the analysis constrained to the secondary visual cortex (BA 18).</title>
<p>For each brain region, the representational connectivity to the BA 18 was calculated by correlating neural similarity matrices, reflecting similarity between the activity patterns for specific words, obtained for the BA 18 and this region. In each participant group, the average correlation between the BA 18 and the target brain regions was subtracted from the results, to visualize regions that show higher-than-average and lower-than-average representational connectivity to the BA 18 during the word processing. The results for such regions are marked in blue (comparison against average correlation, p &lt; 0.05, corrected for multiple comparisons using the false discovery rate). The target brain regions were sorted by the obtained representational connectivity scores. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6.</label>
<caption><title>Searchlight analysis - results presented for each group separately.</title>
<p>(A-B) The results of correlation between the neural similarity matrices, reflecting similarity between the activity patterns for specific words, and the behavioral ratings of physical similarity between the word referents (A) in the blind participants and (B) in the sighted participants. (C) Analogous analysis including the ratings of conceptual similarity between the word referents. The results for the sighted participants are presented. No significant results in the blind group were found. The statistical significance was determined using threshold-free cluster enhancement (TFCE) maps and Monte Carlo simulation. The statistical threshold was set at p &lt; 0.05, corrected for multiple comparisons. The T-values for the effects that survived the threshold are presented for the visualization purposes.</p></caption>
<graphic xlink:href="635293v1_figS6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS7" position="float" orientation="portrait" fig-type="figure">
<label>Figure S7.</label>
<caption><title>Whole-brain univariate analysis: activations for words relative to rest periods.</title>
<p>Activations for spoken and written words, compared to rest, in the blind and the sighted group. The written words were presented in the Braille alphabet in the blind group and in the visual alphabet in the sighted group. As indicated by the font colors in the titles of the figure panels, warm colors indicate stronger activations for words, whereas cold colors indicate stronger activations for rest periods (i.e., deactivations during the processing of words). The statistical threshold was set at p &lt; 0.001, corrected for multiple comparisons using surface patch extent.</p></caption>
<graphic xlink:href="635293v1_figS7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS8" position="float" orientation="portrait" fig-type="figure">
<label>Figure S8.</label>
<caption><title>Whole-brain univariate analysis: comparisons between groups and sensory modalities of word presentations.</title>
<p>The font colors in the titles of the figure panels match groups or conditions with warm and cold colors in the figure. The statistical threshold was set at p &lt; 0.001, corrected for multiple comparisons using surface patch extent.</p></caption>
<graphic xlink:href="635293v1_figS8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS9" position="float" orientation="portrait" fig-type="figure">
<label>Figure S9.</label>
<caption><title>Correlation of activity patterns for words in the visual areas with behavioral similarity ratings: investigating effects of modality of word presentation.</title>
<p>The results of correlation between the neural similarity matrices, reflecting similarity between the activity patterns for specific words in the visual areas of blind and sighted participants, and the behavioral ratings of physical (upper panels) and conceptual (bottom panels) similarity between the word referents. The analysis was run separately for runs in which words were presented in the spoken modality (blue bars) and in the written modality (orange bars). The written words were presented in the Braille alphabet in the blind group and in the visual alphabet in the sighted group. * p &lt; 0.05, ** p &lt; 0.01, corrected for multiple comparisons using the false discovery rate. Error bars represent the standard error of the mean.</p></caption>
<graphic xlink:href="635293v1_figS9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by a National Science Center Poland grant (2020/37/B/HS6/01269) and a Polish National Center for Academic Exchange fellowship (BPN/SEL/2021/1/00004) to Ł.B.</p>
</ack>
<sec id="d1e1104" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Author contributions</title>
<p>M.P., M.U, and Ł.B. conceptualized and designed the study; M.P., M.U. and M.D. collected the data; M.P. and Ł.B. performed the data analyses; Ł.B. wrote the manuscript; M.P., M.U. and M.D. revised the manuscript.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Abboud</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, &amp; <string-name><surname>Cohen</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Semantic coding in the occipital cortex of early blind individuals</article-title>. <source>BioRxiv</source>, <fpage>539437</fpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amedi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Raz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Pianka</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Malach</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Zohary</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Early ‘visual’ cortex activation correlates with superior verbal memory performance in the blind</article-title>. <source>Nature neuroscience</source>, <volume>6</volume>(<issue>7</issue>), <fpage>758</fpage>–<lpage>766</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amedi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Floel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Knecht</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zohary</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Cohen</surname>, <given-names>L. G</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Transcranial magnetic stimulation of the occipital pole interferes with verbal processing in blind subjects</article-title>. <source>Nature neuroscience</source>, <volume>7</volume>(<issue>11</issue>), <fpage>1266</fpage>–<lpage>1270</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anurova</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Carlson</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Rauschecker</surname>, <given-names>J. P</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Overlapping anatomical networks convey cross-modal suppression in the sighted and coactivation of “visual” and auditory cortex in the blind</article-title>. <source>Cerebral Cortex</source>, <volume>29</volume>(<issue>11</issue>), <fpage>4863</fpage>–<lpage>4876</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bedny</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Evidence from Blindness for a Cognitively Pluripotent Cortex</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>21</volume>(<issue>9</issue>), <fpage>637</fpage>–<lpage>648</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bedny</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dodell-Feder</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Fedorenko</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Saxe</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Language processing in the occipital cortex of congenitally blind adults</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>108</volume>(<issue>11</issue>), <fpage>4429</fpage>–<lpage>4434</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bedny</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Richardson</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Saxe</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2015</year>). <article-title>“Visual” cortex responds to spoken language in blind children</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>33</issue>), <fpage>11674</fpage>–<lpage>11681</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benevento</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Bakkum</surname>, <given-names>B. W.</given-names></string-name>, &amp; <string-name><surname>Cohen</surname>, <given-names>R. S</given-names></string-name></person-group>. (<year>1995</year>). <article-title>gamma-Aminobutyric acid and somatostatin immunoreactivity in the visual cortex of normal and dark-reared rats</article-title>. <source>Brain research</source>, <volume>689</volume>(<issue>2</issue>), <fpage>172</fpage>–<lpage>182</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Hochberg</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal statistical society: series B (Methodological)</source>, <volume>57</volume>(<issue>1</issue>), <fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Caramazza</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Object domain and modality in the ventral visual pathway</article-title>. <source>Trends in cognitive sciences</source>, <volume>20</volume>(<issue>4</issue>), <fpage>282</fpage>–<lpage>290</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Binda</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kurzawski</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Lunghi</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Biagi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tosetti</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Morrone</surname>, <given-names>M. C</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Response to short-term deprivation of the human adult visual cortex measured with 7T BOLD</article-title>. <source>eLife</source>, <volume>7</volume>, <elocation-id>e40014</elocation-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Borghesani</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Buiatti</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Amadon</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Eger</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Piazza</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Word meaning in the ventral visual path: a perceptual to conceptual gradient of semantic coding</article-title>. <source>NeuroImage</source>, <volume>143</volume>, <fpage>128</fpage>–<lpage>140</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Diamond</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Raichle</surname>, <given-names>M. E</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Adaptive changes in early and late blind: a FMRI study of verb generation to heard nouns</article-title>. <source>Journal of neurophysiology</source>, <volume>88</volume>(<issue>6</issue>), <fpage>3359</fpage>–<lpage>3371</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burton</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Diamond</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>McDermott</surname>, <given-names>K. B</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Dissociating cortical regions activated by semantic and phonological tasks: a FMRI study in blind and sighted people</article-title>. <source>Journal of neurophysiology</source>, <volume>90</volume>(<issue>3</issue>), <fpage>1965</fpage>–<lpage>1982</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>C.-C.</given-names></string-name>, &amp; <string-name><surname>Lin</surname>, <given-names>C.-J</given-names></string-name></person-group>. (<year>2011</year>). <article-title>LIBSVM: A library for support vector machines</article-title>. <source>ACM Transactions on Intelligent Systems and Technology (TIST)</source>, <volume>2</volume>(<issue>3</issue>), <fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>L. G.</given-names></string-name>, <string-name><surname>Celnik</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Corwell</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Faiz</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Dambrosia</surname>, <given-names>J.</given-names></string-name>, <etal>…</etal> <string-name><surname>Hallett</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Functional relevance of cross-modal plasticity in blind humans</article-title>. <source>Nature</source>, <volume>389</volume>(<issue>6647</issue>), <fpage>180</fpage>–<lpage>183</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Debowska</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Wolak</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Soluch</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Orzechowski</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kossut</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Design and evaluation of an innovative MRI-compatible Braille stimulator with high spatial and temporal resolution</article-title>. <source>Journal of neuroscience methods</source>, <volume>213</volume>(<issue>1</issue>), <fpage>32</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deniz</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nunez-Elizalde</surname>, <given-names>A. O.</given-names></string-name>, <string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name>, &amp; <string-name><surname>Gallant</surname>, <given-names>J. L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality</article-title>. <source>Journal of Neuroscience</source>, <volume>39</volume>(<issue>39</issue>), <fpage>7722</fpage>–<lpage>7736</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dzięgiel-Fivet</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Jednoróg</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Reduced lateralization of the language network in the blind and its relationship with white matter tract neuroanatomy</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>18</volume>, <fpage>1407557</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferraro</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Van Ackeren</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Mai</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Tassi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Cardinale</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nigri</surname>, <given-names>A.</given-names></string-name>, <etal>…</etal> <string-name><surname>Collignon</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Stereotactic electroencephalography in humans reveals multisensory signal in early visual and auditory cortices</article-title>. <source>Cortex</source>, <volume>126</volume>, <fpage>253</fpage>–<lpage>264</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Holmes</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Poline</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Grasby</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Frackowiak</surname>, <given-names>R. S.</given-names></string-name>, &amp; <string-name><surname>Turner</surname>, <given-names>R</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Analysis of fMRI time-series revisited</article-title>. <source>NeuroImage</source>, <volume>2</volume>(<issue>1</issue>), <fpage>45</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2005</year>). <article-title>A theory of cortical responses</article-title>. <source>Philosophical transactions of the Royal Society B: Biological sciences</source>, <volume>360</volume>(<issue>1456</issue>), <fpage>815</fpage>–<lpage>836</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Caramazza</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bi</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Selectivity for large nonmanipulable objects in scene-selective visual cortex does not require visual experience</article-title>. <source>Neuroimage</source>, <volume>79</volume>, <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heimler</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Striem-Amit</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Amedi</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Origins of task-specific sensory-independent organization in the visual and auditory brain: neuroscience evidence, open questions and clinical implications</article-title>. <source>Current opinion in neurobiology</source>, <volume>35</volume>, <fpage>169</fpage>–<lpage>177</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iurilli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ghezzi</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Olcese</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Lassi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Nazzaro</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Tonini</surname>, <given-names>R.</given-names></string-name>, <etal>…</etal> <string-name><surname>Medini</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Sound-driven synaptic inhibition in primary visual cortex</article-title>. <source>Neuron</source>, <volume>73</volume>(<issue>4</issue>), <fpage>814</fpage>–<lpage>828</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title>. <source>Frontiers in systems neuroscience</source>, <volume>2</volume>, <fpage>249</fpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kupers</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Pappens</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>de Noordhout</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Schoenen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ptito</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Fumal</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2007</year>). <article-title>rTMS of the occipital cortex abolishes Braille reading and repetition priming in blind subjects</article-title>. <source>Neurology</source>, <volume>68</volume>(<issue>9</issue>), <fpage>691</fpage>–<lpage>693</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lane</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kanjlia</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Omaki</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bedny</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2015</year>). <article-title>“Visual” cortex of congenitally blind adults responds to syntactic movement</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>37</issue>), <fpage>12859</fpage>–<lpage>12868</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lane</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kanjlia</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Richardson</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Fulton</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Omaki</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bedny</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Reduced left lateralization of language in congenitally blind individuals</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>29</volume>(<issue>1</issue>), <fpage>65</fpage>–<lpage>78</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laurienti</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Burdette</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Wallace</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Yen</surname>, <given-names>Y. F.</given-names></string-name>, <string-name><surname>Field</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Stein</surname>, <given-names>B. E</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Deactivation of sensory-specific cortex by cross-modal stimuli</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>14</volume>(<issue>3</issue>), <fpage>420</fpage>–<lpage>429</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>T. S.</given-names></string-name>, &amp; <string-name><surname>Mumford</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Hierarchical Bayesian inference in the visual cortex</article-title>. <source>JOSA a</source>, <volume>20</volume>(<issue>7</issue>), <fpage>1434</fpage>–<lpage>1448</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Morgan</surname>, <given-names>P. S.</given-names></string-name>, <string-name><surname>Ashburner</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Rorden</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2016</year>). <article-title>The first step for neuroimaging data analysis: DICOM to NIfTI conversion</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>264</volume>, <fpage>47</fpage>–<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Linton</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2021</year>). <article-title>V1 as an egocentric cognitive map</article-title>. <source>Neuroscience of Consciousness</source>, <volume>2021</volume>(<issue>2</issue>), <fpage>niab017</fpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liuzzi</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Bruffaerts</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Peeters</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Adamczuk</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Keuleers</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>De Deyne</surname>, <given-names>S.</given-names></string-name>, <etal>…</etal> <string-name><surname>Vandenberghe</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Cross-modal representation of spoken and written word meaning in left pars triangularis</article-title>. <source>Neuroimage</source>, <volume>150</volume>, <fpage>292</fpage>–<lpage>307</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Magrou</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Barone</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Markov</surname>, <given-names>N. T.</given-names></string-name>, <string-name><surname>Killackey</surname>, <given-names>H. P.</given-names></string-name>, <string-name><surname>Giroud</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Berland</surname>, <given-names>M.</given-names></string-name>, <etal>…</etal> <string-name><surname>Kennedy</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2018</year>). <article-title>How areal specification shapes the local and interareal circuits in a macaque model of congenital blindness</article-title>. <source>Cerebral Cortex</source>, <volume>28</volume>(<issue>8</issue>), <fpage>3017</fpage>–<lpage>3034</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahon</surname>, <given-names>B. Z.</given-names></string-name>, <string-name><surname>Anzellotti</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schwarzbach</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zampini</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Caramazza</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Category-specific organization in the human brain does not require visual experience</article-title>. <source>Neuron</source>, <volume>63</volume>(<issue>3</issue>), <fpage>397</fpage>–<lpage>405</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Makin</surname>, <given-names>T. R.</given-names></string-name>, &amp; <string-name><surname>Krakauer</surname>, <given-names>J. W</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Against cortical reorganisation</article-title>. <source>eLife</source>, <volume>12</volume>, <elocation-id>e84716</elocation-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malik-Moraleda</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ayyash</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Gallée</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Affourtit</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hoffmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mineroff</surname>, <given-names>Z.</given-names></string-name>, <etal>…</etal> <string-name><surname>Fedorenko</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2022</year>). <article-title>An investigation across 45 languages and 12 language families reveals a universal language network</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>8</issue>), <fpage>1014</fpage>–<lpage>1019</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mandera</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Keuleers</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Wodniecka</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Brysbaert</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Subtlex-pl: subtitle-based word frequency estimates for Polish</article-title>. <source>Behavior research methods</source>, <volume>47</volume>, <fpage>471</fpage>–<lpage>483</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Douglas</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Newsome</surname>, <given-names>R. N.</given-names></string-name>, <string-name><surname>Man</surname>, <given-names>L. L.</given-names></string-name>, &amp; <string-name><surname>Barense</surname>, <given-names>M. D</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title>. <source>eLife</source>, <volume>7</volume>, <elocation-id>e31873</elocation-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Martinelli</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Handjaras</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Betta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Leo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cecchetti</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Pietrini</surname>, <given-names>P.</given-names></string-name>, <etal>…</etal> <string-name><surname>Bottari</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Auditory features modelling reveals sound envelope representation in striate cortex</article-title>. <source>BioRxiv</source>, 2020-04.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merabet</surname>, <given-names>L. B.</given-names></string-name>, <string-name><surname>Hamilton</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Schlaug</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Swisher</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Kiriakopoulos</surname>, <given-names>E. T.</given-names></string-name>, <string-name><surname>Pitskel</surname>, <given-names>N. B.</given-names></string-name>, <etal>…</etal> <string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Rapid and reversible recruitment of early visual cortex for touch</article-title>. <source>PLoS one</source>, <volume>3</volume>(<issue>8</issue>), <fpage>e3046</fpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mesulam</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Rogalski</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Wieneke</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hurley</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Geula</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bigio</surname>, <given-names>E. H.</given-names></string-name>, <etal>…</etal> <string-name><surname>Weintraub</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Primary progressive aphasia and the evolving neurology of the language network</article-title>. <source>Nature Reviews Neurology</source>, <volume>10</volume>(<issue>10</issue>), <fpage>554</fpage>–<lpage>569</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Misaki</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title>. <source>Neuroimage</source>, <volume>53</volume>(<issue>1</issue>), <fpage>103</fpage>–<lpage>118</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morales</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>S. Y.</given-names></string-name>, &amp; <string-name><surname>Kirkwood</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Dark rearing alters the development of GABAergic transmission in visual cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>22</volume>(<issue>18</issue>), <fpage>8084</fpage>–<lpage>8090</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morgan</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Petro</surname>, <given-names>L. S.</given-names></string-name>, &amp; <string-name><surname>Muckli</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Scene representations conveyed by cortical feedback to early visual cortex can be described by line drawings</article-title>. <source>Journal of Neuroscience</source>, <volume>39</volume>(<issue>47</issue>), <fpage>9410</fpage>–<lpage>9423</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Thelen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Romei</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Martuzzi</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Matusz</surname>, <given-names>P. J</given-names></string-name></person-group>. (<year>2016</year>). <article-title>The multisensory function of the human primary visual cortex</article-title>. <source>Neuropsychologia</source>, <volume>83</volume>, <fpage>161</fpage>–<lpage>169</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oosterhof</surname>, <given-names>N. N.</given-names></string-name>, <string-name><surname>Wiestler</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Downing</surname>, <given-names>P. E.</given-names></string-name>, &amp; <string-name><surname>Diedrichsen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2011</year>). <article-title>A comparison of volume-based and surface-based multi-voxel pattern analysis</article-title>. <source>Neuroimage</source>, <volume>56</volume>(<issue>2</issue>), <fpage>593</fpage>–<lpage>600</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oosterhof</surname>, <given-names>N. N.</given-names></string-name>, <string-name><surname>Connolly</surname>, <given-names>A. C.</given-names></string-name>, &amp; <string-name><surname>Haxby</surname>, <given-names>J. V</given-names></string-name></person-group>. (<year>2016</year>). <article-title>CoSMoMVPA: multi-modal multivariate pattern analysis of neuroimaging data in Matlab/GNU Octave</article-title>. <source>Frontiers in neuroinformatics</source>, <volume>10</volume>, <fpage>27</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Hamilton</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2001</year>). <article-title>The metamodal organization of the brain</article-title>. <source>Progress in brain research</source>, <volume>134</volume>, <fpage>427</fpage>–<lpage>445</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peirce</surname>, <given-names>J. W</given-names></string-name></person-group>. (<year>2007</year>). <article-title>PsychoPy - Psychophysics software in Python</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>162</volume>(<issue>1–2</issue>), <fpage>8</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Bracci</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Caramazza</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bi</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Tool selectivity in left occipitotemporal cortex develops without vision</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>25</volume>(<issue>8</issue>), <fpage>1225</fpage>–<lpage>1234</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Caramazza</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bi</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Nonvisual and visual object shape representations in occipitotemporal cortex: evidence from congenitally blind and sighted adults</article-title>. <source>Journal of Neuroscience</source>, <volume>34</volume>(<issue>1</issue>), <fpage>163</fpage>–<lpage>170</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname>, <given-names>R. P.</given-names></string-name>, &amp; <string-name><surname>Ballard</surname>, <given-names>D. H</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature neuroscience</source>, <volume>2</volume>(<issue>1</issue>), <fpage>79</fpage>–<lpage>87</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ricciardi</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bonino</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Pellegrini</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Pietrini</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Mind the blind brain to understand the sighted one! Is there a supramodal cortical functional architecture?</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>41</volume>, <fpage>64</fpage>–<lpage>77</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roelfsema</surname>, <given-names>P. R.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Early visual cortex as a multiscale cognitive blackboard</article-title>. <source>Annual review of vision science</source>, <volume>2</volume>, <fpage>131</fpage>–<lpage>151</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Röder</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Stock</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Bien</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Neville</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Rösler</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Speech processing activates visual cortex in congenitally blind humans</article-title>. <source>European Journal of Neuroscience</source>, <volume>16</volume>(<issue>5</issue>), <fpage>930</fpage>–<lpage>936</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saccone</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Bedny</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Developing cortex is functionally pluripotent: Evidence from blindness</article-title>. <source>Developmental Cognitive Neuroscience</source>, <volume>66</volume>, <fpage>101360</fpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sadato</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Pascual-Leone</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Grafman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ibañez</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Deiber</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Dold</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Hallett</surname>, <given-names>M</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Activation of the primary visual cortex by Braille reading in blind subjects</article-title>. <source>Nature</source>, <volume>380</volume>(<issue>6574</issue>), <fpage>526</fpage>–<lpage>528</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seydell-Greenwald</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Bi</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Striem-Amit</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Spoken language processing activates the primary visual cortex</article-title>. <source>PLoS One</source>, <volume>18</volume>(<issue>8</issue>), <fpage>e0289671</fpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Nichols</surname>, <given-names>T. E</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title>. <source>Neuroimage</source>, <volume>44</volume>(<issue>1</issue>), <fpage>83</fpage>–<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Striem-Amit</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Bi</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Caramazza</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Neural representation of visual concepts in people born blind</article-title>. <source>Nature communications</source>, <volume>9</volume>(<issue>1</issue>), <fpage>5250</fpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Urbaniak</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Paczyńska</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Caramazza</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bola</surname>, <given-names>Ł.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Neural representation of nouns and verbs in congenitally blind and sighted individuals</article-title>. <source>bioRxiv</source>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Men</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Caramazza</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bi</surname>, <given-names>Y</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Two forms of knowledge representations in the human brain</article-title>. <source>Neuron</source>, <volume>107</volume>(<issue>2</issue>), <fpage>383</fpage>–<lpage>393</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Vignali</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sigismondi</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Crepaldi</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bottini</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Collignon</surname>, <given-names>O</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Similar object shape representation encoded in the inferolateral occipitotemporal cortex of sighted and early blind people</article-title>. <source>PLoS Biology</source>, <volume>21</volume>(<issue>7</issue>), <fpage>e3001930</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106103.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study offers <bold>valuable</bold> insights into brain responses to words in the visual cortex of blind and sighted individuals. However, the evidence supporting the authors' claims remains <bold>incomplete</bold>, and the conclusions would benefit from a more comprehensive characterization of the conceptual properties of the word stimuli. This work will be of broad interest to cognitive neuroscientists, psycholinguists, and neurologists investigating meaning representation in the brain.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106103.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This fMRI study shows that two regions of the visual cortex (BA18 and BA19) of blind and sighted individuals carry information about the physical similarity of objects denoted by words. This effect was found for written words (Braille in blind, visual in sighted) but not spoken words. The evidence complements earlier studies reporting physical similarity effects in the occipitotemporal cortex of blind and sighted individuals (e.g., Peelen et al., 2014).</p>
<p>Strengths:</p>
<p>The study addresses an important question in the fields of neural plasticity and visual cortex organization. The study is generally well-conducted and the findings are clearly presented.</p>
<p>Weaknesses:</p>
<p>While the evidence is statistically strong, it is currently incomplete because of missing control analyses (see below). The framing of the results, as arguing against the pluripotent cortex account, is not entirely convincing as it was not clear that the study addressed the key predictions of that account.</p>
<p>Main comments:</p>
<p>(1) The study is framed as a test of Bedny's &quot;cognitively pluripotent cortex&quot; proposal (2017) that attributes the increased visual cortex response to linguistic stimuli in blind individuals to high-level cognitive functions. Key evidence for this account came from studies showing increased responses in blind visual cortex to certain grammatical manipulations and to solving mathematical equations. The current study did not include such manipulations. Instead, the current study focused on the representation of objects denoted by single words. Bedny's account did not make a strong argument that the physical similarity of word referents should be differently represented in blind and sighted individuals - if it did, please state this explicitly. Indeed, evidence that (some regions of) the visual cortex represent objects similarly in blind and sighted individuals does not seem incompatible with it.</p>
<p>(2) Throughout the manuscript (including the abstract) it was not clear what was meant with &quot;visual cortex&quot; or &quot;visual areas&quot;; whether this refers to early visual cortex (V1/BA17) or to visual cortex more generally (e.g., BA17-BA19, occipitotemporal cortex (MT, etc)). This is important for the theoretical arguments and for the interpretation of the results. If visual cortex = BA17, the current results point to potentially important differences between blind and sighted individuals, with the physical similarity of objects only observed in the visual cortex of the blind. If visual cortex is meant to include areas beyond BA17, the blind and sighted show similarities in the current study, although such similarities have been observed before using similar research approaches.</p>
<p>(3) Related to the point above, the abstract does not accurately describe the results, as it only describes the similarities between blind and sighted but not the differences. The study revealed differences between groups, particularly in BA17 - primary visual cortex. The differences between the groups are also illustrated by the strikingly different searchlight results in the two groups separately (Figure S6). These differences do not reach significance in a whole-brain-corrected contrast, but that likely reflects a lack of power (particularly for a between-group contrast).</p>
<p>(4) Results were found for written words but not spoken words (Figure S9). This is somewhat surprising considering that the visual cortex was more strongly activated for written words in the sighted, with this activation presumably not adding any information about the physical properties of word referents. Together with the widespread significance of clusters correlating with the physical similarity matrix (Figure 6), this raises the possibility of a confound. It would be good to ensure that this is not the case, e.g., you could create similarity matrices based on word length, word visual similarity (e.g., overlap in letters), and word frequency, and correlate these matrices with the physical similarity matrix to ensure that these correlations are not positive (or if they are, partial it out).</p>
<p>(5) The study included a task manipulation, with participants either judging physical or conceptual properties. This task manipulation is a central aspect of the design but does not feature anywhere in the results, and is also not discussed or introduced in the text. It would be interesting to know whether the results depend on the property (physical/conceptual) being task-relevant. But more importantly, a potential concern is that the responses in the task (given for each object using a two-response button box) correlate with physical or conceptual similarity and that this explains the fMRI findings. For example, two objects that are elongated would both receive a &quot;yes&quot; button press when participants answer the question &quot;is this elongated&quot;; these objects would also be rated as physically similar. This may apply more to physical than conceptual similarity. To exclude this possibility, the responses need to be analysed and included in the fMRI analyses, either as a regressor in the GLM or as another matrix to be partialed out at the final stage of analysis.</p>
<p>(4) Many of the blind participants had some residual vision (9/20 had light perception, 2/20 had contour perception); this could possibly have prevented the reorganization of visual cortex.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106103.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors show, through rigorous and extensive analyses, that the visual cortex in both congenitally blind and sighted participants represented differences between individual words presented across sensory modalities. In both groups, the activation patterns for words in the visual cortex reflected physical, but not conceptual similarity between word referents. This suggests a similar representation for both groups of words, one derived from vision-oriented mechanisms, and does not reflect significant functional reorganization in blindness.</p>
<p>Strengths:</p>
<p>The theoretical question is sound, as is the analysis approach. The authors' literature discussion is thorough, and the writing is clear.</p>
<p>Weaknesses:</p>
<p>I have only minor concerns left open.</p>
<p>(1) In the representational connectivity analysis, what is the average value across the brain? The authors compare the representational correlation across brain regions to the average value, but the average itself is not reported.</p>
<p>(2) Can the authors add a map showing the representational connectivity values across the brain in addition to the bar plot? It would make it easier to see what networks show similar neural representation to the visual cortex.</p>
<p>(3) Are the participants in the behavioral experiment from which the physical and conceptual similarity between word referents were collected matching in age or education with the fMRI participants?</p>
<p>(4) Although there are no group differences in the correlation of the physical similarity, I think it is important to acknowledge that the effect is only significant at the searchlight level in the blind early visual cortex (Figure S6).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106103.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study examines semantic processing in the visual cortex of both congenitally blind and sighted individuals using fMRI and multivariate pattern analysis (MVPA). The key finding is that the visual cortex in both groups encodes the physical properties of word referents, rather than their conceptual similarities. These results suggest that the same representational mechanisms operate in both the blind and sighted brain.</p>
<p>Strengths:</p>
<p>(1) The findings contribute to a broader understanding of cortical reorganization and provide evidence for top-down processing of word referents, even in the absence of visual experience.</p>
<p>(2) The experiment incorporates both spoken and written word presentations (Braille for blind participants), ensuring that the results are not confounded by modality effects.</p>
<p>(3) The study employs a rigorous methodological approach, combining multivariate and univariate analyses to strengthen the validity of its findings.</p>
<p>(4) The paper is well-structured and clearly written, making it easy to follow.</p>
<p>Weaknesses:</p>
<p>(1) The word stimuli consists of only 20 nouns referring to concrete entities. However, in the behavioral experiment, participants rated the physical and conceptual similarity of only 30 word pairs, which represents just a subset of all possible word pair combinations. The average similarity ratings across subjects were then used to construct stimuli similarity matrices, which were correlated with the fMRI similarity matrices in the MVPA analysis. What is the rationale for presenting only a small subset of all possible word pair combinations to participants? Additionally, the instruction to rate the &quot;conceptual similarity&quot; of word pairs seems somewhat ambiguous. Would &quot;conceptual similarity&quot; correlate with &quot;physical similarity&quot;? Instead of subjective ratings, why not use cosine similarity scores from pretrained language models to construct the &quot;conceptual similarity&quot; matrices? This approach could provide a more objective and reproducible measure of conceptual similarity.</p>
<p>(2) There are only six questions each for assessing the physical and conceptual properties of the words in the fMRI experiment. Most of the physical property questions focus on shape-related attributes (e.g., round, angular, elongated, symmetrical), while the conceptual properties are limited to three pairs of antonyms (living/non-living, natural/manufactured, pleasant/unpleasant). These aspects seem insufficient to comprehensively characterize the physical and conceptual properties of the nouns. What was the rationale behind selecting only these six questions? Could this limited set of attributes introduce bias in how the neural representations in the visual cortex are interpreted?</p>
<p>(3) Two of the blind participants are right-handed, and two may have some form of contour vision. What was the rationale for including these participants? In addition, the sample size for blind participants is relatively small (N = 20). Does the sample size provide sufficient justification for the main conclusion that the visual cortex in both blind and sighted groups represents the physical properties of word referents? Additionally, could individual differences among blind participants impact the results, and were any analyses conducted to account for such variability?</p>
<p>(4) I appreciate the authors' effort to integrate both univariate and multivariate approaches in their analyses. However, the results appear somewhat contradictory: The MVPA results suggest similar neural representations of word referents in the visual cortex for both blind and sighted participants. However, the univariate analyses indicate higher activation in the visual cortex of blind participants. How can these two findings be reconciled? The authors attributed the increased activation in the visual cortex of blind participants to their &quot;enhanced excitability&quot;, but what exactly does &quot;excitability&quot; mean in this context? Could this increased activation instead reflect an alternative neural strategy for processing semantic information in the blind brain? If so, how does this align with the claim that similar representational mechanisms exist in both blind and sighted individuals?</p>
<p>(5) The authors interpret their findings to suggest that the visual cortex can represent the physical properties of words even without visual experience, attributing this to top-down modulation from higher cognitive regions, which then backprojects to the visual cortex. However, it is unclear why only physical properties, and not conceptual properties, are backprojected. If higher cognitive regions modulate the visual cortex in a top-down manner, wouldn't both physical and conceptual attributes be expected to influence its activity? Could the authors clarify the mechanism that selectively supports physical property encoding over conceptual representation?</p>
</body>
</sub-article>
</article>