<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">100666</article-id>
<article-id pub-id-type="doi">10.7554/eLife.100666</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100666.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.5</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Physics of Living Systems</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Geometry and Dimensionality of Brain-wide Activity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0006-1401-9871</contrib-id>
<name>
<surname>Wang</surname>
<given-names>Zezhen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2320-2276</contrib-id>
<name>
<surname>Mai</surname>
<given-names>Weihao</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0184-1824</contrib-id>
<name>
<surname>Chai</surname>
<given-names>Yuming</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Qi</surname>
<given-names>Kexin</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ren</surname>
<given-names>Hongtai</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shen</surname>
<given-names>Chen</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Shiwu</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tan</surname>
<given-names>Guodong</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0790-1605</contrib-id>
<name>
<surname>Hu</surname>
<given-names>Yu</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<email>mahy@ust.hk</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0268-8403</contrib-id>
<name>
<surname>Wen</surname>
<given-names>Quan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>qwen@ustc.edu.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04c4dkn09</institution-id><institution>School of Data Science, University of Science and Technology of China</institution></institution-wrap>, <city>Hefei</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04c4dkn09</institution-id><institution>Division of Life Sciences and Medicine, University of Science and Technology of China</institution></institution-wrap>, <city>Hefei</city>, <country country="CN">China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04c4dkn09</institution-id><institution>Hefei National Laboratory for Physical Sciences at the Microscale, Center for Integrative Imaging, University of Science and Technology of China</institution></institution-wrap>, <city>Hefei</city>, <country country="CN">China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04c4dkn09</institution-id><institution>Department of Precision Machinery and Precision Instrumentation, University of Science and Technology of China</institution></institution-wrap>, <city>Hefei</city>, <country country="CN">China</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00q4vv597</institution-id><institution>Department of Mathematics, The Hong Kong University of Science and Technology</institution></institution-wrap>, <city>Hong Kong SAR</city>, <country country="CN">China</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00q4vv597</institution-id><institution>Division of Life Science, The Hong Kong University of Science and Technology</institution></institution-wrap>, <city>Hong Kong SAR</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Latham</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>Equal contribution</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-01-08">
<day>08</day>
<month>01</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-03-21">
<day>21</day>
<month>03</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP100666</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-03">
<day>03</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-07-04">
<day>04</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.02.23.529673"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-01-08">
<day>08</day>
<month>01</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100666.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.100666.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100666.1.sa1">Joint Public Reviews:</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.100666.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>Â© 2025, Wang et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Wang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-100666-v2.pdf"/>
<abstract>
<p>Understanding neural activity organization is vital for deciphering brain function. By recording whole-brain calcium activity in larval zebrafish during hunting and spontaneous behaviors, we find that the shape of the neural activity space, described by the neural covariance spectrum, is scale-invariant: a smaller, <italic>randomly sampled</italic> cell assembly resembles the entire brain. This phenomenon can be explained by Euclidean Random Matrix theory, where neurons are reorganized from anatomical to functional positions based on their correlations. Three factors contribute to the observed scale invariance: slow neural correlation decay, higher functional space dimension, and neural activity heterogeneity. In addition to matching data from zebrafish and mice, our theory and analysis demonstrate how the geometry of neural activity space evolves with population sizes and sampling methods, thus revealing an organizing principle of brain-wide activity.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We made 5 specific revisions.
1. We evaluated the covariance spectrum of a latent variable model proposed by Morrell et al.
2. We included a comparison with the findings of Manley et al. regarding the issue of saturating dimension in the Discussion section, highlighting the methodological differences and their implications.
3. We added a new mathematical derivation in the Methods section, elucidating the bounded dimensionality using the spectral properties of our model.
4. We have added a sentence in the Discussion section to further emphasize the robustness of our findings by demonstrating their consistency across diverse datasets and experimental techniques.
5. We have incorporated a brief discussion on the implications for neural coding and Fisher information, as highlighted in the recent work by Moosavi et al.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Geometric analysis of neuronal population activity has revealed the fundamental structures of neural representations and brain dynamics (<xref ref-type="bibr" rid="c1">1</xref>â<xref ref-type="bibr" rid="c4">4</xref>). Dimensionality reduction methods, which identify collective or latent variables in neural populations, simplify our view of high-dimensional neural data (<xref ref-type="bibr" rid="c5">5</xref>). Their applications to optical and multi-electrode recordings have begun to reveal important mechanisms by which neural cell assemblies process sensory information (<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>), make decisions (<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>), maintain working memory (<xref ref-type="bibr" rid="c10">10</xref>) and generate motor behaviors (<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c11">11</xref>â<xref ref-type="bibr" rid="c13">13</xref>).</p>
<p>In the past decade, the number of neurons that can be simultaneously recorded <italic>in vivo</italic> has grown exponentially (<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c14">14</xref>â<xref ref-type="bibr" rid="c21">21</xref>). This increase spans various brain regions (<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c22">22</xref>) and the entire mammalian brain (<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>). As more neurons are recorded, the multidimensional neural activity space, with each axis representing a neuronâs activity level (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>), becomes more complex. The changing size of observed cell assemblies raises a number of basic questions. How does this spaceâs geometry evolve and what structures remain invariant with increasing number of neurons recorded? A key measure, the <italic>effective dimension</italic> or <italic>participation ratio</italic> (denoted as <italic>D</italic><sub>PR</sub>, <xref rid="fig1" ref-type="fig">Fig. 1B</xref>), captures a major part of variability in neural activity (<xref ref-type="bibr" rid="c25">25</xref>â<xref ref-type="bibr" rid="c29">29</xref>). How does <italic>D</italic><sub>PR</sub> vary with the number of sampled neurons (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>)? Two scenarios are possible: <italic>D</italic><sub>PR</sub> grows continuously with more sampled neurons; <italic>D</italic><sub>PR</sub> saturates as the sample size increases. Which scenario fits the brain? Furthermore, even if two cell assemblies have the same <italic>D</italic><sub>PR</sub>, they can have different shapes (the geometric configuration of the neural activity space, as dictated by the eigenspectrum of the covariance matrix, <xref rid="fig1" ref-type="fig">Fig. 1C</xref>). How does the shape vary with the number of neurons sampled? Lastly, are we going to observe the same picture of neural activity space when using different recording methods such as two-photon microscopy, which records all neurons in a brain region, versus Neuropixels (<xref ref-type="bibr" rid="c16">16</xref>), which conducts a broad random sampling of neurons?</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>The relationship between the geometric properties of the neural activity space and the size of neural assemblies.</title>
<p><bold>A.</bold> Illustration of how dimensionality of neural activity (<italic>D</italic><sub>PR</sub>) changes with the number of recorded neurons. <bold>B</bold>. The eigenvalues of the neural covariance matrix dictate the geometrical configuration of the neural activity space with <inline-formula><inline-graphic xlink:href="529673v5_inline297.gif" mime-subtype="gif" mimetype="image"/></inline-formula> being the distribution width along a principal axis. <bold>C</bold>. Examples of two neural populations with identical dimensionality (<italic>D</italic><sub>PR</sub> = 25/11 â 2.27) but different spatial configurations, as revealed by the eigenvalue spectrum (green: {Î»<sub>i</sub>} = {7, 7, 1}, blue: {Î»<sub>i</sub>} = {9, 3, 3}).</p></caption>
<graphic xlink:href="529673v5_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Here, we aim to address these questions by analyzing brain-wide Ca<sup>2+</sup> activity in larval zebrafish during hunting or spontaneous behavior (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>) recorded by Fourier light-field microscopy (<xref ref-type="bibr" rid="c30">30</xref>). The small size of this vertebrate brain, together with the volumetric imaging method, enables us to capture a significant amount of neural activity across the entire brain simultaneously. To characterize the geometry of neural activity beyond its dimensionality <italic>D</italic><sub>PR</sub>, we examine the eigenvalues or spectrum of neural covariance (<xref ref-type="bibr" rid="c31">31</xref>) (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>). The covariance spectrum has been instrumental in offering mechanistic insights into neural circuit structure and function, such as the effective strength of local recurrent interactions and the depiction of network motifs (<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>). Intriguingly, we find that both the dimensionality and covariance spectrum remain invariant for cell assemblies that are randomly selected from various regions of the zebrafish brain. We also verify this observation in datasets recorded by different experimental methods, including light-sheet imaging of larval zebrafish (<xref ref-type="bibr" rid="c33">33</xref>), two-photon imaging of mouse visual cortex (<xref ref-type="bibr" rid="c23">23</xref>), and multi-area Neuropixels recording in the mouse (<xref ref-type="bibr" rid="c23">23</xref>). To explain the observed phenomenon, we model the covariance matrix of brain-wide activity by generalizing the Euclidean Random Matrix (ERM) (<xref ref-type="bibr" rid="c34">34</xref>) such that neurons correspond to points distributed in a <italic>d</italic>-dimensional functional or feature space, with pairwise correlation decaying with distance. The ERM theory, studied in theoretical physics (<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>), provides extensive analytical tools for a deep understanding of the neural covariance matrix model, allowing us to unequivocally identify three crucial factors for the observed scale invariance.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Whole-brain calcium imaging of zebrafish neural activity and the phenomenon of its scale-invariant covariance eigenspectrum.</title>
<p><bold>A.</bold> Rapid light-field Ca<sup>2+</sup> imaging system for whole brain neural activity in larval zebrafish. <bold>B</bold>. Inferred firing rate activity from the brain-wide calcium imaging. The ROIs are sorted by their weights in the first principal component (<xref ref-type="bibr" rid="c23">23</xref>). <bold>C</bold>. Procedure of calculating the covariance spectrum on the full and sampled neural activity matrices. <bold>D</bold>. Dimensionality (circles, average across 8 samplings (dots)), as a function of the sampling fraction. The curve is the predicted dimensionality using <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>. <bold>E</bold>. Iteratively sampled covariance matrices. Neurons are sorted in each matrix to maximize values near the diagonal. <bold>F</bold>. The covariance spectra, i.e., eigenvalue vs. rank/N, for randomly sampled neurons of different sizes (colors). The gray dots represent the sorted variances <italic>C</italic><sub><italic>ii</italic></sub> of all neurons. <bold>G to I</bold>. Same as <bold>F</bold> but from three models of covariance (see details in Methods): (G) a Wishart random matrix calculated from a random activity matrix of the same size as the experimental data; (H) replacing the eigenvectors by a random orthogonal set; (I) covariance generated from a randomly connected recurrent network.</p></caption>
<graphic xlink:href="529673v5_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Building upon our theoretical results, we further explore the connection between the spatial arrangement of neurons and their locations in functional space, which allows us to distinguish among three sampling approaches: random sampling, anatomical sampling (akin to optical recording of all neurons within a specific region of the brain) and functional sampling (<xref ref-type="bibr" rid="c20">20</xref>). Our ERM theory makes distinct predictions regarding the scaling relationship between dimensionality and the size of cell assembly, as well as the shape of covariance eigenspetrum under various sampling methods. Taken together, our results offer a new perspective for interpreting brain-wide activity and unambiguously show its organizing principles, with unexplored consequences for neural computation.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1</label>
<title>Geometry of neural activity across random cell assemblies in zebrafish brain</title>
<p>We recorded brain-wide Ca<sup>2+</sup> activity at a volume rate of 10 Hz in head-fixed larval zebrafish (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>) during hunting attempts (Methods) and spontaneous behavior using a Fourier light field microscopy (<xref ref-type="bibr" rid="c30">30</xref>). Approximately 2000 ROIs (1977.3 Â± 677.1, mean Â± SD) with a diameter of 16.84 Â± 8.51 Âµm were analyzed per fish based on voxel activity (Methods). These ROIs likely correspond to multiple nearby neurons with correlated activity. Henceforth, we refer to the ROIs as âneuronsâ for simplicity.</p>
<p>We first investigate the dimensionality of neural activity <italic>D</italic><sub>PR</sub> (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>) in a randomly chosen cell assembly in zebrafish, similar to multi-area Neuropixels recording in a mammalian brain. We focus on how <italic>D</italic><sub>PR</sub> changes with a large sample size <italic>N</italic>. We find that if the mean squared covariance remains finite instead of vanishing with <italic>N</italic>, the dimensionality <italic>D</italic><sub>PR</sub> (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>) becomes sample size independent and depends only on the variance <inline-formula><inline-graphic xlink:href="529673v5_inline1a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the covariance <italic>C</italic><sub><italic>ij</italic></sub> between neurons <italic>i</italic> and <italic>j</italic>:
<disp-formula id="eqn1">
<graphic xlink:href="529673v5_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where E(â¦) denotes average across neurons (Methods and (<xref ref-type="bibr" rid="c29">29</xref>)). The finite mean squared covariance condition is supported by the observation that the neural activity covariance <italic>C</italic><sub><italic>ij</italic></sub> is positively biased and widely distributed with a long tail (<xref rid="figS2" ref-type="fig">Fig. S2A</xref>). As predicted, the data dimensionality grows with sample size and reaches the maximum value specified by <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>).</p>
<p>Next, we investigate the shape of the neural activity space described by the eigenspectrum of the covariance matrix derived from the activity of <italic>N</italic> randomly selected neurons (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). When the eigenvalues are arranged in descending order and plotted against the normalized rank <italic>r</italic>/<italic>N</italic>, where <italic>r</italic> = 1, â¦, <italic>N</italic>, (we refer to it as the <italic>rank plot</italic>), this curve shows an approximate power law that spans 10 folds. Interestingly, as the size of the covariance matrices decreases (<italic>N</italic> decreases), the eigenspectrum curves nearly collapse over a wide range of eigenvalues. This pattern holds across diverse datasets and experimental techniques (<xref rid="fig2" ref-type="fig">Fig. 2F</xref>, <xref rid="figS2" ref-type="fig">Fig. S2E-L</xref>). The similarity of the covariance matrices of randomly sampled neural populations can be intuitively visualized (<xref rid="fig2" ref-type="fig">Fig. 2E</xref>), after properly sorting the neurons (Methods).</p>
<p>The scale invariance in the neural covariance matrix â the collapse of the covariance eigenspectrum under random sampling â is non-trivial. The spectrum is not scale-invariant in a common covariance matrix model based on independent noise (<xref rid="fig2" ref-type="fig">Fig. 2G</xref>). It is absent when replacing the neural covariance matrix eigenvectors with random ones, keeping the eigenvalues identical (<xref rid="fig2" ref-type="fig">Fig. 2H</xref>). A recurrent neural network with random connectivity (<xref ref-type="bibr" rid="c31">31</xref>) does not yield a scale-invariant covariance spectrum (<xref rid="fig2" ref-type="fig">Fig. 2I</xref>). A recently developed latent variable model (<xref ref-type="bibr" rid="c36">36</xref>) (<xref rid="figS23" ref-type="fig">Fig. S23</xref>), which is able to reproduce avalanche criticality, also fails to generate the scale-invariant covariance spectrum. Thus, a new model is needed for the covariance matrix of neural activity.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Modeling covariance by organizing neurons in functional space</title>
<p>Dimension reduction methods simplify and visualize complex neuron interactions by embedding them into a low-dimensional map, within which nearby neurons have similar activities. Inspired by these ideas, we use the Euclidean Random Matrix (ERM (<xref ref-type="bibr" rid="c34">34</xref>)) to model neural covariance. Imagine sprinkling neurons uniformly distributed on a d-dimensional <italic>functional space</italic> of size <italic>L</italic> (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>), where the distance between neurons <italic>i</italic> and <italic>j</italic> affects their correlation. Let <inline-formula><inline-graphic xlink:href="529673v5_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> represent the functional coordinate of the neuron <italic>i</italic>. The distance-correlation dependency is described by <italic>kernel function</italic> <inline-formula><inline-graphic xlink:href="529673v5_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with <italic>f</italic> (0) = 1, indicating closer neurons have stronger correlations, and decreases as distance <inline-formula><inline-graphic xlink:href="529673v5_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> increases (<xref rid="fig3" ref-type="fig">Fig. 3A</xref> and Methods). To model the covariance, we extend the ERM by incorporating heterogeneity of neuron activity levels (shown as the size of the neuron in the functional space in <xref rid="fig3" ref-type="fig">Fig. 3A</xref>)
<disp-formula id="eqn2">
<graphic xlink:href="529673v5_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>The variance of neural activity <inline-formula><inline-graphic xlink:href="529673v5_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is drawn i.i.d. from a given distribution and is independent of neuronsâ position.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>ERM model of covariance and its eigenspectrum.</title>
<p><bold>A.</bold> Schematic of the Euclidean Random Matrix (ERM) model, which reorganizes neurons (circles) from the anatomical space to the functional space (here <italic>d</italic> = 2 is a two-dimensional box). The correlation between a pair of neurons decreases with their distance in the functional space according to a kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline298.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This correlation is then scaled by neuronsâ variance <inline-formula><inline-graphic xlink:href="529673v5_inline299.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (circle size) to obtain the covariance <italic>C</italic><sub><italic>ij</italic></sub>. <bold>B</bold>. An example ERM correlation matrix (i.e., when <inline-formula><inline-graphic xlink:href="529673v5_inline300.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). <bold>C</bold>. Spectrum (same as <xref rid="fig2" ref-type="fig">Fig. 2F</xref>) for the ERM correlation matrix in B. The gray dots represent the sorted variances C<sub>ii</sub> of all neurons (same as in <xref rid="fig2" ref-type="fig">Fig. 2F</xref>). <bold>D</bold>. Visualizing the distribution of the same ERM eigenvalues in <bold>C</bold> by plotting the probability density function (pdf).</p></caption>
<graphic xlink:href="529673v5_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>This multidimensional functional space may represent attributes to which neurons are tuned, such as sensory features (e.g., visual orientation (<xref ref-type="bibr" rid="c37">37</xref>), auditory frequency) and movement characteristics (e.g., direction, speed (<xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c39">39</xref>)). In sensory systems, it represents stimuli as neural activity patterns, with proximity indicating similarity in features. For motor control, it encodes movement parameters and trajectories. In the hippocampus, it represents the place field of a place cell, acting as a cognitive map of physical space (<xref ref-type="bibr" rid="c40">40</xref>â<xref ref-type="bibr" rid="c42">42</xref>).</p>
<p>We first explore the ERM with various forms of <inline-formula><inline-graphic xlink:href="529673v5_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and find that fast-decaying functions like Gaussian and exponential functions do not produce eigenspectra similar to the data and no scale invariance over random sampling (<xref rid="figS4" ref-type="fig">Fig. S4A-H</xref> and Supp. Note). Thus, we turn to slow-decaying functions including the power law, which produce spectra similar to the data (<xref rid="fig3" ref-type="fig">Fig. 3C,D</xref>; see also <xref rid="figS5" ref-type="fig">Fig. S5</xref>). We adopt a particular kernel function because of its closed-form and analytical properties: <inline-formula><inline-graphic xlink:href="529673v5_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (Methods). For large distance <inline-formula><inline-graphic xlink:href="529673v5_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, it approximates a power law <inline-formula><inline-graphic xlink:href="529673v5_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and smoothly transitions at small distance to satisfy the correlation requirement <italic>f</italic> (0) = 1 (<xref rid="figS7" ref-type="fig">Fig. S7I, J</xref>).</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Analytical theory on the conditions of scale invariance in ERM</title>
<p>To determine the conditions for scale invariance in ERM, we analytically calculate the eigenspectrum of covariance matrix <italic>C</italic> (<xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref>) for large <italic>N, L</italic> using the replica method (<xref ref-type="bibr" rid="c34">34</xref>). A key order parameter emerging from this calculation is the neuron density <italic>Ï</italic>:= <italic>N</italic>/<italic>L</italic><sup>d</sup>. In the high-density regime <inline-formula><inline-graphic xlink:href="529673v5_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the covariance spectrum can be approximated in a closed form (Methods). For the slow-decaying kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> defined above, the spectrum for large eigenvalues follows a power law (Supp. Note):
<disp-formula id="eqn3">
<graphic xlink:href="529673v5_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>r</italic> is the rank of the eigenvalues in descending order and <italic>p</italic>(Î») is their probability density function. <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> intuitively explains the scale invariance over random sampling. Sampling in the ERM reduces the neuron density <italic>Ï</italic>. The eigenspectrum is <italic>Ï</italic>-independent whenever <italic>Âµ/d</italic> â 0. This indicates two factors contributing to the scale invariance of the eigenspectrum. First, a small exponent <italic>Âµ</italic> in the kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> means that pairwise correlations slowly decay with functional distance and can be significantly positive across various functional modules and throughout the brain. For a given <italic>Âµ</italic>, an increase in dimension <italic>d</italic> improves the scale invariance. The dimension <italic>d</italic> could represent the number of independent features or latent variables describing neural activity or cognitive states.</p>
<p>We verify our theoretical predictions by comparing sampled eigenspectra in finite-size simulated ERMs across different <italic>Âµ</italic> and <italic>d</italic> (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>). We first consider the case of homogeneous neurons (<inline-formula><inline-graphic xlink:href="529673v5_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref>, revisited later) in these simulations (<xref rid="fig3" ref-type="fig">Fig. 3C, D</xref> and <xref rid="fig4" ref-type="fig">Fig. 4A</xref>), making <italic>C</italic>âs entries correlation coefficients. To quantitatively assess the level of scale invariance, we introduce a <italic>collapse index</italic> (CI, see Methods for a detailed definition). Motivated by <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>, the CI measures the shift of the eigenspectrum when the number of sampled neurons changes. The smaller CI values indicate higher scale invariance. Intuitively, it is defined as the area between spectrum curves from different sample sizes (<xref rid="fig4" ref-type="fig">Fig. 4A</xref> upper right). In the log-log scale rank plot, <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> shows the spectrum shifts vertically with Ï. Thus, we define CI as this average displacement (<xref rid="fig4" ref-type="fig">Fig. 4A</xref> upper right, Methods), and a smaller CI means more scale-invariant. Using CI, <xref rid="fig4" ref-type="fig">Fig. 4A</xref> shows that scale invariance improves with slower correlation decay as <italic>Âµ</italic> decreases and the functional dimension d increases. Conversely, with large <italic>Âµ</italic> and small d, the covariance eigenspectrum varies significantly with scale (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Three factors contributing to scale invariance.</title>
<p><bold>A.</bold> Impact of Âµ and <italic>d</italic> (see text) on the scale invariance of ERM spectrum (same plots as <xref rid="fig3" ref-type="fig">Fig. 3C</xref>) with <inline-formula><inline-graphic xlink:href="529673v5_inline301.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The degree of scale invariance is quantified by the collapse index (CI), which essentially measures the area between different spectrum curves (upper right inset). For comparison, we fix the same coordinate range across panels hence some plots are cropped. The gray dots represent the sorted variances C<sub><italic>ii</italic></sub> of all neurons (same as in <xref rid="fig2" ref-type="fig">Fig. 2F</xref>). <bold>B</bold>. Top: sampled correlation matrix spectrum in an example animal (fish 1). Bottom: Same as top but for the covariance matrix that incorporates heterogeneous variances. The gray dots represent the sorted variances <italic>C</italic><sub><italic>ii</italic></sub> of all neurons (same as in <xref rid="fig2" ref-type="fig">Fig. 2F</xref>). <bold>C</bold>. The CI of the correlation matrix (filled squares) is found to be larger than that for the covariance matrix (opened squares) across different datasets: f1 to f6: six light-field zebrafish data (10 Hz per volume, this paper); fl: light-sheet zebrafish data (2 Hz per volume, (<xref ref-type="bibr" rid="c33">33</xref>)); mn: mouse Neuropixels data (downsampled to 10 Hz per volume); mp: mouse two-photon data, (3 Hz per volume, (<xref ref-type="bibr" rid="c23">23</xref>)).</p></caption>
<graphic xlink:href="529673v5_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Next, we consider the general case of unequal neural activity levels <inline-formula><inline-graphic xlink:href="529673v5_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and check for differences between the correlation (equivalent to <inline-formula><inline-graphic xlink:href="529673v5_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) and covariance matrix spectra. Using the collapsed index (CI), we compare the scale invariance of the two spectra in the experimental data. Intriguingly, the CI of the covariance matrix is consistently smaller (more scale-invariant) across all datasets (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>, <xref rid="figS6" ref-type="fig">Fig. S6C</xref>, open vs. closed squares), indicating that the heterogeneity of neuronal activity variances significantly affects the eigenspectrum and the geometry of neural activity space (<xref ref-type="bibr" rid="c43">43</xref>). By extending our spectrum calculation to the intermediate density regime <inline-formula><inline-graphic xlink:href="529673v5_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (Methods), we show that the ERM model can quantitatively explain the improved scale invariance in the covariance matrix compared to the correlation matrix (<xref rid="figS6" ref-type="fig">Fig. S6B</xref>).</p>
<p>Lastly, we examine factors that turn out to have minimal impact on the scale invariance of the covariance spectrum. First, the shape of the kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> over a small distance (small distance means <italic>f(x)</italic> near <italic>x</italic> = 0 in the functional space, <xref rid="figS7" ref-type="fig">Fig. S7</xref>) does not affect the distribution of large eigenvalues (<xref rid="figS7" ref-type="fig">Fig. S7</xref>, <xref ref-type="table" rid="tblS3">table S3</xref>, <xref rid="figS9" ref-type="fig">Fig. S9A</xref>). This supports our use of a specific <inline-formula><inline-graphic xlink:href="529673v5_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to represent a class of slow-decaying kernels. Second, altering the spatial distribution of neurons in the functional space, whether using a Gaussian, uniform, or clustered distribution, does not affect large covariance eigenvalues, except possibly the leading ones (<xref rid="figS9" ref-type="fig">Fig. S9B</xref>, Methods). Third, different geometries of the functional space, such as a flat square, a sphere, or a hemisphere, result in eigenspectra similar to the original ERM model (<xref rid="figS9" ref-type="fig">Fig. S9C</xref>). These findings indicate that our theory for the covariance spectrumâs scale invariance is robust to various modeling details.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Connection among random sampling, functional sampling, and anatomical sampling</title>
<p>So far, we have focused on random sampling of neurons, but how does the neural activity space change with different sampling methods? To this end, we consider three methods (<xref rid="fig5" ref-type="fig">Fig. 5A1</xref>): random sampling (RSap), anatomical sampling (ASap) where neurons in a brain region are captured by optical imaging (<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>), and functional sampling (FSap) where neurons are selected based on activity similarity (<xref ref-type="bibr" rid="c20">20</xref>). In ASap or FSap, sampling involves expanding regions of interest in anatomical space or functional space while measuring all neural activity within those regions (Methods). The difference among sampling methods depends on the neuron organization throughout the brain. If anatomically localized neurons also cluster functionally (<xref rid="fig5" ref-type="fig">Fig. 5A4</xref>), ASap â FSap; if they are spread in the functional space (<xref rid="fig5" ref-type="fig">Fig. 5A2</xref>), ASap â RSap. Generally, the anatomical-functional relationship is in-between and can be quantified using the Canonical Correlation Analysis (CCA). This technique finds axes (CCA vectors <inline-formula><inline-graphic xlink:href="529673v5_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) in anatomical and functional spaces such that the neuronsâ projection along these axes has the maximum correlation, <italic>R</italic><sub>CCA</sub>. The extreme scenarios described above correspond to <italic>R</italic><sub>CCA</sub> = 1 and <italic>R</italic><sub>CCA</sub> = 0.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>The relationship between the functional and anatomical space and theoretical predictions.</title>
<p><bold>A.</bold> Three sampling methods (A1) and <italic>R</italic><sub>CCA</sub> (see text). When <italic>R</italic><sub>CCA</sub> â 0 (A2), the anatomical sampling (ASap) resembles the random sampling (RSap), and while when <italic>R</italic><sub>CCA</sub> â 1 (A4), ASap is similar to the functional sampling (FSap). <bold>B</bold>. Distribution of neurons in the functional space inferred by MDS. Each neuron is color-coded by its projection along the first canonical direction <inline-formula><inline-graphic xlink:href="529673v5_inline302.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the anatomical space (see text). Data based on fish 6, same for C to E. <bold>C</bold>. Similar to B. but plotting neurons in the anatomical space with color based on their projection along <inline-formula><inline-graphic xlink:href="529673v5_inline303.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the functional space (see text). <bold>D</bold>. Dimensionality (<italic>D</italic><sub>PR</sub>) across sampling methods: average D<sub>PR</sub> under RSap (circles), average and individual brain region <italic>D</italic><sub>PR</sub> under ASap (squares and dots), and <italic>D</italic><sub>PR</sub> under FSap for the most correlated neuron cluster (triangles; Methods). Dashed and solid lines are theoretical predictions for <italic>D</italic><sub>PR</sub> under RSap and FSap, respectively (Methods). <bold>E</bold>. The CI of correlation matrices under three sampling methods in 6 animals (colors). <sup>**</sup>p&lt;0.01; <sup>***</sup>p&lt;0.001; one-sided paired t tests: RSap vs. ASap, p = 0.0010; RSap vs. FSap, p = 0.0004; ASap vs. FSap, p = 0.0014.</p></caption>
<graphic xlink:href="529673v5_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To determine the anatomical-functional relationship in neural data, we infer the functional coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of each neuron by fitting the ERM using multidimensional scaling (MDS) (<xref ref-type="bibr" rid="c46">46</xref>) (Methods). For simplicity and better visualization, we use a low-dimensional functional space where <italic>d</italic> = 2. The fitted functional coordinates confirm the slow decay kernel function in ERM except for a small distance (<xref rid="figS12" ref-type="fig">Fig. S12</xref>). The ERM with inferred coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> also reproduces the experimental covariance matrix, including cluster structures (<xref rid="figS11" ref-type="fig">Fig. S11</xref>) and its sampling eigenspectra (<xref rid="figS10" ref-type="fig">Fig. S10</xref>).</p>
<p>Equipped with the functional and anatomical coordinates, we next use CCA to determine which scenarios illustrated in <xref rid="fig5" ref-type="fig">Fig. 5A</xref> align better with the neural data. <xref rid="fig5" ref-type="fig">Fig. 5B,C</xref> shows a representative fish with a significant <italic>R</italic><sub>CCA</sub> = 0.327 (p-value=0.0042, AndersonâDarling test). Notably, the CCA vector in the anatomical space, <inline-formula><inline-graphic xlink:href="529673v5_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, aligns with the rostrocaudal axis. Coloring each neuron in the functional space by its projection along <inline-formula><inline-graphic xlink:href="529673v5_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> shows a correspondence between clustering and anatomical coordinates (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). Similarly, coloring neurons in the anatomical space (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>) by their projection along <inline-formula><inline-graphic xlink:href="529673v5_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula> reveals distinct localizations in regions like the forebrain and optic tectum. Across animals, functionally clustered neurons show anatomical segregation (<xref ref-type="bibr" rid="c33">33</xref>), with an average <italic>R</italic><sub>CCA</sub> of 0.335Â±0.054 (meanÂ±SD).</p>
<p>Next, we investigate the effects of different sampling methods (<xref rid="fig5" ref-type="fig">Fig. 5A1</xref>) on the geometry of the neural activity space when there is a significant but moderate anatomical-functional correlation as in the experimental data. Interestingly, dimensionality <inline-formula><inline-graphic xlink:href="529673v5_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in data under anatomical sampling consistently falls between random and functional sampling values (<xref rid="fig5" ref-type="fig">Fig. 5D</xref>). This phenomenon can be intuitively explained by the ERM theory. Recall that for large <italic>N</italic>, the key term in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> is <inline-formula><inline-graphic xlink:href="529673v5_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For a fixed number of sampled neurons, this average squared covariance is maximized when neurons are selected closely in the functional space (FSap) and minimized when distributed randomly (RSap). Thus, RSap and FSap D<sub>PR</sub> set the upper and lower bounds of dimensionality, with ASap expected to fall in between. This reasoning can be precisely formulated to obtain quantitative predictions of the bounds (Methods). We predict the ASap dimension at large <italic>N</italic> as
<disp-formula id="eqn4">
<graphic xlink:href="529673v5_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here <italic>D</italic><sub>PR</sub> is the dimensionality under RSap (<xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref>), k represents the fraction of sampled neurons. R<sub>ASap</sub> is the correlation between anatomical and functional coordinates along the direction where the anatomical subregions are divided (Methods), and it is bounded by the canonical correlation <italic>R</italic><sub>ASap</sub> â¤ <italic>R</italic><sub>CCA</sub>. When <italic>R</italic><sub>ASap</sub> = 0, we get the upper bound <inline-formula><inline-graphic xlink:href="529673v5_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref rid="fig5" ref-type="fig">Fig. 5D</xref> dashed line). The lower bound is reached when <italic>R</italic><sub>ASap</sub> = <italic>R</italic><sub>CCA</sub> = 1 (5A4), where <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> shows a scaling relationship <inline-formula><inline-graphic xlink:href="529673v5_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that depends on the sampling fraction <italic>Îº</italic> (<xref rid="fig5" ref-type="fig">Fig. 5D</xref> solid line). This contrasts with the <italic>Îº</italic>-independent dimensionality of RSap in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref>. Furthermore, if <italic>R</italic><sub>ASap</sub> and its upper bound is not close to 1 (precisely <italic>R</italic><sub>ASap</sub> â¤ 0.84 for the ERM model in <xref rid="fig5" ref-type="fig">Fig. 5D</xref>), <inline-formula><inline-graphic xlink:href="529673v5_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> align closer to the upper bound of RSap. This prediction agrees well with our observations in data across animals (<xref rid="fig5" ref-type="fig">Fig. 5D</xref>, <xref rid="figS20" ref-type="fig">Fig. S20</xref> and <xref rid="figS21" ref-type="fig">Fig. S21</xref>).</p>
<p>Beyond dimensionality, our theory predicts the difference in the covariance spectrum between sampling methods based on the neuronal density <italic>Ï</italic> in the functional space (<xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>). This density <italic>Ï</italic> remains constant during FSap (<xref rid="fig5" ref-type="fig">Fig. 5A1</xref>) and decreases under RSap; the average density across anatomical regions â¨<italic>Ï</italic>â© in ASap lies between those of FSap and RSap. Analogous to <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>, the relationship in <italic>Ï</italic> orders the spectra: ASapâs spectrum lies between those of FSap and RSap (Methods). This further implies that the level of scale invariance under ASap should fall between that of RSap and FSap, which is confirmed by our experimental data (<xref rid="fig5" ref-type="fig">Fig. 5E</xref>).</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<sec id="s3a">
<title>Impact of hunting behavior on scale invariance and functional space organization</title>
<p>How does task-related neural activity shape the covariance spectrum and brain-wide functional organization? We examine the hunting behavior in larval zebrafish, marked by eye convergence (both eyes move inward to focus on the central visual field) (<xref ref-type="bibr" rid="c47">47</xref>). We find that scale invariance of the eigenspectra persists and is enhanced even after removing the hunting frames from the Ca<sup>2+</sup> imaging data (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>, <xref rid="figS15" ref-type="fig">Fig. S15AB</xref>, Methods). This is consistent with the scale-invariant spectrum found in other data sets during spontaneous behaviors (<xref rid="figS10" ref-type="fig">Fig. S10F</xref>, <xref rid="figS2" ref-type="fig">Fig. S2GH</xref>), suggesting scale invariance is a general phenomenon.</p>
<p>Interestingly, in the inferred functional space, we observe reorganizations of neurons after removing hunting behavior (<xref rid="figS15" ref-type="fig">Fig. S15C, D</xref>). Neurons in one cluster disperse from their center of mass (<xref rid="figS15" ref-type="fig">Fig. S15D</xref>) and decreases the local neuronal density Ï (Methods and <xref rid="figS15" ref-type="fig">Fig. S15E</xref>). The neurons in this dispersed cluster have a consistent anatomical distribution from the midbrain to the hindbrain in 4 out of 5 fish (<xref rid="figS17" ref-type="fig">Fig. S17</xref>). During hunting, the cluster has robust activations that are widespread in the anatomical space but localized in the functional space(Movie. S1).</p>
<p>Our findings suggest that the functional space could be defined by latent variables that represent cognitive factors such as decision-making, memory, and attention. These variables set the spaceâs dimensions, with neural activity patterns reflecting cognitive state dynamics. Functionally related neurons â through sensory tuning, movement parameters, internal conditions, or cognitive factors â become closer in this space, leading to stronger activity correlations.</p>
</sec>
<sec id="s3b">
<title>Criticality and power law</title>
<p>What drives brain dynamics with a slow-decaying distance-correlation function <inline-formula><inline-graphic xlink:href="529673v5_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in functional space? Long-range connections and a slow decline in projection strength over distance (<xref ref-type="bibr" rid="c48">48</xref>) may cause extensive correlations, enhancing global activity patterns. This behavior is also reminiscent of phase transitions in statistical mechanics (<xref ref-type="bibr" rid="c49">49</xref>), where local interactions lead to expansive correlated behaviors. Studies suggest that critical brains optimize information processing (<xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c51">51</xref>). The link between neural correlation structures and neuronal connectivity topology is an exciting area for future exploration.</p>
<p>In the high-density regime of the ERM model, the rank plot (<xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>) for large eigenvalues (Î» &gt; 1) follows a power law Î» â¼ <italic>r</italic><sup>â<italic>Î±</italic></sup>, with <italic>Î±</italic> = 1 â <italic>Âµ</italic>/<italic>d</italic> &lt; 1. The scale invariant spectrum occurs when <italic>Î±</italic> is close to 1. Experimental data, however, align more closely with the model in the intermediate-density regime, where the power-law spectrum is an approximation and the decay is slower (for ERM model <xref rid="figS3" ref-type="fig">Fig. S3BC</xref>, and for data <italic>Î±</italic> = 0.47 Â± 0.08, mean Â± SD, <italic>n</italic> = 6 fish). Stringer et al. (<xref ref-type="bibr" rid="c6">6</xref>) found an <italic>Î±</italic> â³ 1 decay in the mouse visual cortexâs stimulus trial averaged covariance spectrum, and they argued that this decay optimizes visual code efficiency and smoothness. Our study differs in two fundamental ways. First, we recorded brain-wide activity during spontaneous or hunting behavior, calculating neural covariance from single-trial activity. Much of the neural activity was not driven by sensory stimulus and unrelated to specific tasks, requiring a different interpretation of the neural covariance spectrum. Second, without loss of generality, we normalized the mean variance of neural activity E(<italic>Ï</italic><sup>2</sup>) by scaling the covariance matrix so that its eigenvalues sum up to <italic>N</italic>. This normalization imposes a constraint on the spectrum. In particular, large and small eigenvalues may have different behaviors and do not need to obey a single power law <italic>Î»</italic> â¼ <italic>r</italic><sup>â<italic>Î±</italic></sup> for all <italic>N</italic> eigenvalues (Methods). Stringer et al. (<xref ref-type="bibr" rid="c6">6</xref>) did not take this possibility into account, making their theory less applicable to our analysis.</p>
<p>We draw inspiration from the renormalization group (RG) approach to navigate neural covariance across scales, which has also been explored in the recent literature. Following Kadanoffâs block spin transformation (<xref ref-type="bibr" rid="c49">49</xref>), Meshulam et al. (<xref ref-type="bibr" rid="c20">20</xref>) formed size-dependent neuron clusters and their covariance matrices by iteratively pairing the most correlated neurons and placing them adjacent on a lattice. The groups expanded until the largest reached the system size. The RG process, akin to spatial sampling in functional space (FSap), maintains constant neuron density <italic>Ï</italic>. Thus, for any kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, including the power law and exponential, the covariance eigenspectrum remains invariant across scales (<xref rid="figS19" ref-type="fig">Fig. S19A,B,D,E</xref>).</p>
<p>Morrell et al. (<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c52">52</xref>) proposed a simple model in which a few time-varying latent factors impact the whole neural population. We evaluated if this model could account for the scale invariance seen in our data. Simulations showed that the resulting eigenspectra differed considerably from our findings (<xref rid="figS23" ref-type="fig">Fig. S23</xref>). Although the Morrell model demonstrated a degree of scale invariance under functional sampling (or RG), it did not align with the scale-invariant features under random sampling, suggesting that this simple model might not capture all crucial features in our observations.</p>
<p>We emphasize that the covariance spectrum being a power law is distinct from the scale invariance we define in this study, namely the collapse of spectrum curves under random neuron sampling. The random RNN model in <xref rid="fig2" ref-type="fig">Fig. 2I</xref> shows a power-law behavior, but lacks true scale invariance as spectrum curves for different sizes do not collapse. When connection strength <italic>g</italic> approaches 1, the system exhibits a power law spectrum of <inline-formula><inline-graphic xlink:href="529673v5_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Subsampling causes the spectrum to shift by <inline-formula><inline-graphic xlink:href="529673v5_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>k</italic> = <italic>N</italic><sub><italic>s</italic></sub>/<italic>N</italic> is the sampling fraction (derived from <xref ref-type="disp-formula" rid="eqn24">Eq. 24</xref> in(<xref ref-type="bibr" rid="c31">31</xref>)).</p>
</sec>
<sec id="s3c">
<title>Bounded dimensionality under random sampling</title>
<p>The saturation of the dimensionality D<sub>PR</sub> at large sample sizes indicates a limit to neural assembly complexity, evidenced by the finite mean square covariance. This is in contrast with neural dynamics models such as the balanced excitatory-inhibitory (E-I) neural network (<xref ref-type="bibr" rid="c53">53</xref>), where <inline-formula><inline-graphic xlink:href="529673v5_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula> resulting in an unbounded dimensionality (see Supp. Note). Our results suggest that the brain encodes experiences, sensations, and thoughts using a finite set of dimensions instead of an infinitely complex neural activity space.</p>
<p>We found that the relationship between dimensionality and the number of recorded neurons depends on the sampling method. For functional sampling, the dimensionality scales with the sampling fraction <inline-formula><inline-graphic xlink:href="529673v5_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This suggests that if anatomically sampled neurons are functionally clustered, as with cortical neurons forming functional maps, the increase in dimensionality with neuron number may seem unbounded. This offers new insights for interpreting large-scale neural activity data recorded under various techniques.</p>
<p>Manley et al. (<xref ref-type="bibr" rid="c54">54</xref>) found that, unlike in our study, neural activity dimensionality in head-fixed, spontaneously behaving mice did not saturate. They used shared variance component analysis (SVCA) and noted that PCA-based estimates often show dimensionality saturation, which is consistent with our findings. We intentionally chose PCA in our study for several reasons. First, PCA is a trusted and widely used method in neuroscience, proven to uncover meaningful patterns in neural data. Second, its mathematical properties are well understood, making it particularly suitable for our theoretical analysis. Although newer methods such as SVCA might offer valuable insights, we believe PCA remains the most appropriate method for our research questions.</p>
<p>Itâs important to note that the scale invariance of dimensionality and covariance spectrum are distinct phenomena with different underlying requirements. Dimensionality invariance relies on finite mean square covariance, causing saturation at large sample sizes. In contrast, spectral invariance requires a slow-decaying correlation kernel (small <italic>Âµ</italic>) and/or a high-dimensional functional space (large <italic>d</italic>). Although both features appear in our data, they result from distinct mechanisms. A neural system could show saturating dimensionality without spectral invariance if it has finite mean square covariance but rapidly decaying correlations with functional distance. Understanding these requirements clarifies how neural organization affects different scale-invariant properties.</p>
</sec>
<sec id="s3d">
<title>Computational benefits of a scale-invariant covariance spectrum</title>
<p>Our findings are validated across multiple datasets obtained through various recording techniques and animal models, ranging from single-neuron calcium imaging in larval zebrafish to single-neuron multi-electrode recordings in the mouse brain (see <xref rid="figS2" ref-type="fig">Fig. S2</xref>). The conclusion remains robust when the multi-electrode recording data are reanalyzed under different sampling rates (6 Hz - 24 Hz, <xref rid="figS24" ref-type="fig">Fig. S24</xref>). We also confirm that substituting a few negative covariances with zero retains the spectrum of the data covariance matrix (<xref rid="figS18" ref-type="fig">Fig. S18</xref> and Methods).</p>
<p>The scale invariance of neural activity across different neuron assembly sizes could support efficient multiscale information encoding and processing. This indicates that the neural code is robust and requires minimal adjustments despite changes in population size. One recent study shows that randomly sampled and coarse-grained macrovoxels can predict population neural activity (<xref ref-type="bibr" rid="c55">55</xref>), reinforcing that a random neuron subset may capture overall activity patterns. This enables downstream circuits to readout and process information through random projections (<xref ref-type="bibr" rid="c27">27</xref>). A recent study demonstrates that a scale-invariant noise covariance spectrum with a specific slope <italic>Î±</italic> &lt; 1 enables neurons to convey unlimited stimulus information as the population size increases (<xref ref-type="bibr" rid="c56">56</xref>). The linear Fisher information, in this context, grows at least as <italic>N</italic> <sup>1â<italic>Î±</italic></sup>.</p>
<p>Understanding how dimensionality and spectrum change with sample size also suggests the possibility of extrapolating from small samples to overcome experimental limitations. This is particularly feasible when <italic>Âµ</italic>/<italic>d</italic> â 0, where the dimensionality and spectrum under anatomical, random, and functional sampling coincide (<xref ref-type="disp-formula" rid="eqn3">Equations (3)</xref> and (<xref ref-type="bibr" rid="c4">4</xref>)). Developing extrapolation methods and exploring the benefits of scale-invariant neural code are promising future research directions.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Materials and Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Experimental methods</title>
<p>The handling and care of the zebrafish complied with the guidelines and regulations of the Animal Resources Center of the University of Science and Technology of China (USTC). All larval zebrafish (huc:h2b -GCaMP6f) were raised in E2 embryo medium (comprising 7.5 mM NaCl, 0.25 mM KCl, 0.5 mM MgSO<sub>4</sub>, 0.075 mM KH<sub>2</sub>PO<sub>4</sub>, 0.025 mM Na<sub>2</sub>HPO<sub>4</sub>, 0.5 mM CaCl<sub>2</sub>, and 0.35 mM NaHCO<sub>3</sub>; containing 0.5 mg/L methylene blue) at 28.5 Â°C and with a 14-h light and 10-h dark cycle.</p>
<p>To induce hunting behavior (composed of motor sequences like eye convergence and J turn) in larval zebrafish, we fed them a large amount of paramecia over a period of 4-5 days post-fertilization (dpf). The animals were then subjected to a 24-hour starvation period, after which they were transferred to a specialized experimental chamber. The experimental chamber was 20mm in diameter and 1mm in depth, and the head of each zebrafish was immobilized by applying 2% low melting point agarose. The careful removal of the agarose from the eyes and tail of the fish ensured that these body regions remained free to move during hunting behavior. Thus, characteristic behavioral features such as J-turns and eye convergence could be observed and analyzed. Subsequently, the zebrafish were transferred to an incubator and stayed overnight. At 7 dpf, several paramecia were introduced in front of the previously immobilized animals, each of which was monitored by a stereomicroscope. Those displaying binocular convergence were selected for subsequent Ca<sup>2+</sup> imaging experiments.</p>
<p>We developed a novel optomagnetic system that allows (<xref ref-type="bibr" rid="c1">1</xref>) precise control of the trajectory of the paramecium and (<xref ref-type="bibr" rid="c2">2</xref>) imaging brain-wide Ca<sup>2+</sup> activity during the hunting behavior of zebrafish. To control the movement of the paramecium, we treated these microorganisms with a suspension of ferric tetroxide for 30 minutes and selected those that responded to its magnetic attraction. A magnetic paramecium was then placed in front of a selected larva, and its movement was controlled by changing the magnetic field generated by Helmholtz coils that were integrated into the imaging system. The real-time position of the paramecium, captured by an infrared camera, was identified by online image processing. The positional vector relative to a predetermined target position was calculated. The magnitude and direction of the current in the Helmholtz coils were adjusted accordingly, allowing for precise control of the magnetic field and hence the movement of the paramecium. Multiple target positions could be set to drive the paramecium back and forth between multiple locations.</p>
<p>The experimental setup consisted of head-fixed larval zebrafish undergoing two different types of behavior: induced hunting behavior by a moving paramecium in front of a fish (fish 1-5), and spontaneous behavior without any visual stimulus as a control (fish 6). Experiments were carried out at ambient temperature (ranging from 23Â°C to 25Â°C). The behavior of the zebrafish was monitored by a high-speed infrared camera (Basler acA2000-165umNIR, 0.66x) behind a 4F optical system and recorded at 50 Hz. Brain-wide Ca<sup>2+</sup> imaging was achieved using XLFM. Light-field images were acquired at 10 Hz, using customized LabVIEW software (National Instruments, USA) or Solis software (Oxford Instruments, UK), with the help of a high-speed data acquisition card (PCIe-6321, National Instruments, USA) to synchronize the fluorescence with behavioral imaging.</p>
<sec id="s4a1">
<label>4.1.1</label>
<title>Behavior analysis</title>
<p>The background of each behavior video was removed using the clone stamp tool in Adobe Photoshop CS6. Individual images were then processed by an adaptive thresholding algorithm, and fish head and yolk were selected manually to determine the head orientation. The entire body centerline, extending from head to tail, was divided into 20 segments. The amplitude of a bending segment was defined as the angle between the segment and the head orientation. To identify the paramecium in a noisy environment, we subtracted a background image, averaged over a time window of 100 s, from all the frames. The major axis of the left or right eye was identified using DeepLabCut (<xref ref-type="bibr" rid="c57">57</xref>). The eye orientation was defined as the angle between the rostrocaudal axis and the major axis of an eye; The convergence angle was defined as the angle between the major axes of the left and right eyes. An eye-convergence event was defined as a period of time where the angle between the long axis of the eyes stayed above 50 degrees (<xref ref-type="bibr" rid="c47">47</xref>).</p>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Table of notations.</title></caption>
<graphic xlink:href="529673v5_tblS1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s4a2">
<label>4.1.2</label>
<title>Imaging data acquisition and processing</title>
<p>We used a fast eXtended light field microscope (XLFM, with a volume rate of 10 Hz) to record Ca<sup>2+</sup> activity throughout the brain of head-fixed larval zebrafish. Fish were ordered by the dates of experiments. As previously described (<xref ref-type="bibr" rid="c30">30</xref>), we adopted the Richardson-Lucy deconvolution method to iteratively reconstruct 3D fluorescence stacks (600 Ã 600 Ã 250) from the acquired 2D images (2048 Ã 2048). This algorithm requires an experimentally measured point spread function (PSF) of the XLFM system. The entire recording for each fish is 15.3Â±4.3 min (meanÂ±SD).</p>
<p>To perform image registration and segmentation, we first cropped and resized the original image stack to 400 Ã 308 Ã 210, which corresponded to the size of a standard zebrafish brain (zbb) atlas (<xref ref-type="bibr" rid="c58">58</xref>). This step aimed to reduce substantial memory requirements and computational costs in subsequent operations. Next, we picked a typical volume frame and aligned it with the zbb atlas using a basic 3D affine transformation. This transformed frame was used as a template. We aligned each volume with the template using rigid 3D intensity-based registration (<xref ref-type="bibr" rid="c59">59</xref>) and non-rigid pairwise registration (<xref ref-type="bibr" rid="c60">60</xref>) in the Computational Morphometry Toolkit (CMTK) (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/cmtk/">https://www.nitrc.org/projects/cmtk/</ext-link>). After voxel registration, we computed the pairwise correlation between nearby voxel intensities and performed the watershed algorithm on the correlation map to cluster and segment voxels into consistent ROIs across all volumes. We defined the diameter of each ROI using the maximum Feret diameter (the longest distance between any two voxels within a single ROI).</p>
<p>Finally, we adopted the âOASISâ deconvolution method to denoise and infer neural activity from the fluorescence time sequence (<xref ref-type="bibr" rid="c61">61</xref>). The deconvolved Î<italic>F</italic>/<italic>F</italic> of each ROI was used to infer firing rates for subsequent analysis.</p>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Table S2.</label>
<caption><title>Resources for additional experimental datasets</title></caption>
<graphic xlink:href="529673v5_tblS2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Other experimental datasets analyzed</title>
<p>To validate our findings across different recording methods and animal models, we also analyzed three additional datasets. We include a brief description below for completeness. Further details can be found in the respective reference. The first dataset includes whole-brain light-sheet Ca<sup>2+</sup> imaging of immobilized larval zebrafish in the presence of visual stimuli as well as in a spontaneous state (<xref ref-type="bibr" rid="c33">33</xref>). Each volume of the brain was scanned through 2.11 Â± 0.21 planes per sec, providing a near-simultaneous readout of neuronal Ca<sup>2+</sup> signals. We analyzed fish 8 (69,207 neurons Ã 7,890 frames), 9 (79,704 neurons Ã 7,720 frames) and 11 (101,729 neurons Ã 8,528 frames), which are the first three fish data with more than 7,200 frames. For simplicity, we labeled them l2, l3, and l1(fl). The second dataset consists of Neuropixels recordings from approximately ten different brain areas in mice during spontaneous behavior (<xref ref-type="bibr" rid="c23">23</xref>). Data from the three mice, <italic>Kerbs, Robbins</italic>, and <italic>Waksman</italic>, include the firing rate matrices of 1,462 neurons Ã 39,053 frames, 2,296 neurons Ã 66,409 frames, and 2,688 neurons Ã 74,368 frames, respectively. The last dataset comprises two-photon Ca<sup>2+</sup> imaging data (2-3 Hz) obtained from the visual cortex of mice during spontaneous behavior. While this dataset includes numerous animals, we focused on the first three animals that exhibited spontaneous behavior: <monospace>spont_M150824_MP019_2016-04-05</monospace> (11,983 neurons Ã 21,055 frames), <monospace>spont_M160825_MP027_2016-12-12</monospace> (11,624 neurons Ã 23,259 frames), and <monospace>spont_M160907_MP028_2016-09-26</monospace> (9,392 neurons Ã 10,301 frames) (<xref ref-type="bibr" rid="c23">23</xref>).</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Covariance matrix, eigenspectrum and sampling procedures</title>
<p>To begin, we multiplied the inferred firing rate of each neuron (see <xref ref-type="sec" rid="s4a2">section 4.1.2</xref>) by a constant such that in the resulting activity trace a<sub><italic>i</italic></sub>, the mean of <italic>a</italic><sub><italic>i</italic></sub><italic>(t) over the nonzero time frames</italic> equaled one (<xref ref-type="bibr" rid="c20">20</xref>). Consistent with the literature (<xref ref-type="bibr" rid="c20">20</xref>), this step aimed to eliminate possible confounding factors in the raw activity traces, such as the heterogeneous expression level of the fluorescence protein within neurons and the non-linear conversion of the electrical signal to Ca<sup>2+</sup> concentration. Note that after this scaling, neurons could still have different activity levels characterized by the variance of <italic>a</italic><sub><italic>i</italic></sub><italic>(t)</italic> over time, due to differences in the sparsity of activity (proportion of nonzero frames) and the distribution of nonzero <italic>a</italic><sub><italic>i</italic></sub><italic>(t)</italic> values. Without normalization, the covariance matrix becomes nearly diagonal, causing significant underestimation of the covariance structures.</p>
<p>The three models of covariance in <xref rid="fig2" ref-type="fig">Fig. 2G-I</xref> were constructed as follows. For model in <xref rid="fig2" ref-type="fig">Fig. 2G</xref>, the entries of matrix <italic>G</italic> (with dimensions <italic>N</italic> Ã <italic>T</italic>) were sampled from an i.i.d. Gaussian distribution with zero mean and standard deviation <italic>Ï</italic> = 1. In <xref rid="fig2" ref-type="fig">Fig. 2H</xref>, we constructed the composite covariance matrix for fish 1 achieved by maintaining the eigenvalues from the fish 1 data covariance matrix and replacing the eigenvectors <italic>U</italic> with a set of random orthonormal basis. Lastly, the covariance matrix in <xref rid="fig2" ref-type="fig">Fig. 2I</xref> was generated from a randomly connected recurrent network of linear rate neurons. The entries in the synaptic weight matrix are normally distributed with <italic>J</italic><sub><italic>ij</italic></sub> â¼ ð© (0, g<sup>2</sup>/<italic>N</italic>), with a coupling strength <italic>g</italic> = 0.95 (<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>). For consistency, we used the same number of time frames <italic>T</italic> = 7, 200 when comparing CI across all the datasets (<xref rid="fig4" ref-type="fig">Fig. 4BC</xref>, <xref rid="fig5" ref-type="fig">Fig. 5DE</xref>, <xref rid="figS6" ref-type="fig">Fig. S6C</xref>). For other cases, we analyzed the full length of the data (number of time frames: fish 1 - 7495, fish 2 - 9774, fish 3 - 13904, fish 4 - 7318, fish 5 - 7200, fish 6 - 9388). Next, the covariance matrix was calculated as <inline-formula><inline-graphic xlink:href="529673v5_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="529673v5_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the mean of <italic>a</italic><sub><italic>i</italic></sub><italic>(t)</italic> over time. Finally, to visualize covariance matrices on a common scale, we multiplied matrix C by a constant such that the average of its diagonal entries equaled one, that is, Tr(C)/<italic>N</italic> = 1. This scaling did not alter the shape of covariance eigenvalue distribution, but set the mean at 1 (see also <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>).</p>
<p>To maintain consistency across data sets, we fixed the same initial number of neurons at <italic>N</italic><sub>0</sub> = 1, 024. These <italic>N</italic><sub>0</sub> neurons were randomly chosen once for each zebrafish dataset and then used throughout the subsequent analyses. We adopted this setting for all analyses except in two particular instances: (<xref ref-type="bibr" rid="c1">1</xref>) for comparisons among the three sampling methods (RSap, ASap, and FSap), we specifically chose 1,024 neurons centered along the anterior-posterior axis, mainly from the midbrain to the anterior hindbrain regions (<xref rid="fig5" ref-type="fig">Fig. 5DE</xref>, <xref rid="figS20" ref-type="fig">Fig. S20</xref>). (<xref ref-type="bibr" rid="c2">2</xref>) When investigating the impact of hunting behavior on scale invariance, we included the entire neuronal population (<xref ref-type="sec" rid="s4k">section 4.11</xref>).</p>
<p>We used an iterative procedure to sample the covariance matrix <italic>C</italic> (calculated from data or as simulated ERMs). For RSap, in the first iteration, we randomly selected half of the neurons. The covariance matrix for these selected neurons was a <italic>N</italic>/2 Ã <italic>N</italic>/2 diagonal block of <italic>C</italic>. Similarly, the covariance matrix of the unselected neurons was another diagonal block of the same size. In the next iteration, we similarly created two new sampled blocks with half the number of neurons for each of the blocks we had. Repeating this process for n iterations resulted in 2<sup>n</sup> blocks, each containing <italic>N</italic> := <italic>N</italic><sub>0</sub>/2<sup>n</sup> neurons. At each iteration, the eigenvalues of each block were calculated and averaged across the blocks after being sorted in descending order. Finally, the averaged eigenvalues were plotted against rank/<italic>N</italic> on a log-log scale.</p>
<p>In the case of ASap and FSap, the process of selecting neurons was different, although the remaining procedures followed the RSap protocol. In ASap, the selection of neurons was based on a spatial criterion: neurons close to the anterior end on the anterior-posterior axis were grouped to create a diagonal block of size <inline-formula><inline-graphic xlink:href="529673v5_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with the remaining neurons forming a separate block. FSap, on the other hand, used the Renormalization Group (RG) framework (<xref ref-type="bibr" rid="c20">20</xref>) to define the blocks (details in <xref ref-type="sec" rid="s4l">section 4.12</xref>). In each iteration, the cluster of neurons within a block that showed the highest average correlation <inline-formula><inline-graphic xlink:href="529673v5_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was identified and labeled as the most correlated cluster (refer to <xref rid="fig5" ref-type="fig">Fig. 5D</xref>, <xref rid="figS20" ref-type="fig">Figures S20</xref> and <xref rid="figS21" ref-type="fig">S21</xref>).</p>
<p>In the ERM model, as part of implementing ASap, we generated anatomical and functional coordinates for neurons with a specified CCA properties as described in <xref ref-type="sec" rid="s4i">section 4.9</xref>. Mirroring the approach taken with our data, ASap segmented neurons into groups based on the first dimension of their anatomical coordinates, akin to the anterier-posterior axis. FSap employed the same RG procedures outlined earlier (<xref ref-type="sec" rid="s4l">section 4.12</xref>).</p>
<p>To determine the overall power-law coefficient of the eigenspectra, <italic>Î±</italic>, throughout sampling, we fitted a straight line in the log-log rank plot to the large eigenvalues that combined the original and three iterations of sampled covariance matrices (selecting the top 10% eigenvalues for each matrix and excluding the first four largest ones for each matrix). We averaged the estimated <italic>Î±</italic> over 10 repetitions of the entire sampling procedure. <italic>R</italic><sup>2</sup> of the power-law fit was computed in a similar way. To visualize the statistical structures of the original and sampled covariance matrices, the orders of the neurons (i.e. columns and rows) are determined by the following algorithm. We first construct a symmetric Toeplitz matrix ð¯, with entries ð¯<sub><italic>i,j</italic></sub> = <italic>t</italic><sub><italic>i</italic>â<italic>j</italic></sub> and <italic>t</italic><sub><italic>i</italic>â<italic>j</italic></sub> â¡ <italic>t</italic><sub><italic>j</italic>â<italic>i</italic></sub>. The vector <inline-formula><inline-graphic xlink:href="529673v5_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is equal to the mean covariance vector of each neuron calculated below. Let <inline-formula><inline-graphic xlink:href="529673v5_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula> be a row vector of the data covariance matrix; we identify <inline-formula><inline-graphic xlink:href="529673v5_inline42.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>D</italic>(Â·) denotes a numerical ordering operator, namely rearranging the elements in a vector <inline-formula><inline-graphic xlink:href="529673v5_inline43.gif" mime-subtype="gif" mimetype="image"/></inline-formula> such that <italic>c</italic><sub>0</sub> â¥ <italic>c</italic><sub>1</sub> â¦ â¥ <italic>c</italic><sub>Nâ1</sub>. The second step is to find a permutation matrix <italic>P</italic> such that âð¯ â PCP<sup>T</sup> â<sub>F</sub> is minimized, where â â<sub>F</sub> denotes the Frobenius norm. This quadratic assignment problem is solved by simulated annealing. Note that after sampling, the smaller matrix will appear different from the larger one. We need to perform the above reordering algorithm for every sampled matrix so that matrices of different sizes become similar in <xref rid="fig2" ref-type="fig">Fig. 2E</xref>.</p>
<p>The composite covariance matrix with substituted eigenvectors in (<xref rid="fig2" ref-type="fig">Fig. 2H</xref>) was created as described in the following steps. First, we generated a random orthogonal matrix <italic>U</italic><sub><italic>r</italic></sub> (based on the Haar measure) for the new eigenvectors. This was achieved by QR decomposition <italic>A</italic> = <italic>U</italic><sub><italic>r</italic></sub><italic>R</italic> of a random matrix <italic>A</italic> with i.i.d. entries <italic>A</italic><sub><italic>ij</italic></sub> â¼ ð© (0, 1/<italic>N</italic>). The composite covariance matrix <italic>C</italic><sub><italic>r</italic></sub> was then defined as <inline-formula><inline-graphic xlink:href="529673v5_inline44.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where Î is a diagonal matrix that contains the eigenvalues of <italic>C</italic>. Note that since all the eigenvalues are real and <italic>U</italic><sub><italic>r</italic></sub> is orthogonal, the resulting <italic>C</italic><sub><italic>r</italic></sub> is a real and symmetric matrix. By construction, <italic>C</italic><sub><italic>r</italic></sub> and <italic>C</italic> have the same eigenvalues, but their <italic>sampled</italic> eigenspectra can differ.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Dimensionality</title>
<p>In this section, we introduce the Participation Ratio (<italic>D</italic><sub>PR</sub>) as a metric for effective dimensionality of a system, based on (25â29, 62). <italic>D</italic><sub>PR</sub> is defined as:
<disp-formula id="eqn5">
<graphic xlink:href="529673v5_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here, <italic>Î»</italic><sub><italic>i</italic></sub> are the eigenvalues of the covariance matrix <italic>C</italic>, representing variances of neural activities. Tr(<italic>Â·</italic>) denotes the trace of the matrix. The term <inline-formula><inline-graphic xlink:href="529673v5_inline45.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes the expected value of the squared elements that lie off the main diagonal of <italic>C</italic>. This represents the average squared covariance between the activities of distinct pairs of neurons.</p>
<p>With these definitions, we explore the asymptotic behavior of <italic>D</italic><sub>PR</sub> as the number of neurons <italic>N</italic> approaches infinity:
<disp-formula id="ueqn1">
<graphic xlink:href="529673v5_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>This limit highlights the relationship between the PR dimension and the average squared covariance among different pairs of neurons. To predict how <italic>D</italic><sub>PR</sub> scales with the number of neurons (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>), we first estimated these statistical quantities <inline-formula><inline-graphic xlink:href="529673v5_inline46.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and E(<italic>Ï</italic><sup>4</sup>)) using all available neurons, then applied <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> for different values of <italic>N</italic>. It is worth mentioning that a similar theoretical finding is established by Dahmen et. al. (<xref ref-type="bibr" rid="c29">29</xref>). The transition from increasing <italic>D</italic><sub>PR</sub> with <italic>N</italic> to approaching the saturation point occurs when <italic>N</italic> is significantly larger than <italic>D</italic><sub>PR</sub>.</p>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>ERM model</title>
<p>We consider the eigenvalue distribution or spectrum of the matrix <italic>C</italic> at the limit of <italic>N</italic> â« 1 and <italic>L</italic> â« 1. This spectrum can be analytically calculated in both high-density and intermediate-density scenarios using the replica method (<xref ref-type="bibr" rid="c34">34</xref>). The following sketch shows our approach, and detailed derivations can be found in Supp. Note. To calculate the probability density function of the eigenvalues (or eigendensity), we first compute the resolvent or Stieltjes transform <inline-formula><inline-graphic xlink:href="529673v5_inline47.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Here â¨â¦â© is the average across the realizations of <italic>C</italic> (that is, random <inline-formula><inline-graphic xlink:href="529673v5_inline48.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline49.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). The relationship between the resolvent and the eigendensity is given by the Sokhotski-Plemelj formula:
<disp-formula id="eqn6">
<graphic xlink:href="529673v5_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where Im means imaginary part.</p>
<p>Here we follow the field-theoretic approach (<xref ref-type="bibr" rid="c34">34</xref>), which turns the problem of calculating the resolvent to a calculation of the partition function in statistical physics by using the replica method. In the limit <italic>N</italic> â â, <italic>L</italic><sup><italic>d</italic></sup> â â, <italic>Ï</italic> being finite, by performing a leading order expansion of the canonical partition function at large <italic>z</italic> (Supp. Note), we find the resolvent is given by
<disp-formula id="eqn7">
<graphic xlink:href="529673v5_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>In the <italic>high-density</italic> regime, the probability density function (pdf) of the covariance eigenvalues can be approximated and expressed from <xref ref-type="disp-formula" rid="eqn6">Equations (6)</xref> and <xref ref-type="disp-formula" rid="eqn7">(7)</xref> using the Fourier transform of the kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline50.gif" mime-subtype="gif" mimetype="image"/></inline-formula>:
<disp-formula id="eqn8">
<graphic xlink:href="529673v5_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>Î´</italic>(<italic>x</italic>) is the Dirac delta function and E(<italic>Ï</italic><sup>2</sup>) is the expected value of the variances of neural activity. Intuitively, <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> means that <italic>Î»/Ï</italic> are distributed with a density proportional to the area of <inline-formula><inline-graphic xlink:href="529673v5_inline51.gif" mime-subtype="gif" mimetype="image"/></inline-formula>â level sets (i.e., isosurfaces).</p>
<p>In <xref ref-type="sec" rid="s2c">section 2.3</xref>, we found that the covariance matrix consistently shows greater scale invariance compared to the correlation matrix across all datasets. This suggests that the variability in neuronal activity significantly influences the eigenspectrum. This finding, however, cannot be explained by the high-density theory, which predicts that the eigenspectrum of the covariance matrix is simply a rescaling of the correlation eigenspectrum by <inline-formula><inline-graphic xlink:href="529673v5_inline52.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the expected value of the variances of neural activity. Without loss of generality, we can always standardize the fluctuation level of neural activity by setting E(<italic>Ï</italic><sup>2</sup>) = 1. This is equivalent to multiplying the covariance matrix <italic>C</italic> by a constant such that Tr(<italic>C</italic>)<italic>/N</italic> = 1, which in turn scales all the eigenvalues of <italic>C</italic> by the same factor. Consequently, the heterogeneity of <inline-formula><inline-graphic xlink:href="529673v5_inline53.gif" mime-subtype="gif" mimetype="image"/></inline-formula> has no effect on the scale invariance of the eigenspectrum (see <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>). This theoretical prediction is indeed correct and is confirmed by direct numerical simulations and quantifying the scale invariance using the CI (<xref rid="figS6" ref-type="fig">Fig. S6A</xref>).</p>
<p>Fortunately, the inconsistency between theory and experimental results can be resolved by focusing the ERM within the intermediate density regime <italic>ÏÏµ</italic><sup><italic>d</italic></sup> âª 1, where neurons are positioned at a moderate distance from each other. As mentioned above, we set E(<italic>Ï</italic><sup>2</sup>) = 1 in our model and vary the diversity of activity fluctuations among neurons represented by E(<italic>Ï</italic><sup>4</sup>). Consistent with the experimental observations, we find that the CI decreases with E(<italic>Ï</italic><sup>4</sup>) (see <xref rid="figS6" ref-type="fig">Fig. S6B</xref>). This agreement indicates that the neural data are better explained by the ERM in the intermediate density regime.</p>
<p>To gain a deeper understanding of this behavior, we use the Gaussian variational method (<xref ref-type="bibr" rid="c34">34</xref>) to calculate the eigenspectrum. Unlike the high-density theory where the eigendensity has an explicit expression, in the intermediate density the resolvent <italic>g</italic>(<italic>z</italic>) no longer has an explicit expression and is given by the following equation
<disp-formula id="eqn9">
<graphic xlink:href="529673v5_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where â¨â¦â©<sub><italic>Ï</italic></sub> computes the expectation value of the term within the bracket with respect to <italic>Ï</italic>, namely â¨â¦â©<sub><italic>Ï</italic></sub> â¡â¦<italic>p</italic>(<italic>Ï</italic>)d<italic>Ï</italic>. Here and in the following, we denote <inline-formula><inline-graphic xlink:href="529673v5_inline54.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The function <inline-formula><inline-graphic xlink:href="529673v5_inline55.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is determined by a self-consistent equation,
<disp-formula id="eqn10">
<graphic xlink:href="529673v5_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We can solve <inline-formula><inline-graphic xlink:href="529673v5_inline56.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref> numerically and below is an outline, and the details are explained in Supp. Note. Let us define the integral <inline-formula><inline-graphic xlink:href="529673v5_inline57.gif" mime-subtype="gif" mimetype="image"/></inline-formula> First, we substitute <italic>z Î»</italic> + <italic>iÎ·</italic> into <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref> and write ð¢ = <bold>Re</bold>ð¢ +<italic>i</italic><bold>Im</bold>ð¢. <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref> can thus be decomposed into its real part and imaginary part, and a set of nonlinear and integral equations, each of which involves both <bold>Re</bold><italic>ð¢</italic> and <bold>Im</bold><italic>ð¢</italic>. We solve these equations at the limit <italic>Î·</italic> â 0 using a fixed-point iteration that alternates between updating <bold>Re</bold><italic>ð¢</italic> and <bold>Im</bold><italic>ð¢</italic> until convergence.</p>
<p>We find that the variational approximations exhibit excellent agreement with the numerical simulation for both large and intermediate <italic>Ï</italic> where the high-density theory starts to deviate significantly (for <italic>Ï</italic> = 256 and <italic>Ï</italic> = 10.24, <italic>Ïµ</italic> = 0.03125, <xref rid="figS3" ref-type="fig">Fig. S3</xref>). Note that the departure of the leading eigenvalues in these plots is expected, since the power-law kernel function we use is not integrable (see <xref ref-type="sec" rid="s4f">section 4.6</xref>).</p>
<p>To elucidate the connection between the two different methods, we estimate the condition when the result of the high-density theory (<xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>) matches that of the variational method (<xref ref-type="disp-formula" rid="eqn9">Equations (9)</xref> and <xref ref-type="disp-formula" rid="eqn10">(10)</xref>) (Supp. Note). The transition between these two density regimes can also be understood (see section 4.8.1 and Supp. Note).</p>
<p>Importantly, the scale invariance of the spectrum at <italic>Âµ/d</italic> â 0 previously derived using the high-density result (<xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>) can be extended to the intermediate-density regime by proving the <italic>Ï</italic>-independence using the variational method (Supp. Note).</p>
<p>Finally, using the variational method and the integration limit estimated by simulation (see section 4.7.2), we show that the heterogeneity of the variance of neural activity, quantified by E(<italic>Ï</italic><sup>4</sup>), indeed improves the collapse of the eigenspectra for intermediate <italic>Ï</italic> (Supp. Note). Our theoretical results agree excellently with the ERM simulation (<xref rid="figS6" ref-type="fig">Fig. S6A, B</xref>).</p>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Kernel function</title>
<p>Throughout the paper, we have mainly considered a particular approximate power-law kernel function inspired by the Studentâs t distribution (section 2.2)
<disp-formula id="eqn11">
<graphic xlink:href="529673v5_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>To understand how to choose <italic>Ïµ</italic> and <italic>Âµ</italic>, see section 4.8.1. Variations of <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> near <italic>x</italic> = 0 have also been explored; see a summary in <xref ref-type="table" rid="tblS3">table S3</xref>.</p>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Table S3.</label>
<caption><title>Modifications of the shape of <inline-formula><inline-graphic xlink:href="529673v5_inline348.gif" mime-subtype="gif" mimetype="image"/></inline-formula> near <inline-formula><inline-graphic xlink:href="529673v5_inline349.gif" mime-subtype="gif" mimetype="image"/></inline-formula> used in <xref rid="figS7" ref-type="fig">Fig. S7</xref>, <xref rid="figS8" ref-type="fig">Fig. S8</xref> and <xref rid="figS9" ref-type="fig">Fig. S9</xref>.</title>
<p>Flat: when <inline-formula><inline-graphic xlink:href="529673v5_inline350.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,. Tangent: when <inline-formula><inline-graphic xlink:href="529673v5_inline351.gif" mime-subtype="gif" mimetype="image"/></inline-formula> follows a tangent line of the exact power law (<inline-formula><inline-graphic xlink:href="529673v5_inline352.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline353.gif" mime-subtype="gif" mimetype="image"/></inline-formula> have a same first-order derivative when <inline-formula><inline-graphic xlink:href="529673v5_inline354.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). b and c are constants. Tent: when <inline-formula><inline-graphic xlink:href="529673v5_inline355.gif" mime-subtype="gif" mimetype="image"/></inline-formula> follows a straight line while the slope is not the same as the tangent case. Parabola: when <inline-formula><inline-graphic xlink:href="529673v5_inline356.gif" mime-subtype="gif" mimetype="image"/></inline-formula> follows a quadratic function (ax<sup>2</sup> + 1 and <inline-formula><inline-graphic xlink:href="529673v5_inline357.gif" mime-subtype="gif" mimetype="image"/></inline-formula> have same first-order derivative). t pdf: mimic the smoothing treatment like the t distribution. All the constant parameters are set such that f (0) = 1.</p></caption>
<graphic xlink:href="529673v5_tblS3.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>It is worth mentioning that a power law is not the only slow decaying function that can produce a scale-invariant covariance spectrum (<xref rid="figS5" ref-type="fig">Fig. S5</xref>). We choose it for its analytical tractability in calculating the eigenspectrum. Importantly, we find numerically that the two contributing factors to scale invariance â namely, slow spatial decay and higher functional space â can be generalized to other <italic>nonpower-law</italic> functions. An example is the stretched exponential function <inline-formula><inline-graphic xlink:href="529673v5_inline58.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with 0 <italic>&lt; Î· &lt;</italic> 1. When <italic>Î·</italic> is small and <italic>d</italic> is large, the covariance eigenspectra also display a similar collapse upon random sampling (<xref rid="figS5" ref-type="fig">Fig. S5</xref>).</p>
<p>This approximate power-law <inline-formula><inline-graphic xlink:href="529673v5_inline59.gif" mime-subtype="gif" mimetype="image"/></inline-formula> has the advantage of having an analytical expression for its Fourier transform, which is crucial for the high-density theory (<xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>),
<disp-formula id="eqn12">
<graphic xlink:href="529673v5_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here <italic>K</italic><sub><italic>Î±</italic></sub>(<italic>x</italic>) is the modified Bessel function of the second kind, and Î(<italic>x</italic>) is the Gamma function. We calculated the above formulas analytically for <italic>d</italic> = 1, 2, 3 with the assistance of Mathematica and conjectured the case for general dimension <italic>d</italic>, which we confirmed numerically for <italic>d</italic> â¤ 10.</p>
<p>We want to explain two technical points relevant to the interpretation of our numerical results and the choice of <inline-formula><inline-graphic xlink:href="529673v5_inline60.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Unlike the case in the usual ERM, here we allow <inline-formula><inline-graphic xlink:href="529673v5_inline61.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to be non-integrable (over â<sup><italic>d</italic></sup>), which is crucial to allow power law <inline-formula><inline-graphic xlink:href="529673v5_inline62.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The nonintegrability violates a condition in the classical convergence results of the ERM spectrum (<xref ref-type="bibr" rid="c63">63</xref>) as <italic>N â â</italic>. We believe that this is exactly the reason for the departure of the first few eigenvalues from our theoretical spectrum (e.g., in <xref rid="fig3" ref-type="fig">Fig. 3</xref>). Our hypothesis is also supported by ERM simulations with integrable <inline-formula><inline-graphic xlink:href="529673v5_inline63.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref rid="figS4" ref-type="fig">Fig. S4</xref>), where the numerical eigenspectrum matches closely with our theoretical one, including the leading eigenvalues. For ERM to be a legitimate model for covariance matrices, we need to ensure that the resulting matrix <italic>C</italic> is positive semidefinite. According to the Bochner theorem (<xref ref-type="bibr" rid="c64">64</xref>), this is equivalent to the Fourier transform (FT) of the kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline64.gif" mime-subtype="gif" mimetype="image"/></inline-formula> being nonnegative for all frequencies. For example, in 1D, a rectangle function <inline-formula><inline-graphic xlink:href="529673v5_inline65.gif" mime-subtype="gif" mimetype="image"/></inline-formula> does not meet the condition (its FT is <inline-formula><inline-graphic xlink:href="529673v5_inline66.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, but a tent function tent<inline-formula><inline-graphic xlink:href="529673v5_inline67.gif" mime-subtype="gif" mimetype="image"/></inline-formula> otherwise does (its FT is sinc<sup>2</sup>(<italic>x</italic>)). For the particular kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline68.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>, this condition can be easily verified using the analytical expressions of its Fourier transform (<xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref>). The integral expression for <italic>K</italic><sub><italic>Î±</italic></sub>(<italic>x</italic>), given as <inline-formula><inline-graphic xlink:href="529673v5_inline69.gif" mime-subtype="gif" mimetype="image"/></inline-formula> shows that <italic>K</italic><sub><italic>Î±</italic></sub>(<italic>x</italic>) is positive for all <italic>x &gt;</italic> 0. Likewise, the Gamma function Î(<italic>x</italic>) <italic>&gt;</italic> 0. Therefore, the Fourier transform of <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> is positive and the resultingmatrix <italic>C</italic> (of any size and values of <inline-formula><inline-graphic xlink:href="529673v5_inline70.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) is guaranteed to be positive definite.</p>
<p>Building upon the theory outlined above, numerical simulations further validated the empirical robustness of our ERM model, as showcased in <xref rid="fig3" ref-type="fig">Fig. 3B-D</xref> and <xref rid="fig4" ref-type="fig">Fig. 4A</xref>. In <xref rid="fig3" ref-type="fig">Fig. 3B-D</xref>, the ERM was characterized by the parameters <italic>N</italic> = 1024, <italic>d</italic> = 2, <italic>L</italic> = 10, <italic>Ï</italic> = 10.24 and <italic>Âµ</italic> = 0.5 and <italic>Ïµ</italic> = 0.03125 for <inline-formula><inline-graphic xlink:href="529673v5_inline71.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. To numerically compute the eigenvalue probability density function, we generated the ERM 100 times, each sampled using the method described in section 4.3. The probability density function (pdf) was computed by calculating the pdf of each ERM realization and averaging these across the instances. The curves in <xref rid="fig3" ref-type="fig">Fig. 3D</xref> showed the average of over 100 ERM simulations. The shaded area (most of which is smaller than the marker size) represented the SEM. For <xref rid="fig4" ref-type="fig">Fig. 4A</xref>, the columns from left to right were corresponded to <italic>Âµ</italic> = 0.5, 0.9, 1.3, and the rows from top to bottom were corresponded to <italic>d</italic> = 1, 2, 3. Other ERM simulation parameters: <italic>N</italic> = 4096, <italic>Ï</italic> = 256, <italic>L</italic> = (<italic>N/Ï</italic>)<sup>1<italic>/d</italic></sup>, <italic>Ïµ</italic> = 0.03125 and <inline-formula><inline-graphic xlink:href="529673v5_inline72.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. It should be noted that for <xref rid="fig4" ref-type="fig">Fig. 4A</xref>, the presented data pertain to a single ERM realization.</p>
</sec>
<sec id="s4g">
<label>4.7</label>
<title>Collapse index (CI)</title>
<p>We quantify the extent of scale invariance using CI defined as the area between two spectrum curves (<xref rid="fig4" ref-type="fig">Fig. 4A</xref> upper right), providing an intuitive measure of the shift of the eigenspectrum when varying the number of sampled neurons. We chose the CI over other measures of distance between distributions for several reasons. First, it directly quantifies the shift of the eigenspectrum, providing a clear and interpretable measure of scale invariance. Second, unlike methods that rely on estimating the full distribution, the CI avoids potential inaccuracies in estimating the probability of the top leading eigenvalues. Finally, the use of CI is motivated by theoretical considerations, namely the ERM in the high-density regime, which provides an analytical expression for the covariance spectrum (<xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>) valid for large eigenvalues.
<disp-formula id="eqn13">
<graphic xlink:href="529673v5_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
we set <italic>q</italic><sub>1</sub> such that <italic>Î»</italic>(<italic>q</italic><sub>1</sub>) = 1, which is the mean of the eigenvalues of a normalized covariance matrix. The other integration limit <italic>q</italic><sub>0</sub> is set to 0.01 such that <italic>Î»</italic>(<italic>q</italic><sub>0</sub>) is the 1% largest eigenvalue.</p>
<p>Here we provide numerical details on calculating CI for the ERM simulations and experimental data.</p>
<sec id="s4g1">
<label>4.7.1</label>
<title>A calculation of collapse index for experimental datasets/ERM model</title>
<p>To calculate CI for a covariance matrix <italic>C</italic> of size <italic>N</italic><sub>0</sub>, we first computed its eigenvalues <inline-formula><inline-graphic xlink:href="529673v5_inline73.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and those of the sampled block <italic>C</italic><sub><italic>s</italic></sub> of size <italic>N</italic><sub><italic>s</italic></sub> = <italic>N</italic><sub>0</sub><italic>/</italic>2, denoted as <inline-formula><inline-graphic xlink:href="529673v5_inline74.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (averaged over 20 times for the ERM simulation and 2000 times in experimental data). Next, we estimated log<italic>Î»</italic>(<italic>q</italic>) using the eigenvalues of <italic>C</italic><sub>0</sub> and <italic>C</italic><sub><italic>s</italic></sub> at <italic>q</italic> = <italic>i/N</italic><sub><italic>s</italic></sub>, <italic>i</italic> = 1, 2,â¦, <italic>N</italic><sub><italic>s</italic></sub>. For the sampled <italic>C</italic><sub><italic>s</italic></sub>, we simply had <inline-formula><inline-graphic xlink:href="529673v5_inline75.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, its <italic>i</italic>-th largest eigenvalue. For the original <italic>C</italic><sub>0</sub>, log<italic>Î»</italic>(<italic>q</italic> = <italic>i/N</italic><sub><italic>s</italic></sub>) was estimated by a linear interpolation, <italic>on the</italic> log<italic>Î»-</italic>log<italic>q scale</italic>, using the value of log<italic>Î»</italic>(<italic>q</italic>) in the nearest neighboring <italic>q</italic> = <italic>i/N</italic><sub>0</sub>âs (which again are simply <inline-formula><inline-graphic xlink:href="529673v5_inline76.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). Finally, the integral (<xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>) was computed using the trapezoidal rule, discretized at <italic>q</italic> = <italic>i/N</italic><sub><italic>s</italic></sub>â s, using the finite difference <inline-formula><inline-graphic xlink:href="529673v5_inline77.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where Î denotes the difference between the original eigenvalues of <italic>C</italic><sub>0</sub> and those of sampled <italic>C</italic><sub><italic>s</italic></sub>.</p>
</sec>
<sec id="s4g2">
<label>4.7.2</label>
<title>Estimating CI using the variational method</title>
<p>In the definition of CI (<xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>, calculating <italic>Î»</italic>(<italic>q</italic>) and <inline-formula><inline-graphic xlink:href="529673v5_inline78.gif" mime-subtype="gif" mimetype="image"/></inline-formula> directly using the variational method is difficult, but we can make use of an implicit differentiation
<disp-formula id="eqn14">
<graphic xlink:href="529673v5_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline79.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the complementary cdf (the inverse function of <italic>Î»</italic>(<italic>q</italic>) in section 4.7.1). Using this, the integral in CI (<xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>) can be rewritten as
<disp-formula id="eqn15">
<graphic xlink:href="529673v5_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Since <inline-formula><inline-graphic xlink:href="529673v5_inline80.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we switch the order of the integration interval in the final expression of <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>.</p>
<p>First, we explain how to compute the complementary cdf <italic>q</italic>(<italic>Î»</italic>) numerically using the variational method. The key is to integrate the probability density function <italic>p</italic>(<italic>Î»</italic>) from <italic>Î»</italic> to a finite <italic>Î»</italic>(<italic>q</italic><sub><italic>s</italic></sub>) rather than to infinity,
<disp-formula id="eqn16">
<graphic xlink:href="529673v5_eqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The integration limit <italic>Î»</italic>(<italic>q</italic><sub><italic>s</italic></sub>) cannot be calculated directly using the variational method. We thus used the value of <italic>Î»</italic><sup><italic>s</italic></sup>(<italic>q</italic><sub><italic>s</italic></sub> â <italic>q</italic><sub>0</sub>) (section 4.7) from simulations of the ERM with a large <italic>N</italic> = 1024 as an approximation. Furthermore, we employed a smoothing technique to reduce bias in the estimation of <italic>Î»</italic><sup><italic>s</italic></sup>(<italic>q</italic><sub><italic>s</italic></sub>) due to the leading zigzag eigenvalues (i.e., the largest eigenvalues) of the eigenspectrum. Specifically, we determined the nearest rank <italic>j</italic> &lt; <italic>N</italic><sub><italic>q</italic>0</sub> and then smoothed the eigenvalue log Î»<sup><italic>s</italic></sup>(<italic>q</italic><sub><italic>s</italic></sub>) on the log-log scale using the formula <inline-formula><inline-graphic xlink:href="529673v5_inline81.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="529673v5_inline82.gif" mime-subtype="gif" mimetype="image"/></inline-formula> averaging over 100 ERM simulations.</p>
<p>Note that we can alternatively use the high-density theory (Supp. Note) to compute the integration limit <italic>Î»</italic>(<italic>q</italic><sub><italic>s</italic></sub> = 1<italic>/N</italic>) instead of resorting to simulations. However, since the true value deviates from the <italic>Î»</italic><sup><italic>h</italic></sup>(<italic>q</italic><sub><italic>s</italic></sub> = 1<italic>/N</italic>) derived from high-density theory, this approach introduces a constant bias (<xref rid="figS6" ref-type="fig">Fig. S6</xref>) when computing the integral in <xref ref-type="disp-formula" rid="eqn16">Eq. (16)</xref>. Therefore we used the simulation value <italic>Î»</italic><sup><italic>s</italic></sup>(<italic>q</italic><sub><italic>s</italic></sub> â <italic>q</italic><sub>0</sub>) when producing <xref rid="figS6" ref-type="fig">Fig. S6AB</xref>.</p>
<p>Next, we describe how each term within the integral of <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> was numerically estimated. First, we calculated <inline-formula><inline-graphic xlink:href="529673v5_inline83.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with a similar method described in section 4.7.1. Briefly, we calculated <italic>q</italic><sub>0</sub>(<italic>Î»</italic>) for density <inline-formula><inline-graphic xlink:href="529673v5_inline84.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <italic>q</italic><sub><italic>s</italic></sub>(<italic>Î»</italic>) for density <inline-formula><inline-graphic xlink:href="529673v5_inline85.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and then used the finite difference <inline-formula><inline-graphic xlink:href="529673v5_inline86.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Second, <inline-formula><inline-graphic xlink:href="529673v5_inline87.gif" mime-subtype="gif" mimetype="image"/></inline-formula> was evaluated at <inline-formula><inline-graphic xlink:href="529673v5_inline88.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>i</italic> = 0, 1, 2,â¦, <italic>k</italic> â 1, and we used <italic>k</italic> = 20. Finally, we performed a cubic spline interpolation of the term <inline-formula><inline-graphic xlink:href="529673v5_inline89.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and obtained the theoretical CI by an integration of <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref>. <xref rid="figS6" ref-type="fig">Fig. S6A,B</xref> shows a comparison between theoretical CI and that obtained by numerical simulations of ERM (section 4.7.1).</p>
</sec>
</sec>
<sec id="s4h">
<label>4.8</label>
<title>Fitting ERM to data</title>
<sec id="s4h1">
<label>4.8.1</label>
<title>Estimating the ERM parameters</title>
<p>Our ERM model has 4 parameters: <italic>Âµ</italic> and <italic>Ïµ</italic> dictate the kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline90.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, whereas the box size <italic>L</italic> and the embedding dimension <italic>d</italic> determine the neuronal density <italic>Ï</italic>. In the following, we describe an approximate method to estimate these parameters from pairwise correlations measured experimentally <inline-formula><inline-graphic xlink:href="529673v5_inline91.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We proceed by deriving a relationship between the correlation probability density distribution <italic>h</italic>(<italic>R</italic>) and the pairwise distance probability density distribution <inline-formula><inline-graphic xlink:href="529673v5_inline92.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the functional space, from which the parameters of the ERM can be estimated.</p>
<p>Consider a distribution of neurons in the functional space with a coordinate distribution <inline-formula><inline-graphic xlink:href="529673v5_inline93.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The pairwise distance density function <italic>g</italic>(<italic>u</italic>) is related to the spatial point density by the following formula:
<disp-formula id="eqn17">
<graphic xlink:href="529673v5_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>For ease of notation, we subsequently omit the region of integration, which is the same as here. In the case of a uniform distribution, <inline-formula><inline-graphic xlink:href="529673v5_inline94.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For other spatial distributions, <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> cannot be explicitly evaluated. We therefore make a similar approximation by focusing on a small pairwise distance (i.e., large correlation):
<disp-formula id="eqn18">
<graphic xlink:href="529673v5_eqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>By a change of variables:
<disp-formula id="ueqn2">
<graphic xlink:href="529673v5_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> can be rewritten as
<disp-formula id="eqn19">
<graphic xlink:href="529673v5_eqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>S</italic><sub><italic>d</italic>â1</sub>(<italic>u</italic>) is the surface area of <italic>d</italic> â 1 sphere with radius <italic>u</italic>. Note that the approximation of <italic>g</italic>(<italic>u</italic>) is not normalized to 1, as <xref ref-type="disp-formula" rid="eqn19">Eq. (19)</xref> provides an approximation valid only for small pairwise distances (i.e., large correlation). Therefore, we believe this does not pose an issue.</p>
<p>With the approximate power-law kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline95.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the probability density function of pairwise correlation <italic>h</italic>(<italic>R</italic>) is given by:
<disp-formula id="eqn20">
<graphic xlink:href="529673v5_eqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Taking the logarithm on both sides
<disp-formula id="eqn21">
<graphic xlink:href="529673v5_eqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><xref ref-type="disp-formula" rid="eqn21">Eq. (21)</xref> is the key formula for ERM parameters estimation. In the case of a uniform spatial distribution, <inline-formula><inline-graphic xlink:href="529673v5_inline96.gif" mime-subtype="gif" mimetype="image"/></inline-formula>For a given dimension <italic>d</italic>, we can therefore estimate <italic>Âµ</italic> and (<italic>Ïµ/L</italic>)<sup><italic>d</italic></sup> separately by fitting <italic>h</italic>(<italic>R</italic>) on the log-log scale using the linear least squares. Lastly, we fit the distribution of <italic>Ï</italic><sup>2</sup> (the diagonal entries of the covariance matrix <italic>C</italic>) to a log-normal distribution by estimating the maximum likelihood.</p>
<p>There is a redundancy between the unit of the functional space (using a rescaled <italic>Ïµ</italic><sub><italic>Î´</italic></sub> â¡ <italic>Ïµ/Î´</italic>) and the unit of <inline-formula><inline-graphic xlink:href="529673v5_inline97.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (using a rescaled <inline-formula><inline-graphic xlink:href="529673v5_inline98.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, thus <italic>Ïµ</italic> and <italic>L</italic> are a pair of redundant parameters: once <italic>Ïµ</italic> is given, <italic>L</italic> is also determined. We set <italic>Ïµ</italic> = 0.03125 throughout the article. In summary, for a given dimension <italic>d</italic> and <italic>Ïµ, Âµ</italic> of <inline-formula><inline-graphic xlink:href="529673v5_inline99.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>), the distribution of <italic>Ï</italic><sup>2</sup> (section 2.2) and <italic>Ï</italic> (or equivalently <italic>L</italic>) (section 2.2) can be fitted by comparing the distribution of pairwise correlations in experimental data and ERM. Furthermore, knowing (<italic>Ïµ/L</italic>)<sup><italic>d</italic></sup> enables us to determine <italic>a fundamental dimensionless parameter</italic>
<disp-formula id="ueqn3">
<graphic xlink:href="529673v5_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which tells us whether the experimental data are better described by the high-density theory or the Gaussian variational method (Supp. Note). Indeed, the fitted <italic>ÏÏµ</italic><sup><italic>d</italic></sup> â½ 10<sup>â3</sup> â 10<sup>0</sup> is much smaller than 1, consistent with our earlier conclusion that neural data are better described by an ERM model in the intermediate-density regime.</p>
<p>Notably, we found that a smaller embedding dimension <italic>d</italic> â¤ 5 gave a better fit to the overall pairwise correlation distribution. The following is an empirical explanation. As <italic>d</italic> grows, to best fit the slope of log<italic>h</italic>(<italic>R</italic>) â log <italic>R, Âµ</italic> will also grow. However, for very high dimensions <italic>d</italic>, the y-intercept would become very negative, or equivalently, the fitted correlation would become extremely small. This can be verified by examining the leading order log <italic>R</italic> independent term in <xref ref-type="disp-formula" rid="eqn21">Eq. (21)</xref> which can be approximated as <inline-formula><inline-graphic xlink:href="529673v5_inline100.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. It becomes very negative for larg <italic>d</italic> since <italic>Ïµ</italic> âª<italic>L</italic> by construction. Throughout this article, we use <italic>d</italic> = 2 when fitting the experimental data with our ERM model.</p>
<p>The above calculation can be extended to the cases where the coordinate distribution <inline-formula><inline-graphic xlink:href="529673v5_inline101.gif" mime-subtype="gif" mimetype="image"/></inline-formula> becomes dependent on other parameters. To estimate the parameters in coordinate distributions that can generate ERMs with a similar pairwise correlation distribution (<xref rid="figS9" ref-type="fig">Fig. S9</xref>), we fixed the integral value <inline-formula><inline-graphic xlink:href="529673v5_inline102.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Consider, for example, a transformation of the uniform coordinate distribution to the normal distribution <inline-formula><inline-graphic xlink:href="529673v5_inline103.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in â<sup>2</sup>. We imposed <inline-formula><inline-graphic xlink:href="529673v5_inline104.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For the log-normal distribution, a similar calculation led to <inline-formula><inline-graphic xlink:href="529673v5_inline105.gif" mime-subtype="gif" mimetype="image"/></inline-formula> The numerical values for these parameters are shown in section 4.10. However, note that due to the approximation we used (<xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>), our estimate of the ERM parameters becomes less accurate if the density function <inline-formula><inline-graphic xlink:href="529673v5_inline106.gif" mime-subtype="gif" mimetype="image"/></inline-formula> changes rapidly over a short distance in the functional space. More sophisticated methods, such as grid search, may be needed to tackle such a scenario.</p>
<p>After determining the parameters of the ERM, we first examine the spectrum of the ERM with uniformly distributed random functional coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline107.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref rid="figS10" ref-type="fig">Fig. S10M-R</xref>). Second, we use <inline-formula><inline-graphic xlink:href="529673v5_inline108.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to translate experimental pairwise correlations into pairwise distances for all neurons in the functional space (<xref rid="figS11" ref-type="fig">Fig. S11</xref>, <xref rid="figS10" ref-type="fig">Fig. S10G-L</xref>). The embedding coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline109.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the functional space can then be solved through Multidimensional Scaling (MDS) by minimizing the Sammon error (section 4.8.3). The similarity between the spectra of the uniformly distributed coordinates (<xref rid="figS10" ref-type="fig">Fig. S10M-R</xref>) and those of the embedding coordinates (<xref rid="figS10" ref-type="fig">Fig. S10G-L</xref>) is also consistent with the notion that specific coordinate distributions in the functional space have little impact on the shape of the eigenspectrum (<xref rid="figS9" ref-type="fig">Fig. S9</xref>).</p>
</sec>
<sec id="s4h2">
<label>4.8.2</label>
<title>Nonnegativity of data covariance</title>
<p>To use ERM to model the covariance matrix, the pairwise correlation is given by a <italic>non-negative</italic> kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline110.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that monotonically decreases with the distance between neurons in the functional space. This nonnegativeness brings about a potential issue when applied to experimental data, where, in fact, a small fraction of pairwise correlations/covariances are negative. We have verified that the spectrum of the data covariance matrix (<xref rid="figS18" ref-type="fig">Fig. S18</xref>) remains virtually unchanged when replacing these negative covariances with zero (<xref rid="figS18" ref-type="fig">Fig. S18</xref>). This confirms that the ERM remains a good model when the neural dynamics is in a regime where pairwise covariances are mostly positive (<xref ref-type="bibr" rid="c51">51</xref>) (see also <xref rid="figS2" ref-type="fig">Fig. S2B</xref>, <xref rid="figS2" ref-type="fig">Fig. S2B-D</xref>).</p>
</sec>
<sec id="s4h3">
<label>4.8.3</label>
<title>Multidimensional Scaling (MDS)</title>
<p>With the estimated ERM parameters (<italic>Âµ</italic> in <inline-formula><inline-graphic xlink:href="529673v5_inline111.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the box size <italic>L</italic> for given <italic>Ïµ</italic> and <italic>d</italic>, see section 4.8.1), we performed MDS to infer neuronal coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline112.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in functional space. First, we computed a pairwise correlation <inline-formula><inline-graphic xlink:href="529673v5_inline113.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from the data covariances. Next, we calculated the pairwise distance, denoted by <inline-formula><inline-graphic xlink:href="529673v5_inline114.gif" mime-subtype="gif" mimetype="image"/></inline-formula> by computing the inverse function of <inline-formula><inline-graphic xlink:href="529673v5_inline115.gif" mime-subtype="gif" mimetype="image"/></inline-formula>with respect to the absolute value <inline-formula><inline-graphic xlink:href="529673v5_inline116.gif" mime-subtype="gif" mimetype="image"/></inline-formula> We used the absolute value | <italic>R</italic><sub><italic>ij</italic></sub> | instead of <italic>R</italic><sub><italic>ij</italic></sub> as a small percentage of <italic>R</italic><sub><italic>ij</italic></sub> are negative (<xref rid="figS2" ref-type="fig">Fig. S2A-D</xref>) where the distance is undefined. This substitution by the absolute value serves as a simple workaround for the issue and is only used here in the analysis to infer the neuronal coordinates by MDS. Finally, we estimated the embedding coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline117.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for each neuron by the SMACOF algorithm (Scaling by MAjorizing a COmplicated Function), which minimizes the Sammon error
<disp-formula id="eqn22">
<graphic xlink:href="529673v5_eqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline118.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the pairwise distance in the embedding space calculated above.</p>
<p>To reduce errors at large distances (i.e., small correlations with <italic>R</italic><sub><italic>ij</italic></sub> <italic>&lt; f</italic>(<italic>L</italic>), where <italic>L</italic> is the estimated box size), we performed a soft cut-off at a large distance:
<disp-formula id="eqn23">
<graphic xlink:href="529673v5_eqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>During the optimization process, we started at the embedding coordinates estimated by the classical MDS (<xref ref-type="bibr" rid="c46">46</xref>), with an initial sum of squares distance error that can be calculated directly, and ended with an error or its gradient smaller than 10<sup>â4</sup>.</p>
<p>The fitted ERM with the embedding coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline119.gif" mime-subtype="gif" mimetype="image"/></inline-formula> reproduced the experimental covariance matrix including the cluster structures (<xref rid="figS11" ref-type="fig">Fig. S11</xref>) and its sampling eigenspectra (<xref rid="figS10" ref-type="fig">Fig. S10</xref>).</p>
</sec>
</sec>
<sec id="s4i">
<label>4.9</label>
<title>Canonical-Correlation Analysis (CCA)</title>
<p>Here we briefly explain the CCA method (<xref ref-type="bibr" rid="c65">65</xref>) for completeness. The basis vectors <inline-formula><inline-graphic xlink:href="529673v5_inline120.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline121.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, in functional and anatomical space, respectively, were found by maximizing the correlation <inline-formula><inline-graphic xlink:href="529673v5_inline122.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. These basis vectors satisfy the condition that the projections of the neuron coordinates along them, <inline-formula><inline-graphic xlink:href="529673v5_inline123.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline124.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, are maximally correlated among all possible choices of <inline-formula><inline-graphic xlink:href="529673v5_inline125.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline126.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Here <inline-formula><inline-graphic xlink:href="529673v5_inline127.gif" mime-subtype="gif" mimetype="image"/></inline-formula> represent the coordinates in functional and anatomical spaces, respectively. The resulting maximum correlation is <italic>R</italic><sub>CCA</sub>. To check the significance of the canonical correlation, we shuffled the functional space coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline128.gif" mime-subtype="gif" mimetype="image"/></inline-formula> across neuronsâ identity and re-calculated the canonical correlation with the anatomical coordinates, as shown in <xref rid="figS13" ref-type="fig">Fig. S13</xref>.</p>
<p>To study the effect of functional-anatomical relation described by <italic>R</italic><sub>CCA</sub> in the ERM model, we generated three dimensional anatomical coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline129.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and two dimensional functional coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline130.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for each neuron which are jointly five-dimensional zero-mean multivariate Gaussian random variables. The coordinates are independent among each other, except for the first dimension <inline-formula><inline-graphic xlink:href="529673v5_inline131.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of the functional coordinates and the first dimension <inline-formula><inline-graphic xlink:href="529673v5_inline132.gif" mime-subtype="gif" mimetype="image"/></inline-formula> which are assigned to have a correlation coefficient equals to <italic>R</italic><sub>CCA</sub>. The variances of the coordinates are <inline-formula><inline-graphic xlink:href="529673v5_inline133.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline134.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the numerics in <xref rid="figS21" ref-type="fig">Fig. S21</xref>. Under this construction, the first canonical correlation between the anatomical and functional coordinates equals <italic>R</italic><sub>CCA</sub>, and the first canonical direction <inline-formula><inline-graphic xlink:href="529673v5_inline135.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the anatomical space is (1, 0, 0)<sup><italic>T</italic></sup> and the first canonical direction <inline-formula><inline-graphic xlink:href="529673v5_inline136.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the functional space is (1, 0)<sup><italic>T</italic></sup>.</p>
</sec>
<sec id="s4j">
<label>4.10</label>
<title>Extensions of ERM and factors not affecting the scale invariance</title>
<p>In <xref rid="figS9" ref-type="fig">Fig. S9</xref> we considered five additional types of spatial density distributions (coordinate distributions) in functional space and two additional functional space geometries. We examined the points distributed according to the uniform distribution <inline-formula><inline-graphic xlink:href="529673v5_inline137.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the normal distribution <inline-formula><inline-graphic xlink:href="529673v5_inline138.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the log-normal distribution <inline-formula><inline-graphic xlink:href="529673v5_inline139.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We used the method described in Methods section 4.8.1 to adjust the parameters of the coordinate distributions based on the uniform distribution case, so that they all generate similar pairwise correlation distributions. The relationships between these parameters are described in Methods section 4.8.1. In <xref rid="figS9" ref-type="fig">Fig. S9B</xref>, we used the following parameters: <italic>d</italic> = 2; <italic>L</italic> = 10 for the uniform distribution; <italic>Âµ</italic><sub><italic>p</italic></sub> = 0, <italic>Ï</italic><sub><italic>p</italic></sub> = 2.82 for the normal distribution; and <italic>Âµ</italic><sub><italic>p</italic></sub> = 2, <italic>Ï</italic><sub><italic>p</italic></sub> = 0.39 for the log-normal distribution.</p>
<p>Second, we introduced multiple clusters of neurons in the functional space, with each cluster uniformly distributed in a box. We considered three arrangements: (<xref ref-type="bibr" rid="c1">1</xref>) two closely situated clusters (with a box size of <inline-formula><inline-graphic xlink:href="529673v5_inline140.gif" mime-subtype="gif" mimetype="image"/></inline-formula>the distance between two cluster centers being <italic>L</italic><sub><italic>c</italic></sub> = <italic>L</italic>), (<xref ref-type="bibr" rid="c2">2</xref>) two distantly situated clusters (with a box size of <inline-formula><inline-graphic xlink:href="529673v5_inline141.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the distance between clusters <italic>L</italic><sub><italic>c</italic></sub> = 4<italic>L</italic>), and three clusters arranged symmetrically in an equilateral triangle (with a box size of <inline-formula><inline-graphic xlink:href="529673v5_inline142.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the distance between clusters <italic>L</italic><sub><italic>c</italic></sub> = <italic>L</italic>).</p>
<p>Finally, we examined the scenario in which the points were uniformly distributed on the surface of a sphere (4<italic>Ïl</italic><sup>2</sup> = <italic>L</italic><sup>2</sup>, <italic>l</italic> being the radius of the sphere) or a hemisphere (2<italic>Ïl</italic><sup>2</sup> = <italic>L</italic><sup>2</sup>) embedded in â <sup>3</sup> (the pairwise distance is that in â <sup>3</sup>). It should be noted that both cases have the same surface area as the 2D box.</p>
</sec>
<sec id="s4k">
<label>4.11</label>
<title>Analyzing the effects of removing neural activity data during hunting</title>
<p>To identify and remove the time frames corresponding to putative hunting behaviors, the following procedure was used. The hunting interval was defined as 10 frames (1 sec) preceding the onset of an eye convergence (see Methods section 4.1.1) to 10 frames after the offset of this eye convergence. These frames were then excluded from the data before recalculating the covariance matrix (see Methods section 4.3) and subsequently the sampled eigenspectra (<xref rid="figS15" ref-type="fig">Fig. S15B</xref>, <xref rid="figS16" ref-type="fig">Fig. S16B,D,F,H</xref>). As a control to the removal of the hunting frame, an equal number of time frames that are not within those hunting intervals were randomly selected and then removed and analyzed (<xref rid="figS15" ref-type="fig">Fig. S15C</xref>, <xref rid="figS16" ref-type="fig">Fig. S16A,C,E,G</xref>). The number of hunting interval frames and total recording frames for five fish exhibiting hunting behaviors are as follows: fish 1 - 268/7495, fish 2 - 565/9774, fish 3 - 2734/13904, fish 4 - 843/7318 and fish 5 - 1066/7200. Fish 6 (number of time frames: 9388) was not exposed to a prey stimulus and, therefore, was excluded from the analysis.</p>
<p>To assess the impact of hunting removal on CI, we calculated the CI of the covariance matrix using all neurons recorded in each fish (without sampling to 1024 neurons). For the control case, we repeated the removal of the nonhunting frame 10 times to generate 10 covariance matrices and computed their CIs. We used a one-sample t-test to determine the level of statistical significance between the control CIs and the CI obtained after removal of the hunting frame.</p>
<p>Using fitted ERM parameters by full data, we performed a MDS on the control data and hunting-removed data to infer the functional coordinates. Note that the functional coordinates inferred by MDS are not unique: rotations and translations give equivalent solutions. For visualization purposes (not needed for analysis), we first used the Umeyama algorithm to optimally align the functional coordinates of control and hunting-removed data.</p>
<p>To identify distinct clusters within the functional coordinates, we fit Gaussian Mixture Models (GMMs) using the âGaussianMixturesâ package in Julia. We chose the number of clusters <italic>K</italic> based on giving the smallest Bayesian Information Criterion (BIC) score. After fitting the GMMs, a list of probabilities <italic>p</italic><sub><italic>ik</italic></sub>, <italic>k</italic> = 1, 2,â¦, <italic>K</italic> was given for each neuron <italic>i</italic> specifying the probability of the neuron belonging to the cluster <italic>k</italic>. The mean and covariance parameters were estimated for each Gaussian distributed cluster. For visualization (but not for analysis), a neuron was colored according to cluster <italic>k</italic><sup>*</sup> where <italic>k</italic><sup>*</sup> = arg max<sub>1â¤<italic>k</italic>â¤<italic>K</italic></sub> <italic>p</italic><sub><italic>ik</italic></sub>.</p>
<p>We used the following method to measure the size of the cluster and its fold change. For a 2D (recall <italic>d</italic> = 2 in our ERM) Gaussian distributed cluster, let us consider an ellipse centered on its mean, and its axes are aligned with the eigenvectors of its covariance matrix <italic>C</italic><sub>2<italic>Ã</italic>2</sub>. Let the eigenvalues of <italic>C</italic> be <italic>Î»</italic><sub>1</sub>, <italic>Î»</italic><sub>2</sub>. Then we set the length of the half-axis of the ellipse to be <inline-formula><inline-graphic xlink:href="529673v5_inline143.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, respectively. Here <italic>c &gt;</italic> 0 is a constant determined below. Note that the ellipse axes correspond to linear combinations of 2D Gaussian random variables that are independent and <italic>Î»</italic><sub><italic>i</italic></sub>âs are the variance of these linear combinations. From this fact, it is straightforward to show that the probability that a sample from the Gaussian cluster lies in the above ellipse depends only on <italic>c</italic>, that is, <inline-formula><inline-graphic xlink:href="529673v5_inline144.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and not on the shape of the cluster. So, the ellipse represents a region that covers a fixed proportion of neurons for any cluster, and its area can be used as a measure for the size of the Gaussian cluster. Note that the area of the ellipse is <inline-formula><inline-graphic xlink:href="529673v5_inline145.gif" mime-subtype="gif" mimetype="image"/></inline-formula> In <xref rid="figS17" ref-type="fig">Fig. S17</xref>, we plot the ellipses to help visualize the clusters and their changes. We choose <italic>c</italic> such that the ellipse covers 95% of the probability (that is, the fraction of neurons belonging to the cluster).</p>
<p>In the control functional map where we fit the GMMs, we directly calculated the size measure <inline-formula><inline-graphic xlink:href="529673v5_inline146.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from the estimated covariance <italic>C</italic> for each Gaussian cluster. In the hunting-removed functional map, we needed to estimate the covariance <italic>C</italic><sup><italic>â²</italic></sup> for neurons belonging to a cluster <italic>k</italic> under the new coordinates (we assume that the new distribution can still be approximated by a Gaussian distribution). We performed this estimation in a probabilistic manner to avoid issues of highly overlapping clusters where the cluster membership could be ambiguous for some neurons. First, we estimated the center/mean of the new Gaussian distribution by
<disp-formula id="ueqn4">
<graphic xlink:href="529673v5_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here the summation goes over all the <italic>N</italic> neurons in the functional space and <italic>p</italic><sub><italic>ik</italic></sub> is the membership probability defined above, and (<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>) is the coordinate of neuron <italic>i</italic> in the hunting-removed map. Similarly, we can use a weighted average to estimate the entries in the covariance matrix <inline-formula><inline-graphic xlink:href="529673v5_inline147.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For example,
<disp-formula id="ueqn5">
<graphic xlink:href="529673v5_ueqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Then we calculated the size of the cluster on the new map as <inline-formula><inline-graphic xlink:href="529673v5_inline148.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Finally, we computed the fold change in size as <inline-formula><inline-graphic xlink:href="529673v5_inline149.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
</sec>
<sec id="s4l">
<label>4.12</label>
<title>Renormalization-Group (RG) Approach</title>
<p>Here we briefly summarize the RG approach used in (<xref ref-type="bibr" rid="c20">20</xref>) and elucidate the adjustments required when applying the RG approach to ERM. The method consists of two stages: (i) iterative agglomerate clustering of neurons, and (ii) computing the spectrum of a block of the <italic>original</italic> covariance matrix corresponding to a cluster of the desired size based on the previous clustering result.</p>
<sec id="s4l1">
<title>4.12.1Stage (i): Iterative Clustering</title>
<p>We begin with <italic>N</italic><sub>0</sub> neurons, where <italic>N</italic><sub>0</sub> is assumed to be a power of 2. In the first iteration, we compute Pearsonâs correlation coefficients for all neuron pairs. We then search greedily for the most correlated pairs and group the half pairs with the highest correlation into the first cluster; the remaining neurons form the second cluster. For each pair (<italic>a, b</italic>), we define a coarse-grained variable according to:
<disp-formula id="eqn24">
<graphic xlink:href="529673v5_eqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline150.gif" mime-subtype="gif" mimetype="image"/></inline-formula> normalizes the average to ensure unit nonzero activity. This process reduces the number of neurons to <italic>N</italic><sub>1</sub> = <italic>N</italic><sub>0</sub><italic>/</italic>2. In subsequent iterations, we continue grouping the most correlated pairs of the coarse-grained neurons, iteratively reducing the number of neurons by half at each step. This process continues until the desired level of coarse-graining is achieved.</p>
<p>When applying the RG approach to ERM, instead of combining neural activity, we merge correlation matrices to traverse different scales. During the <italic>k</italic>th iteration, we compute the coarse-grained covariance as:
<disp-formula id="eqn25">
<graphic xlink:href="529673v5_eqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and the variance as:
<disp-formula id="eqn26">
<graphic xlink:href="529673v5_eqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Following these calculations, we normalize the coarse-grained covariance matrix to ensure that all variances are equal to one. Note that these coarse-grained covariances are only used in stage (i) and not used to calculate the spectrum.</p>
</sec>
<sec id="s4l2">
<label>4.12.2</label>
<title>Stage (ii): Eigenspectrum Calculation</title>
<p>The calculation of eigenspectra at different scales proceeds through three sequential steps. First, for each cluster identified in Stage (i), we compute the covariance matrix using the original firing rates of neurons within that cluster (not the coarse-grained activities). Second, we calculate the eigenspectrum for each cluster. Finally, we average these eigenspectra across all clusters at a given iteration level to obtain the representative eigenspectrum for that scale.</p>
<p>In stage (ii), we calculate the eigenspectra of the sub-covariance matrices across different cluster sizes as described in (<xref ref-type="bibr" rid="c20">20</xref>). Let <italic>N</italic><sub>0</sub> = 2<sup><italic>n</italic></sup> be the original number of neurons. To reduce it to size <italic>N</italic> = <italic>N</italic><sub>0</sub><italic>/</italic>2<sup><italic>k</italic></sup> = 2<sup><italic>n</italic>â<italic>k</italic></sup>, where <italic>k</italic> is the kth reduction step, consider the coarse-grained neurons in step <italic>n â k</italic> in stage (i). Each coarse-grained neuron is a cluster of 2<sup><italic>n</italic>â<italic>k</italic></sup> neurons. We then calculate spectrum of the block of the original covariance matrix corresponding to neurons of each cluster (there are 2<sup><italic>k</italic></sup> such blocks). Lastly, an average of these 2<sup><italic>k</italic></sup> spectra is computed.</p>
<p>For example, when reducing from <italic>N</italic><sub>0</sub> = 2<sup>3</sup> = 8 to <italic>N</italic> = 2<sup>3â1</sup> = 4 neurons (<italic>k</italic> = 1), we would have two clusters of 4 neurons each. We calculate the eigenspectrum for each 4Ã4 block of the original covariance matrix, then average these two spectra together. To better understand this process through a concrete example, consider a hypothetical scenario where a set of eight neurons, labeled 1, 2, 3, â¦, 7, 8, are subjected to a two-step clustering procedure. In the first step, neurons are grouped based on their maximum correlation pairs, for example, resulting in the formation of four pairs: {1, 2}, {3, 4}, {5, 6}, and {7, 8} (see <xref rid="figS22" ref-type="fig">Fig. S22</xref>). Subsequently, the neurons are further grouped into two clusters based on the results of the RG step mentioned above. Specifically, if the correlation between the coarse-grained variables of the pair {1, 2} and the pair {3, 4} is found to be the largest among all other pairs of coarse-grained variables, the first group consists of neurons {1, 2, 3, 4}, while the second group contains neurons {5, 6, 7, 8}. Next, take the size of the cluster <italic>N</italic> = 4 for example. The eigenspectra of the covariance matrices of the four neurons within each cluster are computed. This results in two eigenspectra, one for each cluster. The correlation matrices used to compute the eigenspectra of different sizes do not involve coarse-grained neurons. It is the real neurons 1, 2, 3, â¦, 7, 8, but with expanding cluster sizes. Finally, the average of the eigenspectra of the two clusters is calculated.</p>
</sec>
</sec>
<sec id="s4m">
<label>4.13.</label>
<title>Spectrum of three types of sampling procedures in ERM model</title>
<p>In section 2.4 we have considered three types of sampling procedures: random sampling (RSap), spatial sampling in the anatomical space (ASap, e.g., recording neurons in a brain region), and spatial sampling in the functional space (FSap), namely spatial sampling in functional space by subdividing the space into smaller regions, is equivalent to the previously reported renormalization group (RG) inspired process (<xref ref-type="bibr" rid="c66">66</xref>, <xref ref-type="bibr" rid="c67">67</xref>). Here we consider the relationship between the spectrum of three types of sampling procedures.</p>
<p>We assume a uniform random distribution of neurons in a <italic>d</italic>-dimensional functional space, [0, <italic>L</italic>]<sup><italic>d</italic></sup>. For RSap procedures, the resulting neuronal density <italic>Ï</italic><sub><italic>R</italic></sub> is reduced to <italic>Ï</italic><sub><italic>R</italic></sub> = <italic>kÏ</italic><sub>0</sub>, with <italic>k</italic> representing the sampling ratio (<italic>k</italic> = <italic>N/N</italic><sub>0</sub>) and <italic>Ï</italic><sub>0</sub> being the initial density. In contrast, FSap maintains the original density, <italic>Ï</italic><sub><italic>F</italic></sub> = <italic>Ï</italic><sub>0</sub>. This constancy in neuronal density under FSap ensures that the covariance eigenspectrum remains invariant across scales for any spatial correlation functions <inline-formula><inline-graphic xlink:href="529673v5_inline151.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, such as power law and exponential, as shown in <xref rid="figS19" ref-type="fig">Fig. S19A,B,D,E</xref>. In contrast, RSap reduces <italic>Ï</italic>, thus demanding more rigorous conditions to achieve a scale-invariant covariance spectrum (e.g., compare <xref rid="figS19" ref-type="fig">Fig. S19A</xref> and <xref rid="figS19" ref-type="fig">C</xref>).</p>
<p>Under ASap, sampled neurons are not spread out evenly in functional space, whereas our theoretical framework assumes a uniform distribution. To reconcile this discrepancy, we employ a uniform approximation of the neural distribution. This approach involves introducing an effective density, <italic>Ï</italic><sup><italic>â²</italic></sup>, defined as the spatial average of the density function <inline-formula><inline-graphic xlink:href="529673v5_inline152.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This adjustment allows our theoretical model to accommodate non-uniform distributions encountered in anatomically spatial sampling.
<disp-formula id="eqn27">
<graphic xlink:href="529673v5_eqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline153.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the normalized density distribution (see Methods section 4.8.1). using the Cauchy-Schwarz inequality, we have
<disp-formula id="eqn28">
<graphic xlink:href="529673v5_eqn28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
thus <italic>Ï</italic><sup><italic>â²</italic></sup> â¥ <italic>kÏ</italic><sub>0</sub>.</p>
<p>According to the condition <inline-formula><inline-graphic xlink:href="529673v5_inline154.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we have <italic>Ï</italic><sup>â²</sup> <italic>Ï</italic><sub>0</sub>, intuitively, sampling within a uniformly distributed neuron population does not increase the density.</p>
<p>So we have <italic>Ï</italic><sub>0</sub> â¥ <italic>Ï</italic><sup><italic>â²</italic></sup><sub><italic>A</italic></sub> â¥ <italic>kÏ</italic><sub>0</sub>, i.e., <italic>Ï</italic><sub><italic>F</italic></sub> â¥ <italic>Ï</italic><sup><italic>â²</italic></sup><sub><italic>A</italic></sub> â¥ <italic>Ï</italic><sub><italic>R</italic></sub>. Thus the spectrum ASap should be between FSap and RSap.</p>
</sec>
<sec id="s4n">
<label>4.14</label>
<title>Dimensions of three types of sampling procedures in ERM model</title>
<sec id="s4n1">
<label>4.14.1</label>
<title>Scaling of Dimensions through Random Samplings</title>
<p>Let us revisit the definition of the Participation Ratio (PR) dimension as defined in Equation <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>:
<disp-formula id="eqn29">
<graphic xlink:href="529673v5_eqn29.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>During the random sampling process, the expected values <italic>E</italic>(<italic>Ï</italic><sup>2</sup>), <italic>E</italic>(<italic>Ï</italic><sup>4</sup>), and <inline-formula><inline-graphic xlink:href="529673v5_inline155.gif" mime-subtype="gif" mimetype="image"/></inline-formula> remain constant. These constants allow for the estimation of the PR dimension across various scales using:
<disp-formula id="eqn30">
<graphic xlink:href="529673v5_eqn30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here, <italic>k</italic> = <italic>N/N</italic><sub>0</sub> represents a scaling factor (fraction) associated with sampling. The key question is to understand how the dimensionality changes with <italic>k</italic>. Under random sampling, as <italic>k</italic> increases, the dimensionality will quickly approaches a saturating point defined by <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref>.</p>
</sec>
<sec id="s4n2">
<label>4.14.2</label>
<title>Scaling of Dimensions through Functional Sampling</title>
<p>In this section, we leverage the uniform ERM model to estimate dimensions within the context of functional sampling, specifically focusing on the estimation of squared pairwise covariance <inline-formula><inline-graphic xlink:href="529673v5_inline156.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and dimensionality.</p>
<p>Adopting an approximation for a power-law kernel function <italic>f</italic>(<italic>x</italic>) â <italic>Ïµ</italic><sup><italic>Âµ</italic></sup>â¥<italic>x</italic>â¥<sup>â<italic>Âµ</italic></sup> allows us to express the expected value of the squared covariance <inline-formula><inline-graphic xlink:href="529673v5_inline157.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as follows:
<disp-formula id="eqn31">
<graphic xlink:href="529673v5_eqn31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>For a set subjected to functional sampling with a sampling fraction <italic>k</italic>, this procedure adjusts the size of the functional space in the ERM model by a factor of <italic>k</italic><sup>â1<italic>/d</italic></sup>. Consequently, the <inline-formula><inline-graphic xlink:href="529673v5_inline158.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for the sampled fraction <italic>k</italic> is given by:
<disp-formula id="eqn32">
<graphic xlink:href="529673v5_eqn32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here we assume that <italic>E</italic>[<italic>Ï</italic><sup>2</sup>] and <italic>E</italic>[<italic>Ï</italic><sup>4</sup>] are constant across the sampling process. This model enables the estimation of the ratio <italic>Âµ/d</italic> as detailed in the Methods section 4.8.1.
<disp-formula id="eqn33">
<graphic xlink:href="529673v5_eqn33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>In the large <italic>N</italic> limit, we observe distinct behaviors in the evolution of dimensionality in both theory and data: it saturates in RSap (dashed line in <xref rid="fig5" ref-type="fig">Fig. 5D</xref>), namely <inline-formula><inline-graphic xlink:href="529673v5_inline159.gif" mime-subtype="gif" mimetype="image"/></inline-formula> defined in <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref>, whereas it follows a different scaling relationship <inline-formula><inline-graphic xlink:href="529673v5_inline160.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in FSap (solid line in <xref rid="fig5" ref-type="fig">Fig. 5D</xref>).</p>
</sec>
<sec id="s4n3">
<label>4.14.3</label>
<title>Comparative Analysis of PR Dimension Across sampling Techniques</title>
<p>This section examines the behavior of the Participation Ratio (PR) dimension under three sampling techniques: anatomical sampling, random sampling, and functional sampling. We show that the average PR dimension following anatomical sampling occupies a middle ground between the extremes presented by random and functional sampling.</p>
<p>The PR dimension, denoted <italic>D</italic><sub>PR</sub>, reflects the sampling impact and depends on the distribution <inline-formula><inline-graphic xlink:href="529673v5_inline161.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of the functional coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline162.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Defining the sampling fraction as <italic>k</italic> = 1<italic>/q</italic>, the mean <italic>D</italic><sub>PR</sub> is represented as:
<disp-formula id="eqn34">
<graphic xlink:href="529673v5_eqn34.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the neuron set 1, 2, â¦, <italic>N</italic> is segmented into <italic>q</italic> clusters <inline-formula><inline-graphic xlink:href="529673v5_inline163.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, each comprising <inline-formula><inline-graphic xlink:href="529673v5_inline164.gif" mime-subtype="gif" mimetype="image"/></inline-formula> neurons. The probability distribution <inline-formula><inline-graphic xlink:href="529673v5_inline165.gif" mime-subtype="gif" mimetype="image"/></inline-formula> corresponds to each cluster <inline-formula><inline-graphic xlink:href="529673v5_inline166.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The probability distribution for each cluster, <inline-formula><inline-graphic xlink:href="529673v5_inline167.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, emerges naturally from the sampling process.</p>
<p>The equivalence of the mean probability density function across the sampled clusters to the original setâs probability density function leads us to the condition:
<disp-formula id="eqn35">
<graphic xlink:href="529673v5_eqn35.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>This condition is a direct consequence of the sampling process, ensuring that the aggregated probability density function of all sampled sets mirrors the overall density distribution of the neurons.</p>
<p>Applying the Lagrange multiplier method to optimize the mean <italic>D</italic><sub>PR</sub>:
<disp-formula id="eqn36">
<graphic xlink:href="529673v5_eqn36.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here <italic>L</italic>(<italic>p, Î»</italic>) is the Lagrangian, <inline-formula><inline-graphic xlink:href="529673v5_inline168.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the Lagrange multiplier, we derive the optimal condition:
<disp-formula id="eqn37">
<graphic xlink:href="529673v5_eqn37.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
yielding:
<disp-formula id="eqn38">
<graphic xlink:href="529673v5_eqn38.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>At the optimal mean <italic>D</italic><sub>PR</sub>, each <inline-formula><inline-graphic xlink:href="529673v5_inline169.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is equivalent, leading to <inline-formula><inline-graphic xlink:href="529673v5_inline170.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (representative of random sampling). Hence, the mean <italic>D</italic><sub>PR</sub> post-random sampling sets the upper limit for the mean <italic>D</italic><sub>PR</sub> after anatomical sampling.</p>
<p>Let us investigate the lower bound of the mean PR dimension with the ERM model. For the minimization of mean(<italic>D</italic><sub>PR</sub>), a key requirement is the functional spatial proximity of neurons within the same cluster, in other words, the neuron set should be distinctly separated in functional space. Consequently, achieving the minimum mean PR dimension necessitates a functional sampling strategy.</p>
</sec>
<sec id="s4n4">
<label>4.14.4</label>
<title>Derive upper bound of dimension from spectrum</title>
<p>To deduce <italic>D</italic><sub><italic>PR</italic></sub> from the spectrum, for simplicity, we focus on the high-density region, where we have an analytical expression for <italic>Î»</italic> that is valid for large eigenvalues:
<disp-formula id="eqn39">
<graphic xlink:href="529673v5_eqn39.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>L</italic> is the size of the functional space, <italic>Î³</italic> is the coefficient in <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>, which depends on <italic>d, Âµ</italic>, and E(<italic>Ï</italic><sup>2</sup>). Note that the eigenvalue <italic>Î»</italic><sub><italic>r</italic></sub> decays rapidly after the threshold <italic>r</italic> = <italic>Î²</italic>(<italic>N</italic>). Since we did not discuss small eigenvalues in this article, we represent them here as an unknown function <italic>Î·</italic>(<italic>r, N, L</italic>):
<disp-formula id="eqn40">
<graphic xlink:href="529673v5_eqn40.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>As discussed in section 4.5, without changing the properties of the spectrum, we can always impose E(<italic>Ï</italic><sup>2</sup>) = 1 such that
<disp-formula id="eqn41">
<graphic xlink:href="529673v5_eqn41.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We emphasize that this constraint requires that large and small eigenvalues behave differently because otherwise <inline-formula><inline-graphic xlink:href="529673v5_inline171.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with <italic>Î± &lt;</italic> 1 would scale as <italic>N</italic> <sup>1â<italic>Î±</italic></sup>, and <inline-formula><inline-graphic xlink:href="529673v5_inline172.gif" mime-subtype="gif" mimetype="image"/></inline-formula><sub><italic>r</italic></sub> is not proportional to <italic>N</italic>.</p>
<p>Using the CauchyâSchwarz inequality, we have an upper bound of <inline-formula><inline-graphic xlink:href="529673v5_inline173.gif" mime-subtype="gif" mimetype="image"/></inline-formula>:
<disp-formula id="eqn42">
<graphic xlink:href="529673v5_eqn42.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>On the other hand, <inline-formula><inline-graphic xlink:href="529673v5_inline174.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is a lower bound of <inline-formula><inline-graphic xlink:href="529673v5_inline175.gif" mime-subtype="gif" mimetype="image"/></inline-formula>:
<disp-formula id="eqn43">
<graphic xlink:href="529673v5_eqn43.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>As a result, the dimensionality
<disp-formula id="ueqn6">
<graphic xlink:href="529673v5_ueqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
is bounded as
<disp-formula id="eqn44">
<graphic xlink:href="529673v5_eqn44.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Under random sampling, <italic>L</italic> remains fixed. Thus, we must have a bounded dimensionality that is independent of <italic>N</italic> for our ERM model. A tighter lower bound of <inline-formula><inline-graphic xlink:href="529673v5_inline176.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is
<disp-formula id="eqn45">
<graphic xlink:href="529673v5_eqn45.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>A tighter upper bound of participation ratio <italic>D</italic><sub><italic>P R</italic></sub> can be written as:
<disp-formula id="eqn46">
<graphic xlink:href="529673v5_eqn46.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>However, in functional sampling, enlarging the region size with constant density <italic>Ï</italic> results in <italic>L â¼ N</italic><sup>1<italic>/d</italic></sup>. Thus, the upper bound of <italic>D</italic><sub><italic>P R</italic></sub> should grow as <italic>N</italic> <sup>2<italic>Âµ/d</italic></sup>, consistent with the previously derived result (<xref ref-type="disp-formula" rid="eqn33">Eq. (33)</xref>) in section 4.14.2.</p>
</sec>
<sec id="s4n5">
<label>4.14.5</label>
<title>Simulating CCA and anatomical sampling</title>
<p>In this section, we estimate the dimensions of the anatomically sampled neuron set. For simplicity, we assume that the functional coordinates of neurons, <italic>X</italic><sub><italic>i</italic></sub>, and the anatomical coordinates of neurons, <italic>Y</italic><sub><italic>i</italic></sub>, both follow a multivariate Gaussian distribution. We define anatomical sampling, which involves sampling on <italic>Y</italic><sub><italic>i</italic></sub>, along a direction chosen arbitrarily and denote this direction as <italic>Y</italic> <sup><italic>A</italic></sup>. Subsequently, we perform sampling on <italic>X</italic><sub><italic>i</italic></sub> in the direction denoted by <italic>X</italic><sup><italic>A</italic></sup>, which is determined to have the highest correlation with <italic>Y</italic> <sup><italic>A</italic></sup> according to Canonical Correlation Analysis (CCA). This process effectively mimics the scenario of functional sampling.</p>
<p>The key to calculating the PR dimension involves computing the expected value <inline-formula><inline-graphic xlink:href="529673v5_inline177.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In the ERM model, the distribution of <italic>C</italic><sub><italic>ij</italic></sub> can be estimated by the distribution of points in the functional space. This allows for the calculation of the PR dimension across anatomical sampling by comparing the distribution of <italic>X</italic><sub><italic>i</italic></sub> after anatomical sampling with that after functional sampling. We can model the distribution of <italic>X</italic><sup><italic>A</italic></sup> and <italic>Y</italic> <sup><italic>A</italic></sup> as follows:
<disp-formula id="eqn47">
<graphic xlink:href="529673v5_eqn47.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here we consider only the projection of the functional coordinate onto the direction <italic>X</italic><sup><italic>A</italic></sup>, which exhibits the highest correlation, denoted by <italic>R</italic><sub>ASap</sub>, with <italic>Y</italic> <sup><italic>A</italic></sup>. Specifically, when selecting the anatomical direction as the first CCA direction, the correlation between <italic>X</italic><sup><italic>A</italic></sup> and <italic>Y</italic> <sup><italic>A</italic></sup> reaches its maximum, such that <italic>R</italic><sub>ASap</sub> = <italic>R</italic><sub>CCA</sub>. In this case, anatomical sampling results in the minimization of the dimensionality.</p>
<p>Now, let us perform anatomical sampling on the neurons. The <inline-formula><inline-graphic xlink:href="529673v5_inline178.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline179.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denote the functional and anatomical coordinates of the <italic>i</italic><sup>th</sup> neuron cluster after anatomical sampling, respectively.</p>
<p>To approximate, we need to calculate the functional coordinate probability distribution <inline-formula><inline-graphic xlink:href="529673v5_inline180.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which is the distribution of the <italic>i</italic><sup>th</sup> neuron cluster after anatomical sampling. <italic>Y</italic> <sup><italic>A</italic></sup> represents the selected direction in anatomical space, and <inline-formula><inline-graphic xlink:href="529673v5_inline181.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denotes the <italic>ik</italic><sup>th</sup> quantile of <italic>Y</italic> <sup><italic>A</italic></sup>, where <italic>k</italic> is the sampled fraction. Note the following relationships and distributions:
<disp-formula id="eqn48">
<graphic xlink:href="529673v5_eqn48.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn49">
<graphic xlink:href="529673v5_eqn49.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The conditional probability distribution <inline-formula><inline-graphic xlink:href="529673v5_inline182.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is equivalent to the distribution of the sum of <inline-formula><inline-graphic xlink:href="529673v5_inline182a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <italic>X</italic><sub>0</sub>, where <inline-formula><inline-graphic xlink:href="529673v5_inline183.gif" mime-subtype="gif" mimetype="image"/></inline-formula>):
<disp-formula id="eqn50">
<graphic xlink:href="529673v5_eqn50.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn51">
<graphic xlink:href="529673v5_eqn51.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The computation of <inline-formula><inline-graphic xlink:href="529673v5_inline184.gif" mime-subtype="gif" mimetype="image"/></inline-formula> involves two technical challenges: 1. The distribution of <inline-formula><inline-graphic xlink:href="529673v5_inline185.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is represented by a non-elementary function (<xref ref-type="disp-formula" rid="eqn51">Eq. (51)</xref>), which complicates the direct calculation of <inline-formula><inline-graphic xlink:href="529673v5_inline186.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which is the sum of <inline-formula><inline-graphic xlink:href="529673v5_inline187.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <italic>X</italic><sub>0</sub>. To facilitate approximation, we model <inline-formula><inline-graphic xlink:href="529673v5_inline188.gif" mime-subtype="gif" mimetype="image"/></inline-formula> using a normal distribution with equivalent variance. 2. Calculating the variance of <inline-formula><inline-graphic xlink:href="529673v5_inline189.gif" mime-subtype="gif" mimetype="image"/></inline-formula> presents direct challenges, and the variance of <inline-formula><inline-graphic xlink:href="529673v5_inline190.gif" mime-subtype="gif" mimetype="image"/></inline-formula> differs across different neuron clusters <italic>i</italic>. Using a uniform distribution for <italic>Y</italic> simplifies this task (this assumption is only used to calculate the variance of <inline-formula><inline-graphic xlink:href="529673v5_inline191.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). Under this assumption, the variance of <inline-formula><inline-graphic xlink:href="529673v5_inline192.gif" mime-subtype="gif" mimetype="image"/></inline-formula> can be straightforwardly calculated as Var <inline-formula><inline-graphic xlink:href="529673v5_inline193.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Consequently, we approximate <inline-formula><inline-graphic xlink:href="529673v5_inline194.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline195.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as follows:
<disp-formula id="eqn52">
<graphic xlink:href="529673v5_eqn52.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn53">
<graphic xlink:href="529673v5_eqn53.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Calculating the PR dimension directly from the distribution of <inline-formula><inline-graphic xlink:href="529673v5_inline196.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is difficult; thus, we approximate anatomical sampling with fraction <italic>k</italic> as functional sampling with fraction <italic>k</italic><sub><italic>f</italic></sub>, leading to:
<disp-formula id="eqn54">
<graphic xlink:href="529673v5_eqn54.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Using the equation for functional sampling <inline-formula><inline-graphic xlink:href="529673v5_inline197.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn32">Eq. (32)</xref>):
<disp-formula id="eqn55">
<graphic xlink:href="529673v5_eqn55.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn56">
<graphic xlink:href="529673v5_eqn56.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s6">
<label>5</label>
<title>Supplementary figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Related to <xref rid="fig2" ref-type="fig">Fig. 2</xref>. Experimental data description.</title>
<p><bold>A</bold>. Spatial distribution of segmented ROIs (shown in different colors). There are 1347 to 3086 ROIs in each animal. Scale bar, 100 Âµm. <bold>B</bold>. Explained variance of the activity data by PCs up to 500 rank. The different colored lines represent different fish data (n=6).</p></caption>
<graphic xlink:href="529673v5_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><title>The phenomenon of scale-invariant eigenspectra across different datasets.</title>
<p><bold>A-D</bold>. Distribution of normalized pairwise covariances, where <inline-formula><inline-graphic xlink:href="529673v5_inline304.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (Methods). <bold>E-H</bold>. Sampled covariance eigenspectra of different datasets. <bold>I-L</bold>. Pdfs of sampled covariance matrix eigenspectra of different datasets. The datasets correspond to the following examples: column 1: fish data (from fish 1, all fish data are shown in <xref rid="figS10" ref-type="fig">Fig. S10A-F</xref>) from whole brain light-field imaging; column 2: fish data from whole brain light-sheet imaging; column 3: mouse data from multi-area Neuropixels recording; column 4: mouse data from two-photon visual cortex recording.</p></caption>
<graphic xlink:href="529673v5_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Comparison between ERM simulation and theory.</title>
<p><bold>A-C</bold>. Rank plots of the normalized eigenspectra (<italic>Î»</italic>/Ï), with the simulations obtained using correlation matrix (sim: corr, <inline-formula><inline-graphic xlink:href="529673v5_inline305.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and covariance matrix (sim: cov, neuronâs activity variance <inline-formula><inline-graphic xlink:href="529673v5_inline306.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is i.i.d. sampled from a log-normal distribution with zero mean and a standard deviation of 0.5 in the natural logarithm of the <inline-formula><inline-graphic xlink:href="529673v5_inline307.gif" mime-subtype="gif" mimetype="image"/></inline-formula> values; we also normalize <inline-formula><inline-graphic xlink:href="529673v5_inline308.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (Methods)). The curves between âsim: corrâ and âsim: covâ are nearly identical in panels A and B. The theoretical predictions of normalized eigenvalues <italic>Î»</italic>/<italic>Ï</italic> are obtained using the high-density theory (cyan, <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref>). The density <italic>Ï</italic> decreases from panel A to panel C (<italic>Ï</italic> = 1024, 256, 10.24 respectively). <bold>D-F</bold>. Numerical validation of the theoretical spectrum by comparing probability density functions for increasing density of covariance ERM (<italic>Ï</italic> = 1024, 256, 10.24 respectively). Other simulation parameters: <italic>N</italic> = 1024, <italic>d</italic> = 2, <italic>L</italic> = (<italic>N</italic>/<italic>Ï</italic>)<sup>1/<italic>d</italic></sup>, <italic>Î¼</italic> = 0.5, <italic>Ïµ</italic> = 0.03125. The ERM simulations were conducted 100 times. The results are presented as the mean Â± SEM.</p></caption>
<graphic xlink:href="529673v5_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Covariance spectra under different kernel functions <inline-formula><inline-graphic xlink:href="529673v5_inline309.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</title>
<p>The figure presents both the sampled eigenvalue rank plot and the pdf of ERM with different functions <inline-formula><inline-graphic xlink:href="529673v5_inline310.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and varying dimensions <italic>d</italic>, where panels <bold>A-D,I,J</bold>. display the rank plot and panels <bold>E-H,K,L</bold>. show the pdf of ERM. <bold>A,E</bold>. Exponential function <inline-formula><inline-graphic xlink:href="529673v5_inline311.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>b</italic> = 1 and dimension <italic>d</italic> = 2. <bold>B,F</bold>. Exponential <inline-formula><inline-graphic xlink:href="529673v5_inline312.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>b</italic> = 1 and dimension <italic>d</italic> = 3. <bold>C,G</bold>. Gaussian pdf <inline-formula><inline-graphic xlink:href="529673v5_inline313.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <inline-formula><inline-graphic xlink:href="529673v5_inline314.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and dimension <italic>d</italic> = 2. <bold>D,H</bold>. Gaussian pdf <inline-formula><inline-graphic xlink:href="529673v5_inline315.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <inline-formula><inline-graphic xlink:href="529673v5_inline316.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and dimension <italic>d</italic> = 3. <bold>I,K</bold>. t pdf (<xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>) and dimension <italic>d</italic> = 2. <bold>J,L</bold>. t pdf (<xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>) and dimension <italic>d</italic> = 3. The ERM simulations were conducted 100 times and each ERM used an identical sampling technique described in (Methods). The results represent mean Â± SEM. <bold>M</bold>. Summary of CIâs for different <inline-formula><inline-graphic xlink:href="529673v5_inline317.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <italic>d</italic>. On the x-axis labels, âeâ denotes the Exponential function <inline-formula><inline-graphic xlink:href="529673v5_inline318.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,âgâ denotes the Gaussian pdf <inline-formula><inline-graphic xlink:href="529673v5_inline319.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,âtâ denotes the t-distribution pdf <inline-formula><inline-graphic xlink:href="529673v5_inline320.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,while â2â and â3â indicate <italic>d</italic> = 2 or <italic>d</italic> = 3, respectively.</p></caption>
<graphic xlink:href="529673v5_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Impact of <italic>Î·</italic> and <italic>d</italic> on the scale invariance of covariance eigenspectra in the ERM with <inline-formula><inline-graphic xlink:href="529673v5_inline321.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</title>
<p>The columns from left to right correspond to <italic>Î·</italic> = 0.3, 0.5, 0.7, 0.9, and the rows from top to bottom correspond to <italic>d</italic> = 1, 2, 3 (<xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref> and <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>). Other ERM simulation parameters: <italic>N</italic> = 4096, <italic>Ï</italic> = 256, <italic>L</italic> = (<italic>N</italic>/<italic>Ï</italic>)<sup>1/<italic>d</italic>,</sup> <italic>Ïµ</italic> = 0.03125 and <italic>Ï</italic><sup>2</sup> = 1. Each panel shows a single ERM realization. For visualization purposes, the views in some panels are truncated since we use the same range for the eigenvalues in all panels.</p></caption>
<graphic xlink:href="529673v5_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6.</label>
<caption><title>Impact of heterogeneous activity levels on the scale invariance.</title>
<p><bold>A</bold>. The CI as a function of the heterogeneity of neural activity levels <inline-formula><inline-graphic xlink:href="529673v5_inline322.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We generate ERM where each neuronâs activity variance <inline-formula><inline-graphic xlink:href="529673v5_inline323.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is i.i.d. sampled from a log-normal distribution where the logarithm of the variable follows a normal distribution with zero mean and a sequence of standard deviation (0, 0.05, 0.1,â¦, 0.5) in the natural logarithm of the values <inline-formula><inline-graphic xlink:href="529673v5_inline324.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.We also <inline-formula><inline-graphic xlink:href="529673v5_inline325.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (Methods). The solid blue line is the average across 100 ERM simulations, and the shaded area represents the SD. The red line results from the Gaussian variational method with simulation value integration limit <inline-formula><inline-graphic xlink:href="529673v5_inline326.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.The green line is the result of the Gaussian variational method with high-density value integration limit <inline-formula><inline-graphic xlink:href="529673v5_inline327.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (Methods). <italic>Ï</italic><sub>0</sub> = 128. <bold>B</bold>. Same as A, but with a smaller <italic>Ï</italic><sub>0</sub> = 10.24. Other parameters: <italic>Î¼</italic> = 0.5, <italic>d</italic> = 2, <italic>N</italic> = 1024, <italic>L</italic> = (<italic>N</italic>/<italic>Ï</italic>)<sup>1/<italic>d</italic></sup>, <italic>Ïµ</italic> = 0.03125. <bold>C</bold>. The collapse index (CI) of the correlation matrix (filled symbols) is larger than that of the covariance matrix (opened symbols) across different datasets excluding those shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. We use 7,200 time frame data across all the datasets. l2 to l3: light-sheet zebrafish data (2 Hz per volume); n2 to n3: Neuropixels mouse data, downsampled to 10 Hz per volume, p2 to p3: two-photon mouse data, (3 Hz per volume).</p></caption>
<graphic xlink:href="529673v5_figS6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7.</label>
<caption><title>Modifications of <inline-formula><inline-graphic xlink:href="529673v5_inline328.gif" mime-subtype="gif" mimetype="image"/></inline-formula> near <italic>x</italic> = 0.</title>
<p>The upper row illustrates the slow-decaying kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline329.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (blue solid line) and its power-law asymptote (red dashed line) along a 1D slice at various <inline-formula><inline-graphic xlink:href="529673v5_inline330.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.The lower row is similar to <bold>A</bold>, but on the log-log scale. The formulas for different <inline-formula><inline-graphic xlink:href="529673v5_inline331.gif" mime-subtype="gif" mimetype="image"/></inline-formula>âs are listed in <xref ref-type="table" rid="tblS3">table S3</xref> in Methods.</p></caption>
<graphic xlink:href="529673v5_figS7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Figure S8.</label>
<caption><title>Comparisons of large eigenvalues across different smoothing interval sizes, <italic>Ïµ</italic>.</title>
<p>Rank plot (upper row) and pdf (lower row) of the covariance eigenspectrum for ERMs with different <inline-formula><inline-graphic xlink:href="529673v5_inline332.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.<bold>A</bold>. <italic>Ïµ</italic> = 0.06. <bold>B</bold>. <italic>Ïµ</italic> = 0.12. <bold>C</bold>. <italic>Ïµ</italic> = 0.3. <bold>D</bold>. <italic>Ïµ</italic> = 0.6. Other ERM simulation parameters: <italic>N</italic> = 4096, <italic>Ï</italic> = 100, <italic>Î¼</italic> = 0.5, <italic>d</italic> = 2, <italic>L</italic> = 6.4, <inline-formula><inline-graphic xlink:href="529673v5_inline333.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.The formulas for different <inline-formula><inline-graphic xlink:href="529673v5_inline334.gif" mime-subtype="gif" mimetype="image"/></inline-formula>âs are listed in <xref ref-type="table" rid="tblS3">table S3</xref> in Methods.</p></caption>
<graphic xlink:href="529673v5_figS8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS9" position="float" fig-type="figure">
<label>Figure S9.</label>
<caption><title>Factors that do not affect the scale invariance.</title>
<p><bold>A</bold>. Rank plot of the covariance eigenspectrum for ERMs with different <inline-formula><inline-graphic xlink:href="529673v5_inline334a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (see <xref ref-type="table" rid="tblS3">table S3</xref>). Diagrams show different slow-decaying kernel functions <inline-formula><inline-graphic xlink:href="529673v5_inline335.gif" mime-subtype="gif" mimetype="image"/></inline-formula> along a 1D slice. <bold>B</bold>. Same as <bold>A</bold> but for different coordinate distributions in the functional space (see text). The diagrams on the right illustrate uniform and clustered coordinate distributions. <bold>C</bold>. Same as <bold>A</bold> but for different geometries of the functional space (see text). Diagrams illustrate spherical and hemispherical surfaces. <bold>D</bold>. CI of the different ERMs considered in A-C. The range on the y-axis is identical to <xref rid="fig4" ref-type="fig">Fig. 4C</xref>. On the x-axis, 1: uniform distribution, 2: normal distribution, 3: log-normal distribution, 4: uniform two nearby clusters, 5: uniform two faraway clusters, 6: uniform 3-cluster, 7: spherical surface in â<sup>3</sup>, 8: hemispherical surface in â<sup>3</sup>. All ERM models in <bold>B, C</bold> are adjusted to have a similar distribution of pairwise correlations (Methods).</p></caption>
<graphic xlink:href="529673v5_figS9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS10" position="float" fig-type="figure">
<label>Figure S10.</label>
<caption><title>Fitting ERM to zebrafish data from our experiments (part 1).</title>
<p>Comparison of sampled covariance eigenspectra in fish data and fitted ERM models. The columns correspond to six light-field zebrafish data: fish 1 to fish 6. Number of time frames: fish 1 - 7495, fish 2 - 9774, fish 3 - 13904, fish 4 - 7318, fish 5 - 7200 and fish 6 - 9388. <bold>A-F</bold>. sampled covariance eigenspectra for different fish data. <bold>G-L</bold>. Same as A-F but for ERM models with fitted parameters (<italic>Î¼</italic>/<italic>d, L</italic>), functional coordinates inferred using MDS, and the experimental <italic>Ïi</italic>. <bold>M-R</bold>. Same as A-F but for ERM models with fitted parameters (<italic>Î¼</italic>/<italic>d, L</italic>), uniform distributed functional coordinates, and a log-normal distribution of <italic>Ï</italic><sup>2</sup>. <italic>Î¼</italic>/<italic>d</italic> = [0.456, 0.258, 0.205, 0.262, 0.302, 0.308] in fish 1-6.</p></caption>
<graphic xlink:href="529673v5_figS10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS11" position="float" fig-type="figure">
<label>Figure S11.</label>
<caption><title>Fitting ERM to all six zebrafish data from our experiments (part 2).</title>
<p>Comparison of the covariance matrix between fish data and our fitted model. The columns correspond to six light-field zebrafish data: fish 1 to fish 6. <bold>A-F</bold>. The covariance matrix of different fish data. <bold>G-L</bold>. The covariance matrix of ERM models with fitted parameters (<italic>Î¼, L</italic>) and functional coordinates inferred using MDS and the experimental <italic>Ïi</italic>.</p></caption>
<graphic xlink:href="529673v5_figS11.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS12" position="float" fig-type="figure">
<label>Figure S12.</label>
<caption><title>Fitting ERM to all six zebrafish data from our experiments (part 3).</title>
<p>Columns correspond to five light-field zebrafish data: fish 1 to fish 6. <bold>A-F</bold>: Comparison of the power-law kernel <inline-formula><inline-graphic xlink:href="529673v5_inline336.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the model (blue line) and the correlation-distance relationship in the data (red line). The distance is calculated from the inferred coordinates using MDS. The shaded area represents the SD. <bold>G-L</bold>: Same as A-D but on the log-log scale.</p></caption>
<graphic xlink:href="529673v5_figS12.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS13" position="float" fig-type="figure">
<label>Figure S13.</label>
<caption><title>Fitting ERM to all six zebrafish data from our experiments (part 4).</title>
<p>Columns correspond to 6 light-field zebrafish data: fish 1 to fish 6. <bold>A-F</bold>: CCA correlation between the first CCA variables with different embedding dimensions in the functional space. Blue line indicates the CCA correlation of example fish data, green line shows the CCA correlation of example fish data with shuffled functional coordinates, and error bars represent the SD.</p></caption>
<graphic xlink:href="529673v5_figS13.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS14" position="float" fig-type="figure">
<label>Figure S14.</label>
<caption><title>Relationship between the functional space and anatomical space for each zebrafish dataset from our experiments.</title>
<p>Columns correspond to five light-field zebrafish data: fish 1 to fish 5 (with fish 6 has been shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>). <bold>A-E</bold>. Distribution of neurons in the functional space, where each neuron is color-coded by the projection of its coordinate along the canonical axis <inline-formula><inline-graphic xlink:href="529673v5_inline337.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in anatomical space (see text in Result section 2.4). Arrow: the first CCA direction <inline-formula><inline-graphic xlink:href="529673v5_inline338.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in functional space. <bold>F-J</bold>. Distribution of neurons in the anatomical space with the forebrain neuron located on the left side and the hindbrain neuron on the right side. Each neuron is color-coded by the projection of its coordinate along the canonical axis <inline-formula><inline-graphic xlink:href="529673v5_inline339.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in functional space (see text in Result section 2.4). Arrow: the first CCA direction <inline-formula><inline-graphic xlink:href="529673v5_inline340.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in anatomical space.</p></caption>
<graphic xlink:href="529673v5_figS14.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS15" position="float" fig-type="figure">
<label>Figure S15.</label>
<caption><title>The effects of hunting behavior on scale invariance and functional space organization.</title>
<p><bold>A,B</bold>. sampled covariance eigenspectra of the data from fish 1 calculated from control (<bold>A</bold>) and hunting removed (<bold>B</bold>) data. Ctrl: We randomly remove the same number of non-hunting frames. This process is repeated 10 times, and the meanÂ±SD of the CI is shown in the plot. Hunting removed: The time frames corresponding to the eye-converged intervals (putative hunting state) are removed when calculating the covariance (Methods). The CI for the hunting-removed data appears to be statistically smaller than in the control case (p-value= 1.5 Ã 10<sup>â9</sup>). <bold>C</bold>. Functional space organization of control data. The neurons are clustered using the Gaussian Mixture Models (GMMs) and their cluster memberships are shown by the color. The color bar represents the proportion of neurons that belong to each cluster. <bold>D</bold>. Similar to <bold>C</bold> but the functional coordinates are inferred from the hunting-removed data. The color code of each neuron is the same as that of the control data (<bold>C</bold>), which allows for a comparison of the changes to the clusters under the hunting-removed condition. See also the Movie. S1. <bold>E</bold>. Fold change in size / area (Methods) for each cluster (top; the gray dashed line represents a fold change of 1, that is, no change in size) and the anatomical distribution of the most dispersed cluster (bottom).</p></caption>
<graphic xlink:href="529673v5_figS15.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS16" position="float" fig-type="figure">
<label>Figure S16.</label>
<caption><title>Removing the time segment of hunting behavior does not obliterate the scale-invariant eigenspectra.</title>
<p>Rows correspond to 4 light-field zebrafish data: fish 2 to fish 5 (results for fish 1 have been shown in <xref rid="figS15" ref-type="fig">Fig. S15</xref>). <bold>A,C,E,G</bold>. Ctrl: we randomly remove the same number of time frames that are <italic>not</italic> the putative hunting frames. We repeat this process 10 times to generate 10 control covariance matrices and the CI is represented by meanÂ±SD. <bold>B,D,F,H</bold>. Hunting removed: data obtained by removing hunting frames from the full data (Methods). The CI for the hunting removed data appears to be significantly smaller than that of the control case (one-sample t-test <italic>p</italic> = 2.2 Ã 10<sup>â10</sup> in fish 2, <italic>p</italic> = 4.6 Ã 10<sup>â9</sup> in fish 3, <italic>p</italic> = 1.7 Ã 10<sup>â9</sup> in fish 4, and <italic>p</italic> = 3.4 Ã 10<sup>â17</sup> in fish 5).</p></caption>
<graphic xlink:href="529673v5_figS16.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS17" position="float" fig-type="figure">
<label>Figure S17.</label>
<caption><title>Hunting behavior reorganizes neurons in the functional space</title>
<p>Rows correspond to 5 light-field recordings of zebrafish engaged in hunting behavior: fish 1 to fish 5. <bold>A,D,G,J,M. (top)</bold> Functional space organization of the control data inferred by fitting the ERM and MDS (section 2.4). Neurons are clustered using the Gaussian Mixture Models (GMMs) and their cluster memberships are shown by the color. The colorbar represents the proportion of neurons belonging to each cluster. <bold>A,D,G,J,M. (bottom)</bold> The coordinate distribution of the cluster in control data which is most dispersed (i.e., largest fold change in size, see below) after hunting-removal. The transparency of the dots (colorbar) is proportional to the probability of the neurons belonging to this cluster (Methods). The cyan ellipse serves as a visual aid for the cluster size: it encloses 95% of the neurons belonging to that cluster (Methods). <bold>B,E,H,K,N. (top)</bold> Similar to <bold>A,D,G,J,M. (top)</bold> but the functional coordinates are inferred from the hunting-removed data. The color code of each neuron is the same as that in the control data, which allows for a comparison of the changes to the clusters under the hunting-removed condition. <bold>B,E,H,K,N. (bottom)</bold> Similar to <bold>A,D,G,J,M. (bottom)</bold> but the functional coordinates are inferred from the hunting-removed data. The transparency of each neuron is the same as in <bold>A,D,G,J,M. (bottom)</bold>, and it represents the probability <italic>pik</italic> (Methods) of neurons belonging to the most dispersed cluster <italic>k</italic> in the control data. Likewise, the cyan ellipse encloses 95% of the neurons belonging to that cluster (Methods). <bold>C,F,I,L,O</bold>. Top, size/area fold change (Methods) for each cluster (the gray dashed line represents a fold change of 1, i.e., no change in size); bottom, the anatomical distribution of the neurons in the most dispersed cluster.</p></caption>
<graphic xlink:href="529673v5_figS17.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="529673v5_figS17a.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS18" position="float" fig-type="figure">
<label>Figure S18.</label>
<caption><title>Negative covariances do not affect the eigenspectrum of the zebrafish data.</title>
<p>Red: eigenspectrum of the original data covariance matrix. Blue: eigenspectrum of the covariance matrix with negative entries replaced by zeros. In this figure, all neurons recorded in each fish were utilized without any sampling.</p></caption>
<graphic xlink:href="529673v5_figS18.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS19" position="float" fig-type="figure">
<label>Figure S19.</label>
<caption><title>Eigenspectra of RG-inspired clustering, direct functional region sampling (FSap), and random sampling (RSap) in ERM.</title>
<p><bold>A,D</bold>. Renormalization-Group (RG) clustered eigenspectra of ERM. The size of the cluster is denoted by <italic>N</italic>, which is the number of neurons in each cluster. We adopt the RG approach (<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c67">67</xref>), but with a specific modification (Methods). <bold>B,E</bold>. Direct spatial sampling in the functional space (FSap) and the corresponding ERM eigenspectra. We began our analysis with a set of <italic>N</italic><sub>0</sub> neurons distributed in the functional space. Initially, we chose <italic>N</italic> = <italic>N</italic><sub>0</sub>/2 neurons that were located exclusively on one side of the x-axis of this space. We then proceeded to select <italic>N</italic> = <italic>N</italic><sub>0</sub>/4 neurons from 4 quadrants. This sampling process was repeated iteratively, generating successively smaller subsets of neurons. <bold>C,F</bold>. Random sampled (RSap) eigenspectra of ERM. ERM parameters: <bold>A-C</bold> Exponential function <inline-formula><inline-graphic xlink:href="529673v5_inline341.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>b</italic> = 1, <italic>Ï</italic> = 10.24 and dimension <italic>d</italic> = 2. <bold>D-F</bold> Approximate power law <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> with <italic>Î¼</italic> = 0.5, <italic>Ï</italic> = 10.24 and dimension <italic>d</italic> = 2. Other parameters are the same as <xref rid="fig3" ref-type="fig">Fig. 3</xref>. The standard error of the mean (SEM) across the clusters is represented by the shaded area of each line.</p></caption>
<graphic xlink:href="529673v5_figS19.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS20" position="float" fig-type="figure">
<label>Figure S20.</label>
<caption><title>Dimensionality (<italic>D</italic><sub>PR</sub>) across sampling methods in fish data.</title>
<p><bold>A-F</bold> Result from fish 1 to fish 6: mean RSap <italic>D</italic><sub>PR</sub> (circles), mean (squares) and individual ASap <italic>D</italic><sub>PR</sub>, and FSapâs most correlated cluster <italic>D</italic><sub>PR</sub> (triangles). Dashed and solid lines indicate RSap and uniform FSap theoretical predictions, respectively. ERM parameter: <italic>Î¼</italic> = 0.6, <italic>d</italic> = 2, functional coordinates follow a multivariate normal distribution with variance <inline-formula><inline-graphic xlink:href="529673v5_inline341a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, anatomical coordinates follow a multivariate normal distribution with variance <inline-formula><inline-graphic xlink:href="529673v5_inline341b.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p></caption>
<graphic xlink:href="529673v5_figS20.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS21" position="float" fig-type="figure">
<label>Figure S21.</label>
<caption><title>Dimensionality (<italic>D</italic><sub>PR</sub>) across sampling methods in ERM.</title>
<p>PR dimensionality result of ERM model, coordinate in funcitonal and anatomical space are multivariate Gaussian distribution, the CCA correlation between funcitonal and anatomical space are <italic>RCCA</italic> = 0.4, 0.6, 0.8 in <bold>A-C</bold>. Mean RSap <italic>D</italic><sub>PR</sub> (circles), mean (squares) and individual ASap <italic>D</italic><sub>PR</sub>, and FSapâs most correlated cluster <italic>D</italic><sub>PR</sub> (triangles). Dashed and solid lines indicate RSap and uniform FSap theoretical predictions, <inline-formula><inline-graphic xlink:href="529673v5_inline342.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p></caption>
<graphic xlink:href="529673v5_figS21.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS22" position="float" fig-type="figure">
<label>Figure S22.</label>
<caption><title>Example of Renormalization Group (RG) approach for a set of eight neurons.</title>
<p>The figure is adapted from (<xref ref-type="bibr" rid="c20">20</xref>). The diagram illustrates the iterative clustering process for eight neurons. In each iteration, neurons are paired based on maximum correlation, with their activities combined through summation and normalized to maintain unit mean for nonzero values. Each neuron can only be paired once per iteration, ensuring all neurons are grouped by the iterationâs end.</p></caption>
<graphic xlink:href="529673v5_figS22.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS23" position="float" fig-type="figure">
<label>Figure S23.</label>
<caption><title>Morrell et al.âs latent variable model.</title>
<p><bold>A-D</bold>: Functional sampled (FSap) eigenspectra of the Morrell et al. model. <bold>E-H</bold>: Random sampled (RSap) eigenspectra of the same model. Briefly, in Morrell et al.âs latent variable model (<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c52">52</xref>), neural activity is driven by <italic>Nf</italic> latent fields and a place field. The latent fields are modeled as Ornstein-Uhlenbeck processes with a time constant <italic>Ï</italic>. The parameters <italic>Ïµ</italic> and <italic>Î·</italic> control the mean and variance of individual neuronsâ firing rates, respectively. The following are the parameter values used. <bold>A,E</bold>: Using the same parameters as in (<xref ref-type="bibr" rid="c52">52</xref>): <italic>Nf</italic> = 10, <italic>Ïµ</italic> = â2.67, <italic>Î·</italic> = 6, <italic>Ï</italic> = 0.1. Half of the cells are also coupled to the place field. <bold>B,C,D,F,G,H</bold>: Using parameters from (<xref ref-type="bibr" rid="c36">36</xref>): <italic>Nf</italic> = 5, <italic>Ïµ</italic> = â3, <italic>Î·</italic> = 4. There is no place field. The time constant <italic>Ï</italic> = 0.1, 1, 10 for <bold>B,F, C,G</bold>, and <bold>D,H</bold>, respectively.</p></caption>
<graphic xlink:href="529673v5_figS23.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS24" position="float" fig-type="figure">
<label>Figure S24.</label>
<caption><title>Scale-invariant properties persist across different temporal sampling rates in neural recordings.</title>
<p>Analysis of multi-area Neuropixels recordings (<xref ref-type="bibr" rid="c23">23</xref>) from 1024 neurons, downsampled to different rates resulting in 7200 time frames per condition (6 Hz, 12 Hz, 18 Hz, and 24 Hz; columns 1-4 respectively). <bold>A-D</bold>. Distribution of pairwise covariances after normalization to unit variance (<inline-formula><inline-graphic xlink:href="529673v5_inline343.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, see Methods). <bold>E-H</bold>. Eigenvalue spectra of the covariance matrices, showing similar power-law scaling across sampling rates. <bold>I-L</bold>. Probability density functions (PDFs) of the eigenvalues, demonstrating that the characteristic shape of the distribution is preserved across different temporal resolutions.</p></caption>
<graphic xlink:href="529673v5_figS24.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s7">
<label>6</label>
<title>Supplementary note</title>
<p>In this appendix, we elaborate upon the sketch introduced in the Methods, and present a full derivation of the covariance eigenspectrum of our ERM model, This section is organized as follows. First, we will briefly introduce the relationship between the eigenvalue probability density distribution and the resolvent. Second, we will turn the problem of calculating the resolvent to a calculation of the partition function using a field-theoretic representation and proceed to manipulate the partition function using the replica method. Third, we will introduce two approximate methods for calculating the partition function, leading to the high-density theory and the Gaussian variational method. We will discuss the implications and predictions of each method. Finally, we will discuss the relationship between the two methods and identify the parameter regime where the high-density theory agrees with the numerical simulation.</p>
<table-wrap id="tblS4" orientation="portrait" position="float">
<label>Table S4.</label>
<caption><title>Table of notations.</title></caption>
<graphic xlink:href="529673v5_tblS4.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<sec id="s7a">
<label>6.1</label>
<title>Resolvent</title>
<p>The eigenvalues Î»<sub><italic>n</italic></sub> of a Hermitian matrix <italic>C</italic> are real. Their probability density function or eigendensity is formally given by
<disp-formula id="eqnS1">
<graphic xlink:href="529673v5_eqnS1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where â¨â¦ â©represents an average across different realizations of <italic>C</italic>. The eigendensity is connected with the resolvent (<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>)
<disp-formula id="eqnS2">
<graphic xlink:href="529673v5_eqnS2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
we therefore compute the eigendensity using the standard inverse formula of Stieltjes tranform:
<disp-formula id="eqnS3">
<graphic xlink:href="529673v5_eqnS3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
</sec>
<sec id="s7b">
<label>6.2</label>
<title>Field representation</title>
<p>In this section, we discuss a field-theoretical representation of the resolvent <italic>g</italic>(<italic>z</italic>). First, we rewrite <xref ref-type="disp-formula" rid="eqnS2">Eq. (S2)</xref> as
<disp-formula id="eqnS4">
<graphic xlink:href="529673v5_eqnS4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The determinant (det(<italic>z</italic> â<italic>C</italic>))<sup>â1/2</sup> can be represented as a Gaussian integral
<disp-formula id="eqnS5">
<graphic xlink:href="529673v5_eqnS5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where Î¦ = [Ï<sub>1</sub>, â¦, <italic>Ï</italic><sub><italic>N</italic></sub>]<sup><italic>T</italic></sup>, and <inline-formula><inline-graphic xlink:href="529673v5_inline198.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.
<disp-formula id="eqnS6">
<graphic xlink:href="529673v5_eqnS6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We thus establish a relationship between the resolvent and Î
<disp-formula id="eqnS7">
<graphic xlink:href="529673v5_eqnS7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Note that the constant term in <xref ref-type="disp-formula" rid="eqnS6">Eq. (S6)</xref> can be killed by â<sub><italic>z</italic></sub> and we will ignore it in the sequel. <xref ref-type="disp-formula" rid="eqnS7">Eq. (S7)</xref> is the central formula in this note. Î(<italic>z</italic>) is also called the partition function in statistical physics. We endeavor to find a way to compute the average of ln Î(<italic>z</italic>).</p>
<p>Recall that in our ERM model (Result <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref> and <xref rid="fig3" ref-type="fig">Fig. 3A</xref>), the covariance between neuron <italic>i</italic> and neuron <italic>j</italic> is determined by the distance kernel function and their neural activity variances:
<disp-formula id="eqnS8">
<graphic xlink:href="529673v5_eqnS8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline199.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are sampled from a uniform coordinate distribution <inline-formula><inline-graphic xlink:href="529673v5_inline200.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are i.i.d. chosen from a probability density distribution <italic>p</italic>(<italic>Ï</italic>) and are independent of the neuron coordinates <inline-formula><inline-graphic xlink:href="529673v5_inline201.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The â¨<italic>â¦</italic> â© in <xref ref-type="disp-formula" rid="eqnS7">Eq. (S7)</xref> is therefore an average over all possible <inline-formula><inline-graphic xlink:href="529673v5_inline202.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <italic>Ï</italic><sub><italic>i</italic></sub>.</p>
<p>In order to compute â¨ln Î(<italic>z</italic>)â©, we apply the replica method based on a smart use of the identity
<disp-formula id="ueqn7">
<graphic xlink:href="529673v5_ueqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><xref ref-type="disp-formula" rid="eqnS7">Eq. (S7)</xref> now becomes
<disp-formula id="eqnS9">
<graphic xlink:href="529673v5_eqnS9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The idea is to compute the right-hand side for finite and integer <italic>n</italic> and then perform the analytic continuation to <italic>n</italic> â 0. Now we seek to determine the value of â¨Î<sup><italic>n</italic></sup>(<italic>z</italic>)â©. It contains <italic>n</italic> copies (replicas) of the original system
<disp-formula id="eqnS10">
<graphic xlink:href="529673v5_eqnS10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Writing it down explicitly, we have
<disp-formula id="eqnS11">
<graphic xlink:href="529673v5_eqnS11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>In order to proceed further, we introduce the following auxiliary fields :
<disp-formula id="eqnS12">
<graphic xlink:href="529673v5_eqnS12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><xref ref-type="disp-formula" rid="eqnS12">Eq. (S12)</xref> can be represented as a following functional integral
<disp-formula id="eqnS13">
<graphic xlink:href="529673v5_eqnS13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS14">
<graphic xlink:href="529673v5_eqnS14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
or we can combine <xref ref-type="disp-formula" rid="eqnS13">Eq. (S13)</xref> and <xref ref-type="disp-formula" rid="eqnS14">Eq. (S14)</xref> as
<disp-formula id="eqnS15">
<graphic xlink:href="529673v5_eqnS15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Using <xref ref-type="disp-formula" rid="eqnS12">Eq. (S12)</xref>, we can write the term <inline-formula><inline-graphic xlink:href="529673v5_inline203.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqnS11">Eq. (S11)</xref> as
<disp-formula id="eqnS16">
<graphic xlink:href="529673v5_eqnS16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We insert the relation <xref ref-type="disp-formula" rid="eqnS15">Eq. (S15)</xref> and <xref ref-type="disp-formula" rid="eqnS16">Eq. (S16)</xref> into <xref ref-type="disp-formula" rid="eqnS11">Eq. (S11)</xref>,
<disp-formula id="eqnS17">
<graphic xlink:href="529673v5_eqnS17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Integrating the last term in <xref ref-type="disp-formula" rid="eqnS17">Eq. (S17)</xref>
<disp-formula id="eqnS18">
<graphic xlink:href="529673v5_eqnS18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
so that â¨Î<sup><italic>n</italic></sup>(<italic>z</italic>)â© from <xref ref-type="disp-formula" rid="eqnS11">Eq. (S11)</xref> can be written as
<disp-formula id="eqnS19">
<graphic xlink:href="529673v5_eqnS19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS20">
<graphic xlink:href="529673v5_eqnS20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS21">
<graphic xlink:href="529673v5_eqnS21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Integrating out the <italic>Ï</italic><sup><italic>Î±</italic></sup> in â¨Î<sup><italic>n</italic></sup>(<italic>z</italic>)â© <xref ref-type="disp-formula" rid="eqnS19">Equations (S19)</xref> and <xref ref-type="disp-formula" rid="eqnS21">(S21)</xref>
<disp-formula id="eqnS22">
<graphic xlink:href="529673v5_eqnS22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here <italic>f</italic><sup>â1</sup> is the inverse kernel satisfying:
<disp-formula id="eqnS23">
<graphic xlink:href="529673v5_eqnS23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
so that â¨Î<sup><italic>n</italic></sup>(<italic>z</italic>)â© can be written as
<disp-formula id="eqnS24">
<graphic xlink:href="529673v5_eqnS24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS25">
<graphic xlink:href="529673v5_eqnS25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The constant term <inline-formula><inline-graphic xlink:href="529673v5_inline204.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of â¨Î<sup><italic>n</italic></sup>(<italic>z</italic>)â© can be ignored because we should compute â<sub><italic>z</italic></sub> â¨ln Î(<italic>z</italic>)â© <xref ref-type="disp-formula" rid="eqnS7">Eq. (S7)</xref> in the end.</p>
<p>To ensure the mathematical rigor in section 6.4, <xref ref-type="disp-formula" rid="eqnS42">Eq. (S42)</xref>, we next apply the Wick rotation <inline-formula><inline-graphic xlink:href="529673v5_inline205.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (section 6.7).
<disp-formula id="eqnS26">
<graphic xlink:href="529673v5_eqnS26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS27">
<graphic xlink:href="529673v5_eqnS27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS28">
<graphic xlink:href="529673v5_eqnS28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s7c">
<label>6.3</label>
<title>High-Density Expansion</title>
<p>In this section, we directly calculate the canonical partition function â¨Î<sup><italic>n</italic></sup>(<italic>z</italic>) â© in the <italic>z</italic> limit by approximating the term <italic>N</italic> ln <italic>A</italic> (<xref ref-type="disp-formula" rid="eqnS27">Eq. (S27)</xref>) to a quadratic action, from which the partition function (<xref ref-type="disp-formula" rid="eqnS26">Eq. (S26)</xref>) would become a Gaussian integral.</p>
<p>Let us first calculate the <italic>A</italic><sup><italic>N</italic></sup> in <italic>z</italic> â â limit
<disp-formula id="eqnS29">
<graphic xlink:href="529673v5_eqnS29.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS30">
<graphic xlink:href="529673v5_eqnS30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Now let us calculate â¨Î<sup><italic>n</italic></sup>(<italic>z</italic>)â© (<xref ref-type="disp-formula" rid="eqnS26">Equations (S26)</xref> to <xref ref-type="disp-formula" rid="eqnS28">(S28)</xref>) by letting <italic>L</italic> â â
<disp-formula id="eqnS31">
<graphic xlink:href="529673v5_eqnS31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the high-density quadratic action
<disp-formula id="eqnS32">
<graphic xlink:href="529673v5_eqnS32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline206.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Next, by integrating out the <inline-formula><inline-graphic xlink:href="529673v5_inline207.gif" mime-subtype="gif" mimetype="image"/></inline-formula> field, we find
<disp-formula id="eqnS33">
<graphic xlink:href="529673v5_eqnS33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Using <xref ref-type="disp-formula" rid="eqnS9">Eq. (S9)</xref> that connects the partition function with the resolvent, we have
<disp-formula id="eqnS34">
<graphic xlink:href="529673v5_eqnS34.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline208.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the Fourier transform of <inline-formula><inline-graphic xlink:href="529673v5_inline209.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>Finally, the eigendensity <italic>p</italic>(<italic>Î»</italic>) (<xref ref-type="disp-formula" rid="eqnS3">Eq. (S3)</xref>) is given by
<disp-formula id="eqnS35">
<graphic xlink:href="529673v5_eqnS35.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<sec id="s7c1">
<label>6.3.1</label>
<title>Derivation of power-law eigenspectrum in high-density limit</title>
<p>Here we calculate the eigendensity of our model, with the kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline210.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="table" rid="tblS3">table S3</xref>). The <xref ref-type="disp-formula" rid="eqnS35">Eq. (S35)</xref> (set E(<italic>Ï</italic><sup>2</sup>) = 1 as in Result section 2.2) can be written as:
<disp-formula id="eqnS36">
<graphic xlink:href="529673v5_eqnS36.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>S</italic><sub><italic>d</italic>â1</sub> is the surface area of <italic>d</italic> â 1 dimensional sphere. Here we consider the approximation <inline-formula><inline-graphic xlink:href="529673v5_inline211.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, whose Fourier transform and its derivative are <inline-formula><inline-graphic xlink:href="529673v5_inline212.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline213.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The constants are given by <inline-formula><inline-graphic xlink:href="529673v5_inline214.gif" mime-subtype="gif" mimetype="image"/></inline-formula>
<disp-formula id="eqnS37">
<graphic xlink:href="529673v5_eqnS37.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s7c2">
<label>6.3.2</label>
<title>Derivation of eigenspectrum with exponential kernel function in high-density limit</title>
<p>Here we consider the exponential kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline215.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, whose Fourier transform and its derivative are <inline-formula><inline-graphic xlink:href="529673v5_inline216.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline217.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="529673v5_inline218.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.
<disp-formula id="eqnS38">
<graphic xlink:href="529673v5_eqnS38.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>It is straightforward to see that this spectrum is not scale invariant. For example, when <italic>d</italic> = 2, the above expression reduces to a perfect power law spectrum <inline-formula><inline-graphic xlink:href="529673v5_inline219.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which changes with scale over sampling.</p>
</sec>
</sec>
<sec id="s7d">
<label>6.4</label>
<title>Variational Approximation</title>
<p>To find a general approximation for the eigenspectrum that goes beyond the high-density limit, we use Gaussian variational approximation in the field representation, namely by looking for the best quadratic action <italic>S</italic><sub><italic>v</italic></sub>,
<disp-formula id="eqnS39">
<graphic xlink:href="529673v5_eqnS39.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
to approximate the action <italic>S</italic><sub>1</sub> in the partition function (<xref ref-type="disp-formula" rid="eqnS26">Equations (S26)</xref> to <xref ref-type="disp-formula" rid="eqnS28">(S28)</xref>). This enables us to represent the partition function by a Gaussian integral, which can be evaluated analytically. We find the best quadratic action <italic>S</italic><sub><italic>v</italic></sub> by minimizing the difference between <italic>S</italic><sub>1</sub> and <italic>S</italic><sub><italic>v</italic></sub>, which is defined as KL divergence between two distributions that are proportional to <inline-formula><inline-graphic xlink:href="529673v5_inline220.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline221.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p>In this section, we will proceed by using the grand canonical ensemble formulation, namely the average in <xref ref-type="disp-formula" rid="eqnS1">Eq. (S1)</xref>, instead of using a fixed covariance matrix size <italic>N</italic>, which is now carried out across all different sizes. If <italic>N</italic> follows a Poisson distribution, it is easy to show (section 6.8) that the grand canonical partition function is given by <xref ref-type="disp-formula" rid="eqnS113">Eq. (S113)</xref>:
<disp-formula id="ueqn8">
<graphic xlink:href="529673v5_ueqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>a</italic> = â¨<italic>N</italic>â©. As a result, the new action <italic>S</italic><sub>1</sub> becomes
<disp-formula id="eqnS40">
<graphic xlink:href="529673v5_eqnS40.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here and below, <italic>N</italic> should be viewed as the average matrix size. The resolvent <italic>g</italic>(<italic>z</italic>) in <xref ref-type="disp-formula" rid="eqnS9">Eq. (S9)</xref> can be similarly generalized to <xref ref-type="disp-formula" rid="eqnS114">Eq. (S114)</xref>,
<disp-formula id="ueqn9">
<graphic xlink:href="529673v5_ueqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>As in statistical physics, we define the free energy as
<disp-formula id="eqnS41">
<graphic xlink:href="529673v5_eqnS41.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We shall define the variational free energy <italic>F</italic><sub><italic>v</italic></sub> such that it would approximate the true free energy <italic>F</italic><sub>1</sub> by minimizing <italic>D</italic><sub><italic>KL</italic></sub>(<italic>P</italic><sub><italic>v</italic></sub>||<italic>P</italic><sub>1</sub>),
<disp-formula id="eqnS42">
<graphic xlink:href="529673v5_eqnS42.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="eqnS43">
<graphic xlink:href="529673v5_eqnS43.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS44">
<graphic xlink:href="529673v5_eqnS44.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The KL divergence <italic>D</italic><sub><italic>KL</italic></sub>(<italic>P</italic><sub><italic>v</italic></sub> <italic>P</italic><sub>1</sub>) is always nonnegative and the free energy <italic>F</italic><sub>1</sub> is independent of the quadratic action <italic>S</italic><sub><italic>v</italic></sub>. Therefore, we need to minimize the variational free energy <italic>F</italic><sub><italic>v</italic></sub>. Let us now examine the variational free energy <italic>F</italic><sub><italic>v</italic></sub>
<disp-formula id="eqnS45">
<graphic xlink:href="529673v5_eqnS45.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here <italic>Z</italic><sub><italic>v</italic></sub> is the normalization factor
<disp-formula id="eqnS46">
<graphic xlink:href="529673v5_eqnS46.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Since we want to minimize <italic>F</italic><sub><italic>v</italic></sub>, the constant term
<disp-formula id="eqnS47">
<graphic xlink:href="529673v5_eqnS47.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
can be ignored, and <xref ref-type="disp-formula" rid="eqnS45">Eq. (S45)</xref> is reduced to
<disp-formula id="eqnS48">
<graphic xlink:href="529673v5_eqnS48.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>To simplify the formula, let us introduce <italic>S</italic><sub>2</sub>
<disp-formula id="eqnS49">
<graphic xlink:href="529673v5_eqnS49.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and rewrite <xref ref-type="disp-formula" rid="eqnS48">Eq. (S48)</xref> as
<disp-formula id="eqnS50">
<graphic xlink:href="529673v5_eqnS50.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Next, we will compute each term in the variational free energy <italic>F</italic><sub><italic>v</italic></sub></p>
<p>First, we calculate the third term ln <italic>Z</italic><sub><italic>v</italic></sub> in <xref ref-type="disp-formula" rid="eqnS50">Eq. (S50)</xref> by Equations (S39) and (S46)
<disp-formula id="eqnS51">
<graphic xlink:href="529673v5_eqnS51.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Second, we calculate the first term <inline-formula><inline-graphic xlink:href="529673v5_inline222.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqnS50">Eq. (S50)</xref>
<disp-formula id="eqnS52">
<graphic xlink:href="529673v5_eqnS52.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Third, we calculate the second term <inline-formula><inline-graphic xlink:href="529673v5_inline223.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqnS50">Eq. (S50)</xref>, recall the term <italic>A</italic> (<xref ref-type="disp-formula" rid="eqnS28">Eq. (S28)</xref>)
<disp-formula id="eqnS53">
<graphic xlink:href="529673v5_eqnS53.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS53a">
<graphic xlink:href="529673v5_eqnS53a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="eqnS54">
<graphic xlink:href="529673v5_eqnS54.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS55">
<graphic xlink:href="529673v5_eqnS55.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS56">
<graphic xlink:href="529673v5_eqnS56.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>In sum, the variational free energy <italic>F</italic><sub><italic>v</italic></sub> is equal to
<disp-formula id="eqnS57">
<graphic xlink:href="529673v5_eqnS57.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Now let us find the best quadratic action <italic>S</italic><sub><italic>v</italic></sub> that minimizes the variational free energy <italic>F</italic><sub><italic>v</italic></sub>
<disp-formula id="eqnS58">
<graphic xlink:href="529673v5_eqnS58.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The solution of <xref ref-type="disp-formula" rid="eqnS58">Eq. (S58)</xref> is given by
<disp-formula id="eqnS59">
<graphic xlink:href="529673v5_eqnS59.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS60">
<graphic xlink:href="529673v5_eqnS60.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline224.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. By using <xref ref-type="disp-formula" rid="eqnS114">Eq. (S114)</xref>), we finally obtain
<disp-formula id="eqnS61">
<graphic xlink:href="529673v5_eqnS61.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s7e">
<label>6.5</label>
<title>Scale invariance of the covariance spectrum in the Gaussian variational Model</title>
<p>In section 2.2 (Result), we point to two factors that contribute to the scale-invariance of eigenspectrum using the high-density theory. In this section, we show that the same conclusion can be drawn by using the Gaussian variational method. Furthermore, we examine how the heterogeneity of neural activity influences the eigendensity calculated by the Gaussian variational model. We show that <inline-formula><inline-graphic xlink:href="529673v5_inline225.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which characterizes the change of eigendensity due to sampling in the functional space, decreases with the heterogeneity of neural activity described by higher-order moment of neural activity variance, e.g., E(<italic>Ï</italic><sup>4</sup>).</p>
<p>Let us rewrite <xref ref-type="disp-formula" rid="eqnS60">Eq. (S60)</xref> as
<disp-formula id="eqnS62">
<graphic xlink:href="529673v5_eqnS62.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>To present a formal expression for the eigendensity, let us define <bold>Re</bold>(ð¢) <italic>g</italic><sub><italic>r</italic></sub>, <bold>Im</bold>(ð¢) <italic>g</italic><sub><italic>i</italic></sub>. From <xref ref-type="disp-formula" rid="eqnS3">Equations (S3)</xref> and <xref ref-type="disp-formula" rid="eqnS61">(S61)</xref>, we find
<disp-formula id="eqnS63">
<graphic xlink:href="529673v5_eqnS63.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where â¨<italic>â¦</italic>â©<sub><italic>Ï</italic></sub> = â« <italic>â¦p</italic>(<italic>Ï</italic>)d<italic>Ï</italic>.</p>
<p>A direct computation of <xref ref-type="disp-formula" rid="eqnS63">Eq. (S63)</xref>, however, remains difficult: the complication arises from the complex function <italic>M</italic>(<italic>z</italic>) in <xref ref-type="disp-formula" rid="eqnS62">Eq. (S62)</xref>, which in turn is an integral function of ð¢. To streamline the calculation, let us further define <bold>Re</bold>(<italic>M</italic>) â¡ <italic>Ïa</italic>, <bold>Im</bold>(<italic>M</italic>) â¡ <italic>Ïb</italic>. Writing it down explicitly, we have
<disp-formula id="eqnS64">
<graphic xlink:href="529673v5_eqnS64.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS65">
<graphic xlink:href="529673v5_eqnS65.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The real and imaginary part of ð¢ can now be expressed as functions of <italic>a</italic> and <italic>b</italic>. Integrating <xref ref-type="disp-formula" rid="eqnS62">Eq. (S62)</xref> in the spherical coordinates, we have
<disp-formula id="eqnS66">
<graphic xlink:href="529673v5_eqnS66.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where for clarity, we have abused the notation a bit by defining <inline-formula><inline-graphic xlink:href="529673v5_inline226.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the surface area of unit <italic>d</italic>-ball in the momentum space. In order to evaluate the integrals analytically, we introduce an ultraviolet cutoff <italic>Ï/Ïµ</italic>. Numerically, whether integrating up to <italic>Ï/Ïµ</italic> or greater than this bound shows little difference.</p>
<sec id="s7e1">
<label>6.5.1</label>
<title>Numerical solution of the Gaussian variational method</title>
<p>With Equations (S63) to (S66), we numerically calculate the eigendensity iteratively from the following steps:</p>
<list list-type="bullet">
<list-item><p>Step 1: set the initial values of <italic>a</italic> and <italic>b</italic> as <italic>a</italic><sub>0</sub> = 1, <italic>b</italic><sub>0</sub> = 1</p></list-item>
<list-item><p>Step 2: solve for <italic>a</italic> in <xref ref-type="disp-formula" rid="eqnS64">Eq. (S64)</xref> with fixed <italic>b</italic></p></list-item>
<list-item><p>Step 3: solve for <italic>b</italic> in <xref ref-type="disp-formula" rid="eqnS65">Eq. (S65)</xref> with fixed <italic>a</italic></p></list-item>
<list-item><p>Step 4: iterate Step 2 and Step 3 10 times</p></list-item>
<list-item><p>Step 5: calculate <italic>p</italic>(<italic>Î»</italic>) using <xref ref-type="disp-formula" rid="eqnS63">Eq. (S63)</xref></p></list-item>
</list>
<p>Note that we plug <xref ref-type="disp-formula" rid="eqnS66">Eq. (S66)</xref> into Equations (S64) and (S65) in step 2-3.</p>
</sec>
<sec id="s7e2">
<label>6.5.2</label>
<title>Two contributing factors on the scale invariance</title>
<p>We next derive an analytical expression for <xref ref-type="disp-formula" rid="eqnS66">Eq. (S66)</xref> by considering the approximate power law kernel function <inline-formula><inline-graphic xlink:href="529673v5_inline227.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, from which the high-density theory results on the scale invariance can be extended.</p>
<p>By a change of variable <inline-formula><inline-graphic xlink:href="529673v5_inline228.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and let <inline-formula><inline-graphic xlink:href="529673v5_inline229.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we have
<disp-formula id="eqnS67">
<graphic xlink:href="529673v5_eqnS67.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where â½ indicates that all constant numerical factors (e.g., <italic>Ï</italic> and Î(<italic>d/</italic>2)) are ignored. To compute <xref ref-type="disp-formula" rid="eqnS67">Eq. (S67)</xref>, we perform a branch cut at [0,], and perform a contour integral on the complex plane following the path in <xref rid="figS25" ref-type="fig">Fig. S25A</xref>. When <inline-formula><inline-graphic xlink:href="529673v5_inline230.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the integral on the large circle Î<sub><italic>R</italic></sub> and the small circle Î<sub><italic>Ïµ</italic></sub> goes to zero as <italic>x</italic><sub><italic>L</italic></sub> â¶ â <italic>x</italic><sub><italic>Ïµ</italic></sub> â¶0, leaving only two simple poles (zeros of the function in the denominator) in the complex plane. By applying the residue theorem, we find an expression for <italic>g</italic><sub><italic>i</italic></sub> in the limit <italic>L</italic> â â, <italic>Ïµ</italic> â 0
<disp-formula id="eqnS68">
<graphic xlink:href="529673v5_eqnS68.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The analytical expression for <italic>g</italic><sub><italic>r</italic></sub> is a bit more involving.
<disp-formula id="eqnS69">
<graphic xlink:href="529673v5_eqnS69.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>It has two terms, the second term is similar to <xref ref-type="disp-formula" rid="eqnS67">Eq. (S67)</xref>; the first term, however, diverges as <italic>x</italic><sub><italic>Ïµ</italic></sub> â¶ 0. Thus, the radius of the small circle Î<sub><italic>Ïµ</italic></sub> in <xref rid="figS25" ref-type="fig">Fig. S25A</xref> cannot shrink to zero: this is precisely the requirement of an ultraviolet cutoff in the wave vector <inline-formula><inline-graphic xlink:href="529673v5_inline231.gif" mime-subtype="gif" mimetype="image"/></inline-formula> The contour integral on the large circle Î<sub><italic>R</italic></sub>, on the other hand, goes to zero as <italic>x</italic><sub><italic>L</italic></sub> â¶ â. Thus, the integral on Î<sub><italic>Ïµ</italic></sub> contributes to the final result. By considering leading order term of <italic>x</italic><sub><italic>Ïµ</italic></sub> for finite but small <italic>x</italic><sub><italic>Ïµ</italic></sub>, we find
<disp-formula id="eqnS70">
<graphic xlink:href="529673v5_eqnS70.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Recall <inline-formula><inline-graphic xlink:href="529673v5_inline232.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and we find that the first term in <italic>g</italic><sub><italic>r</italic></sub> is proportional to <italic>Ï</italic><sup><italic>Âµ</italic></sup><italic>/Âµ</italic>, independent of <italic>Ïµ</italic>.</p>
<fig id="figS25" position="float" fig-type="figure">
<label>Figure S25.</label>
<caption><title>Calculate <italic>g</italic><sub><italic>i</italic></sub> and <italic>g</italic><sub><italic>r</italic></sub>.</title>
<p><bold>A</bold>. The path of the contour integral for <italic>g</italic><sub><italic>i</italic></sub>, <italic>g</italic><sub><italic>r</italic></sub> (<xref ref-type="disp-formula" rid="eqnS67">Eq. (S67)</xref>). <bold>B-C</bold>. The heatmap of <italic>g</italic><sub><italic>r</italic></sub> and <italic>g</italic><sub><italic>i</italic></sub> with respect to <italic>Î»</italic> and <italic>Ï. g</italic><sub><italic>i</italic></sub>, <italic>g</italic><sub><italic>r</italic></sub> in B, C are calculated by the numerical method (section 6.5.1). The parameters are <italic>N</italic> = 1024, <italic>Ï</italic> = 10.24, <italic>d</italic> = 2, <italic>L</italic> = 10, <italic>Âµ</italic> = 0.5, <italic>Ïµ</italic> = 0.03125. <inline-formula><inline-graphic xlink:href="529673v5_inline344.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is i.i.d. sampled from a log-normal distribution with zero mean and a standard deviation of 0.5 in the natural logarithm of the <inline-formula><inline-graphic xlink:href="529673v5_inline345.gif" mime-subtype="gif" mimetype="image"/></inline-formula> values; we also normalize <inline-formula><inline-graphic xlink:href="529673v5_inline346.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p></caption>
<graphic xlink:href="529673v5_figS25.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>According to <xref ref-type="disp-formula" rid="eqnS68">Equations (S68)</xref> and <xref ref-type="disp-formula" rid="eqnS70">(S70)</xref>, one can immediately see that as <italic>Âµ/d</italic> â¶ 0, the <italic>Ï</italic>-dependence relationship vanishes for <italic>g</italic><sub><italic>r</italic></sub> and <italic>g</italic><sub><italic>i</italic></sub>. We therefore conclude that a slower power-law decay in the kernel function and/or a higher dimension of the functional space are two contributing factors for the scale-invariance of the covariance spectrum.</p>
</sec>
<sec id="s7e3">
<label>6.5.3</label>
<title>Heterogeneity of neural activity across neurons enhances scale invariance</title>
<p>Next, we take a more close look at how the eigendensity changes with <italic>Ï</italic> for finite but small <italic>Âµ/d</italic> and when <italic>Î»</italic> â« 1. Using <xref ref-type="disp-formula" rid="eqnS63">Eq. (S63)</xref>, we have
<disp-formula id="eqnS71">
<graphic xlink:href="529673v5_eqnS71.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>From numerical calculation, we find that typically <italic>g</italic><sub><italic>r</italic></sub> â« <italic>g</italic><sub><italic>i</italic></sub>, so one can use the approximation
<disp-formula id="eqnS72">
<graphic xlink:href="529673v5_eqnS72.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Recall <xref ref-type="disp-formula" rid="eqnS63">Eq. (S63)</xref>
<disp-formula id="eqnS73">
<graphic xlink:href="529673v5_eqnS73.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Since <italic>p</italic>(<italic>Î», Ï</italic>) is very small for large <italic>Î»</italic>, a more appropriate measure is to examine
<disp-formula id="eqnS74">
<graphic xlink:href="529673v5_eqnS74.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Considering the large eigenvalue case <italic>Î» Ï</italic><sup>2</sup><italic>g</italic><sub><italic>r</italic></sub> (the numerical value of <italic>g</italic><sub><italic>r</italic></sub> is on the order of 1), we perform Taylor expansion and arrive at
<disp-formula id="eqnS75">
<graphic xlink:href="529673v5_eqnS75.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS76">
<graphic xlink:href="529673v5_eqnS76.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Note â¨<italic>Ï</italic><sup>2</sup>â©<sub><italic>Ï</italic></sub> â¡ E(<italic>Ï</italic><sup>2</sup>) is normalized to 1.
<disp-formula id="eqnS77">
<graphic xlink:href="529673v5_eqnS77.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>By examining <xref ref-type="disp-formula" rid="eqnS68">Equations (S68)</xref> and <xref ref-type="disp-formula" rid="eqnS70">(S70)</xref>, we find that when <italic>Î»</italic> â« <italic>g</italic><sub><italic>r</italic></sub>, <italic>a</italic> â« <italic>b, Î¸ â Ï, g</italic><sub><italic>r</italic></sub> decays weakly with <italic>Ï</italic> while <italic>g</italic><sub><italic>i</italic></sub> increases weakly with <italic>Ï</italic> (also confirmed by numerical calculation, <xref rid="figS25" ref-type="fig">Fig. S25B,C</xref>)
<disp-formula id="ueqn10">
<graphic xlink:href="529673v5_ueqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>It is therefore straightforward to see from <xref ref-type="disp-formula" rid="eqnS77">Eq. (S77)</xref> that the higher-order moment (e.g., E(<italic>Ï</italic><sup>4</sup>)) in the activity variance contributes to reducing the <italic>Ï</italic>-dependence in the eigendensity function.</p>
</sec>
<sec id="s7e4">
<label>6.5.4</label>
<title>The relationship between collapse index (CI) and eigendensity</title>
<p>In this section, we show how the collapse index (CI) introduced in section 4.7 is related to <xref ref-type="disp-formula" rid="eqnS77">Eq. (S77)</xref>, namely how the eigendensity changes with the neuronal density in the functional space. Recall the definition of CI in <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>:
<disp-formula id="ueqn11">
<graphic xlink:href="529673v5_ueqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where
<disp-formula id="ueqn12">
<graphic xlink:href="529673v5_ueqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
we used implicit differentiation to compute <inline-formula><inline-graphic xlink:href="529673v5_inline233.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For clarity, we write the function <italic>q</italic>(<italic>Î», Ï</italic>) explicitly involving <italic>Î»</italic> and <italic>Ï</italic> as <italic>Q</italic>(<italic>Î», Ï</italic>) in <xref ref-type="disp-formula" rid="eqnS78">Equations (S78)</xref> to <xref ref-type="disp-formula" rid="eqnS80">(S80)</xref>.
<disp-formula id="eqnS78">
<graphic xlink:href="529673v5_eqnS78.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS79">
<graphic xlink:href="529673v5_eqnS79.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS80">
<graphic xlink:href="529673v5_eqnS80.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Now we can write CI as
<disp-formula id="eqnS81">
<graphic xlink:href="529673v5_eqnS81.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
from which we arrive at <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> in Methods:
<disp-formula id="eqnS82">
<graphic xlink:href="529673v5_eqnS82.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Finally, we can rewrite CI as a function of <inline-formula><inline-graphic xlink:href="529673v5_inline234.gif" mime-subtype="gif" mimetype="image"/></inline-formula> using a double integral:
<disp-formula id="eqnS83">
<graphic xlink:href="529673v5_eqnS83.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
</sec>
<sec id="s7f">
<label>6.6</label>
<title>Compare high-density theory and Gaussian variational method</title>
<p>This section aims to determine the conditions under which the high-density approximation aligns with the simulation results. To this end, we begin by comparing the kernel operator <inline-formula><inline-graphic xlink:href="529673v5_inline235.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the high-density quadratic action and <inline-formula><inline-graphic xlink:href="529673v5_inline236.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in the variational approximation. We identify the condition when high-density theory would agree with the variational method as well as the numerical simulation, namely <inline-formula><inline-graphic xlink:href="529673v5_inline237.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Secondly, we give a precise re-derivation of the high-density result by incorporating this condition into the variational approximation. Finally, we substitute <inline-formula><inline-graphic xlink:href="529673v5_inline238.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with <inline-formula><inline-graphic xlink:href="529673v5_inline239.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and estimate the parameter regime where the high-density theory would agree with numerical simulation. This analysis yields a deeper understanding of the relationship between high-density theory and variational method, and how they relate to simulation results.</p>
<sec id="s7f1">
<label>6.6.1</label>
<title>A simple comparison of the two methods</title>
<p>For the sake of simplicity, we consider the correlation matrix with <italic>p</italic>(<italic>Ï</italic>) = <italic>Î´</italic>(<italic>Ï</italic> â 1) in this section. Returning to the explicit result (<xref ref-type="disp-formula" rid="eqnS26">Equations (S26)</xref> to <xref ref-type="disp-formula" rid="eqnS28">(S28)</xref>),
<disp-formula id="eqnS84">
<graphic xlink:href="529673v5_eqnS84.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>In the high-density approximation (<xref ref-type="disp-formula" rid="eqnS32">Eq. (S32)</xref>)
<disp-formula id="eqnS85">
<graphic xlink:href="529673v5_eqnS85.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here we introduce <italic>G</italic><sub><italic>h</italic></sub> as the kernel operator in the high-density quadratic action.
<disp-formula id="eqnS86">
<graphic xlink:href="529673v5_eqnS86.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Fourier transform of <italic>G</italic><sub><italic>h</italic></sub> leads to
<disp-formula id="eqnS87">
<graphic xlink:href="529673v5_eqnS87.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>In the variational method (<xref ref-type="disp-formula" rid="eqnS60">Eq. (S60)</xref>), we have
<disp-formula id="eqnS88">
<graphic xlink:href="529673v5_eqnS88.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we introduce <italic>G</italic><sub><italic>v</italic></sub> as the kernel operator in the variational quadratic action. Clearly, the condition that <inline-formula><inline-graphic xlink:href="529673v5_inline240.gif" mime-subtype="gif" mimetype="image"/></inline-formula> approaches <inline-formula><inline-graphic xlink:href="529673v5_inline241.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is given by
<disp-formula id="ueqn13">
<graphic xlink:href="529673v5_ueqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>The function <italic>ratio</italic><sub><italic>v</italic></sub>(<italic>z</italic>) is defined as:
<disp-formula id="ueqn14">
<graphic xlink:href="529673v5_ueqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>As <italic>ratio</italic><sub><italic>v</italic></sub>(<italic>z</italic>) approaches 0, <inline-formula><inline-graphic xlink:href="529673v5_inline242.gif" mime-subtype="gif" mimetype="image"/></inline-formula> becomes identical to <inline-formula><inline-graphic xlink:href="529673v5_inline243.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Note that <inline-formula><inline-graphic xlink:href="529673v5_inline244.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is difficult to compute; instead, we can compute and analyze <inline-formula><inline-graphic xlink:href="529673v5_inline245.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (see section 6.6.3)
<disp-formula id="eqnS89">
<graphic xlink:href="529673v5_eqnS89.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s7f2">
<label>6.6.2</label>
<title>A re-derivation of the high-density result using the grand canonical ensemble</title>
<p>In this section, we re-derive the high-density result from the grand canonical ensemble and the variational method. The derivation also allows us to reproduce the approximation condition discussed in the previous section.</p>
<p>Let us recall the calculation of the free energy <italic>F</italic><sub><italic>v</italic></sub> (<xref ref-type="disp-formula" rid="eqnS57">Eq. (S57)</xref>) in the variational approximation with <italic>p</italic>(<italic>Ï</italic>) = <italic>Î´</italic>(<italic>Ï</italic> â 1)
<disp-formula id="eqnS90">
<graphic xlink:href="529673v5_eqnS90.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS91">
<graphic xlink:href="529673v5_eqnS91.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Following <xref ref-type="disp-formula" rid="eqnS58">Equations (S58)</xref> and <xref ref-type="disp-formula" rid="eqnS60">(S60)</xref>:
<disp-formula id="eqnS92">
<graphic xlink:href="529673v5_eqnS92.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS93">
<graphic xlink:href="529673v5_eqnS93.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS94">
<graphic xlink:href="529673v5_eqnS94.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We can perform the same calculation in the high-density theory by considering the limit <inline-formula><inline-graphic xlink:href="529673v5_inline246.gif" mime-subtype="gif" mimetype="image"/></inline-formula><inline-formula><inline-graphic xlink:href="529673v5_inline246a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>
<disp-formula id="eqnS95">
<graphic xlink:href="529673v5_eqnS95.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Therefore, we can define the free energy <italic>F</italic><sub><italic>h</italic></sub> in the high-density theory as
<disp-formula id="eqnS96">
<graphic xlink:href="529673v5_eqnS96.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
then
<disp-formula id="eqnS97">
<graphic xlink:href="529673v5_eqnS97.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS98">
<graphic xlink:href="529673v5_eqnS98.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>This is precisely <xref ref-type="disp-formula" rid="eqnS87">Eq. (S87)</xref> derived in the previous section.
<disp-formula id="eqnS99">
<graphic xlink:href="529673v5_eqnS99.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which is the resolvent of high-density approximation (<xref ref-type="disp-formula" rid="eqnS34">Eq. (S34)</xref>).</p>
</sec>
<sec id="s7f3">
<label>6.6.3</label>
<title>compute</title>
<p><inline-formula><inline-graphic xlink:href="529673v5_inline247.gif" mime-subtype="gif" mimetype="image"/></inline-formula> In this section, we provide an explicit expression for the integral <inline-formula><inline-graphic xlink:href="529673v5_inline248.gif" mime-subtype="gif" mimetype="image"/></inline-formula> instead of <inline-formula><inline-graphic xlink:href="529673v5_inline249.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which is implicit and can not be calculated analytically. Like the derivation in section 6.5, we consider the lower and upper limits of integration for <inline-formula><inline-graphic xlink:href="529673v5_inline250.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as <inline-formula><inline-graphic xlink:href="529673v5_inline251.gif" mime-subtype="gif" mimetype="image"/></inline-formula> We then approximate the Fourier transform <inline-formula><inline-graphic xlink:href="529673v5_inline252.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as a power-law function. To ensure that the singularity <inline-formula><inline-graphic xlink:href="529673v5_inline253.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of <inline-formula><inline-graphic xlink:href="529673v5_inline254.gif" mime-subtype="gif" mimetype="image"/></inline-formula> falls within the integration range of <inline-formula><inline-graphic xlink:href="529673v5_inline255.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,we introduce a simple correction <inline-formula><inline-graphic xlink:href="529673v5_inline256.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to <inline-formula><inline-graphic xlink:href="529673v5_inline257.gif" mime-subtype="gif" mimetype="image"/></inline-formula>:
<disp-formula id="eqnS100">
<graphic xlink:href="529673v5_eqnS100.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline258.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are all constants depending on the parameters, <italic>d, Î¼</italic>, and <italic>Ïµ</italic>.. Then we compute the contour integral (<xref rid="figS25" ref-type="fig">Fig. S25A</xref>) by Taylor expansion. As a result, we have
<disp-formula id="eqnS101">
<graphic xlink:href="529673v5_eqnS101.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline259.gif" mime-subtype="gif" mimetype="image"/></inline-formula></p>
<p>Now let us take a close look at the behavior of the function <italic>ratio</italic><sub><italic>h</italic></sub>(<italic>z</italic>) (<xref ref-type="disp-formula" rid="eqnS89">Eq. (S89)</xref>), plotted in <xref rid="figS26" ref-type="fig">Fig. S26A,B</xref>. For small <italic>z</italic>, this function is negative. It then crosses zero and has a peak. As <italic>z</italic>, the <italic>ratio</italic><sub><italic>h</italic></sub> approaches zero. This is because <xref ref-type="disp-formula" rid="eqnS101">Eq. (S101)</xref> approaches a positive constant, which is given by
<disp-formula id="ueqn15">
<graphic xlink:href="529673v5_ueqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="529673v5_inline260.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For <italic>z</italic> â¥ 1, we find the leading order expansion at <italic>j</italic> = 1 already gives an accurate approximation (<xref rid="figS26" ref-type="fig">Fig. S26A,B</xref>).
<disp-formula id="eqnS102">
<graphic xlink:href="529673v5_eqnS102.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<fig id="figS26" position="float" fig-type="figure">
<label>Figure S26.</label>
<caption><title>Relationship between <italic>ratio</italic><sub><italic>h</italic></sub> and <italic>z</italic>.</title>
<p><bold>A</bold>. <italic>Ï</italic> = 1024, <bold>B</bold>. <italic>Ï</italic> = 256. Blue line: <italic>ratio</italic><sub><italic>h</italic></sub> calculated numerically. Red line: 100-order expansion of <xref ref-type="disp-formula" rid="eqnS101">Eq. (S101)</xref>, which perfectly overlaps with the blue line. Green line: expansion to the first order. Other parameter: <italic>Âµ</italic> = 0.5, <italic>d</italic> = 2, <italic>Ïµ</italic> = 0.03125. <bold>C</bold>. Relationship between <italic>ÏÏµ</italic><sup><italic>d</italic></sup> and dimension <italic>d</italic> with fixed <inline-formula><inline-graphic xlink:href="529673v5_inline347.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqnS105">Eq. (S105)</xref>).</p></caption>
<graphic xlink:href="529673v5_figS26.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s7f4">
<label>6.6.4</label>
<title>Estimate the parameter condition when the high-density theory best agrees with numerical simulation</title>
<p>By analyzing the properties of the function <inline-formula><inline-graphic xlink:href="529673v5_inline261.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we think the high-density theory provides an accurate approximation when the zero-crossing of <inline-formula><inline-graphic xlink:href="529673v5_inline262.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is near <italic>z</italic> = 1 (the peak of low-density result (<xref ref-type="bibr" rid="c34">34</xref>))</p>
<p>The root <italic>z</italic><sub>0</sub> of the integral <inline-formula><inline-graphic xlink:href="529673v5_inline263.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is given by
<disp-formula id="eqnS103">
<graphic xlink:href="529673v5_eqnS103.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>It is easy to see that <italic>g</italic><sub>1</sub>(<italic>d, Âµ</italic>) is a function of <italic>P</italic> (or <inline-formula><inline-graphic xlink:href="529673v5_inline264.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) from <xref ref-type="disp-formula" rid="eqnS101">Eq. (S101)</xref>. We can rewrite <xref ref-type="disp-formula" rid="eqnS103">Eq. (S103)</xref> as
<disp-formula id="eqnS104">
<graphic xlink:href="529673v5_eqnS104.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here, we can also see that <italic>z</italic><sub>0</sub> can be expressed as:
<disp-formula id="ueqn16">
<graphic xlink:href="529673v5_ueqn16.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Using this expression for <italic>z</italic><sub>0</sub> and letting <italic>z</italic><sub>0</sub> = 1, we can derive the following equation for <italic>ÏÏµ</italic><sup><italic>d</italic></sup>, a <italic>dimensionless parameter</italic> that determines the condition when the high-density theory is an accurate approximation of our ERM model:
<disp-formula id="eqnS105">
<graphic xlink:href="529673v5_eqnS105.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p><xref rid="figS26" ref-type="fig">Fig. S26C</xref> shows how <italic>ÏÏµ</italic><sup><italic>d</italic></sup> changes as a function <italic>d</italic> for a small and fixed <italic>Âµ/d</italic>. For example, when <italic>d</italic> = 2, <italic>Âµ</italic> = 0.5, <italic>Ïµ</italic> = 0.03125, we find
<disp-formula id="ueqn17">
<graphic xlink:href="529673v5_ueqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>This estimate is also consistent with our numerical simulation (<xref rid="figS3" ref-type="fig">Fig. S3</xref>).</p>
</sec>
</sec>
<sec id="s7g">
<label>6.7</label>
<title>Wick rotation</title>
<p>To ensure mathematical rigor in section 6.4, we should make sure that the action <italic>S</italic><sub>1</sub> in <xref ref-type="disp-formula" rid="eqnS43">Eq. (S43)</xref> is a real number. Here we use Wick rotation to transform <xref ref-type="disp-formula" rid="eqnS25">Eq. (S25)</xref> to <xref ref-type="disp-formula" rid="eqnS27">Eq. (S27)</xref>. The Gaussian integral <xref ref-type="disp-formula" rid="eqnS26">Eq. (S26)</xref> can be divergent when <inline-formula><inline-graphic xlink:href="529673v5_inline265.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is not positive definite, To address this issue, we can always write the partition function Î<sup><italic>n</italic></sup>(<italic>z</italic>) as a Gaussian integral by choosing the appropriate axes with Wick rotation.
<disp-formula id="eqnS106">
<graphic xlink:href="529673v5_eqnS106.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>We can now change the integration variables by diagonalizing <inline-formula><inline-graphic xlink:href="529673v5_inline266.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to <inline-formula><inline-graphic xlink:href="529673v5_inline267.gif" mime-subtype="gif" mimetype="image"/></inline-formula> via <inline-formula><inline-graphic xlink:href="529673v5_inline268.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,where <italic>Q</italic> is Fourier base
<disp-formula id="eqnS107">
<graphic xlink:href="529673v5_eqnS107.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqnS108">
<graphic xlink:href="529673v5_eqnS108.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
by letting <italic>L</italic> â â. Note that <inline-formula><inline-graphic xlink:href="529673v5_inline269.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is analytic. Thus if
<disp-formula id="ueqn18">
<graphic xlink:href="529673v5_ueqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and the convergence rate is faster than <inline-formula><inline-graphic xlink:href="529673v5_inline270.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we can apply the Wick rotation <inline-formula><inline-graphic xlink:href="529673v5_inline271.gif" mime-subtype="gif" mimetype="image"/></inline-formula>: instead of computing the integral on the real axis <italic>C</italic><sub>1</sub>, we now rotate the integral line 45 degree clockwise to <italic>C</italic><sub>3</sub> in the complex plane:
<disp-formula id="eqnS109">
<graphic xlink:href="529673v5_eqnS109.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<fig id="figS27" position="float" fig-type="figure">
<label>Figure S27.</label>
<caption><title>Wick rotation in complex plane</title></caption>
<graphic xlink:href="529673v5_figS27.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>On the other hand, if
<disp-formula id="ueqn19">
<graphic xlink:href="529673v5_ueqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and the convergence rate is faster than <inline-formula><inline-graphic xlink:href="529673v5_inline272.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we can apply the Wick rotation <inline-formula><inline-graphic xlink:href="529673v5_inline273.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, namely to rotate the integral line 45 degree counterclockwise to <italic>C</italic><sub>2</sub>:
<disp-formula id="eqnS110">
<graphic xlink:href="529673v5_eqnS110.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>As a simple example, consider a one-dimensional Gaussian integral
<disp-formula id="ueqn20">
<graphic xlink:href="529673v5_ueqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>When <italic>k &gt;</italic> 0, we can use the Wick rotation <inline-formula><inline-graphic xlink:href="529673v5_inline274.gif" mime-subtype="gif" mimetype="image"/></inline-formula>
<disp-formula id="ueqn21">
<graphic xlink:href="529673v5_ueqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>When <italic>k &lt;</italic> 0, we can use the Wick rotation <inline-formula><inline-graphic xlink:href="529673v5_inline275.gif" mime-subtype="gif" mimetype="image"/></inline-formula>
<disp-formula id="ueqn22">
<graphic xlink:href="529673v5_ueqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Without loss of generality, we rotate <inline-formula><inline-graphic xlink:href="529673v5_inline276.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in section 6.4 for subsequent calculations.</p>
</sec>
<sec id="s7h">
<label>6.8</label>
<title>Grand Canonical Ensemble</title>
<p>When using the Gaussian variational Approximation, we consider a critical extension from the <italic>canonical ensemble</italic> to the <italic>grand canonical ensemble</italic> when computing the partition function (<xref ref-type="disp-formula" rid="eqnS6">Eq. (S6)</xref>). We would like to justify this approximation in this section. Recall that the resolvent is given by
<disp-formula id="ueqn23">
<graphic xlink:href="529673v5_ueqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where Î(<italic>z</italic>) can be viewed as the canonical partition function, the â¨<italic>â¦</italic> â© is the average over all random matrices <italic>C</italic> for a given <italic>N</italic>. Let us now generalize (<xref ref-type="disp-formula" rid="eqnS6">Eq. (S6)</xref>) into grand canonical ensemble, namely
<disp-formula id="eqnS111">
<graphic xlink:href="529673v5_eqnS111.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where â¨<italic>â¦</italic> â©<sub><italic>N</italic></sub> indicates that we need to average over all possible random matrices and across <italic>all possible N</italic>, with the probability to have a matrix size <italic>N</italic> given by the Poisson distribution <inline-formula><inline-graphic xlink:href="529673v5_inline277.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>a</italic> = â¨<italic>N</italic>â©. When â¨<italic>N</italic>â© is large, <italic>P</italic>(<italic>N</italic>) has a very sharp peak at â¨<italic>N</italic>â©, and <xref ref-type="disp-formula" rid="eqnS111">Eq. (S111)</xref> can be approximated as
<disp-formula id="eqnS112">
<graphic xlink:href="529673v5_eqnS112.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Using the replica trick, we recall <xref ref-type="disp-formula" rid="eqnS9">Eq. (S9)</xref>
<disp-formula id="ueqn24">
<graphic xlink:href="529673v5_ueqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Let us now define the grand canonical partition function as
<disp-formula id="eqnS113">
<graphic xlink:href="529673v5_eqnS113.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Likewise, the resolvent in <xref ref-type="disp-formula" rid="eqnS9">Eq. (S9)</xref> is generalized to
<disp-formula id="eqnS114">
<graphic xlink:href="529673v5_eqnS114.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>To see whether this definition makes sense, we write
<disp-formula id="eqnS115">
<graphic xlink:href="529673v5_eqnS115.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where the second equality uses the identity
<disp-formula id="ueqn25">
<graphic xlink:href="529673v5_ueqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and the last equality is indeed <xref ref-type="disp-formula" rid="eqnS112">Eq. (S112)</xref> discussed earlier.</p>
<p>Returning back to the explicit form of the grand canonical partition function in our ERM model (<xref ref-type="disp-formula" rid="eqnS26">Equations (S26)</xref> to <xref ref-type="disp-formula" rid="eqnS28">(S28)</xref>), we have
<disp-formula id="eqnS116">
<graphic xlink:href="529673v5_eqnS116.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Here <italic>Ï</italic> is the auxiliary fields (<xref ref-type="disp-formula" rid="eqnS12">Eq. (S12)</xref>),<inline-formula><inline-graphic xlink:href="529673v5_inline278.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and are terms defined in <xref ref-type="disp-formula" rid="eqnS26">Equations (S26)</xref> to <xref ref-type="disp-formula" rid="eqnS28">(S28)</xref>. <xref ref-type="disp-formula" rid="eqnS116">Eq. (S116)</xref> is used in section 6.4 to compute the free energy.</p>
</sec>
<sec id="s7i">
<label>6.9</label>
<title>E-I balanced asynchronized model Summary</title>
<p>In this section, we discuss the E-I balanced asynchronized model (<xref ref-type="bibr" rid="c53">53</xref>), which predicts a different scaling D N under random sampling, since the variance <inline-formula><inline-graphic xlink:href="529673v5_inline279.gif" mime-subtype="gif" mimetype="image"/></inline-formula> scales as 1/N and diminishes as N approaches large limit.</p>
<sec id="s7i1">
<label>6.9.1</label>
<title>Model</title>
<p>The simulation of binary networks involves updating neuron states within a network architecture identical to analytical studies. The update rule is probabilistic, with neuron activities set based on synaptic currents and a firing threshold. The dynamics resolution improves with network size, with neuron time constants effectively representing changes in firing activity. Parameters for simulations include connection probabilities, mean rates, thresholds, and synaptic strengths, scaled appropriately for network size.</p>
<p>Update Rule: <inline-formula><inline-graphic xlink:href="529673v5_inline280.gif" mime-subtype="gif" mimetype="image"/></inline-formula></p>
<p>Dynamics Resolution: <inline-formula><inline-graphic xlink:href="529673v5_inline281.gif" mime-subtype="gif" mimetype="image"/></inline-formula></p>
<p>In the simulation of binary networks, the modelâs dynamics are governed by a set of parameters, each with a specific role:</p>
<p><inline-formula><inline-graphic xlink:href="529673v5_inline282.gif" mime-subtype="gif" mimetype="image"/></inline-formula>: This represents the state of neuron <italic>i</italic> in population <italic>Î±</italic> at the next time step <italic>t</italic> + 1. The state is binary, where 1 indicates the neuron is active (firing) and 0 indicates it is inactive.</p>
<p>Î(): The Heaviside step function used in the update rule. It determines the neuronâs next state by comparing the net input to the neuron against its firing threshold. If the net input exceeds the threshold, the neuronâs state is set to active; otherwise, it remains or becomes inactive.</p>
<p><inline-formula><inline-graphic xlink:href="529673v5_inline283.gif" mime-subtype="gif" mimetype="image"/></inline-formula>: This sum represents the total synaptic input to neuron <italic>i</italic> from all neurons <italic>j</italic> in population <italic>Î²</italic> at time <italic>t. j</italic> at time <italic>t</italic>.</p>
<p><inline-formula><inline-graphic xlink:href="529673v5_inline284.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the synaptic weight from neuron <italic>j</italic> in population <italic>Î²</italic> to neuron <italic>i</italic> in population <italic>Î±</italic>, and <inline-formula><inline-graphic xlink:href="529673v5_inline285.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the state of neuron <italic>j</italic> at time <italic>t</italic>.</p>
<p><inline-formula><inline-graphic xlink:href="529673v5_inline286.gif" mime-subtype="gif" mimetype="image"/></inline-formula>: The firing threshold of neuron <italic>i</italic> in population <italic>Î±</italic>. It is the value against which the net synaptic input is compared to determine whether neuron <italic>i</italic> will fire (transition to state 1) or not (remain in state 0).</p>
<p><italic>Î±</italic> = {<italic>E, I</italic>}, <italic>Î²</italic> = {<italic>E, I, X}</italic> : Represents a specific population of neurons within the network. E: excitatory neurons, I: inhibitory neurons, or X: external source of neurons that provide inputs to the network but are not influenced by the networkâs internal dynamics.</p>
</sec>
<sec id="s7i2">
<label>6.9.2</label>
<title>Firing Rate Correlation r</title>
<p>The mean firing rate correlation <italic>E</italic>(<italic>r</italic>) scales inversely with the network size <italic>N</italic>, specifically, <italic>E</italic>(<italic>r</italic>) â¼ 1<italic>/N</italic>. The standard deviation <italic>Ï</italic><sub><italic>r</italic></sub> of <italic>r</italic> decays only as <inline-formula><inline-graphic xlink:href="529673v5_inline287.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="bibr" rid="c53">53</xref>).</p>
<p>Given that the variance of <italic>r</italic>, denoted as Var(<italic>r</italic>), is <inline-formula><inline-graphic xlink:href="529673v5_inline288.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and the expected value of <italic>r</italic>, denoted as <italic>E</italic>(<italic>r</italic>), is <inline-formula><inline-graphic xlink:href="529673v5_inline289.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>N</italic> is the sample size, and <italic>a</italic> and <italic>b</italic> are constants, we aim to calculate <italic>E</italic>(<italic>r</italic><sup><italic>2</italic></sup>), the expected value of the square of the correlation coefficient <italic>r</italic>.</p>
<p>The term <inline-formula><inline-graphic xlink:href="529673v5_inline290.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in PR dimension is given by:
<disp-formula id="eqnS117">
<graphic xlink:href="529673v5_eqnS117.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Substituting <inline-formula><inline-graphic xlink:href="529673v5_inline291.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline292.gif" mime-subtype="gif" mimetype="image"/></inline-formula> into the equation, we get:
<disp-formula id="eqnS118">
<graphic xlink:href="529673v5_eqnS118.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
<p>Thus in PR dimension <inline-formula><inline-graphic xlink:href="529673v5_inline293.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the term <inline-formula><inline-graphic xlink:href="529673v5_inline294.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="529673v5_inline295.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are of the same order, and the PR dimension will not reach the upper bound <inline-formula><inline-graphic xlink:href="529673v5_inline296.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
</sec>
</sec>
</sec>
<sec id="s8">
<title>Supplementary videos</title>
<fig id="figMS1" position="float" fig-type="figure">
<label>Movie S1.</label>
<caption><title>Neural activity patterns in anatomical and functional space during hunting (click <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=sQ5uq_MiwDg">here</ext-link>).</title>
<p>Single-trial examples of fish 1 and fish 3. <bold>A</bold>. Inferred firing rate activity in anatomical space. Scale bar, 100 Âµm. <bold>B</bold>. Inferred firing rate activity in functional space. Functional space organization of the control data inferred by fitting the ERM and MDS in section 2.4. The cyan ellipse serves as a visual aid for the cluster size: it encloses 95% of the neurons belonging to that cluster (Methods). The inset illustrates the functional space organization, similar to that shown in <xref rid="figS15" ref-type="fig">Fig. S15C</xref>. The colorbars in panels <bold>A</bold> and <bold>B</bold> depict the inferred activity magnitude of individual neurons. <bold>C</bold>. Simultaneous behavior recording alongside the neural activity.</p>
<p>Time, seconds.</p></caption>
<graphic xlink:href="529673v5_figMS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We are grateful to Liqun Luo and Changsong Zhou for their helpful suggestions on our manuscript. QW thanks Hideaki Shimazaki for the suggestion that the functional space could be the feature space for sensory coding. QW thanks Jia Lou for improving the illustration in <xref rid="fig1" ref-type="fig">Figures 1</xref> and <xref rid="fig3" ref-type="fig">3</xref>. QW was supported by the STI2030-Major Projects 2022ZD0211900 and the NSFC-32071008 from the National Science Foundation of China. YH was supported by ECS-26303921 from the Research Grants Council of Hong Kong.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Foster</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Nuyujukian</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name>, and <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name></person-group> <article-title>Neural population dynamics during reaching</article-title>. <source>Nature</source>, <volume>487</volume>(<issue>7405</issue>):<fpage>51</fpage>â<lpage>56</lpage>, <month>July</month> <year>2012</year>. doi: <pub-id pub-id-type="doi">10.1038/nature11129</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Rich</surname>, <given-names>P. D.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>A. K.</given-names></string-name>, and <string-name><surname>Sharpee</surname>, <given-names>T. O.</given-names></string-name></person-group> <article-title>Hippocampal spatial representations exhibit a hyperbolic geometry that expands with experience</article-title>. <source>Nature Neuroscience</source>, <volume>26</volume>(<issue>1</issue>):<fpage>131</fpage>â<lpage>139</lpage>, <month>Jan</month>. <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1038/s41593-022-01212-4</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> and <string-name><surname>Wei</surname>, <given-names>X.-X.</given-names></string-name></person-group> <article-title>Neural tuning and representational geometry</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>22</volume>(<issue>11</issue>): <fpage>703</fpage>â<lpage>718</lpage>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1038/s41583-021-00502-3</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chung</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name></person-group> <article-title>Neural population geometry: An approach for understanding biological and artificial neural networks</article-title>. <source>Current opinion in neurobiology</source>, <volume>70</volume>:<fpage>137</fpage>â<lpage>144</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name> and <string-name><surname>Yu</surname>, <given-names>B. M.</given-names></string-name></person-group> <article-title>Dimensionality reduction for large-scale neural recordings</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>11</issue>): <fpage>1500</fpage>â<lpage>1509</lpage>, <month>Nov</month>. <year>2014</year>. doi: <pub-id pub-id-type="doi">10.1038/nn.3776</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name></person-group> <article-title>High-dimensional geometry of population responses in visual cortex</article-title>. <source>Nature</source>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1038/s41586-019-1346-5</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Si</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kanwal</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Tabone</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Baron</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Berck</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vignoud</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Samuel</surname>, <given-names>A. D.</given-names></string-name></person-group> <article-title>Structured odorant response patterns across a complete olfactory receptor neuron population</article-title>. <source>Neuron</source>, <volume>101</volume>(<issue>5</issue>):<fpage>950</fpage>â<lpage>962.e7,</lpage> <month>Mar</month>. <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.12.030</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mante</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Sussillo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name>, and <string-name><surname>Newsome</surname>, <given-names>W. T.</given-names></string-name></person-group> <article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title>. <source>Nature</source>, <volume>503</volume>(<issue>7474</issue>):<fpage>78</fpage>â<lpage>84</lpage>, <month>Nov</month>. <year>2013</year>. doi: <pub-id pub-id-type="doi">10.1038/nature12742</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Tipparaju</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Li</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>Thalamus-driven functional populations in frontal cortex support decision-making</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>10</issue>):<fpage>1339</fpage>â<lpage>1352</lpage>, <month>Oct</month>. <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1038/s41593-022-01171-w</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xie</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.-J.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Min</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title>. <source>Science</source>, <volume>375</volume>(<issue>6581</issue>):<fpage>632</fpage>â<lpage>639</lpage>, <month>Feb</month>. <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1126/science.abm0204</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Shipley</surname>, <given-names>F. B.</given-names></string-name>, <string-name><surname>Linder</surname>, <given-names>A. N.</given-names></string-name>, <string-name><surname>Plummer</surname>, <given-names>G. S.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Setru</surname>, <given-names>S. U.</given-names></string-name>, <string-name><surname>Shaevitz</surname>, <given-names>J. W.</given-names></string-name>, and <string-name><surname>Leifer</surname>, <given-names>A. M.</given-names></string-name></person-group> <article-title>Whole-brain calcium imaging with cellular resolution in freely behaving Caenorhabditis elegans</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>8</issue>):<fpage>E1074</fpage>â<lpage>E1081</lpage>, <month>Feb</month>. <year>2016</year>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1507110112</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LindÃ©n</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Vestergaard</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Berg</surname>, <given-names>R. W.</given-names></string-name></person-group> <article-title>Movement is governed by rotational neural dynamics in spinal motor networks</article-title>. <source>Nature</source>, <volume>610</volume>(<issue>7932</issue>):<fpage>526</fpage>â<lpage>531</lpage>, <month>Oct</month>. <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1038/s41586-022-05293-w</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Urai</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Doiron</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Leifer</surname>, <given-names>A. M.</given-names></string-name>, and <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name></person-group> <article-title>Large-scale neural recordings call for new insights to link brain and behavior</article-title>. <source>Nature Neuroscience</source>, <volume>25</volume>(<issue>1</issue>):<fpage>11</fpage>â<lpage>19</lpage>, <month>Jan</month>. <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1038/s41593-021-00980-9</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>BuzsÃ¡ki</surname>, <given-names>G.</given-names></string-name></person-group> <article-title>Large-scale recording of neuronal ensembles</article-title>. <source>Nature Neuroscience</source>, <volume>7</volume>(<issue>5</issue>):<fpage>446</fpage>â<lpage>451</lpage>, <month>May</month> <year>2004</year>. doi: <pub-id pub-id-type="doi">10.1038/nn1233</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahrens</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Orger</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Robson</surname>, <given-names>D. N.</given-names></string-name>, <string-name><surname>Schier</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Engert</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Portugues</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Brain-wide neuronal dynamics during motor adaptation in zebrafish</article-title>. <source>Nature</source>, <volume>485</volume>(<issue>7399</issue>):<fpage>471</fpage>â<lpage>477</lpage>, <month>May</month> <year>2012</year>. doi: <pub-id pub-id-type="doi">10.1038/nature11057</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jun</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Siegle</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Denman</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Bauza</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Barbarits</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Anastassiou</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Andrei</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Aydin Barbic</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Blanche</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Bonin</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Couto</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dutta</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Gratiy</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Gutnisky</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>HÃ¤usser</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Karsh</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ledochowitsch</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lopez</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Mitelut</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Musa</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Okun</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Putzeys</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rich</surname>, <given-names>P. D.</given-names></string-name>, <string-name><surname>Rossant</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>W.-l.</given-names></string-name>, <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>OâKeefe</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>T. D.</given-names></string-name></person-group> <article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title>. <source>Nature</source>, <volume>551</volume>(<issue>7679</issue>):<fpage>232</fpage>â<lpage>236</lpage>, <month>Nov</month>. <year>2017</year>. doi: <pub-id pub-id-type="doi">10.1038/nature24636</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stevenson</surname>, <given-names>I. H.</given-names></string-name> and <string-name><surname>Kording</surname>, <given-names>K. P.</given-names></string-name></person-group> <article-title>How advances in neural recording affect data analysis</article-title>. <source>Nature neuroscience</source>, <volume>14</volume>(<issue>2</issue>): <fpage>139</fpage>â<lpage>142</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sofroniew</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Flickinger</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>King</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Svoboda</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</article-title>. <source>eLife</source>, <volume>5</volume>:<issue>e14472</issue>, <month>June</month> <year>2016</year>. doi: <pub-id pub-id-type="doi">10.7554/eLife.14472</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Witvliet</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hernandez-Nunez</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Linderman</surname>, <given-names>S. W.</given-names></string-name>, <string-name><surname>Samuel</surname>, <given-names>A. D.</given-names></string-name>, and <string-name><surname>Venkatachalam</surname>, <given-names>V.</given-names></string-name></person-group> <article-title>Imaging whole-brain activity to understand behaviour</article-title>. <source>Nature Reviews Physics</source>, <volume>4</volume>(<issue>5</issue>):<fpage>292</fpage>â<lpage>305</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meshulam</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gauthier</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Brody</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Tank</surname>, <given-names>D. W.</given-names></string-name>, and <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Coarse graining, fixed points, and scaling in a large population of neurons</article-title>. <source>Physical Review Letters</source>, <volume>123</volume>:<issue>178103</issue>, <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevLett.123.178103</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Demas</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Manley</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Tejera</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Barber</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Traub</surname>, <given-names>F. M.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Vaziri</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>High-speed, cortex-wide volumetric recording of neuroactivity at cellular resolution using light beads microscopy</article-title>. <source>Nature Methods</source>, <volume>18</volume>(<issue>9</issue>):<fpage>1103</fpage>â<lpage>1111</lpage>, <month>Sept</month>. <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1038/s41592-021-01239-8</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Musall</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Juavinett</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Gluf</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Churchland</surname>, <given-names>A. K.</given-names></string-name></person-group> <article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>10</issue>):<fpage>1677</fpage>â<lpage>1686</lpage>, <month>Oct</month>. <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Reddy</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name></person-group> <article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title>. <source>Science</source>, <volume>364</volume>(<issue>6437</issue>):<fpage>eaav7893</fpage>, <month>Apr</month>. <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleinfeld</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Luan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Mitra</surname>, <given-names>P. P.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Sarpeshkar</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Shepard</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>T. D.</given-names></string-name></person-group> <article-title>Can one concurrently record electrical spikes from every neuron in a mammalian brain?</article-title> <source>Neuron</source>, <volume>103</volume>(<issue>6</issue>):<fpage>1005</fpage>â<lpage>1015</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Recanatesi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ocker</surname>, <given-names>G. K.</given-names></string-name>, <string-name><surname>Buice</surname>, <given-names>M. A.</given-names></string-name>, and <string-name><surname>Shea-Brown</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>Dimensionality in recurrent spiking networks: Global trends in activity and local origins in connectivity</article-title>. <source>PLoS computational biology</source>, <volume>15</volume>(<issue>7</issue>):<fpage>e1006446</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Litwin-Kumar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name>, <string-name><surname>Axel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sompolinsky</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Abbott</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Optimal degrees of synaptic connectivity</article-title>. <source>Neuron</source>, <volume>93</volume>(<issue>5</issue>):<fpage>1153</fpage>â<lpage>1164</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Gao</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Trautmann</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Santhanam</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Shenoy</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>A theory of multineuronal dimensionality, dynamics and measurement</article-title>. <source>BioRxiv</source>, page <fpage>214262</fpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clark</surname>, <given-names>D. G.</given-names></string-name>, <string-name><surname>Abbott</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Litwin-Kumar</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Dimension of activity in random neural networks</article-title>. <source>Physical Review Letters</source>, <volume>131</volume> (<issue>11</issue>):<fpage>118401</fpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Dahmen</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Recanatesi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ocker</surname>, <given-names>G. K.</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Helias</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Shea-Brown</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>Strong coupling and local control of dimensionality across brain areas</article-title>. <source>Biorxiv</source>, <year>2020</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cong</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Chai</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Shang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Bai</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Du</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Wen</surname>, <given-names>Q.</given-names></string-name></person-group> <article-title>Rapid whole brain imaging of neural activity in freely behaving larval zebrafish (Danio rerio)</article-title>. <source>eLife</source>, <volume>6</volume>:<elocation-id>e28158</elocation-id>, <month>Sept</month>. <year>2017</year>. doi: <pub-id pub-id-type="doi">10.7554/eLife.28158</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Sompolinsky</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>The spectrum of covariance matrices of randomly connected recurrent neuronal networks with linear dynamics</article-title>. <source>PLoS Computational Biology</source>, <volume>18</volume>, 7 <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1010327</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morales</surname>, <given-names>G. B.</given-names></string-name>, <string-name><surname>di Santo</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>MuÃ±oz</surname>, <given-names>M. A.</given-names></string-name></person-group> <article-title>Quasiuniversal scaling in mouse-brain neuronal activity stems from edge-of-instability critical dynamics</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>120</volume>(<issue>9</issue>):<fpage>e2208998120</fpage>, <month>Feb</month>. <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1073/pnas.2208998120</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Mu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Kuan</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Nikitchenko</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Randlett</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Gavornik</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Sompolinsky</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Engert</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Ahrens</surname>, <given-names>M. B.</given-names></string-name></person-group> <article-title>Brain-wide organization of neuronal activity and convergent sensorimotor transformations in larval zebrafish</article-title>. <source>Neuron</source>, <volume>100</volume>(<issue>4</issue>):<fpage>876</fpage>â<lpage>890.e5,</lpage> <month>Nov</month>. <year>2018</year>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.042</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>MÃ©zard</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Parisi</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Zee</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Spectra of euclidean random matrices</article-title>. <source>Nuclear Physics B</source>, <volume>559</volume>(<issue>3</issue>):<fpage>689</fpage>â<lpage>701</lpage>, <month>Oct</month>. <year>1999</year>. doi: <pub-id pub-id-type="doi">10.1016/S0550-3213(99)00428-9</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Goetschy</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Skipetrov</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Euclidean random matrices and their applications in physics</article-title>. <source>arXiv</source>, <year>2013</year>. doi: <pub-id pub-id-type="doi">10.48550/ARXIV.1303.2880</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morrell</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Nemenman</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Sederberg</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Neural criticality from effective latent variables</article-title>. <source>eLife</source>, <volume>12</volume>:<issue>RP89337</issue>, <month>Mar</month>. <year>2024</year>. doi: <pub-id pub-id-type="doi">10.7554/eLife.89337.3</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname>, <given-names>D. H.</given-names></string-name> and <string-name><surname>Wiesel</surname>, <given-names>T. N.</given-names></string-name></person-group> <article-title>Receptive fields of single neurones in the catâs striate cortex</article-title>. <source>The Journal of physiology</source>, <volume>148</volume> (<issue>3</issue>):<fpage>574</fpage>, <year>1959</year>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stefanini</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Kushnir</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Jimenez</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Jennings</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Woods</surname>, <given-names>N. I.</given-names></string-name>, <string-name><surname>Stuber</surname>, <given-names>G. D.</given-names></string-name>, <string-name><surname>Kheirbek</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Hen</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Fusi</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>A distributed neural code in the dentate gyrus and in CA1</article-title>. <source>Neuron</source>, <volume>107</volume>(<issue>4</issue>):<fpage>703</fpage>â<lpage>716.e4,</lpage> <month>Aug</month>. <year>2020</year>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2020.05.022</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kropff</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Carmichael</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name>, and <string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name></person-group> <article-title>Speed cells in the medial entorhinal cortex</article-title>. <source>Nature</source>, <volume>523</volume>(<issue>7561</issue>): <fpage>419</fpage>â<lpage>424</lpage>, <month>July</month> <year>2015</year>. doi: <pub-id pub-id-type="doi">10.1038/nature14622</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>OâKeefe</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Place units in the hippocampus of the freely moving rat</article-title>. <source>Experimental Neurology</source>, <volume>51</volume>(<issue>1</issue>):<fpage>78</fpage>â<lpage>109</lpage>, <month>Jan</month>. <year>1976</year>. doi: <pub-id pub-id-type="doi">10.1016/0014-4886(76)90055-8</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name>, <string-name><surname>Kropff</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name></person-group> <article-title>Place cells, grid cells, and the brainâs spatial representation system</article-title>. <source>Annual Review of Neuroscience</source>, <volume>31</volume>(<issue>1</issue>):<fpage>69</fpage>â<lpage>89</lpage>, <month>July</month> <year>2008</year>. doi: <pub-id pub-id-type="doi">10.1146/annurev.neuro.31.061307.090723</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tingley</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>BuzsÃ¡ki</surname>, <given-names>G.</given-names></string-name></person-group> <article-title>Transformation of a Spatial Map across the Hippocampal-Lateral Septal Circuit</article-title>. <source>Neuron</source>, <volume>98</volume>(<issue>6</issue>): <fpage>1229</fpage>â<lpage>1242.e5,</lpage> <month>June</month> <year>2018</year>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.04.028</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tian</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Shirhatti</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Greenspon</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Downey</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Freedman</surname>, <given-names>D. J.</given-names></string-name>, and <string-name><surname>Doiron</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Neuronal firing rate diversity lowers the dimension of population covariability</article-title>. <source>bioRxiv</source>, <year>2024</year>. doi: <pub-id pub-id-type="doi">10.1101/2024.08.30.610535</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grewe</surname>, <given-names>B. F.</given-names></string-name> and <string-name><surname>Helmchen</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Optical probing of neuronal ensemble activity</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>19</volume>(<issue>5</issue>): <fpage>520</fpage>â<lpage>529</lpage>, <month>Oct</month>. <year>2009</year>. doi: <pub-id pub-id-type="doi">10.1016/j.conb.2009.09.003</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gauthier</surname>, <given-names>J. L.</given-names></string-name> and <string-name><surname>Tank</surname>, <given-names>D. W.</given-names></string-name></person-group> <article-title>A dedicated population for reward coding in the hippocampus</article-title>. <source>Neuron</source>, <volume>99</volume>(<issue>1</issue>):<fpage>179</fpage>â<lpage>193.e7,</lpage> <month>July</month> <year>2018</year>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.008</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cox</surname>, <given-names>T.</given-names></string-name> and <string-name><surname>Cox</surname>, <given-names>M.</given-names></string-name></person-group> <source>Multidimensional Scaling</source>. <publisher-name>Chapman and Hall/CRC</publisher-name>, <edition>0</edition> edition, <month>Sept</month>. <year>2000</year>. ISBN <isbn>9780367801700</isbn>. doi: <pub-id pub-id-type="doi">10.1201/9780367801700</pub-id>. URL <ext-link ext-link-type="uri" xlink:href="https://www.taylorfrancis.com/books/9781420036121">https://www.taylorfrancis.com/books/9781420036121</ext-link>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bianco</surname>, <given-names>I. H.</given-names></string-name>, <string-name><surname>Kampff</surname>, <given-names>A. R.</given-names></string-name>, and <string-name><surname>Engert</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Prey capture behavior evoked by simple visual stimuli in larval zebrafish</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>5</volume>, <year>2011</year>. doi: <pub-id pub-id-type="doi">10.3389/fnsys.2011.00101</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kunst</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Laurell</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Mokayes</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Kramer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kubo</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Fernandes</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>FÃ¶rster</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Dal Maschio</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Baier</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>A cellular-resolution atlas of the larval zebrafish brain</article-title>. <source>Neuron</source>, <volume>103</volume>(<issue>1</issue>):<fpage>21</fpage>â<lpage>38</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kardar</surname>, <given-names>M.</given-names></string-name></person-group> <source>Statistical Physics of Fields</source>. <publisher-name>Cambridge University Press</publisher-name>, <publisher-loc>Cambridge</publisher-loc>, <year>2007</year>. ISBN <isbn>9780511815881</isbn>. doi: <pub-id pub-id-type="doi">10.1017/CBO9780511815881</pub-id>. URL <ext-link ext-link-type="uri" xlink:href="https://www.cambridge.org/core/books/statistical-physics-of-fields/06F49D11030FB3108683F413269DE945">https://www.cambridge.org/core/books/statistical-physics-of-fields/06F49D11030FB3108683F413269DE945</ext-link>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beggs</surname>, <given-names>J. M.</given-names></string-name> and <string-name><surname>Plenz</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Neuronal avalanches in neocortical circuits</article-title>. <source>Journal of Neuroscience</source>, <volume>23</volume>(<issue>35</issue>):<fpage>11167</fpage>â<lpage>11177</lpage>, <month>Dec</month>. <year>2003</year>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-35-11167.2003</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dahmen</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>GrÃ¼n</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Diesmann</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Helias</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Second type of criticality in the brain uncovers rich multiple-neuron dynamics</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>116</volume>(<issue>26</issue>):<fpage>13051</fpage>â<lpage>13060</lpage>, <month>June</month> <year>2019</year>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1818972116</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morrell</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Sederberg</surname>, <given-names>A. J.</given-names></string-name>, and <string-name><surname>Nemenman</surname>, <given-names>I.</given-names></string-name></person-group> <article-title>Latent dynamical variables produce signatures of spatiotemporal criticality in large biological systems</article-title>. <source>Physical Review Letters</source>, <volume>126</volume>:<fpage>118302</fpage>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevLett.126.118302</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Renart</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>De La Rocha</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bartho</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hollender</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Parga</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Reyes</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name></person-group> <article-title>The asynchronous state in cortical circuits</article-title>. <source>science</source>, <volume>327</volume>(<issue>5965</issue>):<fpage>587</fpage>â<lpage>590</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manley</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Barber</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Demas</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Meyer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Traub</surname>, <given-names>F. M.</given-names></string-name>, and <string-name><surname>Vaziri</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Simultaneous, cortex-wide dynamics of up to 1 million neurons reveal unbounded scaling of dimensionality with neuron number</article-title>. <source>Neuron</source>, <year>2024</year>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoffmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Henninger</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Veith</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Richter</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Judkewitz</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Blazed oblique plane microscopy reveals scale-invariant inference of brain-wide population activity</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>):<fpage>8019</fpage>, <month>Dec</month>. <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1038/s41467-023-43741-x</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Moosavi</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Hindupur</surname>, <given-names>S. S. R.</given-names></string-name>, and <string-name><surname>Shimazaki</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>Population coding under the scale-invariance of high-dimensional noise</article-title>, <source>bioRxiv</source> <month>Aug</month>. <year>2024</year>. <pub-id pub-id-type="doi">10.1101/2024.08.23.608710</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mamidanna</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cury</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Abe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Murthy</surname>, <given-names>V. N.</given-names></string-name>, <string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name>, and <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature Neuroscience</source>, <volume>21</volume>(<issue>9</issue>):<fpage>1281</fpage>â<lpage>1289</lpage>, <month>Sept</month>. <year>2018</year>. doi: <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tabor</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Marquart</surname>, <given-names>G. D.</given-names></string-name>, <string-name><surname>Hurt</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Geoca</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Bhandiwad</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Subedi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sinclair</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Rose</surname>, <given-names>H. M.</given-names></string-name>, <string-name><surname>Polys</surname>, <given-names>N. F.</given-names></string-name>, and <string-name><surname>Burgess</surname>, <given-names>H. A.</given-names></string-name></person-group> <article-title>Brain-wide cellular resolution imaging of Cre transgenic zebrafish lines for functional circuit-mapping</article-title>. <source>eLife</source>, <volume>8</volume>:<elocation-id>e42687</elocation-id>, <month>Feb</month>. <year>2019</year>. doi: <pub-id pub-id-type="doi">10.7554/eLife.42687</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Studholme</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hill</surname>, <given-names>D. L. G.</given-names></string-name>, and <string-name><surname>Hawkes</surname>, <given-names>D. J.</given-names></string-name></person-group> <article-title>Automated three-dimensional registration of magnetic resonance and positron emission tomography brain images by multiresolution optimization of voxel similarity measures</article-title>. <source>Medical Physics</source>, <volume>24</volume>(<issue>1</issue>):<fpage>25</fpage>â<lpage>35</lpage>, <year>1997</year>. doi: <pub-id pub-id-type="doi">10.1118/1.598130</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rueckert</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Sonoda</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hayes</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hill</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Leach</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Hawkes</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Nonrigid registration using free-form deformations: application to breast MR images</article-title>. <source>IEEE Transactions on Medical Imaging</source>, <volume>18</volume>(<issue>8</issue>):<fpage>712</fpage>â<lpage>721</lpage>, <month>Aug</month>. <year>1999</year>. doi: <pub-id pub-id-type="doi">10.1109/42.796284</pub-id>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedrich</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Paninski</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Fast online deconvolution of calcium imaging data</article-title>. <source>PLOS Computational Biology</source>, <volume>13</volume> (<issue>3</issue>):<fpage>e1005423</fpage>, <month>Mar</month>. <year>2017</year>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005423</pub-id>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>On simplicity and complexity in the brave new world of large-scale neuroscience</article-title>. <source>Current opinion in neurobiology</source>, <volume>32</volume>:<fpage>148</fpage>â<lpage>155</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bordenave</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Eigenvalues of Euclidean random matrices</article-title>. <source>Random Structures and Algorithms</source>, <volume>33</volume>(<issue>4</issue>):<fpage>515</fpage>â<lpage>532</lpage>, <month>Dec</month>. <year>2008</year>. doi: <pub-id pub-id-type="doi">10.1002/rsa.20228</pub-id>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rudin</surname>, <given-names>W.</given-names></string-name></person-group> <source>Fourier Analysis on Groups</source>. <publisher-name>Wiley</publisher-name>, <edition>1</edition> edition, <month>Jan</month>. <year>1990</year>. ISBN <isbn>9780470744819</isbn>. doi: <pub-id pub-id-type="doi">10.1002/9781118165621</pub-id>. URL <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118165621">https://onlinelibrary.wiley.com/doi/book/10.1002/9781118165621</ext-link>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knapp</surname>, <given-names>T. R.</given-names></string-name></person-group> <article-title>Canonical correlation analysis: a general parametric significance-testing system</article-title>. <source>Psychological Bulletin</source>, <volume>85</volume>(<issue>2</issue>): <fpage>410</fpage>, <year>1978</year>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bradde</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>PCA meets RG</article-title>. <source>Journal of Statistical Physics</source>, <volume>167</volume>(<issue>3</issue>):<fpage>462</fpage>â<lpage>475</lpage>, <month>May</month> <year>2017</year>. doi: <pub-id pub-id-type="doi">10.1007/s10955-017-1770-6</pub-id>.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Meshulam</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gauthier</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Brody</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Tank</surname>, <given-names>D. W.</given-names></string-name>, and <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Coarseâgraining and hints of scaling in a population of 1000+ neurons</article-title>. <source>arXiv</source> preprint arXiv:<pub-id pub-id-type="arxiv">1812.11904</pub-id>, <year>2018</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.1812.11904</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100666.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Latham</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study shows a surprising scale-invariance of the covariance spectrum of large-scale recordings in the zebrafish brain in vivo. A <bold>convincing</bold> analysis demonstrates that a Euclidean random matrix model of the covariance matrix recapitulates these properties. The results provide several new and insightful approaches for probing large-scale neural recordings.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100666.2.sa1</article-id>
<title-group>
<article-title>Joint public review</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors examine the eigenvalue spectrum of the covariance matrix of neural recordings in the whole-brain larval zebrafish during hunting and spontaneous behavior. They find that the spectrum is approximately power law, and, more importantly, exhibits scale-invariance under random subsampling of neurons. This property is not exhibited by conventional models of covariance spectra, motivating the introduction of the Euclidean random matrix model. The authors show that this tractable model captures the scale invariance they observe. They also examine the effects of subsampling based on anatomical location or functional relationships. Finally, they discuss the benefit of neural codes which can be subsampled without significant loss of information.</p>
<p>Strengths:</p>
<p>With large-scale neural recordings becoming increasingly common, neuroscientists are faced with the question: how should we analyze them? To address that question, this paper proposes the Euclidean random matrix model, which embeds neurons randomly in an abstract feature space. This model is analytically tractable and matches two nontrivial features of the covariance matrix: approximate power law scaling, and invariance under subsampling. It thus introduces an important conceptual and technical advance for understanding large-scale simultaneously recorded neural activity.</p>
<p>Comment:</p>
<p>Are there quantitative comparisons of the collapse indices for the null models in Figure 2 and the data covariance in 2F? If so, this could be potentially useful to report.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100666.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Zezhen</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0006-1401-9871</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mai</surname>
<given-names>Weihao</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2320-2276</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Chai</surname>
<given-names>Yuming</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0184-1824</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Qi</surname>
<given-names>Kexin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ren</surname>
<given-names>Hongtai</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shen</surname>
<given-names>Chen</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Shiwu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tan</surname>
<given-names>Guodong</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hu</surname>
<given-names>Yu</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0790-1605</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Wen</surname>
<given-names>Quan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0268-8403</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authorsâ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p>Summary:</p>
<p>The authors examine the eigenvalue spectrum of the covariance matrix of neural recordings in the whole-brain larval zebrafish during hunting and spontaneous behavior. They find that the spectrum is approximately power law, and, more importantly, exhibits scale-invariance under random subsampling of neurons. This property is not exhibited by conventional models of covariance spectra, motivating the introduction of the Euclidean random matrix model. The authors show that this tractable model captures the scale invariance they observe. They also examine the effects of subsampling based on anatomical location or functional relationships. Finally, they briefly discuss the benefit of neural codes which can be subsampled without significant loss of information.</p>
<p>Strengths:</p>
<p>With large-scale neural recordings becoming increasingly common, neuroscientists are faced with the question: how should we analyze them? To address that question, this paper proposes the Euclidean random matrix model, which embeds neurons randomly in an abstract feature space. This model is analytically tractable and matches two nontrivial features of the covariance matrix: approximate power law scaling, and invariance under subsampling. It thus introduces an important conceptual and technical advance for understanding large-scale simultaneously recorded neural activity.</p>
<p>Weaknesses:</p>
<p>The downside of using summary statistics is that they can be hard to interpret. Often the finding of scale invariance, and approximate power law behavior, points to something interesting. But here caution is in order: for instance, most critical phenomena in neural activity have been explained by relatively simple models that have very little to do with computation (Aitchison et al., PLoS CB 12:e1005110, 2016; Morrell et al., eLife 12, RP89337, 2024). Whether the same holds for the properties found here remains an open question.</p>
</disp-quote>
<p>We are grateful for the thorough and constructive feedback provided on our manuscript. We have addressed each point raised by you.</p>
<p>Regarding the main concern about power law behavior and scale invariance, we would like to clarify that our study does not aim to establish criticality. Instead, we focus on describing and understanding a specific scale-invariant property in terms of collapsed eigenspectra in neural activity. We tested Morrell et al.âs latent-variable model (eLife 12, RP89337, 2024, [1]), where a slowly varying latent factor drives population activity. Although it produces a seemingly power-law-like spectrum, random sampling does not replicate the strict spectral collapse observed in our data (second row in Fig. S23). This highlights that simply adding latent factors does not fully recapitulate the scale invariance we measure, suggesting richer or more intricate processes may be involved in real neural recordings.</p>
<p>Specifically, we have incorporated five key revisions.</p>
<p>â¢ As mentioned, we evaluated the latent variable model proposed by Morrell et al., and found that they fail to reproduce the scale-invariant eigenspectra observed in our data; these results are now presented in the Discussion section and supported by a new Supplementary Figure (Fig. S23).</p>
<p>â¢ We included a comparison with the findings of Manley et al. (2024 [2]) regarding the issue of saturating dimension in the Discussion section, highlighting the methodological differences and their implications.</p>
<p>â¢ We added a new mathematical derivation in the Methods section, elucidating the bounded dimensionality using the spectral properties of our model. â¢ We have added a sentence in the Discussion section to further emphasize the robustness of our findings by demonstrating their consistency across diverse datasets and experimental techniques.</p>
<p>â¢ We have incorporated a brief discussion on the implications for neural coding (lines 330-332). In particular, Fisher information can become unbounded when the slope of the power-law rank plot is less than one, as highlighted in the recent work by Moosavi et al. (bioRxiv 2024.08.23.608710, Aug, 2024 [3]).</p>
<p>We believe these revisions address the concerns raised during the review process and collectively strengthen our manuscript to provides a more comprehensive and robust understanding of the geometry and dimensionality of brain-wide activity. We appreciate your consideration of our revised manuscript and look forward to your feedback.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p>In particular, in our experience replies to the reviewers are getting longer than the paper, and we (and Iâm sure you!) want to avoid that. Maybe just reply explicitly to the ones you disagree with? Weâre pretty flexible on our end.</p>
<p>(1) The main weakness, from our point of view, is whether the finding of scale invariance means something interesting, or should be expected from a null model. We can suggest such model; if it is inconsistent with the data, that would make the results far more interesting.</p>
<p>Morrell et al. (eLife 12, RP89337,2024 [1]) suggest a very simple model in which the whole population is driven by a slowly time-varying quantity. It would be nice to determine whether it matched this data. If it couldnât, that would add some evidence that there is something interesting going on.</p>
</disp-quote>
<p>We appreciate your insightful suggestion to consider the model proposed by Morrell et al. (eLife 12, RP89337, 2024 [1]), where a slowly time-varying quantity drives the entire neural population. We conducted simulations using parameters from Morrell et al. [4, 1], as detailed below.</p>
<p>Our simulations show that Morrellâs model can replicate a degree of scaleinvariance when using <italic>functional sampling</italic> or RG as referred to in Morrell et al, 2021, PRL [4] (FSap, Fig.S23A-D, Author response image 1). However, it fails to fully capture the scale-invariance of collapsing spectra we observed in data under <italic>random sampling</italic> (RSap, Fig.S23E-H). This discrepancy suggests that additional dynamics or structures in the neural activity are not captured by this simple model, indicating the presence of potentially novel and interesting features in the data that merit further investigation.</p>
<p>Unlike random sampling, the collapse of eigenspectra under functional sampling does not require a stringent condition on the kernel function <italic>f</italic>(<italic>x</italic>) in our ERM theory (see Discussion line 269-275), potentially explaining the differing results between Fig.S23A-D and Fig.S23E-H.</p>
<p>We have incorporated these findings into the Result section 2.1 (lines 100-101) and Discussion section (lines 277-282, quoted below):</p>
<p>âMorrell et al. [4, 1] suggested a simple model in which a slow time-varying factor influences the entire neural population. To explore the effects of latent variables, we assessed if this model explains the scale invariance in our data. The model posits that neural activity is primarily driven by a few shared latent factors. Simulations showed that the resulting eigenspectra differed considerably from our findings (Fig. S23). Although the Morrell model demonstrated a degree of scale invariance under functional sampling, it did not align with the scale-invariant features under random sampling observed in our data, suggesting that this simple model might not capture all crucial features in our observations.â</p>
<fig id="sa2fig1">
<label>Author response image 1:</label>
<caption>
<title>Morrellâs latent model.</title>
<p>A: We reproduce the results as presented in Morrell et al., PRL 126(11), 118302 (2021) [4]. Parameters are same as Fig. S23A. Sampled 16 to 256 neurons. Unlike in our study, the mean eigenvalues are not normalized to one. Dashed line: eigenvalues fitted to a power law. See also Morrell et al. [4] Fig.1C. Parameters are same as Author response image 1. <italic>Âµ</italic> is the power law exponent (black) of the fit, which is different from the <italic>Âµ</italic> parameter used to characterize the slow decay of the spatial correlation function, but corresponds to the parameter <italic>Î±</italic> in our study.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>(2) The quantification of the degree of scale invariance is done using a âcollapse indexâ (CI), which could be better explained/motivated. The fact that the measure is computed only for the non-leading eigenvalues makes sense but it is not clear when originally introduced. How does this measure compare to other measures of the distance between distributions?</p>
</disp-quote>
<p>We thank you for raising this important point regarding the explanation and motivation for our Collapse Index (CI). We defined the Collapse Index (CI) instead of other measures of distance between distributions for two main reasons. First, the CI provides an intuitive quantification of the shift of the eigenspectrum motivated by our high-density theory for the ERM model (Eq. 3, Fig. 4A). This high-density theory is only valid for large eigenvalues excluding the leading ones, and hence we compute the CI measure with a similar restriction of the range of area integration. Second, when using distribution to assess the collapse (e.g., we can use kernel density method to estimate the distribution of eigenvalues and then calculate the KL divergence between the two distributions), it is necessary to first estimate the distributions. This estimation step introduces errors, such as inaccuracies in estimating the probability of large eigenvalues.</p>
<p>We agree that a clearer explanation would enhance the manuscript and thus have made modifications accordingly. The CI is now introduced more clearly in the Results section (lines 145-148) and further detailed in the Methods section (lines 630-636). We have also revised the CI diagram in Fig. 4A to better illustrate the shift concept using a more intuitive cartoon representation.</p>
<disp-quote content-type="editor-comment">
<p>(3) The paper focuses on the case in which the dimensionality saturates to a finite value as the number of recorded neurons is increased. It would be useful to contrast with a case in which this does not occur. The paper would be strengthened by a comparison with Manley et al. 2024, which argued that, unlike this study, dimensionality of activity in spontaneously behaving head-fixed mice did not saturate.</p>
</disp-quote>
<p>Thank you for highlighting this comparison. We have included a discussion (lines 303-309) comparing our approach with Manley et al. (2024) [2]. While Manley et al. [2] primarily used shared variance component analysis (SVCA) to estimate neural dimensionality, they observed that using PCA led to dimensionality saturation (see Figure S4D, Manley et al. [2]), consistent with our findings (Fig. 2D). We acknowledge the value of SVCA as an alternative approach and agree that it is an interesting avenue for future research. In our study, we chose to use PCA for several reasons. PCA is a well-established and widely trusted method in the neuroscience community, with a proven track record of revealing meaningful patterns in neural data. Its mathematical properties are well understood, making it particularly suitable for our theoretical analysis. While we appreciate the insights that newer methods like SVCA can provide, we believe PCA remains the most appropriate tool for addressing our specific research questions.</p>
<disp-quote content-type="editor-comment">
<p>(4) More importantly, we donât understand why dimensionality saturates. For the rank plot given in Eq. 3,</p>
<p><inline-graphic xlink:href="elife-100666-inf2.jpg" mimetype="image" mime-subtype="jpeg"/></p>
<p>where k is rank. Using this, one can estimate sums over eigenvalues by integrals. Focusing on the N-dependence, we have</p>
<p><inline-graphic xlink:href="elife-100666-inf3.jpg" mimetype="image" mime-subtype="jpeg"/></p>
<p>This gives</p>
<p><inline-graphic xlink:href="elife-100666-inf4.jpg" mimetype="image" mime-subtype="jpeg"/></p>
<p>We donât think you ever told us what mu/d was (see point 13 below), but in the discussion you implied that it was around 1/2 (line 249). In that case, <italic>D<sub>PR</sub></italic> should be approximately linear in N. Could you explain why it isnât?</p>
</disp-quote>
<p>Thank you for your careful derivation. Along this line of calculations you suggested, we have now added derivations on using the ERM spectrum to estimate the upper bound of the dimension in the Methods (section 4.14.4). To deduce <italic>D<sub>PR</sub></italic> from the spectrum, we focus on the high-density region, where an analytical expression for large eigenvalues <italic>Î»</italic> is given by:</p>
<disp-formula id="sa2equ1">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ1.jpg" mimetype="image"/>
</disp-formula>
<p>Here, <italic>d</italic> is dimension of functional space, <italic>L</italic> is the linear size of functional space, <italic>Ï</italic> is the neuron density and <italic>Î³</italic> is the coefficient in Eq. (3), which only depends on <italic>d</italic>, <italic>Âµ</italic> and E(Ï<sup>2</sup>). The primary difference between your derivation and ours is that the eigenvalue <italic>Î»<sub>r</sub></italic> decays rapidly after the threshold <italic>r</italic> = <italic>Î²</italic>(<italic>N</italic>), which significantly affects the summations <inline-formula id="sa2equ2"><inline-graphic xlink:href="elife-100666-sa2-equ2.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> and <inline-formula id="sa2equ3"><inline-graphic xlink:href="elife-100666-sa2-equ3.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>. Since we did not discuss the small eigenvalues in the article, we represent them here as an unknown function <italic>Î·</italic>(<italic>r,N,L</italic>).</p>
<p>The sum <inline-formula id="sa2equ4"><inline-graphic xlink:href="elife-100666-sa2-equ4.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> is the trace of the covariance matrix <italic>C</italic>. As emphasized in the Methods section, without changing the properties the covariance spectrum, we always consider a normalized covariance matrix such that the mean neural activity variance E(Ï<sup>2</sup>) = 1. Thus</p>
<disp-formula id="sa2equ5">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ5.jpg" mimetype="image"/>
</disp-formula>
<p>rather than</p>
<disp-formula id="sa2equ6">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ6.jpg" mimetype="image"/>
</disp-formula>
<p>The issue stems from overlooking that Eq. (3) is valid only for large eigenvalues (<italic>Î» &gt;</italic> 1).</p>
<p>Using the CauchyâSchwarz inequality, we have a upper bound of <inline-formula id="sa2equ7"><inline-graphic xlink:href="elife-100666-sa2-equ7.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula></p>
<disp-formula id="sa2equ8">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ8.jpg" mimetype="image"/>
</disp-formula>
<p>Conversely, <inline-formula id="sa2equ9"><inline-graphic xlink:href="elife-100666-sa2-equ9.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> provides a lower bound of <inline-formula id="sa2equ10"><inline-graphic xlink:href="elife-100666-sa2-equ10.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>:</p>
<disp-formula id="sa2equ11">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ11.jpg" mimetype="image"/>
</disp-formula>
<p>As a result, we must have</p>
<disp-formula id="sa2equ12">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ12.jpg" mimetype="image"/>
</disp-formula>
<p>In random sampling (RSap), <italic>L</italic> is fixed. We thus must have a bounded dimensionality that is independent of <italic>N</italic> for our ERM model. In functional sampling (FSap), <italic>L</italic> varies while the neuronal density <italic>Ï</italic> is fixed, leading to a different scaling relationship of the upper bound, see Methods (section 4.14.4) for further discussion.</p>
<disp-quote content-type="editor-comment">
<p>(5) The authors work directly with ROIs rather than attempting to separate the signals from each neuron in an ROI. It would be worth discussing whether this has a significant effect on the results.</p>
</disp-quote>
<p>We appreciate your thoughtful question on the potential impact of using ROIs. The use of ROIs likely does not impact our key findings since they are validated across multiple datasets with various recording techniques and animal models, from zebrafish calcium imaging to mouse brain multi-electrode recordings (see Figure S2, S24). The consistency of the scale-invariant covariance spectrum in diverse datasets suggests that ROIs in zebrafish data do not significantly alter the conclusions, and they together enhance the generalizability of our results. We highlight this in the Discussion section (lines 319-323).</p>
<disp-quote content-type="editor-comment">
<p>(6) Does the Euclidean random matrix model allow the authors to infer the value of <italic>D</italic> or <italic>Âµ</italic>? Since the measured observables only depend on <italic>Âµ/D</italic> it seems that one cannot infer the latent dimension where distances between neurons are computed. Are there any experiments that one could, in principle, perform to measure D or mu? Currently the conclusion from the model and data is that <italic>D/Âµ</italic> is a large number so that the spectrum is independent of neuron density rho. What about the heterogeneity of the scales <italic>Ï<sub>i</sub></italic>, can this be constrained by data?</p>
</disp-quote>
<p>Measuring <italic>d</italic> and <italic>Âµ</italic> in the ERM Model</p>
<p>We agree with you that the individual values of <italic>d</italic> and <italic>Âµ</italic> cannot be determined separately from our analysis. In our analysis using the Euclidean Random Matrix (ERM) model, we fit the ratio <italic>Âµ/d</italic>, rather than the individual values of <italic>d</italic> (dimension of the functional space) or <italic>Âµ</italic> (exponent of the distance-dependent kernel function). This limitation is inherent because the modelâs predictions for observable quantities, such as the distribution of pairwise correlation, are dependent solely on this ratio.</p>
<p>Currently there are no directly targeted experiments to measure <italic>d</italic>. The dimensions of the functional space is largely a theoretical construct: it could serve to represent latent variables encoding cognitive factors that are distributed throughout the brain or specific sensory or motor feature maps within a particular brain region. It may also be viewed as the embedding space to describe functional connectivity between neurons. Thus, a direct experimental measurement of the dimensions of the functional space could be challenging. Although there are variations in the biological interpretation of the functional space, the consistent scale invariance observed across various brain regions indicates that the neuronal relationships within the functional space can be described by a uniform slowly decaying kernel function.</p>
<p>Regarding the Heterogeneity <italic>of</italic> <italic>Ï<sub>i</sub></italic></p>
<p>The heterogeneity of neuronal activity variances ( <italic>Ï<sub>i</sub></italic>) is a critical factor in our analysis. Our findings indicate that this heterogeneity:</p>
<p>(1) Enhances scale invariance: The covariance matrix spectrum, which incorporates the heterogeneity of <inline-formula id="sa2equ13"><inline-graphic xlink:href="elife-100666-sa2-equ13.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>, exhibits stronger scale invariance compared to the correlation matrix spectrum, which imposes <inline-formula id="sa2equ14"><inline-graphic xlink:href="elife-100666-sa2-equ14.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> for all neurons. This observation is supported by both experimental data and theoretical predictions from the ERM model, particularly in the intermediate density regime.</p>
<p>(2) Can be constrained by data: We fit a log-normal distribution to the experimentally observed <italic>Ï<sup>2</sup></italic> values to capture the heterogeneity in our model which leads to excellent agreement with data (section 4.8.1). Figure S10 provides evidence for this by directly comparing the eigenspectra obtained from experimental data (Fig S10A-F) with those generated by the fitted ERM model (Fig S10M-R). These results suggest that the data provides valuable information about the distribution of neuronal activity variances.</p>
<p>In conclusion, the ERM model and our analysis cannot separately determine <italic>d</italic> and <italic>Âµ</italic>. We also highlight that the neuronal activity variance heterogeneity, constrained by experimental data, plays a crucial role in improving the scale invariance.</p>
<disp-quote content-type="editor-comment">
<p>(7) Does the fitting procedure for the positions x in the latent space recover a ground truth in your statistical regime (for the number of recorded neurons)? Suppose you sampled some neurons from a Euclidean random matrix theory. Does the MDS technique the authors use recover the correct distances?</p>
</disp-quote>
<p>While sampling neurons from a Euclidean random matrix model, we demonstrated numerically that the MDS technique can accurately recover the true distances, provided that the true parameter <italic>f</italic>(<italic>x</italic>) is known. To quantify the precision of recovery, we applied the CCA analysis (Section 4.9) and compared the true coordinates <inline-formula id="sa2equ15"><inline-graphic xlink:href="elife-100666-sa2-equ15.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> from the original Euclidean random matrix with the fitted coordinates <inline-formula id="sa2equ16"><inline-graphic xlink:href="elife-100666-sa2-equ16.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> obtained through our MDS procedure. The CCA correlation between the true and fitted coordinates in each spatial dimension is nearly 1 (the difference from 1 is less than 10<sup>â7</sup>). When fitting with experimental data, one source of error arises from parameter estimation. To evaluate this, we assess the estimation error of the fitted parameters. When we choose <italic>Âµ</italic> = 0_.<italic>5 in our ERM model and then fit the distribution of the pairwise correlation (Eq. 21), the estimated parameter is <inline-graphic xlink:href="elife-100666-inf18.jpg" mimetype="image" mime-subtype="jpeg"/> = 0</italic>.<italic>503 Â± 0</italic>._007 (standard deviation). Then, we use the MDS-recovered distances to fit the coordinates with the fitted kernel function <inline-formula id="sa2equ17"><inline-graphic xlink:href="elife-100666-sa2-equ17.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> , which is determined by the fitted parameter <sup>â5</sup>).<inline-formula id="sa2equ18"><inline-graphic xlink:href="elife-100666-sa2-equ18.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>. The CCA correlation between the true and fitted coordinates in each direction remains nearly 1 (the difference from 1 is less than 10</p>
<disp-quote content-type="editor-comment">
<p>(8) l. 49: â... both the dimensionality and covariance spectrum remain invariant ...â. Just to be clear, if the spectrum is invariant, then the dimensionality automatically is too. Correct?</p>
</disp-quote>
<p>Thanks for the question. In fact, there is no direct causal relationship between eigenvalue spectrum invariance and dimensionality invariance as we elaborate below and added discussions in lines 311-317. For eigenvalue spectrum invariance, we focus on the large eigenvalues, whereas dimensionality invariance considers the second order statistics of all eigenvalues. Consequently, the invariance results for these two concepts may differ. And dimensional and spectral invariance have different requirements:</p>
<p>(1) The condition for dimensional saturation is finite mean square covariance</p>
<p>The participation ratio <italic>D<sub>PR</sub></italic> for random sampling (RSap) is given by Eq. 5:</p>
<disp-formula id="sa2equ19">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ19.jpg" mimetype="image"/>
</disp-formula>
<p>This expression becomes invariant as <italic>N</italic> â â if the mean square covariance is finite. In contrast, neural dynamics models, such as the balanced excitatory-inhibitory (E-I) neural network [5], exhibit a different behavior, where <inline-formula id="sa2equ20"><inline-graphic xlink:href="elife-100666-sa2-equ20.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>, leading to unbounded dimensionality (see discussion lines 291-295, section 6.9 in SI).</p>
<p>(2) The requirements for spectral invariance involving the kernel function</p>
<p>In our Euclidean Random Matrix (ERM) model, the eigenvalue distribution follows:</p>
<disp-formula id="sa2equ21">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ21.jpg" mimetype="image"/>
</disp-formula>
<p>For spectral invariance to emerge: (1) The eigenvalue distribution must remain unchanged after sampling. (2) Since sampling reduces the neuronal density <italic>Ï</italic>. (3) The ratio <italic>Âµ/d</italic> must approach 0 to maintain invariance.</p>
<p>We can also demonstrate that <italic>D<sub>PR</sub></italic> is independent of density <italic>Ï</italic> in the large <italic>N</italic> limit (see the answer of question 4).</p>
<p>In conclusion, there is no causal relationship between spectral invariance and dimensionality invariance. This is also the reason why we need to consider both properties separately in our analysis.</p>
<disp-quote content-type="editor-comment">
<p>(9) In Eq. 1, the exact expression, which includes i=j, isnât a lot harder than the one with i=j excluded. So why <italic>iâ j</italic>?</p>
</disp-quote>
<p>The choice is for illustration purposes. In Eq. 1, we wanted to demonstrate that the dimension saturates to a value independent of <italic>N</italic>. When dividing the numerator and denominator of this expression by <italic>N<sup>2</sup></italic>, the term <inline-formula id="sa2equ22"><inline-graphic xlink:href="elife-100666-sa2-equ22.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> is independent of the neuron number <italic>N</italic>, but the term associated with the diagonal entries <inline-formula id="sa2equ23"><inline-graphic xlink:href="elife-100666-sa2-equ23.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> is of order <italic>O</italic>(1_/N_) and can be ignored for large <italic>N</italic>.</p>
<disp-quote content-type="editor-comment">
<p>(10) Fig. 2D: Could you explain where the theory line comes from?</p>
</disp-quote>
<p>We first estimate <inline-formula id="sa2equ24"><inline-graphic xlink:href="elife-100666-sa2-equ24.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>] from all neurons, and then compute <italic>D<sub>PR</sub></italic> for different neuron numbers <italic>N</italic> using Eq.5 (<inline-formula id="sa2equ25"><inline-graphic xlink:href="elife-100666-sa2-equ25.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula>). This is further clarified in lines 511-512.</p>
<disp-quote content-type="editor-comment">
<p>(11) l 94-5: âIt [scale invariance] is also absent when replacing the neural covariance matrix eigenvectors with random ones, keeping the eigenvalues identical (Fig. 2H).â If eigenvalues are identical, why does the spectrum change?</p>
</disp-quote>
<p>The eigenspectra of the covariance matrices in full size are the same by construction, but the eigenspectra of the sampled covariance matrices are different because the eigenvectors affect the sampling results. Please also refer to the construction process described in section 4.3 where this is also discussed: âThe composite covariance matrix with substituted eigenvectors in (Fig. 2H) was created as described in the following steps. First, we generated a random orthogonal matrix <italic>U<sub>r&lt;.sup&gt;</sub></italic> (based on the Haar measure) for the new eigenvectors. This was achieved by QR decomposition <italic>A=U<sub>r</sub>R</italic> of a random matrix <italic>A</italic> with i.i.d. entries <italic>A<sub>ij</sub></italic> â¼ N(0_,<italic>1</italic>/N_). The composite covariance matrix <italic>C<sub>r</sub></italic> was then defined as, where Î is a diagonal matrix that contains the eigenvalues of <italic>C</italic>. Note that since all the eigenvalues are real and <italic>U<sub>r</sub></italic> is orthogonal, the resulting <italic>C<sub>r</sub></italic> is a real and symmetric matrix. By construction, <italic>C<sub>r</sub></italic> and C have the same eigenvalues, but their sampled eigenspectra can differ.â</p>
<disp-quote content-type="editor-comment">
<p>(12) Eq 3: Thereâs no dependence on the distribution of sigma. Is that correct?</p>
</disp-quote>
<p>Indeed, this is true in the high-density regime when the neuron density <italic>Ï</italic> is large. The <italic>p</italic>(<italic>Î»</italic>) depends only on <italic>E</italic>(<italic>Ï<sup>2</sup></italic>) rather than the distribution of <italic>Ï</italic> (see Eq. 8). However, in the intermediate density regime, <italic>p</italic>(<italic>Î»</italic>) depends on the distribution of <italic>Ï</italic> (see Eq.9 and Eq.10). In our analysis, we consider <italic>E</italic>(<italic>Ï<sup>4</sup></italic>) as a measure of heterogeneity.</p>
<disp-quote content-type="editor-comment">
<p>(13) Please tell us the best fit values of <italic>Âµ/d</italic>.</p>
</disp-quote>
<p>This information now is added in the figure caption of Fig S10: <italic>Âµ/d</italic> = [0_.<italic>456</italic>,<italic>0</italic>.<italic>258</italic>,<italic>0</italic>.<italic>205</italic>,<italic>0</italic>.<italic>262</italic>,<italic>0</italic>.<italic>302</italic>,<italic>0</italic>._308] in fish 1-6.</p>
<disp-quote content-type="editor-comment">
<p>(14) l 133: âThe eigenspectrum is rho-independent whenever <italic>Âµ/d</italic> â 0.â</p>
<p>It looks to me like rho sets the scale but not the shape. Correct? If so, why do we care about the overall scale â isnât it the shape thatâs important?</p>
</disp-quote>
<p>Yes, our study focuses on the overall scale not only the shape, because many models, such as the ERM with other kernel functions, random RNNs, Morrellâs latent model [4, 1], can exhibit a power-law spectrum. However, these models do not exhibit scale-invariance in terms of spectrum curve collapsing. Therefore, considering the overall scale reveal additional non-trivial phenomenon.</p>
<disp-quote content-type="editor-comment">
<p>(15) Figs. 3 and 4: Are the grey dots the same as in previous figures? Either way, please specify what they are in the figure caption.</p>
</disp-quote>
<p>Yes, they are the same, and thank you for pointing it out. It has been specified in the figure caption now.</p>
<disp-quote content-type="editor-comment">
<p>(16) Fig. 4B: Top is correlation matrix, bottom is covariance matrix, correct? If so, that should be explicit. If not, it should be clear what the plots are.</p>
</disp-quote>
<p>That is correct. Both matrices (correlation - top, covariance - bottom) are labeled in the figure caption and plot (text in the lower left corner).</p>
<disp-quote content-type="editor-comment">
<p>(17) l 158: âFirst, the shape of the kernel function f(x) over a small distance ...â. What does âover a small distanceâ mean?</p>
</disp-quote>
<p>We thank you for seeking clarification on this point. We understand that the phrase âover a small distanceâ could be made clearer. We made a revised explanation in lines 164-165 Here, âover a small distanceâ refers to modifications of the particular kernel function <italic>f(x)</italic> we use Eq. 11 near <italic>x</italic> = 0 in the functional space, while preserving the overall power-law decay at larger distances. The t-distribution based <italic>f(x)</italic> (Eq. 11) has a natural parameter <italic>Ïµ</italic> that describes the transition to near 0. So we modified <italic>f(x)</italic> in different ways, all within this interval of |<italic>x</italic>| â¤ <italic>Ïµ</italic>, and considered different values of <italic>Ïµ</italic>. Table S3 and Figure S7 provide a summary of these modifications. Figure S7 visually compares these modifications to the standard power-law kernel function, highlighting the differences in shape near <italic>x</italic> = 0.</p>
<p>Our findings indicate that these alterations to the kernel function at small distances do not significantly affect the distribution of large eigenvalues in the covariance spectrum. This supports our conclusion that the large eigenvalues are primarily determined by the slow decay of the kernel function at larger distances in the functional space, as this characteristic governs the overall correlations in neural activity.</p>
<disp-quote content-type="editor-comment">
<p>(18) l390 <inline-graphic xlink:href="elife-100666-inf26.jpg" mimetype="image" mime-subtype="jpeg"/>. This <italic>x<sub>i</sub></italic> is, we believe, different from the <italic>x<sub>i</sub></italic> which is position in feature space. Given the difficulty of this paper, it doesnât help to use the same symbol to mean two different things. But maybe weâre wrong?</p>
</disp-quote>
<p>Thank you for your careful reading and suggestion. Indeed here <italic>x<sub>i</sub></italic> was representing activity rather than feature space position. We have thus revised the notation (Line 390 has been updated to line 439 as well.):</p>
<disp-formula id="sa2equ26">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ26.jpg" mimetype="image"/>
</disp-formula>
<p>In this revised notation: <italic>a<sub>i</sub></italic>(<italic>t</italic>) represents the neural activity of neuron <italic>i</italic> at time <italic>t</italic> (typically the firing rate we infer from calcium imaging). <inline-formula id="sa2equ27"><inline-graphic xlink:href="elife-100666-sa2-equ27.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> is simply the mean activity of neuron <italic>i</italic> across time. Meanwhile, weâll keep <italic>x<sub>i</sub></italic> exclusively for denoting positions in the functional space.</p>
<p>This change should make it much easier to distinguish between neural activity measurements and spatial coordinates in the functional space.</p>
<disp-quote content-type="editor-comment">
<p>(19) Eq. 19: is it correct that <italic>g</italic>(<italic>u</italic>) is not normalized to 1? If so, does that matter?</p>
</disp-quote>
<p>It is correct that the approximation of <italic>g</italic>(<italic>u</italic>) is not normalized to 1, as Eq. 19 provides an approximation suitable only for small pairwise distances (i.e., large correlation). Therefore, we believe this does not pose an issue. We have newly added this note in lines 691-693.</p>
<disp-quote content-type="editor-comment">
<p>(20) I get a different answer in Eq. 20:</p>
<p><inline-graphic xlink:href="elife-100666-inf29.jpg" mimetype="image" mime-subtype="jpeg"/></p>
<p>Whereas in Eq. 20,</p>
<p><inline-graphic xlink:href="elife-100666-inf30.jpg" mimetype="image" mime-subtype="jpeg"/>Âµ</p>
<p>Which is correct?</p>
</disp-quote>
<p>Thank you for your careful derivation. We believe the difference arises in the calculation of <italic>g</italic>(<italic>u</italic>).In our calculations:</p>
<disp-formula id="sa2equ28">
<italic>,</italic><graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ28.jpg" mimetype="image"/>
</disp-formula>
<p>(Your first equation seems to missed an 1_/Âµ_ in <italic>R</italic>âs exponent.)</p>
<disp-formula id="sa2equ29">
<italic>,</italic><graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ29.jpg" mimetype="image"/>
</disp-formula>
<p>That is, Eq. 20 is correct. From these, we obtain</p>
<disp-formula id="sa2equ30">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ30.jpg" mimetype="image"/>
</disp-formula>
<p>rather than</p>
<disp-formula id="sa2equ31">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ31.jpg" mimetype="image"/>
</disp-formula>
<p>We hope this clarifies the question.</p>
<disp-quote content-type="editor-comment">
<p>(21) Iâm not sure we fully understand the CCA analysis. First, our guess as to what you did: After sampling (either Asap or Fsap), you used ERM to embed the neurons in a 2-D space, and then applied canonical correlation analysis (CCA). Is that correct? If so, it would be nice if that were more clear.</p>
</disp-quote>
<p>We first used ERM to embed all the neurons in a 2-D functional space, before any sampling. Once we have the embedding, we can quantify how similar the functional coordinates are with the anatomical coordinates using <italic>R<sub>CCA</sub></italic> (section 2.4). We can then use the anatomical and functional coordinates to perform ASap and FSap, respectively. Our theory in section 2.4 predicts the effect on dimension under these samplings given the value of <italic>R<sub>CCA</sub></italic> estimated earlier (Fig. 5D). The detailed description of the CCA analysis is in section 4.9, where we explain how CCA is used to find the axes in both anatomical and functional spaces that maximize the correlation between projections of neuron coordinates.</p>
<disp-quote content-type="editor-comment">
<p>As to how you sampled under Fsap, I could not figure that out â even after reading supplementary information. A clearer explanation would be very helpful.</p>
</disp-quote>
<p>Thank you for your feedback. Functional sampling (FSap) entails the expansion of regions of interest (ROIs) within the functional space, as illustrated in Figure 5A, concurrently with the calculation of the covariance matrix for all neurons contained within the ROI. Technically, we implemented the sampling using the RG approach [6], which is further elaborated in Section 4.12 (lines 852-899), quoted below.</p>
<p>Stage (i): Iterative Clustering We begin with <italic>N</italic>0 neurons, where <italic>N</italic>0 is assumed to be a power of 2. In the first iteration, we compute Pearsonâs correlation coefficients for all neuron pairs. We then search greedily for the most correlated pairs and group the half pairs with the highest correlation into the first cluster; the remaining neurons form the second cluster. For each pair (<italic>a,b</italic>), we define a coarse-grained variable according to:</p>
<disp-formula id="sa2equ32">
<italic>,</italic><graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ32.jpg" mimetype="image"/>
</disp-formula>
<p>Where <inline-formula id="sa2equ33"><inline-graphic xlink:href="elife-100666-sa2-equ33.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> normalizes the average to ensure unit nonzero activity. This process reduces the number of neurons to <italic>N</italic><sub>1</sub> = <italic>N</italic><sub>0</sub>/2. In subsequent iterations, we continue grouping the most correlated pairs of the coarse-grained neurons, iteratively reducing the number of neurons by half at each step. This process continues until the desired level of coarse-graining is achieved.</p>
<p>When applying the RG approach to ERM, instead of combining neural activity, we merge correlation matrices to traverse different scales. During the _k_th iteration, we compute the coarse-grained covariance as:</p>
<disp-formula id="sa2equ34">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ34.jpg" mimetype="image"/>
</disp-formula>
<p>and the variance as:</p>
<disp-formula id="sa2equ35">
<graphic mime-subtype="jpg" xlink:href="elife-100666-sa2-equ35.jpg" mimetype="image"/>
</disp-formula>
<p>Following these calculations, we normalize the coarse-grained covariance matrix to ensure that all variances are equal to one. Note that these coarse-grained covariances are only used in stage (i) and not used to calculate the spectrum.</p>
<p>Stage (ii): Eigenspectrum Calculation The calculation of eigenspectra at different scales proceeds through three sequential steps. First, for each cluster identified in Stage (i), we compute the covariance matrix using the original firing rates of neurons within that cluster (not the coarse-grained activities). Second, we calculate the eigenspectrum for each cluster. Finally, we average these eigenspectra across all clusters at a given iteration level to obtain the representative eigenspectrum for that scale.</p>
<p>In stage (ii), we calculate the eigenspectra of the sub-covariance matrices across different cluster sizes as described in [6]. Let <italic>N</italic><sub>0</sub> = 2<italic><sup>n</sup></italic> be the original number of neurons. To reduce it to size <italic>N</italic> = <italic>N</italic><sub>0</sub>/2<italic><sup>k</sup></italic> = 2<italic><sup>n-k</sup></italic>, where <italic>k</italic> is the kth reduction step, consider the coarse-grained neurons in step <italic>n</italic> â <italic>k</italic> in stage (i). Each coarse-grained neuron is a cluster of 2<italic><sup>n-k</sup></italic> neurons. We then calculate spectrum of the block of the original covariance matrix corresponding to neurons of each cluster (there are 2<italic><sup>k</sup></italic> such blocks). Lastly, an average of these 2<italic><sup>k</sup></italic> spectra is computed.</p>
<p>For example, when reducing from <italic>N</italic><sub>0</sub> = 2<sup>3</sup> = 8 to <italic>N</italic> = 2<sup>3â1</sup> = 4 neurons (<italic>k</italic> = 1), we would have two clusters of 4 neurons each. We calculate the eigenspectrum for each 4x4 block of the original covariance matrix, then average these two spectra together. To better understand this process through a concrete example, consider a hypothetical scenario where a set of eight neurons, labeled 1,2,3,...,7,8, are subjected to a two-step clustering procedure. In the first step, neurons are grouped based on their maximum correlation pairs, for example, resulting in the formation of four pairs: {1,2},{3,4},{5,6}, and {7,8} (see Fig. S22). Subsequently, the neurons are further grouped into two clusters based on the results of the RG step mentioned above. Specifically, if the correlation between the coarse-grained variables of the pair {1,2} and the pair {3,4} is found to be the largest among all other pairs of coarse-grained variables, the first group consists of neurons {1,2,3,4}, while the second group contains neurons {5,6,7,8}. Next, take the size of the cluster <italic>N</italic> = 4 for example. The eigenspectra of the covariance matrices of the four neurons within each cluster are computed. This results in two eigenspectra, one for each cluster. The correlation matrices used to compute the eigenspectra of different sizes do not involve coarse-grained neurons. It is the real neurons 1,2,3,...,7,8, but with expanding cluster sizes. Finally, the average of the eigenspectra of the two clusters is calculated.</p>
<disp-quote content-type="editor-comment">
<p>(22) Line 37: âeven if two cell assemblies have the same <italic>D<sub>PR</sub></italic>, they can have different shapes.â What is meant by shape here isnât clear.</p>
</disp-quote>
<p>Thank you for pointing out this potential ambiguity. The âshapeâ here refers to the geometric configuration of the neural activity space characterized as a highdimensional ellipsoid by the covariance. Specifically, if we denote the eigenvalues of the covariance matrix as <italic>Î»<sub>1</sub>,Î»<sub>2</sub>,...,Î»<sub>N</sub></italic>, then <inline-formula id="sa2equ36"><inline-graphic xlink:href="elife-100666-sa2-equ36.jpg" mimetype="image" mime-subtype="jpeg"/></inline-formula> corresponds to the length of the <italic>i</italic>-th semi-axis of this ellipsoid (Figure 1B). As shown in Figure 1C, two neural populations with the same dimensionality (<italic>D<sub>PR</sub></italic> = 25/11 â 2.27) exhibit different eigenvalue spectra, leading to differently shaped ellipsoids. This clarification is now included in lines 39-40.</p>
<disp-quote content-type="editor-comment">
<p>(23) Please discuss if any information about the latent dimension or kernel function can be inferred from the measurements.</p>
</disp-quote>
<p>Same as comment(6): we would like to clarify that in our analysis using the Euclidean Random Matrix (ERM) model, we fit the ratio <italic>Âµ/d</italic>, rather than the individual values of <italic>d</italic> (dimension of the functional space) or <italic>Âµ</italic> (exponent of the distancedependent kernel function). This limitation is inherent because the modelâs predictions for observable quantities, such as the eigenvalue spectrum of the covariance matrix, are dependent solely on this ratio.</p>
<p>For the kernel function, once the <italic>d</italic> is chosen, we can infer the general shape of the kernel function from data (Figs S12 and S13), up to a certain extent (see also lines 164-166). In particular, we can compare the eigenspectrum of the simulation results for different kernel functions with the eigenspectrum of our data. This allows us to qualitatively exclude certain kernel functions, such as the exponential and Gaussian kernels (Fig. S4), which show clear differences from our data.</p>
<p>References</p>
<p>(1) M. C. Morrell, I. Nemenman, A. Sederberg, Neural criticality from effective latent variables. eLife 12, RP89337 (2024).</p>
<p>(2) J. Manley, S. Lu, K. Barber, J. Demas, H. Kim, D. Meyer, F. M. Traub, A. Vaziri, Simultaneous, cortex-wide dynamics of up to 1 million neurons reveal unbounded scaling of dimensionality with neuron number. Neuron (2024).</p>
<p>(3) S. A. Moosavi, S. S. R. Hindupur, H. Shimazaki, Population coding under the scale-invariance of high-dimensional noise (2024).</p>
<p>(4) M. C. Morrell, A. J. Sederberg, I. Nemenman, Latent dynamical variables produce signatures of spatiotemporal criticality in large biological systems. Physical Review Letters 126, 118302 (2021).</p>
<p>(5) A. Renart, J. De La Rocha, P. Bartho, L. Hollender, N. Parga, A. Reyes, K. D. Harris, The asynchronous state in cortical circuits. science 327, 587â590 (2010).</p>
<p>(6) L. Meshulam, J. L. Gauthier, C. D. Brody, D. W. Tank, W. Bialek, Coarse graining, fixed points, and scaling in a large population of neurons. Physical Review Letters 123, 178103 (2019).</p>
</body>
</sub-article>
</article>