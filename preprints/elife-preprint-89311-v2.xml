<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89311</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89311</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89311.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Finding structure during incremental speech comprehension</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8554-5138</contrib-id>
<name>
<surname>Lyu</surname>
<given-names>Bingjiang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Marslen-Wilson</surname>
<given-names>William D.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fang</surname>
<given-names>Yuxing</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Tyler</surname>
<given-names>Lorraine K.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Changping Laboratory</institution>, Beijing, 102206, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Centre for Speech, Language and the Brain, Department of Psychology, University of Cambridge</institution>, Cambridge, CB2 3EB, <country>United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
<city>Hangzhou</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Beijing Normal University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence: <email>bingjiang.lyu@gmail.com</email> (BL), <email>lkt10@cam.ac.uk</email> (LKT).</corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-08-10">
<day>10</day>
<month>08</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-03-08">
<day>08</day>
<month>03</month>
<year>2024</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89311</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-05-22">
<day>22</day>
<month>05</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-05-11">
<day>11</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.10.25.465687"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-08-10">
<day>10</day>
<month>08</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89311.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.89311.1.sa4">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89311.1.sa3">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89311.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.89311.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Lyu et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Lyu et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89311-v2.pdf"/>
<abstract>
<title>Abstract</title><p>A core aspect of human speech comprehension is the ability to incrementally integrate consecutive words into a structured and coherent interpretation, aligning with the speaker’s intended meaning. This rapid process is subject to multi-dimensional probabilistic constraints, including both linguistic knowledge and non-linguistic information within specific contexts, and it is their interpretative coherence that drives successful comprehension. To study the neural substrates of this process, we extract word-by-word measures of sentential structure from BERT, a deep language model, which effectively approximates the coherent outcomes of the dynamic interplay among various types of constraints. Using representational similarity analysis, we tested BERT parse depths and relevant corpus-based measures against the spatiotemporally resolved brain activity recorded by electro/magnetoencephalography when participants were listening to the same sentences. Our results provide a detailed picture of the neurobiological processes involved in the incremental construction of structured interpretations. These findings show when and where coherent interpretations emerge through the evaluation and integration of multifaceted constraints in the brain, which engages bilateral brain regions extending beyond the classical fronto-temporal language system. Furthermore, this study provides empirical evidence supporting the use artificial neural networks as computational models for revealing the neural dynamics underpinning complex cognitive processes in the brain.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>incremental speech comprehension</kwd>
<kwd>artificial neural networks</kwd>
<kwd>deep language models</kwd>
<kwd>representational similarity analysis</kwd>
<kwd>computational cognitive neuroscience</kwd>
<kwd>EEG/MEG</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>New figures to illustrate the experimental design and analysis pipeline; additional results and revised text to clarify the main findings.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Human speech comprehension involves a complex set of processes that transform an auditory input into the speaker’s intended meaning, wherein each word is sequentially recognized and integrated with the preceding words to obtain a coherent interpretation (Marslen-Wilson and Tyler 1980; Lyu et al. 2019; Choi et al. 2021). Crucially, rather than simple linear concatenation, individual words are combined according to the nonlinear and often discontinuous structure embedded in an utterance as it is delivered over time (Everaert et al. 2015). For example, in the sentence <italic>“The boy who chased the cat was…”</italic>, it is the structurally close word <italic>“boy”</italic>, rather than the linearly close word <italic>“cat”</italic>, that is combined with <italic>“was”</italic>. However, the neural dynamics underpinning the incremental construction of a structured interpretation from a spoken sentence is still unclear.</p>
<p>Previous neuroimaging studies on the structure of language primarily focused on syntax (Matchin and Hickok 2020), contrasting grammatical sentences against word lists or sentences with syntactic violations (Nelson et al. 2017; Law and Pylkkanen 2021), manipulating the syntactic complexity in sentences (Pallier et al. 2011), or studying artificial grammatical rules elicited by structured, unintelligible strings (Friederici et al. 2006). Nevertheless, finding the structure in an unfolding sentence also depends on the constraints jointly placed by other linguistic properties and non-linguistic information such as the broad world knowledge (Bever 1970; Tyler and Marslen-Wilson 1977).</p>
<p>Unlike the <italic>two-stage model</italic> (Frazier and Rayner 1982; Frazier 1987) which posits an initial parsing stage relying solely on syntax, the <italic>constraint-based</italic> approach to sentence processing (MacDonald et al. 1994; Trueswell and Tanenhaus 1994) proposes that speech comprehension is concurrently governed by multiple types of probabilistic constraints (e.g., syntax, semantics, world knowledge), generated by individual words as they are sequentially heard. There is no delay in the utilization of these multifaceted constraints once they become available, neither is a fixed priority assigned to one type of constraint over another; rather, it is the <italic>interpretative coherence</italic> of all available constraints that forms the basis for successful language comprehension (Altmann 1998). Although lexical constraints of individual words can be estimated from large corpora data, it has been challenging to model the dynamic interplay between various types of linguistic and nonlinguistic constraints in a specific context, especially at the sentence level and beyond.</p>
<p>Contemporary deep language models (DLMs) have made great strides in a wide array of natural language processing tasks, including text generation, parsing and translation (Vaswani et al. 2017; Devlin et al. 2019; Brown et al. 2020; Ouyang et al. 2022). While current DLMs are still imperfect in terms of human-level language understanding related to reasoning and complex physical or social situations (Bisk et al. 2020), they are arguably valuable models of general linguistic capacities, due to their ability to identify and leverage relevant statistical regularities of linguistic and non-linguistic world knowledge present in massive training data (Linzen and Baroni 2021; Pavlick 2022). Human language comprehension requires a contextualized integration of multifaceted constraints (Marslen-Wilson 1975; Tyler and Marslen-Wilson 1977; Kuperberg 2007). In this regard, DLMs excel in flexible combination of different types of features (e.g., syntactic structure and semantic meaning) embedded in their rich internal representations (Manning et al. 2020; Bengio et al. 2021; Linzen and Baroni 2021; Pavlick 2022). Their deep contextualized representations capture the distributed regularities that jointly determine the coherent interpretation of a given sentence, providing context-dependent composition and quantitative measures of the underlying sentential structure. These properties relate back to Elman’s recurrent neural network (Elman 1990, 1993) which automatically picks up and encodes lexical syntactic/semantic information in the hidden states.</p>
<p>Recent studies have revealed an overall congruence between language representations in DLMs and those observed in the human brain while processing the same spoken or written input (Schrimpf et al. 2021; Caucheteux et al. 2022; Caucheteux and King 2022; Goldstein et al. 2022; Heilbron et al. 2022; Toneva et al. 2022; Caucheteux et al. 2023), suggesting the potential value of DLMs as a computational tool to investigate the neural basis of language comprehension. To move beyond comparing the similarities between entire model hidden states and brain activity, probing techniques that can extract specific contents from DLMs (Hewitt and Liang 2019; Tenney et al. 2019) make it possible to study the neural dynamics relevant to processing such specific information. The important advance here is that we can leverage the deep learning strengths of DLMs to create rigorously quantified models of the broader and multifaceted constraint environment in which a structured interpretation is constructed. Such models can be compared, dynamically, with more restricted and interpretable factors that capture the specific linguistic combinatorial constraints necessary for successful language comprehension.</p>
<p>Here, we take this approach further by designing sentences with contrasting linguistic structures and using a structural probe technique (Hewitt and Manning 2019) to extract word- by-word contextualized representations of sentential structures from a widely-used DLM, namely, BERT (Devlin <italic>et al.</italic> 2019). This provides the neurocomputational specificity required to elucidate the neural dynamics underlying the incremental construction of a structured interpretation from an unfolding spoken sentence. After a detailed evaluation of BERT structural measures according to the hypothesized <italic>constraint-based</italic> approach and human behavioural results, we used spatiotemporal searchlight representational similarity analysis (ssRSA) (Kriegeskorte et al. 2008) to test these quantitative structural measures and relevant lexical properties against source-localized EMEG data recorded while participants were listening to the same sentences. Our findings reveal how the structured interpretation of a spoken sentence is incrementally built under multifaceted probabilistic constraints in the brain.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>We constructed 60 sets of sentences with varying sentential structures (see Methods) and presented them to human listeners. We also input them word-by-word to BERT to extract incremental structural representations. These natural spoken sentences were constructed to balance off specifically linguistic constraints on interpretation against varying non-linguistic constraints as the sentence is incrementally interpreted, providing a realistic simulation of real- life language use. In each stimulus set, there are two target sentences differing only in the transitivity of the first verb (Verb1) encountered, i.e., how likely it is that Verb1 takes a direct object [see (1) and (2) below and <bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>]:
<list list-type="order">
<list-item><p><italic>The dog found in the park was covered in mud</italic>.</p></list-item>
<list-item><p><italic>The dog walked in the park was covered in mud</italic>.</p></list-item>
</list>
</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Example spoken sentence stimuli and plausible structured interpretations.</title>
<p>The two target sentences in each set differ only in the transitivity of the first verb (Verb1). Each sentence has two possible structured interpretations before the actual main verb is presented: an active interpretation, where the subject noun (SN) performs the action, and a passive interpretation, where the SN is the recipient of the action. The interpretative preference hinges on the likelihood of the SN acting as an agent or a patient (i.e., its thematic role) in conjunction with the transitivity of Verb1. As the sentence progresses to the prepositional phrase, a combination of higher SN agenthood and greater Verb1 intransitivity (i.e., a higher Active index) generally favors an Active interpretation. Conversely, increased SN patienthood coupled with higher Verb1 transitivity (i.e., a higher Passive index) may lead to a Passive interpretation. Note that while the SN is the same for the two target sentences within the same set, it varies across different sentence sets. All images were generated using Midjourney for illustrative purposes.</p></caption>
<graphic xlink:href="465687v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the first sentence, Verb1 (i.e., <italic>“found”</italic>) has high transitivity (HiTrans) and strongly prefers a direct object (e.g., ball), while in the second sentence, Verb1 (i.e., <italic>“walked”</italic>) has relatively low transitivity (LoTrans) and is often used without a following direct object. Critically, (a) the structural interpretation of these sentences is ambiguous at the point Verb1 is encountered and (b) the preferred human resolution of this ambiguity depends on the real-time integration of linguistic and non-linguistic constraints as more of the sentence is heard. In the example above, the sequence <italic>“The dog found</italic>…” could initially have either an Active interpretation – where the dog has found something, or a Passive interpretation – where the dog is found by someone (<bold><xref rid="fig1" ref-type="fig">Figure 1</xref></bold>). Because <italic>“find”</italic> is primarily a transitive verb, the human listener is likely to be biased towards an initial Active interpretation. Similarly, the sequence <italic>“The dog walked…”</italic>, where <italic>walk</italic> is primarily used as an intransitive verb (without a direct object), could also bias the listener to an Active interpretation, where the dog is doing the walking, rather than the less frequent Passive interpretation where someone is taking the dog for a walk (i.e., walking the dog).</p>
<p>This initial structural interpretation up to Verb1 does not, however, just depend on linguistic knowledge such as Verb1 transitivity. It also depends on non-linguistic information, i.e., how likely the subject is (or is not) to adopt the Active (agent) role to perform the specified action (Dowty 1991; Marslen-Wilson et al. 1993), that is, “thematic role” properties of the subject noun. Although it could be reflected by statistical regularities in language, thematic role preference hinges more on world knowledge, plausibility, or real-world statistics. So, regardless of Verb1 transitivity, the Active interpretation should be more strongly favored in <italic>“The <bold>king</bold> found/walked…”</italic> given the higher agenthood of the <italic>“king”</italic> and thus the greater implausibility of a Passive interpretation involving a <italic>“king”</italic> relative to a <italic>“dog”</italic>. Hence, the word-by-word interpretation of the sentential structure – and of the real-world event structure evoked by this interpretation – is determined by the constraints jointly placed by the subject noun and Verb1, which is manifested by the interpretative coherence between non-linguistic world knowledge (i.e., thematic role preference) and linguistic knowledge (i.e., verb transitivity).</p>
<p>As the sentence evolves, and the prepositional phrase <italic>“in the park”</italic> that follows Verb1 is incrementally processed, there is further modulation of the preferred interpretation, again reflecting both Verb1 transitivity and the plausibility of the event being constructed. Specifically, the Passive interpretation will become more preferred in a HiTrans sentence, given the absence of an expected direct object for the highly transitive Verb1, so Verb1 tends to be interpreted as a passive verb [i.e., the head of a reduced relative clause in <italic>“The dog <bold>(that was) found in the park</bold>…”</italic>]. Conversely, in a LoTrans sentence, the Active interpretation of Verb1 is strengthened by the incoming prepositional phrase, which is in accord with the verb’s intransitive use and the event conjured up by the sequence of words heard so far (e.g., <italic>“The dog walked in the park…”</italic>). Hence, these two sentence types are likely to differ in the structural interpretation preferred by the end of the prepositional phrase. However, with the appearance of the actual main verb (e.g., <italic>“was covered”</italic> in the example sentences), the Active interpretation of Verb1 as the main verb will be completely rejected, which resolves the potential ambiguity and confirms the Passive interpretation in both HiTrans and LoTrans sentences.</p>
<p>In brief, understanding these complex sentences require listeners to integrate discontinuous words to solve a long-distance dependency between the subject noun and the actual main verb separated by an intervening clause. This engages the neurobiological processes of integration across different lexical constraints and multiple levels of the sentence processing system. For example, the incremental building, maintenance and update of sentential structure over time might primarily involve activity in the fronto-temporal regions (Friederici 2012), while estimating the plausibility of the event interpreted from the sentence with prior knowledge of the world may elicit neural responses in the default mode network (Yeshurun et al. 2021).</p>
</sec>
<sec id="s3">
<title>Human incremental structural interpretations</title>
<p>As the first step, and to quantify how the stimulus sentences exemplified a constraint-based account of incremental structural interpretation, we conducted two pre-tests where participants listened to sentence fragments, starting from sentence onset and continuing either until the end of Verb1 or to the end of the prepositional phrase (<bold><xref rid="fig2" ref-type="fig">Figure 2A</xref></bold>), and then produced a continuation to complete the sentence (see Methods). Based on the continuations provided by the listeners at these two gating points, we can infer their online structural interpretations.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Human incremental structural interpretations derived from continuation pre-tests.</title>
<p><bold>(A)</bold> An example set of target sentences differing only in the transitivity of Verb1, HiTrans: high transitivity, LoTrans: low transitivity. Det: determiner, SN: subject noun, V1: Verb1, PP1-PP3: prepositional phrase, MV: main verb, END: the last word in the sentence. <bold>(B)</bold> Probability of a direct object (left) and a prepositional phrase (right) continuation after Verb1. <bold>(C)</bold> Probability of a main verb in the continuations after Verb1, which indicates an Active interpretation. <bold>(D)</bold> Correlations between corpus-based lexical constraints and probabilistic interpretations in the two pre-tests. (Spearman rank correlation, black dots indicate significance determined by 10,000 permutations, <italic>PFDR</italic> &lt; 0.05 corrected).</p></caption>
<graphic xlink:href="465687v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the continuations after Verb1, a direct object was more likely to be found in HiTrans sentences, indicating a transitive use of Verb1, while an opposite pattern was found for a PP continuation, indicating an intransitive use of Verb1 (<bold><xref rid="fig2" ref-type="fig">Figure 2B</xref></bold>). As expected, the probability of a main verb (MV) in the continuations after the prepositional phrase was lower in LoTrans sentences (<bold><xref rid="fig2" ref-type="fig">Figure 2C</xref></bold>), suggesting that listeners preferred the Active interpretation and tended to interpret Verb1 as the main verb by the end of the prepositional phrase in LoTrans sentences, and vice versa in HiTrans sentences. Crucially, neither of the two pre-tests resulted in a complete separation between HiTrans and LoTrans sentences; instead, they were characterized by two different but overlapping probabilistic distributions. This finding suggests that Passive and Active interpretations varied in plausibility in each sentence type before the actual main verb was presented, reflecting the probabilistic constraints jointly placed by the combination of the specific subject noun, Verb1, and the prepositional phrase in each sentence.</p>
<p>To relate these human interpretative preferences to the broader landscape of distributional language data, we developed corpus-based measures of the thematic role preference of the subject noun (i.e., how likely it is interpreted as an agent that conducts an action) and the transitivity of Verb1 in each sentence, from which we derived a Passive index and an Active index (see Methods). These indices separately capture the interpretative coherence between these two lexical properties towards Passive and Active interpretations. Both high subject noun agenthood and low Verb1 transitivity coherently preferred an Active interpretation as the prepositional phrase was heard (i.e., a high Active index), and vice versa for the Passive interpretation (i.e., a high Passive index). In accord with the <italic>constraint-based</italic> hypothesis, we found that human interpretative preference for the two types of sentences was significantly correlated with the lexical constraints placed by the subject noun and Verb1 (<bold><xref rid="fig2" ref-type="fig">Figure 2D</xref></bold>).</p>
</sec>
<sec id="s4">
<title>Incremental structural representations extracted from BERT</title>
<p>Next, we extracted structural representations at various positions in the same sentences from BERT and evaluated them according to the <italic>constraint-based</italic> hypothesis and human behavioural results. This evaluation is needed to motivate the use of BERT structural measures to reveal how the structured interpretation of a spoken sentence is incrementally built in the brain.</p>
<p>Typically, the structure of a sentence can be represented by a dependency parse tree (de Marneffe et al. 2006) where words are situated at different depths given their structural dependency (<bold><xref rid="fig3" ref-type="fig">Figure 3A</xref></bold>). Each edge links two structurally proximate words as being the head and the dependent separately (e.g., a verb and its direct object). However, such a parse tree is context-free, that is, it only captures the syntactic relation between each pair of words and abstracts away from the specific lexical (and higher order) contents of the sentence that constrain its structural interpretation. This context-free parse depth is always the same for words at the same position in sentences with the same structured interpretation (e.g., <italic>“found”</italic> and <italic>“walked”</italic> in either of the two parse trees in <bold><xref rid="fig3" ref-type="fig">Figure 3A</xref></bold>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Incremental interpretation of sentential structure by BERT.</title>
<p><bold>(A)</bold> Context-free dependency parse trees of two plausible structural interpretations. Left: Passive interpretation where V1 is the head of a reduced relative clause. Right: Active interpretation where V1 is the main verb. <bold>(B)</bold> Incremental input to BERT structural probing model, with the lightness of dots encoding different positions in the target sentences. Det: determiner, SN: subject noun, V1: Verb1, PP1-PP3: prepositional phrase, MV: main verb, END: the last word in the sentence. <bold>(C)</bold> BERT structural probing model is trained to output a parse depth vector, representing the parse depths of all the words in the sentence input. The BERT parse depth for a specific word is updated incrementally as the sentence unfolds word-by-word. In this example, the parse depth of “<italic>found</italic>” increases with the presence of the prepositional phrase, indicating an increased preference for the Passive interpretation [according to the context-free parse depths in (A)]. <bold>(D)</bold> Incremental interpretation of the dependency between SN and V1 in the model space consisting of the parse depth of Det, SN and V1. Upper: Each colored circle represents the parse depth vector up to V1 derived at a certain position in the sentence [with the same color scheme as in (B)]. The hollow triangle and circle represent the context-free dependency parse vectors for Passive and Active interpretations in (A). Lower: incremental interpretation of the two target sentence types represented by the trajectories of median parse depth. <bold>(E)</bold> Distance from Passive and Active landmarks in the model space as the sentence unfolds [between each colored circle and the two landmarks in the upper panel of (D)] (two-tailed two-sample t-test, *: <italic>P</italic> &lt; 0.05, **: <italic>P</italic> &lt; 0.001, error bars represent SEM).</p></caption>
<graphic xlink:href="465687v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To obtain structural measures that also encode the specific lexical contents in a sentence, we adopted a structural probing technique (Hewitt and Manning 2019) to reconstruct a sentence’s structure by estimating each word’s parse depth based on their contextualized representations generated by BERT (see Methods). Note that BERT is a multi-layer DLM (24 layers in the version used in this study) which may distribute different aspects of its computational solutions over multiple layers. Accordingly, we trained a structural probing model for each layer, and selected the one with the most accurate structural representations while also including its neighboring layers to cover relevant upstream and downstream information. Following this strategy, we used the BERT structural measures obtained from layers 12-16 with the best performance achieved in layer 14 (see <bold>Appendix 1-figure 1</bold> and Methods).</p>
<p>We input each sentence word-by-word to the trained BERT structural probing models, focusing on the incremental structural representation being built as it progressed from Verb1 to the main verb (see examples in <bold><xref rid="fig3" ref-type="fig">Figures 3B</xref> and <xref rid="fig3" ref-type="fig">3C</xref></bold>). Note that we defined the first word after the prepositional phrase as the main verb since its appearance is sufficient to resolve the intended structure where Verb1 is a passive verb. We found that, for each type of sentences, the BERT parse depth of words at the same position formed a distribution ranging around the corresponding context-free parse depths in either the Passive or the Active interpretation (see <bold>Appendix 1-figure 2</bold>), suggesting a word-specific rather than position-specific structural representation. In addition, we quantified the contributions of words at different positions to the variances encoded in BERT parse depth vectors. Our analysis revealed that content words contributed significantly more than function words (i.e., the determiners) (see <bold>Appendix 1-figure 3</bold> for details).</p>
<p>Then we visualized BERT’s word-by-word structural measures, focusing on the dependency between the subject noun and Verb1 that is core to the current interpretation of the sentence – whether the subject noun is the agent or the patient of Verb1. To this end, we built a 3- dimensional vector including BERT parse depths of the first three words up to Verb1 for each sentence (e.g., <italic>“The dog found…”</italic>). This 3D vector was updated incrementally with each additional word in the input, thereby capturing the dynamic interpretation of the structural dependency between the subject noun and Verb1, influenced by the context provided by subsequent words in a specific sentence. Like the probabilistic interpretation found within each type of sentences in human listeners, trajectories of individual HiTrans and LoTrans sentences are considerably distributed and intertwined (see the upper panel of <bold><xref rid="fig3" ref-type="fig">Figure 3D</xref></bold>), implying that BERT structural interpretations are sensitive to the idiosyncratic contents in each sentence.</p>
<p>To make sense of these trajectories, we also vectorized the context-free parse depth of the first three words indicating Passive and Active interpretations (<bold><xref rid="fig3" ref-type="fig">Figure 3A</xref></bold>) separately and located them in the 3D vector space as landmarks (hollow triangle and circle in <bold><xref rid="fig3" ref-type="fig">Figure 3D</xref></bold>), so that the plausibility of either interpretation can be estimated by a sentence’s distance from its landmark. As shown by the trajectories of the median BERT parse depth of the two sentence types (see the lower panel of <bold><xref rid="fig3" ref-type="fig">Figure 3D</xref></bold>), in general, HiTrans sentences continuously moved towards the Passive interpretation landmark after Verb1, with a significant change of distances detected at the main verb (see the orange bars in <bold><xref rid="fig3" ref-type="fig">Figure 3E</xref></bold>). LoTrans sentences started by approaching the Active interpretation landmark but were reorientated to the Passive counterpart with the appearance of the actual main verb, with significant changes of distances detected at both Verb1 and main verb (see the purple bars in <bold><xref rid="fig3" ref-type="fig">Figure 3E</xref></bold>). These results resemble the pattern of human interpretative preference observed in the continuation pre-tests, where the Passive and Active interpretations were separately preferred in HiTrans and LoTrans sentences by the end of the prepositional phrase in a probabilistic manner (<bold><xref rid="fig2" ref-type="fig">Figure 2C</xref></bold>), before the Passive interpretation was established with the appearance of the actual main verb.</p>
<sec id="s4a">
<title>BERT structural measures are correlated with constraints driving human interpretation</title>
<p>To further assess whether BERT’s preferences for structural interpretation align with the constraints considered by human listeners during speech comprehension, we correlated BERT structural measures with relevant corpus-based measures and human behavioural data (see Methods).</p>
<p>We first focused on BERT’s interpretative mismatch quantified as the distance between an unfolding sentence and each of the two landmarks in the model space, which was dynamically updated as the sentence unfolded (<bold><xref rid="fig3" ref-type="fig">Figure 3D</xref></bold>). Consistently, from the incoming prepositional phrase to the main verb, sentences that are closer to the Passive landmark in the model space have higher Verb1 transitivity, a higher Passive index but a lower Active index, while sentences closer to the Active interpretation landmark exhibited higher Active index and lower Passive index (<bold><xref rid="fig4" ref-type="fig">Figures 4A</xref></bold> and <bold><xref rid="fig4" ref-type="fig">4B</xref></bold>). Moreover, at the beginning of the prepositional phrase, the change of distance towards either interpretation landmark between two consecutive words is also correlated with these constraints (<bold><xref rid="fig4" ref-type="fig">Figures 4C</xref></bold> and <bold><xref rid="fig4" ref-type="fig">4D</xref></bold>), suggesting an immediate update in the structural interpretation in combination with the accumulated constraints from the preceding subject noun and Verb1.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Correlation between incremental BERT structural measures and explanatory variables.</title>
<p>BERT structural measures include <bold>(A</bold>, <bold>B)</bold> BERT interpretative mismatch represented by each sentence’s distance from the two landmarks in model space (<xref rid="fig3" ref-type="fig">Figure 3D</xref>); <bold>(C</bold>, <bold>D)</bold> Dynamic updates of BERT interpretative mismatch represented by each sentence’s movement to the two landmarks; <bold>(E</bold>, <bold>F)</bold> Overall structural representations captured by the first two principal components (i.e., PC1 and PC2) of BERT parse depth vectors; <bold>(G</bold>, <bold>H)</bold> BERT Verb1 (V1) parse depth and its dynamic updates. Explanatory variables include lexical constraints derived from massive corpora and the main verb probability derived from the continuation pre-test (Spearman correlation, permutation test, <italic>P</italic>FDR &lt; 0.05, multiple comparisons corrected for all BERT layers, results shown here are based on layer 14, see <bold>Appendix 1 figures 4-6</bold> for the results of all layers, see <bold>Appendix 1-figure 7</bold> for the dynamic change of Verb1 parse depth); PP1-PP3: prepositional phrase, MV: main verb, END: the last word in the sentence.</p></caption>
<graphic xlink:href="465687v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Moreover, we found that both the incremental BERT parse depth vectors as a whole (which are captured by their principal components) and the BERT parse depth of Verb1 (which is the most indicative marker of the interpretation preferred) are correlated with the constraints placed by the subject noun and Verb1 (<bold><xref rid="fig4" ref-type="fig">Figures 4E</xref></bold> to <bold><xref rid="fig4" ref-type="fig">4H</xref></bold>). Moreover, the significant effects consistently found as the sentence unfolds suggest that properties of preceding words are used to constrain the interpretation of the upcoming input, which is key to resolving discontinuous structural dependencies. In addition, we found that BERT structural interpretations were also correlated with the main verb probability in the continuation pre-test which directly reflects human interpretation preference (black bars in <bold><xref rid="fig4" ref-type="fig">Figure 4</xref></bold>).</p>
<p>Overall, these results illustrated, at which position in a sentence, relevant lexical constraints started being encoded by BERT, which also validated the contextualized BERT structural measures in terms of the <italic>constraint-based</italic> hypothesis and human behavioural results, and motivated the use of them to probe the neural processes involved during the incremental structural interpretation of spoken sentences.</p>
</sec>
<sec id="s4b">
<title>Neural dynamics of incremental structural interpretation</title>
<p>To study how the structured interpretation of a spoken sentence is built word-by-word in the brain, we used ssRSA to test the incremental BERT structural measures in source-localized EMEG collected when the same sentences were delivered to human listeners (see <bold><xref rid="fig5" ref-type="fig">Figure 5</xref></bold> for the pipeline of ssRSA). This combination of methods gains improved neurocomputational specificity by probing the spatiotemporally resolved neural activity with detailed structural representations rather than the entire hidden states of BERT. We compared the representational geometry of BERT structural measures with that of neural responses inside a spatiotemporal searchlight moving across the cortical surface, significant model fits showed when and where the incremental structural interpretations or relevant lexical constraints emerge and update in the brain. Given the probabilistic interpretations in BERT and human listeners reported above, we combined HiTrans and LoTrans sentences as one group to increase the range of pair-wise dissimilarity to be modelled in ssRSA.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Illustration of the pipeline for ssRSA.</title>
<p>For each pair of sentences, we extract their BERT or corpus-based measures and calculate the dissimilarity between these measures, resulting in a model representational dissimilarity matrix (RDM). Meanwhile, we also extract the neural activity recorded while participants are listening to these sentences and calculate their dissimilarity to create a data RDM. Specifically, we use a spatiotemporal searchlight in EMEG source space, which moves across the brain and captures the neural activity within a 10-mm-radius sphere over a 60-ms sliding time window. By correlating the model RDM with data RDMs from all spatiotemporal searchlights, we can identify whether, and if so, when and where the brain represents the information captured by the model RDM. The ssRSA is conducted in V1, PP1 and MV epochs respectively, with HiTrans and LoTrans sentences combined as one group (i.e., 120 sentences in total).</p></caption>
<graphic xlink:href="465687v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We began with the BERT parse depth vector containing the parse depth of each word in an incremental input, providing a dynamic structural representation updated as the sentence unfolded. Then, we tested the interpretative mismatch between the incremental BERT parse depth vector and the corresponding context-free parse depth vector for the Passive or the Active interpretation. The degree of this mismatch is proportional to the evidence for or against the two interpretations, i.e., the smaller the distance, the more positively loaded this interpretation. Besides these two measures based on the entire incremental input, we also focused on Verb1 since the potential structural ambiguity lies in whether Verb1 is interpreted as a passive verb or the main verb. Given the context-free parse depth of Verb1 that is 2 in the passive interpretation and 0 in the Active interpretation (<bold><xref rid="fig3" ref-type="fig">Figure 3A</xref></bold>), with each incoming later word, an increased BERT Verb1 parse depth towards 2 or a decreased value towards 0 reflects separately the preference biased to a Passive or an Active interpretation (<bold>Appendix 1-figure 7</bold>). All quantitative measures tested in ssRSA are summarized in <bold>Appendix 1-table 1</bold>).</p>
<p>For the listener’s neural activity, we focused on three critical epochs in each sentence: (a) Verb1 – when its structural dependency with the preceding subject noun was initially established despite potential ambiguity, (b) the preposition – when the initial structural interpretation started being updated, to be either strengthened or weakened by the incoming preposition phrase, and (c) main verb – when the intended Passive interpretation was finally confirmed. We aligned the continuous EMEG data to the onset of Verb1, the preposition, and the main verb respectively and obtained three 600-ms epochs.</p>
<p>We found that the incremental BERT parse depth vectors exhibited significant fits to brain activity consistently in all three epochs, as the corresponding word was being heard at that time (<bold><xref rid="fig6" ref-type="fig">Figures 6A</xref></bold> to <bold><xref rid="fig6" ref-type="fig">6C</xref></bold>). In Verb1 epoch, effects in bilateral frontal and anterior-to-middle temporal regions started immediately from Verb1 onset and continued until the uniqueness point – the point at which the word has been uniquely identified. These early effects could be due to the different subject nouns included in the BERT parse depth vectors. While the BERT parse depth of Verb1 <italic>per se</italic> showed similar effects but with greater duration which peaked exactly at Verb1 uniqueness point (<bold>Appendix 1-figure 10</bold>). As the sentence unfolded, effects of BERT parse depth vectors were found in the left fronto-temporal regions in the two later epochs, starting after the recognition of the preposition or the main verb separately.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Neural dynamics underpinning the emerging structure and interpretation of an unfolding sentence. (A-C)</title>
<p>ssRSA results of BERT parse depth vector up to Verb1 (V1), the preposition (PP1) and the main verb (MV) in epochs separately time-locked to their onsets. <bold>(D-F)</bold> ssRSA results of the mismatch for the preferred structural interpretation (the specific BERT layer from which BERT structural measures were derived was denoted in parentheses). From top to bottom in each panel: vertex t-mass (each vertex’s summed t-value during its significant period); heatmap of time-series of ROI peak t-value (the highest t-value in an ROI at each time-point) with a green bar indicating effect onset and ROI t-mass (each ROI’s summed mean t-value during its significant period); cluster t-mass time-series (summed t-value of all the significant vertices of a cluster at each time-point). [cluster-based permutation test, vertex-wise <italic>P</italic> &lt; 0.01, cluster-wise <italic>P</italic> &lt; 0.05 in (A-E); marginal significance in (F) with cluster-wise <italic>P</italic> = 0.06]. Solid vertical lines indicate the timings of onset, average uniqueness point (UP), and average offset of the word time-locked in the epoch with grey shades indicating the range of one SD. LH/RH: left/right hemisphere. See <bold>Appendix 1-table 2</bold> for full anatomical labels. See <bold>Appendix 1-<xref rid="fig8" ref-type="fig">figure 8</xref></bold> for Spearman’s rho time-series of ROIs in individual participants, see <bold>Appendix 1-figure 9</bold> for the significant results of other BERT layers in the MV epoch.</p></caption>
<graphic xlink:href="465687v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Turning to the interpretative mismatch for the two possible interpretations, we only observed significant effects of the mismatch for Active interpretation in Verb1 epoch (<bold><xref rid="fig6" ref-type="fig">Figure 6D</xref></bold>). However, it was the mismatch for Passive interpretation that fitted brain activity in the preposition and main verb epochs (<bold><xref rid="fig6" ref-type="fig">Figures 6E</xref></bold> and <bold><xref rid="fig6" ref-type="fig">6F</xref></bold>, marginal significance in main verb epoch with cluster-wise <italic>P</italic> = 0.06). These results suggest that listeners, in general, tended to have an initial preference for an Active interpretation (even before the recognition of Verb1) but might start favoring a Passive interpretation when the prepositional phrase began to be heard. This finding is consistent with the tendency to process the first noun encountered at the beginning of a sentence as the agent (Bever 1970; Jackendoff and Jackendoff 2002; Mahowald et al. 2023). Note that our approach does not constitute a direct test for the hypothesis of parallel parsing, as we did not uncover evidence supporting the maintenance of parallel representations of different syntactic structures in the brain; rather, we only found one preferred structure in each epoch.</p>
<p>Effects of the BERT parse depth vectors and those of the interpretative mismatch for the preferred structural interpretation have substantial overlaps in terms of their spatiotemporal patterns in the brain, characterized primarily by a transition from bilateral to left-lateralized fronto-temporal regions as the sentence unfolds. Across the three epochs, the most sustained effects were observed in the left inferior frontal gyrus (IFG) and the anterior temporal lobe (ATL). Notably, with the identification of the actual main verb, effects of the eventually resolved structure also involved regions in the left prefrontal and inferior parietal regions (<bold><xref rid="fig6" ref-type="fig">Figure 6C</xref></bold>) which belong to the multiple-demand network (Duncan 2010). The involvement of the prefrontal regions could be indicative of the varying working memory demands (e.g., the different number of open nodes in the sentence structures for Active and Passive interpretations before the actual main verb is recognized) for building the structure of the unfolding sentence (Pallier <italic>et al.</italic> 2011; Nelson <italic>et al.</italic> 2017).</p>
</sec>
<sec id="s4c">
<title>Structural ambiguity resolution probed using BERT Verb1 parse depth</title>
<p>As mentioned above, the potential ambiguity between a Passive and an Active interpretation centers around whether Verb1 is considered as a passive verb or the main verb, which is resolved upon the appearance of the actual main verb. We probed how this is implemented in the brain using the dynamic BERT parse depth of Verb1. Specifically, the cognitive demands required by this resolution process can be characterized by the change between the updated BERT parse depth of Verb1 when the actual main verb is presented and its initial value when Verb1 is first encountered (see <bold>Appendix 1-figure 7</bold> for the dynamic change of BERT Verb1 parse depth).</p>
<p>We first tested the change of Verb1 parse depth in the main verb epoch. Significant fits to brain activity emerged in the left posterior temporal and inferior parietal regions upon the main verb uniqueness point, and then extended to more anterior temporal regions (<bold><xref rid="fig7" ref-type="fig">Figure 7A</xref></bold>). After the main verb offset, the declining effects of the Verb1 parse depth change in the left anterior temporal region seamlessly overlapped with the arising effects of the updated Verb1 parse depth (<bold><xref rid="fig7" ref-type="fig">Figures 7B</xref></bold> and <bold><xref rid="fig7" ref-type="fig">7C</xref></bold>). These results indicate that the recognition of the actual main verb immediately triggered an update of the previous interpretation of Verb1, with the resolved interpretation emerged in the left temporal lobe and was later delivered to the right posterior temporal and parietal areas. It is also worth noting that the left hippocampus was activated for both measures of Verb1 parse depth after the actual main verb is recognized, suggesting that the episodic memory of experienced events might contribute to the updating of structural interpretations (Bicknell et al. 2010; Metusalem et al. 2012; Altmann and Ekves 2019). These results address the dynamic update of structured interpretation by focusing on the BERT parse depth of Verb1, which complements those of the interpretative mismatch based on the incremental BERT parse depth vector incorporating constraints of all the words heard so far (<bold><xref rid="fig6" ref-type="fig">Figure 6F</xref></bold>).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Neural dynamics updating the incremental structural interpretation. (A)</title>
<p>ssRSA results of BERT Verb1 (V1) parse depth change at the main verb (MV) relative to the parse depth of V1 when it is first encountered. <bold>(B)</bold> ssRSA results of the updated BERT V1 parse depth when the input sentence reaches the MV. <bold>(C)</bold> Spatiotemporal overlap between the effects in (A) and (B). (cluster-based permutation test, vertex-wise <italic>P</italic> &lt; 0.01, cluster-wise <italic>P</italic> &lt; 0.05). See <bold>Appendix 1-figure 14</bold> for Spearman’s rho time-series of ROIs in individual participants.</p></caption>
<graphic xlink:href="465687v4_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4d">
<title>Emergent structural interpretations driven by multifaceted constraints in the brain</title>
<p>Next, we further asked how the multifaceted constraints, considered by human listeners and encoded in BERT parse depths, drive the structured interpretation in the brain. When and where in the brain do these constraints emerge? How are their neural effects related to those of the final resolved sentential structure? To address these questions, we first tested the subject noun thematic role properties obtained from corpus data. Significant effects of agenthood and patienthood were found in the preposition epoch (<bold><xref rid="fig8" ref-type="fig">Figure 8A</xref></bold>) and in the main verb epoch (<bold><xref rid="fig8" ref-type="fig">Figure 8B</xref></bold>) separately. Notably, effects of the subject noun itself preceded those of incremental BERT parse depth vectors modelling the sentence fragments in the same epoch (compare <bold><xref rid="fig8" ref-type="fig">Figure 8A</xref></bold> with <bold><xref rid="fig6" ref-type="fig">Figure 6B</xref></bold>, and <bold><xref rid="fig8" ref-type="fig">Figure 8B</xref></bold> with <bold><xref rid="fig6" ref-type="fig">Figure 6C</xref></bold>). These findings indicate that subject noun thematic role might be evaluated before building the overall structural interpretation of the utterance delivered so far. Specifically, the initial preference for an Active interpretation during Verb1, while present as the preposition started (i.e., subject noun agenthood in <bold><xref rid="fig8" ref-type="fig">Figure 8A</xref></bold>), was superseded by the preference for a Passive interpretation as the rest of the prepositional phrase (<bold><xref rid="fig6" ref-type="fig">Figure 6A</xref></bold>) and the main verb (<bold><xref rid="fig6" ref-type="fig">Figure 6B</xref></bold>) were heard.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Neural dynamics of multifaceted probabilistic constraints underpinning incremental structural interpretations.</title>
<p><bold>(A</bold>, <bold>B)</bold> ssRSA results of SN agenthood and SN patienthood (i.e., plausibility of SN being the agent or the patient of V1) in PP1 and MV epochs separately. <bold>(C)</bold> ssRSA results of non- directional index (i.e., interpretative coherence between SN and V1 regardless of the structure preferred) in MV epoch. <bold>(D)</bold> ssRSA results of Passive index (i.e., interpretative coherence for the Passive interpretation) in MV epoch. <bold>(E)</bold> Influence of the Passive interpretative coherence on the emerging sentential structure in MV epoch revealed by the Granger causal analysis (GCA) based on the non-negative matrix factorization (NMF) components of whole-brain ssRSA results (see <bold>Appendix 1-figure 16</bold> for more details) [(A-D) cluster-based permutation test, vertex-wise <italic>P</italic> &lt; 0.01, cluster-wise <italic>P</italic> &lt; 0.05; (E) permutation test <italic>P</italic>FDR &lt; 0.05]. See <bold>Appendix 1-figure 15</bold> for Spearman’s rho time-series of ROIs in individual participants.</p></caption>
<graphic xlink:href="465687v4_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Despite being jointly constrained by subject noun thematic role preference and Verb1 transitivity in a probabilistic manner, the structural interpretation temporarily held just before the recognition of the actual main verb could differ across sentences (e.g., Passive interpretation in “<italic>The dog found in the park</italic>…” and Active interpretation in “<italic>The dog walked in the park</italic>…”). Therefore, in contrast to the Passive or Active index specialized for one particular structural interpretation, we constructed a non-directional index that merely quantifies the degree of interpretative coherence for one interpretation, whether Passive or Active (see Methods for details). Thus, a higher value only indicates greater interpretative coherence between the subject noun and Verb1 regardless of which interpretation is preferred.</p>
<p>Effects of this non-directional measure of interpretative coherence appeared very soon after the main verb onset in both hemispheres and lasted till its offset (<bold><xref rid="fig8" ref-type="fig">Figure 8C</xref></bold>), suggesting an immediate evaluation of the previously integrated constraints from the subject noun and Verb1 when a listener realized that the sentence had not finished yet. Moreover, these effects roughly co-occurred with the effects of subject noun patienthood (compare <bold><xref rid="fig8" ref-type="fig">Figures 8B</xref></bold> and <bold><xref rid="fig8" ref-type="fig">8C</xref></bold>), indicating that a patient role for the subject noun was considered as the main verb was being recognized. Intriguingly, the most sustained regions associated with this non-directional index, including the left ATL, angular gyrus (AG) and precuneus, are also the classical areas of the default mode network (DMN). This finding is consistent with recent claims that the DMN integrates external input with internal prior knowledge to make sense of an input stimulus such as speech (Yeshurun <italic>et al.</italic> 2021). In particular, precuneus and AG have been found to be involved in building thematic relationships and event structures (Baldassano et al. 2017; Humphreys et al. 2021).</p>
<p>Following the declining effects of the non-directional index upon the recognition of the main verb, we found significant effects of the Passive index in right anterior fronto-temporal regions (<bold><xref rid="fig8" ref-type="fig">Figure 8D</xref></bold>), suggesting that the intended Passive interpretation was eventually established in all sentences. Previous studies have revealed that the relatively narrow sentence-specific information and the broad world knowledge are processed in the left and right hemispheres separately (Jung-Beeman 2005; Metusalem et al. 2016; Troyer et al. 2022). Relevant to this, in the main verb epoch, we found effects of the BERT parse depth vector and those of the Passive index in the left and right hemispheres respectively, arising almost at the same time as the main verb was recognized (compare <bold><xref rid="fig6" ref-type="fig">Figure 6C</xref></bold> and <bold><xref rid="fig8" ref-type="fig">Figure 8D</xref></bold>). Therefore, a critical question is whether and how the online structural interpretation of a specific sentence is facilitated by the interpretative coherence conjured up from lexical constraints that also depend on broad world knowledge (e.g., thematic role).</p>
<p>To address this question, we adopted non-negative matrix factorization to decompose the whole-brain ssRSA fits of the Passive index and the BERT parse depth vector found in the main verb epoch into two sets of components given their temporal synchronizations (see Methods). We then conducted multivariate Granger causality analyses (GCA) to infer directed connections among them. We found only GC connections from the components of Passive index to those of BERT parse depth vector (<bold><xref rid="fig8" ref-type="fig">Figure 8E</xref></bold>). Specifically, we identified information flows from the right hemisphere components of the Passive index to the left hemisphere components of BERT parse depth vector, suggesting that a sentence’s structure represented in the left hemisphere might be influenced by the coarse estimate of the event plausibility concurrently determined by broad world knowledge in the right hemisphere (Jung-Beeman 2005) (see <bold>Appendix 1-figure 16</bold> for more details).</p>
</sec>
<sec id="s4e">
<title>Comparisons between BERT and corpus/behavioural measures in fitting neural activity</title>
<p>To directly assess the performance of BERT structural measures with that of traditional measures extracted from corpus or behavioural data in fitting listeners’ neural activity, we also conducted ssRSA with model RDMs of corpus-based or behavioural measures. In the Verb1 epoch, we tested Verb1 transitivity obtained from either corpus data or human continuations, however, neither of them exhibited significant model fits, which contrasted with the pronounced effects of BERT Verb1 parse depth (<bold>Appendix 1-figure 11</bold>). Similarly, in the PP1 and MV epochs, the probabilities of PP and MV continuations, as determined from behavioral data, did not show any significant model fits (<bold>Appendix 1-figure 12</bold> and <bold>Appendix 1-figure 13</bold>). Furthermore, the effects of BERT parse depth vector in these two epochs (<bold><xref rid="fig6" ref-type="fig">Figure 6B</xref></bold> and <bold><xref rid="fig6" ref-type="fig">6C</xref></bold>) remained largely unchanged after controlling for the variance explained by the behavioural measures. These findings suggest that BERT structural measures, compared to corpus-based and behavioral measures, are better at fitting the neural dynamics during incremental speech comprehension. This might be attributed to the capacity of DLMs to capture more nuanced and contextually rich representations (Linzen and Baroni 2021; Pavlick 2022).</p>
</sec>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>In this study, we investigated the neural dynamics involved in constructing structured interpretations from speech. We combined spatiotemporally resolved brain activity of human listeners, quantitative structural representations derived from a DLM (i.e., BERT), and corpus- based and behavioural measures. Our study revealed the emergence and update of a structured interpretation, jointly constrained by different lexical properties related to both linguistic and non-linguistic world knowledge, in an extensive set of brain regions beyond the core fronto- temporal language network. Specifically, our results show (1) a shift from bi-hemispheric lateral frontal-temporal regions to left-lateralized regions in representing the current structured interpretation as a sentence unfolds, (2) a pattern of sequential activations in the left lateral temporal regions, updating the structured interpretation as syntactic ambiguity is resolved, and (3) the influence of lexical interpretative coherence activated in the right hemisphere over the resolved sentence structure represented in the left hemisphere. These findings provide empirical evidence for the <italic>constraint-based</italic> approach to sentence processing and deepen the understanding of specific spatiotemporal patterning and neuro-computational properties underpinning incremental speech comprehension.</p>
<p>Using artificial neural networks (ANNs) to study the neural substrates of human cognition complements the long-time pursuit of generative rules and interpretable models (Kriegeskorte and Douglas 2018). ANNs have informed our understanding of various cognitive processes in the brain by providing quantifiable predictions that aim to connect behaviors and relevant neural activity (Yamins and DiCarlo 2016; Rabovsky et al. 2018; Donhauser and Baillet 2019; Kietzmann et al. 2019; Yang et al. 2019; Bao et al. 2020; Sheahan et al. 2021; Doerig et al. 2023; Giordano et al. 2023). This is crucial for quantifying the outcome of complex, interrelated constraints that arise in specific contexts, such as spoken sentences, and constructing the representational geometry to be probed in the brain. Where DLMs are concerned, recent studies have systematically compared the internal representations of DLMs to those observed in the human brain during language processing, which highlights the importance of predictive coding and contextual information (Schrimpf <italic>et al.</italic> 2021; Caucheteux <italic>et al.</italic> 2022; Caucheteux and King 2022; Goldstein <italic>et al.</italic> 2022; Heilbron <italic>et al.</italic> 2022; Toneva <italic>et al.</italic> 2022; Caucheteux <italic>et al.</italic> 2023). Furthermore, these studies have motivated the use of DLMs as a computational tool, or hypothesis, to study the neural substrates of language.</p>
<p>Here we asked a more specific question, that is, how a sequence of spoken words is incrementally structured and coherently interpreted in the brain. Our goal was to use quantitative measures of sentence structure that capture the interplay between different types of constraints that simultaneously influence this process. As a potential solution, we extracted detailed structural measures specific to the contents in each sentence from the hidden states of BERT, which was trained on massive corpora from real-life language use. Although DLMs such as BERT are not specifically designed to parse sentences, they can learn from training corpora the multi-dimensional properties related to sentence structure and dependency (Manning <italic>et al.</italic> 2020). In line with this, our analyses confirmed that BERT structural measures incorporate relevant lexical constraints and that they exhibit both behavioural and neural alignments with human listeners.</p>
<p>Taking advantage of the contextualized BERT structural measures, our ssRSA results provide neural evidence for the construction of a coherent interpretation driven by the interaction between linguistic and non-linguistic knowledge evoked by individual words as they are heard sequentially in a spoken sentence. Specifically, neural representations of an unfolding sentence’s structure initially emerged in bilateral fronto-temporal regions and became left- lateralized when more complex syntactic properties, rather than canonical linear adjacency, were considered to build a structured interpretation (e.g., beyond Verb1 in our stimulus sentences). Meanwhile, we found right-hemisphere activations associated with broad world knowledge, which is essential for understanding the intended meaning conveyed by the speaker (Bicknell <italic>et al.</italic> 2010). In addition to the core fronto-temporal language network, we found that the multiple-demand network and the default mode network were also involved during online construction of structured interpretations, which may reflect additional cognitive demands for resolving potential structural ambiguity and evaluating the plausibility of underlying events (Smallwood et al. 2021).</p>
<p>Moreover, our results show that, compared to corpus-based and behavioural measures, BERT structural measures are more effective in fitting listeners’ neural activity, possibly due to its advanced ability in modelling specific contexts within each sentence. Nevertheless, it is important to recognize the important role of corpus-based and behavioral measures as explanatory variables. They are crucial not only in interpreting BERT measures but also in understanding their alignment with listeners’ neural activity. This includes, for instance, the temporal sequence of activations of key lexical constraints and the emerging structure of a sentence (e.g., effects of subject noun patienthood leading those of BERT parse depth vector in the MV epoch, as seen in <bold><xref rid="fig8" ref-type="fig">Figure 8B</xref></bold> and <bold><xref rid="fig6" ref-type="fig">Figure 6C</xref></bold>) and the spatial distribution of their model fits in the brain (e.g., contrasting model fits of Passive index and BERT parse depth vector in the MV epoch across different hemispheres, as shown in <bold><xref rid="fig8" ref-type="fig">Figure 8D</xref></bold> and <bold><xref rid="fig6" ref-type="fig">Figure 6C</xref></bold>). Such an integrative approach allows for a more comprehensive understanding of the complex mental processes underpinning speech comprehension, which takes advantages of the interpretability of traditional measures and the deep contextualized representations of DLMs.</p>
<p>There are two points to note about the use of BERT. Firstly, unlike autoregressive DLMs trained using left-to-right attention and the next-word prediction task, BERT is trained to predict masked words in a sentence with a bi-directional attention mechanism. The additional right-to-left attention provides updated representations of preceding words every time an incoming word is added to the input (e.g., representation of <italic>“dog”</italic> in <italic>“The dog…”</italic> is different from that in <italic>“The dog found…”</italic>). This feature of BERT is useful for tracking the dynamic change of the representation of a specific word as its context evolves (e.g., Verb1 in this study), particularly in sentences with structural ambiguity. Although autoregressive DLMs also update hidden states as the input unfolds and could be used to study complex sentential structures (Jurayj et al. 2022), the updated contextual effects are reflected in the hidden states of the right- most incoming word, while those of the preceding words on the left remain unchanged (i.e., the representation of <italic>“dog”</italic> is the same in <italic>“The dog…”</italic> and <italic>“The dog found…”</italic>). This is different from BERT, where the updated contextual effects are reflected in the hidden states of all preceding words.</p>
<p>Secondly, although we input each sentence word-by-word to BERT, however, unlike human listeners or recurrent neural networks, BERT process two consecutive inputs (e.g., <italic>“The dog…”</italic> and <italic>“The dog found…”</italic>) independently, and there is no direct relationship between the hidden states of these two inputs. In fact, human listeners would not start over from the beginning of a sentence as it unfolds word-by-word, but update it continually as each word is heard and use whatever information currently available to build a coherent interpretation (McRae and Matsuki 2013). This difference, however, does not impede our objective of extracting contextualized structural representations at critical points within a sentence. In the case of BERT case, the representation of each word is continuously updated in a bi-directional manner as a new word is added. This process accounts for the constraints imposed by all the words of the input and their interactions, forming a coherent interpretation. Nonetheless, for other hypotheses in speech comprehension, such as parallel parsing and how various grammatically correct sentence structures are maintained and compete in the brain, DLMs with recurrent memory might be more suitable. Such models can better stimulate the continuous, dynamic updating of interpretations that characterizes human sentence processing.</p>
<p>In summary, recent developments in DLMs have shown great potential in capturing the dynamic interplay between syntax, semantics, and broader world knowledge that is essential for successful language comprehension. The empirical evidence from this study supports the notion that DLMs, when utilized as potential models of brain computation and integrated with advanced neuroimaging techniques within a well-defined framework can offer significant insights in to human cognition (Kriegeskorte and Douglas 2018; Doerig <italic>et al.</italic> 2023). Future DLMs, especially those with more human-like model architecture (McClelland et al. 2020) and subjected to rigorous evaluation (Binz and Schulz 2023), hold the potential to shed light on the neural implementation of various rapid incremental processes that support the rapid transition from sound to meaning in the brain.</p>
</sec>
<sec id="s6">
<title>Materials and Methods</title>
<sec id="s6a">
<title>Participants</title>
<p>Seventeen right-handed native British English speakers participated in this study and provided written consent. One participant was excluded from subsequent analysis due to sleepiness during EMEG scanning, the other sixteen participants were included in the following analyses (aged between 19 and 38 years, 26.5 years on average; 7 females). All participants had normal hearing, and none had any pre-existing neurological condition or mental health issues. This study was approved by the Cambridge Psychology Research Ethics Committee.</p>
</sec>
<sec id="s6b">
<title>Stimuli</title>
<p>We constructed 60 sets of six spoken sentences (360 in total) with varying sentential structures. As shown in the example sentence set (<bold>Appendix 1-table 3</bold>), unambiguous (UNA), high transitivity (HiTrans) and low transitivity (LoTrans) sentences contain a long-distance dependency between the subject noun and the main verb introduced by a full or reduced relative clause inserted in between. Whereas there is no long-distance dependency in the sentences of passive (PAS) and two direct object (DO1 and DO2) conditions.</p>
<p>Unlike the first verb (Verb1) in the UNA sentences which is unambiguously interpreted as the head of a relative clause, Verb1 in both HiTrans and LoTrans sentences can also be considered alternatively as the “main verb” before the actual main verb (e.g., <italic>was covered</italic> in the example set in <bold>Appendix 1-table 3</bold>) was heard. By varying the nature of Verb1 in the reduced relative clause (e.g., <italic>found/walked</italic>), we manipulated the preference for the two plausible structural interpretations in HiTrans and LoTrans sentences before the appearance of the actual main verb (i.e., a Passive interpretation where Verb1 is the head of a relative clause - the subject noun undergoes the action specified by Verb1; an Active interpretation where Verb1 is the main verb - the subject noun performs the action specified by Verb1).</p>
<p>Specifically, in the LoTrans sentences, Verb1 was selected to be optionally transitive according to CELEX (Baayen et al. 1993) and Google n-gram corpus (books.google.com/ngrams), meaning that it can either take a direct object or not. Thus, a listener could be initially “garden- pathed” into the alternative Active interpretation where the Verb1 is considered as the main verb when a following prepositional phrase fits its intransitive use. Whereas Verb1 in HiTrans sentences was selected to have a higher preference for taking a direct object than Verb1 in LoTrans sentences [subcategorization frame (SCF) probability for direct object according to VALEX (Korhonen et al. 2006): HiTrans 0.71 ± 0.16, LoTrans 0.44 ± 0.19, two-tailed two- sample t-test, t(117) = 8.45, p = 9.3 x 10<sup>-14</sup>]. Therefore, Verb1 in HiTrans sentences was more likely to be recognised as the head of a reduced relative clause given the appearance of a prepositional phrase (e.g., <italic>in the park</italic>) rather than a highly expected direct object.</p>
<p>In all the six types of sentences, the subject noun phrase comprised a single-word noun and a preceding determine “<italic>The</italic>”. In each sentence set, sentences of the first four types had the same subject noun, while DO1 and DO2 sentences shared a different subject noun. The reduced relative clause in HiTrans and LoTrans sentences consisted of a head verb, i.e., Verb1 (e.g., <italic>found/walked</italic>) which was followed by a three-word prepositional phrase (e.g., <italic>in the park</italic>).</p>
<p>Note that, in 15 out of the 60 sentence sets, the actual main verb in UNA, RR and GP conditions was preceded by an auxiliary verb (e.g., “<italic>was covered</italic>” in the example set in <bold>Appendix 1-table 3</bold>). In the following analyses, we defined the first word after the prepositional phrase as the main verb since its appearance is sufficient to resolve the intended Passive interpretation where Verb1 is a passive verb (i.e., the head of a reduced relative clause).</p>
<p>Note that, although UNA, PAS, DO1 and DO2 conditions were not included in subsequent analyses, they added variety to the types of syntactic construction of the stimuli and ecological validity of the experiment, which also prevented potential adaption to a particular sentence structure.</p>
</sec>
<sec id="s6c">
<title>Procedure</title>
<p>The experimental stimuli (360 spoken sentences recorded by a female native British English speaker with a neutral intonation throughout) were equally divided into four blocks with 90 experimental trials in each. To maintain participants’ attention while they were listening to the stimuli, the experimental trials in each block were interspersed with 9 additional trials consisting of questions related to the contents in the preceding sentence. These questions were presented in written form on the screen, and a “yes” or “no” response was required by button pressing. Each of these question trials was followed by a filler trial (including a normal spoken sentence outside the experimental stimuli) to ensure that no residual task effects would be picked up in the next experimental trial. Each block started with two filler trials. All question trials and filler trials (20 in each block) were excluded from the following analyses. The order of blocks and the order of trials within each block were pseudorandomized across participants.</p>
<p>Each experimental trial began with a fixation cross presented at the centre of the screen with a random period ranging from 750ms to 1250ms (1000ms on average) before the onset of the spoken sentence. Participants were asked to look at the fixation cross and to avoid eye movement or blinking while listening to the spoken sentences. There was a 1000ms silence from the end of each sentence followed by a “blink cue” that lasted for 1400ms during which participants could blink. E-Prime Studio version 2 (Psychology Software Tools Inc., PA, USA) was used to present stimuli and record participants’ responses.</p>
<p>Auditory stimuli were delivered binaurally through MEG-compatible ER3A insert earphones (Etymotic Research Inc., IL, USA). There was a 26ms ± 2ms delay in sound delivery due to the transmission of auditory signal from the stimulus computer to the earphones. This sound delivery delay was corrected in the following analyses. To ensure that participants were able to hear the stimuli through both earphones, a short hearing test was conducted before the main experiment.</p>
</sec>
<sec id="s6d">
<title>Sentence continuation pre-tests</title>
<p>To obtain human incremental interpretations for each of the HiTrans and LoTrans sentences (120 in total), we conducted two continuation pre-tests which involved two different groups of native British English speakers (30 participants in the first pre-test, 18 participants in the second, aged between 18 and 34 years) who did not participate in the main experiment.</p>
<p>Specifically, participants wore headphones and were seated in front of a computer. They listened to a fragment of one of the HiTrans/LoTrans sentences starting from its onset and continuing until a certain position in the sentence, and then they were asked to complete this sentence by producing a meaningful continuation. The sentence fragment was binaurally presented up to the Verb1 (e.g., <italic>The dog found…</italic>) in the first pre-test, and was presented up to the end of the prepositional phrase in the second pre-test (e.g., <italic>The dog found in the park…</italic>). In the second pre-test, participants were allowed to provide a full stop as a continuation if they thought that what they had heard was a complete sentence.</p>
<p>Based on the continuations obtained in the first pre-test, we calculated the probability of direct object or prepositional phrase in the continuations immediately after the Verb1, i.e., DO probability and PP probability. This provided contextualized measures of the transitive or the intransitive use of Verb1 given the preceding subject noun phrase. Specifically, we defined Verb1 transitivity as DO probability/(1- DO probability) and defined Verb1 intransitivity as (1-DO probability)/DO probability. Given the continuations collected in the second pre-test, we calculated the probability of a main verb in the continuations immediately after the prepositional phrase, i.e., MV probability, which directly reflected a listener’s structural interpretation by the end of the prepositional phrase. The absence of a main verb in the continuation after the prepositional phrase indicated that the Active interpretation was taken and the Verb1 was considered as the “main verb”, whereas the appearance of a main verb in the continuation indicated that the Passive interpretation was taken (i.e., Verb1 was interpreted as a passive verb), and thus a main verb was needed to complete the sentence.</p>
</sec>
<sec id="s6e">
<title>EMEG and MRI acquisition</title>
<p>Participants were seated in a magnetically shielded room (IMEDCO GMBH, Switzerland) with their head placed in the helmet of the MEG scanner. MEG data were collected using a Neuromag Vector View system (Elekta, Helsinki, Finland) with 102 magnetometers and 204 planar gradiometers at 1kHz sampling rate. Simultaneous EEG was recorded at 1kHz sampling rate from 70 Ag-AgCl electrodes within an elastic cap (ESACYCAP GmbH, Herrsching- Breitbrunn, Germany). Vertical and horizontal eye movements were recorded by two EOG electrodes attached below and lateral to the left eye, cardiac signals were recorded by two ECG electrodes attached separately to the right shoulder blade and left torso. Five head position indicator (HPI) coils were used to monitor head motion. A 3D digitizer was used to record the position of EEG electrodes, HPI coils and head points on participants’ scalp relative to the 3 anatomical fiducials (i.e., nasion and bilateral preauricular points). To source localize EMEG data, T1-weighted MPRAGE structural MRI image with 1mm isotropic resolution was acquired using a Siemens Prisma 3T scanner (Siemens, Erlangen, Germany). All EMEG and MRI data were collected at the MRC Cognition and Brain Sciences Unit, University of Cambridge.</p>
</sec>
<sec id="s6f">
<title>EMEG preprocessing and source localization</title>
<p>Maxfilter (Elekta, Helsinki, Finland) was applied to raw MEG data for bad channel removal and head-motion compensation. Signals outside the brain were removed using the temporal extension of signal-space separation (Taulu and Simola 2006). EMEG data were low-pass filtered at 40 Hz and high-pass filtered at 0.5 Hz with a 5<sup>th</sup> order bidirectional Butterworth filter using SPM12 (Wellcome Trust Centre for Neuroimaging, UCL). Independent component analysis (ICA) was conducted using EEGLAB (SCCN, UCSD), components related to blink, eye-movement and physiological noises were removed according to the correlation with EOG, ECG signals and further visual inspection. The preprocessed EMEG data were then downsampled to 200 Hz. Three epochs were extracted from the continuous EMEG recordings of each HiTrans or LoTrans sentence with auditory delivery delay corrected - V1 epoch was aligned to the onset of the Verb1, PP1 epoch was aligned to the onset of the preposition, MV epoch was aligned to the onset of the main verb. All the three epochs were 600ms in length. For all three epochs, baseline correction was performed using the signal from a silent period (i.e., -200 ms to 0 ms relative to sentence onset). Finally, automatic artefact rejection was conducted to exclude trials with signals that exceeded pre-defined amplitude thresholds (60 ft/mm for gradiometers, 3000 ft for magnetometers and 200 uV for EEG electrodes). The uniqueness point of V1/PP1/MV was defined as the earliest point in time when this word can be fully recognized after removing all of its phonological competitors. We first identified the phoneme by which this word can be uniquely recognized according to CELEX (Baayen <italic>et al.</italic> 1993). Then, we manually labelled the offset of this phoneme in the auditory file of the spoken sentence.</p>
<p>EMEG data source localization was performed using SPM12. Source space was modelled by a cortical mesh consisting of 8196 vertices. The sensor positions were co-registered to individual T1-weighted structural image by aligning fiducials and the digitized head shape to the outer scalp mesh. MEG forward model was constructed using the single-shell model (Sarvas 1987), while EEG forward model was built using the boundary element model (Mosher et al. 1999). Inversion of EMEG data was conducted for V1, PP1 and MV epochs separately using the least-squares minimum norm method (Hamalainen and Ilmoniemi 1994) and an empirical Bayesian MEG and EEG data fusion scheme implemented in SPM12 (Henson et al. 2009).</p>
</sec>
<sec id="s6g">
<title>Incremental structural representations of BERT</title>
<p>To obtain incremental structural representations of BERT, we adopted a structural probing approach (Hewitt and Manning 2019) to quantify a sentence’s structure by estimating each word’s parse depth in the corresponding dependency parse tree based on the contextualized word embeddings from the hidden states of BERT, which explicitly considers the specific contents of the words in the input. Specifically, a structural probing model was trained to find an optimal linear transformation to be applied to the BERT contextualized embeddings of words in the input sentence, so that the squared L2 norm of the transformed word embeddings provided the best estimate for each word’s parse depth of in the dependency parse tree of this sentence.</p>
<p>We followed the procedure described in a previous study (Hewitt and Manning 2019) and trained a structural probing model for each BERT layer with the annotated corpus from Penn Treebank (Marcus et al. 1993). Contextualized word embeddings were extracted from each of the 24 layers in a pre-trained version of BERT (BERT-large-cased) using HuggingFace (Wolf et al. 2019). For each BERT layer, the training process was repeated 10 times with different random initializations, the averaged BERT parse depth was used in the following analyses. The performance of structural probing models trained by different BERT layers was evaluated by root accuracy. Root accuracy is defined as the percentage of the sentences in which the samllest parse depth is assigned to the main verb (i.e., the root of the dependency parse tree has a parse depth of 0) when the whole sentence is input to the model.</p>
<p>Each HiTrans or LoTrans sentence was input word-by-word to the trained BERT structural probing models, which resulted in a vector consisting of the parse depth of each word in the incremental input (e.g., a 3-dimensional BERT parse depth vector for the input “<italic>The dog found</italic>…”). Taking advantage of the bi-directional attention mechanism of BERT, the parse depth of each preceding word was constantly updated as the input unfolded word-by-word, capturing the incrementality of speech comprehension. Besides, we defined interpretive mismatch as the cosine distance between an incremental BERT parse depth vector and the corresponding incremental context-free parse depth vector for the Passive or the Active interpretation. The smaller the interpretive mismatch with one particular interpretation, the higher the preference for this interpretation given BERT structural representations.</p>
<p>To determine the contribution of the words at different positions in a sentence to the incremental BERT parse depth vectors, we shuffled the parse depths of the words at a particular position across sentences at a time and kept the parse depths of the other words unchanged. Then we calculated the Spearman distance (i.e., 1-Spearman’s rho) between the original BERT parse depth vector and the shuffled counterpart. The higher this distance, the more important the words at this position are to the BERT parse depth vectors (see <bold>Appendix 1-figure 3</bold>).</p>
</sec>
<sec id="s6h">
<title>Corpus-based measures of multifaceted constraints and their interpretative coherence</title>
<p>We quantified subject noun thematic role properties and Verb1 transitivity preference based on a concatenated corpus (3.4 billion tokens, 162.1 million sentences) consisting of the British National Corpus, the Wikipedia dump (by October 2020) and ukWaC (Baroni et al. 2009). A dependency parser (Mrini et al. 2020) was first applied to each sentence in the concatenated corpus to specify the subject noun and the verb(s) related to it. For the subject noun in each sentence, we used a semantic role labelling model (Li et al. 2020) to obtain its thematic role. For simplicity, we only considered the thematic role of an agent or a patient (Dowty 1991).</p>
<p>For each subject noun in HiTrans and LoTrans sentences, we counted separately how many times it took the thematic role of an agent or a patient in the concatenated corpus. Then we defined its agenthood as the ratio of the number of its appearances as an agent to that of its appearances as a patient, and versa visa for its patienthood. We also counted separately how many times the Verb1 in each HiTrans or LoTrans sentence took a direct object or alternative SCFs in the corpus. Each Verb1’s transitivity was defined as the ratio of the frequency it took a direct object to that it took alternative SCFs, and vice versa for its intransitivity. By doing so, we obtained subject noun agenthood and patienthood, Verb1 transitivity and intransitivity. Note that the Verb1 (in)transitivity estimated from the human continuations in the pre-test is context-dependent given the specific preceding subject noun, while the Verb1 (in)transitivity estimated from the concatenated corpus is context-independent in the sense that it accounted for every appearance of this verb without being biased to a specific context.</p>
<p>We further derived Passive/Active index capturing the interpretative coherence between the subject noun and Verb1 as they affected Passive and Active interpretations separately. The Passive index was obtained by multiplying subject noun patienthood with Verb1 transitivity, given that both high subject noun patienthood and high Verb1 transitivity coherently prefer a Passive interpretation as the prepositional phrase is heard (e.g., <italic>The <bold>dog found</bold> in the park…</italic>). In contrast, the Active index was obtained by multiplying subject noun agenthood by Verb1 intransitivity, capturing the preference for an Active interpretation (e.g., <italic>The <bold>king walked</bold> in the garden…</italic>). Besides, we also calculated a contextualized version Passive/Active index by using Verb1 (in)transitivity derived from human continuation pre-tests instead of that derived from the concatenated corpus. In addition, we derived a non-directional index by applying logarithmic transformation to the ratio measures of lexical constraints (i.e., subject noun agenthood or patienthood, Verb1 intransitivity or transitivity) before multiplying them. This manipulation removed the directionality of the Passive or the Active index, so that the non- directional index only indicates the interpretative coherence between the subject noun and Verb1 regardless of which interpretation is considered (see illustrations of directionality in <bold>Appendix 1-figure 17</bold>).</p>
</sec>
<sec id="s6i">
<title>Spatiotemporal searchlight representational similarity analysis (ssRSA)</title>
<p>ssRSA was conducted to compare the (dis)similarity structure of BERT structural measures or the multifaceted probablistic constraints with the (dis)similarity structure of observed spatiotemporal patterns of listeners’ brain activity. We used a spatiotemporal searchlight with a 10mm spatial radius and 30ms temporal radius (i.e., a 60 ms sliding time window) which was mapped across the whole brain in the source-localized EMEG.</p>
<p>For brain activity, we constructed data representational dissimilarity matrix (RDM) by vectorizing the source-localized EMEG data within each spatiotemporal searchlight for all the trials (i.e., 60 HiTrans sentences and 60 LoTrans sentences) and calculated the pairwise Pearson’s correlation distance (i.e., 1 - Pearson’s r) among them, which resulted in a 120 x 120 data RDM. Multivariate normalisation was applied to improve the reliability of distance measures and reduce the task-irrelevant heteroscedastic structure across trials and vertices (Guggenmos et al. 2018). Model RDMs of the same size (i.e., 120 x 120) were constructed by calculating either the absolute pair-wise difference for a scalar measure (e.g., SN agenthood/patienthood, Passive/Active index, BERT Verb1 parse depth) or the cosine distance among the incremental BERT parse depth vectors of the 120 sentences (see <bold>Appendix 1-table 1</bold> for a summary of all the model RDMs). We used ratio measures to represent subject noun agenthood/patienthood, Verb1 transitivity/intransitivity and Passive/Active index because they provided the directionality needed to differentiate the two opposite aspects of the same lexical constraint in the model RDMs (see illustrations in <bold>Appendix 1-figure 11</bold>) and made it possible to test them separately in the brain.</p>
<p>Each model RDM was compared against the data RDM of a searchlight centered at each vertex and time point using Spearman’s rank correlation, which resulted in a time-series of model fit (i.e., rank correlation coefficient rho) for each vertex. For each time point, a one-tailed one- sample t-test was conducted at each vertex with the fits of all participants for this model RDM to test whether the mean model fit is significantly above zero. Cluster permutation tests were performed for multiple comparison correction with 5,000 nonparametric permutations, vertex- wise <italic>P</italic> &lt; 0.01 and cluster-wise <italic>P</italic> &lt; 0.05.</p>
</sec>
<sec id="s6j">
<title>Granger causality analysis (GCA) based on ssRSA model fits</title>
<p>GCA was conducted to investigate the relationship between multifaceted constraints and BERT structural measures in terms of their effects in the brain, i.e., ssRSA model fits of the corresponding model RDMs. For a given model RDM, each participant’s non-thresholded whole-brain model fit time-series were normalized and concatenated across participants, which resulted in a vertex by time-point matrix. Non-negative matrix factorization (NMF) was applied to this concatenated model fit matrix with negative model fits zeroed. NMF was repeated 20 times with random starting values using a multiplicative update algorithm in MATLAB, results with the least root mean square residual (RMS) was used in the following analyses. The optimal number of NMF factors was determined by searching for the one with the least RMS in a wide range of factor numbers (from 2 to 50). With the optimal number of factors, the resulted time-series of NMF factors from all participants were reshaped into a factor by time-point by participant matrix which was used as the input of multivariate GCA implemented by the Multivariate GCA toolbox (Barnett and Seth 2014). Multivariate GCA was conducted using two sets of NMF factors derived from the fits of two model RDMs with Akaike information criterion (AIC) adopted for GCA model order estimation. GC significance was determined by a permutation test in which 1,000 surrogate data sets were created by randomly rearranging short time windows (of length model order) from the original factor time-course. <italic>P</italic>-values below 0.005 were refined via a tail approximation from the Generalised Pareto Distribution using PALM (Winkler et al. 2016). Multiple comparisons were controlled with false discovery rate (FDR) alpha &lt; 0.05 for both between- and within-model RDM GC connections.</p>
</sec>
</sec>
<sec id="d1e1193" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1307">
<label>Appendix 1</label>
<media xlink:href="supplements/465687_file03.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This research was funded by European Research Council Advanced Investigator Grant to L.K.T. under the European Community’s Horizon 2020 Research and Innovation Programme (2014-2022 ERC Grant Agreement 669820). B.L. was supported by the Ministry of Science and Technology of China and Changping Laboratory. We thank Billi Randall and Barry Devereux for their valuable contributions to early experimental design and to stimulus development; and Hun S. Choi, Benedict Vassileiou, John Hewitt, Tao Li, Yi Zhu, Nai Ding and Giorgio Marinato for helpful discussions.</p>
</ack>
<sec id="s7">
<title>Author contributions</title>
<p>Conceptualization: L.K.T., W.D.M., B.L. Investigation, Data curation: B.L., Y.F. Methodology, Formal Analysis &amp; Visualization: B.L. Funding acquisition &amp; Project administration: L.K.T. Supervision: L.K.T., W.D.M.</p>
<p>Writing – original draft, review &amp; editing: B.L., W.D.M., L.K.T.</p>
</sec>
<sec id="s8">
<title>Declaration of interests</title>
<p>Authors declare no competing interests.</p>
</sec>
<sec id="s9">
<title>Data and materials availability</title>
<p>Preprocessed E/MEG data and scripts are available in the following repository <ext-link ext-link-type="uri" xlink:href="https://osf.io/7u8jp/">https://osf.io/7u8jp/</ext-link>.</p>
</sec>
<ref-list>
<title><bold>References</bold></title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Altmann</surname> <given-names>GT</given-names></string-name>. <year>1998</year>. <article-title>Ambiguity in sentence processing</article-title>. <source>Trends in Cognitive Sciences</source>. <volume>2</volume>:<fpage>146</fpage>–<lpage>152</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Altmann</surname> <given-names>GTM</given-names></string-name>, <string-name><surname>Ekves</surname> <given-names>Z</given-names></string-name>. <year>2019</year>. <article-title>Events as intersecting object histories: A new theory of event representation</article-title>. <source>Psychological Review</source>. <volume>126</volume>:<fpage>817</fpage>–<lpage>840</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Baayen</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Piepenbrock</surname> <given-names>R</given-names></string-name>, <string-name><surname>van</surname> <given-names>H R</given-names></string-name>. <year>1993</year>. <article-title>The {CELEX} lexical data base on {CD-ROM}</article-title>. <string-name><surname>Baldassano</surname> <given-names>C</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zadbood</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Hasson</surname> <given-names>U</given-names></string-name>, <source>Norman KA</source>. 2017. Discovering Event Structure in Continuous Narrative Perception and Memory. Neuron. <volume>95</volume>:<fpage>709</fpage>–<lpage>721</lpage> e705.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Bao</surname> <given-names>P</given-names></string-name>, <string-name><surname>She</surname> <given-names>L</given-names></string-name>, <string-name><surname>McGill</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tsao</surname> <given-names>DY</given-names></string-name>. <year>2020</year>. <article-title>A map of object space in primate inferotemporal cortex</article-title>. <source>Nature</source>. <volume>583</volume>:<fpage>103</fpage>–<lpage>108</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Barnett</surname> <given-names>L</given-names></string-name>, <string-name><surname>Seth</surname> <given-names>AK</given-names></string-name>. <year>2014</year>. <article-title>The MVGC multivariate Granger causality toolbox: a new approach to Granger-causal inference</article-title>. <source>Journal of Neuroscience Methods</source>. <volume>223</volume>:<fpage>50</fpage>–<lpage>68</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Baroni</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bernardini</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ferraresi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zanchetta</surname> <given-names>E</given-names></string-name>. <year>2009</year>. <article-title>The WaCky wide web: a collection of very large linguistically processed web-crawled corpora</article-title>. <source>Language Resources and Evaluation</source>. <volume>43</volume>:<fpage>209</fpage>–<lpage>226</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Bengio</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lecun</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>G</given-names></string-name>. <year>2021</year>. <article-title>Deep Learning for AI</article-title>. <source>Communications of the ACM</source>. <volume>64</volume>:<fpage>58</fpage>–<lpage>65</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="book"><string-name><surname>Bever</surname> <given-names>TG</given-names></string-name>. <year>1970</year>. <chapter-title>The cognitive basis for linguistic structures</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Hayes</surname> <given-names>JR</given-names></string-name></person-group>, editor. <source>C</source><publisher-loc>ognition and the Development of Language New York</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Bicknell</surname> <given-names>K</given-names></string-name>, <string-name><surname>Elman</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Hare</surname> <given-names>M</given-names></string-name>, <string-name><surname>McRae</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>. <year>2010</year>. <article-title>Effects of event knowledge in processing verbal arguments</article-title>. <source>Journal of Memory and Language</source>. <volume>63</volume>:<fpage>489</fpage>–<lpage>505</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Binz</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schulz</surname> <given-names>E</given-names></string-name>. <year>2023</year>. <article-title>Using cognitive psychology to understand GPT-3</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>120</volume>:<fpage>e2218523120</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="book"><string-name><surname>Bisk</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Holtzman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Thomason</surname> <given-names>J</given-names></string-name>, <string-name><surname>Andreas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bengio</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Chai</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lapata</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lazaridou</surname> <given-names>A</given-names></string-name>, <string-name><surname>May</surname> <given-names>J</given-names></string-name>, <string-name><surname>Nisnevich</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pinto</surname> <given-names>N</given-names></string-name>, <string-name><surname>Turian</surname> <given-names>J</given-names></string-name> editors. <source>Experience Grounds Language</source>; <year>2020</year> November; <publisher-name>Online:Association for Computational Linguistics</publisher-name>. <fpage>8718</fpage>–<lpage>8735 p</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Brown</surname> <given-names>T</given-names></string-name>, <string-name><surname>Mann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ryder</surname> <given-names>N</given-names></string-name>, <string-name><surname>Subbiah</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Dhariwal</surname> <given-names>P</given-names></string-name>, <string-name><surname>Neelakantan</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shyam</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sastry</surname> <given-names>G</given-names></string-name>, <string-name><surname>Askell</surname> <given-names>A</given-names></string-name>. <year>2020</year>. <article-title>Language models are few-shot learners</article-title>. <source>Advances in neural information processing systems</source>. <volume>33</volume>:<fpage>1877</fpage>–<lpage>1901</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Caucheteux</surname> <given-names>C</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>King</surname> <given-names>JR</given-names></string-name>. <year>2022</year>. <article-title>Deep language algorithms predict semantic comprehension from brain activity</article-title>. <source>Scientific Reports</source>. <volume>12</volume>:<fpage>16327</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Caucheteux</surname> <given-names>C</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>King</surname> <given-names>JR</given-names></string-name>. <year>2023</year>. <article-title>Evidence of a predictive coding hierarchy in the human brain listening to speech</article-title>. <source>Nature Human Behaviour</source>. <volume>7</volume>:<fpage>430</fpage>–<lpage>441</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Caucheteux</surname> <given-names>C</given-names></string-name>, <string-name><surname>King</surname> <given-names>JR</given-names></string-name>. <year>2022</year>. <article-title>Brains and algorithms partially converge in natural language processing</article-title>. <source>Communications Biology</source>. <volume>5</volume>:<fpage>134</fpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Choi</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Marslen-Wilson</surname> <given-names>WD</given-names></string-name>, <string-name><surname>Lyu</surname> <given-names>B</given-names></string-name>, <string-name><surname>Randall</surname> <given-names>B</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name>. <year>2021</year>. <article-title>Decoding the Real-Time Neurobiological Properties of Incremental Semantic Interpretation</article-title>. <source>Cereb Cortex</source>. <volume>31</volume>:<fpage>233</fpage>–<lpage>247</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="book"><string-name><surname>de Marneffe</surname> <given-names>M-C</given-names></string-name>, <string-name><surname>MacCartney</surname> <given-names>B</given-names></string-name>, <string-name><surname>Manning</surname> <given-names>CD</given-names></string-name> editors. <source>Generating typed dependency parses from phrase structure parses</source>, <publisher-name>Proceedings of the 5th International Conference on Language Resources and Evaluation</publisher-name>; <year>2006</year> May 22-28, 2006; Genoa, <publisher-loc>Italy</publisher-loc>:<publisher-name>European Language Resources Association</publisher-name>. <fpage>449</fpage>–<lpage>454</lpage> p.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="book"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>M-W</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K</given-names></string-name> editors. <source>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</source>, <publisher-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</publisher-name>; <year>2019</year> June 2-7, 2019; Minneapolis, <publisher-loc>MN, USA</publisher-loc>:<publisher-name>Association for Computational Linguistics</publisher-name>. <fpage>4171</fpage>–<lpage>4186</lpage> p.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Doerig</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sommers</surname> <given-names>RP</given-names></string-name>, <string-name><surname>Seeliger</surname> <given-names>K</given-names></string-name>, <string-name><surname>Richards</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ismael</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lindsay</surname> <given-names>GW</given-names></string-name>, <string-name><surname>Kording</surname> <given-names>KP</given-names></string-name>, <string-name><surname>Konkle</surname> <given-names>T</given-names></string-name>, <string-name><surname>van Gerven</surname> <given-names>MAJ</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Kietzmann</surname> <given-names>TC</given-names></string-name>. <year>2023</year>. <article-title>The neuroconnectionist research programme</article-title>. <source>Nature Reviews Neuroscience</source>. <volume>24</volume>:<fpage>431</fpage>–<lpage>450</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Donhauser</surname> <given-names>PW</given-names></string-name>, <string-name><surname>Baillet</surname> <given-names>S</given-names></string-name>. <year>2019</year>. <article-title>Two Distinct Neural Timescales for Predictive Speech Processing</article-title>. <source>Neuron</source>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Dowty</surname> <given-names>D</given-names></string-name>. <year>1991</year>. <article-title>Thematic Proto-Roles and Argument Selection</article-title>. <source>Language</source>. <volume>67</volume>:<fpage>547</fpage>–<lpage>619</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Duncan</surname> <given-names>J</given-names></string-name>. <year>2010</year>. <article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title>. <source>Trends in Cognitive Sciences</source>. <volume>14</volume>:<fpage>172</fpage>–<lpage>179</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Elman</surname> <given-names>JL</given-names></string-name>. <year>1990</year>. <article-title>Finding Structure in Time</article-title>. <source>Cognitive Science</source>. <volume>14</volume>:<fpage>179</fpage>–<lpage>211</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Elman</surname> <given-names>JL</given-names></string-name>. <year>1993</year>. <article-title>Learning and development in neural networks: the importance of starting small</article-title>. <source>Cognition</source>. <volume>48</volume>:<fpage>71</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Everaert</surname> <given-names>MBH</given-names></string-name>, <string-name><surname>Huybregts</surname> <given-names>MAC</given-names></string-name>, <string-name><surname>Chomsky</surname> <given-names>N</given-names></string-name>, <string-name><surname>Berwick</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Bolhuis</surname> <given-names>JJ</given-names></string-name>. <year>2015</year>. <article-title>Structures, Not Strings: Linguistics as Part of the Cognitive Sciences</article-title>. <source>Trends in Cognitive Sciences</source>. <volume>19</volume>:<fpage>729</fpage>–<lpage>743</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Frazier</surname> <given-names>L</given-names></string-name>. <year>1987</year>. <article-title>Syntactic processing: evidence from Dutch</article-title>. <source>Natural Language &amp; Linguistic Theory</source>. <volume>5</volume>:<fpage>519</fpage>–<lpage>559</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Frazier</surname> <given-names>L</given-names></string-name>, <string-name><surname>Rayner</surname> <given-names>K</given-names></string-name>. <year>1982</year>. <article-title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences</article-title>. <source>Cognitive Psychology</source>. <volume>14</volume>:<fpage>178</fpage>–<lpage>210</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Friederici</surname> <given-names>AD</given-names></string-name>. <year>2012</year>. <article-title>The cortical language circuit: from auditory perception to sentence comprehension</article-title>. <source>Trends in Cognitive Sciences</source>. <volume>16</volume>:<fpage>262</fpage>–<lpage>268</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Friederici</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Bahlmann</surname> <given-names>J</given-names></string-name>, <string-name><surname>Heim</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schubotz</surname> <given-names>RI</given-names></string-name>, <string-name><surname>Anwander</surname> <given-names>A</given-names></string-name>. <year>2006</year>. <article-title>The brain differentiates human and non-human grammars: functional localization and structural connectivity</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>103</volume>:<fpage>2458</fpage>–<lpage>2463</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Giordano</surname> <given-names>BL</given-names></string-name>, <string-name><surname>Esposito</surname> <given-names>M</given-names></string-name>, <string-name><surname>Valente</surname> <given-names>G</given-names></string-name>, <string-name><surname>Formisano</surname> <given-names>E</given-names></string-name>. <year>2023</year>. <article-title>Intermediate acoustic-to-semantic representations link behavioral and neural responses to natural sounds</article-title>. <source>Nature Neuroscience</source>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Goldstein</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zada</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Buchnik</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schain</surname> <given-names>M</given-names></string-name>, <string-name><surname>Price</surname> <given-names>A</given-names></string-name>, <string-name><surname>Aubrey</surname> <given-names>B</given-names></string-name>, <string-name><surname>Nastase</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Feder</surname> <given-names>A</given-names></string-name>, <string-name><surname>Emanuel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jansen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gazula</surname> <given-names>H</given-names></string-name>, <string-name><surname>Choe</surname> <given-names>G</given-names></string-name>, <string-name><surname>Rao</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>C</given-names></string-name>, <string-name><surname>Casto</surname> <given-names>C</given-names></string-name>, <string-name><surname>Fanda</surname> <given-names>L</given-names></string-name>, <string-name><surname>Doyle</surname> <given-names>W</given-names></string-name>, <string-name><surname>Friedman</surname> <given-names>D</given-names></string-name>, <string-name><surname>Dugan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Melloni</surname> <given-names>L</given-names></string-name>, <string-name><surname>Reichart</surname> <given-names>R</given-names></string-name>, <string-name><surname>Devore</surname> <given-names>S</given-names></string-name>, <string-name><surname>Flinker</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hasenfratz</surname> <given-names>L</given-names></string-name>, <string-name><surname>Levy</surname> <given-names>O</given-names></string-name>, <string-name><surname>Hassidim</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brenner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Matias</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Norman</surname> <given-names>KA</given-names></string-name>, <string-name><surname>Devinsky</surname> <given-names>O</given-names></string-name>, <string-name><surname>Hasson</surname> <given-names>U</given-names></string-name>. <year>2022</year>. <article-title>Shared computational principles for language processing in humans and deep language models</article-title>. <source>Nature Neuroscience</source>. <volume>25</volume>:<fpage>369</fpage>–<lpage>380</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Guggenmos</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sterzer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cichy</surname> <given-names>RM</given-names></string-name>. <year>2018</year>. <article-title>Multivariate pattern analysis for MEG: A comparison of dissimilarity measures</article-title>. <source>Neuroimage</source>. <volume>173</volume>:<fpage>434</fpage>–<lpage>447</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Hamalainen</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Ilmoniemi</surname> <given-names>RJ</given-names></string-name>. <year>1994</year>. <article-title>Interpreting magnetic fields of the brain: minimum norm estimates</article-title>. <source>Medical &amp; Biological Engineering &amp; Computing</source>. <volume>32</volume>:<fpage>35</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Heilbron</surname> <given-names>M</given-names></string-name>, <string-name><surname>Armeni</surname> <given-names>K</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Hagoort</surname> <given-names>P</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>FP</given-names></string-name>. <year>2022</year>. <article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>119</volume>:<fpage>e2201968119</fpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Henson</surname> <given-names>RN</given-names></string-name>, <string-name><surname>Mouchlianitis</surname> <given-names>E</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name>. <year>2009</year>. <article-title>MEG and EEG data fusion: simultaneous localisation of face-evoked responses</article-title>. <source>Neuroimage</source>. <volume>47</volume>:<fpage>581</fpage>–<lpage>589</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="book"><string-name><surname>Hewitt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Liang</surname> <given-names>P</given-names></string-name> editors. <chapter-title>Designing and interpreting probes with control tasks</chapter-title>, Proceedings of the <year>2019</year> <source>Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing; 2019 November 3-7, 2019</source>; <publisher-loc>Hong Kong, China</publisher-loc>:<publisher-name>Association for Computational Linguistics</publisher-name>. <fpage>2733</fpage>–<lpage>2743</lpage> p.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="book"><string-name><surname>Hewitt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Manning</surname> <given-names>CD</given-names></string-name> editors. <source>A structural probe for finding syntax in word representations</source>, <publisher-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</publisher-name>; 2019 June 2-7, <year>2019</year>; Minneapolis, <publisher-loc>MN, USA</publisher-loc>:<publisher-name>Association for Computational Linguistics</publisher-name>. <fpage>4129</fpage>–<lpage>4138</lpage> p.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Humphreys</surname> <given-names>GF</given-names></string-name>, <string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Simons</surname> <given-names>JS</given-names></string-name>. <year>2021</year>. <article-title>A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition</article-title>. <source>Trends in Neurosciences</source>. <volume>44</volume>:<fpage>452</fpage>–<lpage>463</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="book"><string-name><surname>Jackendoff</surname> <given-names>R</given-names></string-name>, <string-name><surname>Jackendoff</surname> <given-names>RS</given-names></string-name>. <year>2002</year>. <source>Foundations of language: Brain, meaning, grammar, evolution</source>: <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>USA</publisher-loc>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Jung-Beeman</surname> <given-names>M</given-names></string-name>. <year>2005</year>. <article-title>Bilateral brain processes for comprehending natural language</article-title>. <source>Trends in Cognitive Sciences</source>. <volume>9</volume>:<fpage>512</fpage>–<lpage>518</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="book"><string-name><surname>Jurayj</surname> <given-names>W</given-names></string-name>, <string-name><surname>Rudman</surname> <given-names>W</given-names></string-name>, <string-name><surname>Eickhof</surname> <given-names>C</given-names></string-name> editors. <source>Garden Path Traversal in GPT-2</source>, <publisher-name>Proceedings of the 5th BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</publisher-name>; <year>2022</year> December 8, 2022; <publisher-loc>Abu Dhabi, United Arab Emirates</publisher-loc>:<publisher-name>Association for Computational Linguistics</publisher-name>. <fpage>305</fpage>–<lpage>313</lpage> p.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Kietzmann</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Spoerer</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Sorensen</surname> <given-names>LKA</given-names></string-name>, <string-name><surname>Cichy</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Hauk</surname> <given-names>O</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>. <year>2019</year>. <article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>116</volume>:<fpage>21854</fpage>–<lpage>21863</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="book"><string-name><surname>Korhonen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Krymolowski</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Briscoe</surname> <given-names>T</given-names></string-name> editors. <source>A large subcategorization lexicon for natural language processing applications</source>, <publisher-name>Proceedings of International Conference on Language Resources and Evaluation</publisher-name>; <year>2006</year>; Genoa, Italy. <fpage>1015</fpage>–<lpage>1020 p</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Douglas</surname> <given-names>PK</given-names></string-name>. <year>2018</year>. <article-title>Cognitive computational neuroscience</article-title>. <source>Nature Neuroscience</source>. <volume>21</volume>:<fpage>1148</fpage>–<lpage>1160</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Mur</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bandettini</surname> <given-names>P</given-names></string-name>. <year>2008</year>. <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source>Frontiers in Systems Neuroscience</source>. <volume>2</volume>:<fpage>4</fpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Kuperberg</surname> <given-names>GR</given-names></string-name>. <year>2007</year>. <article-title>Neural mechanisms of language comprehension: challenges to syntax</article-title>. <source>Brain Research</source>. <volume>1146</volume>:<fpage>23</fpage>–<lpage>49</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Law</surname> <given-names>R</given-names></string-name>, <string-name><surname>Pylkkanen</surname> <given-names>L</given-names></string-name>. <year>2021</year>. <article-title>Lists with and without Syntax: A New Approach to Measuring the Neural Processing of Syntax</article-title>. <source>Journal of Neuroscience</source>. <volume>41</volume>:<fpage>2186</fpage>–<lpage>2196</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="book"><string-name><surname>Li</surname> <given-names>T</given-names></string-name>, <string-name><surname>Jawale</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Palmer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Srikumar</surname> <given-names>V</given-names></string-name> editors. <source>Structured Tuning for Semantic Role Labeling</source>, <publisher-name>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</publisher-name>; <year>2020</year>; Online.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Linzen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Baroni</surname> <given-names>M</given-names></string-name>. <year>2021</year>. <article-title>Syntactic Structure from Deep Learning</article-title>. <source>Annual Review of Linguistics</source>. <volume>7</volume>:<fpage>195</fpage>–<lpage>212</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Lyu</surname> <given-names>B</given-names></string-name>, <string-name><surname>Choi</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Marslen-Wilson</surname> <given-names>WD</given-names></string-name>, <string-name><surname>Clarke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Randall</surname> <given-names>B</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name>. <year>2019</year>. <article-title>Neural dynamics of semantic composition</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>116</volume>:<fpage>21318</fpage>–<lpage>21327</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>MacDonald</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Pearlmutter</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Seidenberg</surname> <given-names>MS</given-names></string-name>. <year>1994</year>. <article-title>The lexical nature of syntactic ambiguity resolution</article-title>. <source>Psychological Review</source>. <volume>101</volume>:<fpage>676</fpage>–<lpage>703</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Mahowald</surname> <given-names>K</given-names></string-name>, <string-name><surname>Diachek</surname> <given-names>E</given-names></string-name>, <string-name><surname>Gibson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name>, <string-name><surname>Futrell</surname> <given-names>R</given-names></string-name>. <year>2023</year>. <article-title>Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages</article-title>. <source>Cognition</source>. <volume>241</volume>:<fpage>105543</fpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Manning</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hewitt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Khandelwal</surname> <given-names>U</given-names></string-name>, <string-name><surname>Levy</surname> <given-names>O</given-names></string-name>. <year>2020</year>. <article-title>Emergent linguistic structure in artificial neural networks trained by self-supervision</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>117</volume>:<fpage>30046</fpage>–<lpage>30054</lpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Marcus</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Santorini</surname> <given-names>B</given-names></string-name>, <string-name><surname>Marcinkiewicz</surname> <given-names>MA</given-names></string-name>. <year>1993</year>. <article-title>Building a Large Annotated Corpus of English: The Penn Treebank</article-title>. <source>Computational Linguistics</source>. <volume>19</volume>:<fpage>313</fpage>–<lpage>330</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Marslen-Wilson</surname> <given-names>W</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name>. <year>1980</year>. <article-title>The temporal structure of spoken language understanding</article-title>. <source>Cognition</source>. <volume>8</volume>:<fpage>1</fpage>–<lpage>71</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Marslen-Wilson</surname> <given-names>WD</given-names></string-name>. <year>1975</year>. <article-title>Sentence perception as an interactive parallel process</article-title>. <source>Science</source>. <volume>189</volume>:<fpage>226</fpage>–<lpage>228</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Marslen-Wilson</surname> <given-names>WD</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Koster</surname> <given-names>C</given-names></string-name>. <year>1993</year>. <article-title>Integrative Processes in Utterance Resolution</article-title>. <source>Journal of Memory and Language</source>. <volume>32</volume>:<fpage>647</fpage>–<lpage>666</lpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Matchin</surname> <given-names>W</given-names></string-name>, <string-name><surname>Hickok</surname> <given-names>G</given-names></string-name>. 2020. <article-title>The cortical organization of syntax</article-title>. <source>Cereb Cortex</source>. <volume>30</volume>:<fpage>1481</fpage>–<lpage>1498</lpage>. <string-name><surname>McClelland</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Hill</surname> <given-names>F</given-names></string-name>, <string-name><surname>Rudolph</surname> <given-names>M</given-names></string-name>, <string-name><surname>Baldridge</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schutze</surname> <given-names>H</given-names></string-name>. <year>2020</year>. Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Proceedings of the National Academy of Sciences of the United States of America. 117:25966-25974.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>McRae</surname> <given-names>K</given-names></string-name>, <string-name><surname>Matsuki</surname> <given-names>K</given-names></string-name>. <year>2013</year>. <article-title>Constraint-based models of sentence processing</article-title>. <source>Sentence processing</source>. <volume>519</volume>:<fpage>51</fpage>–<lpage>77</lpage>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><surname>Metusalem</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Urbach</surname> <given-names>TP</given-names></string-name>, <string-name><surname>Elman</surname> <given-names>JL</given-names></string-name>. <year>2016</year>. <article-title>Hemispheric asymmetry in event knowledge activation during incremental language comprehension: A visual half-field ERP study</article-title>. <source>Neuropsychologia</source>. <volume>84</volume>:<fpage>252</fpage>–<lpage>271</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Metusalem</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Urbach</surname> <given-names>TP</given-names></string-name>, <string-name><surname>Hare</surname> <given-names>M</given-names></string-name>, <string-name><surname>McRae</surname> <given-names>K</given-names></string-name>, <string-name><surname>Elman</surname> <given-names>JL</given-names></string-name>. <year>2012</year>. <article-title>Generalized event knowledge activation during online sentence comprehension</article-title>. <source>Journal of Memory and Language</source>. <volume>66</volume>:<fpage>545</fpage>–<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><surname>Mosher</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Leahy</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Lewis</surname> <given-names>PS</given-names></string-name>. <year>1999</year>. <article-title>EEG and MEG: forward solutions for inverse methods</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <volume>46</volume>:<fpage>245</fpage>–<lpage>259</lpage>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="other"><string-name><surname>Mrini</surname> <given-names>K</given-names></string-name>, <string-name><surname>Dernoncourt</surname> <given-names>F</given-names></string-name>, <string-name><surname>Tran</surname> <given-names>QH</given-names></string-name>, <string-name><surname>Bui</surname> <given-names>T</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>W</given-names></string-name>, Nakashole N editors. Rethinking Self-Attention: Towards Interpretability in Neural Parsing; <year>2020</year>:<source>Association for Computational Linguistics. 731-742 p</source>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Nelson</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>El Karoui</surname> <given-names>I</given-names></string-name>, <string-name><surname>Giber</surname> <given-names>K</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>LD</given-names></string-name>, <string-name><surname>Koopman</surname> <given-names>HJ</given-names></string-name>, <string-name><surname>Cash</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Naccache</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hale</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Pallier</surname> <given-names>C</given-names></string-name>, <string-name><surname>Dehaene</surname> <given-names>S</given-names></string-name>. <year>2017</year>. <article-title>Neurophysiological dynamics of phrase-structure building during sentence processing</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>114</volume>:<fpage>E3669</fpage>–<lpage>E3678</lpage>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><surname>Ouyang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jiang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Almeida</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wainwright</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Mishkin</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Agarwal</surname> <given-names>S</given-names></string-name>, <string-name><surname>Slama</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ray</surname> <given-names>A</given-names></string-name>. <year>2022</year>. <article-title>Training language models to follow instructions with human feedback</article-title>. <source>Advances in neural information processing systems</source>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><surname>Pallier</surname> <given-names>C</given-names></string-name>, <string-name><surname>Devauchelle</surname> <given-names>A-D</given-names></string-name>, <string-name><surname>Dehaene</surname> <given-names>S</given-names></string-name>. <year>2011</year>. <article-title>Cortical representation of the constituent structure of sentences</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>108</volume>:<fpage>2522</fpage>–<lpage>2527</lpage>.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Pavlick</surname> <given-names>E</given-names></string-name>. <year>2022</year>. <article-title>Semantic Structure in Deep Learning</article-title>. <source>Annual Review of Linguistics</source>. <volume>8</volume>:<fpage>447</fpage>–<lpage>471</lpage>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Rabovsky</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hansen</surname> <given-names>SS</given-names></string-name>, <string-name><surname>McClelland</surname> <given-names>JL</given-names></string-name>. <year>2018</year>. <article-title>Modelling the N400 brain potential as change in a probabilistic representation of meaning</article-title>. <source>Nature Human Behaviour</source>. <volume>2</volume>:<fpage>693</fpage>–<lpage>705</lpage>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><surname>Sarvas</surname> <given-names>J</given-names></string-name>. <year>1987</year>. <article-title>Basic mathematical and electromagnetic concepts of the biomagnetic inverse problem</article-title>. <source>Physics in Medicine &amp; Biology</source>. <volume>32</volume>:<fpage>11</fpage>–<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><surname>Schrimpf</surname> <given-names>M</given-names></string-name>, <string-name><surname>Blank</surname> <given-names>IA</given-names></string-name>, <string-name><surname>Tuckute</surname> <given-names>G</given-names></string-name>, <string-name><surname>Kauf</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hosseini</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name>, <string-name><surname>Tenenbaum</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name>. <year>2021</year>. <article-title>The neural architecture of language: Integrative modeling converges on predictive processing</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <volume>118</volume>:<fpage>e2105646118</fpage>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><surname>Sheahan</surname> <given-names>H</given-names></string-name>, <string-name><surname>Luyckx</surname> <given-names>F</given-names></string-name>, <string-name><surname>Nelli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Teupe</surname> <given-names>C</given-names></string-name>, <string-name><surname>Summerfield</surname> <given-names>C</given-names></string-name>. <year>2021</year>. <article-title>Neural state space alignment for magnitude generalization in humans and recurrent networks</article-title>. <source>Neuron</source>. <volume>109</volume>:<fpage>1214</fpage>–<lpage>1226</lpage> e1218.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><string-name><surname>Smallwood</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bernhardt</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Leech</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bzdok</surname> <given-names>D</given-names></string-name>, <string-name><surname>Jefferies</surname> <given-names>E</given-names></string-name>, <string-name><surname>Margulies</surname> <given-names>DS</given-names></string-name>. <year>2021</year>. <article-title>The default mode network in cognition: a topographical perspective</article-title>. <source>Nature Reviews Neuroscience</source>. <volume>22</volume>:<fpage>503</fpage>–<lpage>513</lpage>.</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><string-name><surname>Taulu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Simola</surname> <given-names>J</given-names></string-name>. <year>2006</year>. <article-title>Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</article-title>. <source>Physics in Medicine &amp; Biology</source>. <volume>51</volume>:<fpage>1759</fpage>–<lpage>1768</lpage>.</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="book"><string-name><surname>Tenney</surname> <given-names>I</given-names></string-name>, <string-name><surname>Xia</surname> <given-names>P</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>A</given-names></string-name>, <string-name><surname>Poliak</surname> <given-names>A</given-names></string-name>, <string-name><surname>McCoy</surname> <given-names>RT</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>N</given-names></string-name>, <string-name><surname>Van Durme</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bowman</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Das</surname> <given-names>D</given-names></string-name> editors. <source>What do you learn from context? probing for sentence structure in contextualized word representations</source>, t<publisher-name>he 7th International Conference on Learning Representations</publisher-name>; <year>2019</year> May 6-9, 2019; New Orleans, LA, USA.</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><string-name><surname>Toneva</surname> <given-names>M</given-names></string-name>, <string-name><surname>Mitchell</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Wehbe</surname> <given-names>L</given-names></string-name>. <year>2022</year>. <article-title>Combining computational controls with natural text reveals aspects of meaning composition</article-title>. <source>Nature Computational Science</source>. <volume>2</volume>:<fpage>745</fpage>–<lpage>757</lpage>.</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><string-name><surname>Troyer</surname> <given-names>M</given-names></string-name>, <string-name><surname>McRae</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>. <year>2022</year>. <article-title>Wrong or right? Brain potentials reveal hemispheric asymmetries to semantic relations during word-by-word sentence reading as a function of (fictional) knowledge</article-title>. <source>Neuropsychologia</source>. <volume>170</volume>:<fpage>108215</fpage>.</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="book"><string-name><surname>Trueswell</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Tanenhaus</surname> <given-names>MK</given-names></string-name>. <year>1994</year>. <source>Toward a lexicalist framework of constraint-based syntactic ambiguity resolution. In. Perspectives on sentence processing Hillsdale</source>, <publisher-loc>NJ, US</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates</publisher-name>, Inc p 155-179.</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Marslen-Wilson</surname> <given-names>WD</given-names></string-name>. <year>1977</year>. <article-title>The On-Line Effects of Semantic Context on Syntactic Processing</article-title>. <source>Journal of Verbal Learning and Verbal Behavior</source>. <volume>16</volume>:<fpage>683</fpage>–<lpage>692</lpage>.</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><string-name><surname>Vaswani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname> <given-names>N</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gomez</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Kaiser</surname> <given-names>Ł</given-names></string-name>, <string-name><surname>Polosukhin</surname> <given-names>I</given-names></string-name>. <year>2017</year>. <article-title>Attention is all you need</article-title>. <source>Advances in neural information processing systems</source>. <volume>30</volume>.</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><string-name><surname>Winkler</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Ridgway</surname> <given-names>GR</given-names></string-name>, <string-name><surname>Douaud</surname> <given-names>G</given-names></string-name>, <string-name><surname>Nichols</surname> <given-names>TE</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SM</given-names></string-name>. <year>2016</year>. <article-title>Faster permutation inference in brain imaging</article-title>. <source>Neuroimage</source>. <volume>141</volume>:<fpage>502</fpage>–<lpage>516</lpage>.</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><string-name><surname>Wolf</surname> <given-names>T</given-names></string-name>, <string-name><surname>Debut</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sanh</surname> <given-names>V</given-names></string-name>, <string-name><surname>Chaumond</surname> <given-names>J</given-names></string-name>, <string-name><surname>Delangue</surname> <given-names>C</given-names></string-name>, <string-name><surname>Moi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cistac</surname> <given-names>P</given-names></string-name>, <string-name><surname>Rault</surname> <given-names>T</given-names></string-name>, <string-name><given-names>Louf</given-names> <surname>Re</surname></string-name>, <string-name><surname>Funtowicz</surname> <given-names>M</given-names></string-name>, <string-name><surname>Brew</surname> <given-names>J</given-names></string-name>. <year>2019</year>. <article-title>HuggingFace’s Transformers: State-of-the-art Natural Language Processing</article-title>. <source>ArXiv. abs/</source><volume>1910</volume>.<fpage>03771</fpage>.</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name>. <year>2016</year>. <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature Neuroscience</source>. <volume>19</volume>:<fpage>356</fpage>–<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>GR</given-names></string-name>, <string-name><surname>Joglekar</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Song</surname> <given-names>HF</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>WT</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>. <year>2019</year>. <article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title>. <source>Nature Neuroscience</source>. <volume>22</volume>:<fpage>297</fpage>–<lpage>306</lpage>.</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><string-name><surname>Yeshurun</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hasson</surname> <given-names>U</given-names></string-name>. <year>2021</year>. <article-title>The default mode network: where the idiosyncratic self meets the shared social world</article-title>. <source>Nature Reviews Neuroscience</source>. <volume>22</volume>:<fpage>181</fpage>–<lpage>192</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89311.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
<city>Hangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study provides insights into how the brain parses the syntactic structure of a spoken sentence. <bold>Convincing</bold> evidence is provided that distributive cortical networks are engaged for incremental parsing of a sentence, and neural activity recorded by MEG correlates with sentence structure measures extracted by a deep neural network language model, i.e., BERT. A contribution of the work is to use a deep neural network model to quantify how the mental representation of syntactic structure updates as a sentence unfolds in time.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89311.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This article is focused on investigating incremental speech processing, as it pertains to building higher order syntactic structure. This is an important question because speech processing in general is lesser studied as compared to reading, and syntactic processes are lesser studied than lower-level sensory processes. The authors claim to shed light on the neural processes that build structured linguistic interpretations. The authors apply modern analysis techniques, and use state-of-the-art large language models in order to facilitate this investigation. They apply this to a cleverly designed experimental paradigm of EMEG data, and compare neural responses of human participants to the activation profiles in different layers of the BERT language model.</p>
<p>Comments on revised version:</p>
<p>Similar to my original review, I find the paper hard to follow, and it is not clear to me that the use of the LLM is adding much to the findings. Using complex language models without substantial motivation dampens my enthusiasm significantly. This concern has not been alleviated since my original review.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89311.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Syntactic parsing is a highly dynamic process: When an incoming word is inconsistent with the presumed syntactic structure, the brain has to reanalyze the sentence and construct an alternative syntactic structure. Since syntactic parsing is a hidden process, it is challenging to describe the syntactic structure a listener internally constructs at each time moment. Here, the authors overcome this problem by (1) asking listeners to complete a sentence at some break point to probe the syntactic structure mentally constructed at the break point, and (2) using a DNN model to extract the most likely structure a listener may extract at a time moment.</p>
<p>After obtaining incremental syntactic features using a DNN model, i.e., BERT, the authors analyze how these syntactic features are represented in the brain using MEG. The advantage of the approach is that BERT can potentially integrate syntactic and semantic knowledge and is a computational model, instead of a static theoretical construct, that may more precisely reflect incremental sentence processing in the human brain. The results indeed confirm the similarity between MEG activity and measures from the BERT model.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89311.2.sa3</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Lyu</surname>
<given-names>Bingjiang</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8554-5138</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Marslen-Wilson</surname>
<given-names>William D.</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fang</surname>
<given-names>Yuxing</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tyler</surname>
<given-names>Lorraine K.</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>eLife assessment</bold></p>
<p>This study provides valuable insights into how the brain parses the syntactic structure of a spoken sentence. A unique contribution of the work is to use a large language model to quantify how the mental representation of syntactic structure updates as a sentence unfolds in time. Solid evidence is provided that distributive cortical networks are engaged for incremental parsing of a sentence, although the contribution could be further strengthened if the authors would further highlight the main results and clarify the benefit of using a large language model.</p>
</disp-quote>
<p>We thank the editors for the overall positive assessment. We have revised our manuscript to further emphasize our main findings and highlight the advantages of using a large language model (LLM) over traditional behavioural and corpus-based data.</p>
<p>This study aims to investigate the neural dynamics underlying the incremental construction of structured interpretation during speech comprehension. While syntactic cues play an important role, they alone do not define the essence of this parsing process. Instead, this incremental process is jointly determined by the interplay of syntax, semantics, and non-linguistic world knowledge, evoked by the specific words heard sequentially by listeners. To better capture these multifaceted constraints, we derived structural measures from BERT, which dynamically represent the evolving structured interpretation as a sentence unfolds word-by-word.</p>
<p>Typically, the syntactic structure of a sentence can be represented by a context-free parse tree, such as a dependency parse tree or a constituency-based parse tree, which abstracts away from specific content, assigning a discrete parse depth to each word regardless of its semantics. However, this context-free parse tree merely represents the result rather than the process of sentence parsing and does not elucidate how a coherent structured interpretation is concurrently determined by multifaceted constraints. In contrast, BERT parse depth, trained to approach the context-free discrete dependency parse depth, is a continuous variable. Crucially, its deviation from the corresponding discrete parse depth indicates the preference for the syntactic structure represented by this context-free parse. As BERT processes a sentence delivered word-by-word, the dynamic change of BERT parse depth reflects the incremental nature of online speech comprehension.</p>
<p>Our results reveal a behavioural alignment between BERT parse depth and human interpretative preference for the same set of sentences. In other words, BERT parse depth could represent a probabilistic interpretation of a sentence’s structure based on its specific contents, making it possible to quantify the preference for each grammatically correct syntactic structure during incremental speech comprehension. Furthermore, both BERT and human interpretations show correlations with linguistic knowledge, such as verb transitivity, and non-linguistic knowledge, like subject noun thematic role preference. Both types of knowledge are essential for achieving a coherent interpretation, in accordance with the “constraint-based hypothesis” of sentence processing.</p>
<p>Motivated by the observed behavioural alignment between BERT and human listeners, we further investigated BERT structural measures in source-localized EEG/MEG using representational similarity analyses (RSA). This approach revealed the neural dynamics underlying incremental speech comprehension on millisecond scales. Our main findings include: (1) a shift from bi-hemispheric lateral frontal-temporal regions to left-lateralized regions in representing the current structured interpretation as a sentence unfolds, (2) a pattern of sequential activations in the left lateral temporal regions, updating the structured interpretation as syntactic ambiguity is resolved, and (3) the influence of lexical interpretative coherence activated in the right hemisphere over the resolved sentence structure represented in the left hemisphere.</p>
<p>From our perspective, the advantages of using a LLM (or deep language model) like BERT are twofold. Conceptually, BERT structural measures offer a deep contextualized structural representation for any given sentence by integrating the multifaceted constraints unique to the specific contents described by the words within that sentence. Modelling this process on a word-by-word basis is challenging to achieve with behavioural or corpus-based metrics. Empirically, as demonstrated in our responses to the reviewers below, BERT measures show better performance compared to behavioural and corpus-based metrics in aligning with listeners’ neural activity. Moreover, when it comes to integrating multiple sources of constraints for achieving a coherent interpretation, BERT measures also show a better fit with the behavioural data of human listeners than corpus-based metrics.</p>
<p>Taken together, we propose that LLMs, akin to other artificial neural networks (ANNs), can be considered as computational models for formulating and testing specific neuroscientific hypotheses, such as the “constraint-based hypothesis” of sentence processing in this study. However, we by no means overlook the importance of corpus-based and behavioural metrics. These metrics play a crucial role in interpreting and assessing whether and how ANNs stimulate human cognitive processes, a fundamental step in employing ANNs to gain new insights into the neural mechanisms of human cognition.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>In this study, the authors investigate where and when brain activity is modulated by incoming linguistic cues during sentence comprehension. Sentence stimuli were designed such that incoming words had varying degrees of constraint on the sentence's structural interpretation as participants listened to them unfolding, i.e. due to varying degrees of verb transitivity and the noun's likelihood of assuming a specific thematic role. Word-by-word &quot;online&quot; structural interpretations for each sentence were extracted from a deep neural network model trained to reproduce language statistics. The authors relate the various metrics of word-by-word predicted sentence structure to brain data through a standard RSA approach at three distinct points of time throughout sentence presentation. The data provide convincing evidence that brain activity reflects preceding linguistic constraints as well as integration difficulty immediately after word onset of disambiguating material.</p>
</disp-quote>
<p>We thank Reviewer #1 (hereinafter referred to as R1) for their recognition of the objectives of our study and the analytical approaches we have employed in this study.</p>
<disp-quote content-type="editor-comment">
<p>The authors confirm that their sentence stimuli vary in degree of constraint on sentence structure through independent behavioral data from a sentence continuation task. They also show a compelling correlation of these behavioral data with the online structure metric extracted from the deep neural network, which seems to pick up on the variation in constraints. In the introduction, the authors argue for the potential benefits of using deep neural networkderived metrics given that it has &quot;historically been challenging to model the dynamic interplay between various types of linguistic and nonlinguistic information&quot;. Similarly, they later conclude that &quot;future DLMs (...) may provide new insights into the neural implementation of the various incremental processing operations(...)&quot;.</p>
</disp-quote>
<p>We appreciate R1’s positive comments on the design, quantitative modelling and behavioural validation of the sentence stimuli used in this experiment.</p>
<disp-quote content-type="editor-comment">
<p>By incorporating structural probing of a deep neural network, a technique developed in the field of natural language processing, into the analysis pipeline for investigating brain data, the authors indeed take an important step towards establishing advanced machine learning techniques for researching the neurobiology of language. However, given the popularity of deep neural networks, an argument for their utility should be carefully evidenced.</p>
</disp-quote>
<p>We fully concur with R1 regarding the need for cautious evaluation and interpretation of deep neural networks’ utility. In fact, this perspective underpinned our decision to conduct extensive correlation analyses using both behavioural and corpus-based metrics to make sense of BERT metrics. These analyses were essential to interpret and validate BERT metrics before employing them to investigate listeners’ neural activity during speech comprehension. We do not in any way undermine the importance of behavioural or corpus-based data in studying language processing in the brain. On the contrary, as evidenced by our findings, these traditional metrics are instrumental in interpreting and guiding the use of metrics derived from LLMs.</p>
<disp-quote content-type="editor-comment">
<p>However, the data presented here don't directly test how large the benefit provided by this tool really is. In fact, the authors show compelling correlations of the neural network-derived metrics with both the behavioral cloze-test data as well as several (corpus-)derived metrics. While this is a convincing illustration of how deep language models can be made more interpretable, it is in itself not novel. The correlation with behavioral data and corpus statistics also raises the question of what is the additional benefit of the computational model? Is it simply saving us the step of not having to collect the behavioral data, not having to compute the corpus statistics or does the model potentially uncover a more nuanced representation of the online comprehension process? This remains unclear because we are lacking a direct comparison of how much variance in the neural data is explained by the neural network-derived metrics beyond those other metrics (for example the main verb probability or the corpusderived &quot;active index&quot; following the prepositional phrase).</p>
</disp-quote>
<p>From our perspective, a primary advantage of using the neural network-derived metrics (or LLMs as computational models of language processing), compared to traditional behavioural and corpus-based metrics, lies in their ability to offer more nuanced, contextualized representations of natural language inputs. There seems no effective way of computationally capturing the distributed and multifaceted constraints within specific contexts until the current generation of LLMs came along. While it is feasible to quantify lexical properties or contextual effects based on the usage of specific words via corpora or behavioural tests, this method appears less effective in modelling the composition of meanings across more words on the sentence level. More critically, it struggles with capturing how various lexical constraints collectively yield a coherent structured interpretation.</p>
<p>Accumulating evidence suggests that models designed for context prediction or next-word prediction, such as word2vec and LLMs, outperform classic count-based distributional semantic models (Baroni et al. 2014) in aligning with neural activity during language comprehension (Schrimpf et al. 2021; Caucheteux and King 2022). Relevant to this, we have conducted additional analyses to directly assess the additional variance of neural data explained by BERT metrics, over and above what traditional metrics account for. Specifically, using RSA, we re-tested model RDMs based on BERT metrics while controlling for the contribution from traditional metrics (via partial correlation).</p>
<p>During the first verb (V1) epoch, we tested model RDMs of V1 transitivity based on data from either the behavioural pre-test (i.e., continuations following V1) or massive corpora. Contrasting sharply with the significant model fits observed for BERT V1 parse depth in bilateral frontal and temporal regions, the two metrics of V1 transitivity did not exhibit any significant effects (see Author response image 1).</p>
<p>Author response image 1</p>
<p>RSA model fits of BERT structural metrics and behavioural/corpus-based metrics in the V1 epoch. (upper) Model fits of BERT V1 parse depth (relevant to Appendix 1-figure 10A); (middle) Model fits of the V1 transitivity based on the continuation pre-rest conducted at the end of V1 (e.g., completing “The dog found …”); (bottom) Model fits of the V1 transitivity based on the corpus data (as described in Methods). Note that verb transitivity is quantified as the proportion of its transitive uses (i.e., followed by a direct object) relative to its intransitive uses.</p>
<disp-formula id="sa3equ1">
<graphic mime-subtype="jpg" xlink:href="elife-89311-sa3-equ1.jpg" mimetype="image"/>
</disp-formula>
<p>In the PP1 epoch, which was aligned to the onset of the preposition in the prepositional phrase (PP), we tested the probability of a PP continuation following V1 (e.g., the probability of a PP after “The dog found…”). While no significant results were found for PP probability, we have plotted the uncorrected results for PP probability (Author response image 2). These model fits have very limited overlap with those of BERT parse depth vector (up to PP1) in the left inferior frontal gyrus (approximately at 360 ms) and the left temporal regions (around 600 ms). It is noteworthy that the model fits of the BERT parse depth vector (up to PP1) remained largely unchanged even when PP probability was controlled for, indicating that the variance explained by BERT metrics cannot be effectively accounted for by the PP probability obtained from the human continuation pre-test.</p>
<p>Author response image 2</p>
<p>Comparison between the RSA model fits of BERT structural metrics and behavioural / corpusbased metrics in the PP1 epoch. (upper) Model fits of BERT parse depth vector up to PP1 (relevant to Figure 6B in the main text); (middle) Model fits of the probability of a PP continuation in the prerest conducted at the end of the first verb; (bottom) Model fits of BERT parse depth vector up to PP1 after partialling out the variance explained by PP probability.</p>
<disp-formula id="sa3equ2">
<graphic mime-subtype="jpg" xlink:href="elife-89311-sa3-equ2.jpg" mimetype="image"/>
</disp-formula>
<p>Finally, in the main verb (MV) epoch, we tested the model RDM based on the probability of a MV continuation following the PP (e.g., the probability after “The dog found in the park…”). When compared with the BERT parse depth vector (up to MV), we observed a similar effect in the left dorsal frontal regions (see Author response image 3). However, this effect did not survive after the whole-brain multiple comparison correction. Subsequent partial correlation analyses revealed that the MV probability accounted for only a small portion of the variance in neural data explained by the BERT metric, primarily the effect observed in the left dorsal frontal regions around 380 ms post MV onset. Meanwhile, the majority of the model fits of the BERT parse depth vector remained largely unchanged after controlling for the MV probability.</p>
<p>Note that the probability of a PP/MV continuation reflect participants’ predictions based on speech input preceding the preposition (e.g., “The dog found…”) or the main verb (e.g., “The dog found in the park…”), respectively. In contrast, BERT parse depth vector is designed to represent the structure of the (partial) sentence in the speech already delivered to listeners, rather than to predict a continuation after it. Therefore, in the PP1 and MV epochs, we separately tested BERT parse depth vectors that included the preposition (e.g., “The dog found in…”) and the main verb (e.g., “The dog found in the park was…”) to accurately capture the sentence structure at these specific points in a sentence. Despite the differences in the nature of information captured by these two types of metrics, the behavioural metrics themselves did not exhibit significant model fits when tested against listeners’ neural activity.</p>
<p>Author response image 3</p>
<p>Comparison between the RSA model fits of BERT structural metrics and behavioural / corpusbased metrics in the MV epoch. (upper) Model fits of BERT parse depth vector up to MV (relevant to Figure 6C in the main text); (middle) Model fits of the probability of a MV continuation in the pre-rest conducted at the end of the prepositional phrase (e.g., “The dog found in the park …”); (bottom) Model fits of BERT parse depth vector up to MV after partialling out the variance explained by MV probability.</p>
<disp-formula id="sa3equ3">
<graphic mime-subtype="jpg" xlink:href="elife-89311-sa3-equ3.jpg" mimetype="image"/>
</disp-formula>
<p>Regarding the corpus-derived interpretative preference, we observed that neither the Active index nor the Passive index showed significant effects in the PP1 epoch. In the MV epoch, while significant model fits of the passive index were observed, which temporally overlapped with the BERT parse depth vector (up to MV) after the recognition point of the MV, the effects of these two model RDMs emerged in different hemispheres, as illustrated in Figures 6C and 8D in the main text. Consequently, we opted not to pursue further partial correlation analysis with the corpus-derived interpretative preference. Besides, as shown in Figure 8A, 8B and 8C, subject noun thematic role preference and non-directional index exhibit significant model fits in the PP1 or the MV epoch. Interesting, these effects lead corresponding effects of BERT metrics in the same epoch (see Figure 6B and 6C), suggesting that the overall structured interpretation emerges after the evaluation and integration of multifaceted lexical constraints.</p>
<p>In summary, our findings indicate that, in comparison to corpus-derived or behavioural metrics, BERT structural metrics are more effective in explaining neural data, in terms of modelling both the unfolding sentence input (i.e., incremental BERT parse vector) and individual words (i.e., V1) within specific sentential contexts. This advantage of BERT metrics might be due to the hypothesized capacity of LLMs to capture more contextually rich representations. Such representations effectively integrate the diverse constraints present in a given sentence, thereby outperforming corpus-based metrics or behavioural metrics in this respect. Concurrently, it is important to recognize the significant role of corpus-based / behavioral metrics as explanatory variables. They are instrumental not only in interpreting BERT metrics but also in understanding their fit to listeners’ neural activity (by examining the temporal sequence and spatial distribution of model fits of these two types of metrics). Such an integrative approach allows for a more comprehensive understanding of the complex neural processes underpinning speech comprehension.</p>
<disp-quote content-type="editor-comment">
<p>With regards to the neural data, the authors show convincing evidence for early modulations of brain activity by linguistic constraints on sentence structure and importantly early modulation by the coherence between multiple constraints to be integrated. Those modulations can be observed across bilateral frontal and temporal areas as well as parts of the default mode network. The methods used are clear and rigorous and allow for a detailed exploration of how multiple linguistic cues are neurally encoded and dynamically shape the final representation of a sentence in the brain. However, at times the consequences of the RSA results remain somewhat vague with regard to the motivation behind different metrics and how they differ from each other. Therefore, some results seem surprising and warrant further discussion, for example: Why does the neural network-derived parse depth metric fit neural data before the V1 uniqueness point if the sentence pairs begin with the same noun phrase? This suggests that the lexical information preceding V1, is driving the results. However, given the additional results, we can already exclude an influence of subject likelihood for a specific thematic role as this did not model the neural data in the V1 epoch to a significant degree.</p>
</disp-quote>
<p>As pointed out by R1, model fits of BERT parse depth vector (up to V1) and its mismatch for the active interpretation were observed before the V1 uniqueness point (Figures 6A and 6D). These early effects could be attributed to the inclusion of different subject nouns in the BERT parse depth vectors. In our MEG data analyses, RSA was performed using all LoTrans and HiTrans sentences. Each of the 60 sentence sets contained one LoTrans sentence and one HiTrans sentence, which resulted in a 120 x 120 neural data RDM for each searchlight ROI across the brain within each sliding time window. Although LoTrans and HiTrans sentences within the same sentence set shared the same subject noun, subject nouns varied across sentence sets. This variation was expected to be reflected in both the model RDM of BERT metrics and the data RDM, a point further clarified in the revised manuscript.</p>
<p>In contrast, when employing a model RDM constructed solely from the BERT V1 parse depth, we observed model fits peaking precisely at the uniqueness point of V1 (see Appendix 1figure 10). It is important to note that BERT V1 parse depth is a contextualized metric influenced by the preceding subject noun, which could account for the effects of BERT V1 parse depth observed before the uniqueness point of V1.</p>
<disp-quote content-type="editor-comment">
<p>Relatedly, In Fig 2C it seems there are systematic differences between HiTrans and LoTrans sentences regarding the parse depth of determiner and subject noun according to the neural network model, while this is not expected according to the context-free parse.</p>
</disp-quote>
<p>We thank R1 for pointing out this issue. Relevant to Figure 3D (Figure 2C in the original manuscript), we presented the distributions of BERT parse depth for individual words as the sentence unfolds in Appendix 1-figure 2. Our analysis revealed that the parse depth of the subject noun in high transitivity (HiTrans) and low transitivity (LoTrans) sentences did not significantly differ, except for the point at which the sentence reached V1 (two-tailed twosample t-test, P = 0.05).</p>
<p>However, we observed a significant difference in the parse depth of the determiner between HiTrans and LoTrans sentences (two-tailed two-sample t-test, P &lt; 0.05 for all results in Appendix 1-figure 2). Additionally, the parse depth of the determiner was found to covary with that of V1 as the input unfolded to different sentence positions (Pearson correlation, P &lt; 0.05 for all plots in Appendix 1-figure 2). This difference, unexpected in terms of the contextfree (dependency) parse used for training the BERT structural probing model, might be indicative of a “leakage” of contextual information during the training of the structural probing model, given the co-variation between the determiner and V1 which was designed to be different in their transitivity in the two types of sentences.</p>
<p>Despite such unexpected differences observed in the BERT parse depths of the determiner, we considered the two sentence types as one group with distributed features (e.g., V1 transitivity) in the RSA, and used the BERT parse depth vector including all words in the sentence input to construct the model RDMs. Moreover, as indicated in Appendix 1-figure 3, compared to the content words, the determiner contributed minimally to the incremental BERT parse depth vector. Consequently, the noted discrepancies in BERT parse depth of the determiner between HiTrans and LoTrans sentences are unlikely to significantly bias our RSA results.</p>
<disp-quote content-type="editor-comment">
<p>&quot;The degree of this mismatch is proportional to the evidence for or against the two interpretations (...). Besides these two measures based on the entire incremental input, we also focused on Verb1 since the potential structural ambiguity lies in whether Verb1 is interpreted as a passive verb or the main verb.&quot; The neural data fits in V1 epoch differ in their temporal profile for the mismatch metrics and the Verb 1 depth respectively. I understand the &quot;degree of mismatch&quot; to be a measure of how strongly the neural network's hidden representations align with the parse depth of an active or passive sentence structure. If this is correct, then it is not clear from the text how far this measure differs from the Verb 1 depth alone, which is also indicating either an active or passive structure.</p>
</disp-quote>
<p>Within the V1 epoch, we tested three distinct types of model RDMs based on BERT metrics: (1) The BERT parse depth vector, representing the neural network’s hidden representation of the incremental sentence structure including all words up to V1. (2) The mismatch metric for either the Active or Passive interpretation, calculated as the distance between the BERT parse depth vector and the context-free parse depth vector for each interpretation. (3) The BERT parse depth of V1, crucial in representing the preferred structural interpretation of the unfolding sentence given its syntactic role as either a passive verb or the main verb.</p>
<p>While the BERT parse depth vector per se does not directly indicate a preferred interpretation, its mismatch with the context-free parse depth vectors of the two possible interpretations reveals the favoured interpretation, as significant neural fit is only anticipated for the mismatch with the interpretation being considered. The contextualized BERT depth of V1 is also indicative of the preferred structure given the context-free V1 parse depth corresponding to different syntactic roles, however, compared to the interpretative mismatch, it does not fully capture contributions from other words in the input. Consequently, we expected the interpretative mismatch and the BERT V1 depth to yield different results. Indeed, our analysis revealed that, although both metrics extracted from the same BERT layer (i.e., layer 13) demonstrated early RSA fits in the left fronto-temporal regions, the V1 depth showed relatively more prolonged effects with a notable peak occurring precisely at the uniqueness point of V1 (compare Figure 6C and Appendix 1-figure 10). These complementary results underscore the capability of BERT metrics to align with neural responses, in terms of both an incrementally unfolding sentence and a specific word within it.</p>
<disp-quote content-type="editor-comment">
<p>In previous studies, differences in neural activity related to distinct amounts of open nodes in the parse tree have been interpreted in terms of distinct working memory demands (Nelson et al. pnas 2017, Udden et al tics 2020). It seems that some of the metrics, for example the neural network-derived parse depth or the V1 depth may be similarly interpreted in the light of working memory demands. After all, during V1 epoch, the sentences do not only differ with respect to predicted sentence structure, but also in the amount of open nodes that need to be maintained. In the discussion, however, the authors interpret these results as &quot;neural representations of an unfolding sentence's structure&quot;.</p>
</disp-quote>
<p>We agree with the reviewer that the Active and Passive interpretations differ in terms of the number of open nodes before the actual main verb is heard. Given the syntactic ambiguity in our sentence stimuli (i.e., LoTrans and Hi Trans sentences), it is infeasible to determine the exact number of open nodes in each sentence as it unfolds. Nevertheless, the RSA fits observed in the dorsal lateral frontal regions could be indicative of the varying working memory demands involved in building the structured interpretations across sentences. We have added this perspective in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>This article is focused on investigating incremental speech processing, as it pertains to building higher-order syntactic structure. This is an important question because speech processing in general is lesser studied as compared to reading, and syntactic processes are lesser studied than lower-level sensory processes. The authors claim to shed light on the neural processes that build structured linguistic interpretations. The authors apply modern analysis techniques, and use state-of-the-art large language models in order to facilitate this investigation. They apply this to a cleverly designed experimental paradigm of EMEG data, and compare neural responses of human participants to the activation profiles in different layers of the BERT language   model.</p>
</disp-quote>
<p>We thank Reviewer #2 (hereinafter referred to as R2) for the overall positive remarks on our study.</p>
<disp-quote content-type="editor-comment">
<p>Strengths:</p>
<p>(1) The study aims to investigate an under-explored aspect of language processing, namely syntactic operations during speech processing</p>
<p>(2) The study is taking advantage of technological advancements in large language models, while also taking linguistic theory into account in building the hypothesis space</p>
<p>(3) The data combine EEG and MEG, which provides a valuable spatio-temporally resolved dataset</p>
<p>(4) The use of behavioural validation of high/low transitive was an elegant demonstration of the validity of their stimuli</p>
</disp-quote>
<p>We thank R2 for recognizing and appreciating the motivation and the methodology employed in this study.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) The manuscript is quite hard to understand, even for someone well-versed in both linguistic theory and LLMs. The questions, design, analysis approach, and conclusions are all quite dense and not easy to follow.</p>
</disp-quote>
<p>To address this issue, we have made dedicated efforts to clarify the key points in our study. We also added figures to visualize our experimental design and methods (see Figure 1, Figure 3C and Figure 5 in the revised main text). We hope that these revisions have made the manuscript more comprehensible and straightforward for the readers.</p>
<disp-quote content-type="editor-comment">
<p>(2) The analyses end up seeming overly complicated when the underlying difference between sentence types is a simple categorical distinction between high and low transitivity. I am not sure why tree depth and BERT are being used to evaluate the degree to which a sentence is being processed as active or passive. If this is necessary, it would be helpful for the authors to motivate this more clearly.</p>
</disp-quote>
<p>Indeed, as pointed by R2, the only difference between LoTrans and HiTrans sentences is the first verb (V1), whose transitivity is crucial for establishing an initial preference for either an Active or a Passive interpretation as the sentence unfolds. Nonetheless, in line with the constraint-based approach to sentence processing and supported by previous research findings, a coherent structured interpretation of a sentence is determined by the combined constraints imposed by all words within that sentence. In our study, the transitivity of V1 alone is insufficient to fully explain the interpretative preference for the sentence structure. The overall sentence-level interpretation also depends on the thematic role preference of the subject noun – its likelihood of being an agent performing an action or a patient receiving the action.</p>
<p>This was evident in our findings, as shown in Author response image 1 above, where the V1 transitivity based on corpus or behavioural data did not fit to the neural data during the V1 epoch. In contrast, BERT structural measures [e.g., BERT parse depth vector (up to V1) and BERT V1 parse depth] offered contextualized representations that are presumed to integrate various lexical constraints present in each sentence. These BERT metrics exhibited significant model fits for the same neural data in the V1 epoch. Besides, a notable feature of BERT is its bi-directional attention mechanism, which allows for the dynamic updating of an earlier word’s representation as more of the sentence is heard, which is also changeling to achieve with corpus or behavioural metrics. For instance, the parse depth of the word “found” in the BERT parse depth vector for “The dog found…” differs from its parse depth in the vector for “The dog found in…”. This feature of BERT is particularly advantageous for investigating the dynamic nature of structured interpretation during speech comprehension, as it stimulates the continual updating of interpretation that occurs as a sentence unfolds (as shown by Figure 7 in the main text). We have elaborated on the rationale for employing BERT parse depth in this regard in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(3) The main data result figures comparing BERT and the EMEG brain data are hard to evaluate because only t-values are provided, and those, only for significant clusters. It would be helpful to see the full 600 ms time course of rho values, with error bars across subjects, to really be able to evaluate it visually. This is a summary statistic that is very far away from the input data</p>
</disp-quote>
<p>We appreciate this suggestion from R2. In the Appendix 1 of the revised manuscript, we have provided individual participants’ Spearman’s rho time courses for every model RDM tested in all the three epochs (see Appendix 1-figures 8-10 &amp; 14-15). Note that RSA was conducted in the source-localized E/MEG, it is infeasible to plot the rho time course for each searchlight at one of the 8196 vertices on the cortical surface mesh. Instead, we plotted the rho time course of each ROI reported in the original manuscript. These plots complement the time-resolved heatmap of peak t-value in Figures 6-8 in the main text.</p>
<disp-quote content-type="editor-comment">
<p>(4) Some details are omitted or not explained clearly. For example, how was BERT masked to give word-by-word predictions? In its default form, I believe that BERT takes in a set of words before and after the keyword that it is predicting. But I assume that here the model is not allowed to see linguistic information in the future.</p>
</disp-quote>
<p>In our analyses, we utilized the pre-trained version of BERT (Devlin et al. 2019) as released by Hugging Face (<ext-link ext-link-type="uri" xlink:href="https://github.com/huggingface">https://github.com/huggingface</ext-link>). It is noteworthy that BERT, as described in the original paper, was initially trained using the Cloze task, involving the prediction of masked words within an input. In our study, however, we neither retrained nor fine-tuned the pre-trained BERT model, nor did we employ it for word-by-word prediction tasks. We used BERT to derive the incremental representation of a sentence’s structure as it unfolded word-by-word.</p>
<p>Specifically, we sequentially input the text of each sentence into the BERT, akin to how a listener would receive the spoken words in a sentence (see Figure 3C in the main text). For each incremental input (such as “The dog found”), we extracted the hidden representations of each word from BERT. These representations were then transformed into their respective BERT parse depths using a structural probing model (which was trained using sentences with annotated dependency parse tress from the Penn Treebank Dataset). The resulting BERT parse depths were subsequently used to create model RDMs, which were then tested against neural data via RSA.</p>
<p>Crucially, in our approach, BERT was not exposed to any future linguistic information in the sentence. We never tested BERT parse depth of a word in an epoch where this word had not been heard by the listener. For example, the three-dimensional BERT parse depth vector for “The dog found” was tested in the V1 epoch corresponding to “found”, while the fourdimensional BERT parse depth vector for “The dog found in” was tested in the PP1 epoch of “in”.</p>
<disp-quote content-type="editor-comment">
<p>How were the auditory stimuli recorded? Was it continuous speech or silences between each word? How was prosody controlled? Was it a natural speaker or a speech synthesiser?</p>
</disp-quote>
<p>Consistent with our previous studies (Kocagoncu et al. 2017; Klimovich-Gray et al. 2019; Lyu et al. 2019; Choi et al. 2021), all auditory stimuli in this study were recorded by a female native British English speaker, ensuring a neutral intonation throughout. We have incorporated this detail into the revised version of our manuscript for clarity.</p>
<disp-quote content-type="editor-comment">
<p>It is difficult for me to fully assess the extent to which the authors achieved their aims, because I am missing important information about the setup of the experiment and the distribution of test statistics across subjects.</p>
</disp-quote>
<p>We are sorry for the previously omitted details regarding the experimental setup and the results of individual participants. As detailed in our responses above, we have now included the necessary information in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Syntactic parsing is a highly dynamic process: When an incoming word is inconsistent with the presumed syntactic structure, the brain has to reanalyze the sentence and construct an alternative syntactic structure. Since syntactic parsing is a hidden process, it is challenging to describe the syntactic structure a listener internally constructs at each time moment. Here, the authors overcome this problem by (1) asking listeners to complete a sentence at some break point to probe the syntactic structure mentally constructed at the break point, and (2) using a DNN model to extract the most likely structure a listener may extract at a time moment. After obtaining incremental syntactic features using the DNN model, the authors analyze how these syntactic features are represented in the brain using MEG.</p>
</disp-quote>
<p>We extend our thanks to Reviewer #3 (referred to as R3 below) for recognizing the methods we used in this study.</p>
<disp-quote content-type="editor-comment">
<p>Although the analyses are detailed, the current conclusion needs to be further specified. For example, in the abstract, it is concluded that &quot;Our results reveal a detailed picture of the neurobiological processes involved in building structured interpretations through the integration across multifaceted constraints&quot;. The readers may remain puzzled after reading this conclusion.</p>
</disp-quote>
<p>Following R3’s suggestion, we have revised the abstract and refined our conclusions in the main text to explicitly highlight our principal findings. These include: (1) a shift from bihemispheric lateral frontal-temporal regions to left-lateralized regions in representing the current structured interpretation as a sentence unfolds, (2) a pattern of sequential activations in the left lateral temporal regions, updating the structured interpretation as syntactic ambiguity is resolved, and (3) the influence of lexical interpretative coherence activated in the right hemisphere over the resolved sentence structure represented in the left hemisphere.</p>
<disp-quote content-type="editor-comment">
<p>Similarly, for the second part of the conclusion, i.e., &quot;including an extensive set of bilateral brain regions beyond the classical fronto-temporal language system, which sheds light on the distributed nature of language processing in the brain.&quot; The more extensive cortical activation may be attributed to the spatial resolution of MEG, and it is quite well acknowledged that language processing is quite distributive in the brain.</p>
</disp-quote>
<p>We fully agree with R3 on the relatively low spatial resolution of MEG. Our emphasis was on the observed peak activations in specific regions outside the classical brain areas related to language processing, such as the precuneus in the default mode network, which are unlikely to be artifacts due to the spatial resolution of MEG. We have revised the relevant contents in the Abstract.</p>
<disp-quote content-type="editor-comment">
<p>The authors should also discuss:</p>
<p>(1) individual differences (whether the BERT representation is a good enough approximation of the mental representation of individual listeners).</p>
</disp-quote>
<p>To address the issue of individual differences which was also suggested by R2, we added individual participants’ model fits in ROIs with significant effects of BERT representations in Appendix 1 of the revised manuscript (see Appendix 1-figures 8-10 &amp; 14-15).</p>
<disp-quote content-type="editor-comment">
<p>(2) parallel parsing (I think the framework here should allow the brain to maintain parallel representations of different syntactic structures but the analysis does not consider parallel representations).</p>
</disp-quote>
<p>In the original manuscript, we did not discuss parallel parsing because the methods we used does not support a direct test for this hypothesis. In our analyses, we assessed the preference for one of two plausible syntactic structures (i.e., Active and Passive interpretations) based on the BERT parse vector of an incremental sentence input. This assessment was accomplished by calculating the mismatch between the BERT parse depth vector and the context-free dependency parse depth vector representing each of the two structures. However, we only observed one preferred interpretation in each epoch (see Figures 6D-6F) and did not find evidence supporting the maintenance of parallel representations of different syntactic structures in the brain. Nevertheless, in the revised manuscript, we have mentioned this possibility, which could be properly explored in future studies.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Consider fitting the behavioral data from the continuation pre-test to the brain data in order to illustrate the claimed advantage of using a computational model beyond more traditional methods.</p>
</disp-quote>
<p>Following R1’s suggestion, we conducted additional RSA using more behavioural and corpusbased metrics. We then directly compared the fits of these traditional metrics to brain data with those of BERT metrics in the same epoch to provide empirical evidence for the advantage of using a computational model like BERT to explain listeners’ neural data (see Appendix 1figures 11-13).</p>
<disp-quote content-type="editor-comment">
<p>Clarify the use of &quot;neural representations: For a clearer assessment of the results, please discuss your results (especially the fits with BERT parse depth) in terms of the potential effects of distinct sentence structure expectations on working memory demands and make clear where these can be disentangled from neural representations of an unfolding sentence's structure.</p>
</disp-quote>
<p>In the revised manuscript, we have noted the working memory demands associated with the online construction of a structured interpretation during incremental speech comprehension. As mentioned in our response to the relevant comment by R1 above, our experimental paradigm is not suitable for quantitatively assessing working memory demands since it is difficult to determine the exact number of open nodes for our stimuli with syntactic ambiguity before the disambiguating point (i.e., the main verb) is reached. Therefore, while we can speculate the potential contribution of varying working memory demands (which might correlate with BERT V1 parse depth) to RSA model fits, we think it is not possible to disentangle their effects from the neural representation of an unfolding sentence’s structure modelled by BERT parse depths in our current study.</p>
<disp-quote content-type="editor-comment">
<p>Please add in methods a description of how the uniqueness point was determined.</p>
</disp-quote>
<p>In this study, we defined the uniqueness point of a word as the earliest point in time when this word can be fully recognized after removing all of its phonological competitors. To determine the uniqueness point for each word of interest, we first identified the phoneme by which this word can be uniquely recognized according to CELEX (Baayen et al. 1993). Then, we manually labelled the offset of this phoneme in the auditory file of the spoken sentence in which this word occurred. We have added relevant description of how the uniqueness point was determined in the Methods section of the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>I found the name &quot;interpretative mismatch&quot; very opaque. Maybe instead consider &quot;preference&quot;.</p>
</disp-quote>
<p>We chose to use the term “interpretative mismatch” rather than “preference” based on the operational definition of this metric, which is the distance between a BERT parse depth vector and one of the two context-free parse depth vectors representing the two possible syntactic structures, so that a smaller distance value (or mismatch) signifies a stronger preference for the corresponding interpretation.</p>
<disp-quote content-type="editor-comment">
<p>In the abstract, the authors describe the cognitive process under investigation as one of incremental combination subject to &quot;multi-dimensional probabilistic constraint, including both linguistic and non-linguistic knowledge&quot;. The non-linguistic knowledge is later also referred to as &quot;broad world knowledge&quot;. These terms lack specificity and across studies have been operationalized in distinct ways. In the current study, this &quot;world knowledge&quot; is operationalized as the likelihood of a subject noun being an agent or patient and the probability for a verb to be transitive, so here a more specific term may have been the &quot;knowledge about statistical regularities in language&quot;.</p>
</disp-quote>
<p>In this study, we specifically define “non-linguistic world knowledge” as the likelihood of a subject noun assuming the role of an agent or patient, which relates to its thematic role preference. This type of knowledge is primarily non-linguistic in nature, as exemplified by comparing nouns like “king” and “desk”. Although it could be reflected by statistical regularities in language, thematic role preference hinges more on world knowledge, plausibility, or real-world statistics. In contrast, “linguistic knowledge” in our study refers to verb transitivity, which focuses on the grammatically correct usage of a verb and is tied to statistical regularities within language itself. In the revised manuscript, we have provided clearer operational definitions for these two concepts and have ensured consistent usage throughout the text.</p>
<disp-quote content-type="editor-comment">
<p>Please spell out what exactly the &quot;constraint-based hypothesis&quot; is (even better, include an explicit description of the alternative hypothesis?).</p>
</disp-quote>
<p>The “constraint-based hypothesis”, as summarized in a review (McRae and Matsuki 2013), posits that various sources of information, referred to as “constraints”, are simultaneously considered by listeners during incremental speech comprehension. These constraints encompass syntax, semantics, knowledge of common events, contextual pragmatic biases, and other forms of information gathered from both intra-sentential and extra-sentential context.
Notably, there is no delay in the utilization of these multifaceted constraints once they become available, neither is a fixed priority assigned to one type of constraint over another. Instead, a diverse set of constraints is immediately brought into play for comprehension as soon as they become available as the relevant spoken word is recognized.</p>
<p>An alternative hypothesis, proposed earlier, is the two-stage garden path model (Frazier and Rayner 1982; Frazier 1987). According to this model, there is an initial parsing stage that relies solely on syntax. This is followed by a second stage where all available information, including semantics and other knowledge, is used to assess the plausibility of the results obtained in the first-stage analysis and to conduct re-analysis if necessary (McRae and Matsuki 2013). In the Introduction of our revised manuscript, we have elaborated on the “constraint-based hypothesis” and mentioned this two-stage garden path model as its alternative.</p>
<disp-quote content-type="editor-comment">
<p>Fig1 B&amp;C: In order to make the data more interpretable, could you estimate how many possible grammatical structural configurations there are / how many different grammatical structures were offered in the pretest, and based on this what would be the &quot;chance probability&quot; of choosing a random structure or for example show how many responded with a punctuation vs alternative continuations?</p>
</disp-quote>
<p>In our analysis of the behavioural results, we categorized the continuations provided by participants in the pre-test at the offset of Verb1 (e.g., “The dog found/walked …”) into 6 categories, including DO (direct object), INTRANS (intransitive), PP (prepositional phrase), INF (infinitival complement), SC (sentential complement) and OTHER (gerund, phrasal verb, etc.).</p>
<table-wrap id="sa3table1">
<label>Author response table 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-89311-sa3-table1.jpg" mimetype="image"/>
</table-wrap>
<p>Similarly, we categorized the continuations that followed the offset of the prepositional phrase (e.g., “The dog found/walked in the park …”) into 7 categories, including MV (main verb), END (i.e., full stop), PP (prepositional phrase), INF (infinitival complement), CONJ (conjunction), ADV (adverb) and OTHER (gerund, sentential complement, etc.).</p>
<table-wrap id="sa3table2">
<label>Author response table 2.</label>
<graphic mime-subtype="jpg" xlink:href="elife-89311-sa3-table2.jpg" mimetype="image"/>
</table-wrap>
<p>It is important to note that the results of these two pre-tests, including the types of continuations and their probabilities, exhibited considerable variability between and within each sentence type (see also Figures 2B and 2C).</p>
<disp-quote content-type="editor-comment">
<p>Typo: &quot;In addition, we found that BERT structural interpretations were also a correlation with the main verb probability&quot; &gt;&gt; correlated instead of correlation.</p>
</disp-quote>
<p>We apologize for this typo. We have conducted a thorough proofreading to identify and correct any other typos present in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>&quot;In this regard, DLMs excel in a flexible combination of different types of features embedded in their rich internal representations&quot;. What are the &quot;different types&quot;, spell out at least some examples for illustration.</p>
</disp-quote>
<p>We have rephrased this sentence to give a more detailed description.</p>
<disp-quote content-type="editor-comment">
<p>Fig 2 caption: &quot;Same color scheme as in (A)&quot; &gt;&gt; should be 'as in (B)'?, and later A instead of B.</p>
</disp-quote>
<p>We are sorry for this typo. We have corrected it in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>My biggest recommendation is to make the paper clearer in two ways: (i) writing style, by hand-holding the reader through each section, and the motivation for each step, in both simple and technical language; (ii) schematic visuals, of the experimental design and the analysis. A schematic of the main experimental manipulation would be helpful, rather than just listing two example sentences. It would also be helpful to provide a schematic of the experimental setup and the analysis approach, so that people can refer to a visual aid in addition to the written explanation. For example, it is not immediately clear what is being correlated with what - I needed to go to the methods to understand that you are doing RSA across all of the trials. Make sure that all of the relevant details are explained, and that you motivate each decision.</p>
</disp-quote>
<p>We thank R2 for these suggestions. In the revised manuscript, we have enhanced the clarity of the main text by providing a more detailed explanation of the motivation behind each analysis and the interpretation of the corresponding results. Additionally, in response to R2’s recommendation, we have added a few figures, including the illustration of the experimental design (Figure 1) and methods (see Figure 3C and Figure 5).</p>
<disp-quote content-type="editor-comment">
<p>Different visualisation of neural results - The main data result figures comparing BERT and the EMEG brain data are hard to evaluate because only t-values are provided, and those, are only for significant clusters. It would be helpful to see the full 600 ms time course of rho values, with error bars across subjects, to really be able to evaluate it visually.</p>
</disp-quote>
<p>In the original manuscript, we opted to present t-value time courses for the sake of simplicity in illustrating the fits of the 12 model RDMs tested in 3 epochs. Following R2’s suggestion, we have included the ROI model fit time courses of each model RDM for all individual participants, as well as the mean model fit time course with standard error in Appendix 1figures 8-10 &amp; 14-15.</p>
<disp-quote content-type="editor-comment">
<p>How are the authors dealing with prosody differences that disambiguate syntactic structures, that BERT does not have access to?</p>
</disp-quote>
<p>All spoken sentence stimuli were recorded by a female native British English speaker, ensuring a neutral intonation throughout. Therefore, prosody is unlikely to vary systematically between different sentence types or be utilized to disambiguate syntactic structures. Sample speech stimuli have been made available in the following repository: <ext-link ext-link-type="uri" xlink:href="https://osf.io/7u8jp/">https://osf.io/7u8jp/</ext-link>.</p>
<disp-quote content-type="editor-comment">
<p>A few writing errors: &quot;was kept updated every time&quot;</p>
</disp-quote>
<p>We are sorry for the typos. We have conducted proof-reading carefully to identify and correct typos throughout the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<p>Explain why the syntactic trees have &quot;in park the&quot; rather than &quot;in the park&quot;?</p>
</disp-quote>
<p>The dependency parse trees (e.g., Figure 3A) were generated according to the conventions of dependency parsing (de Marneffe et al. 2006).</p>
<disp-quote content-type="editor-comment">
<p>Why are there mentions of the multiple demand network in the results? I'm not sure where this comes from.</p>
</disp-quote>
<p>The mention of the multiple demand network was made due to the significant RSA fits observed in the dorsal lateral prefrontal regions and the superior parietal regions, which are parts of the multiple demand network. This observation was particularly notable for the BERT parse depth vector in the main verb epoch when the potential syntactic ambiguity was being resolved. It is plausible that these effects observed are partly attributed to the varying working memory demands required to maintain the “opening nodes” in the different syntactic structures being considered by listeners at this point in the sentence.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>The study first asked human listeners to complete partial sentences, and incremental parsing of the partial sentences can be captured based on the completed sentences. This analysis is helpful and I wonder if the behavioral data here are enough to model the E/MEG responses. For example, if I understood it correctly, the parse depth up to V1 can be extracted based on the completed sentences and used for the E/MEG analysis.</p>
</disp-quote>
<p>The behavioural data alone do not suffice to model the E/MEG data. As we elucidated in our responses to R1, we employed three behavioural metrics derived from the continuation pretests. These metrics include the V1 transitivity and the PP probability, given the continuations after V1 (e.g., after “The dog found…”), as well as the MV probability, given the continuations after the prepositional phrase (e.g., after “The dog found in the park…”). These metrics aimed to capture participants’ prediction based on their structured interpretations at various positions in the sentence. However, none of these behavioural metrics yielded significant model fits to the listeners’ neural activity, which sharply contrasts with the substantial model fits of the BERT metrics in the same epochs. Besides, we also tried to model V1 parse depth as a weighted average based on participants’ continuations. As shown in Figure 3A, V1 parse depth is 0 in the active interpretation, 2 in the passive interpretation, while the parse depth of the determiner and the subject noun does not differ. However, this continuation-based V1 parse depth [i.e., 0 × Probability(active interpretation) + 2 × Probability(passive interpretation)] did not show significant model fits.</p>
<disp-quote content-type="editor-comment">
<p>Related to this point, I wonder if the incremental parse extracted using BERT is consistent with the human results (i.e., parsing extracted based on the completed sentences) on a sentence-bysentence basis.</p>
</disp-quote>
<p>In fact, we did provide evidence showing the alignment between the incremental parse extracted using BERT and the human interpretation for the same partial sentence input (see Figure 4 in the main text and Appendix 1-figures 4-6).</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, in Fig 1d, is it possible to calculate how much variance of the 3 probabilities is explained by the 4 factors, e.g., using a linear model? If these factors can already explain most of the variance of human parsing, is it possible to just use these 4 factors to explain neural activity?</p>
</disp-quote>
<p>Following R3’s suggestion, we have conducted additional linear modelling analyses to compare the extent to which human behavioural data can be explained by corpus metrics and BERT metrics separately. Specifically, for each of the three probabilities obtained in the pretests (i.e., DO, PP, and MV), we constructed two linear models. One model utilized the four corpus-based metrics as regressors (i.e., SN agenthood, V1 transitivity, Passive index, and Active index), while the other model used BERT metrics as regressors (i.e., BERT parse depth of each word up to V1 from layer 13 for DO/PP probability and BERT parse depth of each word up to the end of PP from layer 14 for MV probability, consistent with the BERT layers reported in Figure 6).</p>
<p>As shown in the table below, corpus metrics demonstrate a more effective fit than BERT metrics for predicting the DO/PP probability. The likelihood of a DO/PP continuation is chiefly influenced by the lexical syntactic property of V1 (i.e., transitivity), and appears to rely less on contextual factors. Since V1 transitivity is explicitly included as one of the corpus metrics, it is thus expected to align more closely with the DO/PP probability compared to BERT metrics, primarily reflecting transitive versus intransitive verb usage.</p>
<table-wrap id="sa3table3">
<label>Author response table 3.</label>
<graphic mime-subtype="jpg" xlink:href="elife-89311-sa3-table3.jpg" mimetype="image"/>
</table-wrap>
<p>Actually, BERT V1 parse depth was not correlated with V1 transitivity when the sentence only unfolds to V1 (see Appendix 1-figure 6). This lack of correlation may stem from the fact that the BERT probing model was designed to represent the structure of a (partially) unfolded sentence, rather than to generate a continuation or prediction. Moreover, V1 transitivity alone does not conclusively determine the Active or Passive interpretation by the end of V1. For instance, both transitive and intransitive continuations after V1 are compatible with an Active interpretation. Consequently, the initial preference for an Active interpretation (as depicted by the early effects before V1 was recognized in Figure 6D), might be predominantly driven by the animate subject noun (SN) at the beginning of the sentence, a word order cue in languages like English (Mahowald et al. 2023).</p>
<p>In contrast, when assessing the probability of a MV following the PP (e.g., after “The dog found in the park ...”), BERT metrics significantly outperformed corpus metrics in terms of fitting the MV probability. Although SN thematic role preference and V1 transitivity were designed to be the primary factors constraining the structured interpretation in this experiment, we could only obtain their context-independent estimates from corpora (i.e., considering all contexts). Additionally, despite Active/Passive index (a product of these two factors) are correlated with the MV probability, it may oversimplify the task of capturing the specific context of a given sentence. Furthermore, the PP following V1 is also expected to influence the structured interpretation. For instance, whether “in the park” is a more plausible scenario for people to find a dog or for a dog to find something. Thus, this finding suggests that the corpus-based metrics are not as effective as BERT in representing contextualized structured interpretations (for a longer sentence input), which might require the integration of constraints from every word in the input.</p>
<p>In summary, corpus-based metrics excel in explaining human language behaviour when it primarily relies on specific lexical properties. However, they significantly lag behind BERT metrics when more complex contextual factors come into play at the same time. Regarding their performance in fitting neural data, among the four corpus-based metrics, we only observed significant model fits for the Passive index in the MV epoch when the intended structure for a Passive interpretation was finally resolved, while the other three metrics did not exhibit significant model fits in any epoch. Note that subject noun thematic role preference did fit neural data in the PP and MV epochs (Figure 8A and 8B). In contrast, the incremental BERT parse depth vector exhibited significant model fits in all three epochs we tested (i.e., V1, PP1, and MV).</p>
<disp-quote content-type="editor-comment">
<p>To summarize, I feel that I'm not sure if the structural information BERT extracts reflect the human parsing of the sentences, especially when the known influencing factors are removed.</p>
</disp-quote>
<p>Based on the results presented above and, in the manuscript, BERT metrics align closely with human structured interpretations in terms of both behavioural and neural data. Furthermore, they outperform corpus-based metrics when it comes to integrating multiple constraints within the context of a specific sentence as it unfolds.</p>
<disp-quote content-type="editor-comment">
<p>Minor issues:</p>
<p>Six types of sentences were presented. Three types were not analyzed, but the results for the UNA sentences are not reported either.</p>
</disp-quote>
<p>In this study, we only analysed two out of the six types of sentences, i.e., HiTrans and LoTrans sentences. The remaining four types of sentences were included to ensure a diverse range of sentence structures and avoid potential adaption the same syntactic structure.</p>
<disp-quote content-type="editor-comment">
<p>Fig 1b, If I understood it correctly, each count is a sentence. Providing examples of the sentences may help. Listing the sentences with the corresponding probabilities in the supplementary materials can also help.</p>
</disp-quote>
<p>Yes, each count in Figure 2B (Figure 1B in the original manuscript) is a sentence. All sentence stimuli and results of pre-tests are available in the following repository <ext-link ext-link-type="uri" xlink:href="https://osf.io/7u8jp/">https://osf.io/7u8jp/</ext-link>.</p>
<disp-quote content-type="editor-comment">
<p>&quot;trajectories of individual HiTrans and LoTrans sentences are considerably distributed and intertwined (Fig. 2C, upper), suggesting that BERT structural interpretations are sensitive to the idiosyncratic contents in each sentence.&quot; It may also mean the trajectories are noisy.</p>
</disp-quote>
<p>We agree with R3 that there might be unwanted noise underlying the distributed and intertwined BERT parse depth trajectories of individual sentences. Meanwhile, it is also important to note that the correlation between BERT parse depths and lexical constraints of different words at the same position across sentences is statistically supported.</p>
<p>References</p>
<p>Baayen RH, Piepenbrock R, van H R. 1993. The {CELEX} lexical data base on {CD-ROM}. Baroni M, Dinu G, Kruszewski G. 2014. Don't count, predict! A systematic comparison of contextcounting vs. context-predicting semantic vectors. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Vol 1.238-247.</p>
<p>Caucheteux C, King JR. 2022. Brains and algorithms partially converge in natural language processing. Communications Biology. 5:134.</p>
<p>Choi HS, Marslen-Wilson WD, Lyu B, Randall B, Tyler LK. 2021. Decoding the Real-Time Neurobiological Properties of Incremental Semantic Interpretation. Cereb Cortex. 31:233-247.</p>
<p>de Marneffe M-C, MacCartney B, Manning CD editors. Generating typed dependency parses from phrase structure parses, Proceedings of the 5th International Conference on Language Resources and
Evaluation; 2006 May 22-28, 2006; Genoa, Italy:European Language Resources Association. 449-454 p.</p>
<p>Devlin J, Chang M-W, Lee K, Toutanova K editors. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies; 2019 June 2-7, 2019; Minneapolis, MN, USA:Association for Computational Linguistics. 4171-4186 p.</p>
<p>Frazier L. 1987. Syntactic processing: evidence from Dutch. Natural Language &amp; Linguistic Theory. 5:519-559.</p>
<p>Frazier L, Rayner K. 1982. Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology. 14:178-210.</p>
<p>Klimovich-Gray A, Tyler LK, Randall B, Kocagoncu E, Devereux B, Marslen-Wilson WD. 2019. Balancing Prediction and Sensory Input in Speech Comprehension: The Spatiotemporal Dynamics of Word Recognition in Context. Journal of Neuroscience. 39:519-527.</p>
<p>Kocagoncu E, Clarke A, Devereux BJ, Tyler LK. 2017. Decoding the cortical dynamics of soundmeaning mapping. Journal of Neuroscience. 37:1312-1319.</p>
<p>Lyu B, Choi HS, Marslen-Wilson WD, Clarke A, Randall B, Tyler LK. 2019. Neural dynamics of semantic composition. Proceedings of the National Academy of Sciences of the United States of America. 116:21318-21327.</p>
<p>Mahowald K, Diachek E, Gibson E, Fedorenko E, Futrell R. 2023. Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages. Cognition. 241:105543.</p>
<p>McRae K, Matsuki K. 2013. Constraint-based models of sentence processing. Sentence processing. 519:51-77.</p>
<p>Schrimpf M, Blank IA, Tuckute G, Kauf C, Hosseini EA, Kanwisher N, Tenenbaum JB, Fedorenko E. 2021. The neural architecture of language: Integrative modeling converges on predictive processing. Proceedings of the National Academy of Sciences of the United States of America. 118:e2105646118.</p>
</body>
</sub-article>
</article>