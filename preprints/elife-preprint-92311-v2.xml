<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">92311</article-id>
<article-id pub-id-type="doi">10.7554/eLife.92311</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92311.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Cancer Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Replication of “null results” – Absence of evidence or evidence of absence?</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Pawel</surname>
<given-names>Samuel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="author-notes" rid="afn1">†</xref>
<email xlink:href="mailto:samuel.pawel@uzh.ch">samuel.pawel@uzh.ch</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Heyard</surname>
<given-names>Rachel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="afn1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Micheloud</surname>
<given-names>Charlotte</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Held</surname>
<given-names>Leonhard</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Epidemiology, Biostatistics and Prevention Institute, Center for Reproducible Science, University of Zurich</institution>, <country>Switzerland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Boonstra</surname>
<given-names>Philip</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Michigan</institution>
</institution-wrap>
<city>Ann Arbor</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Rodgers</surname>
<given-names>Peter</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>eLife</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>For correspondence:</bold> <email xlink:href="mailto:samuel.pawel@uzh.ch">samuel.pawel@uzh.ch</email> (SP)</corresp>
<fn id="afn1"><label>†</label><p>Contributed equally</p></fn>
</author-notes>
<pub-date pub-type="epub">
<day>18</day>
<month>12</month>
<year>2023</year>
</pub-date>
<pub-date date-type="original-publication" iso-8601-date="2023-11-22">
<day>22</day>
<month>11</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-02-16">
<day>16</day>
<month>02</month>
<year>2024</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP92311</elocation-id>
<history><date date-type="sent-for-review" iso-8601-date="2023-09-08">
<day>08</day>
<month>09</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-05-08">
<day>08</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2305.04587"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-11-22">
<day>22</day>
<month>11</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92311.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.92311.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.92311.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.92311.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.92311.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Pawel et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Pawel et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-92311-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>In several large-scale replication projects, statistically non-significant results in both the original and the replication study have been interpreted as a “replication success”. Here we discuss the logical problems with this approach: Non-significance in both studies does not ensure that the studies provide evidence for the absence of an effect and “replication success” can virtually always be achieved if the sample sizes are small enough. In addition, the relevant error rates are not controlled. We show how methods, such as equivalence testing and Bayes factors, can be used to adequately quantify the evidence for the absence of an effect and how they can be applied in the replication setting. Using data from the Reproducibility Project: Cancer Biology, the Experimental Philosophy Replicability Project, and the Reproducibility Project: Psychology we illustrate that many original and replication studies with “null results” are in fact inconclusive. We conclude that it is important to also replicate studies with statistically non-significant results, but that they should be designed, analyzed, and interpreted appropriately.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="conflict-interest-statement">
<title>Conflict of interest</title>
<p>We declare no conflict of interest.</p>
</notes>
</notes>
</front>
<body>
<sec id="s1" sec-type="intro">
<title>Introduction</title>
<p><italic>Absence of evidence is not evidence of absence</italic> – the title of the 1995 paper by Douglas Altman and Martin Bland has since become a mantra in the statistical and medical literature (<xref ref-type="bibr" rid="c1">Altman and Bland, 1995</xref>). Yet, the misconception that a statistically non-significant result indicates evidence for the absence of an effect is unfortunately still widespread (<xref ref-type="bibr" rid="c21">Greenland, 2011</xref>; <xref ref-type="bibr" rid="c42">Makin and de Xivry, 2019</xref>). Such a “null result” – typically characterized by a <italic>p</italic>-value <italic>p</italic> &gt; 0.05 for the null hypothesis of an absent effect – may also occur if an effect is actually present. For example, if the sample size of a study is chosen to detect an assumed effect with a power of 80%, null results will incorrectly occur 20% of the time when the assumed effect is actually present. If the power of the study is lower, null results will occur more often. In general, the lower the power of a study, the greater the ambiguity of a null result. To put a null result in context, it is therefore critical to know whether the study was adequately powered and under what assumed effect the power was calculated (<xref ref-type="bibr" rid="c27">Hoenig and Heisey, 2001</xref>; <xref ref-type="bibr" rid="c22">Greenland, 2012</xref>). However, if the goal of a study is to explicitly quantify the evidence for the absence of an effect, more appropriate methods designed for this task, such as equivalence testing (<xref ref-type="bibr" rid="c64">Wellek, 2010</xref>; <xref ref-type="bibr" rid="c37">Lakens, 2017</xref>; <xref ref-type="bibr" rid="c60">Senn, 2021</xref>) or Bayes factors (<xref ref-type="bibr" rid="c31">Kass and Raftery, 1995</xref>; <xref ref-type="bibr" rid="c19">Goodman, 1999</xref>, <xref ref-type="bibr" rid="c20">2005</xref>; <xref ref-type="bibr" rid="c14">Dienes, 2014</xref>; <xref ref-type="bibr" rid="c33">Keysers et al., 2020</xref>), should be used from the outset.</p>
<p>The interpretation of null results becomes even more complicated in the setting of replication studies. In a replication study, researchers attempt to repeat an original study as closely as possible in order to assess whether consistent results can be obtained with new data (<xref ref-type="bibr" rid="c49">National Academies of Sciences, Engineering, and Medicine, 2019</xref>). In the last decade, various large-scale replication projects have been conducted in diverse fields, from the biomedical to the social sciences (<xref ref-type="bibr" rid="c53">Prinz et al., 2011</xref>; <xref ref-type="bibr" rid="c5">Begley and Ellis, 2012</xref>; <xref ref-type="bibr" rid="c34">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="c50">Open Science Collaboration, 2015</xref>; <xref ref-type="bibr" rid="c6">Camerer et al., 2016</xref>, <xref ref-type="bibr" rid="c7">2018</xref>; <xref ref-type="bibr" rid="c35">Klein et al., 2018</xref>; <xref ref-type="bibr" rid="c12">Cova et al., 2018</xref>; <xref ref-type="bibr" rid="c16">Errington et al., 2021</xref>, among others). The majority of these projects reported alarmingly low replicability rates across a broad spectrum of criteria for quantifying replicability. While most of these projects restricted their focus on original studies with statistically significant results (“positive results”), the Reproducibility Project: Cancer Biology (RPCB, <xref ref-type="bibr" rid="c16">Errington et al., 2021</xref>), the Experimental Philosophy Replicability Project (EPRP, <xref ref-type="bibr" rid="c12">Cova et al., 2018</xref>), and the Reproducibility Project: Psychology (RPP, <xref ref-type="bibr" rid="c50">Open Science Collaboration, 2015</xref>) also attempted to replicate some original studies with null results – either non-significant or interpreted as showing no evidence for a meaningful effect by the original authors.</p>
<p>Although the EPRP and RPP interpreted non-significant results in both original and replication study as a “replication success” for some individual replications (see, for example, the replication of <xref ref-type="bibr" rid="c45">McCann (2005</xref>, replication report: <ext-link ext-link-type="uri" xlink:href="https://osf.io/wcm7n">https://osf.io/wcm7n</ext-link>) or the replication of <xref ref-type="bibr" rid="c55">Ranganath and Nosek (2008</xref>, replication report: <ext-link ext-link-type="uri" xlink:href="https://osf.io/9xt25">https://osf.io/9xt25</ext-link>)), they excluded the original null results in the calculation of an overall replicability rate based on significance. In contrast, the RPCB explicitly defined null results in both the original and the replication study as a criterion for “replication success”. According to this “non-significance” criterion, 11/15 = 73% replications of original null effects were successful. Four additional criteria were used to provide a more nuanced assessment of replication success for original null results: (i) whether the original effect estimate was included in the 95% confidence interval of the replication effect estimate (success rate 11/15 = 73%), (ii) whether the replication effect estimate was included in the 95% confidence interval of the original effect estimate (success rate 12/15 = 80%), (iii) whether the replication effect estimate was included in the 95% prediction interval based on the original effect estimate (success rate 12/15 = 80%), (iv) and whether the <italic>p</italic>-value obtained from combining the original and replication effect estimate with a meta-analysis was non-significant (success rate 10/15 = 67%). Criteria (i) to (iii) are useful for assessing compatibility in effect estimates between the original and the replication study. Their suitability has been extensively discussed in the literature. The prediction interval criterion (iii) or equivalent criteria (e.g., the <italic>Q</italic>-test) are usually recommended because they account for the uncertainty from both studies and have adequate error rates when the true effect sizes are the same (<xref ref-type="bibr" rid="c51">Patil et al., 2016</xref>; <xref ref-type="bibr" rid="c43">Mathur and VanderWeele, 2020</xref>; <xref ref-type="bibr" rid="c58">Schauer and Hedges, 2021</xref>).</p>
<p>While the effect estimate criteria (i) to (iii) can be applied regardless of whether or not the original study was non-significant, the “meta-analytic non-significance” criterion (iv) and the aforementioned non-significance criterion refer specifically to original null results. We believe that there are several logical problems with both, and that it is important to highlight and address them, especially since the non-significance criterion has already been used in three replication projects without much scrutiny. It is crucial to note that it is not our intention to diminish the enormously important contributions of the RPCB, the EPRP, and the RPP, but rather to build on their work and provide recommendations for ongoing and future replication projects (e.g., <xref ref-type="bibr" rid="c2">Amaral et al., 2019</xref>; <xref ref-type="bibr" rid="c48">Murphy et al., 2022</xref>).</p>
<p>The logical problems with the non-significance criterion are as follows: First, if the original study had low statistical power, a non-significant result is highly inconclusive and does not provide evidence for the absence of an effect. It is then unclear what exactly the goal of the replication should be – to replicate the inconclusiveness of the original result? On the other hand, if the original study was adequately powered, a non-significant result may indeed provide some evidence for the absence of an effect when analyzed with appropriate methods, so that the goal of the replication is clearer. However, the criterion by itself does not distinguish between these two cases. Second, with this criterion researchers can virtually always achieve replication success by conducting a replication study with a very small sample size, such that the <italic>p</italic>-value is non-significant and the result is inconclusive. This is because the null hypothesis under which the <italic>p</italic>-value is computed is misaligned with the goal of inference, which is to quantify the evidence for the absence of an effect. Third, the criterion does not control the error of falsely claiming the absence of an effect at a predetermined rate. This is in contrast to the standard criterion for replication success, which requires significance from both studies (also known as the two-trials rule, see Section 12.2.8 in <xref ref-type="bibr" rid="c60">Senn, 2021</xref>), and ensures that the error of falsely claiming the presence of an effect is controlled at a rate equal to the squared significance level (for example, 5% × 5% = 0.25% for a 5% significance level). The non-significance criterion may be intended to complement the two-trials rule for null results. However, it fails to do so in this respect, which may be required by regulators and funders. These logical problems are equally applicable to the meta-analytic non-significance criterion.</p>
<p>In the following, we present two principled approaches for analyzing replication studies of null results – frequentist equivalence testing and Bayesian hypothesis testing – that can address the limitations of the non-significance criterion. We use the null results replicated in the RPCB, RPP, and EPRP to illustrate the problems of the non-significance criterion and how they can be addressed. We conclude the paper with practical recommendations for analyzing replication studies of original null results, including simple R code for applying the proposed methods.</p>
</sec>
<sec id="s2">
<title>Null results from the Reproducibility Project: Cancer Biology</title>
<p><xref ref-type="fig" rid="fig1">Figure 1</xref> shows effect estimates on standardized mean difference (SMD) scale with 95% confidence intervals from two RPCB study pairs. In both study pairs, the original and replication studies are “null results” and therefore meet the non-significance criterion for replication success (the two-sided <italic>p</italic>-values are greater than 0.05 in both the original and the replication study). The same is true when applying the meta-analytic non-significance criterion (the two-sided <italic>p</italic>-values of the meta-analyses <italic>p</italic><sub>MA</sub> are greater than 0.05). However, intuition would suggest that the conclusions in the two pairs are very different.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1</label>
<caption><p>Two examples of original and replication study pairs which meet the non-significance replication success criterion from the Reproducibility Project: Cancer Biology (<xref ref-type="bibr" rid="c16">Errington et al., 2021</xref>). Shown are standardized mean difference effect estimates with 95% confidence intervals, sample sizes <italic>n</italic>, and two-sided <italic>p</italic>-values <italic>p</italic> for the null hypothesis that the effect is absent. Effect estimate, 95% confidence interval, and <italic>p</italic>-value from a fixed-effect meta-analysis <italic>p</italic><sub>MA</sub> of original and replication study are shown in gray.</p></caption>
<graphic xlink:href="2305.04587v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The original study from <xref ref-type="bibr" rid="c13">Dawson et al. (2011)</xref> and its replication both show large effect estimates in magnitude, but due to the very small sample sizes, the uncertainty of these estimates is large, too. With such low sample sizes, the results seem inconclusive. In contrast, the effect estimates from <xref ref-type="bibr" rid="c18">Goetz et al. (2011)</xref> and its replication are much smaller in magnitude and their uncertainty is also smaller because the studies used larger sample sizes. Intuitively, the results seem to provide more evidence for a zero (or negligibly small) effect. While these two examples show the qualitative difference between absence of evidence and evidence of absence, we will now discuss how the two can be quantitatively distinguished.</p>
</sec>
<sec id="s3">
<title>Methods for assessing replicability of null results</title>
<p>There are both frequentist and Bayesian methods that can be used for assessing evidence for the absence of an effect. <xref ref-type="bibr" rid="c4">Anderson and Maxwell (2016)</xref> provide an excellent summary in the context of replication studies in psychology. We now briefly discuss two possible approaches – frequentist equivalence testing and Bayesian hypothesis testing – and their application to the RPCB, EPRP, and RPP data.</p>
<sec id="s3-1">
<title>Frequentist equivalence testing</title>
<p>Equivalence testing was developed in the context of clinical trials to assess whether a new treatment – typically cheaper or with fewer side effects than the established treatment – is practically equivalent to the established treatment (<xref ref-type="bibr" rid="c64">Wellek, 2010</xref>; <xref ref-type="bibr" rid="c37">Lakens, 2017</xref>). The method can also be used to assess whether an effect is practically equivalent to an absent effect, usually zero. Using equivalence testing as a way to put non-significant results into context has been suggested by several authors (<xref ref-type="bibr" rid="c24">Hauck and Anderson, 1986</xref>; <xref ref-type="bibr" rid="c8">Campbell and Gustafson, 2018</xref>). The main challenge is to specify the margin Δ &gt; 0 that defines an equivalence range [−Δ, +Δ] in which an effect is considered as absent for practical purposes. The goal is then to reject the null hypothesis that the true effect is outside the equivalence range. This is in contrast to the usual null hypotheses of superiority tests which state that the effect is zero or smaller than zero, see <xref ref-type="fig" rid="fig2">Figure 2</xref> for an illustration.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2</label>
<caption><p>Null hypothesis (<italic>H</italic><sub>0</sub>) and alternative hypothesis (<italic>H</italic><sub>1</sub>) for superiority and equivalence tests (with equivalence margin Δ &gt; 0).</p></caption>
<graphic xlink:href="2305.04587v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To ensure that the null hypothesis is falsely rejected at most <italic>α</italic> × 100% of the time, the standard approach is to declare equivalence if the (1-2<italic>α</italic>)×100% confidence interval for the effect is contained within the equivalence range, for example, a 90% confidence interval for <italic>α</italic> = 0.05 (<xref ref-type="bibr" rid="c65">Westlake, 1972</xref>). This procedure is equivalent to declaring equivalence when two one-sided tests (TOST) for the null hypotheses of the effect being greater/smaller than +Δ and −Δ, are both significant at level <italic>α</italic> (<xref ref-type="bibr" rid="c59">Schuirmann, 1987</xref>). A quantitative measure of evidence for the absence of an effect is then given by the maximum of the two one-sided <italic>p</italic>-values – the TOST <italic>p</italic>-value (<xref ref-type="bibr" rid="c23">Greenland, 2023</xref>, section 4.4). In case a dichotomous replication success criterion for null results is desired, it is natural to require that both the original and the replication TOST <italic>p</italic>-values are smaller than some level <italic>α</italic> (conventionally <italic>α</italic> = 0.05). Equivalently, the criterion would require the (1 – 2<italic>α</italic>) × 100% confidence intervals of the original and the replication to be included in the equivalence region. In contrast to the non-significance criterion, this criterion controls the error of falsely claiming replication success at level <italic>α</italic><sup>2</sup> when there is a true effect outside the equivalence margin, thus complementing the usual two-trials rule in drug regulation (<xref ref-type="bibr" rid="c60">Senn, 2021</xref>, Section 12.2.8).</p>
<p>Returning to the RPCB data, <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the standardized mean difference effect estimates with 90% confidence intervals for all 15 effects which were treated as null results by the RPCB.<sup><xref ref-type="fn" rid="fn1">1</xref></sup> Most of them showed non-significant <italic>p</italic>-values (<italic>p</italic> &gt; 0.05) in the original study. It is noteworthy, however, that two effects from the second experiment of the original paper 48 were regarded as null results despite their statistical significance. According to the non-significance criterion (requiring <italic>p</italic> &gt; 0.05 in original and replication study), there are 11 “successes” out of total 15 null effects, as reported in Table 1 from <xref ref-type="bibr" rid="c16">Errington et al. (2021)</xref>.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3</label>
<caption><p>Effect estimates on standardized mean difference (SMD) scale with 90% confidence interval for the 15 “null results” and their replication studies from the Reproducibility Project: Cancer Biology (<xref ref-type="bibr" rid="c16">Errington et al., 2021</xref>). The title above each plot indicates the original paper, experiment and effect numbers. Two original effect estimates from original paper 48 were statistically significant at <italic>p</italic> &lt; 0.05, but were interpreted as null results by the original authors and therefore treated as null results by the RPCB. The two examples from <xref ref-type="fig" rid="fig1">Figure 1</xref> are indicated in the plot titles. The dashed gray line represents the value of no effect (SMD = 0), while the dotted red lines represent the equivalence range with a margin of Δ = 0.74, classified as “liberal” by <xref ref-type="bibr" rid="c64">Wellek (2010</xref>, Table 1.1). The <italic>p</italic>-value <italic>p</italic><sub>TOST</sub> is the maximum of the two one-sided <italic>p</italic>-values for the null hypotheses of the effect being greater/less than +Δ and −Δ, respectively. The Bayes factor BF<sub>01</sub> quantifies the evidence for the null hypothesis <italic>H</italic><sub>0</sub> : SMD = 0 against the alternative <italic>H</italic><sub>1</sub> : SMD ≠ 0 with normal unit-information prior assigned to the SMD under <italic>H</italic><sub>1</sub>.</p></caption>
<graphic xlink:href="2305.04587v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We will now apply equivalence testing to the RPCB data. The dotted red lines in <xref ref-type="fig" rid="fig3">Figure 3</xref> represent an equivalence range for the margin Δ = 0.74, which <xref ref-type="bibr" rid="c64">Wellek (2010</xref>, Table 1.1) classifies as “liberal”. However, even with this generous margin, only 4 of the 15 study pairs are able to establish replication success at the 5% level, in the sense that both the original and the replication 90% confidence interval fall within the equivalence range (or, equivalently, that their TOST <italic>p</italic>-values are smaller than 0.05). For the remaining 11 studies, the situation remains inconclusive and there is no evidence for the absence or the presence of the effect. For instance, the previously discussed example from <xref ref-type="bibr" rid="c18">Goetz et al. (2011)</xref> marginally fails the criterion (<italic>p</italic><sub>TOST</sub> = 0.06 in the original study and <italic>p</italic><sub>TOST</sub> = 0.04 in the replication), while the example from <xref ref-type="bibr" rid="c13">Dawson et al. (2011)</xref> is a clearer failure (<italic>p</italic><sub>TOST</sub> = 0.75 in the original study and <italic>p</italic><sub>TOST</sub> = 0.88 in the replication) as both effect estimates even lie outside the equivalence margin.</p>
<p>The post-hoc specification of equivalence margins is controversial. Ideally, the margin should be specified on a case-by-case basis in a pre-registered protocol before the studies are conducted by researchers familiar with the subject matter. In the social and medical sciences, the conventions of <xref ref-type="bibr" rid="c10">Cohen (1992)</xref> are typically used to classify SMD effect sizes (SMD = 0.2 small, SMD = 0.5 medium, SMD = 0.8 large). While effect sizes are typically larger in preclinical research, it seems unrealistic to specify margins larger than 1 on SMD scale to represent effect sizes that are absent for practical purposes. It could also be argued that the chosen margin Δ = 0.74 is too lax compared to margins commonly used in clinical research (<xref ref-type="bibr" rid="c38">Lange and Freitag, 2005</xref>). We therefore report a sensitivity analysis regarding the choice of the margin in <xref ref-type="fig" rid="fig4">Figure 4</xref> in <xref ref-type="app" rid="ap1">Appendix A</xref>. This analysis shows that for realistic margins between 0 and 1, the proportion of replication successes remains below 50% for the conventional <italic>α</italic> = 0.05 level. To achieve a success rate of 11/15 = 73%, as was achieved with the non-significance criterion from the RPCB, unrealistic margins of Δ &gt; 2 are required.</p>
<p><xref ref-type="app" rid="ap2">Appendix B</xref> shows similar equivalence test analyses for the four study pairs with original null results from the RPP and EPRP. Three study pair results turn out to be inconclusive due to the large uncertainty around their effect estimates.</p>
</sec>
<sec id="s3-2">
<title>Bayesian hypothesis testing</title>
<p>The distinction between absence of evidence and evidence of absence is naturally built into the Bayesian approach to hypothesis testing. A central measure of evidence is the Bayes factor (<xref ref-type="bibr" rid="c31">Kass and Raftery, 1995</xref>; <xref ref-type="bibr" rid="c19">Goodman, 1999</xref>; <xref ref-type="bibr" rid="c14">Dienes, 2014</xref>; <xref ref-type="bibr" rid="c33">Keysers et al., 2020</xref>), which is the updating factor of the prior odds to the posterior odds of the null hypothesis <italic>H</italic><sub>0</sub> versus the alternative hypothesis <italic>H</italic><sub>1</sub>
<disp-formula id="ed1"><alternatives><mml:math display="block" id="e1"><mml:mrow><mml:munder><mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mtext> given data</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext> given data</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mtext>Posterior odds</mml:mtext></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mtext>Prior odds</mml:mtext></mml:mrow></mml:munder><mml:mo>×</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>data given </mml:mtext><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>data given </mml:mtext><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mtext>Bayes factor </mml:mtext><mml:msub><mml:mrow><mml:mtext>BF</mml:mtext></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="2305.04587v3_eqn1.tif" mimetype="image" mime-subtype="tiff"/></alternatives></disp-formula></p>
<p>The Bayes factor BF<sub>01</sub> quantifies how much the observed data have increased or decreased the probability Pr(<italic>H</italic><sub>0</sub>) of the null hypothesis relative to the probability Pr(<italic>H</italic><sub>1</sub>) of the alternative. As such, Bayes factor are direct measures of evidence for the null hypothesis, in contrast to <italic>p</italic>-values, which are only indirect measures of evidence as they are computed under the assumption that the null hypothesis is true (<xref ref-type="bibr" rid="c26">Held and Ott, 2018</xref>). If the null hypothesis states the absence of an effect, a Bayes factor greater than one (BF<sub>01</sub> &gt; 1) indicates evidence for the absence of the effect and a Bayes factor smaller than one indicates evidence for the presence of the effect (BF<sub>01</sub> &lt; 1), whereas a Bayes factor not much different from one indicates absence of evidence for either hypothesis (BF<sub>01</sub> ≈ 1). Bayes factors are quantitative summaries of the evidence provided by the data in favor of the null hypothesis as opposed to the alternative hypothesis. If a dichotomous criterion for successful replication of a null result is desired, it seems natural to require a Bayes factor larger than some level <italic>γ</italic> &gt; 1 from both studies, for example, <italic>γ</italic> = 3 or <italic>γ</italic> = 10 which are conventional levels for “substantial” and “strong” evidence, respectively (<xref ref-type="bibr" rid="c29">Jeffreys, 1961</xref>). In contrast to the nonsignificance criterion, this criterion provides a genuine measure of evidence that can distinguish absence of evidence from evidence of absence.</p>
<p>The main challenge with Bayes factors is the specification of the effect under the alternative hypothesis <italic>H</italic><sub>1</sub>. The assumed effect under <italic>H</italic><sub>1</sub> is directly related to the Bayes factor, and researchers who assume different effects will end up with different Bayes factors. Instead of specifying a single effect, one therefore typically specifies a “prior distribution” of plausible effects. Importantly, the prior distribution, like the equivalence margin, should be determined by researchers with subject knowledge and before the data are collected.</p>
<p>To compute the Bayes factors for the RPCB null results, we used the observed effect estimates as the data and assumed a normal sampling distribution for them (<xref ref-type="bibr" rid="c14">Dienes, 2014</xref>), as typically done in a meta-analysis. The Bayes factors BF<sub>01</sub> shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> then quantify the evidence for the null hypothesis of no effect against the alternative hypothesis that there is an effect using a normal “unit-information” prior distribution (<xref ref-type="bibr" rid="c32">Kass and Wasserman, 1995</xref>) for the effect size under the alternative <italic>H</italic><sub>1</sub>, see <xref ref-type="app" rid="ap3">Appendix C</xref> for further details on the calculation of these Bayes factors. We see that in most cases there is no substantial evidence for either the absence or the presence of an effect, as with the equivalence tests. For instance, with a lenient Bayes factor threshold of 3, only 1 of the 15 replications are successful, in the sense of having BF<sub>01</sub> &gt; 3 in both the original and the replication study. The Bayes factors for the two previously discussed examples are consistent with our intuitions – in the <xref ref-type="bibr" rid="c18">Goetz et al. (2011)</xref> example there is indeed substantial evidence for the absence of an effect (BF<sub>01</sub> = 5 in the original study and BF<sub>01</sub> = 4.1 in the replication), while in the <xref ref-type="bibr" rid="c13">Dawson et al. (2011)</xref> example there is even anecdotal evidence for the presence of an effect, though the Bayes factors are very close to one due to the small sample sizes (BF<sub>01</sub> = 1/1.1 in the original study and BF<sub>01</sub> = 1/1.8 in the replication).</p>
<p>As with the equivalence margin, the choice of the prior distribution for the SMD under the alternative <italic>H</italic><sub>1</sub> is debatable. The normal unit-information prior seems to be a reasonable default choice, as it implies that small to large effects are plausible under the alternative, but other normal priors with smaller/larger standard deviations could have been considered to make the test more sensitive to smaller/larger true effect sizes. The sensitivity analysis in <xref ref-type="app" rid="ap1">Appendix A</xref> therefore also includes an analysis on the effect of varying prior standard deviations and the Bayes factor thresholds. However, again, to achieve replication success for a larger proportion of replications than the observed 1/15 = 7%, unreasonably large prior standard deviations have to be specified.</p>
<p>Of note, among the 15 RPCB null results, there are three interesting cases (the three effects from original paper 48 by <xref ref-type="bibr" rid="c40">Lin et al., 2012</xref> and its replication by <xref ref-type="bibr" rid="c39">Lewis et al., 2018</xref>) where the Bayes factor is qualitatively different from the equivalence test, revealing a fundamental difference between the two approaches. The Bayes factor is concerned with testing whether the effect is <italic>exactly zero</italic>, whereas the equivalence test is concerned with whether the effect is within an <italic>interval around zero</italic>. Due to the very large sample size in the original study (<italic>n</italic> = 514) and the replication (<italic>n</italic> = 1′153), the data are incompatible with an exactly zero effect, but compatible with effects within the equivalence range. Apart from this example, however, both approaches lead to the same qualitative conclusion – most RPCB null results are highly ambiguous.</p>
<p><xref ref-type="app" rid="ap2">Appendix B</xref> also shows Bayes factor analyses for the four study pairs with original null results from the RPP and EPRP. In contrast to the RPCB results, most Bayes factors indicate non-anecdotal evidence for a null effect in cases where the non-significance criterion was met, possibly because of the larger sample sizes and smaller effects in these fields.</p>
</sec>
</sec>
<sec id="s4" sec-type="conclusions">
<title>Conclusions</title>
<p>The concept of “replication success” is inherently multifaceted. Reducing it to a single criterion seems to be an oversimplification. Nevertheless, we believe that the “non-significance” criterion – declaring a replication as successful if both the original and the replication study produce nonsignificant results – is not fit for purpose. This criterion does not ensure that both studies provide evidence for the absence of an effect, it can be easily achieved for any outcome if the studies have sufficiently small sample sizes, and it does not control the relevant error rates. While it is important to replicate original studies with null results, we believe that they should be analyzed using more informative approaches. <xref ref-type="boxed-text" rid="box1">Box 1</xref> summarizes our recommendations.</p>
<boxed-text id="box1">
<label>Box 1</label>
<caption><p>Recommendations for the analysis of replication studies of original null results. Calculations are based on effect estimates <inline-formula id="I1"><alternatives><mml:math display="inline" id="IN1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="2305.04587v3_ieq1.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula> with standard errors <italic>σ<sub>i</sub></italic> from an original study (<italic>i</italic> = <italic>o</italic>) and its replication (<italic>i</italic> = <italic>r</italic>). Both effect estimates are assumed to be normally distributed around the true effect size <italic>θ</italic> with known variance <italic>σ</italic><sup>2</sup><sub><italic>i</italic></sub>.</p>
<p>The effect size <italic>θ</italic><sub>0</sub> represents the value of no effect, typically <italic>θ</italic><sub>0</sub> = 0.</p></caption>
<sec id="sb1">
<title>Equivalence test</title>
<list list-type="order">
<list-item><p>Specify a margin Δ &gt; 0 that defines an equivalence range [<italic>θ</italic><sub>0</sub> − Δ, <italic>θ</italic><sub>0</sub> + Δ] in which effects are considered absent for practical purposes.</p></list-item>
<list-item><p>Compute the TOST <italic>p</italic>-values for original (<italic>i</italic> = <italic>o</italic>) and replication (<italic>i</italic> = <italic>r</italic>) data
<disp-formula id="ed2"><alternatives><mml:math display="block" id="e2"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>TOST</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="2305.04587v3_eqn2.tif" mimetype="image" mime-subtype="tiff"/></alternatives></disp-formula>
with Φ(⋅) the cumulative distribution function of the standard normal distribution.</p>
<p><fig id="fig7" position="float" fig-type="figure">
<graphic xlink:href="2305.04587v3_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p></list-item>
<list-item><p>Declare replication success at level <italic>α</italic> if <italic>p</italic><sub>TOST,<italic>o</italic></sub> ≤ <italic>α</italic> and <italic>p</italic><sub>TOST,<italic>r</italic></sub> ≤ <italic>α</italic>, conventionally <italic>α</italic> = 0.05.</p></list-item>
<list-item><p>Perform a sensitivity analysis with respect to the margin Δ. For example, visualize the TOST <italic>p</italic>-values for different margins to assess the robustness of the conclusions.</p></list-item>
</list>
</sec>
<sec id="sb2">
<title>Bayes factor</title>
<list list-type="order">
<list-item><p>Specify a prior distribution for the effect size <italic>θ</italic> that represents plausible values under the alternative hypothesis that there is an effect (<italic>H</italic><sub>1</sub> : <italic>θ</italic> ≠ <italic>θ</italic><sub>0</sub>). For example, specify the mean <italic>m</italic> and standard deviation <italic>s</italic> of a normal distribution <italic>θ</italic> | <italic>H</italic><sub>1</sub> ∼ N(<italic>m, s</italic><sup>2</sup>).</p></list-item>
<list-item><p>Compute the Bayes factors contrasting <italic>H</italic><sub>0</sub> : <italic>θ</italic> = <italic>θ</italic><sub>0</sub> to <italic>H</italic><sub>1</sub> : <italic>θ</italic> ≠ <italic>θ</italic><sub>0</sub> for original (<italic>i</italic> = <italic>o</italic>) and replication (<italic>i</italic> = <italic>r</italic>) data. Assuming a normal prior distribution, the Bayes factor is
<disp-formula id="ed3"><alternatives><mml:math display="block" id="e3"><mml:mrow><mml:msub><mml:mrow><mml:mtext>BF</mml:mtext></mml:mrow><mml:mrow><mml:mn>01</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mi>exp</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="2305.04587v3_eqn3.tif" mimetype="image" mime-subtype="tiff"/></alternatives></disp-formula>
<fig id="fig8" position="float" fig-type="figure">
<graphic xlink:href="2305.04587v3_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p></list-item>
<list-item><p>Declare replication success at level <italic>γ</italic> &gt; 1 if BF<sub>01,<italic>o</italic></sub> ≥ <italic>γ</italic> and BF<sub>01,<italic>r</italic></sub> ≥ <italic>γ</italic>, conventionally <italic>γ</italic> = 3 (substantial evidence) or <italic>γ</italic> = 10 (strong evidence).</p></list-item>
<list-item><p>Perform a sensitivity analysis with respect to the prior distribution. For example, visualize the Bayes factors for different prior standard deviations to assess the robustness of the conclusions.</p></list-item>
</list>
</sec>
</boxed-text>
<p>Our reanalysis of the RPCB studies with original null results showed that for most studies that meet the non-significance criterion, the conclusions are much more ambiguous – both with frequentist and Bayesian analyses. While the exact success rate depends on the equivalence margin and the prior distribution, our sensitivity analyses show that even with unrealistically liberal choices, the success rate remains below 40% which is substantially lower than the 73% success rate based on the non-significance criterion.</p>
<p>This is not unexpected, as a study typically requires larger sample sizes to detect the absence of an effect than to detect its presence (<xref ref-type="bibr" rid="c44">Matthews, 2006</xref>, Section 11.5.3). Of note, the RPCB sample sizes were chosen so that each replication had at least 80% power to detect the original effect estimate based on a standard superiority test. However, the design of replication studies should ideally align with the planned analysis (<xref ref-type="bibr" rid="c3">Anderson and Kelley, 2022</xref>) so if the goal of the study is to find evidence for the absence of an effect, the replication sample size should be determined based on a test for equivalence, see <xref ref-type="bibr" rid="c17">Flight and Julious (2015)</xref> and <xref ref-type="bibr" rid="c52">Pawel et al. (2023)</xref> for frequentist and Bayesian approaches, respectively.</p>
<p>Our reanalysis of the RPP and EPRP studies with original null results showed that Bayes factors indeed indicate some evidence for no effect in cases where the non-significance criterion was satisfied, possibly due to the smaller effects and typically larger sample sizes in these fields compared to cancer biology. On the other hand, in most cases the precision of the effect estimates was still limited so that only one study pair achieved replication success with the equivalence testing approach. However, it is important to note that the conclusions from the RPP and EPRP analyses are merely anecdotal, as there were only four study pairs with original null results to analyze.</p>
<p>For both the equivalence test and the Bayes factor approach, it is critical that the equivalence margin and the prior distribution are specified independently of the data, ideally before the original and replication studies are conducted. Typically, however, the original studies were designed to find evidence for the presence of an effect, and the goal of replicating the “null result” was formulated only after failure to do so. It is therefore important that margins and prior distributions are motivated from historical data and/or field conventions (<xref ref-type="bibr" rid="c9">Campbell and Gustafson, 2021</xref>), and that sensitivity analyses regarding their choice are reported.</p>
<p>In addition, when analyzing a single pair of original and replication studies, we recommend interpreting Bayes factors and TOST <italic>p</italic>-values as quantitative measures of evidence and discourage dichotomizing them into “success” or “failure”. For example, two TOST <italic>p</italic>-values <italic>p</italic><sub>TOST</sub> = 0.049 and <italic>p</italic><sub>TOST</sub> = 0.051 carry similar evidential weight regardless of one being slightly smaller and the other being slightly larger than 0.05. On the other hand, when more than one pair of original and replication studies are analyzed, dichotomization may be required for computing an overall success rate. In this case, the rate may be computed for different thresholds that correspond to qualitatively different levels of evidence (e.g., 1, 3, and 10 for Bayes factors, or 0.05, 0.01, and 0.005 for <italic>p</italic>-values).</p>
<p>Researchers may also ask whether the equivalence test or the Bayes factor is “better”. We believe that this is the wrong question to ask, because both methods address different questions and are better in different senses; the equivalence test is calibrated to have certain frequentist error rates, which the Bayes factor is not. The Bayes factor, on the other hand, seems to be a more natural measure of evidence as it treats the null and alternative hypotheses symmetrically and represents the factor by which rational agents should update their beliefs in light of the data. Replication success is ideally evaluated along multiple dimensions, as nicely exemplified by the RPCB, EPRP, and RPP. Replications that are successful on multiple criteria provide more convincing support for the original finding, while replications that are successful on fewer criteria require closer examination. Fortunately, the use of multiple methods is already standard practice in replication assessment, so our proposal to use both of them does not require a major paradigm shift.</p>
<p>While the equivalence test and the Bayes factor are two principled methods for analyzing original and replication studies with null results, they are not the only possible methods for doing so. A straightforward extension would be to first synthesize the original and replication effect estimates with a meta-analysis, and then apply the equivalence and Bayes factor tests to the meta-analytic estimate similar to the meta-analytic non-significance criterion used by the RPCB. This could potentially improve the power of the tests, but consideration must be given to the threshold used for the <italic>p</italic>-values/Bayes factors, as naive use of the same thresholds as in the standard approaches may make the tests too liberal (<xref ref-type="bibr" rid="c61">Shun et al., 2005</xref>). Furthermore, there are various advanced methods for quantifying evidence for absent effects which could potentially improve on the more basic approaches considered here (<xref ref-type="bibr" rid="c41">Lindley, 1998</xref>; <xref ref-type="bibr" rid="c30">Johnson and Rossell, 2010</xref>; <xref ref-type="bibr" rid="c47">Morey and Rouder, 2011</xref>; <xref ref-type="bibr" rid="c36">Kruschke, 2018</xref>; <xref ref-type="bibr" rid="c63">Stahel, 2021</xref>; <xref ref-type="bibr" rid="c46">Micheloud and Held, 2023</xref>; <xref ref-type="bibr" rid="c28">Izbicki et al., 2023</xref>).</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank the RPCB, EPRP, and RPP contributors for their tremendous efforts and for making their data publicly available. We thank Maya Mathur for helpful advice on data preparation. We thank Benjamin Ineichen for helpful comments on drafts of the manuscript. We thank the three reviewers and the reviewing editor for useful comments that substantially improved the paper. Our acknowledgment of these individuals does not imply their endorsement of our work. We thank the Swiss National Science Foundation for financial support (grant #189295).</p>
</ack>
<sec id="s5">
<title>Software and data</title>
<p>The code and data to reproduce our analyses is openly available at <ext-link ext-link-type="uri" xlink:href="https://gitlab.uzh.ch/samuel.pawel/rsAbsence">https://gitlab.uzh.ch/samuel.pawel/rsAbsence</ext-link>. A snapshot of the repository at the time of writing is available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7906792">https://doi.org/10.5281/zenodo.7906792</ext-link>. We used the statistical programming language R version 4.3.2 (<xref ref-type="bibr" rid="c54">R Core Team, 2022</xref>) for analyses. The R packages ggplot2 (<xref ref-type="bibr" rid="c66">Wickham, 2016</xref>), dplyr (<xref ref-type="bibr" rid="c67">Wickham et al., 2022</xref>), knitr (<xref ref-type="bibr" rid="c68">Xie, 2022</xref>), and reporttools (<xref ref-type="bibr" rid="c57">Rufibach, 2009</xref>) were used for plotting, data preparation, dynamic reporting, and formatting, respectively. The data from the RPCB were obtained by downloading the files from <ext-link ext-link-type="uri" xlink:href="https://github.com/mayamathur/rpcb">https://github.com/mayamathur/rpcb</ext-link> (commit a1e0c63) and extracting the relevant variables as indicated in the R script preprocess-rpcb-data.R which is available in our git repository. The RPP and EPRP data were obtained from the RProjects data set available in the R package ReplicationSuccess (<xref ref-type="bibr" rid="c25">Held, 2020</xref>), see the package documentation (<ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=ReplicationSuccess">https://CRAN.R-project.org/package=ReplicationSuccess</ext-link>) for details on data extraction.</p>
</sec>
<app-group>
<app id="ap1">
<title>Appendix A: Sensitivity analyses</title>
<p>The post-hoc specification of equivalence margins Δ and prior distribution for the SMD under the alternative <italic>H</italic><sub>1</sub> is debatable. Commonly used margins in clinical research are much more stringent (<xref ref-type="bibr" rid="c38">Lange and Freitag, 2005</xref>); for instance, in oncology, a margin of Δ = log(1.3) is commonly used for log odds/hazard ratios, whereas in bioequivalence studies a margin of Δ = log(1.25) is the convention (<xref ref-type="bibr" rid="c60">Senn, 2021</xref>, Chapter 22). These margins would translate into margins of Δ = 0.14 and Δ = 0.12 on the SMD scale, respectively, using the SMD = (√3/π)log OR conversion (<xref ref-type="bibr" rid="c11">Cooper et al., 2019</xref>, p. 233). Similarly, for the Bayes factor we specified a normal unit-information prior under the alternative while other normal priors with smaller/larger standard deviations could have been considered. Here, we therefore investigate the sensitivity of our conclusions with respect to these parameters.</p>
<p>The top plot of <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the number of successful replications as a function of the margin Δ and for different TOST <italic>p</italic>-value thresholds. Such an “equivalence curve” approach was first proposed by <xref ref-type="bibr" rid="c24">Hauck and Anderson (1986)</xref>. We see that for realistic margins between 0 and 1, the proportion of replication successes remains below 50% for the conventional <italic>α</italic> = 0.05 level. To achieve a success rate of 11/15 = 73%, as was achieved with the non-significance criterion from the RPCB, unrealistic margins of Δ &gt; 2 are required. Changing the success criterion to a more lenient level (<italic>α</italic> = 0.1) or a more stringent level (<italic>α</italic> = 0.01) hardly changes the conclusion.</p>
    <fig id="fig4" position="float" fig-type="figure">
        <label>Figure 4</label>
        <caption><p>Number of successful replications of original null results in the RPCB as a function of the margin Δ of the equivalence test (<italic>p</italic><sub>TOST</sub> ≤ <italic>α</italic> in both studies for <italic>α</italic> = 0.1, 0.05, 0.01) or the standard deviation of the zero-mean normal prior distribution for the SMD effect size under the alternative <italic>H</italic><sub>1</sub> of the Bayes factor test (BF<sub>01</sub> ≥ <italic>γ</italic> in both studies for <italic>γ</italic> = 3, 6, 10).</p></caption>
        <graphic xlink:href="2305.04587v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
    </fig>
<p>The bottom plot of <xref ref-type="fig" rid="fig4">Figure 4</xref> shows a sensitivity analysis regarding the choice of the prior standard deviation and the Bayes factor threshold. It is uncommon to specify prior standard deviations larger than the unit-information standard deviation of 2, as this corresponds to the assumption of very large effect sizes under the alternatives. However, to achieve replication success for a larger proportion of replications than the observed 1/15 = 7%, unreasonably large prior standard deviations have to be specified. For instance, a standard deviation of roughly 5 is required to achieve replication success in 50% of the replications at a lenient Bayes factor threshold of <italic>γ</italic> = 3. The standard deviation needs to be almost 20 so that the same success rate 11/15 = 73% as with the nonsignificance criterion is achieved. The necessary standard deviations are even higher for stricter Bayes factor thresholds, such as <italic>γ</italic> = 6 or <italic>γ</italic> = 10.</p>
</app>
<app id="ap2">
<title>Appendix B: Null results from the RPP and EPRP</title>
<p>Here we perform equivalence test and Bayes factor analyses for the three original null results from the Reproducibility Project: Psychology (<xref ref-type="bibr" rid="c15">Eastwick and Finkel, 2008</xref>; <xref ref-type="bibr" rid="c55">Ranganath and Nosek, 2008</xref>; <xref ref-type="bibr" rid="c56">Reynolds and Besner, 2008</xref>) and the original null result from the Reproducibility Project: Experimental Philosophy (<xref ref-type="bibr" rid="c45">McCann, 2005</xref>). To enable comparison of effect sizes across different studies, both the RPP and the EPRP provided effect estimates as Fisher <italic>z</italic>-transformed correlations which we use in the following.</p>
<p><xref ref-type="fig" rid="fig5">Figure 5</xref> shows effect estimates with 90% confidence intervals, two-sided <italic>p</italic>-values for the null hypothesis that the effect size is zero, TOST <italic>p</italic>-values for a margin of Δ = 0.2, and Bayes factors using a normal prior centered around zero with a standard deviation of 2. We see that all replications except the replication of <xref ref-type="bibr" rid="c55">Ranganath and Nosek (2008)</xref> would be considered successful with the non-significance criterion, as the original and replication <italic>p</italic>-values are greater than 0.05. In all three cases, the Bayes factors also indicate substantial (BF<sub>01</sub> &gt; 3) to strong evidence (BF<sub>01</sub> &gt; 10) for the null hypothesis of no effect. Compared to the Bayes factors in the RPCB, the evidence is stronger, possibly due to the mostly larger sample sizes in the RPP and EPRP.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5</label>
<caption><p>Effect estimates on Fisher <italic>z</italic>-transformed correlation scale with 90% confidence interval for the “null results” and their replication studies from the Reproducibility Project: Psychology (RPP, <xref ref-type="bibr" rid="c50">Open Science Collaboration, 2015</xref>) and the Experimental Philosophy Replicability Project (EPRP, <xref ref-type="bibr" rid="c12">Cova et al., 2018</xref>). The dashed gray line represents the value of no effect (<italic>z</italic> = 0), while the dotted red lines represent the equivalence range with a margin of Δ = 0.74. The <italic>p</italic>-value <italic>p</italic><sub>TOST</sub> is the maximum of the two one-sided <italic>p</italic>-values for the null hypotheses of the effect being greater/less than +Δ and −Δ, respectively. The Bayes factor BF<sub>01</sub> quantifies the evidence for the null hypothesis <italic>H</italic><sub>0</sub> : z = 0 against the alternative <italic>H</italic><sub>1</sub> : z ≠ 0 with normal prior centered around zero and standard deviation of 2 assigned to the effect size under <italic>H</italic><sub>1</sub>.</p></caption>
<graphic xlink:href="2305.04587v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Interestingly, the opposite conclusion is reached when we analyze the data using an equivalence test with a margin of Δ = 0.2 (which may be considered liberal as it represents a small to medium effect based on the <xref ref-type="bibr" rid="c10">Cohen, 1992</xref> convention). In this case, equivalence at the 5% level can only be established for the <xref ref-type="bibr" rid="c55">Ranganath and Nosek (2008)</xref> original study and its replication simultaneously, as the confidence intervals from the other studies are too wide to be included in the equivalence range. Furthermore, the <xref ref-type="bibr" rid="c55">Ranganath and Nosek (2008)</xref> replication also illustrates the conceptual difference between testing for an <italic>exactly zero</italic> effect versus testing for an effect <italic>within an interval around zero</italic>. That is, the Bayes factor indicates no evidence for a zero effect (because the estimate is clearly not zero), but the equivalence test indicates evidence for a negligible effect (because the estimate is clearly within the equivalence range).</p>
<p>As before, the particular choices of the equivalence margin Δ for the equivalence test and prior standard deviation of the Bayes factor are debatable. We therefore report sensitivity analyses in <xref ref-type="fig" rid="fig6">Figure 6</xref> which show the TOST <italic>p</italic>-values and Bayes factors of original and replication studies for a range of margins and prior standard deviations, respectively. Apart from the <xref ref-type="bibr" rid="c55">Ranganath and Nosek (2008)</xref> study pair, all studies require large margins of about Δ = 0.4 to establish replication success at the 5% level (in the sense of original and replication TOST <italic>p</italic>-values being smaller than 0.05). On the other hand, in all but the <xref ref-type="bibr" rid="c55">Ranganath and Nosek (2008)</xref> replication, the data provide substantial evidence for a null effect (BF<sub>01</sub> &gt; 3) for prior standard deviations of about one, while larger prior standard deviations of about three are required for the data to indicate strong evidence (BF<sub>01</sub> &gt; 10) for a null effect, whereas the data from the <xref ref-type="bibr" rid="c55">Ranganath and Nosek (2008)</xref> replication provide very strong evidence against a null effect for all prior standard deviations considered.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6</label>
<caption><p>Sensitivity analyses for the “null results” and their replication studies from the Reproducibility Project: Psychology (RPP, <xref ref-type="bibr" rid="c50">Open Science Collaboration, 2015</xref>) and the Experimental Philosophy Replicability Project (EPRP, <xref ref-type="bibr" rid="c12">Cova et al., 2018</xref>). The Bayes factor of the replication of <xref ref-type="bibr" rid="c55">Ranganath and Nosek (2008)</xref> decreases very quickly and is only shown for a limited range.</p></caption>
<graphic xlink:href="2305.04587v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</app>
<app id="ap3">
<title>Appendix C: Technical details on Bayes factors</title>
<p>We assume that effect estimates are normally distributed around an unknown effect size <italic>θ</italic> with known variance equal to their squared standard error, i.e.,
<disp-formula id="ed4"><alternatives><mml:math display="block" id="e4"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo>~</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="2305.04587v3_eqn4.tif" mimetype="image" mime-subtype="tiff"/></alternatives></disp-formula>
for original (<italic>i = o</italic>) and replication (<italic>i = r</italic>). This framework is similar to meta-analysis and can be applied to many types of effect sizes and data (<xref ref-type="bibr" rid="c62">Spiegelhalter et al., 2004</xref>, Section 2.4). We want to quantify the evidence for the null hypothesis that the effect size is equal to a null effect (<italic>H</italic><sub>0</sub> : <italic>θ = θ</italic><sub>0</sub>, typically <italic>θ</italic><sub>0</sub> = 0) against the alternative hypothesis that the effect size is non-null (<italic>H</italic><sub>1</sub> : <italic>θ</italic> ≠ <italic>θ</italic><sub>0</sub>). This requires specification of a prior distribution for the effect size under the alternative, and we will assume a normal prior <italic>θ</italic> | <italic>H</italic><sub>1</sub> ~ N(<italic>m, s</italic><sup>2</sup>) in the following. The Bayes factor based on an effect estimate is then given by the ratio of its likelihood under the null hypothesis to its marginal likelihood under the alternative hypothesis, i.e.,
<disp-formula id="ed5"><alternatives><mml:math display="block" id="e5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mtext>BF</mml:mtext><mml:mrow><mml:mn>01</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>  </mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>  </mml:mtext><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mi>exp</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="2305.04587v3_eqn5.tif" mimetype="image" mime-subtype="tiff"/></alternatives></disp-formula></p>
<p>In the main analysis we used a normal unit-information prior, that is, a normal distribution centered around the value of no effect (<italic>m</italic> = 0) with a standard deviation <italic>s</italic> corresponding to the standard error of an SMD estimate based on one observation (<xref ref-type="bibr" rid="c32">Kass and Wasserman, 1995</xref>). Assuming that the group means are normally distributed <inline-formula id="I2"><alternatives><mml:math display="inline" id="IN2"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>~</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="2305.04587v3_ieq2.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula> and <inline-formula id="I3"><alternatives><mml:math display="inline" id="IN3"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>~</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="2305.04587v3_ieq3.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula> with <italic>n</italic> the total sample size and <italic>τ</italic> the known data standard deviation, the distribution of the SMD is <inline-formula id="I4"><alternatives><mml:math display="inline" id="IN4"><mml:mrow><mml:mtext>SMD</mml:mtext><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>τ</mml:mi><mml:mo>~</mml:mo><mml:mtext>N</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="2305.04587v3_ieq4.tif" mimetype="image" mime-subtype="tiff"/></alternatives></inline-formula>. The standard error σ of the SMD based on one unit (<italic>n</italic> = 1), is hence 2, meaning that the standard deviation of the unit-information prior is <italic>s</italic> = 2.</p>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altman</surname>, <given-names>D. G.</given-names></string-name> and <string-name><surname>Bland</surname>, <given-names>J. M.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Statistics notes: Absence of evidence is not evidence of absence</article-title>. <source>BMJ</source>, <volume>311</volume>(<issue>7003</issue>):<fpage>485</fpage>–<lpage>485</lpage>. doi:<pub-id pub-id-type="doi">10.1136/bmj.311.7003.485</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amaral</surname>, <given-names>O. B.</given-names></string-name>, <string-name><surname>Neves</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wasilewska-Sampaio</surname>, <given-names>A. P.</given-names></string-name>, and <string-name><surname>Carneiro</surname>, <given-names>C. F.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Science forum: The Brazilian reproducibility initiative</article-title>. <source>eLife</source>, <volume>8</volume>. doi:<pub-id pub-id-type="doi">10.7554/elife.41602</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>S. F.</given-names></string-name> and <string-name><surname>Kelley</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Sample size planning for replication studies: The devil is in the design</article-title>. <source>Psychological Methods</source>. doi:<pub-id pub-id-type="doi">10.1037/met0000520</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>S. F.</given-names></string-name> and <string-name><surname>Maxwell</surname>, <given-names>S. E.</given-names></string-name></person-group> (<year>2016</year>). <article-title>There's more than one way to conduct a replication study: Beyond statistical significance</article-title>. <source>Psychological Methods</source>, <volume>21</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>12</lpage>. doi:<pub-id pub-id-type="doi">10.1037/met0000051</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Begley</surname>, <given-names>C. G.</given-names></string-name> and <string-name><surname>Ellis</surname>, <given-names>L. M.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Raise standards for preclinical cancer research</article-title>. <source>Nature</source>, <volume>483</volume>(<issue>7391</issue>):<fpage>531</fpage>–<lpage>533</lpage>. doi:<pub-id pub-id-type="doi">10.1038/483531a</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Camerer</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Dreber</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Forsell</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ho</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Huber</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Johannesson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kirchler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Almenberg</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Altmejd</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2016</year>). <article-title>Evaluating replicability of laboratory experiments in economics</article-title>. <source>Science</source>, <volume>351</volume>:<fpage>1433</fpage>–<lpage>1436</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.aaf0918</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Camerer</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Dreber</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Holzmeister</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ho</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Huber</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Johannesson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kirchler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nave</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Nosek</surname>, <given-names>B.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2018</year>). <article-title>Evaluating the replicability of social science experiments in nature and science between 2010 and 2015</article-title>. <source>Nature Human Behavior</source>, <volume>2</volume>:<fpage>637</fpage>–<lpage>644</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41562-018-0399-z</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Campbell</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Gustafson</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Conditional equivalence testing: An alternative remedy for publication bias</article-title>. <source>PLOS ONE</source>, <volume>13</volume>(<issue>4</issue>):<fpage>e0195145</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0195145</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Campbell</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Gustafson</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2021</year>). <article-title>What to make of equivalence testing with a post-specified margin?</article-title> <source>Meta-Psychology</source>, <volume>5</volume>. doi:<pub-id pub-id-type="doi">10.15626/mp.2020.2506</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1992</year>). <article-title>A power primer</article-title>. <source>Psychological Bulletin</source>, <volume>112</volume>(<issue>1</issue>):<fpage>155</fpage>–<lpage>159</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0033-2909.112.1.155</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><surname>Cooper</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hedges</surname>, <given-names>L. V.</given-names></string-name>, and <string-name><surname>Valentine</surname>, <given-names>J. C.</given-names></string-name></person-group>, editors (<year>2019</year>). <source>The Handbook of Research Synthesis and MetaAnalysis</source>. <publisher-name>Russell Sage Foundation</publisher-name>. doi:<pub-id pub-id-type="doi">10.7758/9781610448864</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cova</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Strickland</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Abatista</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Allard</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Andow</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Attie</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Beebe</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Berniūnas</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Boudesseul</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Colombo</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2018</year>). <article-title>Estimating the reproducibility of experimental philosophy</article-title>. <source>Review of Philosophy and Psychology</source>. doi:<pub-id pub-id-type="doi">10.1007/s13164-018-0400-9</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dawson</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Prinjha</surname>, <given-names>R. K.</given-names></string-name>, <string-name><surname>Dittmann</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Giotopoulos</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bantscheff</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chan</surname>, <given-names>W.-I.</given-names></string-name>, <string-name><surname>Robson</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>wa Chung</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hopf</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Savitski</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Huthmacher</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gudgin</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lugo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Beinke</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Chapman</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Roberts</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Soden</surname>, <given-names>P. E.</given-names></string-name>, <string-name><surname>Auger</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Mirguet</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Doehner</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Delwel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Burnett</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Jeffrey</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Drewes</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Huntly</surname>, <given-names>B. J. P.</given-names></string-name>, and <string-name><surname>Kouzarides</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Inhibition of BET recruitment to chromatin as an effective treatment for MLL-fusion leukaemia</article-title>. <source>Nature</source>, <volume>478</volume>(<issue>7370</issue>):<fpage>529</fpage>–<lpage>533</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature10509</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dienes</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Using Bayes to get the most out of non-significant results</article-title>. <source>Frontiers in Psychology</source>, <volume>5</volume>. doi:<pub-id pub-id-type="doi">10.3389/fpsyg.2014.00781</pub-id>. URL <pub-id pub-id-type="doi">10.3389/fpsyg.2014.00781</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eastwick</surname>, <given-names>P. W.</given-names></string-name> and <string-name><surname>Finkel</surname>, <given-names>E. J.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Sex differences in mate preferences revisited: Do people know what they initially desire in a romantic partner?</article-title> <source>Journal of Personality and Social Psychology</source>, <volume>94</volume>(<issue>2</issue>):<fpage>245</fpage>–<lpage>264</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0022-3514.94.2.245</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Errington</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Mathur</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Soderberg</surname>, <given-names>C. K.</given-names></string-name>, <string-name><surname>Denis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Perfito</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Iorns</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Nosek</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Investigating the replicability of preclinical cancer biology</article-title>. <source>eLife</source>, <volume>10</volume>. doi:<pub-id pub-id-type="doi">10.7554/elife.71601</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flight</surname>, <given-names>L.</given-names></string-name> and <string-name><surname>Julious</surname>, <given-names>S. A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Practical guide to sample size calculations: non-inferiority and equivalence trials</article-title>. <source>Pharmaceutical Statistics</source>, <volume>15</volume>(<issue>1</issue>):<fpage>80</fpage>–<lpage>89</lpage>. doi:<pub-id pub-id-type="doi">10.1002/pst.1716</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goetz</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Minguet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Navarro-Lérida</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Lazcano</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Samaniego</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Calvo</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Tello</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Osteso-Ibáñez</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Pellinen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Echarri</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cerezo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Klein-Szanto</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Garcia</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Keely</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Sánchez-Mateos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cukierman</surname>, <given-names>E.</given-names></string-name>, and <string-name><surname>Pozo</surname>, <given-names>M. A. D.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Biomechanical remodeling of the microenvironment by stromal caveolin-1 favors tumor invasion and metastasis</article-title>. <source>Cell</source>, <volume>146</volume>(<issue>1</issue>):<fpage>148</fpage>–<lpage>163</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2011.05.040</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodman</surname>, <given-names>S. N.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Toward evidence-based medical statistics. 2: The Bayes factor</article-title>. <source>Annals of Internal Medicine</source>, <volume>130</volume>(<issue>12</issue>):<fpage>1005</fpage>. doi:<pub-id pub-id-type="doi">10.7326/0003-4819-130-12-199906150-00019</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodman</surname>, <given-names>S. N.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Introduction to Bayesian methods I: measuring the strength of evidence</article-title>. <source>Clinical Trials</source>, <volume>2</volume>(<issue>4</issue>):<fpage>282</fpage>–<lpage>290</lpage>. doi:<pub-id pub-id-type="doi">10.1191/1740774505cn098oa</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenland</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Null misinterpretation in statistical testing and its impact on health risk assessment</article-title>. <source>Preventive Medicine</source>, <volume>53</volume>:<fpage>225</fpage>–<lpage>228</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.ypmed.2011.08.010</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenland</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Nonsignificance plus high power does not imply support for the null over the alternative</article-title>. <source>Annals of Epidemiology</source>, <volume>22</volume>(<issue>5</issue>):<fpage>364</fpage>–<lpage>368</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.annepidem.2012.02.007</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenland</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Divergence versus decision <italic>P</italic>-values: A distinction worth making in theory and keeping in practice: Or, how divergence <italic>P</italic>-values measure evidence even when decision <italic>P</italic>-values do not</article-title>. <source>Scandinavian Journal of Statistics</source>, <volume>50</volume>(<issue>1</issue>):<fpage>54</fpage>–<lpage>88</lpage>. doi:<pub-id pub-id-type="doi">10.1111/sjos.12625</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hauck</surname>, <given-names>W. W.</given-names></string-name> and <string-name><surname>Anderson</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1986</year>). <article-title>A proposal for interpreting and reporting negative studies</article-title>. <source>Statistics in Medicine</source>, <volume>5</volume>(<issue>3</issue>):<fpage>203</fpage>–<lpage>209</lpage>. doi:<pub-id pub-id-type="doi">10.1002/sim.4780050302</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Held</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A new standard for the analysis and design of replication studies (with discussion)</article-title>. <source>Journal of the Royal Statistical Society: Series A (Statistics in Society</source>), <volume>183</volume>(<issue>2</issue>):<fpage>431</fpage>–<lpage>448</lpage>. doi:<pub-id pub-id-type="doi">10.1111/rssa.12493</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Held</surname>, <given-names>L.</given-names></string-name> and <string-name><surname>Ott</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>On <italic>p</italic>-values and Bayes factors</article-title>. <source>Annual Review of Statistics and Its Application</source>, <volume>5</volume>(<issue>1</issue>):<fpage>393</fpage>–<lpage>419</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-statistics-031017-100307</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoenig</surname>, <given-names>J. M.</given-names></string-name> and <string-name><surname>Heisey</surname>, <given-names>D. M.</given-names></string-name></person-group> (<year>2001</year>). <article-title>The abuse of power</article-title>. <source>The American Statistician</source>, <volume>55</volume>(<issue>1</issue>):<fpage>19</fpage>–<lpage>24</lpage>. doi:<pub-id pub-id-type="doi">10.1198/000313001300339897</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Izbicki</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Cabezas</surname>, <given-names>L. M. C.</given-names></string-name>, <string-name><surname>Colugnatti</surname>, <given-names>F. A. B.</given-names></string-name>, <string-name><surname>Lassance</surname>, <given-names>R. F. L.</given-names></string-name>, <string-name><surname>de Souza</surname>, <given-names>A. A. L.</given-names></string-name>, and <string-name><surname>Stern</surname>, <given-names>R. B.</given-names></string-name></person-group> (<year>2023</year>). <source>Rethinking hypothesis tests</source>. <comment>Preprint</comment>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Jeffreys</surname>, <given-names>H.</given-names></string-name></person-group> (<year>1961</year>). <source>Theory of Probability</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Clarendon Press</publisher-name>, <edition>third edition</edition>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>V. E.</given-names></string-name> and <string-name><surname>Rossell</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2010</year>). <article-title>On the use of non-local prior densities in Bayesian hypothesis tests</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology</source>), <volume>72</volume>(<issue>2</issue>):<fpage>143</fpage>–<lpage>170</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1467-9868.2009.00730.x</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kass</surname>, <given-names>R. E.</given-names></string-name> and <string-name><surname>Raftery</surname>, <given-names>A. E.</given-names></string-name></person-group> (<year>1995</year>). <article-title>Bayes factors</article-title>. <source>Journal of the American Statistical Association</source>, <volume>90</volume>(<issue>430</issue>):<fpage>773</fpage>–<lpage>795</lpage>. doi:<pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kass</surname>, <given-names>R. E.</given-names></string-name> and <string-name><surname>Wasserman</surname>, <given-names>L.</given-names></string-name></person-group> (<year>1995</year>). <article-title>A reference Bayesian test for nested hypotheses and its relationship to the Schwarz criterion</article-title>. <source>Journal of the American Statistical Association</source>, <volume>90</volume>(<issue>431</issue>):<fpage>928</fpage>–<lpage>934</lpage>. doi:<pub-id pub-id-type="doi">10.1080/01621459.1995.10476592</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keysers</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gazzola</surname>, <given-names>V.</given-names></string-name>, and <string-name><surname>Wagenmakers</surname>, <given-names>E.-J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Using Bayes factor hypothesis testing in neuroscience to establish evidence of absence</article-title>. <source>Nature Neuroscience</source>, <volume>23</volume>(<issue>7</issue>):<fpage>788</fpage>–<lpage>799</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41593-020-0660-4</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Ratliff</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Vianello</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Adams</surname>, <given-names>R. B.</given-names></string-name>, <string-name><surname>Bahník</surname>, <given-names>v.</given-names></string-name>, <string-name><surname>Bernstein</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Bocian</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Brandt</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Brooks</surname>, <given-names>B.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2014</year>). <article-title>Investigating variation in replicability: A “many labs” replication project</article-title>. <source>Social Psychology</source>, <volume>45</volume>:<fpage>142</fpage>–<lpage>152</lpage>. doi:<pub-id pub-id-type="doi">10.1027/1864-9335/a000178</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Vianello</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hasselman</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Adams</surname>, <given-names>B. G.</given-names></string-name>, <string-name><surname>Reginald</surname> <given-names>B.</given-names></string-name>, <string-name><surname>Adams</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Alper</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Aveyard</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Axt</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Babalola</surname>, <given-names>M. T.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2018</year>). <article-title>Many labs 2: Investigating variation in replicability across samples and settings</article-title>. <source>Advances in Methods and Practices in Psychological Science</source>, <volume>1</volume>(<issue>4</issue>):<fpage>443</fpage>–<lpage>490</lpage>. doi:<pub-id pub-id-type="doi">10.1177/2515245918810225</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kruschke</surname>, <given-names>J. K.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Rejecting or accepting parameter values in Bayesian estimation</article-title>. <source>Advances in Methods and Practices in Psychological Science</source>, <volume>1</volume>(<issue>2</issue>):<fpage>270</fpage>–<lpage>280</lpage>. doi:<pub-id pub-id-type="doi">10.1177/2515245918771304</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lakens</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Equivalence tests</article-title>. <source>Social Psychological and Personality Science</source>, <volume>8</volume>(<issue>4</issue>):<fpage>355</fpage>–<lpage>362</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1948550617697177</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lange</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Freitag</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Choice of delta: Requirements and reality – results of a systematic review</article-title>. <source>Biometrical Journal</source>, <volume>47</volume>(<issue>1</issue>):<fpage>12</fpage>–<lpage>27</lpage>. doi:<pub-id pub-id-type="doi">10.1002/bimj.200410085</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewis</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Edwards</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Meyers</surname>, <given-names>Z. R.</given-names></string-name>, <string-name><surname>Talbot</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Hao</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Blum</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Iorns</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Tsui</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Denis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Perfito</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Errington</surname>, <given-names>T. M.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Replication study: Transcriptional amplification in tumor cells with elevated c-Myc</article-title>. <source>eLife</source>, <volume>7</volume>. doi:<pub-id pub-id-type="doi">10.7554/elife.30274</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname>, <given-names>C. Y.</given-names></string-name>, <string-name><surname>Loven</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rahl</surname>, <given-names>P. B.</given-names></string-name>, <string-name><surname>Paranal</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Burge</surname>, <given-names>C. B.</given-names></string-name>, <string-name><surname>Bradner</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>T. I.</given-names></string-name>, and <string-name><surname>Young</surname>, <given-names>R. A.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Transcriptional amplification in tumor cells with elevated c-Myc</article-title>. <source>Cell</source>, <volume>151</volume>(<issue>1</issue>):<fpage>56</fpage>–<lpage>67</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cell.2012.08.026</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lindley</surname>, <given-names>D. V.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Decision analysis and bioequivalence trials</article-title>. <source>Statistical Science</source>, <volume>13</volume>(<issue>2</issue>). doi:<pub-id pub-id-type="doi">10.1214/ss/1028905932</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Makin</surname>, <given-names>T. R.</given-names></string-name> and <string-name><surname>de Xivry</surname>, <given-names>J.-J. O.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Ten common statistical mistakes to watch out for when writing or reviewing a manuscript</article-title>. <source>eLife</source>, <volume>8</volume>. doi:<pub-id pub-id-type="doi">10.7554/elife.48175</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathur</surname>, <given-names>M. B.</given-names></string-name> and <string-name><surname>VanderWeele</surname>, <given-names>T. J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>New statistical metrics for multisite replication projects</article-title>. <source>Journal of the Royal Statistical Society: Series A (Statistics in Society</source>), <volume>183</volume>(<issue>3</issue>):<fpage>1145</fpage>–<lpage>1166</lpage>. doi:<pub-id pub-id-type="doi">10.1111/rssa.12572</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Matthews</surname>, <given-names>J. N.</given-names></string-name></person-group> (<year>2006</year>). <source>Introduction to Randomized Controlled Clinical Trials</source>. <publisher-name>Chapman and Hall/CRC</publisher-name>, <publisher-loc>New York</publisher-loc>. doi:<pub-id pub-id-type="doi">10.1201/9781420011302</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCann</surname>, <given-names>H. J.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Intentional action and intending: Recent empirical studies</article-title>. <source>Philosophical Psychology</source>, <volume>18</volume>(<issue>6</issue>):<fpage>737</fpage>–<lpage>748</lpage>. doi:<pub-id pub-id-type="doi">10.1080/09515080500355236</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Micheloud</surname>, <given-names>C.</given-names></string-name> and <string-name><surname>Held</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2023</year>). <source>The replication of equivalence studies</source>. doi:<pub-id pub-id-type="doi">10.48550/ARXIV.2204.06960</pub-id>. <comment>arXiv preprint</comment>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morey</surname>, <given-names>R. D.</given-names></string-name> and <string-name><surname>Rouder</surname>, <given-names>J. N.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Bayes factor approaches for testing interval null hypotheses</article-title>. <source>Psychological Methods</source>, <volume>16</volume>(<issue>4</issue>):<fpage>406</fpage>–<lpage>419</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0024377</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murphy</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mesquida</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Caldwell</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Earp</surname>, <given-names>B. D.</given-names></string-name>, and <string-name><surname>Warne</surname>, <given-names>J. P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Proposal of a selection protocol for replication of studies in sports and exercise science</article-title>. <source>Sports Medicine</source>, <volume>53</volume>(<issue>1</issue>):<fpage>281</fpage>–<lpage>291</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s40279-022-01749-1</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="book"><person-group person-group-type="author"><collab>National Academies of Sciences, Engineering, and Medicine</collab></person-group> (<year>2019</year>). <source>Reproducibility and Replicability in Science</source>. <publisher-name>National Academies Press</publisher-name>. doi:<pub-id pub-id-type="doi">10.17226/25303</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group> (<year>2015</year>). <article-title>Estimating the reproducibility of psychological science</article-title>. <source>Science</source>, <volume>349</volume>(<issue>6251</issue>):<fpage>aac4716</fpage>. doi:<pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patil</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Peng</surname>, <given-names>R. D.</given-names></string-name>, and <string-name><surname>Leek</surname>, <given-names>J. T.</given-names></string-name></person-group> (<year>2016</year>). <article-title>What should researchers expect when they replicate studies? A statistical view of replicability in psychological science</article-title>. <source>Perspectives on Psychological Science</source>, <volume>11</volume>(<issue>4</issue>):<fpage>539</fpage>–<lpage>544</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1745691616646366</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pawel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Consonni</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Held</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Bayesian approaches to designing replication studies</article-title>. <source>Psychological Methods</source>. doi:<pub-id pub-id-type="doi">10.1037/met0000604</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prinz</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Schlange</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Asadullah</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Believe it or not: how much can we rely on published data on potential drug targets?</article-title> <source>Nature Reviews Drug Discovery</source>, <volume>10</volume>(<issue>9</issue>):<fpage>712</fpage>–<lpage>712</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrd3439-c1</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="book"><person-group person-group-type="author"><collab>R Core Team</collab></person-group> (<year>2022</year>). <source>R: A Language and Environment for Statistical Computing</source>. <publisher-name>R Foundation for Statistical Computing</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc>. URL <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ranganath</surname>, <given-names>K. A.</given-names></string-name> and <string-name><surname>Nosek</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Implicit attitude generalization occurs immediately; explicit attitude generalization takes time</article-title>. <source>Psychological Science</source>, <volume>19</volume>(<issue>3</issue>):<fpage>249</fpage>–<lpage>254</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02076.x</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reynolds</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Besner</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Contextual effects on reading aloud: Evidence for pathway control</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>34</volume>(<issue>1</issue>):<fpage>50</fpage>–<lpage>64</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0278-7393.34.1.50</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rufibach</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2009</year>). <article-title>reporttools: R functions to generate LATEX tables of descriptive statistics</article-title>. <source>Journal of Statistical Software, Code Snippets</source>, <volume>31</volume>(<issue>1</issue>). doi:<pub-id pub-id-type="doi">10.18637/jss.v031.c01</pub-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schauer</surname>, <given-names>J. M.</given-names></string-name> and <string-name><surname>Hedges</surname>, <given-names>L. V.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Reconsidering statistical methods for assessing replication</article-title>. <source>Psychological Methods</source>, <volume>26</volume>(<issue>1</issue>):<fpage>127</fpage>–<lpage>139</lpage>. doi:<pub-id pub-id-type="doi">10.1037/met0000302</pub-id>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schuirmann</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>1987</year>). <article-title>A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability</article-title>. <source>Journal of Pharmacokinetics and Biopharmaceutics</source>, <volume>15</volume>(<issue>6</issue>):<fpage>657</fpage>–<lpage>680</lpage>. doi:<pub-id pub-id-type="doi">10.1007/bf01068419</pub-id>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Senn</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2021</year>). <source>Statistical Issues in Drug Development</source>. <publisher-name>Wiley</publisher-name>. doi:<pub-id pub-id-type="doi">10.1002/9781119238614</pub-id>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shun</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Chi</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Durrleman</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Fisher</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Statistical consideration of the strategy for demonstrating clinical evidence of effectiveness-one larger vs two smaller pivotal studies</article-title>. <source>Statistics in Medicine</source>, <volume>24</volume>(<issue>11</issue>):<fpage>1619</fpage>–<lpage>1637</lpage>. doi:<pub-id pub-id-type="doi">10.1002/sim.2015</pub-id>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Spiegelhalter</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Abrams</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Myles</surname>, <given-names>J. P.</given-names></string-name></person-group> (<year>2004</year>). <source>Bayesian Approaches to Clinical Trials and Health-Care Evaluation</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stahel</surname>, <given-names>W. A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>New relevance and significance measures to replace p-values</article-title>. <source>PLOS ONE</source>, <volume>16</volume>(<issue>6</issue>):<fpage>e0252991</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0252991</pub-id>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wellek</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2010</year>). <source>Testing statistical hypotheses of equivalence and noninferiority</source>. <publisher-name>CRC press</publisher-name>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Westlake</surname>, <given-names>W. J.</given-names></string-name></person-group> (<year>1972</year>). <article-title>Use of confidence intervals in analysis of comparative bioavailability trials</article-title>. <source>Journal of Pharmaceutical Sciences</source>, <volume>61</volume>(<issue>8</issue>):<fpage>1340</fpage>–<lpage>1341</lpage>. doi:<pub-id pub-id-type="doi">10.1002/jps.2600610845</pub-id>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wickham</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2016</year>). <source>ggplot2: Elegant Graphics for Data Analysis</source>. <publisher-name>Springer International Publishing</publisher-name>. doi:<pub-id pub-id-type="doi">10.1007/978-3-319-24277-4</pub-id>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Wickham</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Francois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Henry</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Muller</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2022</year>). <source>dplyr: A Grammar of Data Manipulation</source>. URL <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=dplyr">https://CRAN.R-project.org/package=dplyr</ext-link>. <comment>R package version 1.0.10</comment>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Xie</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2022</year>). <source>knitr: A General-Purpose Package for Dynamic Report Generation in R</source>. URL <ext-link ext-link-type="uri" xlink:href="https://yihui.org/knitr/">https://yihui.org/knitr/</ext-link>. <comment>R package version 1.40</comment>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1"><label>1</label><p>There are four original studies with null effects for which two or three “internal” replication studies were conducted, leading in total to 20 replications of null effects. As done in the RPCB main analysis (<xref ref-type="bibr" rid="c16">Errington et al., 2021</xref>), we aggregated their SMD estimates into a single SMD estimate with fixed-effect meta-analysis and recomputed the replication <italic>p</italic>-value based on a normal approximation. For the original studies and the single replication studies we report the SMD estimates and <italic>p</italic>-values as provided by the RPCB.</p></fn>
</fn-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92311.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Boonstra</surname>
<given-names>Philip</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Michigan</institution>
</institution-wrap>
<city>Ann Arbor</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>By assessing what it means to replicate a null finding, and by proposing two methods that can be used to evaluate whether null findings have been replicated (frequentist equivalence testing, and Bayes factors), this article represents an <bold>important</bold> contribution to work on reproducibility. Through a <bold>compelling</bold> re-analysis of results from the Reproducibility Project: Cancer Biology, the authors demonstrate that even when 'replication success' is reduced to a single criterion, different methods to assess replication of a null finding can lead to different conclusions.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92311.2.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The goal of Pawel et al. is to provide a more rigorous and quantitative approach for judging whether or not an initial null finding (conventionally with p &gt;= 0.05) has been replicated by a second similarly null finding. They discuss important objections to relying on the qualitative significant/non-significant dichotomy to make this judgement. They present two complementary methods (one frequentist and the other Bayesian) which provide a superior quantitative framework for assessing the replicability of null findings.</p>
<p>Strengths:</p>
<p>
Clear presentation; illuminating examples drawn from the well-known Reproducibility Project: Cancer Biology data set; R-code that implements suggested analyses. Using both methods as suggested provides a superior procedure for judging the replicability of null findings.</p>
<p>Weaknesses:</p>
<p>
The frequentist and the Bayesian methods can be used to make binary assessments of an original finding and its replication. The authors clarify, though, that they can also be used to make continuous quantitative judgements about strength of evidence. I believe that most will use the methods in a binary fashion, but the availability of more nuanced assessments is welcome. This revision has addressed what I initially considered a weakness.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92311.2.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary and strengths:</p>
<p>1. The work provides significant insights because usually non-significant studies can be considered replicated by their null replications as well. The work discuss and provide data demonstrating that when analyzing studies with p &gt; 0.05 for the result to be replicated, equivalence tests and bayes factor approaches are more suitable, since studies can be underpowered even if replications use larger samples than their original studies in general. Non-significant p-values are highly expected even with 80% of power for a true effect.</p>
<p>2. The evidence used features methods and analyses more rigorous than current state-of-the-art research on replicability.</p>
<p>Weaknesses:</p>
<p>
I am satisfied with the revisions made by the authors in response to my initial suggestions, as well as their subsequent responses to my observations throughout the reviewing process.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92311.2.sa3</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Pawel</surname>
<given-names>Samuel</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Heyard</surname>
<given-names>Rachel</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Micheloud</surname>
<given-names>Charlotte</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Held</surname>
<given-names>Leonhard</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>eLife assessment</bold></p>
<p>This work provides a valuable contribution and assessment of what it means to replicate a null study finding, and what are the appropriate methods for doing so (apart from a rote p-value assessment). Through a convincing re-analysis of results from the Reproducibility Project: Cancer Biology using frequentist equivalence testing and Bayes factors, the authors demonstrate that even when reducing 'replicability success' to a single criterion, how precisely replication is measured may yield differing results. Less focus is directed to appropriate replication of non-null findings.</p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The goal of Pawel et al. is to provide a more rigorous and quantitative approach for judging whether or not an initial null finding (conventionally with p ≥ 0.05) has been replicated by a second similarly null finding. They discuss important objections to relying on the qualitative significant/non-significant dichotomy to make this judgment. They present two complementary methods (one frequentist and the other Bayesian) which provide a superior quantitative framework for assessing the replicability of null findings.</p>
<p>Strengths:</p>
<p>Clear presentation; illuminating examples drawn from the well-known Reproducibility Project: Cancer Biology data set; R-code that implements suggested analyses. Using both methods as suggested provides a superior procedure for judging the replicability of null findings.</p>
<p>Weaknesses:</p>
<p>The proposed frequentist and the Bayesian methods both rely on binary assessments of an original finding and its replication. I'm not sure if this is a weakness or is inherent to making binary decisions based on continuous data.</p>
<p>For the frequentist method, a null finding is considered replicated if the original and replication 90% confidence intervals for the effects both fall within the equivalence range. According to this approach, a null finding would be considered replicated if p-values of both equivalences tests (original and replication) were, say, 0.049, whereas would not be considered replicated if, for example, the equivalence test of the original study had a p-value of 0.051 and the replication had a p-value of 0.001. Intuitively, the evidence for replication would seem to be stronger in the second instance. The recommended Bayesian approach similarly relies on a dichotomy (e.g., Bayes factor &gt; 1).</p>
</disp-quote>
<p>Thanks for the suggestions, we now emphasize more strongly in the “Methods for assessing replicability of null results” and “Conclusions” sections that both TOST p-values and Bayes factors are quantitative measures of evidence that do not require dichotomization into “success” or “failure”.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>The study demonstrates how inconclusive replications of studies initially with p &gt; 0.05 can be and employs equivalence tests and Bayesian factor approaches to illustrate this concept. Interestingly, the study reveals that achieving a success rate of 11 out of 15, or 73%, as was accomplished with the non-significance criterion from the RPCB (Reproducibility Project: Cancer Biology), requires unrealistic margins of Δ &gt; 2 for equivalence testing.</p>
<p>Strengths:</p>
<p>The study uses reliable and shareable/open data to demonstrate its findings, sharing as well the code for statistical analysis. The study provides sensitivity analysis for different scenarios of equivalence margin and alfa level, as well as for different scenarios of standard deviations for the prior of Bayes factors and different thresholds to consider. All analysis and code of the work is open and can be replicated. As well, the study demonstrates on a case-by-case basis how the different criteria can diverge, regarding one sample of a field of science: preclinical cancer biology. It also explains clearly what Bayes factors and equivalence tests are.</p>
<p>Weaknesses:</p>
<p>It would be interesting to investigate whether using Bayes factors and equivalence tests in addition to p-values results in a clearer scenario when applied to replication data from other fields. As mentioned by the authors, the Reproducibility Project: Experimental Philosophy (RPEP) and the Reproducibility Project: Psychology (RPP) have data attempting to replicate some original studies with null results. While the RPCB analysis yielded a similar picture when using both criteria, it is worth exploring whether this holds true for RPP and RPEP. Considerations for further research in this direction are suggested. Even if the original null results were excluded in the calculation of an overall replicability rate based on significance, sensitivity analyses considering them could have been conducted. The present authors can demonstrate replication success using the significance criteria in these two projects with initially p &lt; 0.05 studies, both positive and non-positive.</p>
<p>Other comments:</p>
<list list-type="bullet">
<list-item><p>Introduction: The study demonstrates how inconclusive replications of studies initially with p &gt; 0.05 can be and employs equivalence tests and Bayesian factor approaches to illustrate this concept. Interestingly, the study reveals that achieving a success rate of 11 out of 15, or 73%, as was accomplished with the non-significance criterion from the RPCB (Reproducibility Project: Cancer Biology), requires unrealistic margins of Δ &gt; 2 for equivalence testing.</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Overall picture vs. case-by-case scenario: An interesting finding is that the authors observe that in most cases, there is no substantial evidence for either the absence or the presence of an effect, as evidenced by the equivalence tests. Thus, using both suggested criteria results in a picture similar to the one initially raised by the paper itself. The work done by the authors highlights additional criteria that can be used to further analyze replication success on a case-by-case basis, and I believe that this is where the paper's main contributions lie. Despite not changing the overall picture much, I agree that the p-value criterion by itself does not distinguish between (1) a situation where the original study had low statistical power, resulting in a highly inconclusive non-significant result that does not provide evidence for the absence of an effect and (2) a scenario where the original study was adequately powered, and a non-significant result may indeed provide some evidence for the absence of an effect when analyzed with appropriate methods. Equivalence testing and Bayesian factor approaches are valuable tools in both cases.</p>
</list-item></list>
<p>Regarding the 0.05 threshold, the choice of the prior distribution for the SMD under the alternative H1 is debatable, and this also applies to the equivalence margin. Sensitivity analyses, as highlighted by the authors, are helpful in these scenarios.</p>
</disp-quote>
<p>Thank you for the thorough review and constructive feedback. We have added an additional “Appendix C: Null results from the RPP and EPRP” that shows equivalence testing and Bayes factor analyses for the RPP and EPRP null results.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>The paper points out that non-significance in both the original study and a replication does not ensure that the studies provide evidence for the absence of an effect. Also, it can not be considered a &quot;replication success&quot;. The main point of the paper is rather obvious. It may be that both studies are underpowered, in which case their non-significance does not prove anything. The absence of evidence is not evidence of absence! On the other hand, statistical significance is a confusing concept for many, so some extra clarification is always welcome.</p>
<p>One might wonder if the problem that the paper addresses is really a big issue. The authors point to the &quot;Reproducibility Project: Cancer Biology&quot; (RPCB, Errington et al., 2021). They criticize Errington et al. because they &quot;explicitly defined null results in both the original and the replication study as a criterion for replication success.&quot; This is true in a literal sense, but it is also a little bit uncharitable. Errington et al. assessed replication success of &quot;null results&quot; with respect to 5 criteria, just one of which was statistical (non-)significance.</p>
<p>It is very hard to decide if a replication was &quot;successful&quot; or not. After all, the original significant result could have been a false positive, and the original null-result a false negative. In light of these difficulties, I found the paper of Errington et al. quite balanced and thoughtful. Replication has been called &quot;the cornerstone of science&quot; but it turns out that it's actually very difficult to define &quot;replication success&quot;. I find the paper of Pawel, Heyard, Micheloud, and Held to be a useful addition to the discussion.</p>
<p>Strengths:</p>
<p>This is a clearly written paper that is a useful addition to the important discussion of what constitutes a successful replication.</p>
<p>Weaknesses:</p>
<p>To me, it seems rather obvious that non-significance in both the original study and a replication does not ensure that the studies provide evidence for the absence of an effect. I'm not sure how often this mistake is made.</p>
</disp-quote>
<p>Thanks for the feedback. We do not have systematic data on how often the mistake of confusing absence of evidence with evidence of absence has been made in the replication context, but we do know that it has been made in at least three prominent large-scale replication projects (the RPP, RPEP, RPCB). We therefore believe that there is a need for our article.</p>
<p>Moreover, we agree that the RPCB provided a nuanced assessment of replication success using five different criteria for the original null results. We emphasize this now more in the “Introduction” section. However, we do not consider our article as “a little bit uncharitable” to the RPCB, as we discuss all other criteria used in the RPCB and note that our intent is not to diminish the important contributions of the RPCB, but rather to build on their work and provide constructive recommendations for future researchers. Furthermore, in response to comments made by Reviewer #2, we have added an additional “Appendix B: Null results from the RPP and EPRP” that shows equivalence testing and Bayes factor analyses for null results from two other replication projects, where the same issue arises.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>The authors may wish to address the dichotomy issue I raise above, either in the analysis or in the discussion.</p>
</disp-quote>
<p>Thank you, we now emphasize that Bayes factors and TOST p-values do not need to be dichotomized but can be interpreted as quantitative measures of evidence, both in the “Methods for assessing replicability of null results” and the “Conclusions” sections.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>Given that, here follow additional suggestions that the authors should consider in light of the manuscript's word count limit, to avoid confusing the paper's main idea:</p>
<p>1. Referencing: Could you reference the three interesting cases among the 15 RPCB null results (specifically, the three effects from the original paper #48) where the Bayes factor differs qualitatively from the equivalence test?</p>
</disp-quote>
<p>We now explicitly cite the original and replication study from paper #48.</p>
<disp-quote content-type="editor-comment">
<p>1. Equivalence testing: As the authors state, only 4 out of the 15 study pairs are able to establish replication success at the 5% level, in the sense that both the original and the replication 90% confidence intervals fall within the equivalence range. Among these 4, two (Paper #48, Exp #2, Effect #5 and Paper #48, Exp #2, Effect #6) were initially positive with very low p-values, one (Paper #48, Exp #2, Effect #4) had an initial p of 0.06 and was very precisely estimated, and the only one in which equivalence testing provides a clearer picture of replication success is Paper #41, Exp #2, Effect #1, which had an initial p-value of 0.54 and a replication p-value of 0.05. In this latter case (or in all these ones), one might question whether the &quot;liberal&quot; equivalence range of Δ = 0.74 is the most appropriate. As the authors state, &quot;The post-hoc specification of equivalence margins is controversial.&quot;</p>
</disp-quote>
<p>We agree that the post hoc choice of equivalence ranges is a controversial issue. The margins define an equivalence region where effect sizes are considered practically negligible, and we agree that in many contexts SMD = 0.74 is a large effect size that is not practically negligible. We therefore present sensitivity analyses for a wide range of margins. However, we do not think that the choice of this margin is more controversial for the mentioned studies with low p-values than for other studies with greater p-values, since the question of whether a margin plausibly encodes practically negligible effect sizes is not related to the observed p-value of a study. Nevertheless, for the new analyses of the RPP and EPRP data in Appendix B, we have added additional sensitivity analyses showing how the individual TOST p-values and Bayes factors vary as a function of the margin and the prior standard deviation. We think that these analyses provide readers with an even more transparent picture regarding the implications of the choice of these parameters than the “project-wise” sensitivity analyses in Appendix A.</p>
<disp-quote content-type="editor-comment">
<p>1. Bayes factor suggestions: For the Bayes factor approach, it would be interesting to discuss examples where the BF differs slightly. This is likely to occur in scenarios where sample sizes differ significantly between the original study and replication. For example, in Paper #48, Exp #2 and Effect #4, the initial p is 0.06, but the BF is 8.1. In the replication, the BF dramatically drops to &lt; 1/1000, as does the p-value. The initial evidence of 8.1 indicates some evidence for the absence of an effect, but not strong evidence (&quot;strong evidence for H0&quot;), whereas a p-value of 0.06 does not lead to such a conclusion; instead, it favors H1. It would be interesting if the authors discussed other similar cases in the paper. It's worth noting that in Paper #5, Exp #1, Effect #3, the replication p-value is 0.99, while the BF01 is 2.4, almost indicating &quot;moderate&quot; evidence for H0, even though the p-value is inconclusive.</p>
</disp-quote>
<p>We agree that some of the examples nicely illustrate conceptual differences between p-values and Bayes factors, e.g., how they take into account sample size and effect size. As methodologists, we find these aspects interesting ourselves, but we think that emphasizing them is beyond the scope of the paper and would distract eLife readers from the main messages.</p>
<p>Concerning the conceptual differences between Bayes factors and TOST p-values, we already discuss a case where there are qualitative differences in more detail (original paper #48). We added another discussion of this phenomenon in the Appendix C as it also occurs for the replication of Ranganath and Nosek (2008) that was part of the RPP.</p>
<disp-quote content-type="editor-comment">
<p>1. p-values, magnitude and precision: It's noteworthy to emphasize, if the authors decide to discuss this, that the p-value is influenced by both the effect's magnitude and its precision, so in Paper #9, Exp #2, Effect #6, BF01 = 4.1 has a higher p-value than a BF01 = 2.3 in its replication. However, there are cases where both p-values and BF agree. For example, in Paper #15, Exp #2, Effect #2, both the original and replication studies have similar sample sizes, and as the p-value decreases from p = 0.95 to p = 0.23, BF01 decreases from 5.1 (&quot;moderate evidence for H0&quot;) to 1.3 (region of &quot;Absence of evidence&quot;), moving away from H0 in both cases. This also occurs in Paper #24, Exp #3, Effect #6.</p>
</disp-quote>
<p>We appreciate the suggestions but, as explained before, think that the message of our paper is better understood without additional discussion of more general differences between p-values and Bayes factors.</p>
<disp-quote content-type="editor-comment">
<p>1. The grey zone: Given the above topic, it is important to highlight that in the &quot;Absence of evidence grey zone&quot; for the null hypothesis, for example, in Paper #5, Exp #1, Effect #3 with a p = 0.99 and a BF01 = 2.4 in the replication, BF and p-values reach similar conclusions. It's interesting to note, as the authors emphasize, that Dawson et al. (2011), Exp #2, Effect #2 is an interesting example, as the p-value decreases, favoring H1, likely due to the effect's magnitude, even with a small sample size (n = 3 in both original and replications). Bayes factors are very close to one due to the small sample sizes, as discussed by the authors.</p>
</disp-quote>
<p>We appreciate the constructive comments. We think that the two examples from Dawson et al. (2011) and Goetz et al. (2011) already nicely illustrate absence of evidence and evidence of absence, respectively, and therefore decided not to discuss additional examples in detail, to avoid redundancy.</p>
<disp-quote content-type="editor-comment">
<p>1. Using meta-analytical results (?): For papers from RPCB, comparing the initial study with the meta-analytical results using Bayes factor and equivalence testing approaches (thus, increasing the sample size of the analysis, but creating dependency of results since the initial study would affect the meta-analytical one) could change the conclusions. This would be interesting to explore in initial studies that are replicated by much larger ones, such as: Paper #9, Exp #2, Effect #6; Goetz et al. (2011), Exp #1, Effect #1; Paper #28, Exp #3, Effect #3; Paper #41, Exp #2, Effect #1; and Paper #47, Exp #1, Effect #5).</p>
</disp-quote>
<p>Thank you for the suggestion. We considered adding meta-analytic TOST p-values and Bayes factors before, but decided that Figure 3 and the results section are already quite technical, so adding more analyses may confuse more than help. Nevertheless, these meta-analytic approaches are discussed in the “Conclusions” section.</p>
<disp-quote content-type="editor-comment">
<p>1. Other samples of fields of science: It would be interesting to investigate whether using Bayes factors and equivalence tests in addition to p-values results in a clearer scenario when applied to replication data from other fields. As mentioned by the authors, the Reproducibility Project: Experimental Philosophy (RPEP) and the Reproducibility Project: Psychology (RPP) have data attempting to replicate some original studies with null results. While the RPCB analysis yielded a similar picture when using both criteria, it is worth exploring whether this holds true for RPP and RPEP. Considerations for further research in this direction are suggested. Even if the original null results were excluded in the calculation of an overall replicability rate based on significance, sensitivity analyses considering them could have been conducted. The present authors can demonstrate replication success using the significance criteria in these two projects with initially p &lt; 0.05 studies, both positive and non-positive.</p>
</disp-quote>
<p>Thank you for the excellent suggestion. We added an Appendix B where the null results from the RPP and EPRP are analyzed with our proposed approaches. The results are also discussed in the “Results” and “Conclusions” sections.</p>
<disp-quote content-type="editor-comment">
<p>1. Other approaches: I am curious about the potential impact of using an approach based on equivalence testing (as described in <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2308.09112">https://arxiv.org/abs/2308.09112</ext-link>). It would be valuable if the authors could run such analyses or reference the mentioned work.</p>
</disp-quote>
<p>Thank you. We were unaware of this preprint. It seems related to the framework proposed by Stahel W. A. (2021) New relevance and significance measures to replace p-values. PLoS ONE 16(6): e0252991. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0252991">https://doi.org/10.1371/journal.pone.0252991</ext-link></p>
<p>We now cite both papers in the discussion.</p>
<disp-quote content-type="editor-comment">
<p>1. Additional evidence: There is another study in which replications of initially p &gt; 0.05 studies with p &gt; 0.05 replications were also considered as replication successes. You can find it here: <ext-link ext-link-type="uri" xlink:href="https://www.medrxiv.org/content/10.1101/2022.05.31.22275810v2">https://www.medrxiv.org/content/10.1101/2022.05.31.22275810v2</ext-link>. Although it involves a small sample of initially p &gt; 0.05 studies with already large sample sizes, the work is currently under consideration for publication in PLOS ONE, and all data and materials can be accessed through OSF (links provided in the work).</p>
</disp-quote>
<p>Thank you for sharing this interesting study with us. We feel that it is beyond the scope of the paper to include further analyses as there are already analyses of the RPCB, RPP, and EPRP null results. However, we will keep this study in mind for future analysis, especially since all data are openly available.</p>
<disp-quote content-type="editor-comment">
<p>1. Additional evidence 02: Ongoing replication projects, such as the Brazilian Reproducibility Initiative (BRI) and The Sports Replication Centre (<ext-link ext-link-type="uri" xlink:href="https://ssreplicationcentre.com/">https://ssreplicationcentre.com/</ext-link>), continue to generate valuable data. BRI is nearing completion of its results, and it promises interesting data for analyzing replication success using p-values, equivalence regions, and Bayes factor approaches.</p>
</disp-quote>
<p>We now cite these two initiatives as examples of ongoing replication projects in the introduction. Similarly as for your last point, we think that it is beyond the scope of the paper to include further analyses as there are already analyses of the RPCB, RPP, and EPRP null results.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>I have no specific recommendations for the authors.</p>
</disp-quote>
<p>Thank you for the constructive review.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewing Editor (Recommendations For the Authors):</bold></p>
<p>I recognize that it was suggested to the authors by the previous Reviewing Editor to reduce the amount of statistical material to be made more suitable for a non-statistical audience, and so what I am about to say contradicts advice you were given before. But, with this revised version, I actually found it difficult to understand the particulars of the construction of the Bayes Factors and would have appreciated a few more sentences on the underlying models that fed into the calculations. In my opinion, the provided citations (e.g., Dienes Z. 2014. Using Bayes to get the most out of non-significant results) did not provide sufficient background to warrant a lack of more technical presentation here.</p>
</disp-quote>
<p>Thank you for the feedback. We added a new “Appendix C: Technical details on Bayes factors” that provides technical details on the models, priors, and calculations underlying the Bayes factors.</p>
</body>
</sub-article>
</article>