<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">90583</article-id>
<article-id pub-id-type="doi">10.7554/eLife.90583</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.90583.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Body size as a metric for the affordable world</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Feng</surname>
<given-names>Xinran</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Xu</surname>
<given-names>Shan</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Yuannan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Liu</surname>
<given-names>Jia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence, Tsinghua University</institution>, Beijing, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Faculty of Psychology, Beijing Normal University</institution>, Beijing, <country>China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence to: <email>liujiathu@tsinghua.edu.cn</email> (J. Liu)</corresp>
<fn fn-type="equal" id="n1"><label>†</label><p>Equal contribution</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-10-24">
<day>24</day>
<month>10</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP90583</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-08-14">
<day>14</day>
<month>08</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-05-06">
<day>06</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.20.533336"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Feng et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Feng et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-90583-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>The physical body of an organism serves as a vital interface for interactions with its environment. Here we investigated the impact of human body size on the perception of action possibilities (affordances) offered by the environment. We found that the body size delineated a distinct boundary on affordances, dividing objects of continuous real-world sizes into two discrete categories with each affording distinct action sets. Additionally, the boundary shifted with imagined body sizes, suggesting a causal link between body size and affordance perception. Intriguingly, ChatGPT, a large language model lacking physical embodiment, exhibited a modest yet comparable affordance boundary at the scale of human body size, suggesting the boundary is not exclusively derived from organism-environment interactions. A subsequent fMRI experiment showed that only the affordances of objects within the range of body size were represented in the brain, suggesting that objects capable of being manipulated are the only objects capable of offering affordance in the eyes of an organism. In summary, our study suggests a novel definition of object-ness in an affordance-based context, advocating the concept of embodied cognition in understanding the emergence of intelligence constrained by an organism’s physical attributes.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>embodied cognition</kwd>
<kwd>affordance</kwd>
<kwd>body-scaled metric</kwd>
<kwd>affordance boundary</kwd>
<kwd>ChatGPT</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>All figure legends revised in both main text and supplemental files. Minor changes with some sentences.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s2">
<title>Introduction</title>
<sec id="s2a">
<title>Man is the measure of all things. - Protagoras</title>
<p>The assertion by the ancient Greek philosopher Protagoras highlights the notion that reality is defined by how the world is perceived by humans. A contemporary interpretation of this statement is the embodied theory of cognition (e.g., <xref ref-type="bibr" rid="c11">Chemero, 2013</xref>; <xref ref-type="bibr" rid="c17">Gallagher, 2017</xref>; <xref ref-type="bibr" rid="c17a">Gibbs, 2005</xref>; <xref ref-type="bibr" rid="c67">Wilson, 2002</xref>; <xref ref-type="bibr" rid="c63">Varela et al., 2017</xref>), which posits that human body scale (e.g., size) constrains the perception of objects and the generation of motor responses. For instance, humans evaluate the climbability of steps based on their leg length (<xref ref-type="bibr" rid="c41">Mark, 1987</xref>; <xref ref-type="bibr" rid="c64">Warren,1984</xref>), and determine the navigability of apertures according to the critical aperture-to-shoulder-width ratio (Warren &amp; Whang, 1987). Additionally, grasping strategies have been shown to be contingent upon object size relative to one’s body (<xref ref-type="bibr" rid="c10">Cesari &amp; Newell, 2000</xref>; <xref ref-type="bibr" rid="c44">Newell et al., 1989</xref>) or hand size (<xref ref-type="bibr" rid="c9">Castiello et al., 1993</xref>; <xref ref-type="bibr" rid="c60">Tucker &amp; Ellis, 2004</xref>). However, whether body size simply serves as a reference for locomotion and object manipulation, or alternatively, plays a pivotal role in shaping the representation of objects as suggested by Protagoras, remains an open question.</p>
<p>To underscore the latter point, <xref ref-type="bibr" rid="c18">Gibson (1979)</xref>, the pioneer of embodied cognition research, stated that “Detached objects must be comparable in size to the animal under consideration if they are to afford behavior (p.124).” This implies that an object’s affordance, encompassing all action possibilities offered to an animal, is determined by the object’s size relative to the animal’s size rather than its real-world size. For instance, in a naturalistic environment, such as a picnic scene shown in <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, there may exist a qualitative distinction between objects within (the objects with warm tints in <xref rid="fig1" ref-type="fig">Fig. 1a</xref>) and beyond (those with cold tints) the size range of humans. Only objects within the range, such as the apple, the umbrella and the bottles, may afford actions, while those beyond this range, such as the trees and the tent, are largely viewed as part of the environment. Consequently, visual perception may be ecologically constrained, and the body may serve as a metric that facilitates meaningful engagement with the environment by differentiating objects that are accessible for interactions from those not. Based on Gibson’s statement, we proposed two hypotheses: first, the affordance of objects will exhibit a qualitative difference between objects within and beyond the size range of an organism’s body; second, affordance-related neural activity will emerge exclusively for objects within the organism’s size range.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig 1.</label>
<caption><title>An affordance boundary in the affordable world.</title><p><bold>a</bold>, An illustration of a picnic scene, featuring objects of various sizes relative to human body. Example objects within the normal body size range are painted red, and those beyond green. We hypothesized qualitative differences between perceived affordances of these two kinds of objects. <bold>b</bold>, A demonstration of the object-action relation judgement task for human participants (top) and AI models (bottom). The question in task for human participants was presented in Chinese. <bold>c</bold>, The representational similarity matrix (RSM) for objects based on human rating of affordance similarity. Object sizes are denoted with red to green. Two primary clusters emerged in clustering analysis of the similarity pattern are outlined with black boxes. <bold>d</bold>, <italic>Left panel</italic>: The overall affordance similarity and that of each gender (left y-axis) as well as real-world size similarity (right y-axis) between neighboring size ranks. The error bars represent the standard error (SE). <italic>Right panel</italic>: The point clouds of pairwise correlations between objects from the same rank or neighboring ranks. Each colored dot represents the affordance similarity (y-axis) and the average real-world size (x-axis) of a specific object pair. The grey dots indicate the averaged size (x-axis) and pairwise similarity (y-axis) of object pairs in different rank compositions. Left to right: both from size rank 3, from size rank 3 and 4, both from size rank 4, from size rank 4 and 5, both from size rank 5, from size rank 5 and 6, and both from size rank 6. The horizontal error bars represent 95% confidence interval (CI) of the averaged object size in each pair, and the vertical error bars denote the CI of pairwise affordance similarity.</p></caption>
<graphic xlink:href="533336v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To test these hypotheses, we first measured the affordance of a diverse array of objects varying in real-world sizes (e.g., Konkle, &amp; Oliva, 2011). We found a dramatic decline in affordance similarity between objects within and beyond human body size range, as these objects afforded distinct sets of action possibilities. Notably, the affordance boundary shifted in response to the imagined body sizes and could be attained solely from language, as demonstrated by the large language model (LLM), ChatGPT (OpenAI, 2022). A subsequent fMRI experiment corroborated the qualitative difference in affordances demarcated by the body size, as affordances of objects within humans’ size range, but not those beyond, were represented in both dorsal and ventral visual streams of the brain. This study advances our understanding of the role of body size in shaping object representation and underscores the significance of body size as a metric for determining object affordances that facilitates meaningful engagement with the environment.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>To illustrate how human body size affects object affordances with different sizes, we first characterized the affordances of a set of daily objects. In each trial, we presented a matrix consisting of nine objects and asked participants to report which objects afforded a specific action (e.g., sit-able: a chair, a bed, a skateboard, but not a phone, a laptop, an umbrella, a kettle, a plate, or a hammer) (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). In this task, there were 14 actions commonly executed in daily life and 24 object images from the THINGS database (<xref ref-type="bibr" rid="c26">Hebart et al., 2019</xref>), with sizes ranging from size rank 2 to 8 according to <xref ref-type="bibr" rid="c34">Konkle and Oliva (2011</xref>)’s classification. These objects covered real-world sizes from much smaller (17 cm on average, rank 2) to orders of magnitude larger (5,317 cm on average, rank 8) than the human body size (see Methods for details). Consequently, affordances for each object were indexed by a 14-dimensional action vector, with the value for each dimension representing the percentage of participants who agreed on a certain action being afforded by the object (e.g., 88% for the action of grasping on a hammer indicating 88% of participants agreed that a hammer affords grasping). Supplemental Fig. S1 showed the affordances of two example objects.</p>
<p>An affordance similarity matrix was then constructed where each cell corresponded to the similarity in affordances between a pair of objects (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>). A clustering analysis revealed a two-cluster structure. Visual inspection suggested that the upper-left cluster consisted of objects smaller than human body size (red labels), and the lower-right cluster contained objects larger than human body size (green labels). Critically, the between-cluster similarity in the affordance similarity matrix approached zero, suggesting a division in affordances located near the body size. To quantify this observation, we calculated the similarity in affordances between each neighboring size rank. Indeed, we identified a clear trough in affordance similarity, dropping to around zero, between size rank 4 (77cm on average) and 5 (146cm on average), which was significantly smaller than that between size rank 3 and 4 (Z = 3.91, <italic>p</italic> &lt;.001) and that between size rank 5 and 6 (Z = 1.66, <italic>p</italic> = .048). This trough suggested an affordance boundary between size rank 4 and 5, while affordance similarities between neighboring ranks remained high (<italic>r</italic>s &gt; 0.45) and did not significantly differ from each other (<italic>p</italic>s &gt; 0.05) on either side of the boundary (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>, left panel, green lines). This pattern was evident for both genders, indicating no gender difference. Note that the abrupt change in affordance similarity across the boundary cannot be explained by changes in objects’ real-world size, as the similarity in objects’ real-world size was relatively stable across ranks, without any trend of a trough-shape curve (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>, left panel, yellow line). Intriguingly, rank 4 and rank 5 corresponds to 80 cm to 150 cm, a boundary situated between these two ranks is within the range of the body size of a typical human adult. This finding suggested that objects were classified into two categories based on their affordances, with the boundary aligning with human body size.</p>
<p>To better locate the boundary, we focused on the affordance similarity between individual objects within size rank 3 to 6 (approximately ranging from 30cm to 220cm in real-world size, the area with grey shade in <xref rid="fig1" ref-type="fig">Fig. 1d</xref>), where the trough-shape curve was identified. Specifically, we traversed all pairs of objects with similar real-world diagonal sizes (from either the same rank or from neighboring ranks), calculated their average real-world size as an index of the approximate location of boundary between this pair of objects, and plotted the affordance similarity against the average real-world size of each object pair. As shown in the inset (grey box) of <xref rid="fig1" ref-type="fig">Fig. 1d</xref>, consistent with the rank-wise analysis, the abrupt decrease in affordance similarity exclusively happened between objects from size rank 4 and 5 (light green dots). The averaged real-world size in these object pairs was 104 cm (95% CI, 105 to 130 cm) and the affordance similarity in such object pairs was around zero. This result further narrowed the location estimation of the boundary, and demonstrated that the affordance boundary persisted at the level of individual objects.</p>
<p>One may argue that the location of the affordance boundary coincidentally fell within the range of human body size, rather than being influenced by human body size. To establish a causal link between them, we directly manipulated the body schema, referring to an experiential and dynamic functioning of the living body in its environment (<xref ref-type="bibr" rid="c43">Merleau-Ponty &amp; Smith, 1962</xref>), to examine whether the affordance boundary would shift accordingly. Utilizing the same paradigm, we instructed a new group of participants to imagine themselves as small as a cat (typical diagonal size: 77cm, size rank 4, referred to as the “cat condition”), and another new group to envision themselves as large as an elephant (typical diagonal size: 577 cm, size rank 7, referred to as the “elephant condition”) throughout the task (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). This manipulation proved effective, as evidenced by the participants’ reported imagined heights in the cat condition being 42 cm (SD = 25.6) and 450 cm (SD = 426.8) in the elephant condition on average, respectively, when debriefed at the end of the task.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig 2.</label>
<caption><title>A disembodied origin of the affordance boundary.</title><p><bold>a</bold>, The schematic diagram of the imagined size in the cat condition (top) and the elephant condition (bottom), with the mean estimated height reported by participants for each condition. <bold>b</bold>, The affordance similarity between neighboring size ranks for manipulated body sizes (Red line: cat-size body; Green line: elephant-size body). The dashed line marks the boundary of the human-size body. The red and green arrows indicate the corresponding boundary shift in each condition. <bold>c</bold>, The affordance similarity between neighboring size ranks for different large language models, and human data from <xref rid="fig1" ref-type="fig">Fig. 1d</xref> was re-drawn as a reference. The stars indicate significant contrasts between affordance similarities between neighboring data points. <bold>d</bold>, The trough value of each model at between size rank 4-5. The stars here indicate the significant trough value compared to zero. The error bars represent the estimated standard error (SE). *<italic>p</italic>&lt;.05, **<italic>p</italic>&lt;.01, ***<italic>p</italic>&lt;.001.</p></caption>
<graphic xlink:href="533336v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>With exactly the same set of objects, a distinct shift in the affordance boundary was observed for each condition (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>). In the cat condition, the affordance boundary was identified between size rank 3 and 4, with affordance similarity between size rank 3 and 4 being significantly lower than that between size rank 2 and 3 (Z = 1.76, <italic>p</italic> = .039) and that between size rank 4 and 5 (Z = 1.68, <italic>p</italic> = .047). In contrast, in the elephant condition, the affordance boundary shifted to the right, as demonstrated by a decrease in affordance similarity between size rank 6 and 7, and that between size rank 7 and 8 as compared to that between size rank 5 and 6, with a trend towards significance (with size rank 6-7: Z = 1.28, <italic>p</italic> = .099; with size rank 7-8: Z = 1.48, <italic>p</italic> = .069). The observation that the affordance boundary shifted to the left under the cat condition and to the right under the elephant condition suggests that affordance perception is influenced even by imagined body size. Furthermore, the cognitive penetrability (<xref ref-type="bibr" rid="c51">Pylyshyn, 1999</xref>) of affordance perception implies potential susceptibility of affordance perception to semantic or conceptual transformation or modification.</p>
<p>To test the further speculation that the affordance boundary can be derived solely from conceptual knowledge without direct sensorimotor experience, we employed a disembodied agent, the large language model (LLM) ChatGPT (Chat Generative Pre-trained Transformer; <ext-link ext-link-type="uri" xlink:href="https://openai.com/blog/chatgpt/">https://openai.com/blog/chatgpt/</ext-link>). This model was trained on a massive corpus of language materials originated from humans, yet it can not receive any sensorimotor information from the environment. Here we asked whether language alone would be sufficient to form an affordance boundary in ChatGPT models as well as in smaller LLMs, BERT (<xref ref-type="bibr" rid="c12">Devlin et al., 2018</xref>) and GPT-2 (<xref ref-type="bibr" rid="c52">Radford et al., 2018</xref>).</p>
<p>The experimental procedure was similar to that conducted with human participants, except that images were replaced by the corresponding words (see Methods). Given randomness embedded in response generation, each model was tested 20 times to simulate the sampling of human participants. We found that the affordance similarity curves demonstrated by the ChatGPT models were both trough-shaped between size rank 4 and 5, the same location where the boundary emerged in human participants (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>, green lines). Further statistical analyses showed a significant difference in affordance similarity between size rank 3 and 4, and that between size rank 4 and 5 (ChatGPT (GPT-3.5): Z = 1.98, <italic>p</italic> = .024; ChatGPT (GPT-4): Z = 2.73, <italic>p</italic> = .003). The affordance similarity between size rank 4 and 5 was also lower than that between size rank 5 and 6, yet the difference did not reach the significance (ChatGPT (GPT-3.5): Z = 0.96, <italic>p</italic> = .17; ChatGPT (GPT-4): Z = 1.27, <italic>p</italic> = .10).</p>
<p>In contrast, no trough-shaped boundary was observed in either BERT or GPT-2 (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>, yellow lines), despite an apparent but non-significant decrease in affordance similarity in GPT-2 between size rank 5 and 6 (<italic>ps</italic> &gt; .20). To further quantify the magnitude of the decrease in affordance similarity between the size rank 4 and 5, we measured the decrease by subtracting the similarity value at the trough from the neighboring similarity values and then subjected it to a permutation test (see Methods). We found a significant decrease in affordance similarity in humans (permutation N = 5000, 𝑝(𝑇 &gt; 𝑇<sub>𝑜𝑏𝑠</sub>) = 0.015) and ChatGPT (GPT-4) (permutation N = 5000, 𝑝(𝑇 &gt; 𝑇<sub>𝑜𝑏𝑠</sub>) = 0.046), a marginal significant decrease in ChatGPT (GPT-3.5) (permutation N = 5000, 𝑝(𝑇 &gt; 𝑇<sub>𝑜𝑏𝑠</sub>) = 0.061), and no significance in either BERT or GPT-2 (<italic>p</italic>s &gt; .46, <xref rid="fig2" ref-type="fig">Fig. 2d</xref>). Thus, the affordance boundary can be derived from language solely without sensorimotor information from environment. Interesting, it appears to spontaneously emerge when the language processing ability of the LLMs surpasses a certain threshold (i.e., GPT-2/ BERT &lt; ChatGPT models).</p>
<p>A further analysis on the affordances separated by the boundary revealed that objects within human body size range were primarily subjected to hand-related actions such as grasping, holding and throwing. These affordances typically involve object manipulation with humans’ effectors. In contrast, objects beyond the size range of human body predominantly afforded actions such as sitting and standing, which typically require locomotion or posture change of the whole body around or within the objects. The distinct categories of reported affordances demarcated by the boundary imply that the objects on the two sides of the boundary may be represented differently in the brain.</p>
<p>To test this speculation, we used fMRI to measure neural activity in the dorsal and ventral visual streams when participants were instructed to evaluate whether an action was affordable by an object (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). Four objects were chosen from the behavioral experiment: two within the body size range (i.e., bottle and football, WITHIN condition) and the two beyond (i.e., bed and piano, BEYOND condition). Accordingly, four representative actions (to grasp, to kick, to sit and to lift) were selected in relation to the respective objects. During the scan, the participants were asked to decide whether a probe action was affordable (e.g., grasp-able – bottle, Congruent condition) or not (e.g., sit-able – bottle, Incongruent condition) by each subsequently-presented object. The congruence effect, derived from the contrast of Congruent by Incongruent conditions, served as an index of affordance representation.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Affordance representation in the visual streams.</title><p><bold>a</bold>, An example block with the probe action “graspable”. The participants indicated whether each of the subsequently presented objects is graspable by pressing the corresponding button. The action probing question was presented in Chinese during the experiment. <bold>b</bold>, The ROIs included in this experiment. <bold>c</bold>, The activation of each condition in the pFs and SPL. The bars represent the contrast estimates of each condition versus baseline. The stars indicate the significant difference between congruent and incongruent condition. *<italic>p</italic>&lt;.05, **<italic>p</italic>&lt;.01, ***<italic>p</italic>&lt;.001, otherwise non-significance. Error bars represent the standard error (SE).</p></caption>
<graphic xlink:href="533336v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We examined the congruency effect in two object-selective regions defined by the contrast of objects against baseline (see Methods), each representing a corresponding visual stream: the posterior fusiform (pFs) in the ventral stream, which is involved in object recognition (e.g., <xref ref-type="bibr" rid="c23">Grill-Spector et al., 2000</xref>; <xref ref-type="bibr" rid="c40">Malach et al., 1995</xref>) and objects’ real-world size processing (Konkle, &amp; Oliva, 2012; <xref ref-type="bibr" rid="c56">Snow et al., 2011</xref>), and the superior parietal lobule (SPL) in the dorsal stream, one of the core tool network regions (e.g., <xref ref-type="bibr" rid="c16">Filimon et al. 2007</xref>; <xref ref-type="bibr" rid="c42">Matić et al., 2020</xref>). For the rest object-selective regions identified in this experiment, see Supplemental Fig. S2 and Supplemental Table S1. A repeated-measures ANOVA with object type (WITHIN versus BEYOND) and congruency (Congruent versus Incongruent) as within-subject factors was performed for each ROI, respectively. A significant interaction between object type and congruency was observed in both ROIs (SPL: <italic>F</italic>(1,11) = 15.47, <italic>p</italic> =.002, 𝜂<sup>2</sup>=.58; pFs: <italic>F</italic>(1,11) = 24.93, <italic>p</italic> &lt;.001, 𝜂<sup>2</sup>=.69), suggesting that these regions represented affordances differentially based on object type (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>). A <italic>post hoc</italic> simple effect analysis revealed the congruency effect solely for objects within body size range (SPL: <italic>p</italic> &lt;.001; pFs: <italic>p</italic> =.021), not for objects beyond (<italic>p</italic>s &gt;.41). In addition, the main effect of object type was not significant in either ROI (<italic>p</italic>s &gt;.17), suggesting that the absence of the congruency effect for objects beyond the body size cannot be attributable to compromised engagement in viewing these objects. In addition, a whole-brain analysis was performed, and no region showed the congruency effect for the objects beyond the body size. Taken together, the affordance boundary not only separated the objects into two categories based on their relative size to human body, but also delineated the range of objects that receive proper affordance representation in the brain.</p>
<p>In addition to the pFs and SPL, we also examined the congruency effect in the lateral occipital cortex (LO), which provides inputs to both the pFs and SPL (<xref ref-type="bibr" rid="c27">Hebart et al., 2018</xref>), and the primary motor cortex (M1), which receives inputs from the dorsal stream (<xref ref-type="bibr" rid="c62">Vainio &amp; Ellis, 2020</xref>) and executes actions (<xref ref-type="bibr" rid="c4">Binkofski et al., 2002</xref>). Although both the LO and M1 showed a significantly higher response to objects than baseline, no congruency effect in affordance for objects within the body size was observed (main effect of congruency: <italic>F</italic>(1,11) = 1.74, <italic>p</italic> =.214, 𝜂<sup>2</sup>=.13, Supplementary Fig. S3). Therefore, it is unlikely that the representation of affordance is exclusively dictated by visual inputs or automatically engaged in motor execution. This finding suggests that affordance perception likely requires perceptual processing and is not necessarily reflected in motor execution, diverging from Gibsonian concept of direct perception.</p>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>Boundaries highlight the discontinuity in perception in response to the continuity of physical inputs (<xref ref-type="bibr" rid="c25">Harnad, 1987</xref>; <xref ref-type="bibr" rid="c70">Young et al., 1997</xref>). Perceptual boundary has been demonstrated in various domains, such as color perception (Bornstein, &amp; Korda, 1984), speech-sounds perception (<xref ref-type="bibr" rid="c38">Liberman et al., 1957</xref>), and facial gender discrimination (<xref ref-type="bibr" rid="c7">Campanella et al., 2001</xref>). The boundaries reflect a fundamental adaptation of perception to facilitate categorizations necessary for an organism (<xref ref-type="bibr" rid="c21">Goldstone &amp; Hendrickson, 2010</xref>). Our study, for the first time, unveiled a boundary in object affordance, wherein affordance similarity across the boundary was significantly lower than that within the boundary. Critically, the boundary separating object affordances along a size axis coincided with human body size, suggesting that object affordances are characterized in a dimension scaled by human body size.</p>
<p>What is the function of the affordance boundary? About four decades ago, <xref ref-type="bibr" rid="c18">Gibson (1979)</xref> postulated that only objects of sizes comparable to an animal’s body size are amenable to interaction and capable of providing affordances to the animal, thereby possessing ecological values that distinctly differ from those of larger objects. In this study, we expand upon this notion by arguing that the affordance boundary serves to delineate (manipulable) objects from their surrounding environment. In other words, objects within the range of an animal’s body size are indeed objects in the animal’s eye and possess affordances as defined by Gibson. In contrast, objects larger than that range are not <italic>the</italic> “objects” with which the animal is intrinsically inclined to interact, but probably considered less interesting component of the environment.</p>
<p>This speculation aligns with previous fMRI studies where large objects activated the medial portion of the ventral temporal cortex (<xref ref-type="bibr" rid="c29">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="c39">Magri et al., 2021</xref>), overlapping with the parahippocampus gyrus involved in scene representation (<xref ref-type="bibr" rid="c48">Park et al., 2011</xref>; <xref ref-type="bibr" rid="c59">Troiani et al., 2014</xref>), and smaller objects activated the lateral portion, such as the pFs, where the congruency effect of affordance was identified in our study. Furthermore, we found that the congruency effect was only evident for objects within the body size range, but not for objects beyond, supporting the idea that affordance is typically represented only for objects within the body size range. In this context, an animal’s body size and the sensorimotor capacity determine the boundary of manipulation, and thus, the boundary between manipulable objects and the environment. Therefore, our study provides a novel perspective on a long-standing question in psychology, cognitive science, and philosophy: what constitutes an object? Existing psychological studies, especially in the field of vision, define objects in a disembodied manner, primarily relying on their physical properties such as contour (not scrambled). Our identification of the affordance boundary presents a new source of <italic>object-ness</italic>: the capability of being a source of affordance under constraints of an animal’s sensorimotor capacity, which resonates the embodied influence on the formation of abstract concepts (e.g., <xref ref-type="bibr" rid="c1">Barsalou, 1999</xref>; <xref ref-type="bibr" rid="c37">Lakoff &amp; Johnson, 1980</xref>) of objects and environment. In this respect, man is indeed the measure of all things.</p>
<p>The metric provided by the body size, however, was changeable when the body schema was intentionally altered through participants’ imagination of possessing either a cat-or elephant-sized body, with which the participants had no prior sensorimotor experience. Importantly, they perceived new affordances in a manner as if they have had embodied experience with this new body schema. Therefore, this finding suggests that the affordance boundary is cognitively penetrable, arguing against the directness of affordance perception (e.g., <xref ref-type="bibr" rid="c18">Gibson, 1979</xref>; <xref ref-type="bibr" rid="c22">Greeno, 1994</xref>; <xref ref-type="bibr" rid="c50">Prindle et al., 1980</xref>) or the exclusive sensorimotor origin of affordances (e.g., <xref ref-type="bibr" rid="c17">Gallagher, 2017</xref>; <xref ref-type="bibr" rid="c58">Thompson, 2010</xref>; <xref ref-type="bibr" rid="c30">Hutto &amp; Myin, 2012</xref>; <xref ref-type="bibr" rid="c11">Chemero, 2013</xref>). Alternatively, disembodied conceptual knowledge pertinent to action likely modulates affordance perception. Indeed, it has been proposed that conceptual knowledge is grounded in the same neural system as that involved in action (<xref ref-type="bibr" rid="c1">Barsalou, 1999</xref>; <xref ref-type="bibr" rid="c20">Glenberg et al., 2013</xref>; <xref ref-type="bibr" rid="c68">Wilson &amp; Golonka, 2013</xref>), thereby suggesting that sensorimotor information may be embedded in language (e.g., <xref ref-type="bibr" rid="c8">Casasanto, 2011</xref>; <xref ref-type="bibr" rid="c19">Glenberg &amp; Gallese, 2012</xref>; <xref ref-type="bibr" rid="c57">Stanfield &amp; Zwaan, 2001</xref>), as the grounded theory proposed (see <xref ref-type="bibr" rid="c2">Barsalou, 2008</xref> for a review).</p>
<p>Direct evidence for this speculation comes from the disembodied ChatGPT models, which showed an evident affordance boundary despite lacking direct interaction with the environment. We speculated that ChatGPT models may have formed the affordance boundary through a human prism ingrained within its linguistic training corpus. In fact, when inquired about the size of a hypothetical body constructed for its use, ChatGPT (GPT-4) replied, “It could be the size of an average adult human, around 5 feet 6 inches (167.6 cm) tall. This would allow me to interact with the world and people in a <italic>familiar</italic> way.” Critically, this size corresponds to the location where the affordance boundary of ChatGPT models was found. In essence, a virtual body schema may have automatically emerged in ChatGPT models, possibly based on the body schema inherited from humans through language, enabling ChatGPT models to display a preliminary ability to reason the relationship between bodily action and objects. It should be noted that the affordance boundary was not present in all LLMs tested. Specifically, LLMs with a smaller number of parameters, such as BERT and GPT-2, did not exhibit any robust boundary, suggesting the emergence of the boundary may depend on language processing ability determined by the scale of training datasets and the complexity of the model (<xref ref-type="bibr" rid="c28">Hestness et al., 2017</xref>; <xref ref-type="bibr" rid="c6">Brown et al., 2020</xref>), as well as alignment methods used in fine-tuning the model (<xref ref-type="bibr" rid="c47">Ouyang et al., 2022</xref>).</p>
<p>While the primary focus of our study concerns the nature of human perception of affordance, our findings on ChatGPT models raise an intriguing question that extends beyond psychology and neuroscience into the domain of artificial intelligence (AI). The AI field has predominantly concentrated on disembodied cognition, such as vision and language. In contrast, the utilization of sensorimotor information to interact with and adapt to the world, including affordance perception in our study, represents a crucial human cognitive achievement that remains elusive for AI systems. Developing such abilities may facilitate AI-supported robotics in navigation, object manipulation, and other actions essential for survival and goal accomplishment, which is considered a promising direction of the next breakthrough in AI (<xref ref-type="bibr" rid="c24">Gupta et al., 2021</xref>; <xref ref-type="bibr" rid="c55">Smith &amp; Gasser, 2005</xref>).</p>
<p>Although our study showed that the ability to perceive affordance can emerge solely from language, two questions remain. First, the magnitude of the boundary observed in ChatGPT models was smaller than that in humans. This discrepancy might be compensated by merely enhancing the language processing ability of LLMs. Alternatively, direct interaction with the environment may be necessary for LLMs to achieve human-level performance in affordance perception. Second, the size of virtual body schema of ChatGPT models, if present, coincided with human body size. When integrating LLMs with real robots (<xref ref-type="bibr" rid="c14">Driess et al., 2023</xref>), this may pose a challenge because the to-be-supported robots or cars for autopilot might not fall within human body size range. Future studies may be needed to align the inherited body schema with the actual constitution of the robots. Addressing these questions is beyond the scope of the present study but may hold significant implications for the development of AI systems possessing human-level ingenuity and adaptability in interacting with the world.</p>
<p>In summary, our findings regarding the affordance boundary highlight the interdependence between an agent and the external world in shaping cognition. Furthermore, taking our finding with embodied humans and disembodied LLMs into account, we propose a revision to the purely sensorimotor-based concept of affordance by emphasizing a disembodied, perhaps conceptual, addition to it. That is, the embodied cognition and symbolic processing of language may be more intricately and fundamentally connected than previously thought: perception-action problems and language problems can be treated as the same kind of process (<xref ref-type="bibr" rid="c68">Wilson &amp; Golonka, 2013</xref>). In this context, man is the measure of both the world and the words, for both humans and for AIs. The presence of such a metric may shed light on the development of AI systems that can fully capture essential human abilities founded on sensorimotor interactions with the world.</p>
</sec>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Participants</title>
<p>A total of five hundred and thirty-four participants were recruited for the original object-action relation judgement task online (<ext-link ext-link-type="uri" xlink:href="https://www.wjx.cn/">https://www.wjx.cn/</ext-link>). Six participants were excluded from the data analyses because their task completion time did not pass the predetermined minimum completion time criteria, leaving us with a final sample of 528 participants (311 males, aged from 16 to 73, mean age = 24.1 years). For the object-action relation judgement task with manipulated body schema, another one hundred and thirty-nine participants were recruited from the same platform. Data from participants whose imagined height fell within the average human size range (100cm-200cm) were excluded from further analysis, with 100 participants (49 males, aged from 17 to 39 years, mean age = 23.2 years) remained. Each participant completed an online consent form before starting the experiment.</p>
<p>For the fMRI experiment, twelve students (8 males, aged from 19 to 31 years, mean age = 23.7 years) from Tsinghua University participated. All participants reported normal or corrected-to-normal vision. Each participant completed a pre-scan MRI safety questionnaire and a consent form before the experiment.</p>
<p>This study was approved by the Institutional Review Board at Beijing Normal University. All participated were all compensated financially for their time.</p>
</sec>
<sec id="s5b">
<title>Stimuli</title>
<p>For all the behavioural tasks, the stimuli comprised 27 objects from the THINGS database (<xref ref-type="bibr" rid="c26">Hebart et al., 2019</xref>). Each image was portraited a typical exemplar of daily-life object isolated against a white background, sized 400 × 400 pixels. The objects spanned real-world size rank 2 to 8, as classified in <xref ref-type="bibr" rid="c34">Konkle and Oliva (2011)</xref>, where the actual size of each object was measured as the diagonal size of its bounding box. The size rank was calculated as a logarithmic function of the diagonal size, with smaller ranks corresponding to smaller real-world sizes (e.g., the airplane is in size rank 8 and the apple is in size rank 2). The full list of objects, along with their corresponding diagonal size and size rank, was provided in Supplementary Table S2.</p>
<p>For fMRI experiment, the stimuli included images of 4 objects (bed, bottle, ball, and piano), with 5 exemplars for each object. The resulting 20 images (4 objects × 5 exemplars, from the THINGS database) each depicted an isolated object against a white background, all sized 400 × 400 pixels.</p>
</sec>
<sec id="s5c">
<title>Procedure</title>
<sec id="s5c1">
<title>Object-action relation judgement task for human participants</title>
<p>To measure the perceived affordances of objects, we developed an object-action relation judgement task, requiring participants to map 27 objects with 14 actions. The 27 object images were pre-randomly divided into three groups (nine images each) to form nine-box grids for display convenience. The 14 actions covered common interactions between human and objects or environments identified in the kinetics human action video dataset (<xref ref-type="bibr" rid="c33">Kay et al., 2017</xref>).</p>
<p>The task comprised 42 trials (14 actions × 3 object groups) in total. In each trial, one group of object images (nine object images) and a question asking the appropriateness of applying a specific action to each object were shown (e.g., “Which objects are sit-able?”, see <xref rid="fig1" ref-type="fig">Fig. 1b</xref>, top panel). Participants were asked to choose the objects that afforded the specific action according to their own senses. They were informed that there were no right or wrong answers. Each object-action combination would only be presented once during the task. From this task, we would calculate the percentage that one object was judged affording each of the 14 actions across participants. Since previous research has demonstrated a fundamental separation between the processing of animate and inanimate objects (e.g., <xref ref-type="bibr" rid="c36">Konkle &amp; Caramazza, 2013</xref>), and the affordances of inanimate objects differ from those of animate objects (<xref ref-type="bibr" rid="c18">Gibson, 1979</xref>), we only include 24 inanimate objects in the following analysis by excluding 3 animate objects (animals: bird, dog, and horse).</p>
</sec>
<sec id="s5c2">
<title>Manipulation of body schema</title>
<p>To manipulate participants’ perceived body schema, we asked the participants to imagine themselves as small as a cat, or as large as an elephant. Each participant was randomly assigned to one body-schema condition. Before the experiment we would present an instruction screen with an illustration before the experiment start: “Please imagine that you have now grown smaller/larger than your real size, to roughly the same size as a cat/an elephant, as shown in the image below. Please answer the following questions based on this imagined situation.” The illustration was also presented in each trial, above the action question and the object images. At the end of the task, as a manipulation check, participants were asked to indicate their imagined body size by responding to the question: “What is the approximate height (cm) you imagine yourself to be during the whole task?”</p>
</sec>
<sec id="s5c3">
<title>Object-action relation judgement task for large language models</title>
<p>To test the perceived affordance of the same set of objects by large language models (LLMs), BERT (Bidirectional Encoder Representations from Transformers), GPT-2, and ChatGPT models (based on GPT-3.5 and GPT-4, respectively) were tasked with the same object-action judgement task. Different from the human task, nouns were presented to the models instead of object images (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>, bottom panel, for an example).</p>
<p>For BERT, the task was formatted as a mask-filling task, in which the inputs were questions such as “Among airplanes, kettles, plates, umbrellas, laptops, beds, [MASK] can be sit-able.”. We recorded the likelihood score that BERT provided for each listed object at the masked position. For the example question, the possibility score for the word “airplane” was 0.00026.</p>
<p>For GPT-2, the input questions were like, “Among airplanes, kettles, plates, umbrellas, laptops, beds, the thing that can be sit-able is the [blank space].” The likelihood scores GPT-2 provided for each listed object in the position after the input sentence (blank space) were recorded.</p>
<p>To mimic sampling from human participants, we ran BERT and GPT-2 each for 20 times with different random seeds in the dropout layers, considering them as different subjects.</p>
<p>For ChatGPT models, the task was in a direct question-and-answer format. We asked, for example, “Which objects are sit-able: ‘airplane, kettle, plate, …brick’? ” and the models responded by naming a subset of the object list. To get the probability for each object-action pair, ChatGPT models were run on the same task 20 times, with each new conversation on the OpenAI website (<ext-link ext-link-type="uri" xlink:href="https://chat.openai.com/chat">https://chat.openai.com/chat</ext-link>) considered as one subject. The percentage that an object was judged affording each of the 14 actions was calculated by averaging the output across conversations.</p>
</sec>
<sec id="s5c4">
<title>Representational similarity matrix for perceived affordance</title>
<p>For each object, we calculated the probability that it was judged affording each of the 14 actions across participants to create a 14-dimension vector. Affordance similarity (<italic>r</italic>) between each object pair was then calculated based on the Pearson’s correlation between these affordance vectors. A 24 × 24 symmetric matrix was then generated, with the affordance similarity between object <italic>i</italic> and object <italic>j</italic> being denoted in cell (<italic>i</italic>,<italic>j</italic>). A hierarchical clustering analysis was performed and visualized with seaborn clustermap (<xref ref-type="bibr" rid="c66">Waskom, 2021</xref>).</p>
</sec>
<sec id="s5c5">
<title>Affordance similarity between neighbouring size ranks</title>
<p>To test the relationship between object affordance and object sizes, we first averaged the affordance vector among objects within each size rank. Next, the Pearson’s correlation between the average vectors of neighboring size ranks was calculated as the similarity index for each pair of neighboring size ranks, representing how similar the affordance collectively provided by objects in these two ranks. Pearson and Filon’s (1898) Z, implemented in R package “cocor” (<xref ref-type="bibr" rid="c13">Diedenhofen &amp; Musch, 2015</xref>) was used to evaluate the significance of these similarities (alpha level = .05, one-tail test).</p>
</sec>
<sec id="s5c6">
<title>Size similarity between neighbouring size ranks</title>
<p>The size of each object was indexed by its real-world size documented in <xref ref-type="bibr" rid="c34">Konkle and Oliva (2011)</xref>. Size similarity between size rank <italic>i</italic> to <italic>j</italic> was represented as the difference between the averaged diagonal sizes of objects in size rank <italic>i</italic> and <italic>j</italic> relative to that of objects in rank <italic>i</italic>:
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="533336v3_ueqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula></p>
</sec>
<sec id="s5c7">
<title>Object-level affordance similarity</title>
<p>This analysis focused on objects within size rank 3 to 6. Pearson’s correlation between affordance vectors were conducted for objects within the same size rank as well as for objects from adjacent ranks. We traversed all possible object pairs, and plotted the resulting correlation values against the mean sizes of the two objects. We also plotted the average similarity indexes across objects of the same rank composition.</p>
</sec>
<sec id="s5c8">
<title>Trough value</title>
<p>To quantify the magnitude of the trough (sharp decrease) observed in the affordance similarity curve, we first measured the trough value by subtracting the similarity value at the trough from the similarity values at its two banks (the sites neighboring the trough site):
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="533336v3_ueqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where 𝑟<sub>𝑖</sub> indicates the affordance similarity between size rank <italic>i</italic> and size rank <italic>i</italic>+1. The higher the trough value is, the larger the decrease is.</p>
<p>A permutation test was conducted to evaluate if the trough value was significant above zero for both LLMs and human data. The <italic>p</italic>-value for this test follows the formula adapted from Unpingco’s (2016):
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="533336v3_ueqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
where 𝑇<sub>𝑜𝑏𝑠</sub>is the observed trough value, and <italic>I</italic> is the indicator function. Under the alpha level of 0.05, if <italic>p</italic> &lt; .05, then the 𝑇<sub>𝑜𝑏𝑠</sub> is considered a significant value above zero.</p>
</sec>
</sec>
<sec id="s5d">
<title>fMRI experiment</title>
<p>The fMRI scanning consisted of one high-resolution T1 anatomical run and four task runs for each participant. In each task run, participants performed four action blocks (grasp, kick, lift, and sit). The block order was counterbalanced across runs. Within each block (see <xref rid="fig3" ref-type="fig">Fig. 3a</xref>), an introduction screen showing a question “Which objects are [grasp, kick, lift, sit]-able” was presented for 2 s at the beginning to indicate the action type, followed by 20 object images (4 objects × 5 exemplars). The object images were presented in a random order, for 2s each, with a jittered inter-stimulus interval (ISI) varying between 2-4s. Participants were asked to judge whether the object shown was grasp/kick/lift/sit-able or not by pressing corresponding buttons (e.g., yes: right index finger; no: left index finger). The response buttons were also counterbalanced across participants. The task run lasted for 464s in total, with the four blocks separated by 10s fixation periods.</p>
<p>With this design, we were able to measure the neural activation of objects within agent size range and those beyond. Further, for each object, there would be congruent trials (e.g, grasp-able – bottle: affordance = 1) and incongruent trials (e.g., sit-able – bottle: affordance = 0). We were then able to locate the brain regions representing the objects’ affordance by comparing trials in which the presented objects afford the presented action option with those do not, i.e., to locate the regions showing congruency effect (congruent-incongruent).</p>
</sec>
<sec id="s5e">
<title>fMRI Data Acquisition</title>
<p>Imaging data were collected using a 3T Siemens Prisma MRI scanner with a 64-channel phase-arrayed head coil at the Centre for Biomedical Imaging Research in Tsinghua University. High-resolution T1-weighted images were acquired with a magnetization-prepared rapid acquisition gradient-echo (MPRAGE) sequence (TR/TE = 2530/2.27 ms, flip angle = 7°, voxel resolution = 1×1×1 mm). Functional blood-oxygen-level-dependent (BOLD) images were acquired with a T2∗-weighted gradient echo-planar sequence (TR/TE = 2000/34.0 ms, flip angle = 90°, voxel resolution = 2×2×2 mm, FOV = 200×200 mm). Earplugs were used to attenuate the scanner noise, and a foam pillow and extendable padded head clamps were used to restrain head motion. All the stimuli were projected onto a screen at the back of the scanner with a resolution of 1024 × 768, and were viewed from a distance of approximately 110 cm via a mirror placed on the head coil.</p>
</sec>
<sec id="s5f">
<title>fMRI Data Analyses</title>
<p>Structural T1 and functional images were preprocessed using FSL (FMRIB’s Software Library, <ext-link ext-link-type="uri" xlink:href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki</ext-link>) v6.0.5 (<xref ref-type="bibr" rid="c32">Jenkinson et al., 2012</xref>). A standard preprocessing pipeline was applied, including skull stripping using the BET (Brain Extraction Tool; <xref ref-type="bibr" rid="c54">Smith, 2002</xref>), slice-timing correction, motion correction using the MCFLIRT method (<xref ref-type="bibr" rid="c31">Jenkinson et al., 2002</xref>), temporal high-pass filtering (100s), and spatial smoothing using a Gaussian kernel of full width half magnitude (FWHM) 5mm. Each run’s functional data were registered to a T1-weighted standard image (MNI152) with FLIRT.</p>
<p>For functional data analysis, a first-level voxel-wise general linear models (GLM) implemented in a FEAT analysis was performed on each run separately. To get neural activation maps for objects within and beyond versus baseline, the GLM included 3 regressors: objects within body size (bottle and football), objects beyond body size (bed and piano), and fixation period as baseline; ISI period, response key press and introduction image were included as 3 nuisance factors. The resultant first-level contrasts of parameter estimates (COPE) were entered into the next higher-level group analyses, performed using a random-effects model (FLAME stage 1, <xref ref-type="bibr" rid="c3">Beckmann et al., 2003</xref>). We focused on two critical contrasts: objects within vs. fixation, and objects beyond vs. fixation, and the conjunction of these two contrasts. The resulting Z-statistic images were thresholded at Z &gt; 2.3, <italic>p</italic> = .05 (Worsley, 2000), and corrected for multiple comparisons using an adjusted cluster-wise (FWE: family-wise error) significance threshold of <italic>p</italic> = 0.05.</p>
<sec id="s5f1">
<title>Region of interest (ROI) definition</title>
<p>Eight ROIs (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>) of brain regions involved in affordance processing were selected based on the overlap of the outcomes from the whole-brain conjunction map (areas activated for both objects within and beyond) and corresponding functional atlases (pFs and LO from <xref ref-type="bibr" rid="c71">Zhen et al., 2015</xref>; SPL and M1 from <xref ref-type="bibr" rid="c15">Fan et al., 2016</xref>). For pFs and LO’s atlases, the probabilistic activation maps (PAM) were thresholded at around 70%, in which map each voxel contains a percentage of participants who showed activation for seeing objects versus baseline in Zhen et al.’s study, resulting 266 voxels in lpFs, 427 voxels in rpFs, 254 voxels in lLO and 347 voxels in rLO. For SPL and M1, the probabilistic activation maps were thresholded at around 80%, resulting 661 voxels in lSPL, 455 voxels in rSPL, 378 voxels in lM1, and 449 voxels in rM1. Homologous areas within the cortical hemispheres were merged in the following ROI analysis.</p>
</sec>
<sec id="s5f2">
<title>Affordance congruency effect</title>
<p>For the affordance congruency effect of each object type, we modelled another GLM containing 5 regressors: congruent conditions for objects within/beyond, respectively, incongruent conditions for objects within/beyond, respectively, and fixation period as baseline; ISI period, response key press and introduction image were included as 3 nuisance factors. The resultant first-level COPEs were subjected the following ROI analysis. A repeated-measures ANOVA with Object type (WITHIN and BEYOND) and Congruency (Congruent, Incongruent) as within-subjects factors was run on the average beta values (contrast estimate) extracted from their respective contrasts versus the fixation for each ROI.</p>
<p>To search all the possible brain regions that revealed congruency effect of objects beyond, we also ran a whole-brain analysis on the contrast between congruent vs. incongruent condition for objects beyond. The corresponding first-level COPE was entered into the group-level analyses with a random-effects model (FLAME stage 1, <xref ref-type="bibr" rid="c3">Beckmann et al., 2003</xref>). The resulting Z-statistic images were thresholded at Z &gt; 2.3, p = .05 (Worsley, 2000), and corrected for multiple comparisons using an adjusted cluster-wise (FWE: family-wise error) significance threshold of <italic>p</italic> = 0.05.</p>
</sec>
</sec>
</sec>
<sec id="d1e983" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1071">
<label>Supplemental Material</label>
<media xlink:href="supplements/533336_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This study was funded by Natural Science Foundation of China (31600925, 31861143039), Beijing Municipal Science &amp; Technology Commission, Administrative Commission of Zhongguancun Science Park (Z221100002722012), Tsinghua University Guoqiang Institute (2020GQG1016), and Beijing Academy of Artificial Intelligence (BAAI).</p>
</ack>
<sec id="s6">
<title>Data availability</title>
<p>The data and the code that support the findings of this study are available from the corresponding author upon reasonable request.</p>
</sec>
<sec id="s7">
<title>Declaration of conflicting interests</title>
<p>The author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Barsalou</surname>, <given-names>L. W</given-names></string-name>. (<year>1999</year>). <article-title>Perceptual symbol systems</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>22</volume>(<issue>4</issue>), <fpage>637</fpage>–<lpage>660</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Barsalou</surname>, <given-names>L. W</given-names></string-name>. (<year>2008</year>). <article-title>Grounded cognition</article-title>. <source>Annual Review of Psychology</source>, <volume>59</volume>, <fpage>617</fpage>–<lpage>645</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M</given-names></string-name>. (<year>2003</year>). <article-title>General multilevel linear modeling for group analysis in FMRI</article-title>. <source>NeuroImage</source>, <volume>20</volume>(<issue>2</issue>), <fpage>1052</fpage>–<lpage>1063</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Binkofski</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Fink</surname>, <given-names>G. R.</given-names></string-name>, <string-name><surname>Geyer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Buccino</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gruber</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Shah</surname>, <given-names>N. J.</given-names></string-name>, … &amp; <string-name><surname>Freund</surname>, <given-names>H. J.</given-names></string-name> (<year>2002</year>). <article-title>Neural activity in human primary motor cortex areas 4a and 4p is modulated differentially by attention to action</article-title>. <source>Journal of Neurophysiology</source>, <volume>88</volume>(<issue>1</issue>), <fpage>514</fpage>–<lpage>519</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Bornstein</surname>, <given-names>M. H.</given-names></string-name>, &amp; <string-name><surname>Korda</surname>, <given-names>N. O</given-names></string-name>. (<year>1984</year>). <article-title>Discrimination and matching within and between hues measured by reaction times: Some implications for categorical perception and levels of information processing</article-title>. <source>Psychological Research</source>, <volume>46</volume>(<issue>3</issue>), <fpage>207</fpage>–<lpage>222</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="other"><string-name><surname>Brown</surname>, <given-names>T. B.</given-names></string-name>, <string-name><surname>Mann</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ryder</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Subbiah</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kaplan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dhariwal</surname>, <given-names>P.</given-names></string-name>, … &amp; <string-name><surname>Amodei</surname>, <given-names>D.</given-names></string-name> (<year>2020</year>). <article-title>Language models are few-shot learners</article-title>. arXiv 2020. <italic>arXiv preprint arXiv:2005.14165, 4</italic>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Campanella</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Chrysochoos</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Bruyer</surname>, <given-names>R</given-names></string-name>. (<year>2001</year>). <article-title>Categorical perception of facial gender information: Behavioural evidence and the face-space metaphor</article-title>. <source>Visual Cognition</source>, <volume>8</volume>(<issue>2</issue>), <fpage>237</fpage>–<lpage>262</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Casasanto</surname>, <given-names>D</given-names></string-name>. (<year>2011</year>). <article-title>Different bodies, different minds: the body specificity of language and thought</article-title>. <source>Current Directions in Psychological Science</source>, <volume>20</volume>(<issue>6</issue>), <fpage>378</fpage>–<lpage>383</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Castiello</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>K. M. B.</given-names></string-name>, &amp; <string-name><surname>Stelmach</surname>, <given-names>G. E</given-names></string-name>. (<year>1993</year>). <article-title>Reach to grasp: the natural response to perturbation of object size</article-title>. <source>Experimental Brain Research</source>, <volume>94</volume>(<issue>1</issue>), <fpage>163</fpage>–<lpage>178</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Cesari</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Newell</surname>, <given-names>K. M</given-names></string-name>. (<year>2000</year>). <article-title>Body-scaled transitions in human grip configurations</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>26</volume>(<issue>5</issue>), <fpage>1657</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Chemero</surname>, <given-names>A</given-names></string-name>. (<year>2013</year>). <article-title>Radical embodied cognitive science</article-title>. <source>Review of General Psychology</source>, <volume>17</volume>(<issue>2</issue>), <fpage>145</fpage>–<lpage>150</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Devlin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Toutanova</surname>, <given-names>K</given-names></string-name>. (<year>2018</year>). <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source>arXiv preprint arXiv:1810.04805</source>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Diedenhofen</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Musch</surname>, <given-names>J</given-names></string-name>. (<year>2015</year>). <article-title>cocor: A comprehensive solution for the statistical comparison of correlations</article-title>. <source>PloS One</source>, <volume>10</volume>(<issue>4</issue>), <fpage>e0121945</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Driess</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Xia</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Sajjadi</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Lynch</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Chowdhery</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ichter</surname>, <given-names>B.</given-names></string-name>, … &amp; <string-name><surname>Florence</surname>, <given-names>P.</given-names></string-name> (<year>2023</year>). <article-title>PaLM-E: An Embodied Multimodal Language Model</article-title>. <source>arXiv preprint arXiv</source>:<volume>2303</volume>.<fpage>03378</fpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Fan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Zhuo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>L.</given-names></string-name>, … &amp; <string-name><surname>Jiang</surname>, <given-names>T.</given-names></string-name> (<year>2016</year>). <article-title>The human brainnetome atlas: a new brain atlas based on connectional architecture</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>8</issue>), <fpage>3508</fpage>–<lpage>3526</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Filimon</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Hagler</surname>, <given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Sereno</surname>, <given-names>M. I</given-names></string-name>. (<year>2007</year>). <article-title>Human cortical representations for reaching: mirror neurons for execution, observation, and imagery</article-title>. <source>NeuroImage</source>, <volume>37</volume>(<issue>4</issue>), <fpage>1315</fpage>–<lpage>1328</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="book"><string-name><surname>Gallagher</surname>, <given-names>S</given-names></string-name>. (<year>2017</year>). <source>Enactivist interventions: Rethinking the mind</source>. <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c17a"><label>17a.</label><mixed-citation publication-type="book"><string-name><surname>Gibbs</surname> <suffix>Jr</suffix>, <given-names>R. W.</given-names></string-name> (<year>2005</year>). <source>Embodiment and cognitive science</source>. <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="other"><string-name><surname>Gibson</surname>, <given-names>J. J.</given-names></string-name> (<year>1979</year>). <article-title>The ecological approach to visual perception: classic edition</article-title>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Glenberg</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Gallese</surname>, <given-names>V</given-names></string-name>. (<year>2012</year>). <article-title>Action-based language: A theory of language acquisition, comprehension, and production</article-title>. <source>Cortex</source>, <volume>48</volume>(<issue>7</issue>), <fpage>905</fpage>–<lpage>922</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Glenberg</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Witt</surname>, <given-names>J. K.</given-names></string-name>, &amp; <string-name><surname>Metcalfe</surname>, <given-names>J</given-names></string-name>. (<year>2013</year>). <article-title>From the revolution to embodiment: 25 years of cognitive psychology</article-title>. <source>Perspectives on Psychological Science</source>, <volume>8</volume>(<issue>5</issue>), <fpage>573</fpage>–<lpage>585</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Goldstone</surname>, <given-names>R. L.</given-names></string-name>, &amp; <string-name><surname>Hendrickson</surname>, <given-names>A. T</given-names></string-name>. (<year>2010</year>). <article-title>Categorical perception</article-title>. <source>Wiley Interdisciplinary Reviews: Cognitive Science</source>, <volume>1</volume>(<issue>1</issue>), <fpage>69</fpage>–<lpage>78</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Greeno</surname>, <given-names>J. G</given-names></string-name>. (<year>1994</year>). <article-title>Gibson’s Affordances</article-title>. <source>Psychological Review</source>, <volume>101</volume>(<issue>2</issue>), <fpage>336</fpage>–<lpage>342</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kushnir</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hendler</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Malach</surname>, <given-names>R</given-names></string-name>. (<year>2000</year>). <article-title>The dynamics of object-selective activation correlate with recognition performance in humans</article-title>. <source>Nature Neuroscience</source>, <volume>3</volume>(<issue>8</issue>), <fpage>837</fpage>–<lpage>843</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Gupta</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Savarese</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Fei-Fei</surname>, <given-names>L</given-names></string-name>. (<year>2021</year>). <article-title>Embodied intelligence via learning and evolution</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>), <fpage>5721</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="other"><string-name><surname>Harnad</surname>, <given-names>S.</given-names></string-name> (<year>1987</year>). <article-title>Psychophysical and cognitive aspects of categorical perception: A critical overview</article-title>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Dickter</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Kidder</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kwok</surname>, <given-names>W. Y.</given-names></string-name>, <string-name><surname>Corriveau</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Van Wicklin</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2019</year>). <article-title>THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title>. <source>PloS One</source>, <volume>14</volume>(<issue>10</issue>), <fpage>e0223792</fpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Bankson</surname>, <given-names>B. B.</given-names></string-name>, <string-name><surname>Harel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name>, &amp; <string-name><surname>Cichy</surname>, <given-names>R. M</given-names></string-name>. (<year>2018</year>). <article-title>The representational dynamics of task and object processing in humans</article-title>. <source>eLife</source>, <volume>7</volume>, <fpage>e32816</fpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Hestness</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Narang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ardalani</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Diamos</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Jun</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kianinejad</surname>, <given-names>H.</given-names></string-name>, … &amp; <string-name><surname>Zhou</surname>, <given-names>Y.</given-names></string-name> (<year>2017</year>). <article-title>Deep learning scaling is predictable, empirically</article-title>. <source>arXiv preprint arXiv</source>:<volume>1712</volume>.<fpage>00409</fpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Huang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>J</given-names></string-name>. (<year>2022</year>). <article-title>Real-world size of objects serves as an axis of object space</article-title>. <source>Communications Biology</source>, <volume>5</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="book"><string-name><surname>Hutto</surname>, <given-names>D. D.</given-names></string-name>, &amp; <string-name><surname>Myin</surname>, <given-names>E</given-names></string-name>. (<year>2012</year>). <source>Radicalizing enactivism: Basic minds without content</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bannister</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S</given-names></string-name>. (<year>2002</year>). <article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title>. <source>NeuroImage</source>, <volume>17</volume>(<issue>2</issue>), <fpage>825</fpage>–<lpage>841</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M</given-names></string-name>. (<year>2012</year>). <article-title>Fsl</article-title>. <source>NeuroImage</source>, <volume>62</volume>(<issue>2</issue>), <fpage>782</fpage>–<lpage>790</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="other"><string-name><surname>Kay</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Carreira</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Simonyan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Hillier</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Vijayanarasimhan</surname>, <given-names>S.</given-names></string-name>, … &amp; <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <article-title>The kinetics human action video dataset</article-title>. <italic>arXiv preprint arXiv:1705.06950</italic>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name>. (<year>2011</year>). <article-title>Canonical visual size for real-world objects</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>37</volume>(<issue>1</issue>), <fpage>23</fpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name>. (<year>2012</year>). <article-title>A real-world size organization of object responses in occipitotemporal cortex</article-title>. <source>Neuron</source>, <volume>74</volume>(<issue>6</issue>), <fpage>1114</fpage>–<lpage>1124</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Caramazza</surname>, <given-names>A</given-names></string-name>. (<year>2013</year>). <article-title>Tripartite organization of the ventral stream by animacy and object size</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>25</issue>), <fpage>10235</fpage>–<lpage>10242</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Lakoff</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Johnson</surname>, <given-names>M</given-names></string-name>. (<year>1980</year>). <article-title>The metaphorical structure of the human conceptual system</article-title>. <source>Cognitive Science</source>, <volume>4</volume>(<issue>2</issue>), <fpage>195</fpage>–<lpage>208</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Liberman</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Hoffman</surname>, <given-names>H. S.</given-names></string-name>, &amp; <string-name><surname>Griffith</surname>, <given-names>B. C</given-names></string-name>. (<year>1957</year>). <article-title>The discrimination of speech sounds within and across phoneme boundaries</article-title>. <source>Journal of Experimental Psychology</source>, <volume>54</volume>(<issue>5</issue>), <fpage>358</fpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Magri</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Caramazza</surname>, <given-names>A</given-names></string-name>. (<year>2021</year>). <article-title>The contribution of object size, manipulability, and stability on neural responses to inanimate objects</article-title>. <source>NeuroImage</source>, <volume>237</volume>, <fpage>118098</fpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Malach</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Reppas</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Benson</surname>, <given-names>R. R.</given-names></string-name>, <string-name><surname>Kwong</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kennedy</surname>, <given-names>W. A.</given-names></string-name>, … &amp; <string-name><surname>Tootell</surname>, <given-names>R. B.</given-names></string-name> (<year>1995</year>). <article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>92</volume>(<issue>18</issue>), <fpage>8135</fpage>–<lpage>8139</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Mark</surname>, <given-names>L. S</given-names></string-name>. (<year>1987</year>). <article-title>Eyeheight-scaled information about affordances: a study of sitting and stair climbing</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>13</volume>(<issue>3</issue>), <fpage>361</fpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Matić</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>de Beeck</surname>, <given-names>H. O.</given-names></string-name>, &amp; <string-name><surname>Bracci</surname>, <given-names>S.</given-names></string-name> (<year>2020</year>). <article-title>It’s not all about looks: The role of object shape in parietal representations of manual tools</article-title>. <source>Cortex</source>, <volume>133</volume>, <fpage>358</fpage>–<lpage>370</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="book"><string-name><surname>Merleau-Ponty</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>C</given-names></string-name>. (<year>1962</year>). <source>Phenomenology of Perception</source> (Vol. 2012). <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Newell</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Scully</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>McDonald</surname>, <given-names>P. V.</given-names></string-name>, &amp; <string-name><surname>Baillargeon</surname>, <given-names>R</given-names></string-name>. (<year>1989</year>). <article-title>Task constraints and infant grip configurations</article-title>. <source>Developmental Psychobiology: The Journal of the International Society for Developmental Psychobiology</source>, <volume>22</volume>(<issue>8</issue>), <fpage>817</fpage>–<lpage>831</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="web"><collab>OpenAI.</collab> (<year>2022</year>). <source>ChatGPT</source>. <ext-link ext-link-type="uri" xlink:href="https://openai.com/blog/chatgpt">https://openai.com/blog/chatgpt</ext-link></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="report"><collab>OpenAI.</collab> (<year>2023</year>). <source>GPT-4 Technical Report</source>. <ext-link ext-link-type="uri" xlink:href="https://cdn.openai.com/papers/gpt-4.pdf">https://cdn.openai.com/papers/gpt-4.pdf</ext-link></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="report"><string-name><surname>Ouyang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Almeida</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wainwright</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mishkin</surname>, <given-names>P.</given-names></string-name>, … &amp; <string-name><surname>Lowe</surname>, <given-names>R.</given-names></string-name> (<year>2022</year>). <article-title>Training language models to follow instructions with human feedback</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>35</volume>, <fpage>27730</fpage>–<lpage>27744</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Park</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>T. F.</given-names></string-name>, <string-name><surname>Greene</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Oliva</surname>, <given-names>A</given-names></string-name>. (<year>2011</year>). <article-title>Disentangling scene content from spatial boundary: complementary roles for the parahippocampal place area and lateral occipital complex in representing real-world scenes</article-title>. <source>Journal of Neuroscience</source>, <volume>31</volume>(<issue>4</issue>), <fpage>1333</fpage>–<lpage>1340</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="other"><string-name><surname>Pearson</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Filon</surname>, <given-names>L. N. G.</given-names></string-name> (<year>1898</year>). <article-title>VII. Mathematical contributions to the theory of evolution.—IV. On the probable errors of frequency constants and on the influence of random selection on variation and correlation</article-title>. <source>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</source>, (<issue>191</issue>), <fpage>229</fpage>-<lpage>311</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Prindle</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Carello</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Turvey</surname>, <given-names>M. T</given-names></string-name>. (<year>1980</year>). <article-title>Animal-environment mutuality and direct perception</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>3</volume>(<issue>3</issue>), <fpage>395</fpage>–<lpage>397</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Pylyshyn</surname>, <given-names>Z</given-names></string-name>. (<year>1999</year>). <article-title>Is vision continuous with cognition?: The case for cognitive impenetrability of visual perception</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>22</volume>(<issue>3</issue>), <fpage>341</fpage>–<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="other"><string-name><surname>Radford</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Narasimhan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Salimans</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> (<year>2018</year>). <article-title>Improving language understanding by generative pre-training</article-title>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Roux-Sibilon</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kalénine</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pichat</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Peyrin</surname>, <given-names>C</given-names></string-name>. (<year>2018</year>). <article-title>Dorsal and ventral stream contribution to the paired-object affordance effect</article-title>. <source>Neuropsychologia</source>, <volume>112</volume>, <fpage>125</fpage>–<lpage>134</lpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>S. M</given-names></string-name>. (<year>2002</year>). <article-title>Fast robust automated brain extraction</article-title>. <source>Human Brain Mapping</source>, <volume>17</volume>(<issue>3</issue>), <fpage>143</fpage>–<lpage>155</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Gasser</surname>, <given-names>M</given-names></string-name>. (<year>2005</year>). <article-title>The development of embodied cognition: Six lessons from babies</article-title>. <source>Artificial life</source>, <volume>11</volume>(<issue>1-2</issue>), <fpage>13</fpage>–<lpage>29</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Snow</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Pettypiece</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>McAdam</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>McLean</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Stroman</surname>, <given-names>P. W.</given-names></string-name>, <string-name><surname>Goodale</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Culham</surname>, <given-names>J. C</given-names></string-name>. (<year>2011</year>). <article-title>Bringing the real world into the fMRI scanner: Repetition effects for pictures versus real objects</article-title>. <source>Scientific Reports</source>, <volume>1</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Stanfield</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Zwaan</surname>, <given-names>R. A</given-names></string-name>. (<year>2001</year>). <article-title>The effect of implied orientation derived from verbal context on picture recognition</article-title>. <source>Psychological Science</source>, <volume>12</volume>(<issue>2</issue>), <fpage>153</fpage>–<lpage>156</lpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="book"><string-name><surname>Thompson</surname>, <given-names>E</given-names></string-name>. (<year>2010</year>). <source>Mind in life: Biology, phenomenology, and the sciences of mind</source>. <publisher-name>Harvard University Press</publisher-name>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Troiani</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Stigliani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Epstein</surname>, <given-names>R. A</given-names></string-name>. (<year>2014</year>). <article-title>Multiple object properties drive scene-selective regions</article-title>. <source>Cerebral Cortex</source>, <volume>24</volume>(<issue>4</issue>), <fpage>883</fpage>–<lpage>897</lpage>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><surname>Tucker</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Ellis</surname>, <given-names>R</given-names></string-name>. (<year>2004</year>). <article-title>Action priming by briefly presented objects</article-title>. <source>Acta psychologica</source>, <volume>116</volume>(<issue>2</issue>), <fpage>185</fpage>–<lpage>203</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="book"><string-name><surname>Unpingco</surname>, <given-names>J</given-names></string-name>. (<year>2016</year>). <source>Python for probability, statistics, and machine learning (Vol. 1)</source>. <publisher-name>Springer International Publishing</publisher-name>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><surname>Vainio</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Ellis</surname>, <given-names>R</given-names></string-name>. (<year>2020</year>). <article-title>Action inhibition and affordances associated with a non-target object: An integrative review</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>112</volume>, <fpage>487</fpage>–<lpage>502</lpage>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="book"><string-name><surname>Varela</surname>, <given-names>F. J.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Rosch</surname>, <given-names>E</given-names></string-name>. (<year>2017</year>). <source>The embodied mind, revised edition: Cognitive science and human experience</source>. <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Warren</surname>, <given-names>W. H</given-names></string-name>. (<year>1984</year>). <article-title>Perceiving affordances: visual guidance of stair climbing</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>10</volume>(<issue>5</issue>), <fpage>683</fpage>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><surname>Warren Jr</surname>, <given-names>W. H.</given-names></string-name>, &amp; <string-name><surname>Whang</surname>, <given-names>S.</given-names></string-name> (<year>1987</year>). <article-title>Visual guidance of walking through apertures: body-scaled information for affordances</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>13</volume>(<issue>3</issue>), <fpage>371</fpage>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><surname>Waskom</surname>, <given-names>M. L</given-names></string-name>. (<year>2021</year>). <article-title>Seaborn: statistical data visualization</article-title>. <source>Journal of Open Source Software</source>, <volume>6</volume>(<issue>60</issue>), <fpage>3021</fpage>.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname>, <given-names>M</given-names></string-name>. (<year>2002</year>). <article-title>Six views of embodied cognition</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>9</volume>(<issue>4</issue>), <fpage>625</fpage>–<lpage>636</lpage>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Golonka</surname>, <given-names>S</given-names></string-name>. (<year>2013</year>). <article-title>Embodied cognition is not what you think it is</article-title>. <source>Frontiers in Psychology</source>, <volume>4</volume>, <fpage>58</fpage>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><surname>Worsley</surname>, <given-names>K. J</given-names></string-name>. (<year>2001</year>). <article-title>Statistical analysis of activation images</article-title>. <source>Functional MRI: An Introduction to Methods</source>, <volume>14</volume>(<issue>1</issue>), <fpage>251</fpage>–<lpage>270</lpage>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><surname>Young</surname>, <given-names>A. W.</given-names></string-name>, <string-name><surname>Rowland</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Calder</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Etcoff</surname>, <given-names>N. L.</given-names></string-name>, <string-name><surname>Seth</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Perrett</surname>, <given-names>D. I</given-names></string-name>. (<year>1997</year>). <article-title>Facial expression megamix: Tests of dimensional and category accounts of emotion recognition</article-title>. <source>Cognition</source>, <volume>63</volume>(<issue>3</issue>), <fpage>271</fpage>–<lpage>313</lpage>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><surname>Zhen</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kong</surname>, <given-names>X. Z.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Dang</surname>, <given-names>X.</given-names></string-name>, … &amp; <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <article-title>Quantifying interindividual variability and asymmetry of face-selective regions: a probabilistic functional atlas</article-title>. <source>NeuroImage</source>, <volume>113</volume>, <fpage>13</fpage>–<lpage>25</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90583.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper presents <bold>valuable</bold> findings that shed light on the mental organisation of knowledge about real-world objects. It provides diverse if <bold>incomplete</bold> evidence from behaviour, brain, and large language models that this knowledge is divided categorically between relatively small objects that are at the relevant scale for direct manipulation and larger objects that are outside the typical scope of human affordances for action.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90583.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Ps observed 24 objects and were asked which afforded particular actions (14 action types). Affordances for each object were represented by a 14-item vector, values reflecting the percentage of Ps who agreed on a particular action being afforded by the object. An affordance similarity matrix was generated which reflected similarity in affordances between pairs of objects. Two clusters emerged, reflecting correlations between affordance ratings in objects smaller than body size and larger than body size. These clusters did not correlate themselves. There was a trough in similarity ratings between objects ~105 cm and ~130 cm, arguably reflecting the body size boundary. The authors subsequently provide some evidence that this clear demarcation is not simply an incidental reflection of body size, but likely causally related. This evidence comes in the flavour of requiring Ps to imagine themselves as small as a cat or as large as an elephant and showing a predicted shift in the affordance boundary. The manuscript further demonstrates that ChatGPT (theoretically interesting because it's trained on language alone without sensorimotor information; trained now on words rather than images) showed a similar boundary.</p>
<p>The authors also conducted a small MRI study task where Ps decided whether a probe action was affordable (graspable?) and created a congruency factor according to the answer (yes/no). There was an effect of congruency in the posterior fusiform and superior parietal lobule for objects within body size range, but not outside. No effects in LOC or M1.</p>
<p>The major strength of this manuscript in my opinion is the methodological novelty. I felt the correlation matrices were a clever method for demonstrating these demarcations, the imagination manipulation was also exciting, and the ChatGPT analysis provided excellent food for thought. These findings are important for our understanding of the interactions between action and perception, and hence for researchers from a range of domains of cognitive neuroscience.</p>
<p>The major elements that limit conclusions and I'd recommend to be addressed in a revision include justification of the 80% of Ps removed for the imagination analysis, and consideration that an MRI study with 12 P in this context can really only provide pilot data. I'd also encourage the authors to consider theoretically how else this study could really have turned out and therefore the nature of the theoretical progress.</p>
<p>Specifics:</p>
<p>
1. The main behavioural work appears well-powered (&gt;500 Ps). This sample reduces to 100 for the imagination study, after removing Ps whose imagined heights fell within the human range (100-200 cm). Why 100-200 cm? 100 cm is pretty short for an adult. Removing 80% of data feels like conclusions from the imagination study should be made with caution.</p>
<p>2. There are only 12 Ps in the MRI study, which I think should mean the null effects are not interpreted. I would not interpret these data as demonstrating a difference between SPL and LOC/M1, but rather that some analyses happened to fall over the significance threshold and others did not.</p>
<p>3. I found the MRI ROI selection and definition a little arbitrary and not really justified, which rendered me even more cautious of the results. Why these particular sensory and motor regions? Why M1 and not PMC or SMA? Why SPL and not other parietal regions? Relatedly, ROIs were defined by thresholding pF and LOC at &quot;around 70%&quot; and SPL and M1 &quot;around 80%&quot;, and it is unclear how and why these (different) thresholds were determined.</p>
<p>4. Discussion and theoretical implications. The authors discuss that the MRI results are consistent with the idea we only represent affordances within body size range. But the interpretation of the behavioural correlation matrices was that there was this similarity also for objects larger than body size, but forming a distinct cluster. I therefore found the interpretation of the MRI data inconsistent with the behavioural findings.</p>
<p>5. In the discussion, the authors outline how this work is consistent with the idea that conceptual and linguistic knowledge is grounded in sensorimotor systems. But then reference Barsalou. My understanding of Barsalou is the proposition of a connectionist architecture for conceptual representation. I did not think sensorimotor representation was privileged, but rather that all information communicates with all other to constitute a concept.</p>
<p>6. More generally, I believe that the impact and implications of this study would be clearer for the reader if the authors could properly entertain an alternative concerning how objects may be represented. Of course, the authors were going to demonstrate that objects more similar in size afforded more similar actions. It was impossible that Ps would ever have responded that aeroplanes afford grasping and balls afford sitting, for instance. What do the authors now believe about object representation that they did not believe before they conducted the study? Which accounts of object representation are now less likely?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90583.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>
In this work, the authors seek to test a version of an old idea, which is that our perception of the world and our understanding of the objects in it are deeply influenced by the nature of our bodies and the kinds of behaviours and actions that those objects afford. The studies presented here muster three kinds of evidence for a discontinuity in the encoding of objects, with a mental &quot;border&quot; between objects roughly of human body scale or smaller, which tend to relate to similar kinds of actions that are yet distinct from the kinds of actions implied by human-or-larger scale objects. This is demonstrated through observers' judgments of the kinds of actions different objects afford; through similar questioning of AI large-language models (LLMs); and through a neuroimaging study examining how brain regions implicated in object understanding make distinctions between kinds of objects at human and larger-than-human scales.</p>
<p>Strengths </p>
<p>
The authors address questions of longstanding interest in the cognitive neurosciences -- namely how we encode and interact with the many diverse kinds of objects we see and use in daily life. A key strength of the work lies in the application of multiple approaches, as noted in the summary. Examining the correlations among kinds of objects, with respect to their suitability for different action kinds, is novel, as are the complementary tests of judgments made by LLMs.</p>
<p>Weaknesses </p>
<p>
A limitation of the tests of LLMs may be that it is not always known what kinds of training material was used to build these models, leading to a possible &quot;black box&quot; problem. Further, presuming that those models are largely trained on previous human-written material, it may not necessarily be theoretically telling that the &quot;judgments&quot; of these models about action-object pairs show human-like discontinuities. Indeed, verbal descriptions of actions are very likely to mainly refer to typical human behaviour, and so the finding that these models demonstrate an affordance discontinuity may simply reflect those statistics, rather than evidence that affordance boundaries can arise independently even without &quot;organism-environment interactions&quot; as the authors claim here.</p>
<p>The authors include a clever manipulation in which participants are asked to judge action-object pairs, having first adopted the imagined size of either a cat or an elephant, showing that the discontinuity in similarity judgments effectively moved to a new boundary closer to the imagined scale than the veridical human scale. The dynamic nature of the discontinuity suggests a different interpretation of the authors' main findings. It may be that action affordance is not a dimension that stably characterises the long-term representation of object kinds, as suggested by the authors' interpretation of their brain findings, for example. Rather these may be computed more dynamically, &quot;on the fly&quot; in response to direct questions (as here) or perhaps during actual action behaviours with objects in the real world.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90583.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
Feng et al. test the hypothesis that human body size constrains the perception of object affordances, whereby only objects that are smaller than the body size will be perceived as useful and manipulable parts of the environment, whereas larger objects will be perceived as &quot;less interesting components.&quot;</p>
<p>To test this idea, the study employs a multi-method approach consisting of three parts:</p>
<p>In the first part, human observers classify a set of 24 objects that vary systematically in size (e.g., ball, piano, airplane) based on 14 different affordances (e.g., sit, throw, grasp). Based on the average agreement of ratings across participants, the authors compute the similarity of affordance profiles between all object pairs. They report evidence for two homogenous object clusters that are separated based on their size with the boundary between clusters roughly coinciding with the average human body size. In follow-up experiments, the authors show that this boundary is larger/smaller in separate groups of participants who are instructed to imagine themselves as an elephant/cat.</p>
<p>In the second part, the authors ask different large language models (LLMs) to provide ratings for the same set of objects and affordances and conduct equivalent analyses on the obtained data. Some, but not all, of the models produce patterns of ratings that appear to show similar boundary effects, though less pronounced and at a different boundary size than in humans.</p>
<p>In the third part, the authors conduct an fMRI experiment. Human observers are presented with four different objects of different sizes and asked if these objects afford a small set of specific actions. Affordances are either congruent or incongruent with objects. Contrasting brain activity on incongruent trials against brain activity on congruent trials yields significant effects in regions within the ventral and dorsal visual stream, but only for small objects and not for large objects.</p>
<p>The authors interpret their findings as support for their hypothesis that human body size constrains object perception. They further conclude that this effect is cognitively penetrable, and only partly relies on sensorimotor interaction with the environment (and partly on linguistic abilities).</p>
<p>Strengths:</p>
<p>
The authors examine an interesting and relevant question and articulate a plausible (though somewhat underspecified) hypothesis that certainly seems worth testing. Providing more detailed insights into how object affordances shape perception would be highly desirable. Their method of analyzing similarity ratings between sets of objects seems useful and the multi-method approach is quite original and interesting.</p>
<p>Weaknesses:</p>
<p>
The study presents several shortcomings that clearly weaken the link between the obtained evidence and the drawn conclusions. Below I outline my concerns in no particular order:</p>
<p>1. Even after several readings, it is not entirely clear to me what the authors are proposing and to what extent the conducted work actually speaks to this. In the introduction, the authors write that they seek to test if body size serves not merely as a reference for object manipulation but also &quot;plays a pivotal role in shaping the representation of objects.&quot; This motivation seems rather vague motivation and it is not clear to me how it could be falsified.</p>
<p>2. The authors used only a very small set of objects and affordances in their study and they do not describe in sufficient detail how these stimuli were selected. This renders the results rather exploratory and clearly limits their potential to discover general principles of human perception. Much larger sets of objects and affordances and explicit data-driven approaches for their selection would provide a far more convincing approach and allow the authors to rule out that their results are just a consequence of the selected set of objects and actions.</p>
<p>3. Relatedly, the authors could be more thorough in ruling out potential alternative explanations. Object size likely correlates with other variables that could shape human similarity judgments and the estimated boundary is quite broad (depending on the method, either between 80 and 150 cm or between 105 to 130 cm). More precise estimates of the boundary and more rigorous tests of alternative explanations would add a lot to strengthen the authors' interpretation.</p>
<p>4. Even though the division of the set of objects into two homogenous clusters appears defensible, based on visual inspection of the results, the authors should consider using more formal analysis to justify their interpretation of the data. A variety of metrics exist for cluster analysis (e.g., variation of information, silhouette values) and solutions are typically justified by convergent evidence across different metrics. I would recommend the authors consider using a more formal approach to their cluster definition using some of those metrics.</p>
<p>5. While I appreciate the manipulation of imagined body size, as a way to solidify the link between body size and affordance perception, I find it unfortunate that this is implemented in a between-subjects design, as this clearly leaves open the possibility of pre-existing differences between groups. I certainly disagree with the authors' statement that their findings suggest &quot;a causal link between body size and affordance perception.&quot;</p>
<p>6. The use of LLMs in the current study is not clearly motivated and I find it hard to understand what exactly the authors are trying to test through their inclusion. As noted above, I think that the authors should discuss the putative roles of conceptual knowledge, language, and sensorimotor experience already in the introduction to avoid ambiguity about the derived predictions and the chosen methodology. As it currently stands, I find it hard to discern how the presence of perceptual boundaries in LLMs could constitute evidence for affordance-based perception.</p>
<p>7. Along the same lines, the fMRI study also provides very limited evidence to support the authors' claims. The use of congruency effects as a way of probing affordance perception is not well motivated. What exactly can we infer from the fact a region may be more active when an object is paired with an activity that the object doesn't afford? The claim that &quot;only the affordances of objects within the range of body size were represented in the brain&quot; certainly seems far beyond the data.</p>
<p>Importantly (related to my comments under 2) above), the very small set of objects and affordances in this experiment heavily complicates any conclusions about object size being the crucial variable determining the occurrence of congruency effects.</p>
<p>I would also suggest providing a more comprehensive illustration of the results (including the effects of CONGRUENCY, OBJECT SIZE, and their interaction at the whole-brain level).</p>
<p>Overall, I consider the main conclusions of the paper to be far beyond the reported data. Articulating a clearer theoretical framework with more specific hypotheses as well as conducting more principled analyses on more comprehensive data sets could help the authors obtain stronger tests of their ideas.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.90583.1.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Feng</surname>
<given-names>Xinran</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Xu</surname>
<given-names>Shan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Yuannan</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>Jia</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>We appreciate the insightful comments from three reviewers on our manuscript. These comments help us improve the clarity of this manuscript. We will revise our manuscript comprehensively in subsequent revision, and enclose a detailed response to each of these comments. In this public reply, we focus on (a) clarifying the theoretical motivation and implication of the present study, and (b) discussing the implications of our LLM study. Besides, we provide a brief justification regarding some methodological concerns shared by the reviewers.</p>
<p>1. Theoretical rationale and implication</p>
<p>As we stated in the manuscript, the present study tested whether body size serves as a reference for locomotion and object manipulation, or alternatively, plays a pivotal role in shaping the representation of objects as suggested by Protagoras. Behind this question is the long-lasting debate regarding the representation versus direct perception of affordance.</p>
<p>One outstanding theme shared by many embodied theories of cognition is the replacement hypothesis (e.g., Van Gelder, 1998). This hypothesis challenges the necessity of representation in the sense of computationalism cognitive theories (e.g., Fodor, 1975), which implies discretizing/categorizing inputs and then subjecting them to certain abstraction or symbolization so as to create discrete stand-ins for the input (e.g., representations/states). In this sense, our theoretical motivation can be restated explicitly as to test the ‘representationalization’ of affordance. That is, we tested whether object affordance would simply covary with its continuous constraints such as object size, in line with the representation-free view, or, whether affordance would be ‘representationalized’, in line with the representation-based view, under the constrain of body size. Such representationalization would generate categorization between the affordable (the objects) and those beyond affordance (the environment).</p>
<p>Debates regarding the replacement hypothesis often turn into wrestles on the definition of representation (Shapiro, 2019). The present study tried to avoid this pitfall but examined where the embodied and computational theories make opposite hypotheses: discontinuity. Specifically, we considered two computationalism propositions about representation: (a) representations entail discretization of continuous input, and (b) the product of such discretization (representations) is supramodally accessible (that is, transcending sensorimotor processes). These claims are opposite to the prediction based on the idea of direct perception and other representation-free embodied theories.</p>
<p>Thus, we tested whether, for continuous action-related physical features (such as object size relative to the agents), affordance perception introduces discontinuity and qualitative dissociation, i.e., to allow the sensorimotor input to be assigned into discrete states/kinds, as representations envisioned by computationalists. Alternatively, does the activity directly mirror the input, free from discretization/categorization/abstraction, as proposed by the replacement hypothesis that organisms do not need to re-present the world as they are always in contact with the world in a continuous way?</p>
<p>All the experiment settings and analyses in the present study were organized around this motivation, following a progressive logic chain.</p>
<p>First, we tested the discretization hypothesis, that is, whether affordance leads to discontinuity in perception. Here, the discontinuity in affordance perception would be in line with the representation-based view instead of the representation-free proposals. Second, to ensure that the observed discontinuity can be attributed to the discretization of sensorimotor input involved in human-object interaction rather than amodal sources, such as the discrete abstract concepts of the objects (independent from agent motor capability), we tested the embodied nature of this discontinuity through the body imagination experiment. If there is discontinuity in representing embodied information, this discontinuity should be locked to the motor capacity (constrained by the physical constitution such as body size) of the agent, rather than reflecting independent categorization of the absolute size of the objects. Finally, we probed the supramodality of this embodied discontinuity: whether this discontinuity is accessible beyond the sensorimotor domain. To do this, we leveraged the recent advance in AI and tested whether the discretization observed in affordance perception is supramodally accessible to disembodied agents which lack access to sensorimotor input but only have access to the linguistic materials built upon discretized representations, such as large language models (LLM).</p>
<p>In this way, the experiments in the present study collectively contributed to the debate on the replacement theme of the embodiment of cognition, which serves as one of the three key themes of embodied theories of cognition (Shapiro, 2019). By addressing this theme, we hope to shed light on the nature of representation in, and resulting from, the vision-for-action processing. Our finding regarding discontinuity suggested that sensorimotor input undergoes discretization implied in the computationalism idea of representation. Further, not contradictory to the claims of the embodied theories, these representations do shape processes out of the sensorimotor domain, but after discretization.</p>
<p>1. Implication in the development of LLM-based agents</p>
<p>The finding that affordance was representationalized may have profound implications for the development of LLM-based agents. Traditional robots and non-LLM-based agents require implementation-level action instruction, acting as a tool for human beings to achieve desired results. In contrast, LLM-based agents (for a review, see Wang et al., 2023), such as Auto-GPT and BabyAGI, are able to autonomously perform tasks and achieve desired results based on LLMs’ planning ability. In this sense, LLM-based agents show a primary ability to interact on their own with the world. Generative agents, for instance, the agents in Smallville (Park et al., 2023), are a particularly applauded recent advantage in the school of LLM-based agents, which show even larger potentials in this aspect. Drawing on generative models to simulate human behaviors, these agents can formulate their own memories and goals, generate new environment-dependent behaviors, and interact convincingly with humans and other agents and their environments in the course. This brings new possibilities in resolving the long-lasting challenge in artificial general intelligence (AGI) development, that is, to bestow AI with human-level ability in agent-environment interactions. However, it is worth noting that the present investigation in LLM-based agents is still largely confined to virtual environments. This leaves an open question as to how to equip these agents with the ability of agent-environment physical interaction. Especially, according to embodied theories of cognition, sensorimotor interactions with the environment provide unique knowledge upon which various cognitive domains are built. From this point of view, building agents with human-level ability in agent-environment physical interactions might provide an unreplaceable missing piece for AGI.</p>
<p>By probing the representation of action possibilities (affordances) provided by the environment to the agent (or the absence of them), the present study provided a clue in achieving such ability by illustrating the representationalization of affordance and the supramodality of these representations. For instance, the finding of supramodality may alleviate the doubts about the physical interaction ability of LLM-based agents comparable to biological agents. Specifically, LLM-based agents can leverage the affordance representation distilled into language to interact with the physical world. Indeed, by clarifying and aligning such representation with the physical constitutes of LLM-based agents, and even by explicitly constructing an agent-specific object space, we may facilitate the sensorimotor interactions of LLM-based agents so as to achieve animal-level interaction ability with the world. This in turn may provide new instances for embodied theories.</p>
<p>1. Clarification on incomplete evidence</p>
<p>In response to the methodological and validity concerns of the reviewers, we will provide a point-by-point detailed response to reviewers enclosed with the revised manuscript. Here, we reply to the most prominent concerns.</p>
<p>Reviewers were concerned about the statistical power of both the body imagination experiment and the fMRI experiment. Regarding the number of participants in the imagination study, we would like to clarify that we did not remove 80% of the participants. Actually, a separate sample of participants was recruited in the body imagination experiment. The sample size for the body imagination experiment (100 participants) was indeed smaller than that recruited for the first experiment (528 participants). This is because the first experiment was set for exploratory purposes, and was designed to be over-powered.</p>
<p>Admittedly, the fMRI experiment recruited a small sample (12 participants), which might lead to low power in estimating the affordance effect. In revision, we will acknowledge this issue explicitly. Having said this, note that the null hypothesis of this fMRI study is the lack of two-way interaction between object size and object-action congruency, which was rejected by the significant interaction. That is, the interpretation of the present study did not rely on accepting any null effect. In addition, the fMRI experiment provided convergent evidence for the affordance discontinuity at the neural level. We showed that behind the behavioral discontinuity in action judgement, neural activity was qualitatively different between objects within the affordance boundary and those beyond, which reinforces our statement that objects were discretized along the continuous size axis into two broad categories.</p>
<p>Reviewers also commented that more objects and actions should be included. We agree, and in revision, we will advocate future studies with more objects and more actions to comprehensively portray discontinuity. The present set of objects was designated to cover a relatively large range of object sizes, ranging from 14 cm to 7,618 cm to cover most size categories studied in Konkle and Oliva's (2011) work. In addition, the actions were selected to cover daily interactions between human and objects or environments from single-point movements (e.g., hand, foot) to whole-body movements (e.g., lying, standing) referencing the kinetics human action video dataset (Kay et al., 2017). Thus, this set of selected objects and actions is sufficient to test the discontinuity.</p>
<p>References</p>
<p>Fodor, J. A. (1975). The Language of Thought (Vol. 5). Harvard University Press.</p>
<p>Park, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., &amp; Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>Shapiro, L. (2019). Embodied Cognition. Routledge.</p>
<p>Van Gelder, T. (1998). The dynamical hypothesis in cognitive science. Behavioral and Brain Sciences, 21(5), 615-628.</p>
<p>Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., ... &amp; Wen, J. R. (2023). A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432.</p>
</body>
</sub-article>
</article>