<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97230</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97230</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97230.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Inverted encoding of neural responses to audiovisual stimuli reveals super-additive multisensory enhancement</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0002-4249-462X</contrib-id>
<name>
<surname>Buhmann</surname>
<given-names>Zak</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7378-2803</contrib-id>
<name>
<surname>Robinson</surname>
<given-names>Amanda K.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0929-9216</contrib-id>
<name>
<surname>Mattingley</surname>
<given-names>Jason B.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8416-005X</contrib-id>
<name>
<surname>Rideaux</surname>
<given-names>Reuben</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Queensland Brain Institute, The University of Queensland</institution></aff>
<aff id="a2"><label>2</label><institution>School of Psychology, The University of Queensland</institution></aff>
<aff id="a3"><label>3</label><institution>School of Psychology, University of Sydney</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Corresponding author; email: <email>z.buhmann@uq.net.au</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-05-17">
<day>17</day>
<month>05</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97230</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-03-03">
<day>03</day>
<month>03</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-04">
<day>04</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.15.580589"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Buhmann et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Buhmann et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97230-v1.pdf"/>
<abstract>
<title>Abstract</title><p>A central challenge for the brain is how to combine separate sources of information from different sensory modalities to optimally represent objects and events in the external world, such as combining someone’s speech and lip movements to better understand them in a noisy environment. At the level of individual neurons, audiovisual stimuli often elicit super-additive interactions, where the neural response is greater than the sum of auditory and visual responses. However, investigations using electroencephalography (EEG) to record brain activity have revealed inconsistent interactions, with studies reporting a mix of super- and sub-additive effects. A possible explanation for this inconsistency is that standard univariate analyses obscure multisensory interactions present in EEG responses by overlooking multivariate changes in activity across the scalp. To address this shortcoming, we investigated EEG responses to audiovisual stimuli using inverted encoding, a population tuning approach that uses multivariate information to characterise feature-specific neural activity. Participants (n=41) completed a spatial localisation task for both unisensory stimuli (auditory clicks, visual flashes) and combined audiovisual stimuli (spatiotemporally congruent clicks and flashes). To assess multivariate changes in EEG activity, we used inverted encoding to recover stimulus location information from event-related potentials (ERPs). Participants localised audiovisual stimuli more accurately than unisensory stimuli alone. For univariate ERP analyses we found an additive multisensory interaction. By contrast, multivariate analyses revealed a super-additive interaction ∼180 ms following stimulus onset, such that the location of audiovisual stimuli was decoded more accurately than that predicted by maximum likelihood estimation. Our results suggest that super-additive integration of audiovisual information is reflected within multivariate patterns of activity rather than univariate evoked responses.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>A small number of spelling and referencing errors were fixed.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/8CDRA">https://doi.org/10.17605/OSF.IO/8CDRA</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>We exist in a complex, dynamically changing sensory environment. Vertebrates, including humans, have evolved sensory organs that transduce relevant sources of physical information, such as light and changes in air pressure, into patterns of neural activity that support perception (vision and audition) and adaptive behaviour. Such activity patterns are noisy, and often ambiguous, due to a combination of external (environmental) and internal (transduction) factors. Critically, information from the different sensory modalities can be highly correlated because it is often elicited by a common external source or event. For example, the sight and sound of a hammer hitting a nail produces a single, unified perceptual experience, as does the sight of a person’s lips moving as we hear their voice. To improve the reliability of neural representations, the brain leverages these sensory relationships by combining information in a process referred to as <italic>multisensory integration</italic>. The existence of such processes heighten perception, e.g., by making it easier to understand a person’s speech in a noisy setting by looking at their lip movements (<xref ref-type="bibr" rid="c61">Sumby &amp; Pollack, 1954</xref>).</p>
<p>Multisensory integration of audiovisual cues improves performance across a range of behavioural outcomes, including detection accuracy (<xref ref-type="bibr" rid="c8">Bolognini et al., 2005</xref>; <xref ref-type="bibr" rid="c25">Frassinetti et al., 2002</xref>; <xref ref-type="bibr" rid="c36">Lovelace et al., 2003</xref>), response speed (<xref ref-type="bibr" rid="c2">Arieh &amp; Marks, 2008</xref>; <xref ref-type="bibr" rid="c15">Cappe et al., 2009</xref>; <xref ref-type="bibr" rid="c17">Colonius &amp; Diederich, 2004</xref>; <xref ref-type="bibr" rid="c44">Rach &amp; Diederich, 2006</xref>; <xref ref-type="bibr" rid="c54">Senkowski et al., 2011</xref>), and saccade speed and accuracy (<xref ref-type="bibr" rid="c18">Corniel et al., 2002</xref>; <xref ref-type="bibr" rid="c65">Van Wanrooij et al., 2009</xref>). Successful integration requires the constituent stimuli to occur at approximately the same place and time (<xref ref-type="bibr" rid="c35">Leone &amp; McCourt, 2015</xref>). The degree to which behavioural performance is improved follows the principles of maximum likelihood estimation (MLE), wherein sensory information from each modality is weighted and integrated according to its relative reliability (<xref ref-type="bibr" rid="c1">Alais &amp; Burr, 2004</xref>; <xref ref-type="bibr" rid="c22">Ernst &amp; Banks, 2002</xref>; although other processing schemes have also been identified; <xref ref-type="bibr" rid="c48">Rideaux &amp; Welchman, 2018</xref>). As such, behavioural performance that matches MLE predictions is often seen as a benchmark of successful, optimal integration of relevant unisensory cues.</p>
<p>The ubiquity of behavioural enhancements for audiovisual stimuli suggests there are fundamental neural mechanisms that facilitate improved precision. Recordings from single multisensory (audiovisual) neurons within cat superior colliculus have revealed the principle of inverse effectiveness, whereby the increased response to audiovisual stimuli is larger when the constituent unisensory stimuli are weakly stimulating (<xref ref-type="bibr" rid="c18">Corniel et al., 2002</xref>; <xref ref-type="bibr" rid="c37">Meredith &amp; Stein, 1983</xref>). Depending on the intensity of the integrated stimuli, the neural response can be either <italic>super-additive,</italic> where the multisensory response is greater than the sum of the unisensory responses, <italic>additive,</italic> equal to the sum of responses, or <italic>sub-additive</italic>, where the combined response is less than the sum of the unisensory responses (see <xref ref-type="bibr" rid="c56">Stein &amp; Stanford, 2008</xref>). Inverse effectiveness has also been observed in human behavioural experiments, with low intensity audiovisual stimuli eliciting greater multisensory enhancements in response precision than those of high intensity (<xref ref-type="bibr" rid="c17">Colonius &amp; Diederich, 2004</xref>; <xref ref-type="bibr" rid="c18">Corniel et al., 2002</xref>; <xref ref-type="bibr" rid="c44">Rach &amp; Diederich, 2006</xref>; <xref ref-type="bibr" rid="c45">Rach et al., 2010</xref>).</p>
<p>Neuroimaging methods, such as electroencephalography (EEG) and functional magnetic resonance imaging (fMRI), have been used to investigate neural population-level audiovisual integration in humans. These studies have typically applied an additive criterion to quantify multisensory integration, wherein successful integration is marked by a non-linear enhancement of audiovisual responses relative to unisensory responses (<xref ref-type="bibr" rid="c7">Besle et al., 2004</xref>). The super- or sub-additive nature of this enhancement, however, is often inconsistent. In fMRI, neural super-additivity in blood-oxygen-level dependent (BOLD) responses to audiovisual stimuli has been found in a variety of regions, primarily the superior temporal sulcus (STS; <xref ref-type="bibr" rid="c12">Calvert et al., 2000</xref>; <xref ref-type="bibr" rid="c13">Calvert et al., 2001</xref>; <xref ref-type="bibr" rid="c58">Stevenson et al., 2007</xref>; <xref ref-type="bibr" rid="c60">Stevenson &amp; James, 2009</xref>; <xref ref-type="bibr" rid="c69">Werner &amp; Noppeney, 2010</xref>, <xref ref-type="bibr" rid="c70">2011</xref>). However, other studies have failed to replicate audiovisual super-additivity in the STS (<xref ref-type="bibr" rid="c30">Joassin et al., 2011</xref>; <xref ref-type="bibr" rid="c42">Porada et al., 2021</xref>; <xref ref-type="bibr" rid="c52">Ross et al., 2022</xref>; <xref ref-type="bibr" rid="c66">Venezia et al., 2015</xref>), or have found sub-additive responses (see <xref ref-type="bibr" rid="c53">Scheliga et al., 2023</xref> for review). As such, some have argued that BOLD responses are not sensitive enough to adequately characterise super-additive audiovisual interactions within populations of neurons (<xref ref-type="bibr" rid="c4">Beauchamp, 2005</xref>; <xref ref-type="bibr" rid="c29">James et al., 2012</xref>; <xref ref-type="bibr" rid="c34">Laurienti et al., 2005</xref>). In EEG, meanwhile, the evoked response to an audiovisual stimulus typically conforms to a sub-additive principle (<xref ref-type="bibr" rid="c16">Cappe et al., 2010</xref>; <xref ref-type="bibr" rid="c24">Fort et al., 2002</xref>; <xref ref-type="bibr" rid="c27">Giard &amp; Peronnet, 1999</xref>; <xref ref-type="bibr" rid="c39">Murray et al., 2016</xref>; <xref ref-type="bibr" rid="c43">Puce et al., 2007</xref>; <xref ref-type="bibr" rid="c57">Stekelenburg &amp; Vroomen, 2007</xref>; <xref ref-type="bibr" rid="c63">Teder-Sälejärvi et al., 2002</xref>; <xref ref-type="bibr" rid="c67">Vroomen &amp; Stekelenburg, 2010</xref>). However, other studies have found super-additive enhancements to the amplitude of sensory event-related potentials (ERPs) for audiovisual stimuli (<xref ref-type="bibr" rid="c38">Molholm et al., 2002</xref>; <xref ref-type="bibr" rid="c62">Talsma et al., 2007</xref>), especially when considering the influence of stimulus intensity (<xref ref-type="bibr" rid="c54">Senkowski et al., 2011</xref>).</p>
<p>While behavioural outcomes for multisensory stimuli can be predicted by MLE, and single neuron responses follow the principles of inverse effectiveness and super-additivity, among others (<xref ref-type="bibr" rid="c47">Rideaux et al., 2021</xref>), how audiovisual super-additivity manifests within populations of neurons is comparatively unclear given the mixed findings from relevant fMRI and EEG studies. This uncertainty may be due to biophysical limitations of human neuroimaging techniques, but it may also be related to the analytic approaches used to study these recordings. In particular, information encoded by the brain can be represented as increased activity in some areas, accompanied by decreased activity in others, so simplifying complex neural responses to the average rise and fall of activity may obscure relevant multivariate patterns of activity evoked by a stimulus.</p>
<p>Inverted encoding is a multivariate analytic method that can reveal how sensory information is encoded within the brain by recovering patterns of neural activity associated with different stimulus features. This method has been successfully used in fMRI, EEG, and magnetoencephalography studies to characterise the neural representations of a range of stimulus features, including colour (<xref ref-type="bibr" rid="c10">Brouwer &amp; Heeger, 2009</xref>), spatial location (<xref ref-type="bibr" rid="c5">Bednar &amp; Lalor, 2020</xref>; <xref ref-type="bibr" rid="c50">Robinson et al., 2021</xref>) and orientation (<xref ref-type="bibr" rid="c11">Brouwer &amp; Heeger, 2011</xref>; <xref ref-type="bibr" rid="c28">Harrison et al., 2023</xref>; <xref ref-type="bibr" rid="c32">Kok et al., 2017</xref>). A multivariate approach may capture potential non-linear enhancements associated with audiovisual responses and thus could reveal super-additive interactions that would otherwise be hidden within the brain’s univariate responses. The sensitivity of inverted encoding analyses to multivariate neural patterns may provide insight into how audiovisual information is processed and integrated at the population level.</p>
<p>In the present study, we investigated neural super-additivity in human audiovisual sensory processing using inverted encoding of EEG responses during a task where participants had to spatially localise visual, auditory, and audiovisual stimuli. In a separate behavioural experiment, we monitored response accuracy to characterise behavioural improvements to audiovisual relative to unisensory stimuli. Although there was no evidence for super-additivity in response to audiovisual stimuli within univariate ERPs, we observed a reliable non-linear enhancement of multivariate decoding performance at ∼180 ms following stimulus onset when auditory and visual stimuli were presented concurrently as opposed to alone. These findings suggest that population-level super-additive multisensory neural responses are present within multivariate patterns of activity rather than univariate evoked responses.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Participants</title>
<p>Seventy-one human adults were recruited in return for payment. The study was approved by The University of Queensland Human Research Ethics Committee, and informed consent was obtained in all cases. Participants were first required to complete a behavioural session with above chance performance to qualify for the EEG session (see <bold><italic>Behavioural session</italic></bold> for details). Twenty-nine participants failed to meet this criterion and were excluded from further participation and analyses, along with one participant who failed to complete the EEG session with above chance behavioural accuracy. This left a total of 41 participants (<italic>M</italic> = 27.21 yrs; min = 20 yrs; max = 64 yrs; 24 females; 41 right-handed). Participants reported no neurological or psychiatric disorders, and had normal visual acuity (assessed using a standard Snellen eye chart).</p>
</sec>
<sec id="s2b">
<title>Materials and procedure</title>
<p>The experiment was split into two separate sessions, with participants first completing a behavioural session followed by an EEG session. Each session had three conditions, in which the presented stimuli were either visual, auditory, or combined audio and visual (audiovisual). The order in which conditions were presented was counterbalanced across participants. Before each task, participants were given instructions and completed two rounds of practice for each condition.</p>
</sec>
<sec id="s2c">
<title>Apparatus</title>
<p>The experiment was conducted in a dark, acoustically and electromagnetically shielded room. For the EEG session, stimuli were presented on a 24-inch ViewPixx monitor (VPixx Technologies Inc., Saint-Bruno, QC) with 1920x1080-pixel resolution and a refresh rate of 144 Hz. Viewing distance was maintained at 54 cm using a chinrest. For the behavioural session, stimuli were presented on a 32-inch Cambridge Research Systems Display++ LCD monitor with a 1920x1080-pixel resolution, hardware gamma correction and a refresh rate of 144Hz. Viewing distance was maintained at 59.5 cm using a chinrest. Stimuli were generated in MATLAB v2021b (<xref ref-type="bibr" rid="c64">The MathWorks Inc., 2021</xref>) using the Psychophysics Toolbox (<xref ref-type="bibr" rid="c9">Brainard, 1997</xref>). Auditory stimuli were played through two loudspeakers placed either side of the display (25-75 W, 6 Ω). In both experiments, an EyeLink 1000 infrared eye tracker recorded gaze direction (SR Research Ltd., 2009) at a sampling rate of 1000 Hz.</p>
</sec>
<sec id="s2d">
<title>Stimuli</title>
<p>The EEG and behavioural paradigms used the same stimuli within each condition. Visual stimuli were gaussian blobs (0.2 contrast, 16° diameter) presented for 16 ms on a mid-grey background. Auditory stimuli were 100 ms clicks with a flat 850 Hz tone embedded within a decay envelope (sample rate = 44, 100 Hz; volume = 60 dB<sub>A</sub> SPL, as measured at the ears). Audiovisual stimuli were spatially and temporally matched combinations of the audio and visual stimuli, with no changes to stimuli properties. To manipulate spatial location, target stimuli were presented from multiple horizontal locations along the display, centred on linearly spaced locations from 15° visual angle to the left and right of the display centre (eight locations for behavioural, five for EEG). Auditory stimuli were played through two speakers placed equidistantly either side of the display. The perceived source location of auditory stimuli was manipulated via changes to interaural intensity and timing (<xref ref-type="bibr" rid="c71">Whitworth &amp; Jeffress, 1961</xref>; <xref ref-type="bibr" rid="c72">Wightman &amp; Kistler, 1992</xref>). Specifically, auditory stimuli toward the edges of the display had marginally higher volume and were played slightly earlier from the nearest speaker than the other, whereas those toward the centre more uniform volume and less delay between speakers.</p>
</sec>
<sec id="s2e">
<title>Behavioural Session</title>
<p>Prior to data collection, stimulus intensity and timing were manipulated to make visual and auditory stimuli similarly difficult to spatially localize. We employed a two-interval forced choice design to measure participants’ audiovisual localization sensitivity. Participants were presented with two consecutive stimuli and tasked with indicating, via button press, whether the first or second interval contained the more leftward stimulus. Each trial consisted of a central reference stimulus, and a target stimulus presented at one of eight locations along the horizontal azimuth on the display. The presentation order of the reference and target stimuli was randomised across trials. Stimulus modality was either auditory, visual, or audiovisual. Trials were blocked with short (∼2 min) breaks between conditions (see <bold><xref rid="fig1" ref-type="fig">Figure 1A</xref></bold> for an example trial). Each condition consisted of 384 target presentations across the eight origin locations, leading to 48 presentations at each location.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1</label>
<caption><title>Experimental design of behavioural and EEG sessions.</title>
<p><bold>A)</bold> An example trial for the audiovisual condition in the behavioural session. Each trial consisted of a (centred) reference stimulus and a target stimulus presented at one of eight locations along the horizontal meridian of the display. <bold>B)</bold> An example trial for the audiovisual condition in the EEG session. The top row displays the possible locations of stimuli. In each trial, participants were presented with 20 stimuli that were each spatially localised to one of five possible locations along the horizontal meridian. The task was to determine if there were more stimuli presented to the left or right of fixation.</p></caption>
<graphic xlink:href="580589v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2f">
<title>EEG Session</title>
<p>In this session, the experimental task was changed slightly from the behavioural session to increase the number stimulus presentations required for inverted encoding analyses of EEG data. Participants viewed and/or listened to a sequence of 20 stimuli, each of which were presented at one of five horizontal locations along the display (selected at random). At the end of each sequence, participants were tasked with indicating, via button press, whether more presentations appeared on the right or the left of the display. To minimize eye movements, participants were asked to fixate on a black dot presented 8° above the display centre (see <bold><xref rid="fig1" ref-type="fig">Figure 1B</xref></bold> for an example trial). The task used in the EEG session included the same blocked conditions as in the behavioural session, i.e., visual, auditory, and (congruent) audiovisual stimuli. As the locations of stimuli were selected at random, some sequences had an equal number of presentations on each side of the display, and thus had no correct “left” or “right” response; these trials were not included in the analysis of behavioural performance. Each block consisted of 10 trials, followed by a feedback display indicating the number of trials participants answered correctly. Each condition consisted of 12 blocks, yielding a total of 2400 presentations for each.</p>
</sec>
<sec id="s2g">
<title>EEG data pre-processing</title>
<p>EEG data were recorded using a 64-channel BioSemi system at a sampling rate of 1024 Hz, which was down-sampled to 512 Hz during preprocessing. Signals were recorded with reference to the CMS/DRL electrode loop, with bipolar electrodes placed above and below the eye, at the temples, and at each mastoid to monitor for eye-movements and muscle artifacts. EEG preprocessing was undertaken in MATLAB using custom scripts and the EEGLAB toolbox (<xref ref-type="bibr" rid="c21">Delorme &amp; Makeig, 2004</xref>). Data were high-pass filtered at 0.25 Hz to remove baseline drifts, and re-referenced according to the average of all 64 channels. Analyses were stimulus locked, with ERP responses segmented into 600 ms epochs from 100 ms before stimulus presentation to 500 ms after stimulus presentation. Bad channels, identified by the clean_artifacts function (<xref ref-type="bibr" rid="c33">Kothe &amp; Makeig, 2013</xref>), were reconstructed using spherical interpolation from surrounding channels.</p>
</sec>
<sec id="s2h">
<title>Forward model</title>
<p>To describe the neural representations of sensory stimuli, we used an inverted modelling approach to reconstruct the location of stimuli based upon recorded ERPs (<xref ref-type="bibr" rid="c11">Brouwer &amp; Heeger, 2011</xref>; <xref ref-type="bibr" rid="c28">Harrison et al., 2023</xref>). Analyses were performed separately for visual, auditory, and audiovisual stimuli. We first created an encoding model that characterised the patterns of activity in the EEG electrodes given the five locations of the presented stimuli. The encoding model was then used to obtain the inverse decoding model that described the transformation from electrode activity to stimulus location. We used a 10-fold cross-validation approach where 90% of the data were used to obtain the inverse model on which the remaining 10% of the data were decoded. Cross-validation was repeated 10 times such that all the data were decoded. For the purposes of these analyses, we assume that EEG electrode noise is isotropic across locations and additive with the signal.</p>
<p>Prior to the neural decoding analyses, we established the sensors that contained the most location information by treating time as the decoding dimension and obtaining the inverse models from each electrode, using 10-fold cross-validation. This revealed that location was primarily represented in posterior electrodes for visual and audiovisual stimuli, and in central electrodes for auditory stimuli. Thus, for all subsequent analyses we only included signals from the central-temporal, parietal-occipital, occipital and inion sensors for computing the inverse model.</p>
<p>The encoding model contained five hypothetical channels, with evenly distributed idealised location preferences between −15° to +15° viewing angles to the left and right of the display centre. Each channel consisted of a half-wave rectified sinusoid raised to the fifth power. The channels were arranged such that an idealised tuning curve of each location preference could be expressed as a weighted sum of the five channels. The observed activity for each presentation can be described by the following linear model:
<disp-formula>
<graphic xlink:href="580589v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <bold>B</bold> indicates the EEG data (<italic>m</italic> electrodes x <italic>n</italic> presentations), <bold>W</bold> is a weight matrix (<italic>m</italic> electrodes x 5 channels) that describes the transformation from EEG activity to stimulus location, <bold>C</bold> denotes the hypothesized channel activities (5 channels x <italic>n</italic> presentations), and <bold>E</bold> indicates the residual errors.</p>
<p>To compute the inverse model we estimated the weights that, when applied to the data, would reconstruct the channel activities with the least error. Due to the correlation between neighbouring electrodes, we took noise covariance into account when computing the model to optimize it for EEG data (<xref ref-type="bibr" rid="c28">Harrison et al., 2023</xref>; <xref ref-type="bibr" rid="c32">Kok et al., 2017</xref>; <xref ref-type="bibr" rid="c49">Rideaux et al., 2023</xref>). We then used the inverse model to reconstruct the stimulus location from the recorded ERP responses.</p>
<p>To assess how well the forward model captured location information in the neural signal per modality, two measures of performance were analysed. First, decoding accuracy was calculated as the similarity of the decoded location to the presented location, represented in arbitrary units. To test whether a super-additive interaction was present in the multivariate response, an additive threshold against which to compare the audiovisual response was required. However, it is unclear how the arbitrary units used to represent decoding accuracy translate to a measure of the linear summation of auditory and visual accuracy. As used for the behavioural analyses, MLE provides a framework for calculating the estimated optimal sensitivity of the combination of two sensory signals, according to signal detection theory principles. To compute decoding sensitivity (d’), required to apply MLE, we omitted trials where stimuli appeared in the centre of the display. The decoder’s reconstructions of stimulus location were grouped for stimuli appearing on the left and right side of the display, respectively. The proportion of hits and misses was derived by comparing the decoded side to the presented side, which was then used to calculate d’ for each condition (<xref ref-type="bibr" rid="c55">Stanislaw &amp; Todorov, 1999</xref>). The d’ of the auditory and visual conditions can be used to estimate the predicted ‘optimal’ sensitivity of audiovisual signals as calculated through MLE. We can then compare actual audiovisual sensitivity to this auditory + visual sensitivity and test for super-additivity in the audiovisual condition as evidenced by the presence of a nonlinear combination of auditory and visual stimuli. A similar method was previously employed to investigate depth estimation from motion and binocular disparity cues, decoded from BOLD responses (<xref ref-type="bibr" rid="c3">Ban et al., 2012</xref>).</p>
<p>To represent an additional ‘additive’ multivariate signal with which to compare the decoding sensitivity derived through MLE, we first matched the EEG data between unisensory conditions such that the order of presented stimulus locations was the same for the auditory and visual conditions. The auditory and visual condition data were then concatenated across sensors, and inverted encoding analyses were performed on the resulting ‘additive’ audiovisual dataset. This additive condition was designed to represent neural activity evoked by both the auditory and visual conditions, without any non-linear neural interaction, and served as a baseline for the audiovisual condition.</p>
</sec>
<sec id="s2i">
<title>Statistical analyses</title>
<p>Statistical analyses were performed in MATLAB v2021b. Two metrics of accuracy were calculated to assess behavioural performance. For the behavioural session we calculated participants’ sensitivity separately for each modality condition by fitting psychometric functions to the proportion of rightward responses per stimulus location. In the EEG session participants responded to multiple stimuli rather than individual presentations, so behavioural performance was assessed via d’. We derived d’ in each condition from the average proportion of hits and misses for each participant’s performance in discriminating the side of the display on which more stimuli were presented (<xref ref-type="bibr" rid="c55">Stanislaw &amp; Todorov, 1999</xref>). A one-sample Kolmogorov-Smirnov test for each condition revealed all conditions in both sessions violated assumptions of normality. A non-parametric two-sided Wilcoxon signed-rank test was therefore used to test for significant differences in behavioural accuracy between all conditions.</p>
<p>For the neural data, univariate ERPs were calculated by averaging EEG activity across presentations and channels for each stimulus location from −100 to 500 ms around stimulus onset. A cluster correction was applied to account for spurious differences across time. To test for significant differences between conditions, a paired-samples <italic>t</italic>-test was conducted between each condition at each time point. A one-sample <italic>t-</italic>test was used when comparing decoding accuracy against chance (i.e., zero). Next, the summed value of computed <italic>t</italic> statistics associated with each comparison (separately for positive and negative values) was calculated within contiguous temporal clusters of significant values. We then simulated the null distribution of the maximum summed cluster values using permutation (<italic>n</italic> = 5000) of the location labels, from which we derived the 95% percentile threshold value. Clusters identified in the data with a summed effect-size value less than the threshold were considered spurious and removed.</p>
</sec>
<sec id="s2j">
<title>Data availability</title>
<p>The behavioural and EEG data, and the scripts used for analysis and figure creation, are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/8CDRA">https://doi.org/10.17605/OSF.IO/8CDRA</ext-link>.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Behavioural performance</title>
<p>Participants performed well in discriminating stimulus location across all conditions in both the behavioural and EEG sessions (<bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold>). For the behavioural session, the psychometric curves for responses as a function of stimulus location showed stereotypical relationships for the auditory, visual, and audiovisual conditions (<bold><xref rid="fig2" ref-type="fig">Figure 2A</xref></bold>). A quantification of the behavioural sensitivity (i.e., steepness of the curves) revealed significantly greater sensitivity for the audiovisual stimuli than for the auditory stimuli alone (<italic>Z</italic> = −3.09, <italic>p</italic> = .002), and visual stimuli alone (<italic>Z</italic> = −5.28, <italic>p</italic> = 1.288e-7; <bold><xref rid="fig2" ref-type="fig">Figure 2B</xref></bold>). Sensitivity for auditory stimuli was also significantly greater than sensitivity for visual (<italic>Z</italic> = 2.02, <italic>p</italic> = .044). To test for successful integration of stimuli in the audiovisual condition, we calculated the predicted MLE sensitivity from the unisensory auditory and visual results. We found no evidence for a significant difference between the predicted and actual audiovisual sensitivity (<italic>Z</italic> = - 1.54, <italic>p</italic> = 0.125).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption><title>Behavioural performance is improved for audiovisual stimuli.</title>
<p><bold>A)</bold> Average accuracy of responses across participants in the behavioural session at each stimulus location for each stimulus condition fitted to a psychometric curve. Steeper curves indicate greater sensitivity in identifying stimulus location. <bold>B)</bold> Average sensitivity across participants, estimated from psychometric curves, for each stimulus condition. The red cross indicates estimated performance assuming optimal (MLE) integration of unisensory cues. <bold>C)</bold> Average sensitivity across participants in the EEG session for each stimulus condition. Error bars indicate ±1 SEM.</p></caption>
<graphic xlink:href="580589v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We repeated these analyses for behavioural performance in the EEG session (<bold><xref rid="fig2" ref-type="fig">Figure 2C</xref></bold>). We found a similar pattern of results to those in the behavioural session; sensitivity for audiovisual stimuli was significantly greater than auditory (<italic>Z</italic> = −2.27, <italic>p</italic> = .023) and visual stimuli alone (<italic>Z</italic> = −3.52, <italic>p</italic> = 4.345e-4), but not significantly different from the MLE prediction (<italic>Z =</italic> −1.07, <italic>p</italic> = .285). However, sensitivity for auditory stimuli was not significantly different from sensitivity to visual stimuli (<italic>Z</italic> = 1.12, <italic>p</italic> = .262). Sensitivity was higher overall in the EEG session than the behavioural session, likely due to the increased number of stimuli in the EEG task.</p>
</sec>
<sec id="s3b">
<title>Event-related potentials</title>
<p>We plotted the ERPs for auditory, visual, and audiovisual conditions at each stimulus location from −100ms to 500ms around stimulus presentation (<bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold>). For each stimulus location, cluster-corrected <italic>t</italic>-tests were conducted to assess significant differences in ERP amplitude between the unisensory (auditory and visual) and audiovisual conditions. While auditory ERPs did not significantly differ from the audiovisual, visual ERPs were significantly lower in amplitude than audiovisual ERPs at all stimulus locations (typically from ∼80-130 ms following stimulus presentation).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3</label>
<caption><title>Audiovisual ERPs follow an additive principle.</title>
<p>Average ERP amplitude for each modality condition. Five plots represent the different stimulus locations, as indicated by the grey inset, and the final plot (bottom-right) shows the difference between the summed auditory and visual responses and the audiovisual response. Shaded error bars indicate ±1 SEM. Coloured horizontal bars indicate cluster corrected periods of significant difference between visual and audiovisual ERP amplitudes.</p></caption>
<graphic xlink:href="580589v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To test whether the enhancement in response amplitude to audiovisual stimuli was super-additive, we compared this response with the sum of the response amplitudes for visual and auditory conditions, averaged over stimulus location. We found no significant difference between the additive and audiovisual ERPs (<bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold>, bottom right). This result suggests that, using univariate analyses, the audiovisual response was additive and did not show any evidence for super- or sub-additivity.</p>
</sec>
<sec id="s3c">
<title>Inverted encoding results</title>
<p>We next used inverted encoding to calculate the spatial decoding accuracy for auditory, visual, and audiovisual stimuli (<bold><xref rid="fig4" ref-type="fig">Figure 4A</xref></bold>). For all conditions, we found that spatial location could be reliably decoded from approximately ∼100-150 ms after stimulus onset. Decoding for all conditions was consistent for most of the epoch, indicating that location information within the neural signal was relatively persistent and robust.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4</label>
<caption><title>Spatiotemporal representation of audiovisual location.</title>
<p><bold>A</bold>) Accuracy of locations decoded from neural responses for each stimulus condition. Shaded error bars indicate ±1 SEM. Coloured horizontal bars indicate cluster corrected periods that showed a significant difference from chance (0). <bold>B</bold>) Topographic decoding performance in each condition during critical period (grey inset in (<bold>A</bold>)).</p></caption>
<graphic xlink:href="580589v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To assess the spatial representation of the neural signal containing location-relevant information, we computed decoding accuracy at each electrode from 150-250 ms post-stimulus presentation (<bold><xref rid="fig4" ref-type="fig">Figure 4B</xref></bold>). For auditory stimuli, information was primarily based over bilateral temporal regions, whereas for visual and audiovisual stimuli, the occipital electrodes carried the most information.</p>
</sec>
<sec id="s3d">
<title>Multivariate super-additivity</title>
<p>Although the univariate response did not show evidence for super-additivity, we expected the multivariate measure would be more sensitive to nonlinear audiovisual integration. To test whether a super-additive interaction was present in the multivariate response, we calculated the sensitivity of the decoder in discriminating stimuli presented on the left and right side. The pattern of decoding sensitivity for auditory, visual, and audiovisual stimuli (<bold><xref rid="fig5" ref-type="fig">Figure 5A</xref></bold>) was similar to that in decoding accuracy (<bold><xref rid="fig4" ref-type="fig">Figure 4A</xref></bold>). Notably, audiovisual sensitivity was significantly greater than sensitivity to auditory and visual stimuli alone, particularly ∼180 ms following stimulus onset. To test whether this enhanced sensitivity reflected super-additivity, we compared decoding sensitivity for audiovisual stimuli with two estimates of linearly combined unisensory stimuli: 1) MLE predicted sensitivity based on auditory and visual sensitivity and 2) aggregate responses of auditory and visual stimuli (<bold><xref rid="fig5" ref-type="fig">Figure 5B</xref></bold>). We found that audiovisual sensitivity significantly exceeded both estimates of linear combination (MLE, ∼160-220 ms post-stimulus; aggregate, ∼150-250 ms). These results provide evidence of non-linear audiovisual integration in the multivariate pattern of EEG recordings. Taken together with the ERP results, our findings suggest that super-additive integration of audiovisual information is reflected in multivariate patterns of activity, but not univariate evoked responses.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5</label>
<caption><title>Super-additive multisensory interaction in multivariate patterns of EEG activity.</title>
<p><bold>A</bold>) Decoding sensitivity in each stimulus condition across the epoch. Overall trends closely matched decoding accuracy. <bold>B</bold>) Predicted (MLE and aggregate A+V) and actual audiovisual sensitivity across the epoch. Coloured horizontal bars indicate cluster corrected periods where actual sensitivity significantly exceeded that which was predicted. Shaded error bars indicate ±1 SEM.</p></caption>
<graphic xlink:href="580589v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6</label>
<caption><title>Audiovisual decoding sensitivity is significantly positively correlated to behavioural sensitivity.</title>
<p>The correlations (Spearman’s Rho) between decoding and behavioural sensitivity from the EEG session from 150-250 ms post-stimulus onset for each stimulus condition, with a line of best fit.</p></caption>
<graphic xlink:href="580589v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3e">
<title>Neurobehavioural correlations</title>
<p>To test whether neural decoding was related to behaviour, we calculated rank-order correlations (Spearman’s Rho) between the average decoding sensitivity for each participant from 150-250 ms post-stimulus onset and behavioural performance on the EEG task. We found that decoding sensitivity was significantly positively correlated with behavioural sensitivity for audiovisual stimuli (<italic>r</italic> = .43, <italic>p</italic> = .003), but not for auditory (<italic>r</italic> = -.04, <italic>p</italic> = .608) or visual stimuli (<italic>r</italic> = .14, <italic>p</italic> = .170) alone.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>We tested for super-additivity in multivariate patterns of EEG responses to audiovisual stimuli. Participants judged the location of auditory, visual, and audiovisual stimuli while their brain activity was measured using EEG. As expected, participants’ behavioural responses to audiovisual stimuli were more precise than that for unisensory auditory and visual stimuli. ERP analyses showed that although audiovisual stimuli elicited larger responses than visual stimuli, the overall response followed an additive principle. Critically, our multivariate analyses revealed that decoding sensitivity for audiovisual stimuli exceeded predictions of both MLE and aggregate auditory and visual information, indicating non-linear multisensory enhancement (i.e., super-additivity).</p>
<p>Participants localised audiovisual stimuli more accurately than unisensory in both the behavioural and EEG sessions. This behavioural facilitation in response to audiovisual stimuli is well-established within the literature (<xref ref-type="bibr" rid="c8">Bolognini et al., 2005</xref>; <xref ref-type="bibr" rid="c25">Frassinetti et al., 2002</xref>; <xref ref-type="bibr" rid="c36">Lovelace et al., 2003</xref>; <xref ref-type="bibr" rid="c37">Meredith &amp; Stein, 1983</xref>; <xref ref-type="bibr" rid="c54">Senkowski et al., 2011</xref>). In accordance with theories of optimal cue integration, we found participants’ performance for audiovisual stimuli in both sessions matched that predicted by MLE (<xref ref-type="bibr" rid="c22">Ernst &amp; Banks, 2002</xref>). Matching this ‘optimal’ prediction of performance indicates that the auditory and visual cues were successfully integrated when presented together in the audiovisual condition (<xref ref-type="bibr" rid="c23">Fetsch et al., 2013</xref>).</p>
<p>Our EEG analyses revealed that for most spatial locations, audiovisual stimuli elicited a significantly greater neural response than exclusively visual stimuli approximately 80-120 ms after stimulus onset. Despite numerically larger ERPs to audiovisual than auditory stimuli, this effect failed to reach significance, most likely due to greater inter-trial variability in the auditory ERPs. Critically, however, the audiovisual ERPs consistently matched the sum of visual and auditory ERPs. Sub- or super-additive interaction effects in neural responses to multisensory stimuli are a hallmark of successful integration of unisensory cues in ERPs (<xref ref-type="bibr" rid="c7">Besle et al., 2004</xref>; <xref ref-type="bibr" rid="c59">Stevenson et al., 2014</xref>). An additive ERP in this context cannot imply successful multisensory integration, as the multisensory ‘enhancement’ may be the result of recording from distinct populations of unisensory neurons responding to the two unisensory sensory modalities (<xref ref-type="bibr" rid="c6">Besle et al., 2009</xref>). This invites the question of why we see evidence for integration at the behavioural level, but not in the amplitude of neural responses. One explanation could be that the signals measured by EEG simply do not contain evidence of non-linear integration because the super-additive responses are highly spatiotemporally localized and filtered out by the skull before reaching the EEG sensors. Another possibility, however, is that evidence for non-linear integration is only observable within the changing pattern of ERPs across sensors. Indeed, <xref ref-type="bibr" rid="c39">Murray et al. (2016)</xref> found that multisensory interactions followed from changes in scalp topography rather than net gains to ERP amplitude.</p>
<p>Our decoding results reveal that not only do audiovisual stimuli elicit more distinguishable patterns of activity than visual and auditory stimuli, but this enhancement exceeds that predicted by both optimal integration and the aggregate combination of auditory and visual responses. Critically, the non-linear enhancement of decoding sensitivity for audiovisual stimuli indicates the presence of an interactive effect for the integration of auditory and visual stimuli that was not evident from the univariate analyses. This indicates super-additive enhancement of the neural representation of integrated audiovisual cues, and supports the interpretation that increased behavioural performance for multisensory stimuli is related to a facilitation of the neural response (<xref ref-type="bibr" rid="c23">Fetsch et al., 2013</xref>). This interaction was absent from univariate analyses (<xref ref-type="bibr" rid="c40">Nikbakht et al., 2018</xref>), suggesting that the neural facilitation of audiovisual processing is more nuanced than net increased excitation, and may be associated with a complex pattern of excitatory and inhibitory neural activity, e.g., divisive normalization (<xref ref-type="bibr" rid="c41">Ohshiro et al., 2017</xref>).</p>
<p>The non-linear neural enhancement in decoding sensitivity for audiovisual stimuli occurred ∼180 ms after stimulus onset, which is later than previously reported audiovisual interactions (&lt;150 ms; <xref ref-type="bibr" rid="c16">Cappe et al., 2010</xref>; <xref ref-type="bibr" rid="c24">Fort et al., 2002</xref>; <xref ref-type="bibr" rid="c27">Giard &amp; Peronnet, 1999</xref>; <xref ref-type="bibr" rid="c38">Molholm et al., 2002</xref>; <xref ref-type="bibr" rid="c39">Murray et al., 2016</xref>; <xref ref-type="bibr" rid="c54">Senkowski et al., 2011</xref>; <xref ref-type="bibr" rid="c62">Talsma et al., 2007</xref>; <xref ref-type="bibr" rid="c63">Teder-Sälejärvi et al., 2002</xref>). As stimulus characteristics and task requirements are likely to have a significant influence over the timing of multisensory interaction effects in EEG activity (<xref ref-type="bibr" rid="c14">Calvert &amp; Thesen, 2004</xref>; <xref ref-type="bibr" rid="c19">De Meo et al., 2015</xref>), our use of peripheral spatial locations (where previous studies only used stimuli centrally) may explain the slightly later timing of our audiovisual effect. Indeed, our finding is consistent with previous multivariate studies which found that location information in EEG data, for both visual (<xref ref-type="bibr" rid="c46">Rideaux, 2024</xref>; <xref ref-type="bibr" rid="c50">Robinson et al., 2021</xref>) and auditory (<xref ref-type="bibr" rid="c5">Bednar &amp; Lalor, 2020</xref>) stimuli, is maximal at ∼190 ms following stimulus presentation.</p>
<p>We also found a significant positive correlation between participants’ behavioural judgements in the EEG task and decoding sensitivity for audiovisual stimuli, suggesting that participants who were better at identifying stimulus location may have more distinct patterns of neural activity for audiovisual stimuli. Multisensory stimuli have consistently been found to elicit stronger neural responses than unisensory stimuli (<xref ref-type="bibr" rid="c37">Meredith &amp; Stein, 1983</xref>; <xref ref-type="bibr" rid="c43">Puce et al., 2007</xref>; <xref ref-type="bibr" rid="c54">Senkowski et al., 2011</xref>; <xref ref-type="bibr" rid="c67">Vroomen &amp; Stekelenburg, 2010</xref>), which has been associated with behavioural performance (<xref ref-type="bibr" rid="c26">Frens &amp; Van Opstal, 1998</xref>; <xref ref-type="bibr" rid="c68">Wang et al., 2008</xref>). However, the neuro-behavioural correlation we observed suggests that behavioural facilitation from audiovisual integration is not simply represented by the strength of the neural response, but rather by a reliably distinguishable pattern of activity.</p>
<p>Any experimental design that varies stimulus location needs to consider the potential contribution of eye movements. To reduce eye movements during our study, we had participants fixate on a central dot and removed trials with substantial eye-movements (&gt;3.75°) from the analyses. A re-analysis of the data with a very strict eye-movement criterion (i.e., removing trials with eye movements &gt;1.875°) revealed that the super-additive enhancement in decoding accuracy no longer survived cluster correction, suggesting that our results may be impacted by the consistent motor activity of saccades towards presented stimuli. One piece of evidence against this is that we did not observe significant differences between auditory and audiovisual ERP amplitudes, the latter condition being more likely to drive eye movements. Furthermore, we found that the electrodes with the most location information were in occipital and temporal regions of the scalp, brain areas dedicated to sensory processing, rather than frontal regions, which would be expected if activity was dominated by consistent muscular activity evoked by eye movements. The lack of a super-additive enhancement when using the stricter eye-movement criterion, therefore, is perhaps more likely due to a loss of statistical power.</p>
<p>In summary, here we have shown a non-linear enhancement in the neural representation of audiovisual stimuli relative to unisensory (visual/auditory) stimuli. This enhancement was obscured within univariate ERP analyses focusing exclusively on response amplitude but was revealed through inverted encoding analyses in feature-space, suggesting that super-additive integration of audiovisual information is reflected within multivariate patterns of activity rather than univariate evoked responses. Further research on the multivariate representation of audiovisual integration may shed light on the neural mechanisms that facilitate this non-linear enhancement. In particular, future work may consider the influence of different stimulus features and task requirements on the timing and magnitude of the audiovisual enhancement. How and when auditory and visual information are integrated to enhance multisensory processing remains an open question, with evidence for a complex combination of top-down and bottom-up interactions (<xref ref-type="bibr" rid="c20">Delong &amp; Noppeney, 2021</xref>; <xref ref-type="bibr" rid="c31">Keil &amp; Senkowski, 2018</xref>; <xref ref-type="bibr" rid="c51">Rohe &amp; Noppeney, 2018</xref>). Our study highlights the importance of considering multivariate analyses in multisensory research, and the potential loss of stimulus-relevant neural information when relying solely on univariate responses.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank R. West for data collection, and D. Lloyd for technical assistance. This work was supported by Australian Research Council (ARC) Discovery Early Career Researcher Awards awarded to RR (DE210100790) and AKR (DE200101159). RR was also supported by a National Health and Medical Research Council (Australia) Investigator Grant (2026318).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Alais</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Burr</surname>, <given-names>D</given-names></string-name>. (<year>2004</year>). <article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title>. <source>Current Biology</source>, <volume>14</volume>(<issue>3</issue>), <fpage>257</fpage>–<lpage>262</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Arieh</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Marks</surname>, <given-names>L. E</given-names></string-name>. (<year>2008</year>). <article-title>Cross-modal interaction between vision and hearing: a speed-accuracy analysis</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>70</volume>(<issue>3</issue>), <fpage>412</fpage>–<lpage>421</lpage>. <pub-id pub-id-type="doi">10.3758/pp.70.3.412</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Ban</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Preston</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Meeson</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Welchman</surname>, <given-names>A. E</given-names></string-name>. (<year>2012</year>). <article-title>The integration of motion and disparity cues to depth in dorsal visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>15</volume>(<issue>4</issue>), <fpage>636</fpage>–<lpage>643</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3046</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Beauchamp</surname>, <given-names>M. S</given-names></string-name>. (<year>2005</year>). <article-title>Statistical criteria in fMRI studies of multisensory integration</article-title>. <source>Neuroinformatics</source>, <volume>3</volume>(<issue>2</issue>), <fpage>93</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1385/NI:3:2:093</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bednar</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Lalor</surname>, <given-names>E. C</given-names></string-name>. (<year>2020</year>). <article-title>Where is the cocktail party? Decoding locations of attended and unattended moving sound sources using EEG</article-title>. <source>Neuroimage</source>, <volume>205</volume>, <fpage>116283</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116283</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Besle</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Giard</surname>, <given-names>M. H</given-names></string-name>. (<year>2009</year>). <article-title>Electrophysiological (EEG, sEEG, MEG) evidence for multiple audiovisual interactions in the human auditory cortex</article-title>. <source>Hearing Research</source>, <volume>258</volume>(<issue>1</issue>), <fpage>143</fpage>–<lpage>151</lpage>. <pub-id pub-id-type="doi">10.1016/j.heares.2009.06.016</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Besle</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fort</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Giard</surname>, <given-names>M. H</given-names></string-name>. (<year>2004</year>). <article-title>Interest and validity of the additive model in electrophysiological studies of multisensory interactions</article-title>. <source>Cognitive Processing</source>, <volume>5</volume>(<issue>3</issue>), <fpage>189</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1007/s10339-004-0026-y</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Bolognini</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Frassinetti</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Serino</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Ladavas</surname>, <given-names>E</given-names></string-name>. (<year>2005</year>). <article-title>“Acoustical vision” of below threshold stimuli: interaction among spatially converging audiovisual inputs</article-title>. <source>Experimental Brain Research</source>, <volume>160</volume>(<issue>3</issue>), <fpage>273</fpage>–<lpage>282</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-004-2005-z</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname>, <given-names>D. H</given-names></string-name>. (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>–<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Brouwer</surname>, <given-names>G. J.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name>. (<year>2009</year>). <article-title>Decoding and Reconstructing Color from Responses in Human Visual Cortex</article-title>. <source>The Journal of Neuroscience</source>, <volume>29</volume>(<issue>44</issue>), <fpage>13992</fpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3577-09.2009</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Brouwer</surname>, <given-names>G. J.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>, <given-names>D. J</given-names></string-name>. (<year>2011</year>). <article-title>Cross-orientation suppression in human visual cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>106</volume>(<issue>5</issue>), <fpage>2108</fpage>–<lpage>2119</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00540.2011</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Calvert</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Campbell</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Brammer</surname>, <given-names>M. J</given-names></string-name>. (<year>2000</year>). <article-title>Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex</article-title>. <source>Current Biology</source>, <volume>10</volume>(<issue>11</issue>), <fpage>649</fpage>–<lpage>657</lpage>. <pub-id pub-id-type="doi">10.1016/S0960-9822(00)00513-3</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Calvert</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Hansen</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Iversen</surname>, <given-names>S. D.</given-names></string-name>, &amp; <string-name><surname>Brammer</surname>, <given-names>M. J</given-names></string-name>. (<year>2001</year>). <article-title>Detection of Audio-Visual Integration Sites in Humans by Application of Electrophysiological Criteria to the BOLD Effect</article-title>. <source>Neuroimage</source>, <volume>14</volume>(<issue>2</issue>), <fpage>427</fpage>–<lpage>438</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.2001.0812</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Calvert</surname>, <given-names>G. A.</given-names></string-name>, &amp; <string-name><surname>Thesen</surname>, <given-names>T</given-names></string-name>. (<year>2004</year>). <article-title>Multisensory integration: methodological approaches and emerging principles in the human brain</article-title>. <source>Journal of Physiology-Paris</source>, <volume>98</volume>(<issue>1-3</issue>), <fpage>191</fpage>–<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1016/j.jphysparis.2004.03.018</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Cappe</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Romei</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Murray</surname>, <given-names>M. M</given-names></string-name>. (<year>2009</year>). <article-title>Selective integration of auditory-visual looming cues by humans</article-title>. <source>Neuropsychologia</source>, <volume>47</volume>(<issue>4</issue>), <fpage>1045</fpage>–<lpage>1052</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2008.11.003</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Cappe</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Romei</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Murray</surname>, <given-names>M. M</given-names></string-name>. (<year>2010</year>). <article-title>Auditory–Visual Multisensory Interactions in Humans: Timing, Topography, Directionality, and Sources</article-title>. <source>The Journal of Neuroscience</source>, <volume>30</volume>(<issue>38</issue>), <fpage>12572</fpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1099-10.2010</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Colonius</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Diederich</surname>, <given-names>A</given-names></string-name>. (<year>2004</year>). <article-title>Multisensory Interaction in Saccadic Reaction Time: A Time-Window-of-Integration Model</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>16</volume>(<issue>6</issue>), <fpage>1000</fpage>–<lpage>1009</lpage>. <pub-id pub-id-type="doi">10.1162/0898929041502733</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Corniel</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>van Wanrooij</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Munoz</surname>, <given-names>D. P.</given-names></string-name>, &amp; <string-name><surname>van Opstal</surname>, <given-names>J.</given-names></string-name> (<year>2002</year>). <article-title>Auditory-Visual Interactions Subserving Goal-Directed Saccades in a Complex Scene</article-title>. <source>Journal of Neurophysiology</source>, <volume>88</volume>(<issue>1</issue>), <fpage>438</fpage>–<lpage>454</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00699.2001</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>De Meo</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Clarke</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Matusz</surname>, <given-names>P. J.</given-names></string-name> (<year>2015</year>). <article-title>Top-down control and early multisensory processes: chicken vs. egg</article-title>. <source>Frontiers in integrative neuroscience</source>, <volume>9</volume>(<fpage>17</fpage>). <pub-id pub-id-type="doi">10.3389/fnint.2015.00017</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Delong</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U</given-names></string-name>. (<year>2021</year>). <article-title>Semantic and spatial congruency mould audiovisual integration depending on perceptual awareness</article-title>. <source>Scientific Reports</source>, <volume>11</volume>(<issue>1</issue>), <fpage>10832</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-90183-w</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Delorme</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Makeig</surname>, <given-names>S</given-names></string-name>. (<year>2004</year>). <article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>134</volume>(<issue>1</issue>), <fpage>9</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.00</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Ernst</surname>, <given-names>M. O.</given-names></string-name>, &amp; <string-name><surname>Banks</surname>, <given-names>M. S</given-names></string-name>. (<year>2002</year>). <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>, <volume>415</volume>(<issue>6870</issue>), <fpage>429</fpage>–<lpage>433</lpage>. <pub-id pub-id-type="doi">10.1038/415429a</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Fetsch</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>DeAngelis</surname>, <given-names>G. C.</given-names></string-name>, &amp; <string-name><surname>Angelaki</surname>, <given-names>D. E</given-names></string-name>. (<year>2013</year>). <article-title>Bridging the gap between theories of sensory cue integration and the physiology of multisensory neurons</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>14</volume>(<issue>6</issue>), <fpage>429</fpage>–<lpage>442</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3503</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Fort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Delpuech</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pernier</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Giard</surname>, <given-names>M.-H</given-names></string-name>. (<year>2002</year>). <article-title>Dynamics of Cortico-subcortical Cross-modal Operations Involved in Audio-visual Object Detection in Humans</article-title>. <source>Cerebral Cortex</source>, <volume>12</volume>(<issue>10</issue>), <fpage>1031</fpage>–<lpage>1039</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/12.10.1031</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Frassinetti</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bolognini</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Ladavas</surname>, <given-names>E</given-names></string-name>. (<year>2002</year>). <article-title>Enhancement of visual perception by crossmodal visuo-auditory interaction</article-title>. <source>Experimental Brain Research</source>, <volume>147</volume>(<issue>3</issue>), <fpage>332</fpage>–<lpage>343</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-002-1262-y</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Frens</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Van Opstal</surname>, <given-names>A. J.</given-names></string-name> (<year>1998</year>). <article-title>Visual-auditory interactions modulate saccade-related activity in monkey superior colliculus</article-title>. <source>Brain Research Bulletin</source>, <volume>46</volume>(<issue>3</issue>), <fpage>211</fpage>–<lpage>224</lpage>. <pub-id pub-id-type="doi">10.1016/S0361-9230(98)00007-0</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Giard</surname>, <given-names>M. H.</given-names></string-name>, &amp; <string-name><surname>Peronnet</surname>, <given-names>F</given-names></string-name>. (<year>1999</year>). <article-title>Auditory-Visual Integration during Multimodal Object Recognition in Humans: A Behavioral and Electrophysiological Study</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>11</volume>(<issue>5</issue>), <fpage>473</fpage>–<lpage>490</lpage>. <pub-id pub-id-type="doi">10.1162/089892999563544</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Harrison</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Bays</surname>, <given-names>P. M.</given-names></string-name>, &amp; <string-name><surname>Rideaux</surname>, <given-names>R</given-names></string-name>. (<year>2023</year>). <article-title>Neural tuning instantiates prior expectations in the human visual system</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>), <fpage>5320</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-023-41027-w</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>James</surname>, <given-names>T. W.</given-names></string-name>, <string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Kim</surname>, <given-names>S.</given-names></string-name> (<year>2012</year>). <article-title>Inverse Effectiveness and BOLD fMRI</article-title>. In <string-name><given-names>B. E.</given-names> <surname>Stein</surname></string-name> (Ed.), <source>The New Handbook of Multisensory Processing</source> (pp. <fpage>207</fpage>). <pub-id pub-id-type="doi">10.7551/mitpress/8466.003.0020</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Joassin</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Maurage</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Campanella</surname>, <given-names>S</given-names></string-name>. (<year>2011</year>). <article-title>The neural network sustaining the crossmodal processing of human gender from faces and voices: An fMRI study</article-title>. <source>Neuroimage</source>, <volume>54</volume>(<issue>2</issue>), <fpage>1654</fpage>–<lpage>1661</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.08.073</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Keil</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Senkowski</surname>, <given-names>D</given-names></string-name>. (<year>2018</year>). <article-title>Neural Oscillations Orchestrate Multisensory Processing</article-title>. <source>The Neuroscientist</source>, <volume>24</volume>(<issue>6</issue>), <fpage>609</fpage>–<lpage>626</lpage>. <pub-id pub-id-type="doi">10.1177/1073858418755352</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2017</year>). <article-title>Prior expectations induce prestimulus sensory templates</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>39</issue>), <fpage>10473</fpage>–<lpage>10478</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1705652114</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Kothe</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Makeig</surname>, <given-names>S</given-names></string-name>. (<year>2013</year>). <article-title>BCILAB: a platform for brain-computer interface development</article-title>. <source>Journal of Neural Engineering</source>, <volume>10</volume>(<fpage>5</fpage>). <pub-id pub-id-type="doi">10.1088/1741-2560/10/5/056014</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Laurienti</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Perrault</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Stanford</surname>, <given-names>T. R.</given-names></string-name>, <string-name><surname>Wallace</surname>, <given-names>M. T.</given-names></string-name>, &amp; <string-name><surname>Stein</surname>, <given-names>B. E</given-names></string-name>. (<year>2005</year>). <article-title>On the use of superadditivity as a metric for characterizing multisensory integration in functional neuroimaging studies</article-title>. <source>Experimental Brain Research</source>, <volume>166</volume>(<issue>3</issue>), <fpage>289</fpage>–<lpage>297</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-005-2370-2</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Leone</surname>, <given-names>L. M.</given-names></string-name>, &amp; <string-name><surname>McCourt</surname>, <given-names>M. E</given-names></string-name>. (<year>2015</year>). <article-title>Dissociation of perception and action in audiovisual multisensory integration</article-title>. <source>European Journal of Neuroscience</source>, <volume>42</volume>(<issue>11</issue>), <fpage>2915</fpage>–<lpage>2922</lpage>. <pub-id pub-id-type="doi">10.1111/ejn.13087</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Lovelace</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Stein</surname>, <given-names>B. E.</given-names></string-name>, &amp; <string-name><surname>Wallace</surname>, <given-names>M. T</given-names></string-name>. (<year>2003</year>). <article-title>An irrelevant light enhances auditory detection in humans: a psychophysical analysis of multisensory integration in stimulus detection</article-title>. <source>Cognitive Brain Research</source>, <volume>17</volume>(<issue>2</issue>), <fpage>447</fpage>–<lpage>453</lpage>. <pub-id pub-id-type="doi">10.1016/s0926-6410(03)00160-5</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Meredith</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Stein</surname>, <given-names>B. E</given-names></string-name>. (<year>1983</year>). <article-title>Interactions Among Converging Sensory Inputs in the Superior Colliculus</article-title>. <source>Science</source>, <volume>221</volume>(<issue>4608</issue>), <fpage>389</fpage>–<lpage>391</lpage>. <pub-id pub-id-type="doi">10.1126/science.6867718</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Molholm</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ritter</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Javitt</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name>, &amp; <string-name><surname>Foxe</surname>, <given-names>J. J</given-names></string-name>. (<year>2002</year>). <article-title>Multisensory auditory–visual interactions during early sensory processing in humans: a high-density electrical mapping study</article-title>. <source>Cognitive Brain Research</source>, <volume>14</volume>(<issue>1</issue>), <fpage>115</fpage>–<lpage>128</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(02)00066-6</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Murray</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Thelen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Romei</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Martuzzi</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Matusz</surname>, <given-names>P. J</given-names></string-name>. (<year>2016</year>). <article-title>The multisensory function of the human primary visual cortex</article-title>. <source>Neuropsychologia</source>, <volume>83</volume>, <fpage>161</fpage>–<lpage>169</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.08.011</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Nikbakht</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Tafreshiha</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Diamond</surname>, <given-names>M. E</given-names></string-name>. (<year>2018</year>). <article-title>Supralinear and Supramodal Integration of Visual and Tactile Signals in Rats: Psychophysics and Neuronal Mechanisms</article-title>. <source>Neuron</source>, <volume>97</volume>(<issue>3</issue>), <fpage>626</fpage>–<lpage>639</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.003</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Ohshiro</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Angelaki</surname>, <given-names>D. E.</given-names></string-name>, &amp; <string-name><surname>DeAngelis</surname>, <given-names>G. C</given-names></string-name>. (<year>2017</year>). <article-title>A Neural Signature of Divisive Normalization at the Level of Multisensory Integration in Primate Cortex</article-title>. <source>Neuron</source>, <volume>95</volume>(<issue>2</issue>), <fpage>399</fpage>–<lpage>411</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.043</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Porada</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Regenbogen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Freiherr</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Seubert</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Lundström</surname>, <given-names>J. N</given-names></string-name>. (<year>2021</year>). <article-title>Trimodal processing of complex stimuli in inferior parietal cortex is modality-independent</article-title>. <source>Cortex</source>, <volume>139</volume>, <fpage>198</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2021.03.008</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Puce</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Epling</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Thompson</surname>, <given-names>J. C.</given-names></string-name>, &amp; <string-name><surname>Carrick</surname>, <given-names>O. K</given-names></string-name>. (<year>2007</year>). <article-title>Neural responses elicited to face motion and vocalization pairings</article-title>. <source>Neuropsychologia</source>, <volume>45</volume>(<issue>1</issue>), <fpage>93</fpage>–<lpage>106</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.04.017</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Rach</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Diederich</surname>, <given-names>A</given-names></string-name>. (<year>2006</year>). <article-title>Visual-tactile integration: does stimulus duration influence the relative amount of response enhancement?</article-title> <source>Experimental Brain Research</source>, <volume>173</volume>(<issue>3</issue>), <fpage>514</fpage>–<lpage>520</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-006-0452-4</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Rach</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Diederich</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Colonius</surname>, <given-names>H</given-names></string-name>. (<year>2010</year>). <article-title>On quantifying multisensory interaction effects in reaction time and detection rate</article-title>. <source>Psychological Research</source>, <volume>75</volume>(<issue>2</issue>), <fpage>77</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1007/s00426-010-0289-0</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Rideaux</surname>, <given-names>R</given-names></string-name>. (<year>2024</year>). <article-title>Task-related modulation of event-related potentials does not reflect changes to sensory representations</article-title>. <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2024.01.20.576485</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Storrs</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Maiello</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Welchman</surname>, <given-names>A. E</given-names></string-name>. (<year>2021</year>). <article-title>How multisensory neurons solve causal inference</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>32</issue>), <fpage>e2106235118</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2106235118</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Welchman</surname>, <given-names>A. E</given-names></string-name>. (<year>2018</year>). <article-title>Proscription supports robust perceptual integration by suppression in human visual cortex</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>), <fpage>1502</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-03400-y</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Rideaux</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>West</surname>, <given-names>R. K.</given-names></string-name>, <string-name><surname>Rangelov</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Mattingley</surname>, <given-names>J. B</given-names></string-name>. (<year>2023</year>). <article-title>Distinct early and late neural mechanisms regulate feature-specific sensory adaptation in the human visual system</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>120</volume>(<issue>6</issue>), <fpage>e2216192120</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2216192120</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Robinson</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Shatek</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Gerboni</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Holcombe</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name>. (<year>2021</year>). <article-title>Overlapping neural representations for the position of visible and imagined objects</article-title>. <source>Neurons, behavior, data analysis, and theory</source>, <volume>4</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.51628/001c.19129</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Rohe</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U</given-names></string-name>. (<year>2018</year>). <article-title>Reliability-Weighted Integration of Audiovisual Signals Can Be Modulated by Top-down Attention</article-title>. <source>eneuro</source>, <volume>5</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1523/ENEURO.0315-17.2018</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Ross</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Molholm</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Butler</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Bene</surname>, <given-names>V. A. D.</given-names></string-name>, &amp; <string-name><surname>Foxe</surname>, <given-names>J. J</given-names></string-name>. (<year>2022</year>). <article-title>Neural correlates of multisensory enhancement in audiovisual narrative speech perception: A fMRI investigation</article-title>. <source>Neuroimage</source>, <volume>263</volume>, <fpage>119598</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119598</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Scheliga</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kellermann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lampert</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rolke</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Spehr</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Habel</surname>, <given-names>U</given-names></string-name>. (<year>2023</year>). <article-title>Neural correlates of multisensory integration in the human brain: an ALE meta-analysis</article-title>. <source>Reviews in the Neurosciences</source>, <volume>34</volume>(<issue>2</issue>), <fpage>223</fpage>–<lpage>245</lpage>. <pub-id pub-id-type="doi">10.1515/revneuro-2022-0065</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Senkowski</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Saint-Amour</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hofle</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Foxe</surname>, <given-names>J. J</given-names></string-name>. (<year>2011</year>). <article-title>Multisensory interactions in early evoked brain activity follow the principle of inverse effectiveness</article-title>. <source>Neuroimage</source>, <volume>56</volume>(<issue>4</issue>), <fpage>2200</fpage>–<lpage>2208</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.075</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Stanislaw</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Todorov</surname>, <given-names>N</given-names></string-name>. (<year>1999</year>). <article-title>Calculation of signal detection theory measures</article-title>. <source>Behavior Research Methods, Instruments, &amp; Computers</source>, <volume>31</volume>(<issue>1</issue>), <fpage>137</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.3758/BF03207704</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Stein</surname>, <given-names>B. E.</given-names></string-name>, &amp; <string-name><surname>Stanford</surname>, <given-names>T. R</given-names></string-name>. (<year>2008</year>). <article-title>Multisensory integration: current issues from the perspective of the single neuron</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>9</volume>(<issue>4</issue>), <fpage>255</fpage>–<lpage>266</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2331</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Stekelenburg</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Vroomen</surname>, <given-names>J</given-names></string-name>. (<year>2007</year>). <article-title>Neural Correlates of Multisensory Integration of Ecologically Valid Audiovisual Events</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>19</volume>(<issue>12</issue>), <fpage>1964</fpage>–<lpage>1973</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2007.19.12.1964</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Geoghegan</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>James</surname>, <given-names>T. W</given-names></string-name>. (<year>2007</year>). <article-title>Superadditive BOLD activation in superior temporal sulcus with threshold non-speech objects</article-title>. <source>Experimental Brain Research</source>, <volume>179</volume>(<issue>1</issue>), <fpage>85</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-006-0770-6</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Ghose</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Fister</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Sarko</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Altieri</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Nidiffer</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Kurela</surname>, <given-names>L. R.</given-names></string-name>, <string-name><surname>Siemann</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>James</surname>, <given-names>T. W.</given-names></string-name>, &amp; <string-name><surname>Wallace</surname>, <given-names>M. T</given-names></string-name>. (<year>2014</year>). <article-title>Identifying and quantifying multisensory integration: a tutorial review</article-title>. <source>Brain Topography</source>, <volume>27</volume>(<issue>6</issue>), <fpage>707</fpage>–<lpage>730</lpage>. <pub-id pub-id-type="doi">10.1007/s10548-014-0365-7</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Stevenson</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>James</surname>, <given-names>T. W</given-names></string-name>. (<year>2009</year>). <article-title>Audiovisual integration in human superior temporal sulcus: Inverse effectiveness and the neural processing of speech and object recognition</article-title>. <source>Neuroimage</source>, <volume>44</volume>(<issue>3</issue>), <fpage>1210</fpage>–<lpage>1223</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.034</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Sumby</surname>, <given-names>W. H.</given-names></string-name>, &amp; <string-name><surname>Pollack</surname>, <given-names>I</given-names></string-name>. (<year>1954</year>). <article-title>Visual Contribution to Speech Intelligibility in Noise</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>26</volume>(<issue>2</issue>), <fpage>212</fpage>–<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1121/1.1907309</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Talsma</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Doty</surname>, <given-names>T. J.</given-names></string-name>, &amp; <string-name><surname>Woldorff</surname>, <given-names>M. G</given-names></string-name>. (<year>2007</year>). <article-title>Selective Attention and Audiovisual Integration: Is Attending to Both Modalities a Prerequisite for Early Integration?</article-title> <source>Cerebral Cortex</source>, <volume>17</volume>(<issue>3</issue>), <fpage>679</fpage>–<lpage>690</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhk016</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Teder-Sälejärvi</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>McDonald</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Di Russo</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name> (<year>2002</year>). <article-title>An analysis of audio-visual crossmodal integration by means of event-related potential (ERP) recordings</article-title>. <source>Cognitive Brain Research</source>, <volume>14</volume>(<issue>1</issue>), <fpage>106</fpage>–<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(02)00065-4</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="book"><collab>The MathWorks Inc.</collab> (<year>2021</year>). <chapter-title>MATLAB version: 9.11.0 (R2021b), Natick, Massachusetts</chapter-title>. <publisher-name>The MathWorks Inc</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Van Wanrooij</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Bell</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Munoz</surname>, <given-names>D. P.</given-names></string-name>, &amp; <string-name><surname>Van Opstal</surname>, <given-names>A. J.</given-names></string-name> (<year>2009</year>). <article-title>The effect of spatial-temporal audiovisual disparities on saccades in a complex scene</article-title>. <source>Experimental Brain Research</source>, <volume>198</volume>(<issue>2-3</issue>), <fpage>425</fpage>–<lpage>437</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-009-1815-4</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="book"><string-name><surname>Venezia</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Matchin</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Hickok</surname>, <given-names>G.</given-names></string-name> (<year>2015</year>). <chapter-title>Multisensory Integration and Audiovisual Speech Perception</chapter-title>. In <string-name><given-names>A. W.</given-names> <surname>Toga</surname></string-name> (Ed.), <source>Brain Mapping</source> (pp. <fpage>565</fpage>-<lpage>572</lpage>). <publisher-name>Academic Press</publisher-name>. <pub-id pub-id-type="doi">10.1016/B978-0-12-397025-1.00047-6</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>Vroomen</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Stekelenburg</surname>, <given-names>J. J</given-names></string-name>. (<year>2010</year>). <article-title>Visual Anticipatory Information Modulates Multisensory Interactions of Artificial Audiovisual Stimuli</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>22</volume>(<issue>7</issue>), <fpage>1583</fpage>–<lpage>1596</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2009.21308</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Celebrini</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Trotter</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Barone</surname>, <given-names>P</given-names></string-name>. (<year>2008</year>). <article-title>Visuo-auditory interactions in the primary visual cortex of the behaving monkey: Electrophysiological evidence</article-title>. <source>BMC neuroscience</source>, <volume>9</volume>(<issue>1</issue>), <fpage>79</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2202-9-79</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Werner</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U</given-names></string-name>. (<year>2010</year>). <article-title>Superadditive Responses in Superior Temporal Sulcus Predict Audiovisual Benefits in Object Categorization</article-title>. <source>Cerebral Cortex</source>, <volume>20</volume>(<issue>8</issue>), <fpage>1829</fpage>–<lpage>1842</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhp248</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><string-name><surname>Werner</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Noppeney</surname>, <given-names>U</given-names></string-name>. (<year>2011</year>). <article-title>The Contributions of Transient and Sustained Response Codes to Audiovisual Integration</article-title>. <source>Cerebral Cortex</source>, <volume>21</volume>(<issue>4</issue>), <fpage>920</fpage>–<lpage>931</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhq161</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><string-name><surname>Whitworth</surname>, <given-names>R. H.</given-names></string-name>, &amp; <string-name><surname>Jeffress</surname>, <given-names>L. A</given-names></string-name>. (<year>1961</year>). <article-title>Time vs Intensity in the Localization of Tones</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>33</volume>(<issue>7</issue>), <fpage>925</fpage>–<lpage>929</lpage>. <pub-id pub-id-type="doi">10.1121/1.1908849</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><string-name><surname>Wightman</surname>, <given-names>F. L.</given-names></string-name>, &amp; <string-name><surname>Kistler</surname>, <given-names>D. J</given-names></string-name>. (<year>1992</year>). <article-title>The dominant role of low-frequency interaural time differences in sound localization</article-title>. <source>The Journal of the Acoustical Society of America</source>, <volume>91</volume>(<issue>3</issue>), <fpage>1648</fpage>–<lpage>1661</lpage>. <pub-id pub-id-type="doi">10.1121/1.402445</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97230.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>Despite the well-known facilitatory effect that integration across the senses has on behavioural measures, standard neuroimaging approaches have not yet produced reliable and precise neural correlates. In this paper, Buhman et al. harness the decoding of EEG responses, beyond univariate approaches, to capture these correlates in a robust, clear fashion. If confirmed, this approach could be <bold>important</bold> for estimating multisensory integration in humans across a wide range of different domains. However, the strength of evidence to support these claims is still <bold>incomplete</bold> because of the potentially confounding factor of eye movements, which the authors themselves identify in their data, and because of the discrepancies between the behavioural and EEG data.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97230.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study presents a novel application of the inverted encoding (i.e., decoding) approach to detect the correlates of crossmodal integration in the human EEG (electrophysiological) signal. The method is successfully applied to data from a group of 41 participants, performing a spatial localization task on auditory, visual, and audio-visual events. The analyses clearly show a behavioural superiority for audio-visual localization. Like previous studies, the results when using traditional univariate ERP analyses were inconclusive, showing once more the need for alternative, more sophisticated approaches. Instead, the principal approach of this study, harnessing the multivariate nature of the signal, captured clear signs of super-additive responses, considered by many as the hallmark of multisensory integration. Unfortunately, the manuscript lacks many important details in the descriptions of the methodology and analytical pipeline. Although some of these details can eventually be retrieved from the scripts that accompany this paper, the main text should be self-contained and sufficient to gain a clear understanding of what was done. (A list of some of these is included in the comments to the authors). Nevertheless, I believe the main weakness of this work is that the positive results obtained and reported in the results section are conditioned upon eye movements. When artifacts due to eye movements are removed, then the outcomes are no longer significant.</p>
<p>Therefore, whether the authors finally achieved the aims and showed that this method of analysis is truly a reliable way to assess crossmodal integration, does not stand on firm ground. The worst-case scenario is that the results are entirely accounted for by patterns of eye movements in the different conditions. In the best-case scenario, the method might truly work, but further experiments (and/or analyses) would be required to confirm the claims in a conclusive fashion.</p>
<p>If finally successful, this approach could bring important advances in the many fields where multisensory integration has been shown to play a role, by providing a way to bring much-needed coherence across levels of analysis, from behaviour to single-cell electrophysiology. To achieve this, one would have to make sure that the pattern of super-additive effects, the standard self-imposed by the authors as a proxy for multisensory integration, shows up reliably regardless of eye movement or artifact corrections. One first step toward this goal would be, perhaps, to facilitate the understanding of results in context by reporting both the uncorrected and corrected analyses in the main results section. Second, one could try to support the argument given in the discussion, pointing out the origin of the super-additive effects in posterior electrode sites, by also modelling frontal electrode clusters and showing they aren't informative as to the effect of interest.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97230.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript seeks to reconcile observations in multisensory perception - from behavior and neural responses. It is intuitively obvious that perceiving a stimulus via two senses results in better performance than one alone. In fact, it is not uncommon to observe that for a perceptual task, the percentage of correct responses seen with two senses is higher than the sum of the percentage correct obtained with each modality individually. i.e. the gains are &quot;superadditive&quot;. The gains of adding a second sense are typically larger when the performance with the first sense is relatively poor - this effect is often called the principle of inverse effectiveness. More generally, what this tells us is that performance in a multisensory perceptual task is a non-linear sum of performance for each sensory modality alone.</p>
<p>Despite this abundant evidence of behavioral non-linearity in multisensory integration, evoked responses (EEG) to such sensory stimuli often show little evidence of it - and this is the problem this manuscript tackles. The key assertion made is that univariate analysis of the EEG signal is likely to average out the non-linear effects of integration. This is a reasonable assertion, and their analysis does indeed provide evidence that a multivariate approach can reveal non-linear interactions in the evoked responses.</p>
<p>Strengths:</p>
<p>It is of great value to understand how the process of multisensory integration occurs, and despite a wealth of observations of the benefits of perceiving the world with multiple senses, we still lack a reasonable understanding of how the brain integrates information. For example - what underlies the large individual differences in the benefits of two senses over one? One way to tackle this is via brain imaging, but this is problematic if important features of the processing - such as non-linear interactions are obscured by the lack of specificity of the measurements. The approach they take to the analysis of the EEG data allows the authors to look in more detail at the variation in activity across EEG electrodes, which averaging across electrodes cannot.</p>
<p>This version of the manuscript is well-written and for the most part clear. It shows a good understanding of the non-linear effects described above (where many studies show a poor understanding of &quot;superadditivity&quot; of perceptual performance) and the report of non-linear summation of neural responses is convincing.</p>
<p>A particular strength of the paper is their use of a statistical model of multisensory integration as their &quot;null&quot; model of neural responses, and the &quot;inverted-encoder&quot; which infers an internal representation of the stimulus which can explain the EEG responses. This encoder generates a prediction of decoding performance, which can be used to generate predictions of multisensory decoding from unisensory decoding, or from a sum of the unisensory internal representations.</p>
<p>In behavioural performance, it is frequently observed that the performance increase from two senses is close to what is expected from the optimal integration of information across the senses, in a statistical sense. It can be plausibly explained by assuming that people are able to weigh sensory inputs according to their reliability - and somewhat optimally. Critically the apparent &quot;superadditive&quot; effect on performance described above does not require any non-linearity in the sum of information across the senses but can arise from correctly weighting the information according to reliability.</p>
<p>The authors apply a similar model to predict the neural responses expected to audiovisual stimuli from the neural responses to audio and visual stimuli alone, assuming optimal statistical integration of information. The neural responses to audiovisual stimuli exceed the predictions of this model and this is the main evidence supporting their conclusion, and it is convincing.</p>
<p>Weaknesses:</p>
<p>The main weakness of the manuscript is that their behavioural data show no evidence of performance that exceeds the predictions of these statistical models. In fact, the models predict multisensory performance from unisensory performance pretty well. So this manuscript presents the opposite problem to that which motivated the study - neural interactions across the senses which appear to be more non-linear than perception. This makes it hard to interpret their results, as surely if these nonlinear neural interactions underlie the behaviour, then we should be able to see evidence of it in the behaviour? I cannot offer an easy explanation for this.</p>
<p>Overall, therefore, I applaud the motivation and the sophistication of the analysis method and think it shows great promise for tackling these problems, but the manuscript unfortunately brushes over an important problem specific to the results. It appeals to the higher-level reasoning - that non-linearity is a behavioural hallmark of integration and therefore we should see it in neural responses. Yet it ignores the fact that the behaviour observed here does not exceed the predictions of the &quot;null&quot; model applied to the neural response.</p>
<p>Part of the problem, I think, is that the authors never explain the difference between superadditivity of perceptual performance (proportion correct) and superadditivity of the underlying processing, which is implied by the EEG results but not their behavior. This is of course a difficult matter to describe succinctly or clearly (I somehow doubt I have). It is however worth addressing. The literature is full of confusing claims of superadditivity. I believe these authors understand this distinction and have an opportunity to represent it clearly for the benefit of all.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97230.1.sa3</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Buhmann</surname>
<given-names>Zak</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0002-4249-462X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Robinson</surname>
<given-names>Amanda K.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7378-2803</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mattingley</surname>
<given-names>Jason B.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0929-9216</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Rideaux</surname>
<given-names>Reuben</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8416-005X</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p><bold>Response to Reviewer #1 (Public Review):</bold></p>
<p>We thank the reviewer for their constructive criticism of our study, their proposed solutions, and for highlighting areas of the methodology and analytical pipeline where explanations were unclear or unsatisfactory. We will take the reviewer’s feedback into account to improve the clarity and readability of the revised manuscript. We acknowledge the importance of ruling out eye movements as a potential confound. We address these concerns briefly below, but a more detailed explanation (and a full breakdown of the relevant analyses, including the corrected and uncorrected results) will be provided in the revised manuscript.</p>
<p>First, the source of EEG activity recorded from the frontal electrodes is often unclear. Without an external reference, it is challenging to resolve the degree to which frontal EEG activity represents neural or muscular responses1. Thus, as a preventative measure against the potential contribution of eye movement activity, for all our EEG analyses, we only included activity from occipital, temporal, and parietal electrodes (the selected electrodes can be seen in the final inset of Figure 3).</p>
<p>Second, as suggested by the reviewer, we re-ran our analyses using the activity measured from the frontal electrodes alone. If the source of the nonlinear decoding accuracy in the AV condition was muscular activity produced by eye movements, we would expect to observe better decoding accuracy from sensors closer to the source. Instead, we found that decoding accuracy from the frontal electrodes (peak d' = 0.08) was less than half that of decoding accuracy from the more posterior electrodes (peak d' = 0.18). These results suggest that the source of neural activity containing information about stimulus position was located over occipito-parietal areas, consistent with our topographical analyses (inset of Figure 4).</p>
<p>Third, we compared the average eye movements between the three main sensory conditions (auditory, visual, and audiovisual). In the visual condition, there was little difference in eye movements corresponding to the five stimulus locations, likely because the visual stimuli were designed to be spatially diffuse. For the auditory and audiovisual conditions, there was more distinction between eye movements corresponding to the stimulus locations. However, these appeared to be the same between auditory and audiovisual conditions. If consistent saccades to audiovisual stimuli had been responsible for the nonlinear decoding we observed, we would expect to find a higher positive correlation between horizontal eye position and stimulus location in the audiovisual condition than in the auditory or visual conditions. Instead, we found no difference in correlation between audiovisual and auditory stimuli, indicating that eye movements were equivalent in these conditions and unlikely to explain better decoding accuracy for audiovisual stimuli.</p>
<p>Finally, we note that the stricter eye movement criterion acknowledged in the Discussion section of the original manuscript resulted in significantly better audiovisual d' than the MLE prediction, but this difference did not survive cluster correction. This is an important distinction to make as, when combined with the results described above, it seems to support our original interpretation that the stricter criterion combined with our conservative measure of (mass-based) cluster correction2 led to type 2 error.</p>
<p>References</p>
<p>(1) Roy, R. N., Charbonnier, S., &amp; Bonnet, S. (2014). Eye blink characterization from frontal EEG electrodes using source separation and pattern recognition algorithms. Biomedical Signal Processing and Control, 14, 256–264.</p>
<p>(2) Pernet, C. R., Latinus, M., Nichols, T. E., &amp; Rousselet, G. A. (2015). Cluster-based computational methods for mass univariate analyses of event-related brain potentials/fields: A simulation study. Journal of Neuroscience Methods, 250, 85–93.</p>
<p><bold>Response to Reviewer #2 (Public Review):</bold></p>
<p>We thank the reviewer for their insight and constructive feedback. As emphasized in the review, an interesting question that arises from our results is that, if the neural data exceeds the optimal statistical decision (MLE d'), why doesn’t the behavioural data? We agree with the reviewer’s suggestion that more attention should be devoted to this question, and plan to provide a deeper discussion of the relationship between behavioural and neural super-additivity in the revised manuscript. We also note that while this discrepancy remains unexplained, our results are consistent with the literature. That is, both non-linear neural responses (single-cell recordings) and behavioural responses that match MLE are reliable phenomenon in multisensory integration1,2,3,4.</p>
<p>One possible explanation for this puzzling discrepancy is that behavioural responses occur sometime after the initial neural response to sensory input. There are several subsequent neural processes between perception and a behavioural response5, all of which introduce additional noise that may obscure super-additive perceptual sensitivity. In particular, the mismatch between neural and behavioural accuracy may be the result of additional neural processes that translate sensory activity into a motor response to perform the behavioural task.</p>
<p>Our measure of neural super-additivity (exceeding optimally weighted linear summation) differs from how it is traditionally assessed (exceeding summation of single neuron responses)2. However, neither method has yet fully explained how this neural activity translates to behavioural responses, and we think that more work is needed to resolve the abovementioned discrepancy. However, our method will facilitate this work by providing a reliable method of measuring neural super-additivity in humans, using non-invasive recordings.</p>
<p>References</p>
<p>(1) Alais, D., &amp; Burr, D. (2004). The ventriloquist effect results from near-optimal bimodal integration. Current Biology, 14(3), 257–262.</p>
<p>(2) Ernst, M. O., &amp; Banks, M. S., (2002). Humans integrate visual and haptic information in a statistically optimal fashion. Nature, 415(6870), 429–433.</p>
<p>(3) Meredith, M. A., &amp; Stein, B. E. (1993). Interactions among converging sensory inputs in the superior colliculus. Science, 221, 389–391.</p>
<p>(4) Stanford, T. R., &amp; Stein, B. E. (2007). Superadditivity in multisensory integration: putting the computation in context. Neuroreport 18, 787–792.</p>
<p>(5) Heekeren, H., Marrett, S. &amp; Ungerleider, L. (2008). The neural systems that mediate human perceptual decision making. Nature Reviews Neuroscience, 9, 467–479.</p>
</body>
</sub-article>
</article>