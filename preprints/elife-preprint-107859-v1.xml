<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107859</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107859</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107859.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Megabouts: a flexible pipeline for zebrafish locomotion analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6123-5458</contrib-id>
<name>
<surname>Jouary</surname>
<given-names>Adrien</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>adrien.jouary@research.fchampalimaud.org</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3692-0912</contrib-id>
<name>
<surname>Silva</surname>
<given-names>Pedro TM</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6197-3506</contrib-id>
<name>
<surname>Laborde</surname>
<given-names>Alexandre</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9884-0184</contrib-id>
<name>
<surname>Mata</surname>
<given-names>J Miguel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0972-1149</contrib-id>
<name>
<surname>Marques</surname>
<given-names>João C</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0006-0086-9621</contrib-id>
<name>
<surname>Collins</surname>
<given-names>Elena MD</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0727-3469</contrib-id>
<name>
<surname>Peterson</surname>
<given-names>Randall T</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1717-1562</contrib-id>
<name>
<surname>Machens</surname>
<given-names>Christian K</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9763-8902</contrib-id>
<name>
<surname>Orger</surname>
<given-names>Michael B</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>mike.orger@research.fchampalimaud.org</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03g001n57</institution-id><institution>Champalimaud Research, Champalimaud Center for the Unknown</institution></institution-wrap>, <city>Lisbon</city>, <country country="PT">Portugal</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03r0ha626</institution-id><institution>Department of Pharmacology &amp; Toxicology, College of Pharmacy, University of Utah</institution></institution-wrap>, <city>Salt Lake City</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3588-7820</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Wassum</surname>
<given-names>Kate M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution>
</institution-wrap>
<city>Los Angeles</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-10-29">
<day>29</day>
<month>10</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107859</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-08-01">
<day>01</day>
<month>08</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-05-30">
<day>30</day>
<month>05</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.14.613078"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Jouary et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Jouary et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107859-v1.pdf"/>
<abstract>
<p>Accurate quantification of animal behavior is crucial for advancing neuroscience and for defining reliable physiological markers. We introduce Megabouts (megabouts.ai), a software package standardizing zebrafish larvae locomotion analysis across experimental setups. Its flexibility, achieved with a Transformer neural network, allows the classification of actions regardless of tracking methods or frame rates. We demonstrate Megabouts’ ability to quantify sensorimotor transformations and enhance sensitivity to drug-induced phenotypes through high-throughput, high-resolution behavioral analysis.</p>
</abstract>
<kwd-group kwd-group-type="author">
<kwd>Behavioral methods</kwd>
<kwd>Biomarkers</kwd>
<kwd>Machine learning</kwd>
<kwd>Zebrafish</kwd>
<kwd>Transformer neural network</kwd>
<kwd>Cross-laboratory analysis</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.3030/773012</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution>La Caixa</institution>
</institution-wrap>
</funding-source>
<award-id>LCF/BQ/PR20/11770007</award-id>
</award-group>
<award-group id="funding-2a">
<funding-source>
<institution-wrap>
<institution>La Caixa</institution>
</institution-wrap>
</funding-source>
<award-id>LCF/BQ/PR21/11840005</award-id>
</award-group>
<award-group id="funding-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00snfqn58</institution-id>
<institution>Fundação para a Ciência e Tecnologia</institution>
</institution-wrap>
</funding-source>
<award-id>FCT-PTDC/BIA-OUT/32077/2017-IC&amp;DT-LISBOA- 01-0145-FEDER</award-id>
</award-group>
<award-group id="funding-3a">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00snfqn58</institution-id>
<institution>Fundação para a Ciência e Tecnologia</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.54499/sfrh/bd/147089/2019</award-id>
</award-group>
<award-group id="funding-3b">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00snfqn58</institution-id>
<institution>Fundação para a Ciência e Tecnologia</institution>
</institution-wrap>
</funding-source>
<award-id>LISBOA-01-0145-FEDER-022170</award-id>
</award-group>
<award-group id="funding-4">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01cmst727</institution-id>
<institution>Simons Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>543009</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Figure 2.c revised to extend the evaluation of the behavioral phenotype to a larger set of pharmacological compounds.</p></fn>
</fn-group>
</notes>
</front>
<body>
    <sec id="sa">
        <title>Main</title>
<p>Advances in machine learning and computer vision have significantly improved the ability to track animal posture (<xref ref-type="bibr" rid="c8">Mathis and Mathis, 2020</xref>) and decompose postural time series into a sequence of distinct behavioral motifs (<xref ref-type="bibr" rid="c22">Wiltschko et al., 2015</xref>). These developments have driven significant progress in neuroscience (<xref ref-type="bibr" rid="c12">Pereira et al., 2020</xref>) and preclinical research (<xref ref-type="bibr" rid="c23">Wiltschko et al., 2020</xref>). Yet, comparing behavioral data across laboratories remains challenging due to differences in experimental setups, tracking methods, and analysis pipelines.</p>
<p>The larval zebrafish is well-suited for behavioral analysis, as its locomotion is naturally segmented into a burst-and-glide pattern. Previous research has employed unsupervised clustering methods to categorize these swim bouts and unravel the animal’s behavioral repertoire (<xref ref-type="bibr" rid="c7">Marques et al., 2018</xref>; <xref ref-type="bibr" rid="c5">Johnson et al., 2020</xref>; <xref ref-type="bibr" rid="c9">Mearns et al., 2020</xref>). However, these swim classification methods depend on specific camera resolutions, frame rates, tracking algorithms, and analysis pipelines, making it difficult to compare behavioral phenotypes across laboratories (<xref ref-type="bibr" rid="c21">von Ziegler et al., 2021</xref>). Megabouts tackles this challenge by providing a flexible computational pipeline to homogenize data from different sources (see <xref rid="figS1" ref-type="fig">Fig. S1</xref>). Its primary goal is to process zebrafish tracking time series data, segment it into individual swim bouts, and classify each bout according to the zebrafish’s locomotor repertoire (<xref rid="fig1" ref-type="fig">Fig. 1a,b</xref>). While focusing on freely swimming behavior, an add-on pipeline accommodates head-restrained assays which require specialized behavior segmentation and analysis.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Megabouts pipeline overview and performance</title>
<p><bold>(a) Overview of Megabouts’ pipeline</bold>. Megabouts can process either high-resolution tail and trajectory tracking or low-resolution trajectory tracking. The pipeline begins with data loading and preprocessing, followed by swim bout identification via a segmentation algorithm. A Transformer neural network then classifies each swim bout and predicts the location and sign of the first tail beat. <bold>(b) Behavioral repertoire of zebrafish larvae composed of 13 swim bout categories</bold>. Left: Exemplar tail angle time series for each category, representing bouts closest to the category mean. Right: Corresponding head trajectories with arrows indicating final head orientation. <bold>(c) Performance of the Transformer neural network</bold>. Top: Balanced classification accuracy at different frame rates. The downsampled input data was either presented with or without tail masking. Accuracy remains an order of magnitude above chance level (7.7%) in all conditions. Bottom: 95% confidence interval for the predicted location of the first tail beat. This first tail beat is predicted from the Transformer Neural Network along with the bout category. For frame rates below 100 fps, the prediction accuracy is below the data sampling interval, achieving super-resolution.</p></caption>
<graphic xlink:href="613078v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Zebrafish freely swimming behavior is typically studied in shallow arenas, recorded from above using video imaging. Tracking can be achieved with opensource software designed for zebrafish (<xref ref-type="bibr" rid="c10">Mirat et al., 2013</xref>; <xref ref-type="bibr" rid="c19">Štih et al., 2019</xref>; <xref ref-type="bibr" rid="c4">Guilbeault et al., 2021</xref>) or deep learning-based image processing (<xref ref-type="bibr" rid="c8">Mathis and Mathis, 2020</xref>; <xref ref-type="bibr" rid="c13">Pereira et al., 2022</xref>). However, there is considerable variation in the spatial and temporal resolution, with a trade-off between resolution and throughput. Megabouts accommodates these variations (see <xref rid="figS2" ref-type="fig">Fig. S2</xref>) by supporting two configurations for freely swimming behavior: ‘tail+trajectory tracking’, which involves high-speed tracking (100 to 2000 fps) of multiple tail landmarks, as well as head position and angle (cf. <xref ref-type="fig" rid="video1">SV1</xref>); and ‘trajectory tracking’, which requires only the fish’s position and head angle, making it suitable for high-throughput assays (cf. <xref ref-type="fig" rid="video2">SV2</xref>).</p>
<p>To validate Megabouts across different tracking configurations, we used a dataset of 4 million swim bouts recorded at high resolution (see Supp. Note 3.A). The segmentation and classification of these bouts (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>) using the method from <xref ref-type="bibr" rid="c7">Marques et al., 2018</xref> provided our reference labels. Using downsampling and masking strategies (<xref rid="figS4" ref-type="fig">Fig. S4c</xref>), we tested the software’s performance at lower frame rates and with trajectory-only tracking, ensuring high accuracy across conditions.</p>
<p>Segmenting tracking data into swim bouts is straightforward, given zebrafish’s burst-and-glide swimming pattern. We developed a segmentation algorithm that accurately (&gt;94%, cf. <xref rid="figS3" ref-type="fig">Fig. S3</xref> and Supp. Note 4.F.1) detects swim bouts across all frame rates using either tail oscillation speed or head positional and yaw velocities (Supp. Note 4.C).</p>
<p>Once the tracking data is segmented into swim bouts, we characterize the swim bouts using a Transformer neural network (<xref ref-type="bibr" rid="c20">Vaswani, 2017</xref>, cf <xref rid="figS4" ref-type="fig">Fig. S4b</xref> and Supp. Note 4.E). This architecture is well-suited for dealing with missing data. Each input token to the Transformer contains information about the larva’s posture and position at a single time step. The Transformer’s selfattention mechanism identifies relationships between posture measurements and focuses on the most relevant features. This contrasts with previous methods that rely on time series distance (<xref ref-type="bibr" rid="c9">Mearns et al., 2020</xref> or feature extraction <xref ref-type="bibr" rid="c7">Marques et al., 2018</xref>, which are disrupted by missing data, temporal misalignment or frame rate variations. The Transformer architecture overcomes these limitations, offering robust classification across diverse conditions.</p>
<p>We trained the network using a supervised learning approach, the output consists of key movement features: bout category and subcategory, direction, and timing of the first tail beat. We found that the network’s performance reached a plateau at around 120 fps, and the classification accuracy remained high even without tail tracking (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>, <xref rid="figS5" ref-type="fig">Fig. S5a</xref>). The first tail beat location was also predicted with high accuracy (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>), achieving super-resolution for frame rates below 100 fps.</p>
<p>After validating Megabouts’ performance, we explored several use cases that highlight how its detailed behavioral descriptions improve upon coarser measures of behavior.</p>
<p>Understanding how sensory stimuli map to specific motor outputs is central to neuroscience. Megabouts’ swim bout categories were specifically designed to preserve these sensorimotor relationships (<xref ref-type="bibr" rid="c7">Marques et al., 2018</xref>). For example, while O-bend and Short Latency C-Start are both escape-like behaviors, they are known to be triggered by different stimuli (<xref ref-type="bibr" rid="c2">Burgess and Granato, 2007</xref>; <xref ref-type="bibr" rid="c1">Bhattacharyya et al., 2017</xref>). We found that Megabouts improved the mapping between external stimuli and the fish’s actions (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>) as measured by the mutual information (see Supp. Note 4.F.2 and <xref rid="figS6" ref-type="fig">Fig. S6</xref> for subcategory details). Bout direction extracted from the first tail beat laterality is also significant. For instance, escapes are more likely generated contralaterally during aversive stimuli like looming or approaching dot (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). Interestingly, forward movements such as Slow 2 are also directionally biased, occurring more ipsilateral to the grating direction (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). These results illustrate the advantage of using a detailed library to accurately quantify the ‘tuning curve’ of actions.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Sensorimotor coupling and behavioral phenotyping</title>
<p><bold>(a) Sensorimotor coupling</bold>. Conditional probability of swim bout occurrence given a stimulus, computed from 1.95 million bouts from 108 larvae. Bout categories are color-coded as in <xref ref-type="fig" rid="fig1">Fig. 1b</xref>. For directional stimuli, bouts are split into ipsilateral or contralateral based on the direction of the first tail beat relative to the stimulus. <bold>(b) Temporal dynamics of swim bouts recorded using Zebrabox</bold>. Average frequency of swim bout initiation for each category as a function of time relative to the light on/off stimuli, averaged over 4 trials for N=372 larvae. The subplots for each category share the same y-scale. <bold>(c) Enhanced phenotyping of neuroactive drugs</bold>. MiniRocket classifier trained on three behavioural time series: binary movement (top), locomotion speed (middle) and Megabouts action categories (bottom). Vertical dotted line is chance level at 11%.</p></caption>
<graphic xlink:href="613078v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Megabouts can also uncover the temporal structure of behavior. The light on / off protocol is popular for high-throughput phenotyping (<xref ref-type="bibr" rid="c6">Kokel et al., 2010</xref>; <xref ref-type="bibr" rid="c17">Rihel et al., 2010</xref>). Typically, behavior quantification is performed using measures of the animal’s overall locomotion, such as the frequency of swim bouts, modulated by light depending on the fish’s condition (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>). Using Megabouts on data from a commercial recording system at 25 fps, we broke down the locomotion time series into temporal profiles with distinct time scales and on/off selectivity (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>). Notably, we observed consistent O-Bend responses after light off across different well sizes (see <xref rid="figS7" ref-type="fig">Fig. S7</xref> and <xref ref-type="fig" rid="video3">SV3</xref>). This allows for analyses that were previously limited to high frame rate cameras (<xref ref-type="bibr" rid="c15">Randlett et al., 2019</xref>) and opens possibilities to analyze action sequences to detect latent internal states and long-term modulation (<xref ref-type="bibr" rid="c16">Reddy et al., 2022</xref>).</p>
<p>Finally, we asked whether the rich action-level description provided by Megabouts could sharpen the behavioural fingerprint of drugs, building on work showing detailed behavioral quantification in mice improved phenotyping(<xref ref-type="bibr" rid="c23">Wiltschko et al., 2020</xref>). We recorded 162 larvae exposed to nine neuroactive compounds while presenting a battery of visual stimulus (see Supp. Note 2.B). We then trained a MiniRocket time-series classifier citepdempster2021minirocket to identify the compound from behavioural data represented at three different resolutions: a binary “movement vs. no-movement” trace; the head locomotion speed; or the full Megabouts action-category time series. Prediction accuracy rose from 31% with the binary trace to 40% with speed, and leapt to 52% with action categories (chance=11%; <xref rid="fig2" ref-type="fig">Fig. 2c</xref>). Thus, fine-grained segmentation of behaviour substantially boosts the sensitivity and specificity of phenotype-based screening.</p>
<p>Megabouts also includes a pipeline for head-restrained experiments (see Supp. Note 5), commonly used for neural imaging. Head fixation leads to postures not seen during freely swimming, variable duration and modulation of frequency within a single movement (<xref ref-type="bibr" rid="c14">Portugues and Engert, 2011</xref>), making free-swimming classification methods unsuitable. To quantify the less stereotyped, variable-length tail movements, we developed a new approach based on convolutional sparse coding (<xref ref-type="bibr" rid="c24">Wohlberg, 2017</xref>; <xref ref-type="bibr" rid="c3">Cogliati et al., 2017</xref>). This method is grounded in the hypothesis that complex tail movements are generated by sparse control signals driving different tail oscillation motifs (<xref ref-type="bibr" rid="c11">Mullen et al., 2024</xref>). Convolutional sparse coding represents the tail angle as a combination of motifs, learned from the data: slow, fast, turn, and escape/struggle (see <xref rid="figS8" ref-type="fig">Fig. S8a</xref>).</p>
<p>We validated this approach using optomotor behavior, where the fast motif was progressively recruited with increasing grating speed (see <xref rid="figS8" ref-type="fig">Fig. S8c,d</xref>), consistent with free-swimming observations (<xref ref-type="bibr" rid="c18">Severi et al., 2014</xref>). These motifs provide refined motor regressors for neural activity analysis.”</p>
<p>Finally, the website megabouts.ai offers extensive documentation on the code base and provides tutorials for common use cases. By providing a common atlas for zebrafish behavior, the Megabouts Python package has the potential to foster cross-disciplinary collaborations between neuroscience, genomics, and pharmacology, accelerating scientific discovery.</p>
    </sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Emily G. Tippetts for her guidance on the Zebrabox setup, and Dean Rance and Carolina Gonçalves for their valuable insights on the Transformer architecture. Special thanks to Leandro Scholz for his feedback on the manuscript and for being an early user of the package. We also appreciate the helpful feedback from other early users: Claudia Feierstein, Joaquim Contradanças, Katharina Kötter, Daguang Li, and Emiliano Marachlian. Lastly, we are grateful to the Champalimaud Research Fish Platform for their logistical support. This work was realized through funding to M.B.O from an ERC Consolidator Grant (ERC NEUROFISH 773012) and to A.J. from the La Caixa Foundation (LCF/BQ/PR20/11770007). C.K.M and A.J. received support from Fundação para a Ciência e Tecnologia (FCT-PTDC/BIA-OUT/32077/2017-IC&amp;DT-LISBOA-01-0145-FEDER). J.C.M. received the support of a fellowship from “la Caixa” Foundation (ID100010434, LCF/BQ/PR21/11840005). E.M.D.C. received support from Fundação para a Ciência e Tecnologia (Bolsa SFRH/BD/147089/2019). M.B.O and C.K.M received support from the Champalimaud Foundation; C.K.M from the Simons Collaboration on the Global Brain (543009); and M.B.O from the Volkswagen Stiftung Life? Initiative. We received support from the research infrastructure CONGENTO, co-financed by the Lisboa Regional Operational Programme (Lisboa2020) under the PORTUGAL 2020 Partnership Agreement, through the European Regional Development Fund (ERDF) and Fundação para a Ciência e Tecnologia (Portugal), under the project LISBOA-01-0145-FEDER-022170.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
    <sec id="s1">
        <title>Code availability</title>
        <p>Megabouts is available under a non-commercial research and academic license. All software, algorithms and tools are available on GitHub: at <ext-link ext-link-type="uri" xlink:href="https://github.com/orger-lab/megabouts">https://github.com/orger-lab/megabouts</ext-link>. The full documentation of Megabouts is available at <ext-link ext-link-type="uri" xlink:href="http://www.megabouts.ai">megabouts.ai</ext-link>.</p>
    </sec>
    <sec id="s2">
<title>Author contributions</title>
<p>A.J. and P.T.S. designed and performed high-resolution freely swimming behavioral experiments. J.C.M. developed the prey-capture assay and collected the corresponding dataset. E.M.D.C. developed the headrestrained assay and collected the corresponding dataset. J.M.M. and A.J performed pharmacological treatment experiments and collected data. A.J. and R.T.P. designed and conducted Zebrabox experiments. A.L., A.J., and M.B.O. developed the tracking software and visual stimuli presentation code, and contributed to the design of the analysis pipeline. A.J. performed the data analysis and developed the Megabouts software. M.B.O. and C.K.M. contributed to the design of the analysis pipeline and supervised the research. All authors contributed to the manuscript.</p>
</sec>

</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="video1">
<label>Supplementary Video 1.</label>
<caption><title>Use case of Megabouts for computing ethogram from ‘tail+trajectory tracking’ at 700 fps.</title></caption>
<media xlink:href="supplements/613078_file04.mp4"/>
</supplementary-material>
<supplementary-material id="video2">
<label>Supplementary Video 2.</label>
<caption><title>Use case of Megabouts for computing ethogram from ‘trajectory tracking’ at 25 fps.</title></caption>
<media xlink:href="supplements/613078_file03.mp4"/>
</supplementary-material>
<supplementary-material id="video3">
<label>Supplementary Video 3.</label>
<caption><title>High-throughput recording of ethogram during light on/off stimuli.</title></caption>
<media xlink:href="supplements/613078_file02.mp4"/>
</supplementary-material>
</sec>
    <sec id="nt1">
        <title>Note</title>
        <p>This reviewed preprint has been updated to correct level headings.</p>
    </sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bhattacharyya</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>McLean</surname>, <given-names>D. L.</given-names></string-name>, and <string-name><surname>MacIver</surname>, <given-names>M. A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Visual threat assessment and reticulospinal encoding of calibrated responses in larval zebrafish</article-title>. <source>Current Biology</source>, <volume>27</volume> (<issue>18</issue>):<fpage>2751</fpage>–<lpage>2762</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burgess</surname>, <given-names>H. A.</given-names></string-name> and <string-name><surname>Granato</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Sensorimotor gating in larval zebrafish</article-title>. <source>Journal of Neuroscience</source>, <volume>27</volume>(<issue>18</issue>):<fpage>4984</fpage>–<lpage>4994</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cogliati</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Duan</surname>, <given-names>Z.</given-names></string-name>, and <string-name><surname>Wohlberg</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Piano transcription with convolutional sparse lateral inhibition</article-title>. <source>IEEE Signal Processing Letters</source>, <volume>24</volume>(<issue>4</issue>):<fpage>392</fpage>–<lpage>396</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guilbeault</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Guerguiev</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tate</surname>, <given-names>I.</given-names></string-name>, and <string-name><surname>Thiele</surname>, <given-names>T. R.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Bonzeb: Open-source, modular software tools for high-resolution zebrafish tracking and analysis</article-title>. <source>Scientific Reports</source>, <volume>11</volume>(<issue>1</issue>):<fpage>8148</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Linderman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Panier</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wee</surname>, <given-names>C. L.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Herrera</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>A.</given-names></string-name>, and <string-name><surname>Engert</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Probabilistic models of larval zebrafish behavior reveal structure on many scales</article-title>. <source>Current Biology</source>, <volume>30</volume>(<issue>1</issue>):<fpage>70</fpage>–<lpage>82</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kokel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bryan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Laggner</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>White</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Cheung</surname>, <given-names>C. Y. J.</given-names></string-name>, <string-name><surname>Mateus</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Healey</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Werdich</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Haggarty</surname>, <given-names>S. J.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2010</year>). <article-title>Rapid behavior-based identification of neuroactive small molecules in the zebrafish</article-title>. <source>Nature chemical biology</source>, <volume>6</volume>(<issue>3</issue>):<fpage>231</fpage>–<lpage>237</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marques</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Lackner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Félix</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Orger</surname>, <given-names>M. B.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Structure of the zebrafish locomotor repertoire revealed with unsupervised behavioral clustering</article-title>. <source>Current Biology</source>, <volume>28</volume>(<issue>2</issue>):<fpage>181</fpage>–<lpage>195</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>M. W.</given-names></string-name> and <string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title>. <source>Current opinion in neurobiology</source>, <volume>60</volume>:<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mearns</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Donovan</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Fernandes</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Semmelhack</surname>, <given-names>J. L.</given-names></string-name>, and <string-name><surname>Baier</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Deconstructing hunting behavior reveals a tightly coupled stimulus-response loop</article-title>. <source>Current Biology</source>, <volume>30</volume>(<issue>1</issue>):<fpage>54</fpage>–<lpage>69</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mirat</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sternberg</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Severi</surname>, <given-names>K. E.</given-names></string-name>, and <string-name><surname>Wyart</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Zebrazoom: an automated program for high-throughput behavioral analysis and categorization</article-title>. <source>Frontiers in neural circuits</source>, <volume>7</volume>:<fpage>107</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Mullen</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Schimel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hennequin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name>, <string-name><surname>Orger</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Jouary</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Learning interpretable control inputs and dynamics underlying animal locomotion</article-title>. In <conf-name>The Twelfth International Conference on Learning Representations</conf-name>, (<year>2024</year>).</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Shaevitz</surname>, <given-names>J. W.</given-names></string-name>, and <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Quantifying behavior to understand the brain</article-title>. <source>Nature neuroscience</source>, <volume>23</volume>(<issue>12</issue>):<fpage>1537</fpage>–<lpage>1549</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Tabris</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Matsliah</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ravindranath</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Papadoyannis</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Normand</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Deutsch</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z. Y.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2022</year>). <article-title>Sleap: A deep learning system for multi-animal pose tracking</article-title>. <source>Nature methods</source>, <volume>19</volume>(<issue>4</issue>):<fpage>486</fpage>–<lpage>495</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Portugues</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Engert</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Adaptive locomotor behavior in larval zebrafish</article-title>. <source>Frontiers in systems neuroscience</source>, <volume>5</volume>:<fpage>72</fpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Randlett</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Haesemeyer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Forkin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Shoenhard</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Schier</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Engert</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Granato</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Distributed plasticity drives visual habituation learning in larval zebrafish</article-title>. <source>Current Biology</source>, <volume>29</volume>(<issue>8</issue>):<fpage>1337</fpage>–<lpage>1345</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reddy</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Desban</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Tanaka</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Roussel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mirat</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Wyart</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A lexical approach for identifying behavioural action sequences</article-title>. <source>PLoS computational biology</source>, <volume>18</volume> (<issue>1</issue>):<fpage>e1009672</fpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rihel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Prober</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Arvanites</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lam</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zimmerman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Jang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Haggarty</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Kokel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Rubin</surname>, <given-names>L. L.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>R. T.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2010</year>). <article-title>Zebrafish behavioral profiling links drugs to biological targets and rest/wake regulation</article-title>. <source>Science</source>, <volume>327</volume>(<issue>5963</issue>):<fpage>348</fpage>–<lpage>351</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Severi</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Portugues</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Marques</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>O’Malley</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Orger</surname>, <given-names>M. B.</given-names></string-name>, and <string-name><surname>Engert</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Neural control and modulation of swimming speed in the larval zebrafish</article-title>. <source>Neuron</source>, <volume>83</volume>(<issue>3</issue>):<fpage>692</fpage>–<lpage>707</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Štih</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Petrucco</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kist</surname>, <given-names>A. M.</given-names></string-name>, and <string-name><surname>Portugues</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Stytra: An open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments</article-title>. <source>PLoS computational biology</source>, <volume>15</volume>(<issue>4</issue>):<fpage>e1006699</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Vaswani</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Attention is all you need</article-title>. <conf-name>Advances in Neural Information Processing Systems</conf-name>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>von Ziegler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sturman</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Bohacek</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Big behavior: challenges and opportunities in a new era of deep behavior profiling</article-title>. <source>Neuropsychopharmacology</source>, <volume>46</volume>(<issue>1</issue>):<fpage>33</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiltschko</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Iurilli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Katon</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Pashkovski</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Abraira</surname>, <given-names>V. E.</given-names></string-name>, <string-name><surname>Adams</surname>, <given-names>R. P.</given-names></string-name>, and <string-name><surname>Datta</surname>, <given-names>S. R.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Mapping sub-second structure in mouse behavior</article-title>. <source>Neuron</source>, <volume>88</volume>(<issue>6</issue>):<fpage>1121</fpage>–<lpage>1135</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiltschko</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Tsukahara</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Zeine</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Anyoha</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gillis</surname>, <given-names>W. F.</given-names></string-name>, <string-name><surname>Markowitz</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Katon</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>M. J.</given-names></string-name>, and <string-name><surname>Datta</surname>, <given-names>S. R.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title>. <source>Nature neuroscience</source>, <volume>23</volume>(<issue>11</issue>):<fpage>1433</fpage>–<lpage>1443</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Wohlberg</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Sporco: A python package for standard and convolutional sparse representations</article-title>. In <conf-name>SciPy</conf-name>, pages <fpage>1</fpage>–<lpage>8</lpage>, (<year>2017</year>).</mixed-citation></ref>
</ref-list>
<app-group>
<app id="s4">
<title>Supplementary Figure</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Comparison of Megabouts Bout Classification with Labels from Mearns et al., <bold>2020</bold></title>
<p>This contingency table compares swim bout categories classified by Megabouts (rows) and those from <xref ref-type="bibr" rid="c9">Mearns et al., 2020</xref> (columns), computed from approximately 40,000 bouts in the <xref ref-type="bibr" rid="c9">Mearns et al., 2020</xref> dataset. Each cell shows the proportion of bouts in a Megabouts category that match a <xref ref-type="bibr" rid="c9">Mearns et al., 2020</xref> category, normalized by the total number of bouts in each Megabouts category (values from 0 to 1). Darker shades indicate higher correspondence. The differences, even for similarly named categories, highlight the need for a standardized classification provided by Megabouts.</p></caption>
<graphic xlink:href="613078v3_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Tracking configuration compatible with Megabouts</title>
<p>Megabouts supports multiple tracking configurations: ‘tail tracking’ (for head-restrained conditions), ‘head tracking’, or ‘full tracking’. Head tracking requires two keypoints to estimate the position and orientation of the larva. Tail tracking requires at least four keypoints from the swim bladder to the tail tip. Data can be provided as keypoint coordinates or as posture variables, including head yaw and tail curvature.</p></caption>
<graphic xlink:href="613078v3_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption>
<title>Segmentation algorithm and performance</title>
<p><bold>(a) Illustration of the segmentation algorithm</bold>. <bold>(i) High frame-rate segmentation</bold>. A 10-second sample of tail tracking at 700 fps is shown. By applying a threshold to the tail vigor, we can identify the onset (green line) and offset (red dot) of swim bouts. <bold>(ii) Downsampled to 80 fps with tail masking</bold>. The same data is downsampled to 80 fps. Trajectory speed is computed based on the derivative of x,y and yaw coordinates. First, a peak-finding algorithm is used to locate the maximum trajectory speed. Then, a threshold of 20% of this peak value is applied to determine the onset and offset of swim bouts. <bold>(iii) Downsampled to 20 fps with tail masking</bold>. The same process as in (ii) but with downsampling to 20 fps. <bold>(b) Onset jitter</bold>. The distribution of the difference between onset times computed using tail angle vigor at 700 fps and trajectory at 20 fps is shown. The analysis is based on N=4 larvae from high-resolution recordings. <bold>(c) Segmentation accuracy</bold>. A reference segmentation was computed from tail angle at 700fps using 20 larvae. This reference was compared with segmentation obtained from trajectory-only data downsampled from 20 to 700 fps. See Supp. Note 4.F.1.</p>
</caption>
<graphic xlink:href="613078v3_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption>
<title>Transformer input and architecture</title>
<p><bold>(a) Input data for the Transformer</bold>. For each swim bout, the first seven out of ten tail angles (<italic>γ</italic><sub>1</sub>, …, <italic>γ</italic><sub>7</sub>) are used as input, along with coordinates computed after the bout onset, relative to the fish’s position and orientation at onset. <bold>(b) Transformer architecture</bold>. Each token [<italic>T ok t</italic><sub><italic>i</italic></sub>] corresponds to the pose measured at time <italic>t</italic><sub><italic>i</italic></sub> relative to the movement onset. [<italic>T ok t</italic><sub><italic>i</italic></sub>] is a vector containing 11 values: tail angles (<italic>γ</italic><sub>1</sub>(<italic>t</italic><sub><italic>i</italic></sub>),…, <italic>γ</italic><sub>7</sub>(<italic>t</italic><sub><italic>i</italic></sub>)), <italic>x</italic>(<italic>t</italic><sub><italic>i</italic></sub>) and <italic>y</italic>(<italic>t</italic><sub><italic>i</italic></sub>) coordinates, and the head angle cos(<italic>θ</italic>(<italic>t</italic><sub><italic>i</italic></sub>)), sin(<italic>θ</italic>(<italic>t</italic><sub><italic>i</italic></sub>)). Missing measurements are replaced with a fixed learned value. After passing through an embedding layer, temporal embeddings are added. The tokens are then processed by 3 layers of 8-head multi-head self-attention. Finally, outputs are computed from the learned classification token [CLS]. <bold>(c) Illustration of downsampling/masking data augmentation</bold>. The process begins with an example bout. Each line corresponds to a token dimension. A random shift is applied to the segmentation (two vertical black lines) to achieve translational invariance in bout classification and jitter the first tail beat location (shown as the vertical orange line). The time series is then downsampled, and masking can omit the tail information.</p>
</caption>
<graphic xlink:href="613078v3_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Classification accuracy and bout directionality</title>
<p><bold>(a) Confusion matrix</bold>. Left: Confusion matrix at high resolution, corresponding to a balanced accuracy of 89.1%. Right: Confusion matrix using only trajectory data downsampled to 60 fps, corresponding to a balanced accuracy of 71.2%. <bold>(b) Performance in bout subcategory and directionality</bold>. Left: Balanced classification accuracy for swim bout subcategories across different frame rates. The downsampled input data were presented with or without tail masking. Right: Accuracy of the estimation of the first tail half beat amplitude sign, which determines the directionality of the movements.</p></caption>
<graphic xlink:href="613078v3_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6.</label>
<caption><title>Swim bouts by subcategory</title>
<p>(<bold>a</bold>) For each swim bout subcategory, 50 swim bouts were selected from those in the test dataset that were classified with high probability (&gt;98%). (<bold>b</bold>) We illustrate the probability of bouts occurring from specific subcategories given the stimulus presented. The O-bend subcategories O1 and O2 are presented in different plots since they have very different probabilities of occurrence.</p></caption>
<graphic xlink:href="613078v3_figS6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7.</label>
<caption><title>Occurrence of O-Bend in response to light stimuli</title>
<p>Probability of occurrence of O-Bend across time for different well sizes. The probability was computed using N=2 wells for each size. The probability was smoothed using a 1-sec box-car filter.</p></caption>
<graphic xlink:href="613078v3_figS7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Figure S8.</label>
<caption><title>Head-restrained pipeline for motif analysis</title>
<p><bold>(a)</bold> Illustration of the four motifs learnt from the data: slow, fast, turn, struggle. Left: heat map of each motif shows tail beat propagation along rostral-caudal segments and time. Right: Same data as time series with increasing contrast along rostral-caudal axis. (The motif color code is shared in a., b. and d.). <bold>(b)</bold> Illustration of sparse coding and motif vigor. Top. The original tail angle (black) is reconstructed by adding the four components (red). Middle. Each component is computed by the convolution of a sparse code and the motif. Bottom. The contribution of each motif can be computed as the rolling variance of each component (we used a window of 40ms) <bold>(c)</bold> A grating moving at different speeds was displayed below a head-restrained larva. A virtual-reality system was used to adjust the speed of the moving grating depending on the fish tail speed. <bold>(d)</bold> For each motif we computed the vigor (left) or proportion of motif recruitment (right) as a function of the grating speed (filled area corresponds to mean±std.error, N = 25 larvae).</p></caption>
<graphic xlink:href="613078v3_figS8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</app>
<app id="s5">
<title>Supplementary Information</title>
<sec id="s5a">
<title>Supplementary Note 1: Fish Care</title>
<p>Adult fish were maintained at 28<sup><italic>°</italic></sup>C on a 14:10 hour light cycle. Embryos were collected and larvae were raised at 28<sup><italic>°</italic></sup>C in E3 embryo medium (5 mM NaCl, 0.17 mM KCl, 0.33 mM CaCl2 and 0.33 mM MgSO4) in groups of 25. Sexual differentiation occurs at a later stage, and therefore the sex of the animals cannot be reported. Behavioral experiments were conducted using the wild-type line Tubingen (Tu) between 5 and 7 days post fertilization (dpf). Larvae were fed with rotifers (<italic>Brachionus sp</italic>.) from 5 dpf onwards. All experimental procedures were approved by the Champalimaud Foundation Ethics Committee and the Portuguese Direcção Geral Veterinária, and were performed according to the European Directive 2010/63/EU. The Zebrabox experiments were preformed according to the University of Utah’s Institutional Animal Care and Use Committee (IACUC) guidelines under protocol 00,001,487.</p>
</sec>
<sec id="s5b">
<title>Supplementary Note 2: Behavioral experiments</title>
<sec id="s5b1">
<label>2.A.</label>
<title>High-resolution experiments</title>
<sec id="s5b1a">
<label>2.A.1.</label>
<title>Setup</title>
<p>The behavior of each larva was recorded in a circular acrylic arena carved and polished using a CNC machine (Núcleo de Oficinas, Instituto Superior Técnico). The arena had a diameter of 50 mm and a depth of 4 mm, with rounded edges achieved by a progressive bevel with a radius of curvature of 11 mm. Fish were imaged using an infrared array consisting of 64 LEDs at 850 nm (TSHG6400, Vishay Semiconductor) controlled by a T-Cube LED Driver (LEDD1B, Thorlabs), a fixed focal length lens (86-207, f = 16 mm, Edmunds Optics), an infrared long-pass filter (LP780-37, 780 nm, VisionLightTech), and a high-speed camera (EoSensCL MC1362, Mikrotron). Camera images were acquired using a frame grabber (PCIe-1433, National Instrument). The camera settings allow to image a region of 948*948 pixels at 700 fps and a shutter time of 1.423 ms. For visual stimulation, we used a video projector (ML750e, Optoma) along with a neutral density filter (NE06A, Thorlabs) and a cold mirror (64-452, Edmunds Optics) to project an image onto a diffuser screen (three layers of Rosco Cinegel White Diffuser 3000) positioned 5 mm below the larva. A transducer speaker (4501, VISATON) glued to the acrylic plate beneath the fish area delivered acoustic stimuli.</p>
</sec>
<sec id="s5b1b">
<label>2.A.2.</label>
<title>Behavioral tracking</title>
<p>The posture was computed in real-time using a custom-written C# software. This software tracks both eyes and follows the tail curve using 10 segments, starting at the swim bladder.</p>
<p>Image processing is performed on background-subtracted images. The background for each pixel was computed using the mode of the distribution of pixel values recorded in 100 frames sampled over a few seconds to ensure sufficient fish movement. Using the mode rather than the mean prevents contamination of the background by the fish image, assuming that each pixel is imaged in absence of the fish most of the time. A large Gaussian kernel is applied to smooth the background-subtracted image. The maximum value indicates the fish location. From this maximum, a flood-fill algorithm is employed to obtain a binarized image of the fish trunk. The principal axis of the fish trunk is computed from the moments of the binarized region. The fish head direction is then determined by finding the point along the principal axis where the total intensity along the orthogonal line is maximum. This location corresponds to the mid-eye. The relative position of the eye to the fish centroid provides a robust identification of the fish heading direction.</p>
<p>From this, the tail is found by following an iterative arc-search algorithm. This algorithm tracks the skeleton of the fish by localizing the next keypoints along an arc of ± 60<sup><italic>°</italic></sup> around the direction from the previous keypoints. Keypoints are found by computing the weighted mean along the arc (with the weight being zero below a given threshold or equal to the pixel value of the background-subtracted image).</p>
<p>The algorithm runs in real-time at 700 Hz. It leverages the fast frame rate by limiting the search to a region of interest around the fish position in the previous frame. Time consuming full-frame operations are avoided; for instance, thresholding is performed only on pixels along the arc rather than on the entire image.</p>
</sec>
<sec id="s5b1c">
<label>2.A.3.</label>
<title>Visual and acoustic stimuli</title>
<p>We use the term ‘virtual open loop’ to describe a configuration where, although an experimental closed-loop system presents visual stimuli relative to the fish’s position and orientation, the fish’s displacement does not affect the stimuli’s relative position, effectively creating an open-loop condition.</p>
<p>The stimuli presented consisted of:</p>
<list list-type="bullet">
<list-item><p><bold>Approaching Dot</bold>. This stimulus is displayed in a virtual open loop. A black disk (1 mm radius), initially positioned 2 cm away from the fish, approaches at a speed of 5 mm/s. After the approach, the disk remains below the fish for 1 second. The disk moves in a direction that is perpendicular to the fish’s head vector (± 90<sup><italic>°</italic></sup>).</p></list-item>
<list-item><p><bold>Directional Optomotor Response</bold>. This stimulus is displayed in a virtual open loop. A moving grating with a spatial period of 10 mm moves at 10 mm/s at an angle of either 0<sup><italic>°</italic></sup>, 45<sup><italic>°</italic></sup>, 90<sup><italic>°</italic></sup>, 135<sup><italic>°</italic></sup>, 180<sup><italic>°</italic></sup>, 225<sup><italic>°</italic></sup>, 270<sup><italic>°</italic></sup>, or 315<sup><italic>°</italic></sup> relative to the fish’s heading. For each direction, the grating is first presented at a speed of 0 below the fish for 5 seconds, then moves at 10 mm/s for 10 seconds.</p></list-item>
<list-item><p><bold>Looming</bold>. This stimulus is displayed in a virtual open loop. A black disk, with its center 4 cm away from the larva, increases its radius at a linear speed of 10 mm/s until it reaches the larva after 4 seconds, at which point it disappears. The disk center is located in a direction that is perpendicular to the fish’s head vector (±90<sup><italic>°</italic></sup>).</p></list-item>
<list-item><p><bold>Acoustic Startle</bold>. A 100 ms pulse at either 100 Hz or 600 Hz. The sensorimotor mapping displayed in <xref rid="fig2" ref-type="fig">Fig. 2a</xref> counts the bout category probability up to 2 seconds following the pulse onset.</p></list-item>
<list-item><p><bold>Forward Optomotor Response</bold>. A moving grating with a spatial period of 10 mm and a velocity of 8, 16, or 24 mm/s is displayed along the fish axis. This stimulus orientation was in an open loop; regardless of how much the fish rotated, it was always aligned with the grating. However, a closed-loop was used for forward locomotion. The phase of the grid was computed as a function of the forward speed of the fish. Thus, if the fish swims at a speed matching the grating, it would see a static grating orthogonal to its swimming direction.</p></list-item>
<list-item><p><bold>Light/Dark</bold>. Switch the projector intensity from 1000 lux to 0 for 30 seconds. The sensorimotor mapping computed in <xref rid="fig2" ref-type="fig">Fig. 2a</xref> counts the bout in the first 4 seconds following the switch.</p></list-item>
</list>
<p>Visual stimuli were displayed at 60 fps using a custom-written rendering engine using OpenTK and generated using OpenGL Shaders. After 8 minutes of habituation, stimuli were presented in 6 pseudo-random order blocks, including inter-stimuli intervals of 3 minutes. The experiment lasted 3 hours.</p>
</sec>
<sec id="s5b1d">
<label>2.A.4.</label>
<title>Prey Capture</title>
<p>Prey capture assays were performed as described previously (<xref ref-type="bibr" rid="c7">Marques et al., 2018</xref>). The behavioral setup used in these experiments is similar to the one used for the other behaviors, but was fitted with a Schneider apo-Xenoplan 2.0/35 lens (Jos. Schneider Optische Werke GmbH, Germany) and had a higher spatial resolution (365 pixels per cm) to ensure correct tracking of the eyes and prey. Experiments used a 25 mm x 25 mm with 3 mm depth square arena and larvae were illuminated from above with white light (1000 lux). Fish were fed with 50-100 paramecia (<italic>Paramecium caudatum</italic>) or rotifers (<italic>Brachionus plicatilis</italic>) and were allowed to hunt for 1 to 2h. For a subset of the long capture swims larvae roll their body generating tail tracking mistakes. These bouts were excluded from the analysis.</p>
</sec>
</sec>
<sec id="s5b2">
<label>2.B.</label>
<title>Pharmacological screening experiments</title>
<sec id="s5b2a">
<label>2.B.1.</label>
<title>Setup</title>
<p>We recorded larvae swimming in an array of 20 wells. Each larva was placed in an individual circular acrylic arena carved and polished using a CNC machine (Núcleo de Oficinas, Instituto Superior Técnico, Portugal). The arena had a diameter of 44 mm and a depth of 4 mm, with tapered edges achieved by a bevel slope of 3 mm, resulting in a bottom diameter of 38 mm. Fish were imaged using an EoSens 4CXP Camera (MC4086, Mikrotron) running at 400 fps. The frame grabber was a Silicon Software GmbH AQ8 CXP6D. The camera settings allow to image a region of 2336*1728 pixels at 400 fps, the field of view of the camera was 20 cm x 25 cm. Fish were illuminated by an infrared array consisting of 64 LEDs at 850 nm (TSHG6400, Vishay Semiconductor) controlled by a T-Cube LED Driver (LEDD1B, Thorlabs) with a spacing sufficient to cover the full FOV, a fixed focal length lens (Xenoplan 2.0/28, Schneider Optische Werke), an infrared long-pass filter (LP780-37, 780 nm,VisionLightTech). For visual stimulation, a video projector (ML750e, Optoma) and a cold mirror (64-452, Edmunds Optics) projected an image onto a diffuser screen (three layers of Rosco Cinegel White Diffuser #3000) positioned 5 mm below the larva.</p>
</sec>
<sec id="s5b2b">
<label>2.B.2.</label>
<title>Behavioral tracking</title>
<p>Due to the lower resolution resulting from the large field of view (FOV), we limited the tracking to the larva’s position and orientation. The background subtraction and body tracking methods are similar to those described in <xref ref-type="sec" rid="s5b">Supp. Note 2.A</xref>, but they do not include the arc search for tail segments.</p>
</sec>
<sec id="s5b2c">
<label>2.B.3.</label>
<title>Visual stimuli</title>
<p>The stimuli presented consisted of <bold>Approaching Dot, Directional Optomotor Response</bold> and <bold>Light/Dark</bold> described in <xref ref-type="sec" rid="s5b">Supp. Note 2.A</xref>. Note that this experimental protocol presented stimuli with a lower inter stimuli interval than in <xref ref-type="sec" rid="s5b">Supp. Note 2.A</xref>, reaching as little as 15 s between two approaching dot stimuli presentations. This was used to probe potential differences in habituation or fatigue</p>
<p><bold><italic>Drug treatment</italic></bold>. Fish were treated with nine neuroactive drugs:</p>
<list list-type="bullet">
<list-item><p>Ketanserin (serotonin 5-HT2 antagonist, 8.5 µM)</p></list-item>
<list-item><p>Quipazine (serotonin 5-HT2<italic>/</italic>3 agonist, 25 µM)</p></list-item>
<list-item><p>Trazodone (serotonin antagonist and reuptake inhibitor, 25 µM)</p></list-item>
<list-item><p>Quinpirole (selective D<sub>2</sub>/D<sub>3</sub> receptor agonist, 25 µM)</p></list-item>
<list-item><p>Clozapine (atypical antipsychotic, 8.5 µM)</p></list-item>
<list-item><p>Fluoxetine (selective serotonin reuptake inhibitor (SSRI), 8.5 µM)</p></list-item>
<list-item><p>Haloperidol (D<sub>2</sub> receptor antagonist, 8.5 µM)</p></list-item>
<list-item><p>Apomorphine (non-selective dopamine agonist, 25 µM)</p></list-item>
<list-item><p>Valproic acid (GABAergic voltage-gated sodium channel blocker, 25 µM)</p></list-item>
</list>
<p>Stock solutions were prepared by dissolving the compounds in autoclaved Milli-Q water. For drugs with low water solubility, an initial solution in 10% (v/v) DMSO was prepared, ensuring that the final DMSO concentration in the stock did not exceed 0.3%, a threshold below which no behavioral changes have been reported. Drug stocks were stored at -20<sup><italic>°</italic></sup>C after confirming stability via UV-Vis spectroscopy.</p>
<p>To prepare the arena solution, the appropriate stock volume was diluted in 600 mM Tris-buffered E3 medium to a final volume of 5 mL. All arena solutions had a final pH of 7 ± 0.3. The concentration for each drug was set to half of the maximum non-lethal dose. Larvae were allowed to habituate to the arena and drug solution for 1 hour before data collection. Each drug and concentration was tested on 18 fish.</p>
<sec id="s5b2c1">
<title>Dru. Classification Analysis</title>
<p>We applied the Mini-Rocket classifier to three different time-series: binary movement indicators, locomotion speed, and a multivariate one-hot encoded time series of movement categories obtained from Megabouts. We employed a 10-fold stratified cross-validation. Each dataset was transformed using MiniRocketMultivariate with 5000 kernels. The transformed data was then classified using a ridge classifier.</p>
</sec>
</sec>
</sec>
<sec id="s5b3">
<label>2.C.</label>
<title>Zebrabox experiments</title>
<sec id="s5b3a">
<label>2.C.1.</label>
<title>Setup</title>
<p>We used a commercial system for recording zebrafish (Zebrabox, Viewpoint, France). Each experiment lasted 20 minutes, and the light on/off stimulus sequence can be seen in <xref rid="figS7" ref-type="fig">Fig. S7</xref>. We recorded multi-well plates with 6, 12, 24, 48, and 96 wells. For each well size, we ran two experiments with different larvae. Each experiment produced a compressed movie with a resolution of 9 pixels per mm. This setup produces a good edge case, however, we encourage users to work with higher resolution or frame rate if possible. We found that the biggest limitation of the Zebrabox was the long exposure time resulting in very blurry images when the fish is swimming fast. The users should aim to increase the light intensity and decrease the exposure to get optimal results even at low frame rate.</p>
</sec>
<sec id="s5b3b">
<label>2.C.2.</label>
<title>Behavioral tracking</title>
<p>To track the video obtained from the Zebrabox system, we used SLEAP (<xref ref-type="bibr" rid="c13">Pereira et al., 2022</xref>). We tracked two keypoints: one placed between the fish’s eye and another on the swim bladder. Each multiwell video was cropped into several single-well recordings. For each recording we computed the background using the mode of the pixel value distribution sampled along the recording. From the grayscale videos, we generated a composite color that showed the background and a background-substracted image in different color channels. Given the low-resolution, we found that using this composite color to underline the foreground was critical to help SLEAP deal with cases where the fish overlapped with the edge of the arena. SLEAP was trained using the top-down pipeline after labeling 1900 frames. Since each cropped video contained a single fish, we did not need to use cross-frame identity tracking.</p>
</sec>
</sec>
<sec id="s5b4">
<label>2.D.</label>
<title>Head-restrained experiments</title>
<sec id="s5b4a">
<label>2.D.1.</label>
<title>Setup</title>
<p>Experiments were conducted in a cylindrical acrylic arena carved and polished using a CNC machine (Núcleo de Oficinas, Instituto Superior Técnico, Portugal) The larvae were mounted on a flattened cone made from Sylgard (SYLGARD 184 Silicone Elastomer Kit, The Dow Chemical Company). Each larva was recorded using a high-speed camera (EoSensCL 1362, Mikrotron) fitted with an infrared long-pass filter (LP780-37, 780 nm, VisionLightTech) to block visible light and a lens (M30.5mm, Edmund Optics) to optimally focus on it. The camera was positioned above the arena, which was illuminated from below using a mounted high-power infrared LED (M780L2, Thorlabs) and controlled by a T-Cube LED Driver (LEDD1B, Thorlabs). Visual stimuli were displayed using a LED projector (ML500, Optoma). Three cold mirrors (one below, two laterally) were placed around the arena for 270° visual stimulation. Behavior was recorded at 700 fps. Larvae were embedded the evening prior to recording using low-melting point agarose (UltraPure LMP Agarose, Cat#16520100, Invitrogen by Life Technologie). Their tails were freed by removing agarose in a square caudal to the swim bladder to allow for maximum tail movement. The arena was filled with E3, and larvae were allowed to recover and habituate overnight at 28<sup><italic>°</italic></sup>C.</p>
</sec>
<sec id="s5b4b">
<label>2.D.2.</label>
<title>Tracking</title>
<p>The posture was computed in real-time using a custom-written C# software. The tail was tracked using the iterative arc-search described in <xref ref-type="sec" rid="s5c1">Supp. Note 3.A</xref>.</p>
</sec>
<sec id="s5b4c">
<label>2.D.3.</label>
<title>Stimuli</title>
<p>Forward gratings at a range of speeds (15 speeds, 0-32.8 mm/s) were displayed for 20 seconds in a randomised order, with 5 repetitions each. Gratings were stationary for 10 seconds between trials. The width of the stripes was adjusted to match the free-swimming stimuli (1 cm width at 5 mm distance). Grating speed was updated continuously using a closed-loop virtual reality system.</p>
</sec>
</sec>
</sec>
<sec id="s5c">
<title>Supplementary Note 3: Megabouts training dataset</title>
<sec id="s5c1">
<label>3.A.</label>
<title>Freely swimming</title>
<p>To train the Transformer neural network, we gathered 1.95 million swim bouts from 108 larvae from the experiments described in 2.1. We merged this dataset with the data from <xref ref-type="bibr" rid="c7">Marques et al., 2018</xref> leading to ∼4 million bouts. A last reference dataset consisted of prey capture events containing a total of 1700 short capture swims and 3200 long capture swims. All reference datasets were acquired at 700 fps but we used slightly different tracking methods in addition to different area sizes and depths. This variability was critical to ensure the classifier’s ability to tackle out of sample datasets.</p>
<p>We applied the segmentation and classification MATLAB code from <xref ref-type="bibr" rid="c7">Marques et al., 2018</xref> to get the reference bout category. This dataset was split into 80-20% train and test for training the classifier.</p>
<p>For some categories, we applied hierarchical clustering to yield subcategories labels:</p>
<list list-type="bullet">
<list-item><p>the subcategory for double O-bend was based on <xref ref-type="bibr" rid="c15">Randlett et al., 2019</xref>.</p></list-item>
<list-item><p>Burst Swim were split since we found different speed tuning within the subcategories (see <xref rid="figS6" ref-type="fig">Fig. S6</xref>).</p></list-item>
<list-item><p>Short-latency C-start were split based on differences for visual or acoustic stimuli (see <xref rid="figS6" ref-type="fig">Fig. S6</xref>).</p></list-item>
<list-item><p>The Long capture swims were split according to the sign of the turning tail beat.</p></list-item>
</list>
<p>Overall, the subcategories lead to a small but significant increase in the mutual information (a statistical measure of dependency between two variables) between subcategories and stimuli than between categories and stimuli (mean = 0.1076 nats vs mean=0.1025 nats, p=4e-9, Wilcoxon signed-rank test, N=108).</p>
</sec>
<sec id="s5c2">
<label>3.B.</label>
<title>Head-restrained</title>
<p>We collected a dataset from 25 fish performing OMR and Spontaneous Swimming. From this, we extracted 370 5-second epochs containing a variety of tail movements for learning the sparse-coding dictionary.</p>
</sec>
</sec>
<sec id="s5d">
<title>Supplementary Note 4: Megabouts freely swimming pipeline</title>
<sec id="s5d1">
<label>4.A.</label>
<title>Data inputs format</title>
<p>All input positions should be provided in millimeters (mm) and angular values in radians. For tail tracking, at least 4 keypoints from the swim bladder to the tail tip are required. The tail posture will be interpolated to 11 equidistant points, thus giving 10 angular values for it. For trajectory tracking, at least two keypoints are necessary to estimate the larva’s position and head direction. In low spatial resolution conditions, we found that tracking two keypoints could be reliably done using SLEAP to estimate the head orientation (see <xref ref-type="sec" rid="s5b3">Supp. Note 2.C</xref>).</p>
</sec>
<sec id="s5d2">
<label>4.B.</label>
<title>Smoothing</title>
<p>An option for interpolating the data is available, but it is only applied when a few short segments (∼10 ms) are missing from a high-frequency recording. The head trajectory is smoothed using a 1-euro filter, since it can outperform the Kalman filter while being very simple to adjust on a new dataset <xref ref-type="bibr" rid="sc3">(Casiez et al., 2012)</xref>. An option is provided to denoise the tail angle using a principal component analysis bottleneck and keeping only 4 PCs similarly to <xref ref-type="bibr" rid="sc10">Mearns et al., 2020</xref>. The tail angle is also smoothed using the Savitzky-Golay filter.</p>
<p>All default parameters expressed in units of time are scaled according to the frame rate. For instance, the window length of the Savitzky-Golay is 15 milliseconds for a second-order polynomial. The corresponding number of frames is computed according to the recording frame rate. This ensures that the parameter tuning is minimal.</p>
</sec>
<sec id="s5d3">
<label>4.C.</label>
<title>Segmentation</title>
<sec id="s5d3a">
<label>4.C.1.</label>
<title>Segmentation from tail angle</title>
<p>Zebrafish larvae swim in a burst-and-glide pattern with discrete bouts lasting typically between 100 and 200 ms. To identify tail bouts, we compute the speed of the tail. Since first-order numerical differentiation typically introduces noise, we instead used smooth noise-robust differentiators (<xref ref-type="bibr" rid="sc7">Holoborodko, 2008</xref>) that are optimized to produce a smooth estimation of the derivative of a numerical signal. We compute the derivative of the angles along the tail, cumulate the rectified value of the speed and apply a low pass filter. The bouts can then be identified by applying a threshold on the tail angle. See results in <xref rid="figS3" ref-type="fig">Fig. S3i</xref>.</p>
</sec>
<sec id="s5d3b">
<label>4.C.2.</label>
<title>Segmentation from the trajectory</title>
<p>We computed the kinematic activity or trajectory speed using angular and positional speed (estimated using <xref ref-type="bibr" rid="sc7">Holoborodko, 2008</xref>). This measure has the property of peaking near the first tail beat. A peak-finding algorithm is employed to find the approximate location of the first tail beat. The onset and offset are then computed by thresholding at a given percentage relative to the peak (typically 20%). See results in <xref rid="figS3" ref-type="fig">Supplementary Fig. S3ii,iii</xref>.</p>
</sec>
</sec>
<sec id="s5d4">
<label>4.D.</label>
<title>Swim bout characterization</title>
<p>Following the segmentation, the swim bout trajectory will be centered and aligned according to the fish position and direction at the onset (see <xref rid="figS4" ref-type="fig">Fig. S4a</xref>). This will put all swims in a common reference frame. The tail angle input to the transformer is limited to the first 7 out of 10 values since we found no improvement using the segments closer to the tip of the tail, where tracking errors are more common. A tensor containing all swim bouts from a given experiment is fed to the Transformer neural network, which outputs the probability distributions over bout categories and subcategories. Additionally, the network predicts the location and direction of the first tail beat, enabling the calculation of latency to stimulus onset and providing directional information about the swim bout.</p>
</sec>
<sec id="s5d5">
<label>4.E.</label>
<title>Transformer Neural Network</title>
<sec id="s5d5a">
<label>4.E.1.</label>
<title>Data Augmentation</title>
<p>The training dataset described in Supp. Note 3.A consists of tail and trajectory at 700 fps. During training, we used the raw recording values since the downsampling value obtained from smoothing at 700 fps could lead to an artificially high signal-to-noise ratio. Random Gaussian noise was added to 10% of the sample. The head angle was also flipped by 180<sup><italic>°</italic></sup> for random frames to enhance robustness against tracking error. The swim bouts were shifted in time to yield robustness to a difference in onset detection. Down sampling was applied to yield an effective frame rate between 10 and 700 fps (see <xref rid="figS4" ref-type="fig">Fig. S4c</xref>). We also randomly masked the tail information in 50% of the cases to get accurate classification from head trajectory only (see <xref rid="figS4" ref-type="fig">Fig. S4c</xref>). The trajectory was also removed in 5% of the training examples.</p>
</sec>
<sec id="s5d5b">
<label>4.E.2.</label>
<title>Network Architecture</title>
<p>The network architecture is illustrated in <xref rid="figS4" ref-type="fig">Fig. S4b</xref>. The maximum number of tokens was 140, corresponding to 200 ms at 700 fps. The positional encoding was computed as follows:
<disp-formula id="ueqn1">
<graphic xlink:href="613078v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where:</p>
<list list-type="bullet">
<list-item><p>PE<sub><italic>i,k</italic></sub> represents the positional encoding matrix, where <italic>i</italic> is the position in the sequence, and <italic>k</italic> ∈ {0, 1, 2, …, 63} is the embedding index.</p></list-item>
<list-item><p><italic>d</italic> = 64 is the embedding dimension for tail bouts and position.</p></list-item>
<list-item><p><italic>t</italic><sub><italic>i</italic></sub> is the timestamp relative to the onset of movement of the corresponding camera frame in milliseconds.</p></list-item>
</list>
<p>A learned classification token was used at position 0, leading to 141 input tokens. A learned mask was used to fill in missing data for tokens where either tail or trajectory was missing. We used a multi-head self-attention with 8 heads and 3 layers. The output of the network was linearly projected to obtain logits for the subcategories. Let <inline-formula><inline-graphic xlink:href="613078v3_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denote the logits for subcategory <italic>s</italic> and sample <italic>n</italic>, and let <inline-formula><inline-graphic xlink:href="613078v3_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> represent the logits for bout category <italic>c</italic> and sample <italic>n</italic>. The logits for the bout categories were derived from the subcategory logits using the following formula:
<disp-formula id="ueqn2">
<graphic xlink:href="613078v3_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where:</p>
<list list-type="bullet">
<list-item><p><italic>n</italic> indexes the samples in the batch.</p></list-item>
<list-item><p><italic>s</italic> indexes the subcategories.</p></list-item>
<list-item><p><italic>c</italic> indexes the bout categories.</p></list-item>
<list-item><p><italic>𝒮</italic> <sub><italic>c</italic></sub> is the set of subcategories that belong to bout category <italic>c</italic>.</p></list-item>
</list>
<p>Here, <inline-formula><inline-graphic xlink:href="613078v3_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> represents the logit for bout category <italic>c</italic> for sample <italic>n</italic>, and the summation is performed over the subcategory indices <italic>s</italic> that belong to bout category <italic>c</italic>.</p>
<p>Another linear projection from the output classification token was used to predict the direction of the first half-beat. A sigmoid activation function was applied to predict the first tail beat location relative to the first frame.</p>
<p>The network was implemented in PyTorch.</p>
</sec>
<sec id="s5d5c">
<label>4.E.3.</label>
<title>Network Training</title>
<p>Random data augmentation was applied at the beginning of each epoch. For each batch, the input data were balanced across categories. The loss was defined as the weighted sum of the cross entropy value for the bout category, subcategory and sign and the mean squared error for the first half beat location. The weights were computed to give roughly similar value to the four loss terms at the network initialization. We used an Adam optimizer with a learning rate of 1e-4, a batch size of 50 swim bouts, and a dropout rate of 0.1. It ran until convergence on the training set.</p>
</sec>
</sec>
<sec id="s5d6">
<label>4.F.</label>
<title>Pipeline evalutation</title>
<sec id="s5d6a">
<label>4.F.1.</label>
<title>Segmentation evaluation</title>
<p><xref rid="figS3" ref-type="fig">Fig. S3c</xref> shows the accuracy of segmentation based on trajectory compared with segmentation based on tail speed. This metric quantifies how often the onset of movements detected by each segmentation method occurs within a specified time window of each other. To calculate this index, we started by identifying the time window within which we consider two movement onsets to be matching (here, we used 75ms, which corresponds to one and a half samples at 20fps). Then, we examined each onset from the first segmentation method to see if there is at least one onset from the second method occurring within this window, and we did the same in reverse. The coincidence index is then determined by counting these matches and comparing the count to the total number of movement onsets detected by both methods. This index provides a straightforward way to assess the degree of synchrony between two segmentation methods, with higher values indicating greater alignment in detecting movement onsets.</p>
</sec>
<sec id="s5d6b">
<label>4.F.2.</label>
<title>Transformer evaluation</title>
<p>To evaluate the classifier’s performance, we masked and downsampled swim bouts in the test set to simulate different frame rate and tracking configurations. To ensure the performance was evaluated with imperfect bout alignment, bouts were segmented with random shift, yielding the first tail beat to be positioned with equal probability between -14ms and +70ms relative to the first token. This distribution was chosen to roughly match the support of the distribution of segmentation error at 20 fps (see <xref rid="figS3" ref-type="fig">Fig. S3b</xref>).</p>
<p>Since the distribution of bouts strongly depends on the sensory context, we computed the balanced accuracy of bout category and sub-category to evaluate the network performance at different fps and with and without tail tracking (<xref rid="fig1" ref-type="fig">Fig. 1c</xref> and <xref rid="figS5" ref-type="fig">Fig. S5b</xref>). We found that the balanced accuracy for ‘tail + trajectory tracking’ at 700 fps was 89%. It should be noted that we should not expect 100% accuracy. Indeed, the bout categories are defined according to the unsupervised clustering of <xref ref-type="bibr" rid="c7">Marques et al., 2018</xref>. This classification relies on the estimation of a large number of kinematic parameters extracted for each tail beat within a bout, these measures are very sensitive to shift compared with our strategy. Moreover, the unsupervised clustering of swim bouts corresponds to density peaks in the space of swim bouts parameters (<xref ref-type="bibr" rid="sc8">Marques and Orger, 2019</xref>) rather than on isolated clusters. Thus, the continuity in kinematic space makes it difficult to separate similar categories, such as Approach Swim and Slow1, as can be seen from the confusion matrix in <xref rid="figS5" ref-type="fig">Fig. S5a</xref>.</p>
<p>A critical metric for ensuring the quality of the classification is to check that the bout category is informative about the fish’s sensory experience. As reported in the main text, the mutual information was higher using the category from Megabouts than with the MATLAB version (mean = 0.1025 nats vs mean=0.0934 nats, p=4e-17, Wilcoxon signed-rank test, N=108). Along similar lines, another criteria for assessing movement clustering is that the information should be predictive of future actions (<xref ref-type="bibr" rid="sc1">Berman et al., 2016</xref>). We found that the categories from Megabouts were more predictive of future movements during spontaneous activity (for future movement, we used the category defined according to the MATLAB reference code). (mean = 0.1209 nats vs mean=0.1197 nats, p=3e-6, Wilcoxon signed-rank test, N=108). We hypothesized that this improvement results from the augmentation strategy leading to a more robust estimation of bout categories.</p>
<p>In the Zebrabox assay we are missing a high-speed reference evaluation of the bout categories. To overcome this drawback, we used the fact that fish generate O-Bends with high probability when the light is switched off (<xref ref-type="bibr" rid="c2">Burgess and Granato, 2007</xref>; <xref ref-type="bibr" rid="c15">Randlett et al., 2019</xref>). Indeed, we saw that for all the well sizes, a peak in O-Bend probability is present after the light is switched off (see <xref rid="figS7" ref-type="fig">Fig. S7</xref>).</p>
</sec>
</sec>
</sec>
<sec id="s5e">
<title>Supplementary Note 5: Megabouts head-restrained pipeline</title>
<sec id="s5e1">
<label>5.A.</label>
<title>Preprocessing and detrending</title>
<p>The first preprocessing steps are similar to <xref ref-type="sec" rid="s5d2">Supp. Note 4.B</xref>. The baseline removal is an additional step for pre-processing head-restrained tracking. In head-restrained conditions, tail angles commonly exhibit a slow varying baseline. This low-frequency component is first subtracted before running the sparse coding algorithm. The algorithm used to fit the baseline is obtained from the python library pybaselines (Erb). This slow baseline could be relevant and has been associated with activity in the nucleus of the medial longitudinal fascicle (nMLF) (<xref ref-type="bibr" rid="sc5">Dal Maschio et al., 2017</xref>).</p>
</sec>
<sec id="s5e2">
<label>5.B.</label>
<title>Sparse coding</title>
<p>Convolutional sparse coding is an unsupervised method that decomposes a continuous image or multidimensional time series into the sparse contributions of motifs. The ensemble of motifs forms a dictionary that is learned from the data to reconstruct the original signal from a convolution of motifs with a sparse code (see <xref rid="figS8" ref-type="fig">Fig. S8</xref>). The specific algorithm we used was designed to recover the individual keystrokes from piano auditory recordings (<xref ref-type="bibr" rid="c24">Wohlberg, 2017</xref>; <xref ref-type="bibr" rid="c3">Cogliati et al., 2017</xref>). We collected data on the tail angle of head-restrained zebrafish and used the algorithm to learn a dictionary of 4 motifs: slow forward movements, fast forward movements, turns, and escape/struggle-like movements. We found that although the duration of tail movements could exceed 1s in head-fixed conditions, the duration of motifs did not exceed 200ms, suggesting that the basic time scale is similar to freely swimming bouts. From the sparse code, the tail angle time series can be decomposed into the contributions of the four motifs.</p>
</sec>
<sec id="s5e3">
<label>5.C.</label>
<title>Sparse coding dictionary learning</title>
<p>The sparse deconvolution was computed using the <italic>ConvBPDN</italic> algorithm from SPORCO. The dictionary learning used the <italic>ConvBPDNDictLearnConsensus</italic> method. To have dictionary motifs with matching first half beat locations we employed a learning schedule. For each iteration, we increased the sparsity and shifted the learned motif so that their first half beat would be localized at the beginning. After 10 iterations the algorithm converged.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="sc1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berman</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Shaevitz</surname>, <given-names>J. W.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Predictability and hierarchy in drosophila behavior</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>42</issue>):<fpage>11943</fpage>–<lpage>11948</lpage>.</mixed-citation></ref>
<ref id="sc2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burgess</surname>, <given-names>H. A.</given-names></string-name> and <string-name><surname>Granato</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Sensorimotor gating in larval zebrafish</article-title>. <source>Journal of Neuroscience</source>, <volume>27</volume>(<issue>18</issue>):<fpage>4984</fpage>–<lpage>4994</lpage>.</mixed-citation></ref>
<ref id="sc3"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Casiez</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Roussel</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Vogel</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>1 C filter: a simple speed-based low-pass filter for noisy input in interactive systems</article-title>. In <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name>, pages <fpage>2527</fpage>–<lpage>2530</lpage>, (<year>2012</year>).</mixed-citation></ref>
<ref id="sc4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cogliati</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Duan</surname>, <given-names>Z.</given-names></string-name>, and <string-name><surname>Wohlberg</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Piano transcription with convolutional sparse lateral inhibition</article-title>. <source>IEEE Signal Processing Letters</source>, <volume>24</volume>(<issue>4</issue>):<fpage>392</fpage>–<lpage>396</lpage>.</mixed-citation></ref>
<ref id="sc5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dal Maschio</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Donovan</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Helmbrecht</surname>, <given-names>T. O.</given-names></string-name>, and <string-name><surname>Baier</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Linking neurons to network function and behavior by two-photon holographic optogenetics and volumetric imaging</article-title>. <source>Neuron</source>, <volume>94</volume>(<issue>4</issue>):<fpage>774</fpage>–<lpage>789</lpage>.</mixed-citation></ref>
<ref id="sc6"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Erb</surname>, <given-names>D.</given-names></string-name></person-group> <source>pybaselines: A Python library of algorithms for the baseline correction of experimental data</source>. URL <ext-link ext-link-type="uri" xlink:href="https://github.com/derb12/pybaselines">https://github.com/derb12/pybaselines</ext-link>. <year>no date</year></mixed-citation></ref>
<ref id="sc7"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Holoborodko</surname>, <given-names>P.</given-names></string-name></person-group> <source>Smooth noise robust differentiators</source>, (<year>2008</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.holoborodko.com/pavel/numerical-methods/numerical-derivative/smooth-low-noise-differentiators/">http://www.holoborodko.com/pavel/numerical-methods/numerical-derivative/smooth-low-noise-differentiators/</ext-link>.</mixed-citation></ref>
<ref id="sc8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marques</surname>, <given-names>J. C.</given-names></string-name> and <string-name><surname>Orger</surname>, <given-names>M. B.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Clusterdv: a simple density-based clustering method that is robust, general and automatic</article-title>. <source>Bioinformatics</source>, <volume>35</volume>(<issue>12</issue>):<fpage>2125</fpage>–<lpage>2132</lpage>.</mixed-citation></ref>
<ref id="sc9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marques</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Lackner</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Félix</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Orger</surname>, <given-names>M. B.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Structure of the zebrafish locomotor repertoire revealed with unsupervised behavioral clustering</article-title>. <source>Current Biology</source>, <volume>28</volume>(<issue>2</issue>):<fpage>181</fpage>–<lpage>195</lpage>.</mixed-citation></ref>
<ref id="sc10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mearns</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Donovan</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Fernandes</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Semmelhack</surname>, <given-names>J. L.</given-names></string-name>, and <string-name><surname>Baier</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Deconstructing hunting behavior reveals a tightly coupled stimulus-response loop</article-title>. <source>Current Biology</source>, <volume>30</volume>(<issue>1</issue>):<fpage>54</fpage>–<lpage>69</lpage>.</mixed-citation></ref>
<ref id="sc11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Tabris</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Matsliah</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ravindranath</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Papadoyannis</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Normand</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Deutsch</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z. Y.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2022</year>). <article-title>Sleap: A deep learning system for multi-animal pose tracking</article-title>. <source>Nature methods</source>, <volume>19</volume>(<issue>4</issue>):<fpage>486</fpage>–<lpage>495</lpage>.</mixed-citation></ref>
<ref id="sc12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Randlett</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Haesemeyer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Forkin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Shoenhard</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Schier</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Engert</surname>, <given-names>F.</given-names></string-name>, and <string-name><surname>Granato</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Distributed plasticity drives visual habituation learning in larval zebrafish</article-title>. <source>Current Biology</source>, <volume>29</volume>(<issue>8</issue>):<fpage>1337</fpage>–<lpage>1345</lpage>.</mixed-citation></ref>
<ref id="sc13"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Wohlberg</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Sporco: A python package for standard and convolutional sparse representations</article-title>. In <conf-name>SciPy</conf-name>, pages <fpage>1</fpage>–<lpage>8</lpage>, (<year>2017</year>).</mixed-citation></ref>
</ref-list>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107859.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Berman</surname>
<given-names>Gordon J</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3588-7820</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution>
</institution-wrap>
<city>Atlanta</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Useful</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study introduces Megabouts, a transformer-based classifier for larval zebrafish movement bouts. This <bold>useful</bold> tool is thoughtfully implemented and has clear potential to unify analyses across labs. However, the evidence supporting its robustness is <bold>incomplete</bold>. How the method generalizes across datasets, how sensitive it is to noise, and the specific sources of misclassification are unclear. The method would also be strengthened by providing options for users to fine-tune the clusters under different experimental conditions, which would further enhance reliability and flexibility.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107859.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Jouary et al. present Megabouts, a Transformer-based classifier and Python toolbox for automated categorization of zebrafish movement bouts into 13 bout types. This is potentially a very useful tool for the zebrafish community. It is broadly applicable to a wide variety of behavioral paradigms and could help to unify behavioral quantification across labs. The overall implementation is technically sound and thoughtfully engineered. The choice of standard Transformer architecture is well-justified (e.g., it can handle long-term tracking data and process missing data, integrates posture and trajectory information over time, and shows robustness to variable frame rates and partial occlusion). The data augmentation strategies (e.g., downsampling, tail masking, and temporal jitter) are well designed to enhance cross-condition generalization. Thus, I very much support this work.</p>
<p>For the benefit of the end users of this tool, several clarifications and additional analyses would be helpful:</p>
<p>(1) What is the source and nature of the classification errors? The reported accuracy is &lt;80% with trajectory data and still &lt;90% with trajectory + tail data.</p>
<p>(1a) Is this due to model failure (is overfitting a concern? How unbiased were the test sets?), imperfections of the preprocessing step (how sensitive is this to noise in the input data?), or underlying ambiguity in the biological data (e.g., do some &quot;errors&quot; reflect intermediate patterns that don't map neatly onto the 13 discrete classes)?</p>
<p>(1b) A systematic error analysis would be helpful. Which classes are most often confused? Are errors systematic (e.g., slow swims vs. routine turns) or random?</p>
<p>(1c) Can confidence of classification be provided for each bout in the data? How would the authors recommend that the end user deal with misclassifications (e.g., by manual correction)?</p>
<p>
Overall, the end user would benefit greatly from more information on potential failure modes and their root causes.</p>
<p>(2) How well does the trained network generalize across labs and setups? To what extent have the authors tested this on datasets from other labs to determine how well the pretrained model transfers across datasets? Having tested the code provided by the authors on a short stretch of x-y zebrafish trajectory data obtained independently, the pipeline generates phantom movement annotations. The underlying cause is unclear.</p>
<p>(2a) One possibility is that preprocessing steps may be highly sensitive to slight noise in the x-y positional data, which leads to noise in the speed data. The neural net, in turn, classifies noise into movement annotations. It would be helpful if the authors could add Gaussian noise to the x-y trajectory data and then determine the extent to which the computational pipeline is robust to noise.</p>
<p>(2b) When testing the pipeline, some stationary periods are classified as movements. Which step of the pipeline gave rise to the issue is unclear. Thus, explicit cross-lab validation and robustness tests (e.g., adding Gaussian noise to trajectories) would strengthen the claims of this paper.</p>
<p>(2c) Lastly, given the potential issue of generalization across labs, it would be helpful to provide/outline the steps for users in different labs to retrain and fine-tune the model.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107859.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Overall, the manuscript is well organized and clearly written. However, in this reviewer's opinion, the manuscript suffers from multiple major weaknesses.</p>
<p>Strengths:</p>
<p>The strengths of the paper are unclear; they have not been articulated well by the authors.</p>
<p>Weaknesses:</p>
<p>The pipeline is designed to analyze larval zebrafish behaviors, which by definition is considered a highly specialized, if not niche, application. Hence, the scope of this manuscript is extremely narrow, and consequently, the overall significance and the broader impact on the field of behavioral neuroscience are rather low. Broadening the scope would significantly improve the manuscript's impact. Second, it was noted that the authors neglect to present an unbiased discussion of how their pipeline compares to well-established and time-proven pipelines used to track larval zebrafish behaviors. This reviewer also failed to detect any new biological insights presented or improvements compared to existing methods, further questioning the overall significance and impact of this manuscript. Finally, the core claim of the manuscript lacks meaningful experimental data that would allow an unbiased and more definitive evaluation of the claims made regarding the Megabouts pipeline. The critical experiment to achieve this would be to run an identical set of behavioral assays (e.g., PPI, social behaviors) on different platforms (e.g., a commercial and a non-commercial one) and then determine if Megabouts correctly analyzes and integrates the results. While this might sound to the authors like an 'outside the scope' experiment, this reviewer would argue that it is the only meaningful experiment to validate the central claim put forward in this manuscript.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107859.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this manuscript, the authors introduce Megabouts, a software package designed to standardize the analysis of larval zebrafish locomotion, through clustering the 2D posture time series into canonical behavioral categories. Beyond a first, straightforward segmentation that separates glides from powered movements, Megabouts uses a Transformer neural network to classify the powered movements (bouts). This Transformer network is trained with supervised examples. The authors apply their approach to improve the quantification of sensorimotor transformations and enhance the sensitivity of drug-induced phenotype screening. Megabouts also includes a separate pipeline that employs convolutional sparse coding to analyze the less predictable tail movements in head-restrained fish.</p>
<p>I presume that the software works as the authors intend, and I appreciate the focus on quantitative behavior. My primary concerns reflect an implicit oversimplification of animal behavior. Megabouts is ultimately a clustering technique, categorizing powered locomotion into distinct, labelled states which, while effective for analysis, may confuse the continuous and fluid nature of animal behavior. Certainly, Megabouts could potentially miss or misclassify complex, non-stereotypical movements that do not fit the defined categories. In fact, it appears that exactly this situation led the authors to design a new clustering for head-restrained fish. Can we anticipate even more designs for other behavioral conditions?</p>
<p>Ultimately, I am not yet convinced that Megabouts provides a justifiable picture of behavioral control. And if there was a continuous &quot;control knob&quot;, which seems very likely, wouldn't that confuse the clustering process, as many distinct clusters would correspond to, say, different amplitudes of the same control knob?</p>
<p>There has been tremendous recent progress in the measurement and analysis of animal behavior, including both continuous and discrete perspectives. However, the supervised clustering approach described here feels like a throwback to an earlier era. Yes, it's more automatic and quantifiable, and the amount of data is fantastic. But ultimately, the method is conceptually bound to the human eye in conditions where we are already familiar.</p>
</body>
</sub-article>
</article>