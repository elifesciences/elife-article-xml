<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">98725</article-id>
<article-id pub-id-type="doi">10.7554/eLife.98725</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98725.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Functional Connectivity-based Attractor Dynamics in Rest, Task, and Disease</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Englert</surname>
<given-names>Robert</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kincses</surname>
<given-names>Balint</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4604-3367</contrib-id>
<name>
<surname>Kotikalapudi</surname>
<given-names>Raviteja</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gallitto</surname>
<given-names>Giuseppe</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Jialin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hoffschlag</surname>
<given-names>Kevin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7423-5422</contrib-id>
<name>
<surname>Woo</surname>
<given-names>Choong-Wan</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wager</surname>
<given-names>Tor D</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Timmann</surname>
<given-names>Dagmar</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bingel</surname>
<given-names>Ulrike</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2942-0821</contrib-id>
<name>
<surname>Spisak</surname>
<given-names>Tamas</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>tamas.spisak@uk-essen.de</email>
</contrib>
    <aff id="a1"><label>1</label><institution>Center for Translational Neuro- and Behavioral Sciences (C-TNBS), University Medicine Essen</institution>, <city>Essen</city>, <country country="DE">Germany</country></aff>
    <aff id="a2"><label>2</label><institution>Department of Diagnostic and Interventional Radiology and Neuroradiology, University Medicine Essen</institution>, <city>Essen</city>, <country country="DE">Germany</country></aff>
    <aff id="a3"><label>3</label><institution>Department of Neurology, University Medicine Essen</institution>, <city>Essen</city>, <country country="DE">Germany</country></aff>
    <aff id="a4"><label>4</label><institution>Max Planck School of Cognition</institution>, <city>Leipzig</city>, <country country="DE">Germany</country></aff>
    <aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00y0zf565</institution-id><institution>Center for Neuroscience Imaging Research, Institute for Basic Science</institution></institution-wrap>, <city>Suwon</city>, <country country="KR">Republic of Korea</country></aff>
    <aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04q78tk20</institution-id><institution>Department of Biomedical Engineering, Sungkyunkwan University</institution></institution-wrap>, <city>Suwon</city>, <country country="KR">Republic of Korea</country></aff>
    <aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/049s0rh22</institution-id><institution>Department of Psychological and Brain Sciences, Dartmouth College</institution></institution-wrap>, <city>Hanover</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Petkoski</surname>
<given-names>Spase</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/019kqby73</institution-id><institution>Institut de Neurosciences des Systèmes</institution>
</institution-wrap>
<city>Marseille</city>
<country country="FR">France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country country="NL">Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-09-05">
<day>05</day>
<month>09</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-12-24">
<day>24</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP98725</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-28">
<day>28</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-11">
<day>11</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.03.565516"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-09-05">
<day>05</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98725.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.98725.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98725.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98725.1.sa0">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.98725.1.sa3">Author Response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Englert et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Englert et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-98725-v2.pdf"/>
<abstract>
<p>Functional brain connectivity has been instrumental in uncovering the large-scale organization of the brain and its relation to various behavioral and clinical phenotypes. Understanding how this functional architecture relates to the brain’s dynamic activity repertoire is an essential next step towards interpretable generative models of brain function. We propose functional connectivity-based Attractor Neural Networks (fcANNs), a theoretically inspired model of macro-scale brain dynamics, simulating recurrent activity flow among brain regions based on first principles of self-organization. In the fcANN framework, brain dynamics are understood in relation to attractor states; neurobiologically meaningful activity configurations that minimize the free energy of the system. We provide the first evidence that large-scale brain attractors - as reconstructed by fcANNs - exhibit an approximately orthogonal organization, which is a signature of the self-orthogonalization mechanism of the underlying theoretical framework of free-energy-minimizing attractor networks. Analyses of 7 distinct datasets demonstrate that fcANNs can accurately reconstruct and predict brain dynamics under a wide range of conditions, including resting and task states, and brain disorders. By establishing a formal link between connectivity and activity, fcANNs offer a simple and interpretable computational alternative to conventional descriptive analyses.</p>
</abstract>
<abstract abstract-type="summary">
<title>Key Points</title>
<list list-type="bullet">
<list-item><p>We present a simple yet powerful generative computational model for large-scale brain dynamics</p></list-item><list-item><p>Based on the theory of artificial attractor neural networks emerging from first principles of self-organization</p></list-item>
<list-item><p>Model dynamics accurately reconstruct several characteristics of resting-state brain dynamics and confirm theoretical predictions of emergent attractor self-orthogonalization</p></list-item>
<list-item><p>Our model captures both task-induced and pathological changes in brain activity</p></list-item>
<list-item><p>fcANNs offer a simple and interpretable computational alternative to conventional descriptive analyses of brain function</p></list-item>
<list-item><p>Project website (with interactive manuscript): <ext-link ext-link-type="uri" xlink:href="https://pni-lab.github.io/connattractor">https://pni-lab.github.io/connattractor</ext-link></p></list-item>
</list>
</abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft (DFG)</institution>
</institution-wrap>
</funding-source>
<award-id>422744262</award-id>
<principal-award-recipient>
<name>
<surname>Spisak</surname>
<given-names>Tamas</given-names>
</name>
</principal-award-recipient>
    <principal-award-recipient>
        <name>
            <surname>Bingel</surname>
            <given-names>Ulrike</given-names>
        </name>
    </principal-award-recipient>
</award-group>
<award-group id="par-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft (DFG)</institution>
</institution-wrap>
</funding-source>
<award-id>316803389</award-id>
<principal-award-recipient>
<name>
<surname>Bingel</surname>
<given-names>Ulrike</given-names>
</name>
</principal-award-recipient>
    <principal-award-recipient>
        <name>
            <surname>Spisak</surname>
            <given-names>Tamas</given-names>
        </name>
    </principal-award-recipient>
</award-group>
<award-group id="par-5">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003716</institution-id>
<institution>Korea Basic Science Institute (KBSI)</institution>
</institution-wrap>
</funding-source>
<award-id>IBS-R015-D1</award-id>
<principal-award-recipient>
<name>
<surname>Woo</surname>
<given-names>Choong-Wan</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We introduce formal links between our analyses and a new theoretical framework of self-orthogonalizing attractor networks (Spisak &amp; Friston, 2025), providing first evidence for &quot;self-orthogonalization&quot; in the brain, opening several new avenues in terms of interpretation and shedding light on the computational principles underlying brain attractor dynamics.
This revision is part of eLife's revision process, revising the follwonig reviewed preprint:
https://elifesciences.org/reviewed-preprints/98725
References
Spisak T &amp; Friston K (2025). Self-orthogonalizing attractor neural networks emerging from the free energy principle. arXiv preprint arXiv:2505.22749.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Brain function is characterized by the continuous activation and deactivation of anatomically distributed neuronal populations (<xref ref-type="bibr" rid="c5">Buzsaki, 2006</xref>). Irrespective of the presence or absence of explicit stimuli, brain regions appear to work in concert, giving rise to rich and spatiotemporally complex fluctuations (<xref ref-type="bibr" rid="c3">Bassett &amp; Sporns, 2017</xref>). These fluctuations are not random (<xref ref-type="bibr" rid="c44">Liu &amp; Duyn, 2013</xref>; <xref ref-type="bibr" rid="c71">Zalesky <italic>et al</italic>., 2014</xref>); they organize around large-scale gradients (<xref ref-type="bibr" rid="c46">Margulies <italic>et al</italic>., 2016</xref>; <xref ref-type="bibr" rid="c36">Huntenburg <italic>et al</italic>., 2018</xref>) and exhibit quasi-periodic properties, with a limited number of recurring patterns often termed as “brain substates” (<xref ref-type="bibr" rid="c27">Greene <italic>et al</italic>., 2023</xref>; <xref ref-type="bibr" rid="c43">Kringelbach &amp; Deco, 2020</xref>; <xref ref-type="bibr" rid="c67">Vidaurre <italic>et al</italic>., 2017</xref>; <xref ref-type="bibr" rid="c44">Liu &amp; Duyn, 2013</xref>). A wide variety of descriptive techniques have been previously employed to characterize whole-brain dynamics (<xref ref-type="bibr" rid="c64">Smith <italic>et al</italic>., 2012</xref>; <xref ref-type="bibr" rid="c67">Vidaurre <italic>et al</italic>., 2017</xref>; <xref ref-type="bibr" rid="c44">Liu &amp; Duyn, 2013</xref>; <xref ref-type="bibr" rid="c8">Chen <italic>et al</italic>., 2018</xref>). These efforts have provided accumulating evidence not only for the existence of dynamic brain substates but also for their clinical significance (<xref ref-type="bibr" rid="c37">Hutchison <italic>et al</italic>., 2013</xref>; <xref ref-type="bibr" rid="c2">Barttfeld <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="c49">Meer <italic>et al</italic>., 2020</xref>). However, the underlying driving forces remain elusive due to the descriptive nature of such studies.</p>
<p>Conventional computational approaches attempt to solve this puzzle by going all the way down to the biophysical properties of single neurons, and aim to construct a model of larger neural populations, or even the entire brain (<xref ref-type="bibr" rid="c4">Breakspear, 2017</xref>). These approaches have shown numerous successful applications (<xref ref-type="bibr" rid="c52">Murray <italic>et al</italic>., 2018</xref>; <xref ref-type="bibr" rid="c42">Kriegeskorte &amp; Douglas, 2018</xref>; <xref ref-type="bibr" rid="c32">Heinz <italic>et al</italic>., 2019</xref>). However, such models need to estimate a vast number of neurobiologically motivated free parameters to fit the data. This hampers their ability to effectively bridge the gap between explanations at the level of single neurons and the complexity of behavior (<xref ref-type="bibr" rid="c4">Breakspear, 2017</xref>). Recent efforts using coarse-grained brain network models (<xref ref-type="bibr" rid="c61">Schirner <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="c60">Schiff <italic>et al</italic>., 1994</xref>; <xref ref-type="bibr" rid="c54">Papadopoulos <italic>et al</italic>., 2017</xref>; <xref ref-type="bibr" rid="c62">Seguin <italic>et al</italic>., 2023</xref>) and linear network control theory (<xref ref-type="bibr" rid="c9">Chiêm <italic>et al</italic>., 2021</xref>; <xref ref-type="bibr" rid="c59">Scheid <italic>et al</italic>., 2021</xref>; <xref ref-type="bibr" rid="c28">Gu <italic>et al</italic>., 2015</xref>) opted to trade biophysical fidelity to phenomenological validity. Such models have provided insights into some of the inherent key characteristics of the brain as a dynamic system; for instance, the importance of stable patterns, the <italic>attractor states</italic>, in governing brain dynamics (<xref ref-type="bibr" rid="c15">Deco <italic>et al</italic>., 2012</xref>; <xref ref-type="bibr" rid="c26">Golos <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="c30">Hansen <italic>et al</italic>., 2015</xref>). While attractor networks become established models of micro-scale canonical brain circuits in the last four decades (<xref ref-type="bibr" rid="c40">Khona &amp; Fiete, 2022</xref>), these studies suggest that attractor dynamics are essential characteristics of macro-scale brain dynamics as well (<xref ref-type="bibr" rid="c56">Poerio &amp; Karapanagiotidis, 2025</xref>). Attractor networks, however, come in many flavors and the specific forms and behaviors of these networks are heavily influenced by the chosen inference and learning rules, making it unclear which variety should be in focus when modeling brain dynamics. Given that the brain showcases not only multiple signatures of attractor dynamics but also the ability to evolve and adapt through self-organization (i.e., in the absence of any centralized control), investigating attractor models from the point of view of self-organization may be key to narrow down the set of viable models.</p>
<p>In our recent theoretical work (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>), we identified the class of attractor networks that emerge from first principles of self-organization - as articulated by the Free Energy Principle (FEP) (<xref ref-type="bibr" rid="c21">Friston, 2010</xref>; <xref ref-type="bibr" rid="c22">Friston <italic>et al</italic>., 2023</xref>) - and identified the emergent inference and learning rules guiding the dynamics of such systems. This theoretical framework reveals that the minimization of variational free energy locally - e.g., by individual network nodes - gives rise to a dual dynamic: simultaneous inference (updating activity) and learning (optimizing connectivity). The emergent inference process in these systems is equivalent to local Bayesian update dynamics for the individual network nodes, homologous to the stochastic relaxation observed in conventional Boltzmann neural network architectures (e.g. stochastic Hopfield networks, (<xref ref-type="bibr" rid="c34">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="c41">Koiran, 1994</xref>)), and in line with the empirical observation that activity in the brain “flows” following similar dynamics (<xref ref-type="bibr" rid="c12">Cole <italic>et al</italic>., 2016</xref>; <xref ref-type="bibr" rid="c58">Sanchez-Romero <italic>et al</italic>., 2023</xref>; <xref ref-type="bibr" rid="c11">Cole, 2024</xref>). Importantly, in this framework, attractor states are not simply an epiphenomenon of collective dynamics, but serve as global priors in the Bayesian sense, that get combined with the current activity configuration so that the updated activity samples from the posterior (akin to a Markov-Chain Monte Carlo (MCMC) sampling process).</p>
<p>Learning, on the other hand, emerges in this framework in the form of a distinctive coupling plasticity - a local, incremental learning rule - that continuously adjusts coupling weights to preserve low free energy in anticipation of future sensory encounters following a contrastive predictive coding scheme (<xref ref-type="bibr" rid="c51">Millidge <italic>et al</italic>., 2022</xref>), effectively implementing action selection in the active inference sense (<xref ref-type="bibr" rid="c23">Friston <italic>et al</italic>., 2016</xref>). Importantly, the learning dynamics emerging in our theoretical framework provide a strong, testable hypothesis: if the brain operates as a free energy minimizing attractor network, its large-scale attractors should be approximately orthogonal to each other. This is not a general property of all recurrent (attractor) neural networks, but a direct consequence of free energy minimization, shown both mathematically and with simulations in (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>). As our theoretical framework - by design - embraces multiple valid levels of description (through coarse-graining), it is well-suited to serve as a basis for a computational model of large-scale brain dynamics.</p>
<p>In the present work, we translate the results of this novel theoretical framework into a computational model of macro-scale brain dynamics and deploy a diverse set of experimental, clinical, and meta-analytic studies to perform an initial investigation of several of its predictions. We start by showing that - if large-scale brain dynamics evolve and organize according to the emergent rules of this framework - the corresponding attractor model can be effectively approximated from functional connectivity data, as measured with resting-state fMRI. Based on the network topology spanned by functional connectivity, our model assigns a free energy level for any arbitrary activation pattern and determines a “trajectory of least action” towards one of a finite number of attractor states that minimize this energy (<xref rid="fig1" ref-type="fig">Figure 1</xref>). We then perform an initial investigation of the robustness and biological plausibility of the attractor states of the reconstructed network and whether it is able to reproduce various characteristics of resting-state brain dynamics. Importantly, we directly test the framework’s prediction on the emergence of (approximately) orthogonal attractor states. Capitalizing on the generative nature of our model, we also demonstrate how it can capture - and potentially explain - the effects of various perturbations and alterations of these dynamics, from task-induced activity to changes related to brain disorders.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Functional connectivity-based attractor neural networks as models of macro-scale brain dynamics.</title>
<p><bold>A</bold> Free-energy-minimizing artificial neural networks (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>) are a form of recurrent stochastic artificial neural networks that, similarly to classical Hopfield networks (<xref ref-type="bibr" rid="c34">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="c41">Koiran, 1994</xref>), can serve as content-addressable (“associative”) memory systems. More generally, through the learning rule emerging from local free-energy minimization, the weights of these networks will encode a global internal model of the external world. The priors of this internal generative model are represented by the attractor states of the network that - as a special consequence of free-energy minimization - will tend to be orthogonal to each other. During stochastic inference (local free-energy minimization), the network samples from the posterior that combines these priors with the previous brain substates (also encompassing incoming stimuli), akin to Markov chain Monte Carlo (MCMC) sampling. <bold>B</bold> In accordance with this theoretical framework, we consider regions of the brain as nodes of a free-energy-minimizing artificial neural network. Instead of initializing the network with the structural wiring of the brain or training it to solve specific tasks, we set its weights empirically, using information about the interregional “activity flow” across regions, as estimated via functional brain connectivity. Applying the inference rule of our framework - which displays strong analogies with the relaxation rule of Hopfield networks and the activity flow principle that links activity to connectivity in brain networks - results in a generative computational model of macro-scale brain dynamics, that we term a functional connectivity-based (stochastic) attractor neural network (fcANN). <bold>C</bold> The proposed computational framework assigns a free energy level, a probability density and a “trajectory of least action” towards an attractor state to any brain activation pattern and predicts changes of the corresponding dynamics in response to alterations in activity and/or connectivity. The theoretical framework underlying the fcANNs - based on the assumption that the brain operates as a free energy minimizing attractor network - draws formal links between attractor dynamics and multi-level Bayesian active inference.</p></caption>
<graphic xlink:href="565516v4_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2">
<title>Theoretical background</title>
<sec id="s2a">
<title>Free energy minimizing artificial neural networks</title>
<p>The computational model at the heart of this work is a direct implementation of the inference dynamics that emerge from a recently proposed theoretical framework of free energy minimizing (self-orthogonalizing) attractor networks (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>) (FEP-ANNs). FEP-ANNs are a class of attractor neural networks that emerge from the Free Energy Principle (FEP) (<xref ref-type="bibr" rid="c21">Friston, 2010</xref>; <xref ref-type="bibr" rid="c22">Friston <italic>et al</italic>., 2023</xref>). The FEP posits that any self-organizing random dynamical system that maintains its integrity over time (i.e. has a steady-state distribution and a statistical separation from its environment) must act in a way that minimizes its variational free energy. FEP-ANNs apply the FEP recursively. We assume a network of N units, where each unit is represented by a single continuous-valued state variable σ<sub>i</sub> ∈ [−1,1], so that the activity of the network is described by a vector of states σ = (σ<sub>1</sub>, σ<sub>2</sub>, …, σ<sub>N</sub>) and these states are conditionally independent of each other, given boundary states that realize the necessary statistical separation between them (corresponding to a complex Markov blanket structure in the FEP terminology). When assuming σ<sub>i</sub> states that follow a continuous Bernoulli (a.k.a. truncated exponential: <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="565516v4_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> distribution (parameterized by the single parameter κ<sub>i</sub>) and deterministic couplings J, the steady-state distribution can be expressed as:
<disp-formula id="eqn1">
<graphic xlink:href="565516v4_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where b<sub>i</sub> represents the local evidence or bias for each unit i (e.g. external input or intrinsic excitability of a brain region), <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="565516v4_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the symmetric component of the coupling weights between units i and j, and β is an inverse temperature or precision parameter. Note that while this steady-state distribution has the same functional form as continuous-state Boltzmann machines or stochastic Hopfield networks, the true coupling weights J do not have to be symmetric as usually assumed in those architectures. Asymmetric couplings break detailed balance, meaning that p<sup>∗</sup> is no longer an equilibrium distribution. However, the antisymmetric component <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="565516v4_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> does not contribute to the steady-state distribution p<sup>∗</sup> as it only induces circulating (solenoidal) flow in the state space which is tangential to the level sets of p<sup>∗</sup>. Thus, while the overall framework can describe general attractor networks with asymmetric couplings and non-equilibrium steady states (NESS), it also implies that knowing only the symmetric component of the coupling weights is sufficient to reconstruct the steady-state distribution p<sup>∗</sup> of the underlying system. This is a highly useful property for the purposes of the present study, where the couplings are reconstructed from resting state fMRI data, without any explicit information about the directionality of functional connections. For a detailed derivation of the steady state distribution, see (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>) and Supplementary Information 1.</p>
<p>Knowing the steady-state distribution of a free-energy-minimizing attractor network, we can derive two types of emergent dynamics from the single imperative of free energy minimization: inference and learning.</p>
<sec id="s2a1">
<title>Inference: stochastic relaxation dynamics</title>
<p>Inference arises from minimizing free energy with respect to the states σ. For a single unit, this yields a local update rule homologous to the relaxation dynamics in Hopfield networks:
<disp-formula id="eqn2">
<graphic xlink:href="565516v4_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where L is a sigmoidal activation function (a Langevin function in our case). This rule dictates that each unit updates its activity stochastically, based on a weighted sum of the activity of other units, plus its own intrinsic bias. See (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>) and Supplementary Information 2 for a detailed derivation of the inference dynamics.</p>
<p>Note that the rule is expressed in terms of the expected value of the state σ<sub>i</sub>, which is a stochastic quantity. However, in the limiting case of symmetric couplings (which is the case throughout the present study) and least-action dynamics (i.e. no noise), this update rule reduces to the classical relaxation dynamics of (continuous-state) Hopfield networks. In the present study, we use both the deterministic (“least action”) and stochastic variants of the inference rule. The former identifies attractor states; the latter serves as a generative model for large-scale, multistable brain dynamics.</p>
<p>In the present study, we make the simplifying assumption that all nodes have zero bias (b = 0). Furthermore, we allow investigating different scaling factors for the J couplings matrix (given the uncertainties around the magnitude of association in the functional connectome) by introducing a “scaling factor” β. This leads to the following update rule:
<disp-formula id="eqn3">
<graphic xlink:href="565516v4_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The scaling factor β is analogous to the inverse temperature parameter known in Hopfield networks and Boltzmann machines.</p>
<p>In the basis framework (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>), inference is a gradient descent on the variational free energy landscape with respect to the states σ and can be interpreted as a form of approximate Bayesian inference, where the expected value of the state σ<sub>i</sub> is interpreted as the posterior mean given the attractor states currently encoded in the network (serving as a macro-scale prior) and the previous state, including external inputs (serving as likelihood in the Bayesian sense). The stochastic update, therefore, is equivalent to a Markov chain Monte Carlo (MCMC) sampling from this posterior distribution. The inverse temperature parameter β, in this regard, can be interpreted as the precision of the prior encoded in J. This is easy to conceptualize by considering the limiting case of infinite precision, where the system simplifies to a binary-state Hopfield network (β → ∞, L(β u<sub>i</sub>) → sign(u<sub>i</sub>) on [−1,1]) that directly and deterministically converges to the (infinite-precision) prior, completely overriding the Bayesian likelihood (i.e., network input).</p>
</sec>
</sec>
<sec id="s2b">
<title>Free energy minimizing attractor networks as a model of large-scale brain dynamics</title>
<p>Taken together, the novel framework of free energy minimizing attractor networks not only motivates the use of a specific, emergent class of attractor networks as models for large-scale brain dynamics, but also provides a formal connection between these dynamics and Bayesian inference. The present study leverages this theoretical foundation. We aim to model large-scale brain dynamics as a free energy minimizing attractor network. According to our framework, such networks can be reconstructed from the activation time-series data measured in their nodes. Specifically, the weight matrix of the attractor network can be reconstructed as the negative inverse covariance matrix of the regional activation time series: J = −Λ = −Σ<sup>−1</sup>, where Σ is the covariance matrix of the activation time series in all regions, and Λ is the precision matrix. For a detailed derivation, see Supplementary Information 4. Note that this approach can naturally be reduced to different “coarse-grainings” of the system, by pooling network nodes with similar functional properties. In the case of resting-state fMRI data, this corresponds to pooling network nodes into functional parcels. Drawing upon concepts such as the center manifold theorem (<xref ref-type="bibr" rid="c69">Wagner, 1989</xref>), it is posited that rapid, fine-grained dynamics at lower descriptive levels converge to lower-dimensional manifolds, upon which the system evolves via slower processes at coarser scales. It has been previously argued (<xref ref-type="bibr" rid="c48">Medrano <italic>et al</italic>., 2024</xref>) that the temporal and spatial scales of fMRI data happen to align relatively well with the characteristic scales corresponding to meaningful large-scale “coarse-grainings” of brain dynamics.</p>
<p>Thus, we can reconstruct FEP-ANNs from functional connectivity data simply by considering the functional connectome (inverse covariance or partial correlation) as the coupling weights between the nodes of the network, which themselves correspond to brain regions (as defined by the chosen functional brain parcellation). We refer to such network models as functional connectivity-based attractor neural networks - <bold>fcANN</bold>s for short.</p>
<p>Having estimates of the weight matrix J of the attractor network, we can now rely on the deterministic and stochastic versions of the inference procedure (<xref ref-type="disp-formula" rid="eqn3">eq. (3)</xref>) in order to investigate this system. Running the deterministic update to a uniformly drawn sample of initial states, we can identify all attractor states of the network. The stochastic update, on the other hand, can be used to sample from the posterior distribution of the activity states, and thus serves as a generative computational model of the brain dynamics.</p>
</sec>
<sec id="s2c">
<title>Testable predictions of the theoretical framework</title>
<sec id="s2c1">
<title>Self-orthogonalization as a signature of free energy attractor networks</title>
<p>So far, we have only discussed free energy minimization in terms of the activity of the nodes of the network. However, free energy minimization also gives rise to a specific learning rule for the couplings J of the network. This learning rule is a specific local, incremental, contrastive (predictive coding-based) plasticity rule to adjust connection strengths:
<disp-formula id="eqn4">
<graphic xlink:href="565516v4_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
A detailed derivation of the learning dynamics can be found in (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>) and Supplementary Information 2. In the present work, we do not implement this learning rule in our computational model, as the coupling weights J are reconstructed directly from the empirical fMRI activation time series data.</p>
<p>However, this specific learning rule has an important implication for the attractor states of the FEP-ANN: it will naturally drive them towards (approximate) orthogonality during learning. For a mathematical motivation of the mechanisms underlying this important property, termed self-orthogonalization, see (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>) and Supplementary Information 3. Self-orthogonalization is far from being a generic property of all attractor networks (and it is also not a consequence of the above formulated inference dynamics). It has, however, remarkable implications for the computational efficiency of the network and the robustness of its representations. Attractor networks with orthogonal attractor states, often termed the Kanter-Sompolinsky projector neural network (<xref ref-type="bibr" rid="c39">Kanter &amp; Sompolinsky, 1987</xref>), are the computationally most efficient varieties of general attractor networks, with maximal memory capacity and perfect memory recall (without error). Importantly, in Kanter-Sompolinsky projector networks, <bold>the eigenvectors of the coupling matrix and the attractors become equivalent</bold>, providing an important signature for detecting such networks in empirical data.</p>
    <p>Importantly, in the present study we reconstruct attractor networks from functional connectivity data (fcANNs) without relying on the learning rule of the FEP-ANN framework (<xref ref-type="disp-formula" rid="eqn4">eq. (4)</xref>) which imposes orthogonality on the attractors. Thus, if, in these empirically reconstructed fcANNs, an alignment between the eigenvectors of the coupling matrix and the attractors is observed, it can be considered strong evidence that the system approximates a Kanter–Sompolinsky projector network. As FEP-ANNs - together with some other, related models (e.g., “dreaming neural networks” (<xref ref-type="bibr" rid="c35">Hopfield <italic>et al</italic>., 1983</xref>; <xref ref-type="bibr" rid="c55">Plakhov, 1994</xref>; <xref ref-type="bibr" rid="c18">Dotsenko &amp; Tirozzi, 1991</xref>; <xref ref-type="bibr" rid="c19">Fachechi <italic>et al</italic>., 2019</xref>)) - provide a plausible and mathematically rigorous mechanistic model for the emergence of architectures approximating Kanter–Sompolinsky projector networks through biologically plausible local learning rules, this alignment between the eigenvectors of the coupling matrix and the attractors can be considered a signature of an underlying FEP-ANN. We will directly test this prediction in the present study, by investigating the orthogonality of the attractor states of the fcANN model reconstructed from empirical fMRI data.</p>
</sec>
<sec id="s2c2">
<title>Convergence, multistability, biological plausibility and prediction capacity</title>
<p>Beyond (approximate) attractor orthogonality, our framework provides additional testable predictions. If the functional connectome can indeed be considered a proxy for the coupling weights J of an underlying attractor network, we can expect that (i) the reconstructed fcANN model will exhibit multiple stable attractor states, with large basins and biologically plausible spatial patterns, (ii) the relaxation dynamics of the reconstructed model will display fast convergence to attractor states, and (iii) the stochastic relaxation dynamics yield an efficient generative model of the empirical resting-state brain dynamics as well as perturbations thereof caused either by external inputs (stimulations and tasks) or pathologies.</p>
</sec>
</sec>
<sec id="s2d">
<title>Research questions</title>
<p>We have structured the present work around 7 research questions we address in the present study:</p>
<sec id="s2d1">
<title>Q1 - Is the brain an approximate K-S projector ANN (FEP-ANN prediction)?</title>
<p>We test whether fcANN-derived brain attractor states closely resemble the eigenvectors of the functional connectome matrix, in contrast to null models based on temporally phase-randomized time series data (preserving the frequency spectrum and the temporal autocorrelation of data, but destroying conditional dependencies across regions), denoted as <bold>NM1</bold>. Furthermore, in a supplementary analysis, we quantify the similarity of the functional connectome to the weights of an optimal Kanter–Sompolinsky (K-S) network with the same eigenvectors. The similarity (cosine similarity) is contrasted against repeating the same approach on permuted coupling matrices (but retaining symmetry, <bold>NM2</bold>)</p>
</sec>
<sec id="s2d2">
<title>Q2 - Is the functional connectome well suited to function as an attractor network?</title>
<p>We contrast the convergence properties of fcANN deterministic relaxation dynamics with null models with permuted coupling weights (preserving symmetry, sparsity and weight distributions, destroying topological structure) <italic>NM2</italic>.</p>
</sec>
<sec id="s2d3">
<title>Q3 - What are the optimal parameters for the fcANN model?</title>
<p>The number of attractor states is a function of the inverse temperature parameter β. For simplicity, we fix β = 0.04 (4 attractor states) in the current analysis. We perform a rough optimization of the noise parameter ϵ by benchmarking the fcANN’s ability to capture non-Gaussian conditional distributions in the data. This is benchmarked by computing a Wasserstein distance between the distributions of empirical and simulated data and contrasting it to the null model of a multivariate normal distribution with covariance matched to that of the empirical data (<bold>NM3</bold>, representing the case of Gaussian-only conditionals).</p>
</sec>
<sec id="s2d4">
<title>Q4 - Do fcANNs display biologically plausible attractor states?</title>
<p>We qualitatively demonstrate that attractor states obtained with different inverse temperature parameters β and different noise levels (ϵ) exhibit large basins and that these attractor states exhibit spatial patterns consistent with known large-scale brain systems.</p>
</sec>
<sec id="s2d5">
<title>Q5 - Can fcANNs reproduce the characteristics of resting-state brain activity?</title>
<p>We compare how well fcANN attractor states explain variance in unseen (in- and out-of-sample) empirical time series data, relative to the principal components of the empirical data itself. Statistical significance is evaluated via bootstrapping. Furthermore, we compare various characteristics (state occupancy, distribution, temporal trajectory) of the data generated by fcANNs via stochastic updates to empirical resting-state data. As null models, we use covariance-matched multivariate normal distributions (NM3)</p>
</sec>
<sec id="s2d6">
<title>Q6 - Can resting-state fMRI-based fcANNs predict large-scale brain dynamics elicited by tasks or stimuli?</title>
<p>We test whether fcANNs initialized from resting-state functional connectomes and perturbed with weak, condition-specific control signals predict task-evoked large-scale dynamics (pain vs. rest; up-vs. down-regulation). We compare simulated and empirical differences on the fcANN projection and flow fields during stochastic updates. As a null model, we use condition-label shuffling (NM5).</p>
</sec>
<sec id="s2d7">
<title>Q7 - Can resting-state fMRI-based fcANNs predict altered brain dynamics in clinical populations?</title>
<p>We test whether fcANNs initialized with group-level resting-state connectomes from autism spectrum disorder (ASD) patients and typically developing controls (TDC) predict observed group differences in dynamics (state occupancy, attractor-basin activations, flow fields). We compare fcANN-generated dynamics between ASD- and TDC-initialized models and evaluate similarity to empirical contrasts. As a null model, we use group-label shuffling (NM5).</p>
<p>For a summary of null modelling approaches and research questions, see <xref rid="tbl1" ref-type="table">Table 1</xref> and <xref rid="tbl2" ref-type="table">Table 2</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Null models applied in the present study</title></caption>
<graphic xlink:href="565516v4_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Research questions, methodological approaches, and the corresponding null models</title></caption>
<graphic xlink:href="565516v4_tbl2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Functional connectivity-based attractor networks (fcANNs) as a model of brain dynamics</title>
<p>First, we constructed a functional connectivity-based attractor network (fcANN) based on resting-state fMRI data in a sample of n=41 healthy young participants (study 1). Details are described in the Methods. In brief, we estimated interregional activity flow (<xref ref-type="bibr" rid="c12">Cole <italic>et al</italic>., 2016</xref>; <xref ref-type="bibr" rid="c38">Ito <italic>et al</italic>., 2017</xref>) as the study-level average of regularized partial correlations among the resting-state fMRI time series of m=122 functional parcels of the BASC brain atlas (see Methods for details). We then used the standardized functional connectome as the J<sub>i,</sub> weights of a fully connected recurrent fcANN model, see Methods).</p>
<p>Next, we applied the deterministic relaxation procedure to a large number of random initializations (n=100000) to obtain all possible attractor states of the fcANN in study 1 (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). Consistent with theoretical expectations, we observed that increasing the inverse temperature parameter β led to an increasing number of attractor states (<xref rid="fig2" ref-type="fig">Figure 2E</xref>, left, Supplementary Figure 5), appearing in symmetric pairs (i.e. <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="565516v4_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, see <xref rid="fig2" ref-type="fig">Figure 2G</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Attractor states and state-space dynamics of connectome-based Hopfield networks</title>
<p><bold>A</bold> Numbers under each pair report Pearson correlation and two-sided p-values based on 1,000 surrogate data realizations, generated by phase-randomizing the true time series and recomputing the connectivity matrix. For the comprehensive results of the eigenvector–attractor alignment analysis (including a supplementary analysis on weight similarity to the analogous Kanter–Sompolinsky projector network) see Supplementary Figure 1. <bold>B</bold> Example matches from a single permutation of the permutation-based null distribution. For each symmetry-preserving permutation of J, we recomputed the corresponding eigenvectors and attractors and re-matched them. The maps are visibly mismatched and correlations are near zero, illustrating the null against which the empirical correlations in panel A are evaluated. <bold>C</bold> Left panel: Free-energy-minimizing attractor networks have been shown to establish approximately orthogonal attractor states (right), even when presented with correlated patterns (left, adapted from <xref ref-type="bibr" rid="c65">Spisak &amp; Friston (2025)</xref>). fcANN analysis reveals that the brain also exhibits approximately orthogonal attractors. On all three polar plots, pairwise angles between attractor states are shown. Angles concentrating around 90° in the empirical fcANN are consistent with predictions of free-energy-minimizing (Kanter–Sompolinsky-like) networks. (Note, however, that in high-dimensional spaces, random vectors would also tend to be approximately orthogonal.) <bold>D</bold> The fcANN of study 1 seeded with real activation maps (gray dots) of an example participant. All activation maps converge to one of the four attractor states during the deterministic relaxation procedure (without noise) and the system reaches equilibrium. Trajectories are colored by attractor state. <bold>E</bold> Illustration of the stochastic relaxation procedure in the same fcANN model, seeded from a single starting point (activation pattern). With stochastic relaxation, the system no longer converges to an attractor state, but instead traverses the state space in a way restricted by the topology of the connectome and the “gravitational pull” of the attractor states. The shade of the trajectory changes with increasing number of iterations. The trajectory is smoothed with a moving average over 10 iterations for visualization purposes. <bold>F</bold> Real resting state fMRI data of an example participant from study 1, plotted on the fcANN projection. The shade of the trajectory changes with an increasing number of iterations. The trajectory is smoothed with a moving average over 10 iterations for visualization purposes. <bold>G</bold> Consistent with theoretical expectations, we observed that increasing the inverse temperature parameter β led to an increasing number of attractor states, emerging in a nested fashion (i.e. the basin of a new attractor state is fully contained within the basin of a previous one). When contrasting the functional connectome-based ANN with a null model based on symmetry-retaining permuted variations of the connectome (NM2), we found that the topology of the original (unpermuted) functional brain connectome makes it significantly better suited to function as an attractor network than the permuted null model. Table contains the median number of iterations until convergence for the original and permuted connectomes for different temperature parameters β and the p-value derived from a one-sided Wilcoxon signed-rank test (i.e. a non-parametric paired test) comparing the iteration values for each random null instance (1,000 pairs) to the iteration number observed with the original matrix and the same random input; with the null hypothesis that the empirical connectome converges in fewer iterations than the permuted connectome. <bold>H</bold> We optimized the noise parameter ϵ of the stochastic relaxation procedure for 8 different ϵ values over a logarithmic range between ϵ = 0.1 and 1 and contrasted the similarity (Wasserstein distance) between the 122-dimensional distribution of the empirical and the fcANN-generated data against null data generated from a covariance-matched multivariate normal distribution (1000 surrogates). We found that the fcANN reached multistability with ϵ &gt; 0.19 and provided the most accurate reconstruction of the real data with ϵ = 0.37, as compared with its accuracy in retaining the null data, suggesting that the fcANN model is capable of capturing non-Gaussian conditionals in the data. Glass’s Delta quantifies the distance from the null mean, expressed in units of null standard deviation.</p></caption>
<graphic xlink:href="565516v4_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To test research question <bold>Q1</bold>, we matched the eigenvectors of the coupling matrix to the attractor state with which they exhibit the highest correlation. We compared eigenvector–attractor correlations with a null model based on phase-randomized surrogate time-series data (NM1). We found that the eigenvectors of the coupling matrix and the attractor states are significantly more strongly aligned (as measured with Pearson’s correlation coefficient) than those in the null model (two-sided empirical permutation test; 1,000 permutations; correlations and p-values for the first six eigenvector–attractor pairs are reported in <xref rid="fig2" ref-type="fig">Figure 2A</xref>), providing evidence that large-scale brain organization approximates a Kanter–Sompolinsky projector network architecture (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). Eigenvectors with the highest eigenvalues tended to be aligned with the attractor states with the highest fractional occupancy (ratio of time spent on their basins during simulations with stochastic relaxation; see <xref rid="fig2" ref-type="fig">Figure 2F</xref>). No such pattern was observed in the null model (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Further evidence for the functional connectome’s close resemblance to a Kanter–Sompolinsky projector network is provided by the orthogonality of the attractor states to each other (<xref rid="fig2" ref-type="fig">Figure 2C</xref>) and additional analyses reported in Supplementary Figure 1.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><title>The top ten largest changes in average attractor-state activity between autistic and control individuals.</title>
<p>Mean attractor-state activity changes are presented in the order of their absolute effect size. Reported effect sizes are mean attractor activation differences. Note that activation time series were standard scaled independently for each region, so effect size can be interpreted as showing the differences as a proportion of regional variability. All p-values are based on permutation tests (shuffling the group assignment) and corrected for multiple comparisons (via Bonferroni’s correction). For a comprehensive list of significant findings, see Supplementary Figure 11.</p></caption>
<graphic xlink:href="565516v4_tbl3.tif" mime-subtype="tiff" mimetype="image"/>
<graphic xlink:href="565516v4_tbl3a.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>Next, to support the visualization of further analyses, we constructed a simplified, 2-dimensional visual representation of fcANN dynamics, which we apply throughout the remaining manuscript as a high-level visual summary. This 2-dimensional visualization, referred to as the <italic>fcANN</italic>projection, is based on the first two principal components (PCs) of the states sampled from the stochastic relaxation procedure (<xref rid="fig2" ref-type="fig">Figure 2D–F</xref> and Supplementary Figure 2). On this simplified visualization, we observed a clear separation of the attractor states (<xref rid="fig2" ref-type="fig">Figure 2D</xref>), with the two symmetric pairs of attractor states located at the extremes of the first and second PC. To map the attractors’ basins on the space spanned by the first two PCs (<xref rid="fig2" ref-type="fig">Figure 2C</xref>), we obtained the attractor state of each point visited during the stochastic relaxation and fit a multinomial logistic regression model to predict the attractor state from the first two PCs. The resulting model accurately predicted attractor states of arbitrary brain activity patterns, achieving a cross-validated accuracy of 96.5% (two-sided empirical permutation p&lt;0.001; 1,000 label permutations within folds). This allows us to visualize attractor basins on this 2-dimensional projection by delineating the decision boundaries obtained from this model (Supplementary Figure 2 as black lines in <xref rid="fig2" ref-type="fig">Figure 2G-H</xref>). In the rest of the manuscript, we use this 2-dimensional fcANN projection depicted on (<xref rid="fig2" ref-type="fig">Figure 2D-H</xref>) as a simplified visual representation of brain dynamics.</p>
<p>Panel D on <xref rid="fig2" ref-type="fig">Figure 2</xref> uses the fcANN projection to visualize the conventional Hopfield relaxation procedure. It depicts the trajectory of individual activation maps (sampled randomly from the time series data in study 1) until converging to one of the four attractor states. Panel E shows that the system does not converge to an attractor state with the stochastic relaxation procedure. The resulting path is still influenced by the attractor states’ “gravitational pull”, resulting in multistable dynamics that resemble the empirical time series data (example data on panel F).</p>
<p>In study 1, we investigated the convergence process of the fcANN (research question <bold>Q2</bold>) and contrasted it with a null model based on permuted variations of the connectome (while retaining the symmetry of the matrix, NM2). This null model preserves the sparseness and the degree distribution of the connectome, but destroys its topological structure (e.g. clusteredness). We found that the topology of the original (unpermuted) functional brain connectome makes it significantly better suited to function as an attractor network than the permuted null model. For instance, with β = 0.04, the median iteration number for the original and permuted fcANNs to reach convergence was 383 and 3543.5 iterations, respectively (<xref rid="fig2" ref-type="fig">Figure 2G</xref>, Supplementary Figure 6). Similar results were observed, independent of the inverse temperature parameter β. We set the temperature parameter for the rest of the paper to a value of β = 0.04, resulting in 4 distinct attractor states. The primary motivation for selecting β = 0.04 was to reduce the computational burden and the interpretational complexity for further analyses. However, as with increasing temperature attractor states emerge in a nested fashion, we expect that the results of the following analyses would be, although more detailed, qualitatively similar with higher β values.</p>
<p>Next, in line with research question <bold>Q3</bold>, we optimized the noise parameter ϵ of the stochastic relaxation procedure for 8 different ϵ values over a logarithmic range between ϵ = 0.1 and 1 and contrasted the similarity (Wasserstein distance) between the 122-dimensional distribution of the empirical and the fcANN-generated data against null data generated from a covariance-matched multivariate normal distribution (1000 surrogates). We found that the fcANN reached multistability with ϵ &gt; 0.19 and provided the most accurate reconstruction of the non-Gaussian conditional dependencies in the real data with ϵ = 0.37, as compared to its accuracy in retaining the covariance-matched multivariate Gaussian null data (NM3 <xref rid="fig2" ref-type="fig">Figure 2H</xref>; Wasserstein distance: 10.2, Glass’s Delta (distance from null mean, expressed in units of null standard deviation): -11.63, p&lt;0.001 one-sided). Based on this coarse optimization procedure, we set ϵ = 0.37 for all subsequent analyses.</p>
</sec>
<sec id="s3b">
<title>Reconstruction of resting state brain dynamics</title>
<p>Next, we visualized and qualitatively assessed the neuroscientific relevance of the spatial patterns of the obtained attractor states (<bold>Q4</bold>, <xref rid="fig3" ref-type="fig">Figure 3A</xref>), and found that they closely resemble previously described large-scale brain systems. The spatial patterns associated with first pair of attractors (mapped on PC1 on the 2-dimensional projection, horizontal axis, e.g. on <xref rid="fig2" ref-type="fig">Figure 2D-H</xref>) show a close correspondence to two commonly described complementary brain systems, that have been previously found in anatomical, functional, developmental, and evolutionary hierarchies, as well as gene expression, metabolism, and blood flow, (see <xref ref-type="bibr" rid="c66">Sydnor <italic>et al</italic>. (2021)</xref> for a review), and reported under various names, like intrinsic and extrinsic systems (<xref ref-type="bibr" rid="c25">Golland <italic>et al</italic>., 2008</xref>), Visual-Sensorimotor-Auditory and Parieto-Temporo-Frontal “rings” (<xref ref-type="bibr" rid="c10">Cioli <italic>et al</italic>., 2014</xref>), “primary” brain substates (<xref ref-type="bibr" rid="c8">Chen <italic>et al</italic>., 2018</xref>), unimodal-to-transmodal principal gradient (<xref ref-type="bibr" rid="c46">Margulies <italic>et al</italic>., 2016</xref>; <xref ref-type="bibr" rid="c36">Huntenburg <italic>et al</italic>., 2018</xref>) or sensorimotor-association axis (<xref ref-type="bibr" rid="c66">Sydnor <italic>et al</italic>., 2021</xref>). A common interpretation of these two patterns is that they represent (i) an “intrinsic” system for higher-level internal context, commonly referred to as the <italic>default mode network</italic>(<xref ref-type="bibr" rid="c57">Raichle <italic>et al</italic>., 2001</xref>) and (ii) an anti-correlated “extrinsic” system linked to the immediate sensory environment, showing similarities to the recently described “action mode network” (<xref ref-type="bibr" rid="c17">Dosenbach <italic>et al</italic>., 2025</xref>). The other pair of attractors - spanning an approximately orthogonal axis - resemble patterns commonly associated with perception–action cycles (<xref ref-type="bibr" rid="c24">Fuster, 2004</xref>), and described as a gradient across sensorimotor modalities (<xref ref-type="bibr" rid="c36">Huntenburg <italic>et al</italic>., 2018</xref>), recruiting regions associated with active (e.g. motor cortices) and perceptual inference (e.g., visual areas).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Connectome-based attractor networks reconstruct characteristics of real resting-state brain activity.</title>
<p><bold>A</bold> The four attractor states of the fcANN model from study 1 reflect brain activation patterns with high neuroscientific relevance, representing sub-systems previously associated with “internal context” (blue), “external context” (yellow), “action” (red) and “perception” (green) (<xref ref-type="bibr" rid="c25">Golland <italic>et al</italic>., 2008</xref>; <xref ref-type="bibr" rid="c10">Cioli <italic>et al</italic>., 2014</xref>; <xref ref-type="bibr" rid="c8">Chen <italic>et al</italic>., 2018</xref>; <xref ref-type="bibr" rid="c24">Fuster, 2004</xref>; <xref ref-type="bibr" rid="c46">Margulies <italic>et al</italic>., 2016</xref>; <xref ref-type="bibr" rid="c17">Dosenbach <italic>et al</italic>., 2025</xref>). <bold>B</bold> The attractor states show excellent replicability in two external datasets (study 2 and 3, overall mean correlation 0.93). <bold>C</bold> The first two PCs of the fcANN state space (the “fcANN projection”) explain significantly more variance (two-sided percentile bootstrap p&lt;0.0001 on Δ R<sup>2</sup>, 100 resamples) in the real resting-state fMRI data than principal components derived from the real resting-state data itself and generalizes better (two-sided percentile bootstrap p&lt;0.0001) to out-of-sample data (study 2). Error bars denote 99% percentile bootstrapped confidence intervals (100 resamples). <bold>D</bold> The fcANN analysis reliably predicts various characteristics of real resting-state fMRI data, such as the fraction of time spent on the basis of the four attractors (first column, p=0.007, contrasted to the multivariate normal null model NM3), the distribution of the data on the fcANN-projection (second column, p&lt;0.001, contrasted to the multivariate normal null model NM3) and the temporal autocorrelation structure of the real data (third column, p&lt;0.001, contrasted to a null model based on permuting time-frames). The latter analysis was based on flow maps of the mean trajectories (i.e. the characteristic timeframe-to-timeframe transition direction) in fcANN-generated data, as compared to a shuffled null model representing zero temporal autocorrelation. For more details, see Methods. Furthermore, we demonstrate that - in line with the theoretical expectations - fcANNs “leak” their weights during stochastic inference (rightmost column): the time series resulting from the stochastic relaxation procedure mirror the covariance structure of the functional connectome the fcANN model was initialized with. While the “self-reconstruction” property in itself does not strengthen the face validity of the approach (no unknown information is reconstructed), it is a strong indicator of the model’s construct validity; i.e. that systems that behave like the proposed model inevitably “leak” their weights into the activity time series.</p></caption>
<graphic xlink:href="565516v4_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The discovered attractor states demonstrate high replicability across the discovery dataset (study 1) and two independent replication datasets (study 2 and 3, <xref rid="fig3" ref-type="fig">Figure 3C</xref>; overall mean Pearson’s correlation 0.93, pooled across datasets and attractor states). In a supplementary analysis, we have also demonstrated the robustness of fcANNs to imperfect functional connectivity measures: fcANNs were found to be significantly more robust to noise added to the coupling matrix than nodal strength scores (used as a reference with the same dimensionality; see Supplementary Figure 10 for details).</p>
<p>Further analysis in study 1 showed that connectome-based attractor models accurately reconstructed multiple characteristics of true resting-state data (<bold>Q5</bold>). First, the two “axes” of the fcANN projection (corresponding to the first four attractors) accounted for a substantial amount of variance in the real resting-state fMRI data in study 1 (mean <italic>R</italic><sup>2</sup> = 0.399) and generalized well to out-of-sample data (study 2, mean <italic>R</italic><sup>2</sup> = 0.396) (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). The variance explained by the attractors significantly exceeded that of the first two PCs derived directly from the real resting-state fMRI data itself (<italic>R</italic><sup>2</sup> = 0.37 and 0.364 for in- and out-of-sample analyses). PCA - by identifying variance-heavy orthogonal directions - aims to explain the highest amount of variance possible in the data (with the assumption of Gaussian conditionals). While empirical attractors are closely aligned to the PCs (i.e. eigenvectors of the inverse covariance matrix), the alignment is only approximate. Here we quantified whether attractor states are a better fit to the unseen data than the PCs. Obviously, due to the otherwise strong PC–attractor correspondence, this is expected to be only a small improvement. However, this provides important evidence for the validity of our framework, as - together with our analysis addressing Q3 - it shows that attractors are not just a complementary, perhaps “noisier” variety of the PCs, but a “substrate” that generalizes better to unseen data than the PCs themselves.</p>
<p>Second, during stochastic relaxation, the fcANN model was found to spend approximately three-quarters of the time on the basins of the first two attractor states and one-quarter on the basins of the second pair of attractor states (approximately equally distributed between pairs). We observed similar temporal occupancies in the real data (<xref rid="fig3" ref-type="fig">Figure 3D</xref> left column), statistically significant against a covariance-matched multivariate Gaussian null model (NM3, 1,000 surrogates each; observed χ<sup>2</sup> = 21.57, p&lt;0.001; Glass Δ = -5.17; see Supplementary Figure 7 for details and for an alternative null model based on spatial phase-randomization). Fine-grained details of the distribution with bimodal appearance, observed in the real resting-state fMRI data were also convincingly reproduced by the fcANN model (<xref rid="fig3" ref-type="fig">Figure 3F</xref> and <xref rid="fig2" ref-type="fig">Figure 2D</xref>, second column).</p>
<p>Third, not only spatial activity patterns but also time series generated by the fcANN are similar to empirical time series data. Next to the visual similarity shown on <xref rid="fig2" ref-type="fig">Figure 2E</xref> and <xref rid="fig2" ref-type="fig">F</xref>, we observed a statistically significant similarity between the average trajectories of fcANN-generated and real time series “flow” (i.e. the characteristic timeframe-to-timeframe transition direction, Pearson’s r = 0.88, p&lt;0.001, Glass Δ = 4.41), as compared to null models of zero temporal autocorrelation (randomized timeframe order; two-sided empirical permutation test on Pearson’s r with 1,000 permutations; <xref rid="fig3" ref-type="fig">Figure 3D</xref>, third column; Methods).</p>
<p>Finally, we have demonstrated that - as expected from theory - fcANNs generate signal that preserves the covariance structure of the functional connectome they were initialized with, indicating that dynamic systems of this type (including the brain) inevitably “leak” their underlying structure into the activity time series, strengthening the construct validity of our approach (<xref rid="fig3" ref-type="fig">Figure 3D</xref>).</p>
</sec>
<sec id="s3c">
<title>An explanatory framework for task-based brain activity</title>
<p>Next to reproducing various characteristics of spontaneous brain dynamics, fcANNs can also be used to model responses to various perturbations (<bold>Q6</bold>). We obtained task-based fMRI data from a study by <xref ref-type="bibr" rid="c70">Woo <italic>et al</italic>. (2015)</xref> (study 4, n=33, see <xref rid="fig3" ref-type="fig">Figure 3</xref>), investigating the neural correlates of pain and its self-regulation.</p>
<p>We found that activity changes due to pain (taking into account hemodynamics, see Methods) were characterized on the fcANN projection by a shift toward the attractor state of action/execution (NM5: two-sided permutation test on the L2 norm of the mean projection difference; 1,000 within-participant label swaps; p&lt;0.001; Glass’s Δ = 4.34; <xref rid="fig4" ref-type="fig">Figure 4A</xref>, left). Energies, as defined by the fcANN, were also significantly different between the two conditions (NM5: two-sided permutation test on absolute energy difference; 1,000 label swaps; p&lt;0.001; Glass’s Δ = 3.14), with higher energies during pain stimulation.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Functional connectivity-based attractor networks reconstruct real task-based brain activity.</title>
<p><bold>A</bold> Functional MRI time-frames during pain stimulation from study 4 (second fcANN projection plot) and self-regulation (third and fourth) are distributed differently on the fcANN projection than brain substates during rest (first projection, permutation test, p&lt;0.001 for all). Energies, as defined by the Hopfield model, are also significantly different between rest and the pain conditions (permutation test, p&lt;0.001), with higher energies during pain stimulation. Triangles denote participant-level mean activations in the various blocks (corrected for hemodynamics). Small circle plots show the directions of the change for each individual (points) as well as the mean direction across participants (arrow), as compared to the reference state (downregulation for the last circle plot, rest for all other circle plots). <bold>B</bold> Flow-analysis (difference in the average timeframe-to-timeframe transition direction) reveals a nonlinear difference in brain dynamics during pain and rest (left). When introducing weak pain-related signal in the fcANN model during stochastic relaxation, it accurately reproduces these nonlinear flow differences (right). <bold>C</bold> Simulating activity in the Nucleus Accumbens (NAc) (the region showing significant activity differences in <xref ref-type="bibr" rid="c70">Woo <italic>et al</italic>. (2015)</xref>) reconstructs the observed nonlinear flow difference between up- and downregulation (left). <bold>D</bold> Schematic representation of brain dynamics during pain and its up- and downregulation, visualized on the fcANN projection. In the proposed framework, pain does not simply elicit a direct response in certain regions, but instead, shifts spontaneous brain dynamics towards the “action” attractor, converging to a characteristic “ghost attractor” of pain. Down-regulation by NAc activation exerts force towards the attractor of internal context, leading to the brain less frequent “visiting” pain-associated states. <bold>E</bold> Visualizing meta-analytic activation maps (see Supplementary Table 2 for details) on the fcANN projection captures intimate relations between the corresponding tasks and <bold>F</bold> serves as a basis for a fcANN-based theoretical interpretative framework for spontaneous and task-based brain dynamics. In the proposed framework, task-based activity is not a mere response to external stimuli in certain brain locations but a perturbation of the brain’s characteristic dynamic trajectories, constrained by the underlying functional connectivity. From this perspective, “activity maps” from conventional task-based fMRI analyses capture time-averaged differences in these whole brain dynamics.</p></caption>
<graphic xlink:href="565516v4_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>When participants were instructed to up- or downregulate their pain sensation (resulting in increased and decreased pain reports and differential brain activity in the nucleus accumbens, NAc (see <xref ref-type="bibr" rid="c70">Woo <italic>et al</italic>. (2015)</xref> for details), we observed further changes in the location of momentary brain activity patterns on the fcANN projection (two-sided permutation test on the L2 norm of the mean projection difference; 1,000 label swaps; p&lt;0.001; Glass’s Δ = 4.1; <xref rid="fig4" ref-type="fig">Figure 4A</xref>, right), with downregulation pulling brain dynamics toward the attractor state of internal context and perception. Interestingly, self-regulation did not trigger significant energy changes (two-sided permutation test on absolute energy difference; 1,000 label swaps; p=0.37; Glass’s Δ = 0.4).</p>
<p>Next, we conducted a “flow analysis” on the fcANN projection, quantifying how the average timeframe-to- timeframe transition direction differs on the fcANN projection between conditions (see Methods). This analysis unveiled that during pain (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, left side), brain activity tends to gravitate toward a distinct point on the projection on the boundary of the basins of the internal and action attractors, which we term the “ghost attractor” of pain (similar to <xref ref-type="bibr" rid="c68">Vohryzek <italic>et al</italic>. (2020)</xref>). In case of downregulation (as compared to upregulation), brain activity is pulled away from the pain-related “ghost attractor” (<xref rid="fig4" ref-type="fig">Figure 4C</xref>, left side), toward the attractor of internal context.</p>
<p>Our fcANN was able to accurately reconstruct these nonlinear dynamics by adding a small amount of realistic “control signal” (similarly to network control theory, see e.g. <xref ref-type="bibr" rid="c45">Liu <italic>et al</italic>. (2011)</xref> and <xref ref-type="bibr" rid="c28">Gu <italic>et al</italic>. (2015)</xref>). To simulate the alterations in brain dynamics during pain stimulation, we acquired a meta-analytic pain activation map (<xref ref-type="bibr" rid="c72">Zunhammer <italic>et al</italic>., 2021</xref>) (n=603) and incorporated it as a control signal added to each iteration of the stochastic relaxation procedure. The ghost attractor found in the empirical data was present across a relatively wide range of signal-to-noise (SNR) values (Supplementary Figure 8). Results with SNR=0.005 are presented in <xref rid="fig4" ref-type="fig">Figure 4B</xref>, right side (Pearson’s r = 0.46; two-sided permutation p=0.005 based on NM5: randomizing conditions on a perparticipant basis; 1,000 permutations; Glass’s Δ = 2.19).</p>
<p>The same model was also able to reconstruct the observed nonlinear differences in brain dynamics between the up-and downregulation conditions (Pearson’s r = 0.62; p=0.023 based on two-sided permutation test NM5: randomly shuffling conditions in a per-participant basis; 1,000 permutations; Glass’s Δ = 1.84) without any further optimization (SNR=0.005, <xref rid="fig4" ref-type="fig">Figure 4C</xref>, right side). The only change we made to the model was the addition (downregulation) or subtraction (upregulation) of control signal in the NAc (the region in which (<xref ref-type="bibr" rid="c70">Woo <italic>et al</italic>., 2015</xref>) observed significant changes between up- and downregulation), introducing a signal difference of ΔSNR=0.005 (the same value we found optimal in the pain-analysis). Results were reproducible with lower NAc SNRs, too (Supplementary Figure 9).</p>
<p>To provide a comprehensive picture on how tasks and stimuli other than pain map onto the fcANN projection, we obtained various task-based meta-analytic activation maps from Neurosynth (see Methods) and plotted them on the fcANN projection (<xref rid="fig4" ref-type="fig">Figure 4E</xref>). This analysis reinforced and extended our interpretation of the four investigated attractor states and shed more light on how various functions are mapped on the axes of internal vs. external context and perception vs. action. In the coordinate system of the fcANN projection, visual processing is labeled “external-perception”, sensory-motor processes fall into the “external-active” domain, language, verbal cognition and working memory belongs to the “internal-active” region and long-term memory as well as social and autobiographic schemata fall into the “internal-perception” regime (<xref rid="fig4" ref-type="fig">Figure 4F</xref>).</p>
</sec>
<sec id="s3d">
<title>Clinical relevance</title>
<p>To demonstrate fcANN models’ potential to capture altered brain dynamics in clinical populations (<bold>Q7</bold>), we obtained data from n=172 autism spectrum disorder (ASD) and typically developing control (TDC) individuals, acquired at the New York University Langone Medical Center, New York, NY, USA (NYU) and generously shared in the Autism Brain Imaging Data Exchange dataset (study 7: ABIDE, (<xref ref-type="bibr" rid="c16">Di Martino <italic>et al</italic>., 2014</xref>)). After excluding high-motion cases (with the same approach as in studies 1–4, see Methods), we visualized the distribution of time-frames on the fcANN-projection separately for the ASD and TDC groups (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). First, we assigned all timeframes to one of the 4 attractor states with the fcANN from study 1 and found several significant differences in the mean activity on the attractor basins (see Methods) of the ASD group as compared to the respective controls (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). Strongest differences were found on the “action-perception” axis (<xref rid="tbl3" ref-type="table">Table 3</xref>), with increased activity of the sensory-motor and middle cingular cortices during “action-execution” related states and increased visual and decreased sensory and auditory activity during “perception” states, likely reflecting the widely acknowledged, yet poorly understood, perceptual atypicalities in ASD (<xref ref-type="bibr" rid="c29">Hadad &amp; Schwartz, 2019</xref>). ASD related changes in the internal-external axis were characterized by more involvement of the posterior cingulate, the precuneus, the nucleus accumbens, the dorsolateral prefrontal cortex (dlPFC), the cerebellum (Crus II, lobule VII) and inferior temporal regions during activity of the internalizing subsystem (<xref rid="tbl3" ref-type="table">Table 3</xref>). While similar, default mode network (DMN)-related changes have often been attributed to an atypical integration of information about the “self” and the “other” (<xref ref-type="bibr" rid="c53">Padmanabhan <italic>et al</italic>., 2017</xref>), a more detailed fcANN-analysis may help to further disentangle the specific nature of these changes.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption>
<title>Connectome-based Hopfield analysis of autism spectrum disorder.</title>
<p><bold>A</bold> The distribution of time-frames on the fcANN-projection separately for ASD patients and typically developing control (TDC) participants. <bold>B</bold> We quantified attractor state activations in the Autism Brain Imaging Data Exchange datasets (study 7) as the individual-level mean activation of all time-frames belonging to the same attractor state. This analysis captured alterations similar to those previously associated with ASD-related perceptual atypicalities (visual, auditory and somatosensory cortices) as well as atypical integration of information about the “self” and the “other” (default mode network regions). All results are corrected for multiple comparisons across brain regions and attractor states (122×4 comparisons) with Bonferroni correction. See <xref rid="tbl3" ref-type="table">Table 3</xref> and Supplementary Figure 11 for detailed results. <bold>C</bold> The comparison of data generated by fcANNs initialized with ASD and TDC connectomes, respectively, revealed a characteristic pattern of differences in the system’s dynamics, with increased pull towards (and potentially a higher separation between) the action and perception attractors and a lower tendency of trajectories going towards the internal and external attractors. <bold><italic>Abbreviations</italic></bold>: MCC: middle cingulate cortex, ACC: anterior cingulate cortex, pg: perigenual, PFC: prefrontal cortex, dm: dorsomedial, dl: dorsolateral, STG: superior temporal gyrus, ITG: inferior temporal gyrus, Caud/Acc: caudate-accumbens, SM: sensorimotor, V1: primary visual, A1: primary auditory, SMA: supplementary motor cortex, ASD: autism spectrum disorder, TDC: typically developing control.</p>
</caption>
<graphic xlink:href="565516v4_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Thus, we contrasted the characteristic trajectories derived from the fcANN models of the two groups (initialized with the group-level functional connectomes). Our fcANN-based flow analysis predicted that in ASD, there is an increased likelihood of states returning towards the middle (more noisy states) from the internal-external axis and an increased likelihood of states transitioning towards the extremes of the action-perception axis (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). We observed a highly similar pattern in the real data (Pearson’s r = 0.66), statistically significant after two-sided permutation testing (shuffling the group assignment; 1,000 permutations; p=0.009).</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>The notion that the brain functions as an attractor network has long been proposed (<xref ref-type="bibr" rid="c20">Freeman, 1987</xref>; <xref ref-type="bibr" rid="c1">Amit, 1989</xref>; <xref ref-type="bibr" rid="c14">Deco &amp; Jirsa, 2012</xref>; <xref ref-type="bibr" rid="c15">Deco <italic>et al</italic>., 2012</xref>; <xref ref-type="bibr" rid="c26">Golos <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="c30">Hansen <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="c68">Vohryzek <italic>et al</italic>., 2020</xref>), although the exact functional form of the network underlying large-scale brain dynamics - or at least useful approximation thereof - remained elusive. The theoretical framework of free energy minimizing attractor neural networks (FEP-ANN) (<xref ref-type="bibr" rid="c65">Spisak &amp; Friston, 2025</xref>) identifies a specific class of attractor networks that emerge from first principles of self-organization - as articulated by the Free Energy Principle (FEP) (<xref ref-type="bibr" rid="c21">Friston, 2010</xref>; <xref ref-type="bibr" rid="c22">Friston <italic>et al</italic>., 2023</xref>). Therefore it provides a plausible candidate model for large-scale brain attractor dynamics and yields testable predictions - measurable signatures of these special attractor networks that can be validated empirically. In this study, we have introduced, and performed initial validation of, a simple and robust network-level generative computational model, rooted in the FEP-ANN framework and providing the opportunity to test these predictions empirically. Our model, termed a functional connectivity-based attractor network (fcANN), exploits special characteristics of the emergent inference rule of FEP-ANNs. This is a stochastic rule that governs how activity evolves in time with a given set of fixed coupling weights and leads to a Markov chain Monte Carlo (MCMC) sampling process. As a consequence, the activation time series data measured in each network node can be used to reconstruct the attractor network’s internal structure. Specifically, the coupling weights can be estimated as the negative inverse covariance matrix of the activation time series data. fcANN modeling applies this concept to large-scale brain dynamics as measured by resting-state fMRI data (as an estimate of weights corresponding to the steady-state distribution of the system).</p>
<p>The core idea underlying the fcANN reconstruction approach - the use of functional connectivity as a proxy for weighted information flow in the brain - is in line with previous empirical observations about the relationship between functional connectivity and brain activity, as articulated by the activity flow principle, first introduced by <xref ref-type="bibr" rid="c12">Cole <italic>et al</italic>. (2016)</xref>. The activity flow principle states that activity in a brain region can be predicted by a weighted combination of the activity of all other regions, where the weights are set to the functional connectivity of those regions to the held-out region. This principle has been shown to hold across a wide range of experimental and clinical conditions (<xref ref-type="bibr" rid="c12">Cole <italic>et al</italic>., 2016</xref>; <xref ref-type="bibr" rid="c38">Ito <italic>et al</italic>., 2017</xref>; <xref ref-type="bibr" rid="c50">Mill <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="c31">Hearne <italic>et al</italic>., 2021</xref>; <xref ref-type="bibr" rid="c8">Chen <italic>et al</italic>., 2018</xref>). Considering that the repeated, iterative application of the activity flow equation (extended with an arbitrary sigmoidal activation function) naturally reproduces certain types of recurrent artificial neural networks, e.g. Hopfield networks (<xref ref-type="bibr" rid="c34">Hopfield, 1982</xref>), yields an intuitive understanding of how the fcANN model works.</p>
<p>However, beyond this analogy, we need concrete evidence that the fcANN model and the underlying FEP-ANN framework is a valid model of large-scale brain dynamics.</p>
<p>Here we have tested multiple predictions of the FEP-ANN framework. Most importantly FEP-ANNs - through their emergent predictive coding-based learning rule - have been shown to develop approximately orthogonal attractor representations, and thereby approximate the so-called Kanter-Sompolinsky projector neural networks (K-S network, for short) (<xref ref-type="bibr" rid="c39">Kanter &amp; Sompolinsky, 1987</xref>). K-S networks are a special class of attractor networks that have been shown to be highly effective for pattern recognition and learning. In these networks, the attractor states are orthogonal to each other, and become equivalent with those eigenvectors of the coupling matrix, that have positive eigenvalues. This is a very strong prediction: K-S networks are a very special class of attractor networks, which do not arise from conventional learning rules, like Hebbian learning. To date, the predictive coding-based learning rule of FEP-ANNs is the only known local, incremental learning rule that can effectively approximate K-S networks in a single phase (but see “dreaming neural networks” (<xref ref-type="bibr" rid="c19">Fachechi et al., 2019</xref>) for a similar, two-phase approach). Thus, showing that fcANN models approximate K-S networks can be interpreted as evidence for the brain functioning akin to a FEP-ANN. Our results show that this is indeed the case: the fcANN models reconstructed from resting-state fMRI data approximate K-S networks, and thereby exhibit approximately orthogonal attractor states.</p>
<p>In the FEP-ANN framework, approximate attractor orthogonality has important computational implications: it allows the system achieve maximal “storage capacity” (the number of attractors that can be stored and retrieved without interference). Furthermore, in the FEP-ANN framework, attractor states can be interpreted as learned priors that capture the statistical structure of the environment, while the stochastic dynamics implement posterior sampling. Orthogonal attractors emerge as an efficient way to span the subspace of interest, fostering generalization to unseen data (as long as it is from the subspace spanned by the existing attractors).</p>
<p>Next, we have demonstrated that fcANN models exhibit multiple biologically plausible attractor states, with large basins and showed that the relaxation dynamics of the reconstructed model display fast convergence to attractor states - another signature that the functional connectome being a valid proxy for the coupling weights of an underlying attractor network. Relying on previous work, we can establish a relatively straightforward (although somewhat speculative) correspondence between attractor states and brain function, mapping brain activation on the axes of internal vs. external context (<xref ref-type="bibr" rid="c25">Golland <italic>et al</italic>., 2008</xref>; <xref ref-type="bibr" rid="c10">Cioli <italic>et al</italic>., 2014</xref>), as well as perception vs. action (<xref ref-type="bibr" rid="c24">Fuster, 2004</xref>). In our framework, the attractor states can be interpreted as learned priors that capture the statistical structure of the environment, while the stochastic dynamics implement posterior sampling. This connection suggests that canonical resting-state networks may represent the brain’s internal generative model of the world, continuously updated through the emergent learning dynamics we described theoretically. Furthermore, the relation between fcANN models and the FEP-ANN framework substantiates that the reconstructed attractor states are not solely local minima in the state space but act as a driving force for the dynamic trajectories of brain activity. We argue that attractor dynamics may be the main driving factor for the spatial and temporal autocorrelation structure of the brain, recently described to be predictive of network topology in relation to age, subclinical symptoms of dementia, and pharmacological manipulations with serotonergic drugs (<xref ref-type="bibr" rid="c63">Shinn <italic>et al</italic>., 2023</xref>). Nevertheless, attractor states should not be confused with the conventional notion of brain substates (<xref ref-type="bibr" rid="c7">Chen <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="c43">Kringelbach &amp; Deco, 2020</xref>) and large-scale functional gradients (<xref ref-type="bibr" rid="c46">Margulies <italic>et al</italic>., 2016</xref>). In the fcANN framework, attractor states can rather be conceptualized as “Platonic idealizations” of brain activity, that are continuously approximated - but never reached - by the brain, resulting in re-occurring patterns (brain substates) and smooth gradual transitions (large-scale gradients).</p>
<p>Considering the functional connectome as weights of a neural network distinguishes our methodology from conventional biophysical and phenomenological computational modeling strategies, which usually rely on the structural connectome to model polysynaptic connectivity (<xref ref-type="bibr" rid="c6">Cabral <italic>et al</italic>., 2017</xref>; <xref ref-type="bibr" rid="c15">Deco <italic>et al</italic>., 2012</xref>; <xref ref-type="bibr" rid="c26">Golos <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="c30">Hansen <italic>et al</italic>., 2015</xref>). Given the challenges of accurately modeling the structure-function coupling in the brain (<xref ref-type="bibr" rid="c62">Seguin <italic>et al</italic>., 2023</xref>), such models are currently limited in terms of reconstruction accuracy, hindering translational applications. By working with direct, functional MRI-based activity flow estimates, fcANNs bypass the challenge of modeling the structural-functional coupling and are able to provide a more accurate representation of the brain’s dynamic activity propagation (although at the cost of losing the ability to provide biophysical details on the underlying mechanisms). Another advantage of the proposed model is its simplicity. While many conventional computational models rely on the optimization of a high number of free parameters, the basic form of the fcANN approach comprises solely two, easily interpretable “hyperparameters” (temperature and noise) and yields notably consistent outcomes across an extensive range of these parameters (Supplementary Figure 3, 5, 7, 8, 9). To underscore the potency of this simplicity and stability, in the present work, we avoided any unnecessary parameter optimization, leaving a negligible chance of overfitting. It is likely, however, that extensive parameter optimization could further improve the accuracy of the model.</p>
<p>Further, the fcANN approach links brain dynamics directly to dynamical systems theory and the free energy principle, conceptualizes the emergence of large-scale canonical brain networks (<xref ref-type="bibr" rid="c71">Zalesky et al., 2014</xref>) in terms of multistability, and sheds light on the origin of characteristic task-responses that are accounted for by “ghost attractors” in the system (<xref ref-type="bibr" rid="c14">Deco &amp; Jirsa, 2012</xref>; <xref ref-type="bibr" rid="c68">Vohryzek <italic>et al</italic>., 2020</xref>). As fcANNs do not need to be trained to solve any explicit tasks, they are well suited to examine spontaneous brain dynamics. However, it is worth mentioning that fcANNs can also be further trained via the predictive coding-based learning rule of FEP-ANNs, to solve various tasks or to match developmental dynamics or pathological alterations. In this promising future direction, the training procedure itself becomes part of the model, providing testable hypotheses about the formation, and various malformations, of brain dynamics. A promising application of this is to consider structural brain connectivity (as measured by diffusion MRI) as a sparsity constraint for the coupling weights and then train the fcANN model to match the observed resting-state brain dynamics. If the resulting structural-functional ANN model is able to closely match the observed functional brain substate dynamics, it can be used as a novel approach to quantify and understand the structural functional coupling in the brain.</p>
<p>Given its simplicity, it is noteworthy how well the fcANN model is able to reconstruct and predict brain dynamics under a wide range of conditions. First and foremost, we have found that the topology of the functional connectome seems to be well suited to function as an attractor network, as it converges much faster than the respective null models. Second, we found that the two-dimensional fcANN projection can explain more variance in real (unseen) resting-state fMRI data than the first two principal components derived from the data itself. This may indicate that through the known noise tolerance of attractor neural networks, fcANNs are able to capture essential characteristics of the underlying dynamic processes even if our empirical measurements are corrupted by noise and low sampling rate. Indeed, fcANN attractor states were found to be robust to noisy weights (Supplementary Figure 10) and highly replicable across datasets acquired at different sites, with different scanners and imaging sequences (study 2 and 3). The observed high level of replicability allowed us to re-use the fcANN model constructed with the functional connectome of study 1 for all subsequent analyses, without any further fine-tuning or study-specific parameter optimization.</p>
<p>Both conceptually and in terms of analysis practices, resting and task states are often treated as separate phenomena. However, in the fcANN framework, the differentiation between task and resting states is considered an artificial dichotomy. Task-based brain activity in the fcANN framework is not a mere response to external stimuli in certain brain locations but a perturbation of the brain’s characteristic dynamic trajectories, with increased preference for certain locations on the energy landscape (“ghost attractors”). In our analyses, the fcANN approach captured and predicted participant-level activity changes induced by pain and its self-regulation and gave a mechanistic account for how relatively small activity changes in a single region (NAcc) may result in a significantly altered pain experience. Our control-signal analysis is different from, but compatible with, linear network control theory-based approaches (<xref ref-type="bibr" rid="c45">Liu <italic>et al</italic>., 2011</xref>; <xref ref-type="bibr" rid="c28">Gu <italic>et al</italic>., 2015</xref>). Combining network control theory with the fcANN approach could provide a powerful framework for understanding the effects of various tasks, conditions, and interventions (e.g., brain stimulation) on brain dynamics.</p>
<p>Brain dynamics can not only be perturbed by task or other types of experimental or naturalistic interventions, but also by pathological alterations. Here we provide an initial demonstration (study 7) of how fcANN-based analyses can characterize and predict altered brain dynamics in autism spectrum disorder (ASD). The observed ASD-associated changes in brain dynamics are indicative of a reduced ability to flexibly switch between perception and internal representations, corroborating previous findings that in ASD, sensory-driven connectivity transitions do not converge to transmodal areas (<xref ref-type="bibr" rid="c33">Hong <italic>et al</italic>., 2019</xref>). Such findings are in line with previous reports of a reduced influence of context on the interpretation of incoming sensory information in ASD (e.g., the violation of Weber’s law) (<xref ref-type="bibr" rid="c29">Hadad &amp; Schwartz, 2019</xref>).</p>
<p>Our findings open up a series of exciting opportunities for the better understanding of brain function in health and disease. First, fcANN analyses may provide insights into the causes of changes in brain dynamics, by, for instance, identifying the regions or connections that act as an “Achilles’ heel” in generating such changes. Such control analyses could, for instance, aid the differentiation of primary causes and secondary effects of activity or connectivity changes in various clinical conditions. Rather than viewing pathology as static connectivity differences, our approach suggests that disorders may reflect altered attractor landscapes that bias brain dynamics toward maladaptive states. This perspective could inform the development of targeted interventions that aim to reshape these landscapes through neurofeedback, brain stimulation, or pharmacological approaches.</p>
<p>Second, as a generative model, fcANNs provide testable predictions about the effects of various interventions on brain dynamics, including pharmacological modulations as well as non-invasive brain stimulation (e.g., transcranial magnetic or direct current stimulation, focused ultrasound, etc.) and neurofeedback. Obtaining the optimal stimulation or treatment target within the fcANN framework (e.g., by means of network control theory (<xref ref-type="bibr" rid="c45">Liu <italic>et al</italic>., 2011</xref>)) is one of the most promising future directions with the potential to significantly advance the development of novel, personalized treatment approaches.</p>
<p>Third, the theoretical integration of the fcANN model with the FEP-ANN framework positions our work within a broader scientific program that seeks to understand the brain as a self-organizing, information-processing system governed by fundamental physical and computational principles. The empirical validation of attractor orthogonality represents a crucial step toward establishing this unified framework for understanding brain function across scales and contexts.</p>
<p>The proposed approach is not without limitations. First, fcANNs do not incorporate information about anatomical connectivity and do not explicitly model biophysical details. Thus, in its present form, the model is not suitable to study the structure-function coupling and cannot yield mechanistic explanations underlying (altered) polysynaptic connections, at the level of biophysical details. Nevertheless, our approach showcases that many characteristics of brain dynamics, like multistability, temporal autocorrelations, states and gradients, can be explained, and predicted, by a very simple nonlinear phenomenological model. Second, our model assumes a stationary functional connectome, which seems to contradict notions of dynamic connectivity. However, while the underlying FEP-ANN framework focuses on the long-term steady-state distribution of the system, it also naturally incorporates multistable fluctuations and the related dynamic connectivity changes through the stochastic relaxation dynamics. This is in line with the notion of “latent functional connectivity”, an intrinsic brain network architecture built up from connectivity properties that are persistent across brain substates (<xref ref-type="bibr" rid="c47">McCormick <italic>et al</italic>., 2022</xref>).</p>
<p>In this initial work, we presented the simplest possible implementation of the fcANN concept. It is clear that the presented analyses exploit only a small proportion of the richness of the full state-space dynamics reconstructed by the fcANN model. There are many potential ways to further improve the utility of the fcANN approach. Increasing the number of reconstructed attractor states (by increasing the temperature parameter), investigating higher-dimensional dynamics, fine-tuning the hyperparameters, and testing the effect of different initializations and perturbations are all important directions for future work, with the potential to further improve the model’s accuracy and usefulness.</p>
</sec>
<sec id="s5">
<title>Conclusion</title>
<p>Here we have proposed a principled, lightweight, theory-driven framework that instantiates the inference dynamics of free-energy-minimizing attractor networks (FEP-ANNs) to model large-scale brain activity. Initialized with empirical functional connectivity, the fcANN links brain connectivity to activity and identifies neurobiologically meaningful attractor states underlying large-scale brain dynamics. We demonstrated that the fcANN models display signs of attractor self-orthogonalization - a hallmark of FEP-ANN systems. The proposed framework provides a simple, interpretable, and predictive basis for studying rest, task perturbations, and disease, and for model-guided interventions.</p>
</sec>
    <sec id="s10">
        <title>Methods</title>
        <sec id="s10a">
            <title>Statistical analysis and reporting</title>
            <p>Unless stated otherwise, all hypothesis tests are two-sided. When permutation tests are used, reported p-values are empirical permutation p-values computed as the proportion of null replicates with a test statistic at least as extreme as the observed one (with the usual +1 numerator/denominator correction). We report the statistic used in each comparison (e.g., Pearson correlation r, chi-square dissimilarity, L2 norm of position differences, median iterations to convergence). Degrees of freedom are reported where parametric tests are used; permutation-based p-values do not have degrees of freedom. By default, permutation counts were 1,000, except where explicitly noted (e.g., 50,000 label shuffles in clinical analyses). Confidence intervals based on bootstrap are percentile bootstrap intervals; where shown as 99% CIs, they reflect the percentile range from the bootstrap samples described below. For multiple comparisons in the clinical analyses, we applied Bonferroni correction across regions and attractor states as specified below.</p>
        </sec>
        <sec id="s10b">
            <title>Data</title>
            <p>We obtained functional MRI data from 7 sources (Methods <xref rid="tbl1" ref-type="table">Table 1</xref>). MRI sequence parameters for studies 1–4 are summarized in Supplementary Table 1. We included three resting-state studies with healthy volunteers (study 1, study 2, study 3, <italic>n</italic><sub><italic>total</italic></sub>= 118), one task-based study (study 4, <italic>n</italic><sub><italic>total</italic></sub>= 33 participants, 9 runs each), an individual-participant meta-analytic activation map of pain (study 5, <italic>n</italic><sub><italic>total</italic></sub>= 603 from 20 studies), 8 task-based activation patterns obtained from coordinate-based meta-analyses via Neurosynth (study 6, 14,371 studies in total; see Supplementary Table 2), and a resting-state dataset focusing on autism spectrum disorder (ASD) from ABIDE (study 7, <italic>n</italic><sub><italic>total</italic></sub>= 1,112, <xref ref-type="bibr" rid="c16">Di Martino <italic>et al</italic>. (2014)</xref>).</p>
            <table-wrap id="tblM1" orientation="portrait" position="float">
                <label>Methods Table 1:</label>
                <caption><title>Datasets and studies.</title>
                    <p>The table includes details about the study modality, analysis aims, sample size used for analyses, mean age, gender ratio, and references.</p></caption>
                <graphic xlink:href="565516v4_tblM1.tif" mime-subtype="tiff" mimetype="image"/>
            </table-wrap>
            <p>Study 1 was used to evaluate whether the resting-state functional connectome can be treated as an attractor network, to optimize the temperature (β) and noise (ε) parameters of the fcANN model, and to evaluate the proposed approach for reconstructing resting-state brain dynamics. Studies 2 and 3 served as replication datasets. Studies 1–3 are well suited to examine replicability and generalizability; data were acquired in three centers across two countries, by different research staff, with different scanners (Philips, Siemens, GE) and imaging sequences. Further details on studies 1–3 are described in <xref ref-type="bibr" rid="c88">Spisak <italic>et al</italic>. (2020)</xref>. The ability of the proposed approach to model task-based perturbations of brain dynamics was evaluated in study 4, which consisted of nine task-based fMRI runs for each of the 33 healthy volunteers. In all runs, participants received heat pain stimulation. Each stimulus lasted 12.5 seconds, with a 3-second ramp-up and 2-second ramp-down and 7.5 seconds at target temperature. Six temperature levels were administered (44.3°C, 45.3°C, 46.3°C, 47.3°C, 48.3°C, 49.3°C). We used run 1 (passive experience), run 3 (down-regulation), and run 7 (up-regulation). In runs 3 and 7, participants were asked to cognitively increase (regulate up) or decrease (regulate down) pain intensity. No self-regulation instructions were provided in run 1. See <xref ref-type="bibr" rid="c70">Woo <italic>et al</italic>. (2015)</xref> for details. Pain control signal for our task-based trajectory analyses on data from study 4 was derived from our individual participant meta-analysis of 20 pain fMRI studies (study 5, n=603). For details, see <xref ref-type="bibr" rid="c72">Zunhammer <italic>et al</italic>. (2021)</xref>. To obtain fMRI activation maps for other tasks, we used Neurosynth (Tor D. (2011)), a web-based platform for large-scale, automated synthesis of fMRI data. We performed eight coordinate-based meta-analyses with the terms “motor”, “auditory”, “visual”, “face”, “autobiographical”, “theory mind”, “language”, and “pain” (Supplementary Table 2) and obtained Z-score maps from a two-way ANOVA, comparing coordinates reported for studies with and without the term of interest and testing for a nonzero association between term use and voxel activation. In study 7 (ABIDE), we obtained preprocessed regional time-series data from the Preprocessed Connectome Project (<xref ref-type="bibr" rid="c78">Craddock <italic>et al</italic>., 2013</xref>), as shared at <ext-link ext-link-type="uri" xlink:href="https://osf.io/hc4md">https://osf.io/hc4md</ext-link> by <xref ref-type="bibr" rid="c79">Dadi <italic>et al</italic>. (2019)</xref>. Preprocessed time-series data were obtained with the 122-region version of the BASC (Bootstrap Analysis of Stable Clusters) brain atlas (<xref ref-type="bibr" rid="c76">Bellec <italic>et al</italic>., 2010</xref>).</p>
        </sec>
        <sec id="s10c">
            <title>Preprocessing and time-series extraction</title>
            <p>Functional MRI data from studies 1–4 were preprocessed with our in-house analysis pipeline, the RPN-pipeline (<ext-link ext-link-type="uri" xlink:href="https://github.com/spisakt/RPN-signature">https://github.com/spisakt/RPN-signature</ext-link>). The RPN-pipeline is based on PUMI (Neuroimaging Pipelines Using Modular workflow Integration, <ext-link ext-link-type="uri" xlink:href="https://github.com/pni-lab/PUMI">https://github.com/pni-lab/PUMI</ext-link>), a nipype-based (<xref ref-type="bibr" rid="c83">Gorgolewski <italic>et al</italic>., 2011</xref>) workflow management system. It capitalizes on tools from FSL (<xref ref-type="bibr" rid="c84">Jenkinson <italic>et al</italic>., 2012</xref>), ANTs (<xref ref-type="bibr" rid="c74">Avants <italic>et al</italic>., 2011</xref>), and AFNI (<xref ref-type="bibr" rid="c77">Cox, 1996</xref>), with code partially adapted from C-PAC (<xref ref-type="bibr" rid="c78">Craddock <italic>et al</italic>., 2013</xref>) and niworkflows (<xref ref-type="bibr" rid="c81">Esteban <italic>et al</italic>., 2019</xref>), as well as in-house Python routines.</p>
            <p>Brain extraction from both anatomical and structural images, as well as tissue segmentation from the anatomical images, was performed with FSL BET and FAST. Anatomical images were linearly and nonlinearly co-registered to the 1 mm MNI152 standard brain template with ANTs (see <ext-link ext-link-type="uri" xlink:href="https://gist.github.com/spisakt/0caa7ec4bc18d3ed736d3a4e49da7415">https://gist.github.com/spisakt/0caa7ec4bc18d3ed736d3a4e49da7415</ext-link> for parameters). Functional images were co-registered to the anatomical images with FSL FLIRT’s boundary-based registration. All resulting transformations were saved for further use. Preprocessing of functional images was performed in native space, without resampling. Realignment-based motion correction was performed with FSL MCFLIRT. The resulting six head motion estimates (three rotations, three translations), their squared terms, their derivatives, and the squared derivatives (Friston-24 expansion, <xref ref-type="bibr" rid="c82">Friston <italic>et al</italic>. (1996)</xref>) were calculated as nuisance signals. Additionally, head motion was summarized as framewise displacement (FD) time series, according to Power’s method (<xref ref-type="bibr" rid="c86">Power <italic>et al</italic>., 2012</xref>), for use in censoring and exclusion. After motion correction, outliers (e.g., motion spikes) in time-series data were attenuated using AFNI despike. The union of eroded white-matter maps and ventricle masks was transformed to native functional space and used to extract noise signals for anatomical CompCor correction (<xref ref-type="bibr" rid="c75">Behzadi <italic>et al</italic>., 2007</xref>).</p>
            <p>In a nuisance regression step, six CompCor parameters (the first six principal components of noise-region time series), the Friston-24 motion parameters, and the linear trend were removed with a general linear model. On the residual data, temporal band-pass filtering was performed with AFNI’s 3dBandpass to retain the 0.008–0.08 Hz band. To further attenuate motion artifacts, potentially motion-contaminated time frames, defined by a conservative FD &gt; 0.15 mm threshold, were dropped (scrubbing, <xref ref-type="bibr" rid="c87">Satterthwaite <italic>et al</italic>. (2013)</xref>). Participants were excluded if more than 50% of frames were scrubbed.</p>
            <p>The 122-parcel BASC (Bootstrap Analysis of Stable Clusters) atlas (<xref ref-type="bibr" rid="c76">Bellec <italic>et al</italic>., 2010</xref>) was individualized by transforming it to each participant’s native functional space (nearest-neighbour interpolation) and masking by that participant’s grey-matter mask to retain only grey-matter voxels. Voxel time series were then averaged within BASC regions.</p>
            <p>All these preprocessing steps are part of the containerized version of the RPN-pipeline (<ext-link ext-link-type="uri" xlink:href="https://spisakt.github.io/RPN-signature">https://spisakt.github.io/RPN-signature</ext-link>), which we run with default parameters for all studies, as in <xref ref-type="bibr" rid="c88">Spisak <italic>et al</italic>. (2020)</xref>.</p>
        </sec>
        <sec id="s10d">
            <title>Functional connectome</title>
            <p>Regional time series were ordered into large-scale functional modules (defined by the 7-parcel level of the BASC atlas) for visualization. Next, in all datasets, we estimated study-level mean connectivity matrices by regularized partial correlation via the Ledoit–Wolf shrinkage estimator (<xref ref-type="bibr" rid="c85">Ledoit &amp; Wolf, 2004</xref>), as implemented in Nilearn (<xref ref-type="bibr" rid="c73">Abraham <italic>et al</italic>., 2014</xref>). The precision matrix was obtained by inverting the shrinkage covariance, and partial correlations were derived from the precision; diagonal elements were set to zero.</p>
        </sec>
        <sec id="s10e">
            <title>fcANN inference (FEP-ANN) and update rules</title>
            <p>Our fcANN instantiates the inference dynamics of free-energy-minimizing attractor neural networks (FEP-ANNs) at the macro-scale. Each node represents a brain region with continuous activity <bold>σ</bold> = (σ<sub>1</sub>, …, σ<sub><italic>m</italic></sub>), and couplings are given by the symmetrized matrix <bold><italic>J</italic></bold> (see Functional connectome). Unless noted otherwise, biases are zero (<bold><italic>b</italic></bold> = <bold>0</bold>).</p>
            <p>Deterministic inference. In the noise-free symmetric case, activities are updated by repeatedly applying a sigmoidal nonlinearity to the weighted input
                <disp-formula id="eqn1a">
                    <graphic xlink:href="565516v4_eqn1a.gif" mime-subtype="gif" mimetype="image"/>
                </disp-formula>
                where <italic>S</italic>is a smooth odd sigmoid (we used <italic>tanh</italic>as a practical, fast surrogate for the Langevin function) and β is the inverse temperature (precision) scaling the couplings. As the inference rule was derived as a gradient descent on free energy, iterations monotonically decrease the free energy function and therefore converge to a local free-energy minimum without any external optimizer. Thus, convergence does not require any optimization procedure with an external optimizer. Instead, it arises as the fixed point of repeated local inference updates, which implement gradient descent on free energy in the deterministic symmetric case (see main text).</p>
            <p>Stochastic (Langevin-style) inference. For generative modeling of dynamics, we adopt a slight variation of the FEP-ANN inference rule: starting from the deterministic update above, we add zero-mean Gaussian noise directly to the post-activation state (Langevin-style)
                <disp-formula id="eqn2a">
                    <graphic xlink:href="565516v4_eqn2a.gif" mime-subtype="gif" mimetype="image"/>
                </disp-formula>
                This explicit additive Gaussian noise differs from the continuous-Bernoulli noise implied by the theoretical derivation but aligns with common Langevin formulations and was empirically robust. We use deterministic updates to identify attractors and study convergence; we use stochastic updates as a generative model of multistable dynamics.</p>
        </sec>
        <sec id="s10f">
            <title>fcANN convergence and attractors</title>
            <p>We investigated convergence under the deterministic update (<xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref>) by contrasting iterations-to-convergence of the empirical fcANN against a permutation-based null. The null was constructed by randomly permuting the upper triangle of <bold><italic>J</italic></bold> and reflecting it to preserve symmetry (destroying topology while preserving weight distribution). For each of 1,000 permutations, we initialized both models with the same random state and counted iterations to convergence. Statistical significance of faster convergence in the empirical connectome was assessed via a one-sided Wilcoxon signed-rank test on paired iteration counts (1,000 pairs), testing whether the empirical connectome converges in fewer iterations than its permuted counterpart. We repeated this procedure across inverse-temperature values β ∈ {0.035, 0.040, 0.045, 0.050, 0.055, 0.060} (yielding 2–8 attractor states). See Supplementary Figure 6 for detailed results.</p>
        </sec>
        <sec id="s10g">
            <title>fcANN projection</title>
            <p>We mapped out the fcANN state space by initializing the model with a random input and applying the stochastic update (<xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref>) for iterations, storing visited activity configurations. We performed principal component analysis (PCA) on the samples and used the first two PCs as the coordinate system for the fcANN projection. Using multinomial logistic regression, we predicted attractor identity from the first two PCs. Performance was evaluated with 10-fold cross-validation and a two-sided permutation test (1,000 label permutations within folds). We visualized attractor positions and decision boundaries based on this classifier. The fcANN projection is used for visualization; unless otherwise noted, inferential comparisons between empirical and simulated data are performed in the full 122-dimensional space.</p>
        </sec>
        <sec id="s10h">
            <title>Parameter optimization</title>
            <p>Based on the convergence analysis, we selected the inverse temperature providing the fastest median convergence (β = 0.04) for subsequent analyses, to minimize computational costs. We then optimized the stochastic noise level ε for the Langevin-style update by comparing the full 122-dimensional distributions of empirical and fcANN-generated data.</p>
            <p>Specifically, for eight ε values spaced logarithmically between 0.1 and 1, we generated samples with <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref> and computed the 2-Wasserstein (Earth Mover’s) distance between the 122-dimensional empirical and simulated distributions (after per-region z-scoring). As a null, we drew surrogate samples from a covariance-matched multivariate normal distribution 𝒩(<bold>0, Σ</bold>), where <bold>Σ</bold> is the empirical covariance of the regional time series. For each ε, we generated 1,000 null surrogates and contrasted the empirical Wasserstein distance to the null distribution, summarizing the separation with Glass’s Δ and permutation p-values. We selected ε = 0.37 for all subsequent analyses.</p>
        </sec>
        <sec id="s10i">
            <title>Replicability</title>
            <p>We obtained the four attractor states in study 1, as described above. We then constructed two other fcANNs, based on the study-mean functional connectome obtained in studies 2 and 3 and obtained all attractor states of these models, with the same parameter settings (β = 0.04 and ε = 0.37) as in study 1. In both replication studies we found four attractor states. The spatial similarity of attractor states across studies was evaluated by Pearson’s correlation coefficient.</p>
        </sec>
        <sec id="s10j">
            <title>Evaluation: resting state dynamics</title>
            <p>To evaluate the explanatory power of the fcANN projection, we performed PCA on the preprocessed fMRI time frames from study 1 (analogous to the methodology of the fcANN projection, but on the empirical time-series data). Next, we fitted linear regression models using the first two fcANN-based or data-based PCs as regressors to reconstruct the real fMRI time frames. In-sample explained variances and confidence intervals were calculated for both models with bootstrapping (100 samples; percentile 99% CIs). Differences in explained variance between fcANN- and data-based PCs were assessed with a two-sided percentile bootstrap on the difference in <italic>R</italic><sup>2</sup>. To evaluate out-of-sample generalization of the PCs (fcANN- and data-based) from study 1, we calculated how much variance they explain in study 2.</p>
            <p>Similarity between state occupancy and distribution was calculated during parameter optimization. More detail on the associated null models can be found in Supplementary Figure 7.</p>
            <p>To confirm that the real and fcANN temporal sequences (from the stochastic relaxation) display similar temporal autocorrelation properties, we compared both to their randomly shuffled variant with a “flow analysis”. First, we calculated the direction on the fcANN projection plane between each successive TR (a vector on the fcANN projection plane for each TR transition), both for the empirical and the shuffled data. Next, we obtained two-dimensional binned means for both the x and y coordinates of these transition vectors (pooled across all participants), calculated over a 100×100 grid of uniformly distributed bins in the [−6, 6] range (arbitrary units) and applied Gaussian smoothing with σ = 5 bins (same approach as described in the Parameter optimization section). Finally, we visualized the difference between the binned-mean trajectories of the empirical and the shuffled data as a “streamplot”, with the Python package matplotlib. The same approach was repeated with the fcANN-generated data. The similarity of the real and fcANN-generated flow analysis was quantified with Pearson’s correlation coefficient (two-sided); p-values were obtained with permutation testing (1,000 permutations), by shuffling temporal order (for resting-state analyses) or condition labels (for task analyses; see below).</p>
        </sec>
        <sec id="s10k">
            <title>Evaluation: task-based dynamics</title>
            <p>We used study 4 to evaluate the ability of the fcANN approach to capture and predict task-induced alterations in large-scale brain dynamics. First, runs 1, 3 and 7, investigating the passive experience and the down- and up-regulation of pain, respectively, were preprocessed with the same workflow used to preprocess studies 1-3 (Preprocessing and time-series extraction). Regional time series were grouped into “pain” and “rest” blocks, with a 6-second delay to adjust for the hemodynamic response time. All activation time frames were transformed to the fcANN projection plane obtained from study 1. Within-participant differences in the average location on the fcANN projection were calculated and visualized with radial plots, showing the participant-level mean trajectory on the projection plane from rest to pain (circles), as well as the group-level trajectory (arrow). The significance of the position difference and energy difference of the participant-level mean activations on the projection plane was tested with a two-sided permutation test (1,000 permutations), using the L2 norm of the two-dimensional position difference and the absolute energy difference, respectively, as test statistics, and randomly swapping the conditions within each participant.</p>
            <p>To further highlight the difference between the task and rest conditions, a “flow analysis” was performed to investigate the dynamic trajectory differences between the conditions rest and pain. The analysis method was identical to the flow analysis of the resting state data (Evaluation: resting state dynamics). First, we calculated the direction in the projection plane between each successive TR during the rest conditions (a vector on the fcANN projection plane for each TR transition). Next, we obtained two-dimensional binned means for the x and y coordinates of these transition vectors (pooled across all participants), calculated over a two-dimensional grid of 100×100 uniformly distributed bins in the [-6,6] range (arbitrary units) and applied Gaussian smoothing with σ = 5 bins. The same procedure was repeated for the pain condition and the difference in the mean directions between the two conditions was visualized as “streamplots” (using Python’s matplotlib). We used the same approach to quantify the difference in characteristic state transition trajectories between the up- and downregulation conditions. The empirically estimated trajectory differences (from real fMRI data) were contrasted to the trajectory differences predicted by the fcANN model from study 1. The similarity between real and simulated flow maps was quantified with Pearson’s correlation coefficient (two-sided), and significance was assessed via permutation testing (1,000 permutations) by randomly swapping condition labels within participants.</p>
            <p>To obtain fcANN-simulated state transitions in resting conditions, we used the stochastic relaxation procedure (<xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref>), with μ set to zero. To simulate the effect of pain-related activation on large-scale brain dynamics, we set μ<sub>(</sub> during the stochastic relaxation procedure to a value representing pain-elicited activity in region i. The region-wise activations were obtained calculating the parcel-level mean activations from the meta-analytic pain activation map from (<xref ref-type="bibr" rid="c72">Zunhammer <italic>et al</italic>., 2021</xref>), which contained Hedges’ g effect sizes from an individual participant-level meta-analysis of 20 pain studies, encompassing a total of n=603 participants. The whole activation map was scaled with five different values ranging from 10<sup>-3</sup> to 10<sup>-1</sup>, spaced logarithmically, to investigate various signal-to-noise scenarios. We obtained the activity patterns of 10<sup>5</sup> iterations from this stochastic relaxation procedure and calculated the state transition trajectories with the same approach used with the empirical data. Next we calculated the fcANN-generated difference between the rest and pain conditions and compared it to the actual difference through a permutation test with 1,000 permutations, randomly swapping the conditions within each participant in the real data and using Pearson’s correlation coefficient between the real (permuted) and fcANN-generated flow maps as the test statistic. From the five investigated signal-to-noise values, we chose the one that provided the highest similarity to the real pain vs. rest trajectory difference.</p>
            <p>When comparing the simulated and real trajectory differences between pain up- and downregulation, we used the same procedure, with two differences. First, when calculating the simulated state transition vectors for the self-regulation conditions, we used the same procedure as for the pain condition, but introduced and additional signal in the nucleus accumbens, with a negative and positive sign, for up- and downregulation, respectively. We did not optimize the signal-to-noise ratio for the nucleus accumbens signal but, instead, simply used the value optimized for the pain vs. rest contrast (For a robustness analysis, see Supplementary figure 9).</p>
        </sec>
        <sec id="s10l">
            <title>Clinical data</title>
            <p>To demonstrate the sensitivity of the fcANN approach to clinically relevant alterations of large-scale brain dynamics in autism spectrum disorder (ASD), we obtained data from n=172 individuals, acquired at the New York University Langone Medical Center (NYU) as shared in the Autism Brain Imaging Data Exchange dataset (study 7: ABIDE, (<xref ref-type="bibr" rid="c16">Di Martino <italic>et al</italic>., 2014</xref>)). We focused on the largest ABIDE imaging center to ensure that results were not biased by center effects. We excluded high-motion cases similarly to studies 1–4, i.e., by scrubbing volumes with FD &gt; 0.15 and excluding participants with &gt;50% of data scrubbed. Time-series data were pooled and visualized on the fcANN projection of study 1, separately for ASD and control participants. Next, for each participant, we grouped time frames from the regional time-series data according to the corresponding attractor states (obtained with the fcANN model from study 1) and averaged time frames corresponding to the same attractor state to calculate participant-level mean attractor activations. We assessed mean attractor-activity differences between groups with a two-sided permutation test, randomly reassigning group labels 50,000 times. Reported effect sizes in the clinical tables are mean activation differences. Note that activation time series were standard-scaled independently for each region, so effect size can be interpreted as the proportion of regional variability. We adjusted the significance threshold with a Bonferroni-correction, accounting for tests across 4 states and 122 regions, resulting in α = 0.0001. Finally, we calculated trajectory differences between the two groups, as predicted by the group-specific fcANNs (initialized with the ASD and TDC connectomes), and - similarly to the approach used in study 4 - contrasted the fcANN predictions with trajectory differences observed in real rsfMRI data. As in the previous flow analyses, we tested the significance of the similarity (Pearson’s correlation) between predicted and observed trajectory differences with a two-sided permutation test (1,000 permutations), by shuffling group labels.</p>
        </sec>
    </sec>

</body>
<back>

<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>Studies 1, 2 and 4 are available at openneuro.org (ds002608, ds002608, ds000140). Data for study 3 are available upon request. Data for studies 5-6 are available at the GitHub page of the project: <ext-link ext-link-type="uri" xlink:href="https://github.com/pni-lab/connattractor">https://github.com/pni-lab/connattractor</ext-link>. Study 7 is available at <ext-link ext-link-type="uri" xlink:href="https://fcon_1000.projects.nitrc.org/indi/abide/">https://fcon_1000.projects.nitrc.org/indi/abide/</ext-link>, preprocessed data are available at <ext-link ext-link-type="uri" xlink:href="http://preprocessed-connectomes-project.org/">http://preprocessed-connectomes-project.org/</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>The work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation; projects ‘TRR289 - Treatment Expectation’, ID 422744262 and ‘SFB1280 - Extinction Learning’, ID 316803389) and by IBS-R015-D1 (Institute for Basic Science; C.W.-W.).</p></ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s6">
<title>Analysis source code</title>
<p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/pni-lab/connattractor">https://github.com/pni-lab/connattractor</ext-link></p>
</sec>
<sec id="s7">
<title>Project website</title>
<p>
<ext-link ext-link-type="uri" xlink:href="https://pni-lab.github.io/connattractor/">https://pni-lab.github.io/connattractor/</ext-link></p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary Information</label>
<media xlink:href="supplements/565516_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Amit</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>1989</year>). <source>Modeling Brain Function: The World of Attractor Neural Networks</source>. <publisher-name>Cambridge University Press</publisher-name>. <pub-id pub-id-type="doi">10.1017/cbo9780511623257</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barttfeld</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Uhrig</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sitt</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Sigman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jarraya</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Signature of consciousness in the dynamics of resting-state brain activity</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>112</volume>(<issue>3</issue>), <fpage>887</fpage>–<lpage>892</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, &amp; <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Network neuroscience</article-title>. <source>Nature Neuroscience</source>, <volume>20</volume>(<issue>3</issue>), <fpage>353</fpage>–<lpage>364</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Dynamic models of large-scale brain activity</article-title>. <source>Nature Neuroscience</source>, <volume>20</volume>(<issue>3</issue>), <fpage>340</fpage>–<lpage>352</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Buzsaki</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2006</year>). <source>Rhythms of the Brain</source>. <publisher-name>Oxford university press</publisher-name>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cabral</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kringelbach</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Functional connectivity dynamically evolves on multiple time-scales over a static structural connectome: Models and mechanisms</article-title>. <source>NeuroImage</source>, <volume>160</volume>, <fpage>84</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Greicius</surname>, <given-names>M. D.</given-names></string-name>, &amp; <string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Introducing co-activation pattern metrics to quantify spontaneous brain network dynamics</article-title>. <source>Neuroimage</source>, <volume>111</volume>, <fpage>476</fpage>–<lpage>488</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>R. H.</given-names></string-name>, <string-name><surname>Ito</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kulkarni</surname>, <given-names>K. R.</given-names></string-name>, &amp; <string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2018</year>). <article-title>The human brain traverses a common activation-pattern state space across task and rest</article-title>. <source>Brain Connectivity</source>, <volume>8</volume>(<issue>7</issue>), <fpage>429</fpage>–<lpage>443</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chiêm</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Crevecoeur</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Delvenne</surname>, <given-names>J.-C.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Structure-informed functional connectivity driven by identifiable and state-specific control regions</article-title>. <source>Network Neuroscience</source>, <volume>5</volume>(<issue>2</issue>), <fpage>591</fpage>–<lpage>613</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cioli</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Abdi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Beaton</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Burnod</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Mesmoudi</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Differences in human cortical gene expression match the temporal properties of large-scale functional networks</article-title>. <source>PloS One</source>, <volume>9</volume>(<issue>12</issue>), <elocation-id>e115913</elocation-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2024</year>). <article-title>The explanatory power of activity flow models of brain function</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/ARXIV.2402.02191</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Ito</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, &amp; <string-name><surname>Schultz</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Activity flow over resting-state networks shapes cognitive task activations</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>12</issue>), <fpage>1718</fpage>–<lpage>1726</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Ito</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, &amp; <string-name><surname>Schultz</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Activity flow over resting-state networks shapes cognitive task activations</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>12</issue>), <fpage>1718</fpage>–<lpage>1726</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4406</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Jirsa</surname>, <given-names>V. K.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Ongoing cortical activity at rest: criticality, multistability, and ghost attractors</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>(<issue>10</issue>), <fpage>3366</fpage>–<lpage>3375</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Senden</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Jirsa</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2012</year>). <article-title>How anatomy shapes dynamics: a semi-analytical study of the brain at rest by a simple spin model</article-title>. <source>Frontiers in Computational Neuroscience</source>, <volume>6</volume>, <fpage>68</fpage>.</mixed-citation></ref>
    <ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Martino</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>C.-G.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Denio</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Castellanos</surname>, <given-names>F. X.</given-names></string-name>, <string-name><surname>Alaerts</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Assaf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bookheimer</surname>, <given-names>S. Y.</given-names></string-name>, <string-name><surname>Dapretto</surname>, <given-names>M.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2014</year>). <article-title>The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</article-title>. <source>Molecular Psychiatry</source>, <volume>19</volume>(<issue>6</issue>), <fpage>659</fpage>–<lpage>667</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dosenbach</surname>, <given-names>N. U. F.</given-names></string-name>, <string-name><surname>Raichle</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Gordon</surname>, <given-names>E. M.</given-names></string-name></person-group> (<year>2025</year>). <article-title>The brain’s action-mode network</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>26</volume>(<issue>3</issue>), <fpage>158</fpage>–<lpage>168</lpage>. <pub-id pub-id-type="doi">10.1038/s41583-024-00895-x</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dotsenko</surname>, <given-names>V. S.</given-names></string-name>, &amp; <string-name><surname>Tirozzi</surname>, <given-names>B.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Replica symmetry breaking in neural networks with modified pseudo-inverse interactions</article-title>. <source>Journal of Physics A: Mathematical and General</source>, <volume>24</volume>(<issue>21</issue>), <fpage>5163</fpage>–<lpage>5180</lpage>. <pub-id pub-id-type="doi">10.1088/0305-4470/24/21/026</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fachechi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Agliari</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Barra</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Dreaming neural networks: Forgetting spurious memories and reinforcing pure ones</article-title>. <source>Neural Networks</source>, <volume>112</volume>, <fpage>24</fpage>–<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2019.01.006</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freeman</surname>, <given-names>W. J.</given-names></string-name></person-group> (<year>1987</year>). <article-title>Simulation of chaotic EEG patterns with a dynamic model of the olfactory system</article-title>. <source>Biological Cybernetics</source>, <volume>56</volume>(<issue>2–3</issue>), <fpage>139</fpage>–<lpage>150</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2010</year>). <article-title>The free-energy principle: a unified brain theory?</article-title> <source>Nature Reviews Neuroscience</source>, <volume>11</volume>(<issue>2</issue>), <fpage>127</fpage>–<lpage>138</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2787</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Da Costa</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sajid</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Heins</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ueltzhöffer</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Pavliotis</surname>, <given-names>G. A.</given-names></string-name>, &amp; <string-name><surname>Parr</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The free energy principle made simpler but not too simple</article-title>. <source>Physics Reports</source>, <volume>1024</volume>, <fpage>1</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1016/j.physrep.2023.07.001</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>FitzGerald</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rigoli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Schwartenbeck</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>O’Doherty</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Pezzulo</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Active inference and learning</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>68</volume>, <fpage>862</fpage>–<lpage>879</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.06.022</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuster</surname>, <given-names>J. M.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Upper processing stages of the perception–action cycle</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>8</volume>(<issue>4</issue>), <fpage>143</fpage>–<lpage>145</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golland</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Golland</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bentin</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Malach</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Data-driven clustering reveals a fundamental subdivision of the human cortex into two global systems</article-title>. <source>Neuropsychologia</source>, <volume>46</volume>(<issue>2</issue>), <fpage>540</fpage>–<lpage>553</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golos</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jirsa</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Daucé</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Multistability in large scale models of brain activity</article-title>. <source>PLoS Computational Biology</source>, <volume>11</volume>(<issue>12</issue>), <elocation-id>e1004644</elocation-id>.</mixed-citation></ref>
    <ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greene</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Horien</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Barson</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Why is everyone talking about brain state?</article-title> <source>Trends in Neurosciences</source>.</mixed-citation></ref>
    <ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pasqualetti</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Cieslak</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Telesford</surname>, <given-names>Q. K.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Kahn</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Medaglia</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Vettel</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Grafton</surname>, <given-names>S. T.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2015</year>). <article-title>Controllability of structural brain networks</article-title>. <source>Nature Communications</source>, <volume>6</volume>(<issue>1</issue>), <fpage>8414</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hadad</surname>, <given-names>B.-S.</given-names></string-name>, &amp; <string-name><surname>Schwartz</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Perception in autism does not adhere to Weber’s law</article-title>. <source>eLife</source>, <volume>8</volume>, <elocation-id>e42223</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.42223</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hansen</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Battaglia</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Spiegler</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Jirsa</surname>, <given-names>V. K.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Functional connectivity dynamics: modeling the switching behavior of the resting state</article-title>. <source>Neuroimage</source>, <volume>105</volume>, <fpage>525</fpage>–<lpage>535</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hearne</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Mill</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Keane</surname>, <given-names>B. P.</given-names></string-name>, <string-name><surname>Repovš</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Anticevic</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Activity flow underlying abnormalities in brain activations and cognition in schizophrenia</article-title>. <source>Science Advances</source>, <volume>7</volume>(<issue>29</issue>), <elocation-id>eabf2513</elocation-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heinz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>G. K.</given-names></string-name>, <string-name><surname>Schlagenhauf</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Grace</surname>, <given-names>A. A.</given-names></string-name>, &amp; <string-name><surname>Waltz</surname>, <given-names>J. A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Towards a unifying cognitive, neurophysiological, and computational neuroscience account of schizophrenia</article-title>. <source>Schizophrenia Bulletin</source>, <volume>45</volume>(<issue>5</issue>), <fpage>1092</fpage>–<lpage>1100</lpage>.</mixed-citation></ref>
    <ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hong</surname>, <given-names>S.-J.</given-names></string-name>, <string-name><surname>Vos de Wael</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Bethlehem</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Lariviere</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Paquola</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Valk</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Milham</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Di Martino</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Margulies</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Smallwood</surname>, <given-names>J.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2019</year>). <article-title>Atypical functional connectome hierarchy in autism</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), <fpage>1022</fpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hopfield</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>1982</year>). <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>79</volume>(<issue>8</issue>), <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hopfield</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Feinstein</surname>, <given-names>D. I.</given-names></string-name>, &amp; <string-name><surname>Palmer</surname>, <given-names>R. G.</given-names></string-name></person-group> (<year>1983</year>). <article-title>‘Unlearning’ has a stabilizing effect in collective memories</article-title>. <source>Nature</source>, <volume>304</volume>(<issue>5922</issue>), <fpage>158</fpage>–<lpage>159</lpage>. <pub-id pub-id-type="doi">10.1038/304158a0</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huntenburg</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Bazin</surname>, <given-names>P.-L.</given-names></string-name>, &amp; <string-name><surname>Margulies</surname>, <given-names>D. S.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Large-scale gradients in human cortical organization</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>22</volume>(<issue>1</issue>), <fpage>21</fpage>–<lpage>31</lpage>.</mixed-citation></ref>
    <ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hutchison</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Womelsdorf</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Calhoun</surname>, <given-names>V. D.</given-names></string-name>, <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Della Penna</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Duyn</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Gonzalez-Castillo</surname>, <given-names>J.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2013</year>). <article-title>Dynamic functional connectivity: promise, issues, and interpretations</article-title>. <source>Neuroimage</source>, <volume>80</volume>, <fpage>360</fpage>–<lpage>378</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ito</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kulkarni</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Schultz</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Mill</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>R. H.</given-names></string-name>, <string-name><surname>Solomyak</surname>, <given-names>L. I.</given-names></string-name>, &amp; <string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Cognitive task information is transferred between brain regions via resting-state network topology</article-title>. <source>Nature Communications</source>, <volume>8</volume>(<issue>1</issue>), <fpage>1027</fpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanter</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Sompolinsky</surname>, <given-names>H.</given-names></string-name></person-group> (<year>1987</year>). <article-title>Associative recall of memory without errors</article-title>. <source>Physical Review A</source>, <volume>35</volume>(<issue>1</issue>), <fpage>380</fpage>–<lpage>392</lpage>. <pub-id pub-id-type="doi">10.1103/physreva.35.380</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khona</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Fiete</surname>, <given-names>I. R.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Attractor and integrator networks in the brain</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>23</volume>(<issue>12</issue>), <fpage>744</fpage>–<lpage>766</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koiran</surname>, <given-names>P.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Dynamics of discrete time, continuous state Hopfield networks</article-title>. <source>Neural Computation</source>, <volume>6</volume>(<issue>3</issue>), <fpage>459</fpage>–<lpage>468</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Douglas</surname>, <given-names>P. K.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Cognitive computational neuroscience</article-title>. <source>Nature Neuroscience</source>, <volume>21</volume>(<issue>9</issue>), <fpage>1148</fpage>–<lpage>1160</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kringelbach</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Brain States and Transitions: Insights from Computational Neuroscience</article-title>. <source>Cell Reports</source>, <volume>32</volume>(<issue>10</issue>), <fpage>108128</fpage>. <pub-id pub-id-type="doi">10.1016/j.celrep.2020.108128</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Duyn</surname>, <given-names>J. H.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Time-varying functional network information extracted from brief instances of spontaneous brain activity</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>110</volume>(<issue>11</issue>), <fpage>4392</fpage>–<lpage>4397</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>Y.-Y.</given-names></string-name>, <string-name><surname>Slotine</surname>, <given-names>J.-J.</given-names></string-name>, &amp; <string-name><surname>Barabási</surname>, <given-names>A.-L.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Controllability of complex networks</article-title>. <source>Nature</source>, <volume>473</volume>(<issue>7346</issue>), <fpage>167</fpage>–<lpage>173</lpage>.</mixed-citation></ref>
    <ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Margulies</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Ghosh</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Goulas</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Falkiewicz</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Huntenburg</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Langs</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bezgin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Castellanos</surname>, <given-names>F. X.</given-names></string-name>, <string-name><surname>Petrides</surname>, <given-names>M.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2016</year>). <article-title>Situating the default-mode network along a principal gradient of macroscale cortical organization</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>44</issue>), <fpage>12574</fpage>–<lpage>12579</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCormick</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Arnemann</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Ito</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hanson</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Latent functional connectivity underlying multiple brain states</article-title>. <source>Network Neuroscience</source>, <volume>6</volume>(<issue>2</issue>), <fpage>570</fpage>–<lpage>590</lpage>. <pub-id pub-id-type="doi">10.1162/netn_a_00234</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Medrano</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Zeidman</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Linking fast and slow: The case for generative models</article-title>. <source>Network Neuroscience</source>, <volume>8</volume>(<issue>1</issue>), <fpage>24</fpage>–<lpage>43</lpage>. <pub-id pub-id-type="doi">10.1162/netn_a_00343</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meer</surname>, <given-names>J. N. van der</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Sonkusare</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Cocchi</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Movie viewing elicits rich and reliable brain state dynamics</article-title>. <source>Nature Communications</source>, <volume>11</volume>(<issue>1</issue>), <fpage>5004</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mill</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Hamilton</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Winfield</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Lalta</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>R. H.</given-names></string-name>, &amp; <string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Network modeling of dynamic brain interactions predicts emergence of neural information that supports human cognitive behavior</article-title>. <source>PLoS Biology</source>, <volume>20</volume>(<issue>8</issue>), <elocation-id>e3001686</elocation-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Millidge</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Salvatori</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Lukasiewicz</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Bogacz</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2022</year>). <article-title>A Theoretical Framework for Inference and Learning in Predictive Coding Networks</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/ARXIV.2207.12316</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Demirtaş</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Anticevic</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Biophysical modeling of large-scale brain dynamics and applications for computational psychiatry</article-title>. <source>Biological Psychiatry: Cognitive Neuroscience and Neuroimaging</source>, <volume>3</volume>(<issue>9</issue>), <fpage>777</fpage>–<lpage>787</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Padmanabhan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lynch</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Schaer</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Menon</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2017</year>). <article-title>The default mode network in autism</article-title>. <source>Biological Psychiatry: Cognitive Neuroscience and Neuroimaging</source>, <volume>2</volume>(<issue>6</issue>), <fpage>476</fpage>–<lpage>486</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Papadopoulos</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>J. Z.</given-names></string-name>, <string-name><surname>Kurths</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Development of structural correlations and synchronization from adaptive rewiring in networks of Kuramoto oscillators</article-title>. <source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source>, <volume>27</volume>(<issue>7</issue>).</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Plakhov</surname>, <given-names>A. Yu</given-names></string-name></person-group>. (n.d.). <article-title>The converging unlearning algorithm for the Hopfield neural network: optimal strategy</article-title>. <conf-name>Proceedings of the 12th IAPR International Conference on Pattern Recognition (Cat. No.94CH3440-5)</conf-name>, <volume>2</volume>, <fpage>104</fpage>–<lpage>106</lpage>. <pub-id pub-id-type="doi">10.1109/icpr.1994.576884</pub-id> <year>1994</year></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poerio</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Karapanagiotidis</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2025</year>). <article-title>The default mode network and the complex dynamics of ongoing experience: an attractor-state perspective</article-title>. <source>Current Opinion in Behavioral Sciences</source>, <volume>65</volume>, <fpage>101546</fpage>. <pub-id pub-id-type="doi">10.1016/j.cobeha.2025.101546</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raichle</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>MacLeod</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Powers</surname>, <given-names>W. J.</given-names></string-name>, <string-name><surname>Gusnard</surname>, <given-names>D. A.</given-names></string-name>, &amp; <string-name><surname>Shulman</surname>, <given-names>G. L.</given-names></string-name></person-group> (<year>2001</year>). <article-title>A default mode of brain function</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>98</volume>(<issue>2</issue>), <fpage>676</fpage>–<lpage>682</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanchez-Romero</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Ito</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Mill</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Hanson</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Causally informed activity flow models provide mechanistic insight into network-generated cognitive activations</article-title>. <source>NeuroImage</source>, <volume>278</volume>, <fpage>120300</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120300</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scheid</surname>, <given-names>B. H.</given-names></string-name>, <string-name><surname>Ashourvan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stiso</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Davis</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Mikhail</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Pasqualetti</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Litt</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Time-evolving controllability of effective connectivity networks during seizure progression</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>5</issue>), <elocation-id>e2006436118</elocation-id>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schiff</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Jerger</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Duong</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Spano</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Ditto</surname>, <given-names>W. L.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Controlling chaos in the brain</article-title>. <source>Nature</source>, <volume>370</volume>(<issue>6491</issue>), <fpage>615</fpage>–<lpage>620</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schirner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kong</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Yeo</surname>, <given-names>B. T.</given-names></string-name>, <string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Ritter</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Dynamic primitives of brain network interaction</article-title>. <source>NeuroImage</source>, <volume>250</volume>, <fpage>118928</fpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seguin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Zalesky</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Brain network communication: concepts, models and applications</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>24</volume>(<issue>9</issue>), <fpage>557</fpage>–<lpage>574</lpage>.</mixed-citation></ref>
    <ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shinn</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Noble</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Preller</surname>, <given-names>K. H.</given-names></string-name>, <string-name><surname>Ji</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Moujaes</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Achard</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2023</year>). <article-title>Functional brain networks reflect spatial and temporal autocorrelation</article-title>. <source>Nature Neuroscience</source>, <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
    <ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Auerbach</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2012</year>). <article-title>Temporally-independent functional modes of spontaneous brain activity</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>8</issue>), <fpage>3131</fpage>–<lpage>3136</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Spisak</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2025</year>). <article-title>Self-orthogonalizing attractor neural networks emerging from the free energy principle</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/ARXIV.2505.22749</pub-id></mixed-citation></ref>
    <ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sydnor</surname>, <given-names>V. J.</given-names></string-name>, <string-name><surname>Larsen</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Alexander-Bloch</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fair</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Liston</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mackey</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Milham</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Pines</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Roalf</surname>, <given-names>D. R.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2021</year>). <article-title>Neurodevelopment of the association cortices: Patterns, mechanisms, and implications for psychopathology</article-title>. <source>Neuron</source>, <volume>109</volume>(<issue>18</issue>), <fpage>2820</fpage>–<lpage>2846</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Brain network dynamics are hierarchically organized in time</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>48</issue>), <fpage>12827</fpage>–<lpage>12832</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vohryzek</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Deco</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Cessac</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Kringelbach</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Cabral</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Ghost attractors in spontaneous brain activity: Recurrent excursions into functionally-relevant BOLD phase-locking states</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>14</volume>, <fpage>20</fpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wagner</surname>, <given-names>D. H.</given-names></string-name></person-group> (<year>1989</year>). <article-title>The Existence and Behavior of Viscous Structure for Plane Detonation Waves</article-title>. <source>SIAM Journal on Mathematical Analysis</source>, <volume>20</volume>(<issue>5</issue>), <fpage>1035</fpage>–<lpage>1054</lpage>. <pub-id pub-id-type="doi">10.1137/0520069</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woo</surname>, <given-names>C.-W.</given-names></string-name>, <string-name><surname>Roy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Buhle</surname>, <given-names>J. T.</given-names></string-name>, &amp; <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Distinct brain systems mediate the effects of nociceptive input and self-regulation on pain</article-title>. <source>PLoS Biology</source>, <volume>13</volume>(<issue>1</issue>), <elocation-id>e1002036</elocation-id>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zalesky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fornito</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cocchi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gollo</surname>, <given-names>L. L.</given-names></string-name>, &amp; <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Time-resolved resting-state brain networks</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>28</issue>), <fpage>10341</fpage>–<lpage>10346</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zunhammer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Spisák</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name><surname>Bingel</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Meta-analysis of neural systems underlying placebo analgesia from individual participant fMRI data</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>), <fpage>1391</fpage>.</mixed-citation></ref>
    <ref id="dataref1"><mixed-citation publication-type="data" specific-use="analyzed"><person-group person-group-type="author"><string-name><surname>Spisak</surname><given-names>T.</given-names></string-name> <etal>et al</etal></person-group> (<year iso-8601-date="2020">2020</year>) <article-title>RPN - Study 1</article-title>. <source>OpenNeuro</source>. <pub-id pub-id-type="doi">10.18112/openneuro.ds002608.v1.0.2</pub-id> <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002608">https://openneuro.org/datasets/ds002608</ext-link> </mixed-citation></ref>
    <ref id="dataref2"><mixed-citation publication-type="data" specific-use="analyzed"><person-group person-group-type="author"><string-name><surname>Spisak</surname><given-names>T.</given-names></string-name> <etal>et al</etal></person-group> (<year iso-8601-date="2020">2020</year>) <article-title>RPN - Study 2</article-title>. <source>OpenNeuro</source>. <pub-id pub-id-type="doi">10.18112/openneuro.ds002609.v1.0.3</pub-id> <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002609">https://openneuro.org/datasets/ds002609</ext-link></mixed-citation></ref>
    <ref id="dataref3"><mixed-citation publication-type="data" specific-use="analyzed"><person-group person-group-type="author"><string-name><surname>Woo</surname><given-names>C.-W.</given-names></string-name> <etal>et al</etal></person-group> (<year iso-8601-date="2018">2018</year>) <article-title>Distinct brain systems mediate the effects of nociceptive input and self-regulation on pain</article-title>. <source>OpenNeuro</source>. <pub-id pub-id-type="accession">ds000140</pub-id> <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds000140/versions/00001">https://openneuro.org/datasets/ds000140/versions/00001</ext-link></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abraham</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gervais</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mueller</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kossaifi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Machine learning for neuroimaging with scikit-learn</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>8</volume>, <fpage>14</fpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avants</surname>, <given-names>B. B.</given-names></string-name>, <string-name><surname>Tustison</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Cook</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Klein</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Gee</surname>, <given-names>J. C.</given-names></string-name></person-group> (<year>2011</year>). <article-title>A reproducible evaluation of ANTs similarity metric performance in brain image registration</article-title>. <source>Neuroimage</source>, <volume>54</volume>(<issue>3</issue>), <fpage>2033</fpage>–<lpage>2044</lpage>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Behzadi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Restom</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Liau</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T. T.</given-names></string-name></person-group> (<year>2007</year>). <article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title>. <source>Neuroimage</source>, <volume>37</volume>(<issue>1</issue>), <fpage>90</fpage>–<lpage>101</lpage>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bellec</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rosa-Neto</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lyttelton</surname>, <given-names>O. C.</given-names></string-name>, <string-name><surname>Benali</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Evans</surname>, <given-names>A. C.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Multi-level bootstrap analysis of stable clusters in resting-state fMRI</article-title>. <source>Neuroimage</source>, <volume>51</volume>(<issue>3</issue>), <fpage>1126</fpage>–<lpage>1139</lpage>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cox</surname>, <given-names>R. W.</given-names></string-name></person-group> (<year>1996</year>). <article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title>. <source>Computers and Biomedical Research</source>, <volume>29</volume>(<issue>3</issue>), <fpage>162</fpage>–<lpage>173</lpage>.</mixed-citation></ref>
    <ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Craddock</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sikka</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Cheung</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Khanuja</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Ghosh</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Lurie</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Vogelstein</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Burns</surname>, <given-names>R.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2013</year>). <article-title>Towards automated analysis of connectomes: The configurable pipeline for the analysis of connectomes (c-pac)</article-title>. <source>Front Neuroinform</source>, <volume>42</volume>(10.3389).</mixed-citation></ref>
    <ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dadi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Rahim</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Abraham</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chyzhyk</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Milham</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Initiative</surname>, <given-names>A. D. N.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2019</year>). <article-title>Benchmarking functional connectome-based predictive models for resting-state fMRI</article-title>. <source>NeuroImage</source>, <volume>192</volume>, <fpage>115</fpage>–<lpage>134</lpage>.</mixed-citation></ref>
    <ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Martino</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>C.-G.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Denio</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Castellanos</surname>, <given-names>F. X.</given-names></string-name>, <string-name><surname>Alaerts</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Assaf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bookheimer</surname>, <given-names>S. Y.</given-names></string-name>, <string-name><surname>Dapretto</surname>, <given-names>M.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2014</year>). <article-title>The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</article-title>. <source>Molecular Psychiatry</source>, <volume>19</volume>(<issue>6</issue>), <fpage>659</fpage>–<lpage>667</lpage>.</mixed-citation></ref>
    <ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteban</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Markiewicz</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Blair</surname>, <given-names>R. W.</given-names></string-name>, <string-name><surname>Moodie</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Isik</surname>, <given-names>A. I.</given-names></string-name>, <string-name><surname>Erramuzpe</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kent</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Goncalves</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>DuPre</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>M.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2019</year>). <article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title>. <source>Nature Methods</source>, <volume>16</volume>(<issue>1</issue>), <fpage>111</fpage>–<lpage>116</lpage>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Howard</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Frackowiak</surname>, <given-names>R. S.</given-names></string-name>, &amp; <string-name><surname>Turner</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Movement-related effects in fMRI time-series</article-title>. <source>Magnetic Resonance in Medicine</source>, <volume>35</volume>(<issue>3</issue>), <fpage>346</fpage>–<lpage>355</lpage>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gorgolewski</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Burns</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Madison</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Clark</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Halchenko</surname>, <given-names>Y. O.</given-names></string-name>, <string-name><surname>Waskom</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Ghosh</surname>, <given-names>S. S.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>5</volume>, <fpage>13</fpage>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Fsl</article-title>. <source>Neuroimage</source>, <volume>62</volume>(<issue>2</issue>), <fpage>782</fpage>–<lpage>790</lpage>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ledoit</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Wolf</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2004</year>). <article-title>A well-conditioned estimator for large-dimensional covariance matrices</article-title>. <source>Journal of Multivariate Analysis</source>, <volume>88</volume>(<issue>2</issue>), <fpage>365</fpage>–<lpage>411</lpage>. <pub-id pub-id-type="doi">10.1016/s0047-259x(03)00096-4</pub-id></mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Power</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Barnes</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Schlaggar</surname>, <given-names>B. L.</given-names></string-name>, &amp; <string-name><surname>Petersen</surname>, <given-names>S. E.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</article-title>. <source>Neuroimage</source>, <volume>59</volume>(<issue>3</issue>), <fpage>2142</fpage>–<lpage>2154</lpage>.</mixed-citation></ref>
    <ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Satterthwaite</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Elliott</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Gerraty</surname>, <given-names>R. T.</given-names></string-name>, <string-name><surname>Ruparel</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Loughead</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Calkins</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Hakonarson</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Gur</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Gur</surname>, <given-names>R. E.</given-names></string-name> <etal>et al</etal></person-group>. (<year>2013</year>). <article-title>An improved framework for confound regression and filtering for control of motion artifact in the preprocessing of resting-state functional connectivity data</article-title>. <source>Neuroimage</source>, <volume>64</volume>, <fpage>240</fpage>–<lpage>256</lpage>.</mixed-citation></ref>
    <ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spisak</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kincses</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Schlitt</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Zunhammer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schmidt-Wilcke</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kincses</surname>, <given-names>Z. T.</given-names></string-name>, &amp; <string-name><surname>Bingel</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Pain-free resting-state functional brain connectivity predicts individual pain sensitivity</article-title>. <source>Nature Communications</source> <volume>11</volume> (<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/s41467-019-13785-z</pub-id></mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tor</surname> <given-names>D. W.</given-names></string-name></person-group> (<year>2011</year>). <article-title>NeuroSynth: a new platform for large-scale automated synthesis of human functional neuroimaging data</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>5</volume>. <pub-id pub-id-type="doi">10.3389/conf.fninf.2011.08.00058</pub-id></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woo</surname>, <given-names>C.-W.</given-names></string-name>, <string-name><surname>Roy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Buhle</surname>, <given-names>J. T.</given-names></string-name>, &amp; <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Distinct brain systems mediate the effects of nociceptive input and self-regulation on pain</article-title>. <source>PLoS Biology</source>, <volume>13</volume>(<issue>1</issue>), <elocation-id>e1002036</elocation-id>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woo</surname>, <given-names>C.-W.</given-names></string-name>, <string-name><surname>Roy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Buhle</surname>, <given-names>J. T.</given-names></string-name>, &amp; <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Distinct Brain Systems Mediate the Effects of Nociceptive Input and Self-Regulation on Pain</article-title>. <source>PLoS Biology</source>, <volume>13</volume>(<issue>1</issue>), <elocation-id>e1002036</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pbio.1002036</pub-id></mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarkoni</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title>. <source>Nature Methods</source>, <volume>8</volume>(<issue>8</issue>), <fpage>665</fpage>–<lpage>670</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.1635</pub-id></mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zunhammer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Spisák</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name><surname>Bingel</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Meta-analysis of neural systems underlying placebo analgesia from individual participant fMRI data</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<issue>1</issue>), <fpage>1391</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98725.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Petkoski</surname>
<given-names>Spase</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/019kqby73</institution-id><institution>Institut de Neurosciences des Systèmes</institution>
</institution-wrap>
<city>Marseille</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a <bold>valuable</bold> approach for revealing large-scale brain attractor dynamics during resting states, task processing, and disease conditions using insights from Hopfield neural networks. The evidence supporting the findings is <bold>convincing</bold> across the many datasets analysed. The work will be of broad interest to neuroscientists using neuroimaging data with interest in computational modelling of brain activity.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98725.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Englert et al. proposed a functional connectivity-based Attractor Neural Network (fcANN) to reveal attractor states and activity flows across various conditions, including resting state, task-evoked, and pathological conditions. The large-scale brain attractors reconstructed by fcANNs are orthogonal organization, which is in line with the free-energy theoretical framework. Additionally, the fcANN demonstrates differences in attractor states between individuals with autism and typically developing individuals.</p>
<p>The study used seven datasets, which ensures robust replication and validation of generalization across various conditions. The study is a representative example that combines experimental evidence based on fcANN and the theoretical framework. The fcANN projection offers an interesting way of visualization, allowing researchers to observe attractor states and activity flow patterns directly. Overall, the study may offer valuable insights into brain dynamics and computational neuroscience.</p>
<p>Comments on revision:</p>
<p>The authors have addressed my previous concerns and substantially improved the manuscript. Fig.4 and Fig.5 still keep fcHNN rather than the updated fcANN.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98725.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Englert et al. use a novel modelling approach called functional connectome-based Hopfield Neural Networks (fcHNN) to describe spontaneous and task-evoked brain activity, and the alterations in brain disorders. Given its novelty, the authors first validate the model parameters (the temperature and noise) with empirical resting-state function data and against null models. Through the optimisation of the temperature parameter, they first show that the optimal number of attractor states is four before fixing the optimal noise that best reflects the empirical data, through stochastic relaxation. Then, they demonstrate how these fcHNN generated dynamics predict task-based functional activity relating to pain and self-regulation. To do so, they characterise the different brain states (here as different conditions of the experimental pain paradigm) in terms of the distribution of the data on the fcHNN projections and flow-analysis. Lastly, a similar analysis was performed on a population with autism condition. Through Hopfield modeling, this work proposes a comprehensive framework that links various types of functional activity under a unified interpretation with high predictive validity.</p>
<p>Strengths:</p>
<p>The phenomenological nature of the Hopfield model and its validation across multiple datasets presents a comprehensive and intuitive framework for the analysis of functional activity. The results presented in this work further motivate the study of phenomenological models as an adequate mechanistic characterisation of large-scale brain activity.</p>
<p>Following up from Cole et al. 2016, the authors put forward a hypothesis that many of the changes to the brain activity, here, in terms of task-evoked and clinical data, can be inferred from the resting-state brain data alone. This brings together neatly the idea of different facets of brain activity emerging from a common space of functional (ghost) attractors.</p>
<p>The use of the null models motivates the benefit for non-linear dynamics in the context of phenomenological models when assessing the similarity to the real empirical data.</p>
<p>Comments on revision:</p>
<p>I am happy with how the authors addressed the comments and am happy to move ahead without further comments.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98725.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Englert</surname>
<given-names>Robert</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kincses</surname>
<given-names>Balint</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kotikalapudi</surname>
<given-names>Raviteja</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4604-3367</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Gallitto</surname>
<given-names>Giuseppe</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Li</surname>
<given-names>Jialin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hoffschlag</surname>
<given-names>Kevin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Woo</surname>
<given-names>Choong-Wan</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7423-5422</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Wager</surname>
<given-names>Tor D</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Timmann</surname>
<given-names>Dagmar</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bingel</surname>
<given-names>Ulrike</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Spisak</surname>
<given-names>Tamas</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2942-0821</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>Englert et al. proposed a functional connectome-based Hopfield artificial neural network (fcHNN) architecture to reveal attractor states and activity flows across various conditions, including resting state, task-evoked, and pathological conditions. The fcHNN can reconstruct characteristics of resting-state and task-evoked brain activities. Additionally, the fcHNN demonstrates differences in attractor states between individuals with autism and typically developing individuals.</p>
<p>Strengths:</p>
<p>(1) The study used seven datasets, which somewhat ensures robust replication and validation of generalization across various conditions.</p>
<p>(2) The proposed fcHNN improves upon existing activity flow models by mimicking artificial neural networks, thereby enhancing the representational ability of the model. This advancement enables the model to more accurately reconstruct the dynamic characteristics of brain activity.</p>
<p>(3) The fcHNN projection offers an interesting visualization, allowing researchers to observe attractor states and activity flow patterns directly.</p>
</disp-quote>
<p>We are grateful to the reviewer for highlighting the robustness of our findings across multiple datasets and for appreciating the novelty and representational advantages of our fcHNN model (which has been renamed to fcANN in the revised manuscript).</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) The fcHNN projection can offer low-dimensional dynamic visualizations, but its interpretability is limited, making it difficult to make strong claims based on these projections. The interpretability should be enhanced in the results and discussion.</p>
</disp-quote>
<p>We thank the reviewer for these important points. We agree that the interpretability of the low-dimensional projection is limited. In the revised manuscript, we have reframed the fcANN projection primarily as a visualization tool (see e.g. line 359) and moved the corresponding part of Figure 2 to the Supplementary Material (Supplementary Figure 2). We have also implemented a substantial revision of the manuscript, which now directly links our analysis to the novel theoretical framework of self-orthogonalizing attractor networks (Spisak &amp; Friston, 2025), opening several new avenues in terms of interpretation and shedding light on the computational principles underlying attractor dynamics in the brain (see the revised introduction and the new section “Theoretical background”, starting at lines 128, but also the Mathematical Appendices 1-2 in the Supplementary Material for a comprehensive formal derivation). As part of these efforts, we now provide evidence for the brain’s functional organization approximating a special, computationally efficient class of attractor networks, the so-called Kanter-Sompolinsky projector network (Figure 2A-C, line 346, see also our answer to your next comment). This is exactly, what the theoretical framework of free-energy-minimizing attractor networks predicts.</p>
<disp-quote content-type="editor-comment">
<p>(2) The presentation of results is not clear enough, including figures, wording, and statistical analysis, which contributes to the overall difficulty in understanding the manuscript. This lack of clarity in presenting key findings can obscure the insights that the study aims to convey, making it challenging for readers to fully grasp the implications and significance of the research.</p>
</disp-quote>
<p>We have thoroughly revised the manuscript for clarity in wording, figures (see e.g. lines 257, 482, 529 in the Results and lines 1128, 1266, 1300, 1367 in the Methods). We carefully improved statistical reporting and ensured that we always report test statistics, effect sizes and clearly refer to the null modelling approach used (e.g. lines 461, 542, 550, 565, 573, 619, as well as Figures 2-4). As absolute effect sizes, in many analyses, do not have a straightforward interpretation, we provided Glass’ , as a standardized effect size measure, expressing the distance of the true observation from the null distribution as a ratio of the null standard deviation. To further improve clarity, we now clearly define our research questions and the corresponding analyses and null models in the revised manuscript, both in the main text and in two new tables (Tables 1 and 2). We denoted research questions and null model with Q1-7 and NM1-5, respectively and refer to them at multiple instances when detailing the analyses and the results.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>Englert et al. use a novel modelling approach called functional connectome-based Hopfield Neural Networks (fcHNN) to describe spontaneous and task-evoked brain activity and the alterations in brain disorders. Given its novelty, the authors first validate the model parameters (the temperature and noise) with empirical resting-state function data and against null models. Through the optimisation of the temperature parameter, they first show that the optimal number of attractor states is four before fixing the optimal noise that best reflects the empirical data, through stochastic relaxation. Then, they demonstrate how these fcHNN-generated dynamics predict task-based functional activity relating to pain and self-regulation. To do so, they characterise the different brain states (here as different conditions of the experimental pain paradigm) in terms of the distribution of the data on the fcHNN projections and flow analysis. Lastly, a similar analysis was performed on a population with autism condition. Through Hopfield modeling, this work proposes a comprehensive framework that links various types of functional activity under a unified interpretation with high predictive validity.</p>
<p>Strengths:</p>
<p>The phenomenological nature of the Hopfield model and its validation across multiple datasets presents a comprehensive and intuitive framework for the analysis of functional activity. The results presented in this work further motivate the study of phenomenological models as an adequate mechanistic characterisation of large-scale brain activity.</p>
<p>Following up on Cole et al. 2016, the authors put forward a hypothesis that many of the changes to the brain activity, here, in terms of task-evoked and clinical data, can be inferred from the resting-state brain data alone. This brings together neatly the idea of different facets of brain activity emerging from a common space of functional (ghost) attractors.</p>
<p>The use of the null models motivates the benefit of non-linear dynamics in the context of phenomenological models when assessing the similarity to the real empirical data.</p>
</disp-quote>
<p>We thank the reviewer for recognizing the comprehensive and intuitive nature of our framework and for acknowledging the strength of our hypothesis that diverse brain activity facets emerge from a common resting state attractor landscape.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>While the use of the Hopfield model is neat and very well presented, it still begs the question of why to use the functional connectome (as derived by activity flow analysis from Cole et al. 2016). Deriving the functional connectome on the resting-state data that are then used for the analysis reads as circular.</p>
</disp-quote>
<p>We agree that starting from functional couplings to study dynamics is in stark contrast with the common practice of estimating the interregional couplings based on structural connectome data. We now explicitly discuss how this affects the scope of the questions we can address with the approach, with explicit notes on the inability of this approach to study the structure-function coupling and its limitations in deriving mechanistic insights at the level of biophysical implementation.</p>
<p>Line 894:</p>
<p>“The proposed approach is not without limitations. First, as the proposed approach does not incorporate information about anatomical connectivity and does not explitly model biophysical details. Thus, in its present form, the model is not suitable to study the structure-function coupling and cannot yiled mechanistic explanations underlying (altered) polysynaptic connections, at the level of biophysical details.”</p>
<p>We are confident, however, that our approach is not circular. At the high level, our approach can be considered as a function-to-function generative model, with twofold aims.</p>
<p>First, we link large-scale brain dynamics to theoretical artificial neural network models and show that the functional connectome display characteristics that render it as an exceptionally “well-behaving” attractor network (e.g. superior convergence properties, as contrasted against appropriate respective null models). In the revised manuscript, we have significantly improved upon this aspect by explicitly linking the fcANN model to the theoretical framework of self-orthogonalizing attractor networks (Spisak &amp; Friston, 2025) (see the revised introduction and the new section “Theoretical background”, starting at lines 128, but also the Mathematical Appendices 1-2 in the Supplementary Material for a comprehensive formal derivation). As part of these efforts, we now provide evidence for the brain’s functional organization approximating a special, computationally efficient class of attractor networks, the so-called Kanter-Sompolinsky projector network (Figure 2A-C, line 346, see also our answer to your next comment). This is exactly, what the theoretical framework of free-energy-minimizing attractor networks predicts. This result is not circular, as the empirical model does not use the key mechanism (the Hebbian/anti-Hebbian learning rule) that induces self-orthogonalization in the theoretical framework. We clarify this in the revised manuscript, e.g. in line 736.</p>
<p>Second, we benchmark ability of the proposed function-to-function generative model to predict unseen data (new datasets) or data characteristics that are not directly encompassed in the connectivity matrix (e.g. non-Gaussian conditional dependencies, temporal autocorrelation, dynamical responses to perturbations on the system). These benchmarks are constructed against well defined null models, which provide reasonable references. We have now significantly improved the discussion of these null models in the revised manuscript (Tables 1 and 2, lines 257). We not only show, that our model - when reconstructing resting state dynamics - can generalize to unseen data over and beyond what is possible with the baseline descriptive measure (e.g. covariance measures and PCA), but also demonstrate the ability of the framework to reconstruct the effects of perturbations on this dynamics (such as task-evoked changes), based solely on the resting state data form another sample.</p>
<disp-quote content-type="editor-comment">
<p>If the fcHNN derives the basins of four attractors that reflect the first two principal components of functional connectivity, it perhaps suffices to use the empirically derived components alone and project the task and clinical data on it without the need for the fcHNN framework.</p>
</disp-quote>
<p>We are thankful for the reviewer for highlighting this important point, which encouraged us to develop a detailed understanding of the origins of the close alignment between attractors and principal components (eigenvectors of the coupling matrix) and the corresponding (approximate) orthogonality. Here, we would like to emphasize that the attractor-eigenvector correspondence is by no means a general feature of any arbitrary attractor network. In fact, such networks are a very special class of attractor neural networks (the so-called Kanter-Sompolinsky projector neural network (Kanter &amp; Sompolinsky, 1987)), with a high degree of computational efficiency, maximal memory capacity and perfect memory recall. It has been rigorously shown that in such networks, the eigenvectors of the coupling matrix (i.e. PCA on the timeseries data) and the attractors become equivalent (Kanter &amp; Sompolinsky, 1987). This in turn made us ask the question, what are the learning and plasticity rules that drive attractor networks towards developing approximately orthogonal attractors? We found that this is a general tendency of networks obeying the free energy principle ( Figure 2A-C, line 346, see also our answer to your next comment). The formal derivation of this framework in now presented in an accompanying theoretical piece (Spisak &amp; Friston, 2025). In the revised manuscript, we provide a short, high-level overview of these results (in the Introduction form line 55 and in the new section “Theoretical background”, line 128, but also the Mathematical Appendices 1-2 in the Supplementary Material for a comprehensive formal derivation). According to this new theoretical model, attractor states can be understood as a set of priors (in the Bayesian sense) that together constitute an optimal orthogonal basis, equipping the update process (which is akin to a Markov-chain Monte Carlo sampling) to find posteriors that generalize effectively within the spanned subspace. Thus, in sum, understanding brain function in terms of attractor dynamics - instead of PCA-like descriptive projections - provides important links towards a Bayesian interpretation of brain activity. At the same time, the eigenvector-attractor correspondence also explains, why descriptive decomposition approaches, like PCA or ICA are so effective at capturing the dynamics of the system, at the first place.</p>
<disp-quote content-type="editor-comment">
<p>As presented here, the Hopfield model is excellent in its simplicity and power, and it seems suited to tackle the structure-function relationship with the power of going further to explain task-evoked and clinical data. The work could be strengthened if that was taken into consideration. As such the model would not suffer from circularity problems and it would be possible to claim its mechanistic properties. Furthermore, as mentioned above, in the current setup, the connectivity matrix is based on statistical properties of functional activity amongst regions, and as such it is difficult to talk about a certain mechanism. This contention has for example been addressed in the Cole et al. 2016 paper with the use of a biophysical model linking structure and function, thus strengthening the mechanistic claim of the work.</p>
</disp-quote>
<p>We agree that investigating how the structural connectome constraints macro-scale dynamics is a crucial next step. Linking our results with the theoretical framework of self-orthogonalizing attractor networks provides a principled approach to this, as the “self-orthogonalizing” learning rule in the accompanying theoretical work provides the opportunity to fit attractor networks with structural constraints to functional data, shedding light on the plastic processes which maintain the observed approximate orthogonality even in the presence of these structural constraints. We have revised the manuscript to clarify that our phenomenological approach is inherently limited in its ability to answer mechanistic questions at the level of biophysical details (lines 894) and discuss this promising direction as follows:</p>
<p>Lines 803:</p>
<p>“A promising application of this is to consider structural brain connectivity (as measured by diffusion MRI) as a sparsity constraint for the coupling weights and then train the fcANN model to match the observed resting-state brain dynamics. If the resulting structural-functional ANN model is able to closely match the observed functional brain substate dynamics, it can be used as a novel approach to quantify and understand the structural functional coupling in the brain”.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>(1) The statistical analyses are poorly described throughout the manuscript. The authors should provide more details on the statistical methods used for each comparison, as well as the corresponding statistics and degrees of freedom, rather than solely reporting p-values.</p>
</disp-quote>
<p>We thank the reviewer for pointing this out. We have revised the manuscript to include the specific test statistics, precise p-values and raw effect sizes for all reported analyses to ensure full transparency and replicability, see e.g. lines 461, 542, 550, 565, 573, 619, as well as Figures 2-4. Additionally, as absolute effect sizes - in many analyses - do not have a straightforward interpretation, we provided Glass’ Δ, as a standardized effect size measure, expressing the distance of the true observation from the null distribution as a ratio of the null standard deviation.</p>
<p>We have also improved the description of the statistical methods used in the manuscript (lines 1270, 1306, 1339, 1367, 1404) and added two overview tables (Tables 1 and 2) that summarize the methodological approaches and the corresponding null models.</p>
<p>Furthermore, we have fully revised the analysis corresponding to noise optimization. We only retained null model 2 (covariance-matched Gaussian) in the main text and on Figure 3, and moved model 1 (spatial phase randomization) into the Supplementary Material (Supplementary Figure 6) and is less appropriate for this analysis (trivially significant in all cases). Furthermore, as test statistic, no we use a Wasserstein distance between the 122-dimensional empirical and the simulated data (instead of focusing on the 2-dimensional projection). This analysis now directly quantifies the capacity of the fcANN model to capture non-Gaussian conditionals in the data.</p>
<disp-quote content-type="editor-comment">
<p>(2) The convergence procedure is not clearly explained in the manuscript. Is this an optimization procedure to minimize energy? If so, the authors should provide more details about the optimizer used.</p>
</disp-quote>
<p>We apologize for the lack of clarity. The convergence is not an optimization procedure per se, in a sense that it does not involve any external optimizer. It is simply the repeated (deterministic) application of the same update rule also known from Hopfield networks or Boltzmann machines. However, as detailed in the accompanying theoretical paper, this update rule (or inference rule) inherently solves and optimization problem: it performs gradient descent on the free energy landscape of the network. As such, it is guaranteed to converge to a local free energy minimum in the deterministic case. We have clarified this process in the Results and Methods sections as follows:</p>
<p>Line 161:</p>
<p>“Inference arises from minimizing free energy with respect to the states \sigma. For a single unit, this yields a local update rule homologous to the relaxation dynamics in Hopfield networks”.</p>
<p>Line 181:</p>
<p>“In the basis framework (Spisak &amp; Friston, 2025), inference is a gradient descent on the variational free energy landscape with respect to the states σ and can be interpreted as a form of approximate Bayesian inference, where the expected value of the state σ<sub>i</sub> is interpreted as the posterior mean given the attractor states currently encoded in the network (serving as a macro-scale prior) and the previous state, including external inputs (serving as likelihood in the Bayesian sense)”.</p>
<p>Line 1252:</p>
<p>“As the inference rule was derived as a gradient descent on free energy, iterations monotonically decrease the free energy function and therefore converge to a local free‑energy minimum without any external optimizer. Thus, convergence does not require any optimization procedure with an external optimizer. Instead, it arises as the fixed point of repeated local inference updates, which implement gradient descent on free energy in the deterministic symmetric case.”</p>
<disp-quote content-type="editor-comment">
<p>(3) In Figure 2G, the beta values range from 0.035 to 0.06, but they are reported as 0.4 in the main text and the Supplementary Figure. Please clarify this discrepancy.</p>
</disp-quote>
<p>We are grateful to the reviewer for spotting this typo. The correct value for β is 0.04, as reported in the Methods section. We have corrected this inconsistency in the revised manuscript and as well as in Supplementary Figure 5.</p>
<disp-quote content-type="editor-comment">
<p>(4) Line 174: What type of null model was used to evaluate the impact of the beta values? The authors did not provide details on this anywhere in the manuscript.</p>
</disp-quote>
<p>We apologize for this omission. The null model is based on permuting the connectome weights while retaining the matrix symmetry, which destroys the specific topological structure but preserves the overall weight distribution. We have now clarified this at multiple places in the revised manuscript (lines 432, Table 1-2, Figure 2), and added new overview tables (Tables 1 and 2) to summarize the methodological approaches and the corresponding null models.</p>
<disp-quote content-type="editor-comment">
<p>(5) Figure 3B: It appears that the authors only demonstrate the reproducibility of the “internal” attractor across different datasets. What about other states?</p>
</disp-quote>
<p>Thank you for noticing this. We now visualize all attractor states in Figure 3B (note that these essentially consist of two symmetric pairs).</p>
<disp-quote content-type="editor-comment">
<p>(6) Figure 3: What does “empirical” represent in Figure 3? Is it PCA? If the “empirical” method, which is a much simpler method, can achieve results similar to those of the fcHNN in terms of state occupancy, distribution, and activity flow, what are the benefits of the proposed method? Furthermore, the authors claim that the explanatory power of the fcHNN is higher than that of the empirical model and shows significant differences. However, from my perspective, this difference is not substantial (37.0% vs. 39.9%). What does this signify, particularly in comparison to PCA?</p>
</disp-quote>
<p>This is a crucial point that is now a central theme of our revised manuscript. The reviewer is correct that the “empirical” method is PCA. PCA - by identifying variance-heavy orthogonal directions - aims to explain the highest amount of variance possible in the data (with the assumption of Gaussian conditionals). While empirical attractors are closely aligned to the PCs (i.e. eigenvectors of the inverse covariance matrix, as shown in the new analysis Q1), the alignment is only approximate. We basically take advantage of this small “gap” to quantify, weather attractor states are a better fit to the unseen data than the PCs. Obviously, due to the otherwise strong PC-attractor correspondence, this is expected to be only a small improvement. However, it is an important piece of evidence for the validity of our framework, as it shows that attractors are not just a complementary, perhaps “noisier” variety of the PCs, but a “substrate” that generalizes better to unseen data than the PCs themselves. We have revised the manuscript to clarify this point (lines 528).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>For clarity, it might be useful to define and use consistently certain key terms. Connectome often refers to structural (anatomical) connectivity unless defined specifically this should be considered, in Figure 1B title for example Brain state often refers to different conditions ie autism, neurotypical, sleep, etc... see for review Kringelbach et al. 2020, Cell Reports. When referring to attractors of brain activity they might be called substates.</p>
</disp-quote>
<p>We thank the reviewer for these helpful suggestions. We have carefully revised the manuscript to ensure our terminology is precise and consistent. We now explicitly refer to the “functional connectome” (including the title) and avoid using the too general term “brain state” and use “substates” instead.</p>
<disp-quote content-type="editor-comment">
<p>In Figure 2 some terms are not defined. Noise is sigma in the text but elpsilon in the figure. Only in methods, the link becomes clear. Perhaps define epsilon in the caption for clarity. The same applies to μ in the methods. It is only described above in the methods, I suggest repeating the epsilon definition for clarity</p>
</disp-quote>
<p>We appreciate this feedback and apologize for the inconsistency. We have revised all figures and the Methods section to ensure that all mathematical symbols (including ε, σ, and μ) are clearly and consistently defined upon their first appearance and in all figure captions. For instance, noise level is now consistently referred to as ϵ. We improved the consistency and clarity for other terms, too, including:</p>
<p>functional connectome-based Hopfiled network (fcHNN) =&gt; functional connectivity-based attractor network (fcANN);</p>
<p>temperature =&gt; inverse temperature;</p>
<p>And improved grammar and language throughout the manuscript.</p>
<p>References</p>
<p>Kanter, I., &amp; Sompolinsky, H. (1987). Associative recall of memory without errors. Physical Review A, 35(1), 380–392. 10.1103/physreva.35.380</p>
<p>Spisak T &amp; Friston K (2025). Self-orthogonalizing attractor neural networks emerging from the free energy principle. arXiv preprint arXiv:2505.22749.</p>
</body>
</sub-article>
</article>