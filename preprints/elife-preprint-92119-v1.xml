<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">92119</article-id>
<article-id pub-id-type="doi">10.7554/eLife.92119</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92119.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Movies reveal the fine-grained organization of infant visual cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Ellis</surname>
<given-names>C. T.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yates</surname>
<given-names>T. S.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4612-9921</contrib-id>
<name>
<surname>Arcaro</surname>
<given-names>M. J.</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Turk-Browne</surname>
<given-names>N. B.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, Stanford University</institution>, Palo Alto, CA 94304</aff>
<aff id="a2"><label>2</label><institution>Department of Psychology, Yale University</institution>, New Haven, CT 06511</aff>
<aff id="a3"><label>3</label><institution>Department of Psychology, University of Pennsylvania</institution>, Philadelphia, PA 19104</aff>
<aff id="a4"><label>4</label><institution>Wu Tsai Institute, Yale University</institution>, New Haven, CT 06511</aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Dubois</surname>
<given-names>Jessica</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Inserm Unité NeuroDiderot, Université Paris Cité</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>de Lange</surname>
<given-names>Floris P</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Donders Institute for Brain, Cognition and Behaviour</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><italic>Corresponding author and lead contact:</italic> <email>cte@stanford.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-12-19">
<day>19</day>
<month>12</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP92119</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-09-14">
<day>14</day>
<month>09</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-08-23">
<day>23</day>
<month>08</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.22.554318"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Ellis et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Ellis et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-92119-v1.pdf"/>
<abstract>
<p>Studying infant minds with movies is a promising way to increase engagement relative to traditional tasks. However, the spatial specificity and functional significance of movie-evoked activity in infants remains unclear. Here we investigated what movies can reveal about the organization of the infant visual system. We collected fMRI data from 15 awake infants aged 5–23 months who attentively watched a movie. The activity evoked by the movie reflected the functional profile of visual areas. Namely, homotopic areas from the two hemispheres responded similarly to the movie, whereas distinct areas responded dissimilarly, especially across dorsal and ventral visual cortex. Moreover, visual maps that typically require time-intensive and complicated retinotopic mapping could be predicted from movie-evoked activity in both data-driven analyses (i.e., independent components analysis) at the individual level and by using functional alignment into a common low-dimensional embedding to generalize across participants. These results suggest that the infant visual system is already structured to process dynamic, naturalistic information and that fine-grained cortical organization can be discovered from movie data.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>naturalistic tasks</kwd>
<kwd>child development</kwd>
<kwd>fMRI</kwd>
<kwd>ventral and dorsal visual streams</kwd>
<kwd>retino-topic mapping</kwd>
<kwd>functional alignment</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<p>Studying the function and organization of the youngest human brains remains a challenge. Among the most important obstacles facing this research is that infants are unable to maintain focus for long periods of time and struggle to complete traditional cognitive tasks<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Movies can be a useful tool for studying the developing mind<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, as has been shown in older children<sup><xref ref-type="bibr" rid="c3">3</xref>–<xref ref-type="bibr" rid="c5">5</xref></sup>. The dynamic, continuous, and content-rich nature of movie stimuli<sup><xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref></sup> make them effective at capturing infant attention<sup><xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref></sup>. Here, we examine what can be revealed about the functional organization of the infant brain during movie-watching.</p>
<p>We focus on visual cortex because its organization at multiple spatial scales is well under-stood from traditional, task-based fMRI. The mammalian visual cortex is divided into multiple areas with partially distinct functional roles<sup><xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref></sup>. Within visual areas, there are orderly, topographic representations, or maps, of visual space<sup><xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref></sup>. These maps capture information about the location and spatial extent of visual stimuli with respect to fixation. Thus, maps reflect sensitivity to polar angle, measured via alternations between horizontal and vertical meridians that define area boundaries<sup><xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref></sup>, and sensitivity to spatial frequency, reflected in gradients of sensitivity to high and low spatial frequencies from foveal to peripheral vision, respectively<sup><xref ref-type="bibr" rid="c16">16</xref></sup>. Previously, we reported that these maps could be revealed by a retinotopy task in infants as young as 5 months of age<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. However, each session of infant fMRI allows for only 5–10 minutes of data collection<sup><xref ref-type="bibr" rid="c1">1</xref></sup>, and so requiring retinotopic mapping tasks to define regions of interest would leave little time for other tasks. Moreover, retinotopy tasks typically require controlled behavior (e.g., fixation) which can lead to attrition.</p>
<p>The primary goal of the current study is to investigate the organization of visual cortex with movie-watching data. Movies drive strong and naturalistic responses in sensory regions while minimizing task demands<sup><xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c18">18</xref></sup>. Movies have been useful in awake infant fMRI for studying event segmentation<sup><xref ref-type="bibr" rid="c19">19</xref></sup>, functional alignment<sup><xref ref-type="bibr" rid="c20">20</xref></sup>, and brain networks<sup><xref ref-type="bibr" rid="c21">21</xref></sup>. However, this past work did not address the granularity and specificity of cortical organization that movies can identify. For example, infants have similar evoked responses to movie content across large portions of visual cortex<sup><xref ref-type="bibr" rid="c19">19</xref></sup>, but the structure of those responses remains unclear, especially how they vary across visual areas. There are several reasons for skepticism that movies could evoke detailed, retinotopic organization: Movies may not fully sample the stimulus parameters (e.g., spatial frequencies) or visual functions needed to find topographic maps and areas in visual cortex. Even if movies contain the necessary visual properties, they may unfold at a faster rate than can be detected by fMRI. Additionally, naturalistic stimuli may not drive visual responses as robustly as experimenter-defined stimuli that are designed for retinotopic mapping with discrete onsets and high contrast. Finally, the complexity of movie stimuli may result in variable attention between participants, impeding discovery of reliable visual structure across individuals.</p>
<p>We conducted several analyses to probe different kinds of visual granularity in infant movie-watching fMRI data. First, we asked whether distinct areas of the infant visual cortex have different functional profiles. Second, we asked whether the topographic organization of visual areas can be recovered within participants. Third, we asked whether this within-area organization is aligned across participants. These three analyses assess key indicators of the mature visual system: functional specialization between areas, organization within areas, and consistency between individuals. Throughout, we explored the feasibility of <italic>only</italic> relying on movies to perform retinotopic mapping of infant visual areas.</p>
<sec id="s1">
<title>Results</title>
<p>We performed fMRI in awake, behaving infants using a protocol described previously<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. The dataset consisted of 15 sessions of infant participants (4.8–23.1 months old) who had both movie-watching data and retinotopic mapping data collected in the same session (<xref rid="tbls1" ref-type="table">Table S1</xref>). All available movies from each session were included (<xref rid="tbls2" ref-type="table">Table S2</xref>).</p>
<p>The retinotopic-mapping data from the same infants<sup><xref ref-type="bibr" rid="c17">17</xref></sup> allowed us to generate infant-specific meridian maps (horizontal versus vertical stimulation) and spatial frequency maps (high versus low stimulation). The meridian maps were used to define regions of interest (ROIs) for visual areas V1, V2, V3, V4, and V3A/B.</p>
<sec id="s1a">
<title>Evidence of area organization with homotopic similarity</title>
<p>To determine what movies can reveal about the organization of areas in visual cortex, we compared activity across left and right hemispheres. Namely, we correlated timecourses of movie-related BOLD activity between retinotopically defined, participant-specific ROIs (7.3 regions per participant per hemisphere, range: 6– 8)<sup><xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref></sup>. Higher correlations between the same (i.e., homotopic) areas than different areas indicates differentiation of function between areas. Moreover, other than V1, homotopic visual areas are anatomically separated across the hemispheres, so similar responses are unlikely to be attributable to spatial autocorrelation.</p>
<p>Homotopic areas (e.g., left ventral V1 and right ventral V1; diagonal of <xref rid="fig1" ref-type="fig">Figure 1A</xref>) were highly correlated (M=0.88, range of area means: 0.85–0.90), and more correlated than non-homotopic areas, such as the same visual area across streams (e.g., left ventral V1 and right dorsal V1; <xref rid="fig1" ref-type="fig">Figure 1B</xref>; Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.42, p<italic>&lt;</italic>0.001). To clarify, we use the term ‘stream’ to liberally distinguish visual regions that are <italic>more</italic> dorsal or <italic>more</italic> ventral, as opposed to the functional definition used in reference to the ‘what’ and ‘where’ streams<sup><xref ref-type="bibr" rid="c24">24</xref></sup>. Within stream (<xref rid="fig1" ref-type="fig">Figure 1C</xref>), homotopic areas were more correlated than adjacent areas in the visual hierarchy (e.g., left ventral V1 and right ventral V2; Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.09, p<italic>&lt;</italic>0.001), and adjacent areas were more correlated than distal areas (e.g., left ventral V1 and right ventral V4; Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.20, p<italic>&lt;</italic>0.001). Hence, movies elicit distinct processing dynamics across areas of infant visual cortex defined independently using retinotopic mapping.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Homotopic correlations between retinotopic areas. (A) Average correlation of the time-course of activity evoked during movie watching for all areas. This is done for the left and right hemisphere separately, creating a matrix that is not diagonally symmetric. The color triangles overlaid on the corners of the matrix cells indicate which cells contributed to the summary data of different comparisons in subpanels B and C. (B) Across-hemisphere similarity of the same visual area from the same stream (e.g., left ventral V1 and right ventral V1) and from different streams (e.g., left ventral V1 and right dorsal V1). (C) Across-hemisphere similarity in the same stream when matching the same area (e.g., left ventral V1 and right ventral V1), matching to an adjacent area (e.g., left ventral V1 and right ventral V2), or matching to a distal area (e.g., left ventral V1 and right ventral V4). Grey lines represent individual participants. *** = p<italic>&lt;</italic>0.001 from bootstrap resampling</p></caption>
<graphic xlink:href="554318v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We previously found<sup><xref ref-type="bibr" rid="c17">17</xref></sup> that an anatomical segmentation of visual cortex<sup><xref ref-type="bibr" rid="c25">25</xref></sup> could identify these same areas reasonably well. Indeed, the results above were replicated when using visual areas defined anatomically (<xref rid="figs1" ref-type="fig">Figure S1</xref>). However, a key advantage of anatomical segmentation is that it can define visual areas not mapped by a functional retinotopy task. This could help address limitations of the analyses above, namely that there was a variable number of retinotopic areas identified across infants and these areas covered only part of visually responsive cortex. Focusing on broader areas that include portions of the ventral and dorsal stream in the adult visual cortex<sup><xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref></sup>, we tested for functional differentiation of these streams in infants. We applied multi-dimensional scaling (MDS) — a data-driven method for assessing the clustering of data — to the average cross-correlation matrix across participants (<xref rid="figs1" ref-type="fig">Figure S1</xref>). The stress of fitting these data with a two-dimensional MDS was in the acceptable range (0.076). Clear organization was present (<xref rid="fig2" ref-type="fig">Figure 2</xref>): areas in the adult-defined ventral stream (e.g., VO, PHC) differentiated from areas in the adult-defined dorsal stream (e.g., V3A/B). Indeed, we see a slight separation between canonical dorsal areas and the recently defined lateral pathway<sup><xref ref-type="bibr" rid="c26">26</xref></sup> (e.g., LO1, hMT), although more evidence is needed to substantiate this distinction. Again, this organization cannot be attributed to mere spatial autocorrelation within stream because analyses were conducted across hemispheres (at significant anatomical distance). These results thus provide evidence of a dissociation in the functional profile of anatomically defined ventral and dorsal streams during infant movie-watching.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Multi-dimensional scaling (MDS) of movie-evoked activity in visual cortex. A) Anatomically defined areas<sup><xref ref-type="bibr" rid="c25">25</xref></sup> used for this analysis, separated into dorsal (red) and ventral (blue) visual cortex, overlaid on a flatmap of visual cortex. B) The timecourse of functional activity for each area was extracted and compared across hemispheres (e.g., left V1 was correlated with right V1). This matrix was averaged across participants and used to create a Euclidean dissimilarity matrix. MDS captured the structure of this matrix in two dimensions with suitably low stress. The plot shows a projection that emphasizes the similarity to the brain’s organization.</p></caption>
<graphic xlink:href="554318v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s1b">
<title>Evidence of within-area organization with independent components analysis</title>
<p>We next explored whether movies can reveal fine-grained organization <italic>within</italic> visual areas by using independent components analysis (ICA) to discover visual maps in individual infant brains<sup><xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref></sup>. ICA is a method for decomposing a source into constituent signals by finding components that account for independent variance. When applied to fMRI data (using MELODIC in FSL), these components have spatial structure that varies in strength over time. Many of these components reflect noise (e.g., motion, breathing) or task-related signals (e.g., face responses), while other components reflect the functional architecture of the brain (e.g., topographic maps)<sup><xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c27">27</xref></sup>. We visually inspected each component and categorized it as a potential spatial frequency map, a potential meridian map, or neither. This process was blind to the ground-truth of what the visual maps look like for that participant from the retinotopic mapping task, simulating what would be possible if retinotopy data from the participants were unavailable. Success in this process requires that 1) movies and ICA can reveal retinotopic organization and 2) experimenters can accurately identify these components.</p>
<p>Multiple maps could be identified because there were more than one candidate that the experimenter thought was a suitable map. Across infant participants, we identified an average of 2.4 (range: 0–5) components as potential spatial frequency maps and 1.1 (range: 0–4) components as potential meridian maps. To evaluate the quality of these maps, we compared them to the ground-truth of that participant’s task-evoked maps (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Spatial frequency and meridian maps are defined by their systematic gradients of intensity across the cortical surface<sup><xref ref-type="bibr" rid="c29">29</xref></sup>. Lines drawn parallel to area boundaries show monotonic gradients on spatial frequency maps, with stronger responses to high spatial frequency at the fovea, and stronger responses to low spatial frequencies in the periphery. By contrast, lines drawn perpendicular to the area boundaries show oscillations in sensitivity to horizontal and vertical meridians on meridian maps.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Example retintopic task vs. ICA-based spatial frequency maps. A) Spatial frequency map of a 17.1 month old infant. The retinotopic task data are from a prior study<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. The view is of the flattened occipital cortex with visual areas traced in black. B) Component captured by ICA of movie data from the same participant. This component was chosen as a spatial frequency map in this participant. The sign of ICA is arbitrary so it was flipped here for visualization. C) Gradients in spatial frequency within-area from the task-evoked map in subpanel A. Lines parallel to the area boundaries (emanating from fovea to periphery) were manually traced and used to capture the changes in response to high versus low spatial frequency stimulation. D) Gradients in the component map. These gradients are similar to those found in the task-evoked spatial frequency map, confirming that this is an appropriate component.</p></caption>
<graphic xlink:href="554318v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To assess the selected component maps, we correlated the gradients (described above) of the task-evoked and component maps. <xref rid="fig4" ref-type="fig">Figure 4A</xref> shows the absolute correlations between the task-evoked maps and the manually identified spatial frequency components (M=0.52, range: 0.23– 0.85). To evaluate whether movies are a viable method for defining retinotopic maps, we tested whether the task-evoked retinotopic maps were more similar to manually identified components than other components. We identified the best component in 6 of 13 participants (<xref rid="fig4" ref-type="fig">Figure 4B</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Similarity between visual maps from the retinotopy task and ICA applied to movies. Absolute correlation between the task-evoked and component spatial frequency maps (absolute values used because sign of ICA maps is arbitrary). Each dot is a manually identified component. At least one component was identified in 13 out of 15 participants. The bar plot is the average across participants. The error bar is the standard error across participants. B) Ranked correlations for the manually identified spatial frequency components relative to all components identified by ICA. Bar plot is same as A. C) Same as A but for meridian maps. At least one component was identified in 9 out of 15 participants. D) Same as B but for meridian maps.</p></caption>
<graphic xlink:href="554318v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The percentile of the average manually identified component was high (M=63.8 percentile, range: 26.7–98.1) and significantly above chance (ΔM=13.8, CI=[3.3–23.9], p=.010). This illustrates that the manually identified components derived from movie-watching data are similar to the spatial frequency maps derived from retinotopic mapping. This provides support for using data-driven techniques like ICA to reveal functional organization within visual areas in infants. The fact that this can work also indicates the underlying architecture of the infant visual system influences how movies are processed.</p>
<p>We performed the same analyses on the meridian maps. As noted above, the lines were now traced perpendicular to the boundaries. <xref rid="fig4" ref-type="fig">Figure 4C</xref> shows the correlation between the task-evoked meridian maps and the manually identified components (M=0.46, range: 0.03–0.81). Compared to all possible components identified by ICA, the best possible component was identified for 1 out of 9 participants (<xref rid="fig4" ref-type="fig">Figure 4D</xref>). Although the percentile of the average manually identified component was numerically high (M=67.6 percentile, range: 3.0–100.0), it was not significantly above chance (ΔM=17.6, CI=[-1.8–33.0], p=.074). This difference in performance compared to spatial frequency is also evident in the fact that fewer components were identified as potential meridian maps, and that several participants had no such maps. Because it is possible, albeit less likely, to identify meridian maps from ICA, the structure may be present in the data but more susceptible to noise or gaze variability. Spatial frequency maps have a coarser structure than meridian maps, and are more invariant to fixation, which may explain why they are easier to identify.</p>
</sec>
<sec id="s1c">
<title>Evidence of within area organization with shared response modeling</title>
<p>Finally, we investigated whether the organization of visual cortex in one infant can be predicted from movie-watching data in <italic>other</italic> participants using functional alignment<sup><xref ref-type="bibr" rid="c30">30</xref></sup>. For such functional alignment to work, stimulus-driven responses to the movie must be shared across participants. These analyses also benefit from greater amounts of data, so we expanded the sample in two ways (<xref rid="tbls2" ref-type="table">Table S2</xref>): First, we added 71 movie-watching datasets from additional infants who saw the same movies but didn’t have usable retinotopy data (and thus were not included in the analyses above that compared movie and retinotopy data within participant). Second, we recruited 8 adult participants to watch a subset of the movies we showed infants and to complete the retinotopy task that infants completed. Additionally, 41 datasets from adults who had seen the movies shown to infants but did not have retinotopy data were included in these analyses.</p>
<p>With this expanded dataset, we used shared response modeling (SRM)<sup><xref ref-type="bibr" rid="c31">31</xref></sup> to predict visual maps from other participants (<xref rid="fig5" ref-type="fig">Figure 5</xref>). Specifically, we held out one participant for testing purposes and used SRM to learn a low-dimensional, shared feature space from the movie-watching data of the remaining participants in a mask of occipital cortex. This shared space represented the responses to that movie in visual cortex that were shared across participants, agnostic to the precise localization of these responses across voxels in each individual. The number of features in the shared space (K=10) was determined via a cross-validation procedure on movie-watching data in adults (<xref rid="figs2" ref-type="fig">Figure S2</xref>). We then mapped the held-out participant’s movie data into the learned shared space without changing the shared space.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Pipeline for predicting visual maps from movie data. All participants watched the same movie, and one participant’s data were held out. The remaining participants were mapped into a lower-dimensional feature space using shared response modeling (SRM)<sup><xref ref-type="bibr" rid="c31">31</xref></sup>. The visual maps from these participants were transformed into the shared space and averaged. By mapping the held-out participant into the shared space (with only their movie data), the average visual maps in shared space could be transformed into their brain space, resulting in a predicted retinotopic map that can be validated against their real map from the retinotopy task.</p></caption>
<graphic xlink:href="554318v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The task-evoked retinotopic maps from all but the held-out participant were transformed into this shared space and averaged, separately for each map type. Taking the inverse of the held-out participant’s mapping allowed us to transform the averaged shared space representation of visual maps into the held-out participant’s brain space. This predicted visual organization was compared to the participant’s actual visual map from the retinotopy task (using the same methods as for ICA). Critically, predicting the retinotopic maps used no retinotopy data from the held-out participant. Moreover, it is completely unconstrained anatomically (except for a liberal occipital lobe mask). Hence, the similarity of the SRM-predicted map to the task-evoked map is due to representations of visual space in other participants being mapped into the shared space.</p>
<p>We trained SRMs on two populations to predict a held-out infant’s maps: (1) other infants and (2) adults. There may be advantages to either approach: infants are likely more similar to each other than adults in terms of how they respond to the movie; however, their data is more contaminated by motion. When using the infants to predict a held out infant, the spatial frequency map (<xref rid="fig6" ref-type="fig">Figure 6A</xref>) and meridian map (<xref rid="fig6" ref-type="fig">Figure 6C</xref>) predictions are moderately correlated with task-evoked retinotopy data (spatial frequency: M=0.46, range: -0.06-0.78; meridian: M=0.24, range: -0.12-0.78).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Similarity of SRM-predicted maps and task-evoked retinotopic maps. Correlation between the gradients of the A) spatial frequency maps and C) meridian maps predicted with SRM from other infants and task-evoked retinotopy maps. B, D) Same as A, except using adult participants to train the SRM and predict maps. Dot color indicates the movie used for fitting the SRM. The end of the line indicates the correlation of the task-evoked retinotopy map and the predicted map when using flipped training data for SRM. Hence, lines extending below the dot indicate that the true performance was higher than a baseline fit. The bar plot is the average across participants. The error bar is the standard error across participants.</p></caption>
<graphic xlink:href="554318v1_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To evaluate whether success was due to fitting the shared response, we flipped the held-out participant’s training data (i.e., the first timepoint became the last timepoint and vice versa) so that an appropriate fit should not be learnable. The vertical lines for each movie in <xref rid="fig6" ref-type="fig">Figure 6</xref> indicate the change in performance for this baseline. Indeed, flipping significantly worsened prediction of the spatial frequency map (Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.52, CI=[0.24–0.80], p<italic>&lt;</italic>.001) and the meridian map (Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.24, CI=[0.02–0.49], p=.034). Hence, the movie-evoked response enables the mapping of other infants’ retinotopic maps into a held-out infant.</p>
<p>Using adult data to predict infant data also results in maps similar to task-evoked spatial frequency maps (<xref rid="fig6" ref-type="fig">Figure 6B</xref>; M=0.56, range: 0.17–0.79) and meridian maps (<xref rid="fig6" ref-type="fig">Figure 6D</xref>; M=0.34, range: -0.27-0.64). Again, flipping the SRM training data significantly worsened prediction of the held-out participant’s spatial frequency map (Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.40, CI=[0.17–0.65], p<italic>&lt;</italic>.001) and meridian map (Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.33, CI=[0.12–0.55], p=.002). There was no significant difference in SRM performance when using adults versus infants as the training set (spatial frequency: Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.14, CI=[-0.00–0.27], p=.054; meridian: Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.11, CI=[-0.05–0.28], p=.179). In sum, SRM could be used to predict visual maps with moderate accuracy. This indicates that functional alignment methods like SRM can partially capture the retinotopic organization of visual cortex from infant movie-watching data.</p>
</sec>
</sec>
<sec id="s2">
<title>Discussion</title>
<p>We present evidence that movies can reveal the organization of infant visual cortex at different spatial scales. We found that movies evoke differential function across areas, topographic organization of function within areas, and this topographic organization is shared across participants.</p>
<p>We show that the movie-evoked response in a visual area is more similar to the same area in the other hemisphere than to different areas in the other hemisphere. By comparing across anatomically distant hemispheres, we reduced the impact of spatial autocorrelation and isolated the stimulus-driven signals in the brain activity<sup><xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c32">32</xref></sup>. The greater across-hemisphere similarity for same versus different areas provides some of the first evidence that visual areas and streams are functionally differentiated in infants as young as 5 months old. Previous work suggests that functions of the dorsal and ventral streams are detectable in young infants<sup><xref ref-type="bibr" rid="c33">33</xref></sup> but that the localization of these functions is immature<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. Despite this, we find that the areas of infant visual cortex that will mature into the dorsal and ventral streams have distinct activity profiles during movie watching.</p>
<p>Additionally, we used a data-driven approach (ICA) to uncover the retinotopic organization of visual cortex in infants. We observed components that were highly similar to a spatial frequency map obtained from the same infant in a retinotopy task. Importantly, the components could be identified without knowledge of these ground-truth maps. This was also true for the meridian maps, to a lesser degree. One caveat for interpreting these results is that although some of the components are <italic>similar</italic> to a spatial frequency map or meridian map, they could reflect a different kind of visual map. For instance, the spatial frequency map is highly correlated with the eccentricity map<sup><xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c35">35</xref>–<xref ref-type="bibr" rid="c37">37</xref></sup> (which itself is related to receptive field size). This means it is inappropriate to make strong claims about the underlying function of the components based on their similarity to visual maps alone. Nonetheless, these results do show that it is possible to discover visual maps in infants with movie-watching data and ICA.</p>
<p>We also asked whether functional alignment<sup><xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c31">31</xref></sup> could be used to detect visual maps in infants. Using a shared response model<sup><xref ref-type="bibr" rid="c31">31</xref></sup> trained on movie-watching data of infants or adults, we transformed the visual maps of other individuals into a held-out infant’s brain to evaluate the fit to visual maps from a retinotopy task<sup><xref ref-type="bibr" rid="c30">30</xref></sup>. Like ICA, this was more successful for the spatial frequency maps, but it was still possible in some cases with the meridian maps. This is remarkable because the complex pattern of brain activity underlying these visual maps could be ‘compressed’ by SRM into only 10 dimensions in the shared space (i.e., the visual maps were summarized by a vector of 10 values). The weight matrix that ‘decompressed’ visual maps from this low-dimensional space into the held-out infant was learned from their movie-watching data alone. Hence, success with this approach means that visual maps are engaged during infant movie-watching. Furthermore, this result shows that functional alignment is practical for studies in awake infants that produce small amounts of data. Functional alignment can increase signal quality<sup><xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c38">38</xref></sup> and reveal changing function over development<sup><xref ref-type="bibr" rid="c39">39</xref></sup>, so it may prove especially useful for infant fMRI<sup><xref ref-type="bibr" rid="c40">40</xref></sup>. In sum, movies evoke sufficiently reliable activity across infants and adults to find a shared response, and this shared response contains information about the organization of infant visual cortex.</p>
<p>The fact that movies are engaging to infants and serve multiple purposes<sup><xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c19">19</xref></sup> makes them an efficient approach for crude retinotopic mapping in infants, given the limited amount of awake data that can be obtained per session. To be clear, we are not suggesting that movies work well enough to <italic>replace</italic> a retinotopy task when accurate maps are needed. For instance, even though ICA found components that were highly correlated with the spatial frequency map, we also selected some components that turned out to have lower correlations. Without knowing the ground-truth from a retinotopy task, there would be no way to weed these out. On a positive note, our analyses provide additional support for our previous conclusion<sup><xref ref-type="bibr" rid="c17">17</xref></sup> that infant visual areas can be defined anatomically. Not only do visual areas have highly similar movie responses across hemispheres, but the function of visual areas showed interpretable structure when visualized with multi-dimensional scaling. Hence, some combination of anatomical landmarks and movie data may be adequate for defining infant visual areas for most research questions.</p>
<p>In conclusion, movies evoke activity in infants that recapitulate the organization of the visual cortex. This activity is differentiated across visual areas and contains information about the visual maps at the foundation of visual processing. The work presented here is another demonstration of the power of content-rich, dynamic, and naturalistic stimuli to reveal insights in cognitive neuroscience.</p>
</sec>
<sec id="s3">
<title>Methods</title>
<sec id="s3a">
<title>Participants</title>
<p>Infant participants with retinotopy data were previously reported in another study<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. Of those 17 original sessions, 15 had usable movie data collected in the same session and thus could be included in the current study. In this subsample, the age range was 4.8–23.1 months (<italic>M</italic>=13.0; 12 female; <xref rid="tbls1" ref-type="table">Table S1</xref>). The combinations of movies that infants saw were inconsistent, so the types of comparisons vary across analyses reported here. In brief, all possible infant participant sessions (15) were used in the Homotopy analyses and ICA, whereas two of these sessions (ages = 18.5, 23.1 months) could not be used in the SRM analyses. <xref rid="tbls1" ref-type="table">Table S1</xref> reports demographic information for the infant participants. <xref rid="tbls2" ref-type="table">Table S2</xref> reports participant information about each of the movies. It also reports the number and age of participants that were used to bolster the SRM analyses.</p>
<p>The adult sample used for SRM was collected to include both retinotopy and movie watching data (N=8, 3 females). The adult participants saw the five most common movies that were seen by infants in our retinotopy sample (3–8 total sessions per movie in the main infant sample). To support the SRM analyses, we also utilized any other available adult data from sessions in which we had shown the main movies in otherwise identical circumstances (<xref rid="tbls2" ref-type="table">Table S2</xref>).</p>
<p>Participants were recruited through fliers, word of mouth, or the Yale Baby School. This study was approved by the Human Subjects Committee at Yale University. Adults provided in-formed consent for themselves or their child.</p>
</sec>
<sec id="s3b">
<title>Materials</title>
<p>Our experiment display code can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/experiment">https://github.com/ntblab/experiment</ext-link> menu/tree/Movies/ and <ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/experiment">https://github.com/ntblab/experiment</ext-link> menu/tree/retinotopy/. The code used to perform the data analyses is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ntblab/infant">https://github.com/ntblab/infant</ext-link> neuropipe/tree/predict retinotopy/; this code uses tools from the Brain Imaging Analysis Kit<sup><xref ref-type="bibr" rid="c41">41</xref></sup>; <ext-link ext-link-type="uri" xlink:href="https://brainiak.org/docs/">https://brainiak.org/docs/</ext-link>). Raw and preprocessed functional and anatomical data will be available here: <ext-link ext-link-type="uri" xlink:href="https://datadryad.org/">https://datadryad.org/</ext-link> PENDING URL but is available from this temporary link while under review: <ext-link ext-link-type="uri" xlink:href="https://drive.google">https://drive.google</ext-link>. com/drive/folders/1zKWLluNhUz48MZMAS8-I-xEINpLWYVo ?usp=sharing).</p>
</sec>
<sec id="s3c">
<title>Data acquisition</title>
<p>Data were collected at the Brain Imaging Center (BIC) in the Faculty of Arts and Sciences at Yale University. We used a Siemens Prisma (3T) MRI and only the bottom half of the 20-channel head coil. Functional images were acquired with a whole-brain T2* gradient-echo EPI sequence (TR=2s, TE=30ms, flip angle=71, matrix=64x64, slices=34, resolution=3mm iso, interleaved slice acquisition). Anatomical images were acquired with a T1 PETRA sequence for infants (TR1=3.32ms, TR2=2250ms, TE=0.07ms, flip angle=6, matrix=320x320, slices=320, resolution=0.94mm iso, radial slices=30000) and a T1 MPRAGE sequence for adults, with the top of the head coil attached, (TR=2300ms, TE=2.96ms, TI=900ms, flip angle=9, iPAT=2, slices=176, matrix=256x256, resolution=1.0mm iso).</p>
</sec>
<sec id="s3d">
<title>Procedure</title>
<p>Our approach for collecting fMRI data from awake infants has been described in a previous methods paper<sup><xref ref-type="bibr" rid="c1">1</xref></sup>, with important details repeated below. Infants were first brought in for a mock scanning session to acclimate them and their parent to the scanning environment. Scans were scheduled when the infants were typically calm and happy. Participants were carefully screened for metal. We applied hearing protection in three layers for the infants: silicon inner ear putty, over-ear adhesive covers and ear muffs. For the infants that were played sound (see below), Optoacoustics noise cancelling headphones were used instead of the ear muffs. The infant was placed on a vacuum pillow on the bed that comfortably reduced their movement. The top of the head coil was not placed over the infant in order to maintain comfort. Stimuli were projected directly on to the surface of the bore. A video camera (High Resolution camera, MRC systems) recorded the infant’s face during scanning. Adult participants underwent the same procedure with the following exceptions: they did not attend a mock scanning session, hearing protection was only two layers (earplugs and Optoacoustics headphones), and they were not on a vacuum pillow. Some infants participated in additional tasks during their scanning session.</p>
<p>When the infant was focused, experimental stimuli were shown using Psychtoolbox<sup><xref ref-type="bibr" rid="c42">42</xref></sup> for MATLAB. The details for the retinotopy task are explained fully elsewhere<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. In short, we showed two types of blocks. For the meridian mapping blocks, a bow tie cut-out of a colorful, large, flickering checkerboard was presented in either a vertical or horizontal orientation<sup><xref ref-type="bibr" rid="c43">43</xref></sup>. For the spatial frequency mapping blocks, the stimuli were grayscale Gaussian random fields of high (1.5 cycles per visual degree) or low (0.05 cycles per visual degree) spatial frequency<sup><xref ref-type="bibr" rid="c22">22</xref></sup>. For all blocks, a smaller (1.5 visual degree) grayscale movie was played at center to encourage fixation. Each block type contained two phases of stimulation. The first phase consisted of one of the conditions (e.g., horizontal or high) for 20s, followed immediately by the second phase with the other condition of the same block type (e.g., vertical or low, respectively) for 20s. At the end of each block there was at least 6s rest before the start of the next block. Infant participants saw up to 12 blocks of this stimulus, resulting in 24 epochs of stimuli. Adults all saw 12 blocks.</p>
<p>Participants saw a broad range of movies in this study (<xref rid="tbls3" ref-type="table">Table S3</xref>), some of which have been reported previously<sup><xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c21">21</xref></sup>. The movie titled ‘Child Play’ comprises the concatenation of four silent videos that range in duration from 64–143s and were shown in the same order (with 6s in-between). They extended 40.8° wide by 25.5° high on the screen. The other movies were stylistically similar, computer-generated animations that each lasted 180s. These movies extended 45.0° wide by 25.5° high. Some of the movies were collected as part of an unpublished experiment in which we either played the full movie or inserted drops every 10s (i.e., the screen went blank while the audio continued). We included the ‘Dropped’ movies in the Homotopy analyses and ICA (average number of ‘Dropped’ movies per participant: 0.9, range: 0–3); however, we did not include them in the SRM analyses. Moreover, we only included 4 (out of 17) of these movies in the SRM analyses because there were insufficient numbers of infant participants to enable the training of the SRM.</p>
</sec>
<sec id="s3e">
<title>Gaze Coding</title>
<p>The infant gaze coding procedure for the retinotopy data was the same as reported previously<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. The gaze coding for the movies was also the same as reported previously<sup><xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c21">21</xref></sup>. Participants looked at the screen for an average of 93.7% of the time (range: 78–99) for the movies used in the homotopy and ICA analyses, and 94.5% of the time (range: 82–99) for the movies used in the SRM analyses (<xref rid="tbls1" ref-type="table">Table S1</xref>) Adult participants were not gaze coded, but they were monitored online for inattentiveness. One adult participant was drowsy so they were manually coded. This resulted in the removal of four out of the 24 epochs of retinotopy.</p>
</sec>
<sec id="s3f">
<title>Preprocessing</title>
<p>We used FSL’s FEAT analyses with modifications in order to implement infant-specific preprocessing of the data<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. If infants participated in other experiments during the same functional run (14 sessions), the data was split to create a pseudorun. Three burn-in volumes were discarded from the beginning of each run/pseudorun when available. To determine the reference volume for alignment and motion correction, the Euclidean distance between all volumes was calculated and the volume that minimized the distance between all points was chosen as reference (the ‘centroid volume’). Adjacent timepoints with greater than 3mm of movement were interpolated. To create the brain mask we calculated the SFNR<sup><xref ref-type="bibr" rid="c44">44</xref></sup> for each voxel in the centroid volume. This produced a bimodal distribution reflecting the signal properties of brain and non-brain voxels. We thresholded the brain voxels at the trough between these two peaks. We performed Gaussian smoothing (FWHM=5mm). Motion correction with 6 degrees of freedom was performed using the centroid volume. AFNI’s despiking algorithm attenuated voxels with aberrant timepoints. The data for each movie were <italic>z</italic>-scored in time.</p>
<p>We registered the centroid volume to a homogenized and skull-stripped anatomical volume from each participant. Initial alignment was performed using FLIRT with a normalized mutual information cost function. This automatic registration was manually inspected and then corrected if necessary using mrAlign from mrTools<sup><xref ref-type="bibr" rid="c45">45</xref></sup>.</p>
<p>The final step common across analyses created a transformation into surface space. Surfaces were reconstructed from iBEAT v2.0<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. These surfaces were then aligned into standard Buckner40 standard surface space<sup><xref ref-type="bibr" rid="c47">47</xref></sup> using FreeSurfer<sup><xref ref-type="bibr" rid="c47">47</xref></sup>.</p>
<p>Additional preprocessing steps were taken for the SRM analyses. For each individual movie (including each movie that makes up ‘Child Play’), the fMRI data was time-shifted by 4s and the break after the movie finished was cropped. This was done to account for hemodynamic lag, so that the first TR and last TR of the data approximately<sup><xref ref-type="bibr" rid="c48">48</xref></sup> corresponded to the brain’s response to the first and last 2s of the movie, respectively.</p>
<p>Occipital masks were aligned to the participant’s native space for the SRM analyses. To produce these, a mapping from native functional space to standard space was determined. This was enabled using non-linear alignment of the anatomical image to standard space using ANTs<sup><xref ref-type="bibr" rid="c49">49</xref></sup>. For infants, an initial linear alignment with 12 DOF was used to align anatomical data to the age-specific infant template<sup><xref ref-type="bibr" rid="c50">50</xref></sup>, followed by non-linear warping using diffeomorphic symmetric normalization. Then, we used a predefined transformation (12 DOF) to linearly align between the infant template and adult standard. For adults, we used the same alignment procedure, except participants were directly aligned to adult standard. We used the occipital mask from the MNI structural atlas<sup><xref ref-type="bibr" rid="c51">51</xref></sup> in standard space and used the inverted transform to put it into native functional space.</p>
</sec>
</sec>
<sec id="s4">
<title>Analysis</title>
<sec id="s4a">
<title>Retinotopy</title>
<p>For our measure of task-evoked retinotopy in infants, we used the outputs of the retinotopy analyses from our previous paper<sup><xref ref-type="bibr" rid="c17">17</xref></sup> that are publicly released. In brief, we performed separate univariate contrasts between conditions in the study (horizontal<italic>&gt;</italic>vertical, high spatial frequency<italic>&gt;</italic>low spatial frequency). We then mapped these contrasts into surface space. Then, in surface space rendered by AFNI<sup><xref ref-type="bibr" rid="c52">52</xref></sup>, we demarcated the visual areas V1, V2, V3, V4, and V3A/B using traditional protocols based on the meridian map contrast<sup><xref ref-type="bibr" rid="c53">53</xref></sup>. We traced lines perpendicular and parallel to the area boundaries to quantify gradients in the visual areas. The anatomically-defined areas of interest<sup><xref ref-type="bibr" rid="c25">25</xref></sup> used in <xref rid="fig2" ref-type="fig">Figure 2</xref> were available in this standard surface space.</p>
</sec>
<sec id="s4b">
<title>Homotopy</title>
<p>The homotopy analyses compared the time course of functional activity across visual areas in different hemispheres of each infant. For the participants that had more than one movie in a session (N=9), all the movies were concatenated along with burn out time between the movies (Mean number of movies per participant=2.7, range: 1–6, Mean duration of movies=540.7s, range: 186–1116). For the areas that were defined with the retinotopy task (average number of areas traced in each hemisphere = 7.3, range: 6.0–8.0), the functional activity was averaged within area and then Pearson correlated between all other areas. The resulting cross-correlation matrix was Fisher Z transformed before different cells were averaged or compared. If infants did not have an area traced then those areas were ignored in the analyses. We grouped visual areas according to stream, where areas that are <italic>more</italic> dorsal of V1 were called ‘dorsal’ stream and areas <italic>more</italic> ventral were called ‘ventral’ stream. To assess the functional similarity of visual areas, Fisher Z correlations between the same areas in the same stream were averaged, and compared to the correlations of approximately equivalent areas from different streams (e.g., dorsal V2 compared with ventral V2). The averages for each of the two conditions (same stream vs. different stream) were evaluated statistically using bootstrap resampling<sup><xref ref-type="bibr" rid="c54">54</xref></sup>. Specifically, we computed the mean difference between conditions in a pseudosample, generated by sampling participants with replacement. We created 10,000 such pseudosamples and took the proportion of differences that showed a different sign than the true mean, multiplied by two to get the two-tailed <italic>p</italic>-value. To evaluate how distance affects similarity, we additionally compared with bootstrap resampling the Fisher Z correlations of areas across hemispheres in the same stream: same area to adjacent areas (e.g., ventral V1 with ventral V2), to distal areas (e.g., ventral V1 with ventral V3). Before reporting the results in the figures, the Fisher Z values were converted back into Pearson correlation values.</p>
<p>As an additional analysis to the one described above, we used an atlas of anatomically-defined visual areas from adults<sup><xref ref-type="bibr" rid="c25">25</xref></sup> to define both early and later visual areas. Specifically, we used the areas labeled as part of the ventral and dorsal stream (excluding the intraparietal sulcus and frontal eye fields since they often cluster separately<sup><xref ref-type="bibr" rid="c55">55</xref></sup>), and then averaged the functional response within each area. The functional responses were then correlated across hemispheres, as in the main analysis. Multi-dimensional scaling was then performed on the cross-correlation matrix, and the dimensionality that fell below the threshold for stress (0.2) was chosen. In this case, that was a dimensionality of 2 (stress=0.076). We then visualized the resulting output of the data in these two dimensions.</p>
</sec>
<sec id="s4c">
<title>Independent Components Analysis (ICA)</title>
<p>To conduct ICA, we provided the preprocessed movie data to FSL’s MELODIC<sup><xref ref-type="bibr" rid="c27">27</xref></sup>. Like in the homotopy analyses, we used all of the movie data available per session. The algorithm found a range of components across participants (M=76.4 components, range: 31–167). With this large number of possible components, an individual coder (CE) sorted through them to determine whether each one looked like a meridian map, spatial frequency map, or neither (critically, without referring to the ground truth from the retinotopy task). We initially visually inspected each component in volumetric space, looking for the following features: First, we searched for whether there was a strong weighting of the component in visual cortex. Second, we looked for components that had a symmetrical pattern in visual cortex between the two hemispheres. To identify the spatial frequency maps, we looked for a continuous gradient emanating out from the early visual cortex. For meridian maps, we looked for sharp alternations in the sign of the component, particularly near the midline of the two hemispheres. Based on these criteria, we then chose a small set of components that were further scrutinized in surface space. On the surface, we looked for features that clearly define a visual map topography. Again, this selection process was blind to the task-evoked retinotopic maps, so that a person without retinotopy data could take the same steps and potentially find maps.</p>
<p>These components were then tested against that participant’s task-evoked retinotopic maps. If the component was labeled as a potential spatial frequency map, we tested whether there was a monotonic gradient from fovea to periphery. Specifically, we measured the component response along lines drawn parallel to the area boundaries, averaged across these lines, and then correlated this pattern with the same response in the actual map. The absolute correlation was used because the sign of ICA is arbitrary. For each participant, we then ranked the components to ask if the ones that were chosen were the best ones possible out of all those derived from MELODIC. To test whether the identified components were better than the non-identified components, we ranked all the components correlation to the task-evoked maps. This ranking was converted into a percentile, where 100% means it is the best possible component. We took the identified component’s percentile (or averaged the percentiles if there were multiple components chosen) and compared it to chance (50%). This difference from chance was used for bootstrap resampling to evaluate whether the identified components were significantly better than chance. We performed the same kind of analysis for meridian maps, except in this case the lines used for testing were those drawn perpendicular to the areas. In this case, we were testing whether the components showed oscillations in the sign of the intensity.</p>
</sec>
<sec id="s4d">
<title>Shared Response Modeling (SRM)</title>
<p>We based our SRM analyses on previous approaches<sup><xref ref-type="bibr" rid="c30">30</xref></sup>. SRM embeds the brain activity of multiple individuals viewing a common stimulus into a shared space with a small number of feature dimensions. Each voxel of each participant is assigned a weight for each feature. The weight reflects how much the voxel loads onto that feature. For our study, the SRM was either trained on infant movie-watching data or adult movie-watching data to learn the shared response, and the mapping of the training participants into this shared space. For the infant SRM, we used a leave-one-out approach. We took a movie that the held-out infant saw (e.g., ‘Aeronaut’) and considered all other infant participants that saw that movie (including additional participants without any retinotopy data). We fit an SRM model on all of the participants except the held-out one. This model has 10 features, as was determined based on cross-validation with adult data (<xref rid="figs2" ref-type="fig">Figure S2</xref>). We used an occipital anatomical mask to fit the SRM. Using the learned individual weight matrices, the retinotopic maps from the infants in the training set were then transformed into the shared space and averaged across participants. The held-out participant’s movie data were used to learn a mapping to the learned SRM features. By applying the inverse of this mapping, we transformed the averaged visual maps of the training set in shared space into the brain space of the held-out participant to predict their visual maps. Using the same methods as described for ICA above, we compared the task-evoked and predicted gradient responses. These analysis steps were also followed for the adult SRM, with the difference being that the group of participants used to create the SRM model and to create the averaged visual maps were adults. As with the infant SRM, additional adult participants without retinotopy data were used for training. Across both types of analysis, the held-out participant was completely ignored when fitting the SRM, and no retinotopy data went into training the SRM.</p>
<p>To test the benefit of SRM, we performed a control analysis in which we scrambled the movie data from the held-out participant before learning their mapping into the shared space. Specifically, we flipped the timecourse of the data so that the first timepoint became the last, and vice versa. By creating a mismatch in the movie sequence across participants, this procedure should result in meaningless weights for the held-out participant and, in turn, the prediction of visual maps using SRM will fail. We compared ‘real’ and ‘flipped’ SRM procedures by computing the difference in fit (transformed into Fisher Z) for each movie, and then averaging that difference within participant. Those differences were then bootstrap resampled to evaluate significance. We also performed bootstrap resampling to compare the ‘real’ SRM accuracy when using infants versus adults for training.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Contributions</title>
<p>C.T.E. and M.J.A. conceived of the analyses. C.T.E., T.S.Y., &amp; N.B.T-B. collected the data. C.T.E. &amp; T.S.Y. preprocessed the data. C.T.E. performed the analyses. All authors contributed to the drafting of the manuscript.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We are thankful to the families of infants who participated. We also acknowledge the hard work of the Yale Baby School team, including L. Rait, J. Daniels, A. Letrou, and K. Armstrong for recruitment, scheduling, and administration, and L. Skalaban, A. Bracher, D. Choi, and J. Trach for help in infant fMRI data collection. Thank you to J. Wu, J. Fel, and A. Klein for help with gaze coding, and R. Watts for technical support. We are grateful for internal funding from the Department of Psychology and Faculty of Arts and Sciences at Yale University. N.B.T-B. was further supported by the Canadian Institute for Advanced Research and the James S. McDonnell Foundation (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.37717/2020-1208">https://doi.org/10.37717/2020-1208</ext-link>).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Ellis</surname>, <given-names>C. T.</given-names></string-name> <etal>et al.</etal> <article-title>Re-imagining fmri for awake behaving infants</article-title>. <source>Nature Communications</source> <volume>11</volume> (<year>2020</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Vanderwal</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Eilbott</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Castellanos</surname>, <given-names>F. X</given-names></string-name>. <article-title>Movies in the magnet: Naturalistic paradigms in developmental functional neuroimaging</article-title>. <source>Developmental Cognitive Neuroscience</source> <volume>36</volume>, <fpage>100600</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Vanderwal</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kelly</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Eilbott</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mayes</surname>, <given-names>L. C.</given-names></string-name> &amp; <string-name><surname>Castellanos</surname>, <given-names>F. X</given-names></string-name>. <article-title>Inscapes: A movie paradigm to improve compliance in functional magnetic resonance imaging</article-title>. <source>NeuroImage</source> <volume>122</volume>, <fpage>222</fpage>–<lpage>232</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Richardson</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lisandrelli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Riobueno-Naylor</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Saxe</surname>, <given-names>R</given-names></string-name>. <article-title>Development of the social brain from age three to twelve years</article-title>. <source>Nature Communications</source> <volume>9</volume>, <fpage>1027</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Alexander</surname>, <given-names>L. M.</given-names></string-name> <etal>et al.</etal> <article-title>An open resource for transdiagnostic research in pediatric mental health and learning disorders</article-title>. <source>Scientific Data</source> <volume>4</volume>, <fpage>1</fpage>–<lpage>26</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Nastase</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Goldstein</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Hasson</surname>, <given-names>U</given-names></string-name>. <article-title>Keep it real: rethinking the primacy of experimental control in cognitive neuroscience</article-title>. <source>NeuroImage</source> <volume>222</volume>, <fpage>117254</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Glerean</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name> &amp; <string-name><surname>Vanderwal</surname>, <given-names>T</given-names></string-name>. <article-title>Naturalistic imaging: The use of ecologically valid conditions to study brain function</article-title>. <source>NeuroImage</source> <volume>247</volume> (<year>2022</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Franchak</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name> &amp; <string-name><surname>Adolph</surname>, <given-names>K. E</given-names></string-name>. <article-title>Free viewing gaze behavior in infants and adults</article-title>. <source>Infancy</source> <volume>21</volume>, <fpage>262</fpage>–<lpage>287</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Tran</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Cabral</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Cusack</surname>, <given-names>R</given-names></string-name>. <article-title>Online recruitment and testing of infants with mechanical turk</article-title>. <source>Journal of Experimental Child Psychology</source> <volume>156</volume>, <fpage>168</fpage>–<lpage>178</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="other"><string-name><surname>Brodmann</surname>, <given-names>K.</given-names></string-name> <article-title>Vergleichende Lokalisationslehre der Grosshirnrinde in ihren Prinzipien dargestellt auf Grund des Zellenbaues</article-title> (<publisher-name>Barth</publisher-name>, <year>1909</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="book"><string-name><surname>Felleman</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name> <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cerebral Cortex</source> <italic>(</italic><publisher-loc><italic>New York, NY</italic></publisher-loc>: 1991<italic>)</italic> <volume>1</volume>, <fpage>1</fpage>–<lpage>47</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Kaas</surname>, <given-names>J. H</given-names></string-name>. <article-title>Topographic maps are fundamental to sensory processing</article-title>. <source>Brain research bulletin</source> <volume>44</volume>, <fpage>107</fpage>–<lpage>112</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>White</surname>, <given-names>L. E.</given-names></string-name> &amp; <string-name><surname>Fitzpatrick</surname>, <given-names>D.</given-names></string-name> <article-title>Vision and cortical map development</article-title>. <source>Neuron</source> <volume>56</volume>, <fpage>327</fpage>–<lpage>338</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Fox</surname>, <given-names>P. T.</given-names></string-name>, <string-name><surname>Miezin</surname>, <given-names>F. M.</given-names></string-name>, <string-name><surname>Allman</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name><surname>Raichle</surname>, <given-names>M. E</given-names></string-name>. <article-title>Retinotopic organization of human visual cortex mapped with positron-emission tomography</article-title>. <source>Journal of Neuroscience</source> <volume>7</volume>, <fpage>913</fpage>–<lpage>922</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Schneider</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Noll</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name><surname>Cohen</surname>, <given-names>J. D</given-names></string-name>. <article-title>Functional topographic mapping of the cortical ribbon in human vision with conventional mri scanners</article-title>. <source>Nature</source> <volume>365</volume>, <fpage>150</fpage>–<lpage>153</lpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Henriksson</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Nurminen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hyvärinen</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Vanni</surname>, <given-names>S.</given-names></string-name> <article-title>Spatial frequency tuning in human retinotopic visual areas</article-title>. <source>Journal of Vision</source> <volume>8</volume>, <fpage>1</fpage>–<lpage>13</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Ellis</surname>, <given-names>C. T.</given-names></string-name> <etal>et al.</etal> <article-title>Retinotopic organization of visual cortex in human infants</article-title>. <source>Neuron</source> <volume>109</volume>, <fpage>2616</fpage>–<lpage>2626</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Loiotile</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Cusack</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Bedny</surname>, <given-names>M</given-names></string-name>. <article-title>Naturalistic audio-movies and narrative synchronize “visual” cortices across congenitally blind but not sighted individuals</article-title>. <source>Journal of Neuroscience</source> <volume>39</volume>, <fpage>8940</fpage>–<lpage>8948</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Yates</surname>, <given-names>T. S.</given-names></string-name> <etal>et al.</etal> <article-title>Neural event segmentation of continuous experience in human infants</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>119</volume>, <fpage>e2200257119</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="book"><string-name><surname>Turek</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Ellis</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Skalaban</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Turk-Browne</surname>, <given-names>N. B.</given-names></string-name> &amp; <string-name><surname>Willke</surname>, <given-names>T. L</given-names></string-name>. <article-title>Capturing shared and individual information in fmri data</article-title>. <source>In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>, <fpage>826</fpage>–<lpage>830</lpage> (<publisher-name>IEEE</publisher-name>, <year>2018</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Yates</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Ellis</surname>, <given-names>C. T.</given-names></string-name> &amp; <string-name><surname>Turk-Browne</surname>, <given-names>N. B</given-names></string-name>. <article-title>Functional networks in the infant brain during sleep and wake states</article-title>. <source>bioRxiv</source> <fpage>2023</fpage>–<lpage>02</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Arcaro</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Livingstone</surname>, <given-names>M. S</given-names></string-name>. <article-title>A hierarchical, retinotopic proto-organization of the primate visual system at birth</article-title>. <source>eLife</source> <volume>6</volume>, <fpage>e26196</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Development of visual cortex in human neonates is selectively modified by postnatal experience</article-title>. <source>eLife</source> <volume>11</volume>, <fpage>e78733</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="book"><string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name> &amp; <string-name><surname>Mishkin</surname>, <given-names>M.</given-names></string-name> <article-title>Two cortical visual systems</article-title>. In <person-group person-group-type="editor"><string-name><surname>Ingle D</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Goodale</surname> <given-names>MA</given-names></string-name></person-group> (ed.) <source>Analysis of visual behavior</source>, <fpage>549</fpage>–<lpage>586</lpage> (<publisher-name>MIT Press, Cambridge</publisher-name>, <year>1982</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Mruczek</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Arcaro</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Kastner</surname>, <given-names>S</given-names></string-name>. <article-title>Probabilistic maps of visual topography in human cortex</article-title>. <source>Cerebral Cortex</source> <volume>25</volume>, <fpage>3911</fpage>–<lpage>3931</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name> &amp; <string-name><surname>Gomez</surname>, <given-names>J</given-names></string-name>. <article-title>Third visual pathway, anatomy, and cognition across species</article-title>. <source>Trends in Cognitive Sciences</source> <volume>25</volume>, <fpage>548</fpage>–<lpage>549</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>DeLuca</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Devlin</surname>, <given-names>J. T.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S. M</given-names></string-name>. <article-title>Investigations into resting-state connectivity using independent component analysis</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>360</volume>, <fpage>1001</fpage>–<lpage>1013</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Lu</surname>, <given-names>K.-H.</given-names></string-name>, <string-name><surname>Jeong</surname>, <given-names>J. Y.</given-names></string-name>, <string-name><surname>Wen</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Liu</surname>, <given-names>Z</given-names></string-name>. <article-title>Spontaneous activity in the visual cortex is organized by visual streams</article-title>. <source>Human Brain Mapping</source> <volume>38</volume>, <fpage>4613</fpage>–<lpage>4630</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Arcaro</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>McMains</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Singer</surname>, <given-names>B. D.</given-names></string-name> &amp; <string-name><surname>Kastner</surname>, <given-names>S</given-names></string-name>. <article-title>Retinotopic organization of human ventral visual cortex</article-title>. <source>Journal of Neuroscience</source> <volume>29</volume>, <fpage>10638</fpage>–<lpage>10652</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Guntupalli</surname>, <given-names>J. S.</given-names></string-name> <etal>et al.</etal> <article-title>A model of representational spaces in human cortex</article-title>. <source>Cerebral cortex</source> <volume>26</volume>, <fpage>2919</fpage>–<lpage>2934</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Chen</surname>, <given-names>P.-H.</given-names></string-name>, <etal>et al.</etal> <article-title>A reduced-dimension fmri shared response model</article-title><collab>. In</collab> <source>NIPS</source>, vol. <volume>28</volume>, <fpage>460</fpage>–<lpage>468</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Smyser</surname>, <given-names>C. D.</given-names></string-name> <etal>et al.</etal> <article-title>Longitudinal analysis of neural network development in preterm infants</article-title>. <source>Cerebral Cortex</source> <volume>20</volume>, <fpage>2852</fpage>–<lpage>2862</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Wattam-Bell</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Reorganization of global form and motion processing during human visual development</article-title>. <source>Current Biology</source> <volume>20</volume>, <fpage>411</fpage>–<lpage>415</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Braddick</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>Atkinson</surname>, <given-names>J</given-names></string-name>. <article-title>Development of human visual function</article-title>. <source>Vision Research</source> <volume>51</volume>, <fpage>1588</fpage>–<lpage>1609</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>K. D.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Greenlee</surname>, <given-names>M. W</given-names></string-name>. <article-title>Estimating receptive field size from fmri data in human striate and extrastriate visual cortex</article-title>. <source>Cerebral Cortex</source> <volume>11</volume>, <fpage>1182</fpage>–<lpage>1190</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Srihasam</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Vincent</surname>, <given-names>J. L.</given-names></string-name> &amp; <string-name><surname>Livingstone</surname>, <given-names>M. S</given-names></string-name>. <article-title>Novel domain formation reveals proto-architecture in inferotemporal cortex</article-title>. <source>Nature Neuroscience</source> <volume>17</volume>, <fpage>1776</fpage>–<lpage>1783</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Tolhurst</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Thompson</surname>, <given-names>I</given-names></string-name>. <article-title>On the variety of spatial frequency selectivities shown by neurons in area 17 of the cat</article-title>. <source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source> <volume>213</volume>, <fpage>183</fpage>–<lpage>199</lpage> (<year>1981</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Busch</surname>, <given-names>E. L.</given-names></string-name> <etal>et al.</etal> <article-title>Hybrid hyperalignment: A single high-dimensional model of shared information embedded in cortical patterns of response and functional connectivity</article-title>. <source>NeuroImage</source> <volume>233</volume>, <issue>117975</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Yates</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Ellis</surname>, <given-names>C. T.</given-names></string-name> &amp; <string-name><surname>Turk-Browne</surname>, <given-names>N. B</given-names></string-name>. <article-title>Emergence and organization of adult brain function throughout child development</article-title>. <source>NeuroImage</source> <volume>226</volume>, <fpage>117606</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Ellis</surname>, <given-names>C. T.</given-names></string-name> &amp; <string-name><surname>Turk-Browne</surname>, <given-names>N. B</given-names></string-name>. <article-title>Infant fmri: A model system for cognitive neuroscience</article-title>. <source>Trends in Cognitive Sciences</source> <volume>22</volume>, <fpage>375</fpage>–<lpage>387</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Kumar</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>BrainIAK tutorials: User-friendly learning materials for advanced fMRI analysis</article-title>. <source>PLOS Computational Biology</source> <volume>16</volume>, <fpage>e1007549</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>What’s new in psychtoolbox-3</article-title>. <source>Perception</source> <volume>36</volume>, <issue>1</issue> (<year>2007</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Tootell</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal> <article-title>Functional analysis of human mt and related visual cortical areas using magnetic resonance imaging</article-title>. <source>Journal of Neuroscience</source> <volume>15</volume>, <fpage>3215</fpage>–<lpage>3230</lpage> (<year>1995</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Friedman</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Glover</surname>, <given-names>G. H</given-names></string-name>. <article-title>Report on a multicenter fmri quality assurance protocol</article-title>. <source>Journal of Magnetic Resonance Imaging</source> <volume>23</volume>, <fpage>827</fpage>–<lpage>839</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Gardner</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Merriam</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Schluppeck</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Besle</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Heeger</surname>, <given-names>D.</given-names></string-name> <article-title>mrtools: Analysis and visualization package for functional magnetic resonance imaging data</article-title>. <source>Zenodo</source> <volume>10</volume> (<year>2018</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal> <article-title>ibeat v2. 0: a multisite-applicable, deep learning-based pipeline for infant cerebral cortical surface reconstruction</article-title>. <source>Nature Protocols</source> <fpage>1</fpage>–<lpage>32</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Sereno</surname>, <given-names>M. I</given-names></string-name>. <article-title>Cortical surface-based analysis: I. segmentation and surface reconstruction</article-title>. <source>NeuroImage</source> <volume>9</volume>, <fpage>179</fpage>–<lpage>194</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Poppe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Arichi</surname>, <given-names>T</given-names></string-name>. <article-title>Individual focused studies of functional brain development in early human infancy</article-title>. <source>Current Opinion in Behavioral Sciences</source> <volume>40</volume>, <fpage>137</fpage>–<lpage>143</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Avants</surname>, <given-names>B. B.</given-names></string-name> <etal>et al.</etal> <article-title>A reproducible evaluation of ants similarity metric performance in brain image registration</article-title>. <source>NeuroImage</source> <volume>54</volume>, <fpage>2033</fpage>–<lpage>2044</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Fonov</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal> <article-title>Unbiased average age-appropriate atlases for pediatric studies</article-title>. <source>NeuroImage</source> <volume>54</volume>, <fpage>313</fpage>–<lpage>327</lpage> (<year>2011</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1053811910010062">http://www.sciencedirect.com/science/article/pii/S1053811910010062</ext-link>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Mazziotta</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>A probabilistic atlas and reference system for the human brain: International consortium for brain mapping (icbm)</article-title>. <source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source> <volume>356</volume>, <fpage>1293</fpage>–<lpage>1322</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Cox</surname>, <given-names>R. W</given-names></string-name>. <article-title>Afni: software for analysis and visualization of functional magnetic resonance neuroimages</article-title>. <source>Computers and Biomedical research</source> <volume>29</volume>, <fpage>162</fpage>–<lpage>173</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Dumoulin</surname>, <given-names>S. O.</given-names></string-name> &amp; <string-name><surname>Brewer</surname>, <given-names>A. A</given-names></string-name>. <article-title>Visual field maps in human cortex</article-title>. <source>Neuron</source> <volume>56</volume>, <fpage>366</fpage>–<lpage>383</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Efron</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Tibshirani</surname>, <given-names>R</given-names></string-name>. <article-title>Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy</article-title>. <source>Statistical Science</source> <fpage>54</fpage>–<lpage>75</lpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Haak</surname>, <given-names>K. V.</given-names></string-name> &amp; <string-name><surname>Beckmann</surname>, <given-names>C. F</given-names></string-name>. <article-title>Objective analysis of the topological organization of the human cortical visual connectome suggests three visual pathways</article-title>. <source>Cortex</source> <volume>98</volume>, <fpage>73</fpage>–<lpage>83</lpage> (<year>2018</year>).</mixed-citation></ref>
</ref-list>
<sec id="s6">
<title>Supplementary materials</title>
<table-wrap id="tbls1" orientation="portrait" position="float">
<label>Table S1:</label><caption><p>Demographic and dataset information for infant participants in the study. ‘Age’ is recorded in months. ‘Sex’ is the assigned sex at birth. ‘Retinotopy areas’ is the number of areas segmented from task-evoked retinotopy, averaged across hemispheres. Information about the movie data is separated based on analysis type: whereas all movie data is used for homotopy and ICA analyses, a subset of data is used for SRM. ‘Num.’ is the number of movies used. ‘Length’ is the duration in seconds of the run used for these analyses (includes both movie and rest periods). ‘Drops’ is the number of movies that include dropped periods. Runs’ says how many runs or pseudoruns of movie data there were. ‘Gaze’ is the percentage of the data where the participants were looking at the movie.</p></caption>
<graphic xlink:href="554318v1_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="float">
<label>Table S2:</label><caption><p>Number of participants per movie. The first column is the movie name, where ‘Drop-’ indicates that it was a movie containing alternating epochs of blank screens. ‘SRM’ indicates whether the movie is used in SRM analyses. The movies that are not included in SRM are used for homotopy and ICA. ‘Ret. infants’ and ‘Ret. adults’ refers to the number of participants with retinotopy data that saw this movie. ‘Infant SRM’ and ‘Adult SRM’ refer to the number of additional participants available to use for training the SRM but who did not have retinotopy data. ‘Infant Ages’ is the average age in months of the infant participants included in the SRM, with the range of ages included in parentheses.</p></caption>
<graphic xlink:href="554318v1_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls3" orientation="portrait" position="float">
<label>Table S3:</label><caption><p>Details for each movie used in this study.‘Name’ specifies the movie name. ‘Duration’ specifies the duration of the movie in seconds. Movies were edited to standardize length and remove inappropriate content. ‘Sound’ is whether sound was played during the movie. These sounds include background music, animal noises, and sound effects, but no language. ‘Description’ gives a brief description of the movie, as well as a current link to it when appropriate. All movies are provided in the data release.</p></caption>
<graphic xlink:href="554318v1_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1:</label>
<caption><p>Homotopic correlations between anatomically defined areas. A) Average correlation of the time course of activity evoked during movie watching for ventral and dorsal areas in an anatomical segmentation<sup><xref ref-type="bibr" rid="c25">25</xref></sup>. This is done for the left and right hemispheres separately, which is why the matrix is not diagonally symmetric. The triangles overlaid on the matrix corner highlights the area-wise comparisons used in B and C. Only areas that we were able to retinotopically mapped (i.e., those that overlap with <xref rid="fig1" ref-type="fig">Figure 1</xref>) were used for this analysis. B) Correlation of the same area and same stream (e.g., left ventral V1 and right ventral V1) versus the same area and different stream (e.g., left ventral V1 and right dorsal V1). Difference with bootstrap resampling: Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.37, p<italic>&lt;</italic>0.001. C) Correlation within the same stream between the same areas, adjacent areas (e.g., left ventral V1 and right ventral V2), or distal areas (e.g., left ventral V1 and right ventral V4). Difference with bootstrap resampling: Same <italic>&gt;</italic> Adjacent Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.09, p<italic>&lt;</italic>0.001; Adjacent <italic>&gt;</italic> Distal Δ<italic><sub>Fisher</sub> <sub>Z</sub></italic> M=0.18, p<italic>&lt;</italic>0.001. Grey lines represent individual participants. *** = p<italic>&lt;</italic>0.001 from bootstrap resampling</p></caption>
<graphic xlink:href="554318v1_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2:</label>
<caption><p>Cross-validation of the number of features in SRM. The movie data from all adult participants (<xref rid="tbls2" ref-type="table">Table S2</xref>) was split in half, with a 10 TR buffer between sets. The data were masked only to include occipital lobe voxels. The first half of the movie was used for training the SRM in all but one participant. The number of features learned by the SRM was varied across analyses from 1–25. The second half of the movie was then used to generate a shared response (i.e., the activity time course in each feature). To test the SRM, the held-out participant’s first half of data is used to learn a mapping of that participant into the SRM space (this mapping does not change the features learned and is not based on the second half of data). The second half of the held-out participant’s data is then mapped into the shared response space, like the other participants. Time-segment matching was performed on the shared response<sup><xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c31">31</xref></sup>. In brief, time-segment matching tests whether a segment of the data (10 TRs) in the held-out participant can be matched to its correct time point based on the other participants. This tests whether the SRM succeeds in making the held-out participant similar to the others. This analysis was performed on each participant and movie separately (each has a line). The dashed line is chance for time-segment matching, averaged across all movies and participants. The black solid line at features=10 reflects the number of features chosen.</p></caption>
<graphic xlink:href="554318v1_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92119.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dubois</surname>
<given-names>Jessica</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Inserm Unité NeuroDiderot, Université Paris Cité</institution>
</institution-wrap>
<city>Paris</city>
<country>France</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents <bold>valuable</bold> findings on the potential of short-movie viewing fMRI protocol to explore the functional and topographical organization of the visual system in awake infants and toddlers. Although the data are <bold>compelling</bold> given the difficulty of studying this population, the evidence presented is <bold>incomplete</bold> and would be strengthened by additional analyses to support the authors' claims. This study will be of interest to cognitive neuroscientists and developmental psychologists, especially those interested in using fMRI to investigate brain organisation in pediatric and clinical populations with limited fMRI tolerance.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92119.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
Ellis et al. investigated the functional and topographical organization of the visual cortex in infants and toddlers, as evidenced by movie-viewing data. They build directly on prior research that revealed topographic maps in infants who completed a retinotopy task, claiming that even a limited amount of rich, naturalistic movie-viewing data is sufficient to reveal this organization, within and across participants. Generating this evidence required methodological innovations to acquire high-quality fMRI data from awake infants (which have been described by this group, and elsewhere) and analytical creativity. The authors provide evidence for structured functional responses in infant visual cortex at multiple levels of analyses; homotopic brain regions (defined based on a retinotopy task) responded more similarly to one another than to other brain regions in visual cortex during movie-viewing; ICA applied to movie-viewing data revealed components that were identifiable as spatial frequency, and to a lesser degree, meridian maps, and shared response modeling analyses suggested that visual cortex responses were similar across infants/toddlers, as well as across infants/toddlers and adults. These results are suggestive of fairly mature functional response profiles in the visual cortex in infants/toddlers and highlight the potential of movie-viewing data for studying finer-grained aspects of functional brain responses, but further evidence is necessary to support their claims and the study motivation needs refining, in light of prior research.</p>
<p>Strengths:</p>
<p>
- This study links the authors' prior evidence for retinotopic organization of visual cortex in human infants (Ellis et al., 2021) and research by others using movie-viewing fMRI experiments with adults to reveal retinotopic organization (Knapen, 2021).</p>
<p>- Awake infant fMRI data are rare, time-consuming, and expensive to collect; they are therefore of high value to the community. The raw and preprocessed fMRI and anatomical data analyzed will be made publicly available.</p>
<p>Weaknesses:</p>
<p>
- The Methods are at times difficult to understand and in some cases seem inappropriate for the conclusions drawn. For example, I believe that the movie-defined ICA components were validated using independent data from the retinotopy task, but this was a point of confusion among reviewers. In either case: more analyses should be done to support the conclusion that the components identified from the movie reproduce retinotopic maps (for example, by comparing the performance of movie-viewing maps to available alternatives (anatomical ROIs, group-defined ROIs). Also, the ROIs used for the homotopy analyses were defined based on the retinotopic task rather than based on movie-viewing data alone - leaving it unclear whether movie-viewing data alone can be used to recover functionally distinct regions within the visual cortex.</p>
<p>- The authors previously reported on retinotopic organization of the visual cortex in human infants (Ellis et al., 2021) and suggest that the feasibility of using movie-viewing experiments to recover these topographic maps is still in question. They point out that movies may not fully sample the stimulus parameters necessary for revealing topographic maps/areas in the visual cortex, or the time-resolution constraints of fMRI might limit the use of movie stimuli, or the rich, uncontrolled nature of movies might make them inferior to stimuli that are designed for retinotopic mapping, or might lead to variable attention between participants that makes measuring the structure of visual responses across individuals challenging. This motivation doesn't sufficiently highlight the importance or value of testing this question in infants. Further, it's unclear if/how this motivation takes into account prior research using movie-viewing fMRI experiments to reveal retinotopic organization in adults (e.g., Knapen, 2021). Given the evidence for retinotopic organization in infants and evidence for the use of movie-viewing experiments in adults, an alternative framing of the novel contribution of this study is that it tests whether retinotopic organization is measurable using a limited amount of movie-viewing data (i.e., a methodological stress test). The study motivation and discussion could be strengthened by more attention to relevant work with adults and/or more explanation of the importance of testing this question in infants (is the reason to test this question in infants purely methodological - i.e., as a way to negate the need for retinotopic tasks in subsequent research, given the time constraints of scanning human infants?).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92119.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
This manuscript shows evidence from a dataset with awake movie-watching in infants, that the infant brain contains areas with distinct functions, consistent with previous studies using resting state and awake task-based infant fMRI. However, substantial new analyses would be required to support the novel claim that movie-watching data in infants can be used to identify retinotopic areas or to capture within-area functional organization.</p>
<p>Strengths:</p>
<p>
The authors have collected a unique dataset: the same individual infants both watched naturalistic animations and a specific retinotopy task. These data position the authors to test their novel claim, that movie-watching data in infants can be used to identify retinotopic areas.</p>
<p>Weaknesses:</p>
<p>
To claim that movie-watching data can identify retinotopic regions, the authors should provide evidence for two claims:</p>
<p>- Retinotopic areas defined based only on movie-watching data, predict retinotopic responses in independent retinotopy-task-driven data.</p>
<p>- Defining retinotopic areas based on the infant's own movie-watching response is more accurate than alternative approaches that don't require any movie-watching data, like anatomical parcellations or shared response activation from independent groups of participants.</p>
<p>Both of these analyses are possible, using the (valuable!) data that these authors have collected, but these are not the analyses that the authors have done so far. Instead, the authors report the inverse of (1): regions identified by the retinotopy task can be used to predict responses in the movies. The authors report one part of (2), shared responses from other participants can be used to predict individual infants' responses in the movies, but they do not test whether movie data from the same individual infant can be used to make better predictions of the retinotopy task data, than the shared response maps.</p>
<p>So to be clear, to support the claims of this paper, I recommend that the authors use the retinotopic task responses in each individual infant as the independent &quot;Test&quot; data, and compare the accuracy in predicting those responses, based on:</p>
<p>- The same infant's movie-watching data, analysed with MELODIC, when blind experimenters select components for the SF and meridian boundaries with no access to the ground-truth retinotopy data.</p>
<p>
- Anatomical parcellations in the same infant.</p>
<p>
- Shared response maps from groups of other infants or adults.</p>
<p>
- (If possible, ICA of resting state data, in the same infant, or from independent groups of infants).</p>
<p>Or, possibly, combinations of these techniques.</p>
<p>If the infant's own movie-watching data leads to improved predictions of the infant's retinotopic task-driven response, relative to these existing alternatives that don't require movie-watching data from the same infant, then the authors' main claim will be supported.</p>
<p>The proposed analysis above solves a critical problem with the analyses presented in the current manuscript: the data used to generate maps is identical to the data used to validate those maps. For the task-evoked maps, the same data are used to draw the lines along gradients and then test for gradient organization. For the component maps, the maps are manually selected to show the clearest gradients among many noisy options, and then the same data are tested for gradient organization. This is a double-dipping error. To fix this problem, the data must be split into independent train and test subsets.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92119.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript reports data collected in awake toddlers recording BOLD while watching videos. The authors analyse the BOLD time series using two different statistical approaches, both very complex but do not require any a priori determination of the movie features or contents to be associated with regressors. The two main messages are that 1) toddlers have occipital visual areas very similar to adults, given that an SRM model derived from adult BOLD is consistent with the infant brains as well; 2) the retinotopic organization and the spatial frequency selectivity of the occipital maps derived by applying correlation analysis are consistent with the maps obtained by standard and conventional mapping.</p>
<p>Clearly, the data are important, and the author has achieved important and original results. However, the manuscript is totally unclear and very difficult to follow; the figures are not informative; the reader needs to trust the authors because no data to verify the output of the statistical analysis are presented (localization maps with proper statistics) nor so any validation of the statistical analysis provided. Indeed what I think that manuscript means, or better what I understood, may be very far from what the authors want to present, given how obscure the methods and the result presentation are.</p>
<p>In the present form, this reviewer considers that the manuscript needs to be totally rewritten, the results presented each technique with appropriate validation or comparison that the reader can evaluate.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.92119.1.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ellis</surname>
<given-names>C. T.</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Yates</surname>
<given-names>T. S.</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Arcaro</surname>
<given-names>M. J.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4612-9921</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Turk-Browne</surname>
<given-names>N. B.</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>We are grateful to the reviewers for their thorough and thoughtful critiques, including their agreement on the significant value of this dataset. We intend to respond to their comments in full with a revision in the near future. However, we would like to make an initial comment at this stage. A key concern raised by the reviewers was that the analyses described do not adequately support the claim that &quot;movie-watching data can identify retinotopic regions&quot; (quoted from R2, similar sentiment expressed by R1). To be clear, we agree with this assessment. Our primary aim was not to identify visual areas with movie-watching data. Rather, our focus was on how movies can reveal fine-grained organization in infant visual cortex, which would support their potential utility for understanding the development of dynamic visual processing. To demonstrate this potential, we tested and found that maps of visual activity generated from movies are significantly similar to those generated by a retinotopy task. Nevertheless, we did not intend to argue that movie-based maps are sufficiently accurate to replace task-based retinotopic maps when defining visual areas, nor did we test this possibility. We accept that this point was unclear in the original manuscript and will make edits to avoid this miscommunication. We also plan to incorporate the reviewers’ many other helpful recommendations, including addressing concerns about the clarity of the presentation and double dipping, as well as adding several new analyses we hope will provide greater confidence in the findings and interpretation.</p>
</body>
</sub-article>
</article>