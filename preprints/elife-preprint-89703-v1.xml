<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89703</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89703</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89703.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Get the gist of the story: Neural map of topic keywords in multi-speaker environment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7527-8280</contrib-id>
<name>
<surname>Park</surname>
<given-names>Hyojin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3994-1006</contrib-id>
<name>
<surname>Gross</surname>
<given-names>Joachim</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Centre for Human Brain Health (CHBH), School of Psychology, University of Birmingham</institution>, Birmingham, <country>United Kingdom</country></aff>
<aff id="a2"><label>2</label><institution>Institute for Biomagnetism and Biosignalanalysis, University of Muenster</institution>, Muenster, <country>Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence to: Dr. Hyojin Park: <email>h.park@bham.ac.uk</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-09-18">
<day>18</day>
<month>09</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89703</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-06-06">
<day>06</day>
<month>06</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2022-05-05">
<day>05</day>
<month>05</month>
<year>2022</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.05.05.490770"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Park &amp; Gross</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Park &amp; Gross</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89703-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Neural representation of lexico-semantics in speech processing has been revealed in recent years. However, to date, how the brain makes sense of the higher-level semantic gist (topic keywords) of a continuous speech remains mysterious. Capitalizing on a generative probabilistic topic modelling algorithm on speech materials to which participants listened while their brain activities were recorded by Magnetoencephalography (MEG), here we show spatio-temporal neural representation of topic keywords in a multi-speaker environment where task-relevant (attended) and -irrelevant (unattended) speech co-exits. We report the difference of neural representation between salient and less salient semantic gist of both attended and unattended speech. Moreover, we show that greater sensitivity to semantically salient unattended speech in the left auditory and motor cortices negatively mediates attended speech comprehension.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Magnetoencephalography (MEG)</kwd>
<kwd>Electroencephalography (EEG)</kwd>
<kwd>Natural Language Processing (NLP)</kwd>
<kwd>Topic model</kwd>
<kwd>Latent Dirichlet Allocation (LDA)</kwd>
<kwd>multivariate Temporal Response Function (mTRF)</kwd>
<kwd>Receptive field model</kwd>
<kwd>Natural speech</kwd>
<kwd>Semantic processing</kwd>
<kwd>Multi-speaker</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Humans have the remarkable ability to get the gist of the story. Imagine that you are listening to a talk without any prior information about the topic of the talk. As the talk unfolds, you will identify keywords that will enable you to infer the topic of the talk. How does the human brain extract topic keywords from the story, and how does the brain distinguish critical, relevant words (i.e., keywords) from less critical, irrelevant words? What are the spatio-temporal characteristics in brain activity that reflect the identification and processing of topic keywords, particularly in the context of effortful listening, for instance, at a cocktail party?</p>
<p>An increasing number of studies has demonstrated that neural representations of acoustic (<xref ref-type="bibr" rid="c50">Park et al., 2015</xref>; <xref ref-type="bibr" rid="c49">Park et al., 2016</xref>; <xref ref-type="bibr" rid="c26">Hauswald et al., 2018</xref>; <xref ref-type="bibr" rid="c51">Park et al., 2018b</xref>; <xref ref-type="bibr" rid="c48">Park et al., 2018a</xref>; <xref ref-type="bibr" rid="c4">Biau et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Haider et al., 2022</xref>) and linguistic, e.g., semantic features (<xref ref-type="bibr" rid="c35">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="c57">Strauss et al., 2014</xref>; <xref ref-type="bibr" rid="c30">Huth et al., 2016</xref>; <xref ref-type="bibr" rid="c63">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="c10">Broderick et al., 2019</xref>; <xref ref-type="bibr" rid="c31">Kaufeld et al., 2020</xref>) of naturalistic auditory or audiovisual speech are quantifiable in Magnetoencephalography (MEG) or Electroencephalography (EEG) recordings based on frequency-domain synchronization analysis or time-domain regression analysis. Furthermore, recent developments of natural language processing (NLP) models based on machine learning algorithms, such as word vectors (<xref ref-type="bibr" rid="c42">Mikolov et al., 2013</xref>), have brought breakthroughs not only to the area of artificial intelligence (AI), for example, speech/text recognition, machine translations (e.g., speech-to-text), but also to the neuroscientific study of rich, naturalistic speech stimuli (<xref ref-type="bibr" rid="c11">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="c54">Pereira et al., 2018</xref>) or movie (<xref ref-type="bibr" rid="c44">Nishida et al., 2021</xref>). For example, <xref ref-type="bibr" rid="c11">Broderick et al. (2018)</xref> has shown that using a trained computational language model, semantic vectors for content words of the continuous stimuli, which were given to participants during EEG recordings, can be used to identify semantic neural correlates.</p>
<p>Traditionally, neural semantic processing has been studied through semantic violations (<xref ref-type="bibr" rid="c35">Kutas and Federmeier, 2011</xref>). More recently, semantic analysis has been extended to word-level prediction (<xref ref-type="bibr" rid="c63">Wang et al., 2018</xref>) and even to the processing of continuous speech (<xref ref-type="bibr" rid="c11">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="c10">Broderick et al., 2019</xref>; <xref ref-type="bibr" rid="c33">Koskinen et al., 2020</xref>). However, the field still lacks the quantification of neural processes involved in higher-level semantic processing, such as investigating how the brain understands the topic of the story (main idea, keywords, semantic core). Comprehension of sentences in continuous speech requires multiple levels of hierarchical processing. One of the key components in this processing is the understanding of word meanings, i.e., lexico-semantic processing, which is a critical building block supporting the construction of a semantic core of the story. However, the understanding of latent meanings in idiomatic expressions, for example, cannot be attained through word-level processing. The comprehension rather builds upon complex contextual information. The neural mechanism underlying such processing remains unclear. As such, in the current study, we aim to investigate how the brain extracts the main topic in a continuous speech and how this process is implemented in multi-speaker environment where task-relevant (attended) and -irrelevant (unattended) stimuli co-exist.</p>
<p>To investigate brain responses engaging in the understanding of the main topic of the story, we first identified topic keywords in the spoken speech materials delivered to the participants during MEG measurement. Here we used a topic model algorithm, Latent Dirichlet Allocation (LDA), a text mining technique that has been developed to delineate short descriptions of the collection of text corpora (<xref ref-type="bibr" rid="c7">Blei et al., 2003</xref>) on speech chunks segmented in a perceptually relevant manner. LDA analysis results in sets of topic keywords and the probability of the words belonging to the topic. Thus, LDA enables identifying the topic probability for each speech chunk. For characterizing the brain representations of the topic keywords, the segmented speech chunks were sorted according to topic probability and divided into two conditions (high vs. low topic probability condition). Speech envelope in each condition and corresponding brain activities were used to fit encoding and decoding models of the multivariate temporal response function (mTRF).</p>
<p>Results from the decoding model prediction accuracy show that speech chunks with high topic probability are better reconstructed when compared to speech chunks with low topic probability within the attended and unattended talk. Strikingly, this was evident between speech chunks with high topic probability from the unattended talk and speech chunks with low topic probability from the attended talk. Moreover, we provide evidence that the processing of unattended speech chunks with semantic gist in the left frontal, auditory and motor areas negatively mediates attended speech comprehension. We show, for the first time to our knowledge, how the brain processes topic keywords in a continuous story in a challenging listening situation in a behaviourally relevant manner.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Speech data processing and computational topic modelling</title>
<sec id="s2a1">
<title>Segmentation of natural speech</title>
<p>Embedded linguistic units (e.g., embedded phrases or conjoined sentences) in natural speech and its temporal complexity with varying lengths of phrases and sentences have made it difficult to study high-level semantic processing beyond word-level during natural speech perception. Those time-varying semantic chunks are often marked by intermittent pauses (i.e., silences) from the speaker before moving to the next phrases or sentences. In order to derive topic keywords from a talk via a computational modelling algorithm, we first segmented each talk into phrases or sentences based on these acoustic properties. Using the library Syllable Nuclei (<xref ref-type="bibr" rid="c18">de Jong and Wempe, 2009</xref>) in Praat software (<xref ref-type="bibr" rid="c8">Boersma and Weenink, 2018</xref>), we obtained acoustic chunks with parameters of 0.25 s minimum pause (silence) durations, -25 dB silence threshold, and 1 s minimum duration of each speech chunk (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). We detected 129 speech chunks on average with a mean duration of 3.37 s from the seven talks used in the current study. We calculated speech rates by the number of syllables divided by the duration of the talk. For detecting peak intensity, 2 dB above the median was used as a threshold (the minimum dip between peaks). The profiles of the talks – for example, the number of syllables, speech rate, articulation rate (the number of syllables divided by phonation (speaking) time), the number of speech chunks for each talk – are reported in Supplementary Table 1.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Segmentation of speech and topic modelling application.</title>
<p><bold>a, Perceptually driven segmentation of speech</bold>. A continuous speech was segmented into phrases or sentences in a perceptually relevant manner. Using the library Syllable Nuclei in Praat, we obtained acoustic speech chunks by the following thresholding parameters: pause (silence) duration (minimum 0.25 s long), loudness (−25 dB silence), and the length of speech chunks (minimum 1 s long). We detected 129 speech chunks on average with a mean duration of 3.37 s for seven continuous talks used in the study (also see Supplementary Table 1). A snapshot from one example talk is shown in the figure. Each row depicts a raw speech signal, spectrogram with intensity (in yellow) and pitch (in blue), the number of syllables, and annotations. <bold>b, Text preprocessing and topic modelling</bold>. Left column: An example of segmented speech chunks from a representative talk is shown. In this talk, 135 speech chunks were obtained. Middle column: Preprocessing of annotations of spoken speech materials was performed using a python library, spaCy, through the following three steps: tokenization, lemmatization and removal of stop words. Right column: Schematic illustration of topic modelling algorithm, Latent Dirichlet Allocation (LDA), is displayed. A fixed number of topics was specified in advance (4 in the current study), and bi-gram model was used in the topic model to optimally capture topic messages. <bold>c, Extracted topic keywords</bold>. Out of the LDA model, documents (referred to as speech chunks in the current study) assigned to topic <italic>t</italic> and words with high probability for topic <italic>t</italic> are obtained as outputs (document-topic matrix). The most common words with the highest probability are shown for each topic. <bold>d, Distribution of topic probability across speech chunks in a representative talk</bold>. Vectors of topic probabilities for each speech chunk were depicted using a stacked bar chart (topic mixture) where x- and y-axes depict topic probability and the identity of speech chunks, respectively. Color-coded bars in each row represent the probability distribution across 4 topics in a given speech chunk. In this sample talk, topic 3 has the highest probability across all speech chunks, which indicates the representative topic keywords for this talk. <bold>e, t-SNE visualization of topics colored and sized by topic probability</bold>. The document-topic matrix from the LDA model was subjected to t-SNE dimensionality reduction model, which was then fitted to be visualized in 2-d embedded space scatterpie chart. Each data point as a scatterpie chart represents each speech chunk which is clustered into a certain topic according to the highest probability for the topic. The scatterpie chart also shows the probability distribution over topics. <bold>f, Speech chunks sorted according to the topic probability of the representative topic</bold> (topic 3 in this example talk). The speech chunks shown in b were sorted from highest to lowest topic probability as to topic 3. To investigate the neural processing of topic representation, we split these speech chunks into high vs. low topic probability conditions. <bold>g, Segmentation and allocation of the MEG and speech signals into topic probability conditions</bold>. Corresponding brain data at sensor and source level and auditory speech envelope were split into high vs. low topic probability conditions as well, resulting in trial-based epochs.</p></caption>
<graphic xlink:href="490770v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="490770v1_fig1a.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2a2">
<title>Text preprocessing</title>
<p>Transcriptions of TED talks for video filmings were double-checked after the recordings by a professional speaker for a possible difference between transcriptions and actual speech during the filmings. Different parts were updated, and these finalized transcriptions were used for the annotations of the segmented speech chunks derived by the Syllable Nuclei (see the first column in <xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Preprocessing of text materials was performed using a python library, spaCy (<xref ref-type="bibr" rid="c29">Honnibal and Montani, 2017</xref>), an open-source software library for advanced natural language processing (NLP), which is similar to NLTK (Natural Language Toolkit) – a popular open-source library released in 2001 (<xref ref-type="bibr" rid="c5">Bird et al., 2009</xref>). The preprocessing can be summarized into the following three steps (the middle column in <xref rid="fig1" ref-type="fig">Fig. 1b</xref>) – 1) tokenization, 2) lemmatization, and 3) removal of stop words, described below in more detail.</p>
<p>Using an English language model - OntoNotes 5 (<xref ref-type="bibr" rid="c64">Weischedel et al., 2013</xref>) by Linguistic Data Consortium (LDC), trained on a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) - embedded in spaCy, raw texts were tokenized into component pieces, e.g., prefix, suffix, infix and other exceptions. Next, these basic building blocks of document objects (tokens) were lemmatized, which is similar to stemming (word reduction, e.g., “boat” for “boats”, “boating”, “boater”) in other libraries, which works well, but with some issues given numerous exceptions in English (e.g., irregular verbs such as begin-began-begun). Lemmatization implements beyond the stemming (word reduction) and considers a language’s full vocabulary to apply a morphological analysis to words. For instance, the lemma of ‘was’ is ‘be’, and the lemma of ‘mice’ is ‘mouse’. In some cases, the lemma of ‘meeting’ might be ‘meet’ or ‘meeting’ depending on the context (e.g., “I’m meeting my boss tomorrow at the meeting.”). Lemmatization in spaCy is assumed to provide correct lemmas even in such cases by considering surrounding texts to determine a word’s part of speech, which has motivated our use of the spaCy library in the preprocessing steps for the subsequent topic model analysis. Then, stop words, which are presumed to be semantically uninformative in representing a content of a text, such as “a/an”, “the”, be-verbs, “and”, “with”, “seems”, “also” etc. were filtered out from the text. We used a ∼300 stop words list built in a machine learning python library, scikit-learn (<xref ref-type="bibr" rid="c52">Pedregosa et al., 2011</xref>) feature extraction module for texts (sklearn.feature_extraction.text). Furthermore, to rule out the possibility of emotional valence driven topic keywords representation in the brain, we confirmed that all the speech materials used in the study are with neutral sentiment by sentiment analysis (see Supplementary Table 2 for more details).</p>
</sec>
<sec id="s2a3">
<title>Topic modelling</title>
<p>Topic models are collective algorithms that try to uncover the hidden thematic structure in document collections (<xref ref-type="bibr" rid="c6">Blei, 2012</xref>). Recent advances in state-of-the-art machine learning algorithms and the era of big data enable the developments of the subfield of Natural Language Processing (NLP) at different levels, such as syntactic, lexical semantics, or discourse. The current study aims to map spatiotemporal neural representation of topic processing during natural speech perception, so we used one of the widely-used topic modelling algorithms, Latent Dirichlet Allocation (LDA) (<xref ref-type="bibr" rid="c7">Blei et al., 2003</xref>) in order to extract topic keywords in a talk. Latent Semantic Analysis (LSA) (<xref ref-type="bibr" rid="c37">Landauer and Dumais, 1997</xref>) is another widely used topic model (<xref ref-type="bibr" rid="c28">Hoffman, 2019</xref>). LDA and LSA are conceptually similar and allow us to efficiently analyze a large volume of text data by clustering documents into a certain number of topics. They are unsupervised learning algorithms as the text data is unlabelled. The assumption behind the topic modelling is that documents with similar topics use similar groups of words, so latent topics can be detected by the frequent co-occurrence of groups of words in documents across the corpus. Both LDA and LSA algorithms operate on word-document co-occurrence matrices in a low-dimensional space so that the occurrence of sets of words in multiple documents can be computed. LSA uses singular value decomposition (SVD) that the created singular vector of the co-occurrence assumed to be orthogonal. LDA is a generative probabilistic model as its name based off (Dirichlet distribution) that produces probabilities that a word derived from the distribution for a particular topic. More detailed differences between LSA and LDA are discussed in the previous literature (<xref ref-type="bibr" rid="c22">Griffiths et al., 2007</xref>; <xref ref-type="bibr" rid="c53">Pereira et al., 2011</xref>).</p>
</sec>
<sec id="s2a4">
<title>Latent Dirichlet Allocation (LDA) analysis</title>
<p>LDA topic model analysis was performed using the scikit-learn (<xref ref-type="bibr" rid="c52">Pedregosa et al., 2011</xref>) (class: sklearn.decomposition.LatentDirichletAllocation), following the preprocessing of texts described above. First, the preprocessed text documents are converted to a matrix of token counts (i.e., a sparse representation of token counts, also known as document-term matrix via scikit-learn class, sklearn.feature_extraction.text.CountVectorizer), then the LDA model is fitted to the document-term matrix.</p>
<p>LDA (Blei; Chen; <xref ref-type="bibr" rid="c7">Blei et al., 2003</xref>; <xref ref-type="bibr" rid="c6">Blei, 2012</xref>) is a generative probabilistic model for collections of texts implementing a three-level hierarchical Bayesian model. In the model, each item of a collection is modelled as a mixture over an underlying set of topics, in which each topic is modelled as a mixture over an underlying set of topic probabilities. These probabilities provide an explicit representation of a document. The algorithm is typically used to extract topics over different documents to classify the documents according to their topics. Here we used the LDA to derive topic keywords that best represent the main idea of each talk across the segmented speech chunks described above. Also, the main idea of a talk is better represented in a phrase, not a single word, for example, “save energy” rather than “save” or “energy”, so we used bi-gram (two words) model in the algorithm to identify topic keywords. For fitting the model, a fixed number of topics should be specified in advance, and we set 4 in the current study.</p>
<p>LDA model represents documents as mixtures of topics with certain probabilities. The assumption of the model posits that documents (speech chunks in the current study) are probability distributions over latent topics (top figure in the third column in <xref rid="fig1" ref-type="fig">Fig. 1b</xref>), and topics are probability distributions over words (bi-grams in the current study) in a given corpus (bottom figure in the third column in <xref rid="fig1" ref-type="fig">Fig. 1b</xref>). In the model, it is assumed that documents have been produced as follows: First, the number of words in the document is decided; second, a topic mixture is chosen for the document according to a probability distribution over a fixed set of k number of topics which has been set in advance (k=4), for example, 60% of “machine age”, 20% of “industrial revolution”, 10% of “race (with the) machine(s)”, 10% of “productivity time”; third, each word is generated in the document by choosing a topic according to the multinomial distribution in the previous step and using the topic to generate the word based on the topic’s multinomial distribution. This way, the LDA model learns the topic representation of each document and the words associated with each topic. While going through each document, the model randomly assigns each word in the document to one of the k topics. This process results in topic representations of all the documents and word distributions of all topics, albeit initial topics are likely to be inadequate. This step is iterated over every word in every document to improve the model performance.</p>
<p>For every word (bi-gram in the current study) in every document (speech chunk in the current study) and for each topic:
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="490770v1_ueqn1.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
Then the model reassigns <italic>w</italic> a new topic where topic <italic>t</italic> with probability:
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="490770v1_ueqn2.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
that represents the probability that topic <italic>t</italic> generated word <italic>w</italic>.</p>
<p>Following the process above, documents assigned to topic <italic>t</italic> and words with a high probability for topic <italic>t</italic> are obtained as outputs (document-topic matrix). Here, the most common words (bi-grams in the current study) with the highest probability for topic <italic>t</italic> can be obtained, as shown in <xref rid="fig1" ref-type="fig">Fig. 1c</xref>. Also, vectors of topic probabilities for each document (speech chunk in the current study) were depicted using a stacked bar chart (topic mixture), as shown in <xref rid="fig1" ref-type="fig">Fig. 1d</xref>.</p>
</sec>
<sec id="s2a5">
<title>T-SNE representation of LDA result</title>
<p>In addition to a figure for topic mixture showing the probability distribution of each speech chunk over topics, the outputs from the LDA topic model were analyzed using the t-SNE (t-distributed Stochastic Neighbor Embedding) technique (<xref ref-type="bibr" rid="c61">van der Maaten and Hinton, 2008</xref>) via scikit-learn (class: sklearn.manifold.TSNE). T-SNE is an unsupervised, nonlinear dimensionality reduction algorithm and is used as a tool for visualizing high-dimensional data by converting pairwise similarities (Euclidean distance as a metric) between data points to joint probabilities by minimizing the Kullback-Leibler divergence (<xref ref-type="bibr" rid="c34">Kullback and Leibler, 1951</xref>) between the joint probabilities of the low-dimensional embedding and the original high-dimensional data. Student t-distribution is used for the creation of low-dimensional space instead of a Gaussian distribution for better modelling of distances since t-distribution is heavier-tailed than the Gaussian distribution. Returning values are embeddings of the training data in low-dimensional (2-d) space represented (t-SNE dimensions 1 and 2). The document-topic matrix from LDA topic model analysis was subjected to t-SNE dimensionality reduction model which was then fitted to be visualized in 2-d embedded space. The output from t-SNE technique was visualized using scatterpie chart (<xref ref-type="bibr" rid="c66">Yu, 2020</xref>) via ggplot2 (<xref ref-type="bibr" rid="c65">Wickham, 2016</xref>) in R (2020), as shown in <xref rid="fig1" ref-type="fig">Fig. 1e</xref>. Each data point as a scatterpie chart represents each speech chunk that is clustered into a certain topic <italic>t</italic> according to the topic’s highest probability. In addition, the scatterpie chart shows the probability distribution over topics in a given speech chunk.</p>
</sec>
</sec>
<sec id="s2b">
<title>Analysis of neural processing of topic keywords</title>
<sec id="s2b1">
<title>Selection of representative topic keywords</title>
<p>Topic probabilities for each speech chunk were averaged across all speech chunks in a talk in order to identify the representative topic keywords (bi-gram in the current study). For instance, averaging topic probabilities across speech chunks for each topic 1-4 (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>) produced topic keywords with the highest topic probability (e.g., topic 3 in this sample talk (see <xref rid="fig1" ref-type="fig">Fig. 1c</xref>), “The key to growth? Race with the machines” by Erik Brynjolfsson). Next, the speech chunks were sorted from highest to lowest topic probability as to this topic 3. In order to investigate the neural processing of topic representation, we split these speech chunks into high and low topic probability conditions (<xref rid="fig1" ref-type="fig">Fig. 1f</xref>). Corresponding brain data both at sensor and source level were split to high and low topic probability conditions as well, resulting in trial-based epochs, as shown in <xref rid="fig1" ref-type="fig">Fig. 1g</xref>.</p>
</sec>
<sec id="s2b2">
<title>Topic keywords processing of attended vs. unattended talk</title>
<p>The current study aims to identify how the brain processes topic keywords not only in an attended talk, but also in an unattended talk. In the context of natural speech with competing speakers, highly semantic words and sentences in unattended talks can interfere with focused attention to attended talks due to intermittent silences and pauses and semantically less important contents in attended talks, which enables shifting attention to unattended talks. In order to identify the effect of highly semantic speech chunks in unattended talks on the attended speech processing, we performed the same analysis described above for unattended talks as well.</p>
</sec>
</sec>
<sec id="s2c">
<title>Global brain activity reflects topic probability in speech chunks</title>
<sec id="s2c1">
<title>Mean-field power analysis at sensor level</title>
<p>Speech chunks with high and low topic probability in both attended and unattended talks were first yielded to mean-field power analysis at the sensor level for a sanity check. The averaged power across left auditory sensors (inset) was depicted in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, and effects are shown for each condition (high and low topic probability) for attended and unattended, as well as combined effects for topic probability (regardless of attention) and attention (regardless of topic probability).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Mean-field power analysis at sensor level.</title>
<p><bold>a</bold>, Mean-field power from 0.25 s to 0.5 s time-locked to the onset of speech chunks over left auditory sensors (inset) were averaged and compared between all combinations of topic probability (high, low) and attention (attended, unattended) as follows via paired t-test: attended, high vs. attended, low: t<sub>43</sub> = 2.21, p = 0.03; attended, high vs. unattended, high: t<sub>43</sub> = 3.21, p = 0.002; attended, high vs. unattended, low: t<sub>43</sub> = 4.35, p &lt; 0.0001; attended, low vs. unattended, high: t<sub>43</sub> = 1.51, p = 0.14; attended, low vs. unattended, low: t<sub>43</sub> = 2.91, p = 0.005; unattended, high vs. unattended, low: t<sub>43</sub> = 1.44, p = 0.16; attended vs. unattended talk pooling across high and low topic probability: t<sub>43</sub> = 4.17, p = 0.0001; high vs. low pooling across attended and unattended: t<sub>43</sub> = 2.62, p = 0.01). <bold>b-d</bold>, Temporally unfolded mean-field power by root mean square averaged over the same left auditory sensors during -0.05 s to 1 s time-locked to the speech chunk onset are displayed for all individual conditions (<bold>b</bold>) and combined effects for topic probability (<bold>c</bold>) and attention (<bold>d</bold>).</p></caption>
<graphic xlink:href="490770v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>First, we averaged mean-field power from 0.25 s to 0.5 s time-locked to the onset of speech chunks over left auditory sensors and then statistically compared all combinations of topic probability (high, low) and attention (attended, unattended) (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>, paired <italic>t</italic>-test; attended, high vs. attended, low: t<sub>43</sub> = 2.21, p = 0.03; attended, high vs. unattended, high: t<sub>43</sub> = 3.21, p = 0.002; attended, high vs. unattended, low: t<sub>43</sub> = 4.35, p &lt; 0.0001; attended, low vs. unattended, high: t<sub>43</sub> = 1.51, p = 0.14; attended, low vs. unattended, low: t<sub>43</sub> = 2.91, p = 0.005; unattended, high vs. unattended, low: t<sub>43</sub> = 1.44, p = 0.16). In addition, speech chunks from the attended and unattended talk comprising high and low topic probability, i.e., regardless of topic probability, were compared (attended vs. unattended: t<sub>43</sub> = 4.17, p = 0.0001). Speech chunks from high and low topic probability comprising attended and unattended talk, i.e., regardless of attention, were compared as well (high vs. low topic probability: t<sub>43</sub> = 2.62, p = 0.01). To elaborate this, we show mean-field power by root mean square in <xref rid="fig2" ref-type="fig">Fig. 2b-d</xref> averaged over the same left auditory sensors during -0.05 s to 1 s for each condition (b) and combined effects for topic probability (c) and attention (d).</p>
<p>Speech chunks with high probability from attended talks show the strongest field power. Across both levels of topic probability, attended talk shows stronger field power than unattended talk (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). Across both levels of attention, speech chunks with high topic probability show stronger mean field power than speech chunks with low topic probability (<xref rid="fig2" ref-type="fig">Fig. 2d</xref>). The peak around 0.4 s in <xref rid="fig2" ref-type="fig">Fig. 2b-d</xref> is likely to reflect meaning processing in the brain (<xref ref-type="bibr" rid="c35">Kutas and Federmeier, 2011</xref>).</p>
</sec>
</sec>
<sec id="s2d">
<title>Receptive field model estimation for neural topic keywords processing</title>
<p>We next implemented a receptive field model, also known as a multivariate temporal response function (mTRF) (<xref ref-type="bibr" rid="c15">Crosse et al., 2016</xref>), which can be interpreted as a linear filter in the brain processing a stimulus feature (speech envelope S(t)) mapped onto the continuous neural responses (MEG response over time, r(t)). The approach has been used in recent studies to study the mapping between brain responses and naturalistic speech feature representations (<xref ref-type="bibr" rid="c11">Broderick et al., 2018</xref>; <xref ref-type="bibr" rid="c59">Teng et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Haider et al., 2022</xref>). The schematic flowchart for our analysis is shown in <xref rid="fig2" ref-type="fig">Fig. 2a</xref>, which was adapted from <xref rid="fig1" ref-type="fig">Fig. 1</xref> in <xref ref-type="bibr" rid="c15">Crosse et al. (2016)</xref>.</p>
<p>The stimulus feature-neural response mapping can be modelled bidirectionally (forward and backward modelling). In the encoding (forward: stimulus to brain) model, the model function mathematically describes how the speech amplitude envelope is encoded in MEG responses which can be interpreted as underlying neural generators (<xref ref-type="bibr" rid="c15">Crosse et al., 2016</xref>). To derive the encoding model estimation, a time-lagged (−0.2 s to 0.8 s in steps of 8 ms) speech envelope from each epoch in each condition (high, low topic probability in each attended and unattended talk) was used as an input feature to predict corresponding neural responses (248 sensor MEG response).
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="490770v1_ueqn3.gif" mimetype="image" mime-subtype="gif"/></alternatives>
</disp-formula>
The fitting of the encoding model implemented the ridge regression (also known as Tikhonov regularization), where the loss function is the linear least-squares function and regularization (ridge parameter λ) is given by the L2-norm. The estimate is improved by using quadratic penalization (M) of the difference between two neighbouring terms of w (<xref ref-type="bibr" rid="c36">Lalor et al., 2006</xref>). The K-fold (k=3) cross-validation was used to split data into train and test sets (5:5), and fitting the model was iterated through the subset. This computation was performed via MNE-Python (<xref ref-type="bibr" rid="c21">Gramfort et al., 2013</xref>) class mne.decoding.ReceptiveField and separately for each condition (attended, high; attended, low; unattended, high; unattended, low). Model coefficients maps (weight vectors from the model estimation; TRF w, number of cross-validation x number of sensors x number of time delays: 3 × 248 × 126) were obtained and averaged over cross-validation splits. The prediction score (derived as the correlation coefficient, <italic>r</italic>) between predicted and original neural responses was also computed (number of cross-validation × number of sensors) and averaged across cross-validation splits.</p>
<p>We performed the decoding model (backward: brain to stimulus; speech reconstruction) estimation as well (see for more details in Supplementary Figure 2) and investigated model coefficients map and prediction scores between conditions of combinations of attention (attended, unattended) and topic probability (high, low). The prediction score (derived as the correlation coefficient, <italic>r</italic>) between the reconstructed and original speech envelope was computed and averaged across cross-validation splits, as shown in <xref rid="fig3" ref-type="fig">Fig. 3b</xref>. One representative result for the reconstructed speech envelope is shown on the left bottom in <xref rid="fig3" ref-type="fig">Fig 3a</xref>.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Receptive field model estimation analysis for topic probability and attention effects. a, Schematic flowchart for the model estimation.</title>
<p>The stimulus feature-neural response mapping was modelled bidirectionally using encoding and decoding models (also known as forward and backward models). Encoding and decoding model analyses were performed separately for high and low topic probability conditions as well as for attended and unattended talks. Figure adapted from <xref ref-type="bibr" rid="c15">Crosse et al. (2016)</xref>. <bold>b, Decoding model performance</bold>. Model prediction accuracy to reconstruct speech envelope for each condition was obtained by correlation coefficient score (<italic>r</italic>-value) between original and reconstructed speech, which were then compared between high and low topic probability conditions separately for each attended and unattended talk. The prediction accuracy was significantly stronger for speech with high than low topic probability in both attended (between 1<sup>st</sup> and 2<sup>nd</sup> columns: attended, high vs. attended, low; t<sub>43</sub> = 11.43, p = 1.27e-14) and unattended talk (between 3rd and 4th columns: unattended, high vs. unattended, low; t<sub>43</sub> = 29.98, p = 1.88e-30) with a greater difference for unattended than attended talk. Interestingly, the prediction accuracy was stronger for the high topic probability condition in unattended talk than the low topic probability condition in attended talk (between 2nd and 3rd columns: unattended, high vs. attended, low; t<sub>43</sub> = 9.62, p = 2.74e-12). However, no significant difference was observed between high topic probability conditions (between 1st and 3rd columns: attended, high vs. unattended, high; t<sub>43</sub> = 0.80, p = 0.43). All statistical comparisons were performed via two-tailed paired t-test. Dots and lines represent individual results.</p></caption>
<graphic xlink:href="490770v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>For the investigation of model coefficients map at the source level, dynamic statistical parametric mapping (dSPM) (<xref ref-type="bibr" rid="c17">Dale et al., 2000</xref>; <xref ref-type="bibr" rid="c20">Gramfort et al., 2014</xref>) was applied as an inverse solution. The computation was performed separately for each condition, and the statistical significance maps between the conditions were derived. The results from the encoding model map, as summary clusters, are shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, and the results for significant temporal clusters from the encoding model can be found in Supplementary Figure 1. Decoding model weights are not interpretable in a neurophysiological sense due to potential spurious observations; however, inverse-transformed decoder weights, i.e., transforming the backward model into the forward model via pseudo-inverse, can provide physiologically interpretable information (<xref ref-type="bibr" rid="c25">Haufe et al., 2014</xref>). For this work, please see Supplementary Figure 2.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Encoding model weights mapped onto source space.</title>
<p>Encoding model coefficients were mapped onto source space via dSPM method and statistically compared between high and low topic probability conditions using cluster-level spatio-temporal permutation test (p &lt; 0.05; two-tailed; 1024 permutations). Summary clusters (averaged across all significant temporal clusters) are shown. <bold>a</bold>, attended, high vs. attended, low. <bold>b</bold>, unattended, high vs. unattended low. <bold>c</bold>, unattended, high vs. attended, low. T-values in significant clusters are scaled corresponding to the duration spanned by the cluster (for more details, see Statistical test in Materials and Methods).</p></caption>
<graphic xlink:href="490770v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2e">
<title>Stronger speech reconstruction accuracy for speech chunks with high than low topic probability</title>
<p>The decoder model can provide a complementary view on the interpretation of the stimulus feature-neural response relationship as follows. First, the decoder can be used to reconstruct stimulus features from the neural responses. In the current study, a trained decoder was used to reconstruct the speech envelope. The prediction accuracy to reconstruct speech, defined as correlation coefficient score (<italic>r</italic>) between original and reconstructed speech envelope, was significantly stronger for speech with high than low topic probability in both attended (attended, high vs. attended, low: paired t-test; t<sub>43</sub> = 11.43, p = 1.27e-14; between 1<sup>st</sup> and 2<sup>nd</sup> columns in <xref rid="fig3" ref-type="fig">Fig. 3b</xref>) and unattended talk (unattended, high vs. unattended, low: paired t-test; t<sub>43</sub> = 29.98, p = 1.88e-30; between 3<sup>rd</sup> and 4<sup>th</sup> columns in <xref rid="fig3" ref-type="fig">Fig. 3b</xref>). The difference between high and low topic probability speech is greater for speech chunks derived from unattended talk when compared to attended talk. Furthermore, speech reconstruction performance is stronger for high topic probability condition in unattended talk than low topic probability condition in attended talk (unattended, high vs. attended, low: paired t-test; t<sub>43</sub> = 9.62, p = 2.74e-12; between 2<sup>nd</sup> and 3<sup>rd</sup> columns in <xref rid="fig3" ref-type="fig">Fig. 3b</xref>). This implies that topic keywords in unattended talks are still captured and are processed to the same degree as those in attended talks. No significant difference was observed between high topic probability conditions (attended, high vs. unattended, high: paired t-test; t<sub>43</sub> = 0.80, p = 0.43; between 1<sup>st</sup> and 3<sup>rd</sup> columns in <xref rid="fig3" ref-type="fig">Fig. 3b</xref>). All other paired t-test results between conditions are as follows: attended, high vs. unattended, low: t<sub>43</sub> = 22.61, p = 1.71e-25; between 1<sup>st</sup> and 4<sup>th</sup> columns and attended, low vs. unattended, low: t<sub>43</sub> = 11.79, p = 4.62e-15; between 2<sup>nd</sup> and 4<sup>th</sup> columns in <xref rid="fig3" ref-type="fig">Fig. 3b</xref>).</p>
<p>Next, we investigated the spatial representations of topic keywords processing between high and low topic probability condition pairs that showed significant prediction performance above (attended, high vs. attended, low; unattended, high vs. unattended, low; unattended, high vs. attended, low). The encoding model coefficients for each condition were mapped on the brain surface using the dynamic statistical parametric mapping (dSPM) source localization method. The statistical difference was derived via two-tailed cluster-level spatio-temporal permutation t-test (p &lt; 0.05; 1024 permutations) for 0 to 0.5 s with respect to the speech chunk onset in steps of 0.008 s in MNE-Python (function: mne.stats.spatio_temporal_cluster_1samp_test). Here we show summary clusters, i.e., all significant clusters pooled across the temporal clusters. For temporally unfolded clusters, including before speech onset (−0.2 to 0 s), please see Supplementary Figure 1.</p>
<p>For attended speech (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>), the difference between high and low topic probability was observed in the left inferior frontal (BA 44, 45), somatosensory areas, as well as primary motor (BA 4), premotor (BA 6) cortices and right supramarginal, angular and posterior superior temporal gyri. For unattended speech (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>), the difference between high and low topic probability was observed in the right inferior frontal areas, insula and temporal areas and left frontal, posterior temporal and visual cortex. We further performed the same statistical test between unattended high vs. attended low topic probability conditions where significant speech reconstruction performance was observed. The result (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>) showed similar brain regions as to attended high vs. attended low (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>), though to a lesser extent, suggesting speech chunks with high topic probability in unattended talks are processed to a similar degree as those in attended talks.</p>
</sec>
<sec id="s2f">
<title>Semantically salient speech in unattended talk causally mediates attended speech comprehension</title>
<p>As shown above, the statistical difference between high and low topic probability was greater for unattended talks than for attended talks. Furthermore, the difference was evident for unattended but with high topic probability speech chunks when compared to attended but with low topic probability condition. These findings suggest that distracting speech chunks are still processed when they contain semantic gist. This has led us to hypothesize that attention to semantically salient unattended speech negatively mediates (i.e., suppresses) attended speech comprehension. We employed the mediation analysis (<xref ref-type="bibr" rid="c2">Baron and Kenny, 1986</xref>) to inspect the relationship between unattended speech and attended speech comprehension. Encoding model coefficients for 0.5 s after the onset of speech chunks were averaged within each parcellation of the PALS-B12-Brodmann atlas (<xref ref-type="bibr" rid="c62">Van Essen, 2005</xref>) within each individual subject. The PALS-B12-Brodmann atlas parcellation has 82 cortical structures with 41 structures for each hemisphere. In the mediation model, encoding model coefficients of attended speech chunks with high topic probability, encoding model coefficients of unattended speech chunks with high topic probability, and speech comprehension accuracy for attended speech were used as predictor (<italic>X</italic>, independent), mediator (<italic>M</italic>), and target (<italic>Y</italic>, dependent) variables, respectively.</p>
<p>We found significant negative indirect effects for 4 out of 82 cortical regions: BA41 (primary auditory cortex), BA4 (primary motor cortex), BA6 (premotor cortex) in the left hemisphere and BA9 (dorsolateral prefrontal cortex) in the right hemisphere (Left BA41: path ab β = -2.03, p = 0.03, 95% CI = -5.53, -0.29; Left BA4: path ab β = -3.82, p = 0.02, 95% CI = -11.12, -0.71; Left BA6: path ab β = -3.51, p = 0.009, 95% CI = -10.84, -0.93; Right BA9: path ab; β = -6.43, p = 0.03, 95% CI = -13.44, -0.95; 5000 bootstrap iterations performed; <xref rid="fig5" ref-type="fig">Fig. 5a</xref>). There were no significant direct effects. As such, the findings support the complete (full) mediation effects (also referred to as causal mediation effect), indicating the increased activities in these brain regions to unattended (<italic>to-be-ignored</italic>) speech suppress attention to attended (<italic>to-be-attended</italic>) speech leading to poor speech comprehension performance. The encoding model coefficients in these regions for all conditions including low conditions are shown in Supplementary Figure 3.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Causal relationship between attended and unattended speech on speech comprehension. a, Salient unattended speech negatively mediates attended speech comprehension.</title>
<p>Mediation analysis was performed to test the hypothesis that attention to semantically salient unattended speech negatively mediates (i.e., suppresses) attended speech comprehension. In the mediation model, encoding model coefficients of attended speech chunks with high topic probability, encoding model coefficients of unattended speech chunks with high topic probability, and speech comprehension accuracy for attended speech were used as predictor (<italic>X</italic>, independent), mediator (<italic>M</italic>), and target (<italic>Y</italic>, dependent) variables, respectively. The encoding model coefficients during 0 - 0.5 s with respect to the speech chunk onset were averaged within each region in PALS-B12-Brodmann atlas in MNE-Python within each individual. Significant negative indirect effects were identified for left BA41 (path ab β = -2.03, p = 0.03, 95% CI = -5.53, -0.29), left BA4 (path ab β = -3.82, p = 0.02, 95% CI = -11.12, -0.71), left BA6 (path ab β = -3.51, p = 0.009, 95% CI = -10.84, -0.93) and right BA9: path ab; β = -6.43, p = 0.03, 95% CI = -13.44, -0.95) with 5000 bootstrap iterations. <bold>b, Increased sensitivity to attended speech in the regions enhances speech comprehension</bold>. Sensitivity index, analogous to d-prime, defined by the difference between <italic>Z</italic>-transformed model coefficients of attended speech with high topic probability and unattended speech with high topic probability (Z (attended high) – Z (unattended high)) was created for each of 4 regions and averaged. The sensitivity index was significantly correlated with speech comprehension accuracy across participants (Spearman rank correlation: r = 0.47, p = 0.001), supporting the hypothesis that participants with increased sensitivity to semantically salient attended speech in these regions show better speech comprehension.</p></caption>
<graphic xlink:href="490770v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To substantiate this modulatory effect, we tested if increased sensitivity to attended speech in these regions is associated with better speech comprehension. For this, we made a sensitivity index using the difference between <italic>Z</italic>-transformed coefficients of attended speech with high topic probability and unattended speech with high topic probability (<italic>Z</italic> (attended, high) – <italic>Z</italic> (unattended, high)), analogous to the d-prime (i.e., sensitivity index, also known as discriminability or detectability). Then, the sensitivity index values were averaged across the four brain regions and correlated with speech comprehension accuracy using Spearman rank correlation over participants (r = 0.47, p = 0.001 in <xref rid="fig5" ref-type="fig">Fig. 5b</xref>). The result supports that participants with increased sensitivity to attended (<italic>to-be-attended</italic>) talks with high topic probability in these brain regions indeed show better speech comprehension.</p>
</sec>
</sec>
<sec id="s3">
<title>Materials and Methods</title>
<sec id="s3a">
<title>Participants</title>
<p>Forty-six native English speakers participated in the study. All participants reported normal hearing (confirmed by two hearing tests using research applications on an iPad: uHear (Unitron Hearing Limited) and Hearing-Check (RNID)) as well as normal or corrected-to-normal vision. They all had no history of neurological, developmental, or psychological disorders and were all right-handed, confirmed by the Edinburgh Handedness Inventory (<xref ref-type="bibr" rid="c46">Oldfield, 1971</xref>). Data from 44 subjects were analyzed (26 females; age range: 18–30 y; mean age: 20.54 ± 2.58 y) after two subjects were excluded since one subject fell asleep and one had excessive signal noise). Other analyses of these data were presented in previous reports (<xref ref-type="bibr" rid="c49">Park et al., 2016</xref>; <xref ref-type="bibr" rid="c51">Park et al., 2018b</xref>). All subjects provided informed written consent before the experiment and received monetary compensation for their participation. The study was approved by the local ethics committee (CSE01321; College of Science and Engineering, University of Glasgow) and undertaken in accordance with the ethical guidelines in the Declaration of Helsinki.</p>
</sec>
<sec id="s3b">
<title>Stimuli and Experiment</title>
<sec id="s3b1">
<title>Natural speech materials</title>
<p>Each auditory speech presented to the participants during MEG recordings was about a certain coherent topic, and the materials we presented to the participants were originally taken from TED talks (<ext-link ext-link-type="uri" xlink:href="http://www.ted.com/talks/">www.ted.com/talks/</ext-link>) and modified to be appropriate for our own filming (e.g., “The key to growth? Race with the machines” by Erik Brynjolfsson. Transcription for each talk was downloaded and edited to be appropriate for our own filming by editing words such as referring to visual materials, the gender of the speaker etc. The talks address a specific topic belonging to informative, persuasive, inspiring categories on the website. Please note that they no longer provide these categories explicitly. We additionally validated the speech materials in a separate behavioural study (33 participants with 19 females; aged 18–31 years; mean age: 22.27 ± 2.64 years), where participants rated the talks in terms of arousal, familiarity, valence, complexity, significance (informativeness), agreement (persuasiveness), concreteness, self-relatedness, and level of understanding using Likert scale (<xref ref-type="bibr" rid="c38">Likert, 1932</xref>) 1 to 5 (for an example of concreteness, 1: very abstract, 2: abstract, 3: neither abstract nor concrete, 4: concrete, 5: very concrete). Talks with excessive mean scores (below 1 and over 4) were excluded, and eight out of eleven videos were selected for the experiment, and selected talks were used in different experimental conditions, which were also used in our previous study (<xref ref-type="bibr" rid="c49">Park et al., 2016</xref>). High-quality audiovisual video clips were filmed by a professional filming company while a professional male speaker was talking continuously. The duration of talks is 7 to 9 minutes with a sampling rate of 48 kHz for audio and 25 frames per second (fps) for video in 1,920 × 1,080 pixels. Filmed videos were edited for experimental manipulations (i.e., recombinations of auditory and visual speech stimuli) using Final Pro Cut × (Apple, Cupertino, CA).</p>
</sec>
<sec id="s3b2">
<title>Experimental condition</title>
<p>We employed four experimental conditions as described in our previous study (<xref ref-type="bibr" rid="c49">Park et al., 2016</xref>). In the current study, we focused on the “AV congruent” condition where two different talks are delivered to the left and right ear with one auditory speech matching the visual lip movement, and the speech presented to the other ear serves as a distractor. Participants were instructed to pay attention to the talk that matches visual lip movement. The side of attention was counterbalanced, resulting in half of the participants (N = 22) paying attention to the left-ear talk, whereas the other half (N = 22) paying attention to the right-ear talk. Participants were instructed to fixate on the visual information, i.e., the speaker’s lip movement and subjects’ eye movements were monitored using an eye tracker (EyeLink 1000, SR Research). To assist this, a fixation cross color-coded either yellow or blue was overlaid on the speaker’s lip, and the color of the fixation cross indicated the side of attention. During the experiment, the audiovisual stimuli were presented via Psychtoolbox (<xref ref-type="bibr" rid="c9">Brainard, 1997</xref>) in MATLAB (MATLAB, R2019b). Auditory stimuli were delivered at a 48 kHz sampling rate via a sound pressure transducer through 2 five-meter-long plastic tubes terminating in plastic insert earpieces, and visual stimuli were presented with a resolution of 1,280 × 720 pixels at 25 fps (mp4 format).</p>
</sec>
<sec id="s3b3">
<title>Behavioural performance</title>
<p>In order to assess the level of comprehension of the talk, a questionnaire for speech comprehension was administered after the talk. The questionnaire consists of 10 questions about the attended talk, such as “What is the speaker’s job?” and “What would be the best title of this talk?”. The questionnaire itself was validated in a separate behavioural study (16 subjects; 13 females; aged 18–23 y; mean age: 19.88 ± 1.71 y) in terms of accuracy (the same level of difficulty), response time, and the length (word count). The comprehension scores for left- and right-ear attention group did not differ (two-sample t-test for “attended to left” vs. “attended to right” group, t<sub>42</sub> = -0.13, p = 0.90). In this study, we pooled across both groups in all data analyses so that attentional effects for a particular side (e.g., left or right) are expected to cancel out.</p>
</sec>
</sec>
<sec id="s3c">
<title>MEG and MRI (T1) data processing</title>
<sec id="s3c1">
<title>Data acquisition</title>
<p>Neuromagnetic signals were measured using a 248 magnetometers whole-head MEG system (MAGNES 3600 WH, 4-D Neuroimaging) in a magnetically shielded room with a sampling rate of 1,017 Hz. The signals were resampled to 250 Hz and denoised with information from the reference sensors using the denoise_pca function in the FieldTrip toolbox (<xref ref-type="bibr" rid="c47">Oostenveld et al., 2011</xref>). Bad MEG sensors were excluded by visual inspection, and electrooculographic (EOG) and electrocardiographic (ECG) artefacts were rejected using independent component analysis (ICA). In order to map MEG data onto cortical source space, structural T1-weighted MRIs of each participant were acquired at 3 T Siemens Trio Tim scanner (Siemens, Erlangen, Germany) with the following parameters: 1.0 × 1.0 × 1.0 mm<sup>3</sup> voxels; 192 sagittal slices; field of view (FOV): 256 × 256 matrix.</p>
</sec>
<sec id="s3c2">
<title>Coregistration between MRI and MEG data</title>
<p>Structural T1 MR images recorded from each participant were coregistered to the MEG coordinate system via a semiautomatic procedure. Anatomical fiduciary landmarks such as nasion and bilateral preauricular points were identified before the MEG recording and also manually identified in the individual’s MR images. Based on these landmarks, both MEG and MRI coordinate systems were initially aligned, followed by numerical optimization achieved by using the ICP algorithm (<xref ref-type="bibr" rid="c3">Besl and McKay, 1992</xref>).</p>
</sec>
<sec id="s3c3">
<title>Source localization</title>
<p>Reported results in the current study were analyzed in both Fieldtrip (<xref ref-type="bibr" rid="c47">Oostenveld et al., 2011</xref>) and MNE-Python (<xref ref-type="bibr" rid="c20">Gramfort et al., 2014</xref>), so workflow for inverse solution followed the standard procedure in each software. In Fieldtrip, a head model was created for each individual from their structural MRI using normalization and segmentation routines in FieldTrip and SPM8. For the calculation of the leadfield, we used a single-shell volume conductor model (<xref ref-type="bibr" rid="c45">Nolte, 2003</xref>) employing an 8 mm grid defined on the template brain provided by MNI (Montreal Neurological Institute). The template grid was linearly transformed into individual headspace for spatial normalization. In MNE-Python, construction of the forward model solution and MRI segmentation are performed in the FreeSurfer package (<xref ref-type="bibr" rid="c16">Dale et al., 1999</xref>). The Boundary Element Model (BEM) of individual MRI was created using the FreeSurfer watershed algorithm, and surface-based source space with a 4.9 mm source spacing resolution was computed. Then individual source map was morphed to the template MRI (fsaverage) to compare output activities across subjects in common source space.</p>
</sec>
</sec>
<sec id="s3d">
<title>Audiovisual speech signal processing</title>
<sec id="s3d1">
<title>Auditory speech signal</title>
<p>The amplitude envelope of sound signals was computed following the approach introduced in <xref ref-type="bibr" rid="c13">Chandrasekaran et al. (2009)</xref>. We first constructed eight frequency bands in the range of 100–10,000 Hz to be equidistant on the cochlear map (<xref ref-type="bibr" rid="c56">Smith et al., 2002</xref>). Then the auditory sound speech signals were band-pass filtered in these bands using a fourth-order forward and reverse Butterworth filter followed by Hilbert transform to obtain amplitude envelopes for each band of the signal. These signals were then averaged across bands resulting in a wideband amplitude envelope. Signals were downsampled to 250 Hz for further analysis to match the sampling rate of preprocessed MEG data.</p>
</sec>
<sec id="s3d2">
<title>Visual speech signal</title>
<p>A lip movement signal was computed using an in-house MATLAB script as in our previous report (<xref ref-type="bibr" rid="c49">Park et al., 2016</xref>), which demonstrated oscillatory brain activities entrained by visual speech. We first extracted the outline lip contour of the speaker for each frame of the video stimuli. From the lip contour outline, we computed the frame-by-frame lip area (area within lip contour). This signal was resampled at 250 Hz to match the sampling rate of the preprocessed MEG signal and auditory sound envelope signal.</p>
</sec>
</sec>
<sec id="s3e">
<title>Mediation analysis</title>
<p>We used mediation analysis (<xref ref-type="bibr" rid="c2">Baron and Kenny, 1986</xref>) to test our hypothesis that attention to semantically salient unattended speech negatively mediates (i.e., suppresses) attended speech comprehension using mediation_analysis module in Pingouin package (<xref ref-type="bibr" rid="c60">Vallat, 2018</xref>). Here we used predefined cortical parcellation, the PALS-B12-Brodmann atlas (<xref ref-type="bibr" rid="c62">Van Essen, 2005</xref>). The parcellation provides 82 cortical surface structures (41 in each hemisphere). Encoding model coefficients (TRF) is averaged across 0 s to 0.5 s after the onset of speech chunks within all the areas in the PALS-B12-Brodmann parcellation for each subject. In the mediation model, these values of attended speech chunks with high topic probability and unattended speech chunks with high topic probability were used as predictor (X, independent) and mediator (M) variables, respectively. For the dependent variable (Y), speech comprehension accuracy for attended speech was used. Five thousand bootstrap iterations were performed for confidence intervals and p-values estimation. To confirm the mediation effects, we performed correlational analysis between the sensitivity index, defined by the difference between z transformed attended high (analogous to hit) and unattended high (analogous to false alarm) conditions, and speech comprehension accuracy.</p>
</sec>
<sec id="s3f">
<title>Statistical test</title>
<p>All the analyses described in the Methods section were performed individually and then yielded to group-level statistical tests on the data of all 44 participants.</p>
<p>For receptive field model estimation (mTRF), the non-parametric cluster-level paired permutation test based on a t-statistic (<xref ref-type="bibr" rid="c39">Maris and Oostenveld, 2007</xref>) was performed at the spatio-temporal level (function: mne.stats.spatio_temporal_cluster_1samp_test) between high vs. low topic probability conditions after morphing into common cortical space (fsaverage) in MNE-Python (<xref ref-type="bibr" rid="c21">Gramfort et al., 2013</xref>). The function detects significant clusters at both spatial and temporal regions. A spatial adjacency matrix was used for clustering in source space, and 1024 permutations were computed. T-values in significant clusters are scaled corresponding to the duration spanned by the cluster (shown as “t-value (scaled)” in figures) in which the scaling factors are the significance of the t-value and the unit of time step between samples (e.g., 0.008 s) in the data. In more detail, the significance of t-values at a given time point (1 if significant, 0 if not) is scaled by the unit of the duration (e.g., significance (1 or 0) × 0.008 s). These values are summed up across significant time points. Each temporal cluster from the encoding model statistics is shown in Supplementary Figure 1.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In the current study, we used computational topic modelling to investigate how the brain processes high-level semantics in naturalistic speech. Topic modelling techniques, such as Latent Dirichlet Allocation (LDA), have broad application not only to text materials but to other domains, for example, content-based images (<xref ref-type="bibr" rid="c7">Blei et al., 2003</xref>). The LDA technique provides scalable and quantifiable measures in identifying topics using a generative probabilistic approach. To date, studies of neural mechanisms underlying semantic processing during natural speech perception have largely relied on the word level violations or predictions (<xref ref-type="bibr" rid="c35">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="c63">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="c10">Broderick et al., 2019</xref>; <xref ref-type="bibr" rid="c33">Koskinen et al., 2020</xref>). Investigating the neural representation of latent word meanings, such as arising from idiomaticity, or semantic gist (i.e., topic keywords) across supporting contextual information in a connected speech, requires moving beyond inspecting lexico-semantic level processing; however, it has been challenged due to the lack of appropriate quantifiable approaches. Here we harness the state-of-the-art machine learning-based natural language processing (NLP) algorithm to delineate the neural signatures in topic keywords processing. To our knowledge, this is the first study that investigates neural signatures of high-level semantics processing beyond lexico-semantics in continuous speech.</p>
<p>By applying the topic model to speech chunks that were driven in a perceptually relevant manner by acoustic silences in natural speaking, we were able to extract representative topic keywords of a certain story. Subsequently, speech chunks were split into two statistical conditions according to the probability of the main topic keywords. Corresponding epochs of brain activities were grouped to the conditions. Here we focused our analysis on the topic keywords processing in the context of the multi-speaker environment where the attention to a particular talk is manipulated. Brain responses and stimulus feature (speech envelope) were used to fit the encoding and decoding model to investigate the neural map of semantic gist throughout the speech.</p>
<p>Our finding showing stronger brain activity for attended speech than unattended speech (<xref rid="fig2" ref-type="fig">Fig. 2</xref>) is consistent with other findings that showed brain responses to unattended speech is attenuated (<xref ref-type="bibr" rid="c32">Kong et al., 2014</xref>). However, strikingly, the difference in reconstruction of speech envelope from neural activities between high and low topic probability conditions in unattended speech is greater when compared to the same difference in attended speech. The finding is even striking for the significant difference between unattended high vs. attended low condition. This result supports the notion that unattended (i.e., task-irrelevant) speech is still processed in the brain due to the failure of selective attention to fully suppress distracting sensory input (<xref ref-type="bibr" rid="c24">Har-Shai Yahav and Zion Golumbic, 2021</xref>). Despite decades of debate (<xref ref-type="bibr" rid="c12">Bronkhorst, 2015</xref>), a recent study by <xref ref-type="bibr" rid="c24">Har-Shai Yahav and Zion Golumbic (2021)</xref> has shown that the phrasal structure of structured task-irrelevant stimuli was represented in the neural responses and competed with task-relevant (attended) speech. Our finding extends this significantly by showing that this is even the case for higher-level semantic processing, indicating that the human brain can capture the semantic gist of the task-irrelevant speech in a multi-speaker context.</p>
<p>We report source mapping using encoding model coefficients via the dSPM source reconstruction method. For attended speech (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>), the patterns of the difference between high vs. low topic probability conditions are mapped onto the left inferior frontal, dorsolateral prefrontal and extensive temporal areas corresponding to the language network. For the unattended speech (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>), the difference was mapped onto the right inferior frontal/insular areas and the left superior/dorsolateral prefrontal areas. The prominent involvement of the right inferior frontal and insular cortex might suggest lower-level processing of incoming but task-irrelevant speech. While the left hemisphere is actively engaging in semantic processing of goal-directed attended speech, the right hemisphere processes perceptual features such as pitch contours of unattended speech consistent with the notion of the division of labour between hemispheres (<xref ref-type="bibr" rid="c19">Flinker et al., 2019</xref>). In <xref ref-type="bibr" rid="c19">Flinker et al. (2019)</xref>, the authors suggest differential hemispheric contributions in auditory processing in which right lateralization for spectral modulation, such as gender identification, while left lateralization for intelligibility task (<xref rid="fig5" ref-type="fig">Fig. 5</xref> in <xref ref-type="bibr" rid="c19">Flinker et al. (2019)</xref>). As such, we suggest that this finding might reflect processing towards a more perceptual level, e.g., pitch contours, with a potential shift of attentional state leading to shallow level semantic processing given the complex nature of dichotic listening (multi-speaker) environment.</p>
<p>Our mediation analysis allows us to further investigate the causal relationship between attended and unattended speech on speech comprehension. The negative full mediation effect (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>) indicates that the failure to suppress salient distractors of unattended speech leads to poor speech comprehension of attended speech. Interestingly this effect was observed in the left BA41 (primary auditory cortex), BA4 (primary motor cortex), BA6 (premotor cortex) and right BA9 (dorsolateral prefrontal cortex), which are sub-regions of the speech processing network. Particularly, intrinsic coupling between the auditory and motor system has been previously reported (<xref ref-type="bibr" rid="c1">Assaneo and Poeppel, 2018</xref>). However, the role of motor and premotor cortices in speech perception is controversial (<xref ref-type="bibr" rid="c41">Meister et al., 2007</xref>; <xref ref-type="bibr" rid="c27">Hickok, 2010</xref>; <xref ref-type="bibr" rid="c55">Skipper et al., 2017</xref>), and the contribution to the processing of high-level semantics is not well-known. In our previous results using the same dataset, however, the motor cortex was involved in conveying greater information than the linear summation of individual auditory and visual perceptual information (i.e., synergistic information processing) and supported behavioural performance (<xref ref-type="bibr" rid="c51">Park et al., 2018b</xref>). Given additional manipulations for high-level semantic processing and attention in the current study, we interpret the motor and premotor cortex might support top-down modulated active sensing (<xref ref-type="bibr" rid="c43">Morillon et al., 2015</xref>; <xref ref-type="bibr" rid="c50">Park et al., 2015</xref>) for semantic gist in temporally rapidly changing and dynamically evolving naturalistic speech perception in multi-speaker environment. Shifting between task-relevant and -irrelevant semantic gist is suggested to be modulated by these regions, as shown in the result that subjects with increased sensitivity to semantic gist of goal-directed attended speech exhibit better behavioural performance (<xref rid="fig5" ref-type="fig">Fig 5b</xref>).</p>
<p>Further investigation will be required to gain deeper insights into high-level semantic processing by fully characterizing the difference from the neural mechanisms underlying other speech features, including lexico-semantic processing, which is expected to provide a direct computational link between our high-level semantic approach and previous work focused on word similarity using other NLP algorithms such as word2vec (<xref ref-type="bibr" rid="c42">Mikolov et al., 2013</xref>) as used in <xref ref-type="bibr" rid="c11">Broderick et al. (2018)</xref>, or semantic neural representation shown in <xref ref-type="bibr" rid="c30">Huth et al. (2016)</xref>. Furthermore, future studies should be able to provide evidence of how the brain builds up the semantic core with time as the supporting building blocks of different levels of semantic features are accumulated.</p>
<p>In summary, we provide evidence that temporal and spatial neural signatures for high-level semantic gist (i.e., topic keywords) processing in the context of multi-speaker environment and how the semantic gist in unattended speech affects attended speech comprehension.</p>
</sec>
<sec id="d1e1218" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1295">
<label>Supplementary Information</label>
<media xlink:href="supplements/490770_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
</ack>
<sec id="s5">
<title>Funding</title>
<p>Wellcome Trust Senior Investigator Award <ext-link ext-link-type="uri" xlink:href="https://wellcome.ac.uk/">https://wellcome.ac.uk/</ext-link> (grant number 098433) and DFG project funding (GR 2024/5-1, GR 2024/11-1) to JG. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
</sec>
<sec id="s6">
<title>Conflict of interest statement</title>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Assaneo</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2018</year>) <article-title>The coupling between auditory and motor cortices is rate-restricted: Evidence for an intrinsic speech-motor rhythm</article-title>. <source>Science Advances</source> <volume>4</volume>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Baron</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Kenny</surname> <given-names>DA</given-names></string-name> (<year>1986</year>) <article-title>The moderator-mediator variable distinction in social psychological research: conceptual, strategic, and statistical considerations</article-title>. <source>J Pers Soc Psychol</source> <volume>51</volume>:<fpage>1173</fpage>–<lpage>1182</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Besl</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>McKay</surname> <given-names>ND</given-names></string-name> (<year>1992</year>) <article-title>A method for registration of 3-D shapes</article-title>. <source>IEEE T Pattern Anal</source>:<fpage>239</fpage>–<lpage>256</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Biau</surname> <given-names>E</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O</given-names></string-name>, <string-name><surname>Hanslmayr</surname> <given-names>S</given-names></string-name> (<year>2021</year>) <article-title>Auditory detection is modulated by theta phase of silent lip movements</article-title>. <source>Current Research in Neurobiology</source> <volume>2</volume>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="web"><string-name><surname>Bird</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ewan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Loper</surname> <given-names>E</given-names></string-name> (<year>2009</year>) <article-title>Natural Language Processing with Python: O’Reilly Media, Inc</article-title>. <source>Blei DM Topic modeling. In</source>: <ext-link ext-link-type="uri" xlink:href="http://www.cs.columbia.edu/~blei/topicmodeling.html">http://www.cs.columbia.edu/~blei/topicmodeling.html</ext-link>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Blei</surname> <given-names>DM</given-names></string-name> (<year>2012</year>) <article-title>Probabilistic Topic Models</article-title>. <source>Commun Acm</source> <volume>55</volume>:<fpage>77</fpage>–<lpage>84</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Blei</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Ng</surname> <given-names>AY</given-names></string-name>, <string-name><surname>Jordan</surname> <given-names>MI</given-names></string-name> (<year>2003</year>) <article-title>Latent Dirichlet allocation</article-title>. <source>J Mach Learn Res</source> <volume>3</volume>:<fpage>993</fpage>–<lpage>1022</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="web"><string-name><surname>Boersma</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weenink</surname> <given-names>D</given-names></string-name> (<year>2018</year>) <article-title>Praat: doing phonetics by computer [Computer program]</article-title>. <source>Version 6.0.37</source>:retrieved 14 March 2018 from <ext-link ext-link-type="uri" xlink:href="http://www.praat.org/">http://www.praat.org/</ext-link>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial vision</source> <volume>10</volume>:<fpage>433</fpage>–<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Broderick</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name> (<year>2019</year>) <article-title>Semantic Context Enhances the Early Auditory Encoding of Natural Speech</article-title>. <source>J Neurosci</source> <volume>39</volume>:<fpage>7564</fpage>–<lpage>7575</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Broderick</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name> (<year>2018</year>) <article-title>Electrophysiological Correlates of Semantic Dissimilarity Reflect the Comprehension of Natural, Narrative Speech</article-title>. <source>Curr Biol</source> <volume>28</volume>:<fpage>803</fpage>–<lpage>809 e803</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Bronkhorst</surname> <given-names>AW</given-names></string-name> (<year>2015</year>) <article-title>The cocktail-party problem revisited: early processing and selection of multi-talker speech</article-title>. <source>Atten Percept Psychophys</source> <volume>77</volume>:<fpage>1465</fpage>–<lpage>1487</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Chandrasekaran</surname> <given-names>C</given-names></string-name>, <string-name><surname>Trubanova</surname> <given-names>A</given-names></string-name>, <string-name><surname>Stillittano</surname> <given-names>S</given-names></string-name>, <string-name><surname>Caplier</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ghazanfar</surname> <given-names>AA</given-names></string-name> (<year>2009</year>) <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS computational biology</source> <volume>5</volume>:<fpage>e1000436</fpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="web"><string-name><surname>Chen</surname> <given-names>E</given-names></string-name> <source>Introduction to Latent Dirichlet Allocation. In</source>: <ext-link ext-link-type="uri" xlink:href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/</ext-link>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Bednar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name> (<year>2016</year>) <article-title>The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli</article-title>. <source>Front Hum Neurosci</source> <volume>10</volume>:<fpage>604</fpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name> (<year>1999</year>) <article-title>Cortical surface-based analysis. I. Segmentation and surface reconstruction</article-title>. <source>NeuroImage</source> <volume>9</volume>:<fpage>179</fpage>–<lpage>194</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Buckner</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Belliveau</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Lewine</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Halgren</surname> <given-names>E</given-names></string-name> (<year>2000</year>) <article-title>Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imaging of cortical activity</article-title>. <source>Neuron</source> <volume>26</volume>:<fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>de Jong</surname> <given-names>NH</given-names></string-name>, <string-name><surname>Wempe</surname> <given-names>T</given-names></string-name> (<year>2009</year>) <article-title>Praat script to detect syllable nuclei and measure speech rate automatically</article-title>. <source>Behav Res Methods</source> <volume>41</volume>:<fpage>385</fpage>–<lpage>390</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Flinker</surname> <given-names>A</given-names></string-name>, <string-name><surname>Doyle</surname> <given-names>WK</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Devinsky</surname> <given-names>O</given-names></string-name>, <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2019</year>) <article-title>Spectrotemporal modulation provides a unifying framework for auditory cortical asymmetries</article-title>. <source>Nat Hum Behav</source> <volume>3</volume>:<fpage>393</fpage>–<lpage>405</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Luessi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Larson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Engemann</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Strohmeier</surname> <given-names>D</given-names></string-name>, <string-name><surname>Brodbeck</surname> <given-names>C</given-names></string-name>, <string-name><surname>Parkkonen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hamalainen</surname> <given-names>MS</given-names></string-name> (<year>2014</year>) <article-title>MNE software for processing MEG and EEG data</article-title>. <source>NeuroImage</source> <volume>86</volume>:<fpage>446</fpage>–<lpage>460</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Luessi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Larson</surname> <given-names>E</given-names></string-name>, <string-name><surname>Engemann</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Strohmeier</surname> <given-names>D</given-names></string-name>, <string-name><surname>Brodbeck</surname> <given-names>C</given-names></string-name>, <string-name><surname>Goj</surname> <given-names>R</given-names></string-name>, <string-name><surname>Jas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Brooks</surname> <given-names>T</given-names></string-name>, <string-name><surname>Parkkonen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hamalainen</surname> <given-names>M</given-names></string-name> (<year>2013</year>) <article-title>MEG and EEG data analysis with MNE-Python</article-title>. <source>Front Neurosci</source> <volume>7</volume>:<fpage>267</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Griffiths</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Steyvers</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tenenbaum</surname> <given-names>JB</given-names></string-name> (<year>2007</year>) <article-title>Topics in semantic representation</article-title>. <source>Psychol Rev</source> <volume>114</volume>:<fpage>211</fpage>–<lpage>244</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Haider</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Suess</surname> <given-names>N</given-names></string-name>, <string-name><surname>Hauswald</surname> <given-names>A</given-names></string-name>, <string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Weisz</surname> <given-names>N</given-names></string-name> (<year>2022</year>) <article-title>Masking of the mouth area impairs reconstruction of acoustic speech features and higher-level segmentational features in the presence of a distractor speaker</article-title>. <source>NeuroImage</source> <volume>252</volume>:<fpage>119044</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Har-Shai Yahav</surname> <given-names>P</given-names></string-name>, <string-name><given-names>Zion</given-names> <surname>Golumbic</surname></string-name> E (<year>2021</year>) <article-title>Linguistic processing of task-irrelevant speech at a cocktail party</article-title>. <source>Elife</source> <volume>10</volume>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Haufe</surname> <given-names>S</given-names></string-name>, <string-name><surname>Meinecke</surname> <given-names>F</given-names></string-name>, <string-name><surname>Gorgen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Dahne</surname> <given-names>S</given-names></string-name>, <string-name><surname>Haynes</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Blankertz</surname> <given-names>B</given-names></string-name>, <string-name><surname>Biessmann</surname> <given-names>F</given-names></string-name> (<year>2014</year>) <article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title>. <source>NeuroImage</source> <volume>87</volume>:<fpage>96</fpage>–<lpage>110</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Hauswald</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lithari</surname> <given-names>C</given-names></string-name>, <string-name><surname>Collignon</surname> <given-names>O</given-names></string-name>, <string-name><surname>Leonardelli</surname> <given-names>E</given-names></string-name>, <string-name><surname>Weisz</surname> <given-names>N</given-names></string-name> (<year>2018</year>) <article-title>A Visual Cortical Network for Deriving Phonological Information from Intelligible Lip Movements</article-title>. <source>Curr Biol</source> <volume>28</volume>:<fpage>1453</fpage>–<lpage>1459 e1453</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Hickok</surname> <given-names>G</given-names></string-name> (<year>2010</year>) <article-title>The role of mirror neurons in speech and language processing</article-title>. <source>Brain Lang</source> <volume>112</volume>:<fpage>1</fpage>–<lpage>2</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Hoffman</surname> <given-names>P</given-names></string-name> (<year>2019</year>) <article-title>Reductions in prefrontal activation predict off-topic utterances during speech production</article-title>. <source>Nat Commun</source> <volume>10</volume>:<fpage>515</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="other"><string-name><surname>Honnibal</surname> <given-names>M</given-names></string-name>, <string-name><surname>Montani</surname> <given-names>I</given-names></string-name> (<year>2017</year>) <source>spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</source>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Huth</surname> <given-names>AG</given-names></string-name>, <string-name><surname>de Heer</surname> <given-names>WA</given-names></string-name>, <string-name><surname>Griffiths</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Theunissen</surname> <given-names>FE</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name> (<year>2016</year>) <article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title>. <source>Nature</source> <volume>532</volume>:<fpage>453</fpage>–<lpage>458</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Kaufeld</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bosker</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Ten Oever</surname> <given-names>S</given-names></string-name>, <string-name><surname>Alday</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>AE</given-names></string-name> (<year>2020</year>) <article-title>Linguistic Structure and Meaning Organize Neural Oscillations into a Content-Specific Hierarchy</article-title>. <source>J Neurosci</source> <volume>40</volume>:<fpage>9467</fpage>–<lpage>9475</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Kong</surname> <given-names>YY</given-names></string-name>, <string-name><surname>Mullangi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>N</given-names></string-name> (<year>2014</year>) <article-title>Differential modulation of auditory responses to attended and unattended speech in different listening conditions</article-title>. <source>Hear Res</source> <volume>316</volume>:<fpage>73</fpage>–<lpage>81</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Koskinen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kurimo</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hyvarinen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hari</surname> <given-names>R</given-names></string-name> (<year>2020</year>) <article-title>Brain activity reflects the predictability of word sequences in listened continuous speech</article-title>. <source>NeuroImage</source> <volume>219</volume>:<fpage>116936</fpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Kullback</surname> <given-names>S</given-names></string-name>, <string-name><surname>Leibler</surname> <given-names>RA</given-names></string-name> (<year>1951</year>) <article-title>On Information and Sufficiency</article-title>. <source>Ann Math Stat</source> <volume>22</volume>:<fpage>79</fpage>–<lpage>86</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Kutas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Federmeier</surname> <given-names>KD</given-names></string-name> (<year>2011</year>) <article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title>. <source>Annu Rev Psychol</source> <volume>62</volume>:<fpage>621</fpage>–<lpage>647</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name>, <string-name><surname>Pearlmutter</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Reilly</surname> <given-names>RB</given-names></string-name>, <string-name><surname>McDarby</surname> <given-names>G</given-names></string-name>, <string-name><surname>Foxe</surname> <given-names>JJ</given-names></string-name> (<year>2006</year>) <article-title>The VESPA: a method for the rapid estimation of a visual evoked potential</article-title>. <source>NeuroImage</source> <volume>32</volume>:<fpage>1549</fpage>–<lpage>1561</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Landauer</surname> <given-names>TK</given-names></string-name>, <string-name><surname>Dumais</surname> <given-names>ST</given-names></string-name> (<year>1997</year>) <article-title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</article-title>. <source>Psychological Review</source> <volume>104</volume>:<fpage>211</fpage>–<lpage>240</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Likert</surname> <given-names>R</given-names></string-name> (<year>1932</year>) <article-title>A technique for the measurement of attitudes</article-title>. <source>Archives of Psychology</source> <volume>22</volume>:<fpage>1</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name> (<year>2007</year>) <article-title>Nonparametric statistical testing of EEG-and MEG-data</article-title>. <source>J Neurosci Methods</source> <volume>164</volume>:<fpage>177</fpage>–<lpage>190</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="other"><collab>MATLAB (R2019b) Natick, Massachusetts</collab>. <source>The MathWorks Inc</source>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Meister</surname> <given-names>IG</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Deblieck</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Iacoboni</surname> <given-names>M</given-names></string-name> (<year>2007</year>) <article-title>The essential role of premotor cortex in speech perception</article-title>. <source>Curr Biol</source> <volume>17</volume>:<fpage>1692</fpage>–<lpage>1696</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="other"><string-name><surname>Mikolov</surname> <given-names>T</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Corrado</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dean</surname> <given-names>J</given-names></string-name> (<year>2013</year>) <article-title>Efficient estimation of word representations in vector space</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1301.3781</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Morillon</surname> <given-names>B</given-names></string-name>, <string-name><surname>Hackett</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Kajikawa</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Schroeder</surname> <given-names>CE</given-names></string-name> (<year>2015</year>) <article-title>Predictive motor control of sensory dynamics in auditory active sensing</article-title>. <source>Curr Opin Neurobiol</source> <volume>31</volume>:<fpage>230</fpage>–<lpage>238</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Nishida</surname> <given-names>S</given-names></string-name>, <string-name><surname>Blanc</surname> <given-names>A</given-names></string-name>, <string-name><surname>Maeda</surname> <given-names>N</given-names></string-name>, <string-name><surname>Kado</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name> (<year>2021</year>) <article-title>Behavioral correlates of cortical semantic representations modeled by word vectors</article-title>. <source>PLoS computational biology</source> <volume>17</volume>:<fpage>e1009138</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Nolte</surname> <given-names>G</given-names></string-name> (<year>2003</year>) <article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title>. <source>Physics in medicine and biology</source> <volume>48</volume>:<fpage>3637</fpage>–<lpage>3652</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Oldfield</surname> <given-names>RC</given-names></string-name> (<year>1971</year>) <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source> <volume>9</volume>:<fpage>97</fpage>–<lpage>113</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fries</surname> <given-names>P</given-names></string-name>, <string-name><surname>Maris</surname> <given-names>E</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name> (<year>2011</year>) <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Computational intelligence and neuroscience</source> <volume>2011</volume>:<fpage>156869</fpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="other"><string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Thut</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>J</given-names></string-name> (<year>2018a</year>) <article-title>Predictive entrainment of natural speech through two fronto-motor top-down channels</article-title>. <source>Language, Cognition and Neuroscience</source>:<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kayser</surname> <given-names>C</given-names></string-name>, <string-name><surname>Thut</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>J</given-names></string-name> (<year>2016</year>) <article-title>Lip movements entrain the observers’ low-frequency brain oscillations to facilitate speech intelligibility</article-title>. <source>Elife</source> <volume>5</volume>:<fpage>e14521</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Ince</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Schyns</surname> <given-names>PG</given-names></string-name>, <string-name><surname>Thut</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>J</given-names></string-name> (<year>2015</year>) <article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title>. <source>Curr Biol</source> <volume>25</volume>:<fpage>1649</fpage>–<lpage>1653</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Ince</surname> <given-names>RAA</given-names></string-name>, <string-name><surname>Schyns</surname> <given-names>PG</given-names></string-name>, <string-name><surname>Thut</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>J</given-names></string-name> (<year>2018b</year>) <article-title>Representational interactions during audiovisual speech entrainment: Redundancy in left posterior superior temporal gyrus and synergy in left motor cortex</article-title>. <source>PLoS biology</source> <volume>16</volume>:<fpage>e2006558</fpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Michel</surname> <given-names>V</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name>, <string-name><surname>Grisel</surname> <given-names>O</given-names></string-name>, <string-name><surname>Blondel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Prettenhofer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dubourg</surname> <given-names>V</given-names></string-name>, <string-name><surname>Vanderplas</surname> <given-names>J</given-names></string-name>, <string-name><surname>Passos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cournapeau</surname> <given-names>D</given-names></string-name>, <string-name><surname>Brucher</surname> <given-names>M</given-names></string-name>, <string-name><surname>Perrot</surname> <given-names>M</given-names></string-name>, <string-name><surname>Duchesnay</surname> <given-names>E</given-names></string-name> (<year>2011</year>) <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>J Mach Learn Res</source> <volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Pereira</surname> <given-names>F</given-names></string-name>, <string-name><surname>Detre</surname> <given-names>G</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>M</given-names></string-name> (<year>2011</year>) <article-title>Generating text from functional brain images</article-title>. <source>Front Hum Neurosci</source> <volume>5</volume>:<fpage>72</fpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Pereira</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lou</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pritchett</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ritter</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name> (<year>2018</year>) <article-title>Toward a universal decoder of linguistic meaning from brain activation</article-title>. <source>Nat Commun</source> <volume>9</volume>:<fpage>963</fpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Skipper</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Devlin</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Lametti</surname> <given-names>DR</given-names></string-name> (<year>2017</year>) <article-title>The hearing ear is always found close to the speaking tongue: Review of the role of the motor system in speech perception</article-title>. <source>Brain and Language</source> <volume>164</volume>:<fpage>77</fpage>–<lpage>105</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Smith</surname> <given-names>ZM</given-names></string-name>, <string-name><surname>Delgutte</surname> <given-names>B</given-names></string-name>, <string-name><surname>Oxenham</surname> <given-names>AJ</given-names></string-name> (<year>2002</year>) <article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title>. <source>Nature</source> <volume>416</volume>:<fpage>87</fpage>–<lpage>90</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Strauss</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kotz</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Scharinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name> (<year>2014</year>) <article-title>Alpha and theta brain oscillations index dissociable processes in spoken word recognition</article-title>. <source>NeuroImage</source> <volume>97</volume>:<fpage>387</fpage>–<lpage>395</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="book"><collab>Team RC</collab> (<year>2020</year>) <source>R: A language and environment for statistical computing</source>. <publisher-name>R Foundation for Statistical Computing</publisher-name>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Teng</surname> <given-names>X</given-names></string-name>, <string-name><surname>Meng</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2021</year>) <article-title>Modulation Spectra Capture EEG Responses to Speech Signals and Drive Distinct Temporal Response Functions</article-title>. <source>eNeuro</source> <volume>8</volume>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Vallat</surname> <given-names>R</given-names></string-name> (<year>2018</year>) <article-title>Pingouin: statistics in Python</article-title>. <source>Journal of Open Source Software</source> <volume>3</volume>:<fpage>1026</fpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>van der Maaten</surname> <given-names>LJP</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>GE</given-names></string-name> (<year>2008</year>) <article-title>Visualizing Data using t-SNE</article-title>. <source>J Mach Learn Res</source> <volume>9</volume>:<fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name> (<year>2005</year>) <article-title>A Population-Average, Landmark- and Surface-based (PALS) atlas of human cerebral cortex</article-title>. <source>NeuroImage</source> <volume>28</volume>:<fpage>635</fpage>–<lpage>662</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kuperberg</surname> <given-names>G</given-names></string-name>, <string-name><surname>Jensen</surname> <given-names>O</given-names></string-name> (<year>2018</year>) <article-title>Specific lexico-semantic predictions are associated with unique spatial and temporal patterns of neural activity</article-title>. <source>Elife</source> <volume>7</volume>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="other"><string-name><surname>Weischedel</surname> <given-names>R</given-names></string-name>, <string-name><surname>Palmer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Marcus</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hovy</surname> <given-names>E</given-names></string-name>, <string-name><surname>Pradhan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ramshaw</surname> <given-names>L</given-names></string-name>, <string-name><surname>Xue</surname> <given-names>N</given-names></string-name>, <string-name><surname>Taylor</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kaufman</surname> <given-names>J</given-names></string-name>, <string-name><surname>Franchini</surname> <given-names>M</given-names></string-name>, <string-name><surname>El-Bachouti</surname> <given-names>M</given-names></string-name>, <string-name><surname>Belvin</surname> <given-names>R</given-names></string-name>, <string-name><surname>Houston</surname> <given-names>A</given-names></string-name> (<year>2013</year>) <article-title>OntoNotes Release 5.0 LDC2013T19</article-title>. <source>Web Download. Philadelphia: Linguistic Data Consortium</source>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="book"><string-name><surname>Wickham</surname> <given-names>H</given-names></string-name> (<year>2016</year>) <source>ggplot2: Elegant Graphics for Data Analysis</source>: <publisher-name>Springer-Verlag</publisher-name> <publisher-loc>New York</publisher-loc>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="other"><string-name><surname>Yu</surname> <given-names>G</given-names></string-name> (<year>2020</year>) <article-title>scatterpie: Scatter Pie Plot</article-title>. <source>In, R package version 0.1.5. Edition</source>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89703.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peelle</surname>
<given-names>Jonathan Erik</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Northeastern University</institution>
</institution-wrap>
<city>Boston</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study advances our understanding of how listeners understand speech when there are multiple talkers by showing that the content of the speech affects acoustic processing. The evidence is generally <bold>solid</bold>, although additional details on the methods to allow replication would strengthen the study. The work will be of use to researchers interested in the neuroscience of speech and language processing.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89703.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The manuscript describes a study in which younger, normal-hearing adults listened to two concurrent speech streams (audio-visual presentation) while magnetoencephalography (MEG) was recorded. They were asked to attend to one and ignore the other speech stream. Speech materials were processed using natural language processing (NLP) model approaches to categorize speech chunks of about 3.5 s duration as being of either high or low probability based on topic modeling. MEG results show that decoding performance (reconstruction of speech) was high for the high-probability speech chunks under both the attend and ignore conditions, suggesting that semantic information in the unattended speech was still processed. The conclusions of this paper are mostly well supported by the data.</p>
<p>Strengths:</p>
<p>1. The authors use sophisticated analyses using natural language processing models - that are beyond the state-of-the-art - to make inferences about semantic speech processing in the brain. The analytic methods are well described, enabling readers to possibly implement the approach for their own analyses.</p>
<p>2. The study shows that highly salient semantic information of speech is processed in the brain even when a listener attends to something different. The work has implications for selective attention models that are concerned with how individuals process speech.</p>
<p>Weaknesses:</p>
<p>1. The title of the manuscript may be a bit misleading: &quot;Get the gist of the story: Neural map of topic keywords in multi-speaker environment&quot;. The study was not about the gist of the story but about the gist of speech chunks of about 3.5 s. The study shows important evidence that neural activity is sensitive to the gist of short speech segments, even in unattended speech, but the gist of the story is a yet more abstract level that cannot be reduced to the gist of short speech chunks.</p>
<p>2. The calculations of t-values for the spatial maps showing significant clusters were non-standard, which makes interpreting the magnitude of the t-values difficult. Better motivation for why the specific approach was chosen would be important, or perhaps replacing it with a more standard approach. It further appears that the region of interest analyses were carried out without multiple comparison corrections, possibly suggesting a note of caution about some of the source-localization results.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89703.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study by Park and Gross investigates the spatiotemporal neural representation of semantic information most pertinent to the gist of speech materials presented to subjects as magnetoencephalography was recorded. Participants heard and saw naturalistic continuous speech recordings (with the auditory component presented to one ear), while also presented with distractor auditory speech (presented in the other ear). Participants were instructed to attend to the speech stream that matched the video of the speaker. The stimuli were semantically parsed to create short segments to which topic probabilities were assigned. These segments were then organized into high and low topic probabilities for each of the four topics (determined using Latent Dirichlet Allocation (LDA) analysis). The results suggest clear differences in the fidelity of neural encoding of the speech envelope during high-topic probability segments, which is interpreted as the brain representing key information for a story whether that information is explicitly attended to.</p>
<p>Strengths:</p>
<p>
The use of LDA analysis makes possible the quantification of whether a particular speech segment is relevant to a particular topic and enables analysis based on this high-temporal resolution of semantic salience. The authors show clear differences between attended and unattended speech conditions, as well as, surprisingly, differences between semantically salient unattended speech and attended, less semantically relevant speech.</p>
<p>Weaknesses:</p>
<p>
Though the effect sizes of the results of this study show clear differences between stimulus conditions, clarification of the experimental methods is needed to appreciate their interpretation. Broadly, I would suggest adding a clearer description of the task during data collection, even though it has been published elsewhere.</p>
<p>One key piece of information that is missing is how semantically relevant topics are assigned, so that salient semantic information can be compared between attended and unattended stories. It's unclear to me how results are combined across topics and stories. If a particular speech segment is assigned 4 topic probabilities, that segment has both a high probability of belonging to one topic and a low probability of belonging to another. I understand how this can be used to create the experimental conditions for a single topic, but how are results combined across topics?</p>
<p>I think some discussion of using the encoding and decoding of the speech envelope as a measure of what is semantically relevant is warranted. The fidelity with which the speech envelop is represented has been used as a proxy for how well that speech is attended to, but it is unclear to me whether we should expect to see high-fidelity encoding of speech envelop outside of the primary and secondary auditory regions of the brain, or how it relates to the semantic information contained in the speech signal.</p>
<p>Additionally, I wonder if it might be more informative to decode the topic labels themselves directly by building a model to predict the topic probabilities from the neural data? This might give a more direct measure of where and when semantically relevant information is represented.</p>
</body>
</sub-article>
</article>