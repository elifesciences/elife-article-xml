<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99047</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99047</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99047.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The inevitability and superfluousness of cell types in spatial cognition</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5297-2114</contrib-id>
<name>
<surname>Luo</surname>
<given-names>Xiaoliang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7261-9257</contrib-id>
<name>
<surname>Mok</surname>
<given-names>Robert M</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="author-notes" rid="n1">**</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7883-7076</contrib-id>
<name>
<surname>Love</surname>
<given-names>Bradley C</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="author-notes" rid="n1">**</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Experimental Psychology, University College London</institution>, <city>London</city>, <country>UK</country></aff>
<aff id="a2"><label>2</label><institution>MRC Cognition and Brain Sciences Unit, University of Cambridge</institution>, <city>Cambridge</city>, <country>UK</country></aff>
<aff id="a3"><label>3</label><institution>Department of Psychology</institution>, Royal Holloway, <institution>University of London Royal Holloway, University of London</institution>, <city>Egham</city>, <country>UK</country></aff>
<aff id="a4"><label>4</label><institution>The Alan Turing Institute</institution>, <city>London</city>, <country>UK</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>To whom correspondence should be addressed; E-mail: <email>xiao.luo.17@ucl.ac.uk</email></corresp>
<fn id="n1"><label>**</label><p>Co-senior Authors.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-08-28">
<day>28</day>
<month>08</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99047</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-08">
<day>08</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-12">
<day>12</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.01.10.575026"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Luo et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Luo et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99047-v1.pdf"/>
<abstract>
<title>Summary</title><p>Discoveries of functional cell types, exemplified by the cataloging of spatial cells in the hippocampal formation, are heralded as scientific breakthroughs. We question whether the identification of cell types based on human intuitions has scientific merit and suggest that “spatial cells” may arise in non-spatial computations of sufficient complexity. We show that deep neural networks (DNNs) for object recognition, which lack spatial grounding, contain numerous units resembling place, border, and head-direction cells. Strikingly, even untrained DNNs with randomized weights contained such units and support decoding of spatial information. Moreover, when these “spatial” units are excluded, spatial information can be decoded from the remaining DNN units, which highlights the superfluousness of cell types to spatial cognition. Now that large-scale simulations are feasible, the complexity of the brain should be respected and intuitive notions of cell type, which can be misleading and arise in any complex network, should be relegated to history.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label><title>Introduction</title>
<p>Spatial cognition encompasses our cognitive abilities to know where we are and navigate in complex environments, which includes self-localization, comprehension of environmental layouts, and efficient navigation toward distant goal locations. The prevailing view of the field attributes these functions to neural machinery within the hippocampal formation which contain a diverse array of “spatial” cell types including place cells, head-direction cells, border cells, and grid cells (e.g., <xref ref-type="bibr" rid="c19">Hartley et al. 2014</xref>; <xref ref-type="bibr" rid="c37">Moser et al. 2008</xref>; <xref ref-type="bibr" rid="c14">Grieves and Jeffery 2017</xref>), and that these cells collectively form the foundation of an internal cognitive map of space that underlie our spatial abilities (<xref ref-type="bibr" rid="c39">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="c50">Taube et al., 1990</xref>; <xref ref-type="bibr" rid="c29">Lever et al., 2009</xref>; <xref ref-type="bibr" rid="c17">Hafting et al., 2005</xref>). As such, a substantial proportion of the field focuses on characterizing particular functional “cell types” based on neural activity patterns in these regions. However, these “spatial cells” were identified due to their intriguing firing patterns that piqued the interest of neuroscientists and later defined based on subjective criteria – place cells appear to encode particular locations, border cells respond to borders, head-direction cells are sensitive to a heading direction, grid cells exhibit firing fields in a grid-like pattern. In practice, the field’s initial fascination with these cells has led to a stubborn tendency to focus on “cell type” classification and overlook the fact that the criteria for distinct cell types seldom align seamlessly with empirical data and often exhibit mixed selectivity across task and environmental variables (e.g., <xref ref-type="bibr" rid="c22">Hollup et al. 2001</xref>; <xref ref-type="bibr" rid="c32">McKenzie et al. 2013</xref>; <xref ref-type="bibr" rid="c6">Dupret et al. 2010</xref>; <xref ref-type="bibr" rid="c8">Eichenbaum 2015</xref>; <xref ref-type="bibr" rid="c14">Grieves and Jeffery 2017</xref>; <xref ref-type="bibr" rid="c28">Latuske et al. 2015</xref>; <xref ref-type="bibr" rid="c48">Tang et al. 2015</xref>), raising questions about their precise roles, if any.</p>
<p>Could these cells simply arise from any systems with complex computations, unrelated to processes specific to spatial or navigation? There are strong hints that “spatial” cells may arise simply from domain-general learning algorithms that can lead to both concept and spatial neural representations (<xref ref-type="bibr" rid="c35">Mok and Love, 2019</xref>, <xref ref-type="bibr" rid="c36">2023</xref>; <xref ref-type="bibr" rid="c47">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="c55">Whittington et al., 2020</xref>). Indeed, models show that spatial firing patterns can arise from factors entirely unrelated to spatial cognition, such as sparseness constraints (<xref ref-type="bibr" rid="c27">Kropff and Treves, 2008</xref>; <xref ref-type="bibr" rid="c11">Franzius et al., 2007a</xref>,<xref ref-type="bibr" rid="c12">b</xref>).</p>
<p>Another question is whether these cells are particularly important for spatial cognition. Research has shown that “spatial” cells may not be functionally more critical than those with other cells with multiplexed or hard-to-interpret firing patterns for spatial localization (<xref ref-type="bibr" rid="c4">Diehl et al., 2017</xref>) or for explaining neuronal response profiles in the medial entorhinal cortex (<xref ref-type="bibr" rid="c38">Nayebi et al., 2021</xref>). Even lesion studies in the hippocampal formation do not consistently result in specific navigation impairments (<xref ref-type="bibr" rid="c18">Hales et al., 2014</xref>; <xref ref-type="bibr" rid="c53">Whishaw and Tomie, 1997</xref>; <xref ref-type="bibr" rid="c54">Whishaw et al., 1997</xref>).</p>
<p>Considering that both biological and artificial “cells” rarely perfectly align with human-defined criteria with many cells exhibit “uninterpretable” firing patterns, one unsettling possibility is that the neuroscience may have inadvertently constrained its pursuit by fixating on the search for interpretable “cell types”. By adopting this top-down, perhaps näıve approach, the field may be investing a large proportion of its resources in a potentially fruitless quest, ignoring other explanations for the emergence of spatial cells and the foundations of spatial cognition.</p>
<p>A harsh appraisal of the field is that it looks for whatever cell type superficially reflects the answer sought. For example, how do animals localize themselves? The naive answer is that there must be place cells. Setting aside this answer provides zero insight into how such a cell came to be, there is no reason for the brain to perfectly align with our intuitions and, equally problematic, cells with these properties might arise but not serve the top-down function neuroscientist ascribe to them. Ascribing interpretable functions to cells and naming them may be good for neuroscience careers, but is it good for neuroscience?</p>
<p>In this contribution, we offer an alternative to the prevailing notion that cells with readily interpretable receptive fields in regions like hippocampus underlie spatial systems. We propose that what we commonly refer to as “spatial cells” might be inevitable derivatives of general computational mechanisms, and are in no way intrinsically spatial. Our results demonstrate that these cells could manifest within any sufficiently rich information processing system, such as perception. Remarkably, not only do we find spatial signals within a non-spatial system but also demonstrate that these purported signals, ostensibly representing spatial knowledge, do not appear to have a privileged role for tangible downstream tasks related to spatial cognition as previously claimed.</p>
<p>To evaluate these possibilities, we turned to deep neural networks (DNNs) that were information processing systems devoid of components dedicated to spatial processing. Specifically, we analyzed deep learning models of perception with hundreds of millions of parameters. DNNs trained on natural images have achieved human-level performance in recognizing real-world objects in photographs (<xref ref-type="bibr" rid="c26">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="c45">Simonyan and Zisserman, 2015</xref>). These models exhibit strong parallels with representations in the primate ventral visual stream (<xref ref-type="bibr" rid="c25">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="c56">Yamins et al., 2014</xref>; Güçlü and van Gerven, 2015; <xref ref-type="bibr" rid="c10">Eickenberg et al., 2017</xref>; <xref ref-type="bibr" rid="c57">Zeman et al., 2020</xref>). Our investigation aims to determine whether processing egocentric visual information within such complex – yet non-spatial – models can lead to brain-like representations of allocentric spatial environments, and whether these representations are essential for spatial cognition. Additionally, we considered whether untrained DNNs (absent any experiences) can account for the emergence of spatial cells and spatial cognition.</p>
<p>To evaluate functional spatial information in perceptual DNN models, we began by creating a three-dimensional virtual environment reminiscent of a laboratory used in animal studies in neuroscience (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). In this virtual setting, an agent randomly forages in a two-dimensional square area, much like an animal would explore an enclosure. The agent “sees” images of the room within its field of view over many locations and heading directions. Images from the first-person perspective are processed by a DNN, and we assess whether the internal representations of the model contain various kinds of spatial knowledge. We adopted a decoding framework which parallels experimental techniques employed by neuroscientists. Specifically, we trained linear regression models using various levels of representations (unit activations from visual inputs) generated by DNNs of different architectures to decode navigation-relevant variables, including self-localization, heading direction, and distance to the closest wall on locations and heading directions they have not encountered before. To test whether DNN units carry information like “spatial” cells in the brain, we classified and visualized model units based on standard criteria for place, direction, and border cells (<xref ref-type="bibr" rid="c49">Tanni et al., 2022</xref>; <xref ref-type="bibr" rid="c2">Banino et al., 2018</xref>). To evaluate whether these cells play a privileged role in spatial cognition, we excluded units classified as spatial units based on traditional spatial cell criteria and assessed their spatial knowledge.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Assessing spatial knowledge in non-spatial perception systems using a linear decoding approach in a virtual environment.</title>
<p>(A) A three-dimensional virtual space is created to resemble a realistic laboratory environment with a variety of visual features. An agent moves randomly in a two-dimensional area which is within a three-dimensional space and processes first-person views of the environment. Central image shows the threedimensional environment. Surrounding images are example views taken by the agent at different locations and heading directions. (B) Top-down view of the area where the agent can explore. We define spatial knowledge of the agent with four values. The agent’s location is denoted by the Cartesian coordinates t<italic><sub>x</sub></italic>, t<italic><sub>y</sub></italic>. The agent’s heading direction is denoted by the angle t<italic><sub>r</sub></italic>. The distance between the agent and the nearest wall is t<italic><sub>b</sub></italic>. (C) Individual views are processed by perception models (deep neural networks of object recognition). We train linear regression models with various levels of internal representations from these networks to assess spatial knowledge related to self-location (t<italic><sub>x</sub></italic>, t<italic><sub>y</sub></italic>), heading direction (t<italic><sub>r</sub></italic>) and distance to the closest wall (t<italic><sub>b</sub></italic>).</p></caption>
<graphic xlink:href="575026v1_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To foreshadow our findings, we discovered that DNNs, regardless of their architectural variations, representation levels, or training states, possess noteworthy proficiency in allocentric spatial understanding. Spatial variables including location, heading direction and distance to boundaries can be accurately decoded from the DNN models. Additionally, a substantial number of DNN units exhibit spatial firing patterns that align with the criteria used in neuroscience to classify place, head-direction, and border cells. Notably, a significant portion of these units employs mixed-selectivity and conjunctive coding, akin to characteristics found in hippocampal neurons (<xref ref-type="bibr" rid="c8">Eichenbaum, 2015</xref>). We further revealed that excluding units meeting the classical criteria of spatial cell types has minimal impact on spatial decoding performance, reinforcing the perspective that the code underpinning spatial cognition is more distributed and diverse than previously thought, relying on neural population coding rather than individual and specialized cell types (<xref ref-type="bibr" rid="c21">Hebb, 1949</xref>; <xref ref-type="bibr" rid="c9">Eichenbaum, 2018</xref>; <xref ref-type="bibr" rid="c7">Ebitz and Hayden, 2021</xref>). In summary, our analyses suggest that “spatial” cell types may be inevitable and superfluous in complex information processing systems.</p>
</sec>
<sec id="s2">
<label>2</label><title>Results</title>
<sec id="s2a">
<label>2.1</label><title>Spatial Knowledge through Non-spatial Systems</title>
<p>We predict that spatial knowledge can arise in computational systems with sufficient complexity absent a spatial grounding. We test this hypothesis by analyzing the percepts of an agent freely moving in a virtual environment. The percepts take the form of viewpoints fed into a deep neural network with its millions of weights either randomly initialized or trained for object recognition, a non-spatial task. From various network layers, we attempted to decode spatial information such as location, from this complex computational system.</p>
<p>To assess information latent in complex networks, we trained linear regression models to decode the agent’s location, head-direction and distance to the closet border using fixed unit activity extracted at various levels of the networks. While sequential information is important for spatial systems, our decoding framework does not utilize sequential information of sampled views during training, making it a stricter test for the central claim that spatial knowledge can form out of general information processing. We expect that one’s location, heading-direction and distance from border may be decodable from visual information alone due to the inherent continuity of perception across spatial locations and/or directions. All results reported here are tested on out-of-sample locations and views using the perception model VGG-16 (Simonyan et al., 2014), unless noted otherwise. For details referring to training and testing procedure, see Methods. For results of other models such as Resnet-50 (<xref ref-type="bibr" rid="c20">He et al., 2016</xref>) and Vision Transformers (ViT; <xref ref-type="bibr" rid="c5">Dosovitskiy et al. 2020</xref>), see Appendix.</p>
<p>We find that spatial knowledge, namely one’s own location (spatial coordinates; <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, left panel), heading direction (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, middle panel) and distance to the nearest wall (<xref rid="fig2" ref-type="fig">Fig. 2B</xref>, right panel) can be successfully decoded from unit activity in a perception model. We quantified the degree of spatial knowledge by the decoding error, that is, the deviation of the model’s prediction of its own location, heading direction, or distance to the nearest wall relative to the ground truth (i.e., the physical unit of space or angle in the virtual environment). We normalized decoding error with respect to the maximal unit length of the moving area (see Methods). For example, a normalized error of 0.05 for location prediction means the decoded location is 5% off from the true location relative to the maximal width/depth of the entire moving area. As more locations and views are sampled for training the agent (x-axis), decoding performance improves. The agent demonstrates impressive decoding performance across all spatial knowledge tests, even when trained on just 30% of randomly selected independent locations and associated viewpoints within the entire space. The moving area encompasses 289 distinct locations, each featuring 24 evenly spaced views covering the full 360-degree spectrum (for more details, please see Methods).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Perception models absent a spatial basis possess extensive spatial knowledge.</title>
<p>(A) Decoding performance across tasks and model layers. Across three tasks, mid-to-late layers exhibited lower decoding errors compared to early layers and the penultimate layer of VGG-16 (fc2). All layers outperformed chance (green and blue lines). (B) Decoding performance across a number of deep neural network architectures (penultimate layer; see Appendix for full results), including convolutional networks and vision transformers. All pre-trained and untrained models outperformed baseline measures. Error is in normalized virtual environment units (see Methods). See full results across models, layers and sampling rates in the Appendix. Shaded areas in B and error bars in C represent 95% confidence intervals of the mean decoding error (bootstrapped across locations).</p></caption>
<graphic xlink:href="575026v1_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Decoding performance varied across layers of the network where deeper layers typically achieved lower decoding error (apart from the penultimate layer in VGG-16, i.e., fc2). Crucially, decoders across all sampling rates and levels of representation show substantially better performance than chance (green and blue lines), determined by strategies invariant to visual inputs (i.e., random or predicting the center of possible choices; see Methods). Our results demonstrate there is sufficient spatial information to achieve remarkably low decoding error in a perception model based on a few visual snapshots of the virtual space, which could be further improved through the integration of information across views in a perception-based navigation system.</p>
<p>To test the generality our claim that spatial knowledge can arise from complex computational systems irrespective of their architectural variations or training states, we examined several computational systems including deep convolutional neural networks (DCNN) and Vision Transformers (ViT). We evaluated these models in both their pre-trained form (optimized for visual object recognition), and in an untrained state (randomly initialized parameters), following the same decoding approach described earlier. Here, we present the results for the penultimate layer representation and uniformly sampled 30% of all locations and their views for training the decoder, but similar results were observed across layers (for full results across various models, layers, and sampling rates, see Appendix.). We found that all models possessed latent spatial knowledge, exhibiting lower errors far below chance (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Notably, this was true even in untrained networks. While one might contend that deep convolutional neural networks, with their reliance on local convolution operations, inherently possess a predisposition for extracting spatial knowledge from visual information, the revelation that an untrained ViT—comprising nothing more than a hierarchy of fully-connected layers (i.e., self-attention) and non-linear operations—can achieve superior decoding performance illustrates the inevitability of computational complexity in facilitating the extraction of spatial information necessary for cognitive spatial processes, even in non-spatial systems.</p>
</sec>
<sec id="s2b">
<label>2.2</label><title>“Spatial Cells” Inevitably Arise in Complex Computational Systems</title>
<p>How was it possible to decode spatial information from non-spatial networks? The field’s common conception is that cells exhibiting spatial firing profiles play a pivotal role in supporting spatial cognition, as these profiles appear intuitively useful to spatial tasks such as navigation. Could spatial cells be responsible for the impressive decoding of spatial information within a perception system like we have shown? If a perception system devoid of a spatial component demonstrates classically spatially-tuned unit representations, such as place, head-direction, and border cells, can “spatial cells” truly be regarded as “spatial”? Might they be inevitable derivatives of any complex system? If so, do they drive spatial knowledge, or are they in fact superfluous in spatial cognition?</p>
<p>To determine if the model’s spatial knowledge is primarily supported by spatially-tuned units like those found in the brain, we classified every hidden unit in each model based on criteria used to identify spatial cells in neuroscience for place cells (<xref ref-type="bibr" rid="c49">Tanni et al., 2022</xref>), head-direction cells (<xref ref-type="bibr" rid="c2">Banino et al., 2018</xref>), and border cells (<xref ref-type="bibr" rid="c2">Banino et al., 2018</xref>), respectively (see Methods). The composition of cell types across layers is shown in <xref rid="fig3" ref-type="fig">Fig. 3A</xref>. Contrary to the predominant view that “spatial cells” are unique properties of spatial systems and are grounded in space-related tasks, we found many units in the non-spatial model VGG-16 that satisfied the criteria of place, head-direction, and border cells (see Methods for criteria; for other models, see Appendix). The majority of units show mixed selectivity irrespective of layer depth. Notably, a significant proportion of units show both place and directional tuning (P+D), matching a common observation in the literature (e.g., <xref ref-type="bibr" rid="c34">McNaughton et al. 1991</xref>; <xref ref-type="bibr" rid="c48">Tang et al. 2015</xref>; <xref ref-type="bibr" rid="c14">Grieves and Jeffery 2017</xref>). We selected example units and plotted their spatial activation patterns in the two-dimensional virtual space (irrespective of direction), and their direction selectivity in polar plots (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>-<xref rid="fig3" ref-type="fig">3E</xref>; for more examples, see Appendix). These examples show spatially-tuned units without directional tuning that match hippocampal place cells (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>), direction-selective units with strong direction selectivity accompanied by minor spatial selectivity matching head-directional cells (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>), and units with boundary cell-like tuning (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>). There were also many units that exhibited both strong place and directional tuning (<xref rid="fig3" ref-type="fig">Fig. 3E</xref>). These results support our theory that “spatial cells” might arise in any computational systems, even in systems designed for nonspatial tasks (e.g., object recognition). Consistent with our view, we found no clear relationship between cell type distribution and spatial information in each layer. This raises the possibility that “spatial cells” do not play a pivotal role in spatial tasks as is broadly assumed.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Representations of perception models developed for object recognition exhibit typical spatial cell-like firing profiles.</title>
<p>Active units were classified across model layers based on standard criteria used to identify place cells (P), head-direction cells (D), and border cells (B). Example units from VGG-16 (see Appendix for more examples from other models). (A) Pie charts illustrating the proportion of “spatial” cell types identified in DNNs, including units that are inactive. Many units satisfied the criteria for place, head-direction and border cells irrespective of layer depth. Many units exhibited mixed selectivity, with a significant amount of units displaying strong place and directional tuning. (B-E) Spatial firing profiles of model units in spatial activation maps and polar plots . For activation maps, each unit’s activation was plotted at each location in the two-dimensional area irrespective of heading direction. For direction selectivity, polar plots show the average activity of the activity map across location at a given angle, which reflects the tuning magnitude to each heading direction). (B) Example place cell units that show strong spatial selecitivity with little direction selectivity. (C) Examples of head-direction cell units that show strong direction selectivity but weak location selectivity. (D) Examples of border cell units that respond strongly to boundaries of the environment. (E) Examples of mixed-selective place and head-direction cell units with strong spatial and directional tuning. Examples presented are from VGG-16. See Appendix for more examples.</p></caption>
<graphic xlink:href="575026v1_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2c">
<label>2.3</label><title>“Spatial Cells” Are Superfluous in Spatial Cognition</title>
<p>We have establishd that“spatial cells” can arise in non-spatial systems. Here, we consider whether units with spatial properties are necessary to decode spatial information. It may very well be that spatial units, akin to place cells, might not only arise in complex (including random) networks but are also superfluous to spatial cognition. To test this possibility, we performed a systematic exclusion analysis to assess whether model units that exhibit the strongest spatial firing properties contribute to spatial knowledge. First, we scored and ranked each unit classified as place, head-direction, and border cell units (see Methods), and then re-trained linear decoders without the top n units of a specific cell type and evaluated the model’s corresponding spatial knowledge of the environment. That is, we test model’s location, heading direction and distance to closet border decoding ability by excluding place, head-direction and border cell units, respectively. We repeated this procedure with a progressively higher exclusion ratio (see Methods).</p>
<p>In line with our hypothesis, excluding spatial units in the models that scored highest on each of the corresponding criteria (place field activity, number of place fields, directional tuning, and border tuning), had minimal effect on decoding performance even with a large proportion of the highest ranked spatial units being excluded (<xref rid="fig4" ref-type="fig">Fig 4B</xref>, top panel). To assess whether any detriment was observed was significant and specific to excluding highly-ranked spatial units, we randomly excluded the same number of units without regard to the spatial score (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>, bottom panel) and observed a similar pattern across four exclusion scenarios, meaning that highly-tuned spatial units contributed no more to spatial cognition than a random selection of units.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>“Spatial” cells do not play a privileged role in spatial cognition.</title>
<p>Exclusion analyses showed that units exhibiting traditional spatial firing profiles do not form the basis of the model’s spatial knowledge. (A) Units in each layer were ranked based on standard criteria for place (maximum place field activity, number of place fields), head-direction (strength of directional tuning), and border (strength of border tuning) cells. As more highly-ranked spatial units were excluded, the overall decoding performance remained relatively stable across all tasks (top). Excluding an equivalent number of units randomly yielded similar performance (bottom). (B) Model units’ contribution to spatial knowledge based on their contribution to decoding performance (magnitude of regression coefficients). More highly-ranked task-relevant units excluded results in a marked deterioration of decoding performance by decoders trained on the remaining units (top). Randomly excluding an equivalent number of units had minimal impact on performance (bottom). Results here are from VGG-16. For other models, please refer to the Appendix.</p></caption>
<graphic xlink:href="575026v1_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Finally, we performed an additional analysis where we excluded model units that contributed most to each of the tasks. Specifically, we identified each unit’s importance for each task, based on the magnitudes of the coefficients learned by the linear decoders (trained in <xref rid="s2a" ref-type="sec">Section 2.1</xref>), excluded the top n units, and re-trained linear decoders for each respective task using the remaining units. This approach had a greater impact on decoding error compared to spatial unit criteria or random exclusion, with performance suffering more as the exclusion ratio rose (see <xref rid="fig4" ref-type="fig">Figure 4C</xref>). Nevertheless, decoding was still possible at high exclusion ratios, suggesting that spatial information is widely distributed across network units.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label><title>Discussion</title>
<p>The prevailing perspective in neuroscience conflates spatial knowledge and the variety of spatial cognitive abilities in animals with “spatial cells” in the hippocampal formation. In this work, we proposed an alternative perspective where “spatial cells” can simply arise in non-spatial computational systems with sufficient complexity, and that these units may, in fact, be by-products of such systems absent any leading role in spatial cognition.</p>
<p>Deep neural networks (DNNs) for object recognition served as an ideal testbed for our hypotheses. Object recognition DNNs, designed to generate translation-invariant visual features for successful categorization, lack inherent spatial elements. Unlike spatial navigation systems (e.g., <xref ref-type="bibr" rid="c2">Banino et al. 2018</xref>; <xref ref-type="bibr" rid="c3">Cueva and Wei 2018</xref>), DNNs in object recognition do not purposefully incorporate information about velocity, heading direction, or event sequences. Exploring various DNN architectures for object recognition allows a comprehensive examination of the connection between spatial representations and non-spatial complex systems.</p>
<p>We applied DNNs to a three-dimensional virtual environment to simulate an agent foraging in a two-dimensional space, akin to how an animal explores an enclosure. Detailed analyses revealed that DNN’s representations contained sufficient spatial information to decode key variables, despite these DNNs being non-spatial perceptual systems. We found that many DNN units passed established criteria for classification as place, head-direction, and border cells, suggesting such units can be easily found in various non-spatial computational systems. Furthermore, excluding the “spatial” units had minimal impact on spatial decoding tasks, suggesting a non-essential role for spatial cognition. These results call into question utility of labelling these cell “types”.</p>
<p>We considered object recognition models from two prominent DNN families: deep convolutional networks, drawing inspiration from the mammalian visual system (<xref ref-type="bibr" rid="c13">Fukushima, 1980</xref>), and visual transformers, adapted from the Transformer architecture originally designed for natural language processing (<xref ref-type="bibr" rid="c52">Vaswani et al., 2017</xref>). Spatial information was readily decoded from models from both families, including untrained models with randomly initialized weights. The decoding results from the untrained visual transformer are particularly striking because transformers incorporate minimal inductive biases, such as constraints that respect spatial proximity (e.g., convolutions). In summary, complex networks that are not spatial systems, coupled with environmental input, appear sufficient to decode spatial information.</p>
<p>Our work raises the question of whether focusing on single cells and their “type” is the correct approach to understanding spatial cognition, or indeed any cognitive function. The field has maintained a persistent inclination towards identifying cell types based on firing profiles that are subjectively interpreted to be useful for the behavior of interest (<xref ref-type="bibr" rid="c19">Hartley et al., 2014</xref>; <xref ref-type="bibr" rid="c37">Moser et al., 2008</xref>; <xref ref-type="bibr" rid="c44">Sarel et al., 2017</xref>; <xref ref-type="bibr" rid="c51">Tsao et al., 2018</xref>; Høydal et al., 2019; <xref ref-type="bibr" rid="c15">Grieves et al., 2020</xref>; <xref ref-type="bibr" rid="c41">Ormond and O’Keefe, 2022</xref>). This is likely due to the historical backdrop in neuroscientific research where pioneers in visual neuroscience discovered and named individual cells in V1 by observing their firing profiles (<xref ref-type="bibr" rid="c23">Hubel and Wiesel, 1959</xref>) which won the Nobel Prize (1981) and marked a foundational era for the field. This paradigm, led to discoveries of place and grid cells (Nobel Prize in Physiology or Medicine, 2014), and the “Jennifer Aniston” neuron or “concept cells” (<xref ref-type="bibr" rid="c42">Quiroga et al., 2005</xref>), which were intuitively compelling and attractive as an apparent explanation. These cells may in fact play a role, but our work suggests that this must be critically assessed rather than assumed, as they may not play a privileged role compared to other cells in the population, and could even be superfluous for the context at hand. The general assumption that the simplicity and interpretability of firing profiles somehow lends support to a crucial role of these cells should be questioned. With a growing recognition of the imperative to move beyond mere cell type categorization, as underscored by <xref ref-type="bibr" rid="c40">Olshausen and Field (2006)</xref> on the insufficient characterization of a majority of V1 cells, there is a new shift to focus on a broader perspective where neural assemblies or populations form the fundamental computational unit (<xref ref-type="bibr" rid="c21">Hebb, 1949</xref>; <xref ref-type="bibr" rid="c9">Eichenbaum, 2018</xref>; <xref ref-type="bibr" rid="c7">Ebitz and Hayden, 2021</xref>).</p>
<p>We pose a broader question: do our preconceptions of how complex systems <italic>should</italic> work hinder our aim to understand the brain’s inner workings? Notably, spatial representations conventionally linked to the hippocampal formation have been unveiled in sensory areas (<xref ref-type="bibr" rid="c43">Saleem et al., 2018</xref>; <xref ref-type="bibr" rid="c31">Long et al., 2021</xref>; <xref ref-type="bibr" rid="c30">Long and Zhang, 2021</xref>). While prior attributions pointed to modulatory signals from the hippocampus, our study raises concerns about potential biases stemming from preconceived notions and an underestimation of the complexity inherent in the sensory system—it may be shouldering more extensive responsibilities than initially presumed. Work in other domains are reaching similar conclusions (cf. <xref ref-type="bibr" rid="c33">McMahon and Isik 2023</xref>).</p>
<p>One upshot of our contribution is that the pursuit of identifying and cataloging cell types aligned with subjective notions of how the brain works might impede scientific progress. While searching for cell types was a reasonable strategy in an era before it was straightforward to conduct large-scale simulations, this quest should now be relegated to history. Otherwise, apparent discoveries of cell types are likely to reflect the activity of broad classes of complex networks that implement a variety of functions or none at all in the case of the random networks we considered. It is far too easy for neuroscientists, including computational neuroscientists who design their models to manifest a variety of cell types, to fool themselves when the underlying complexity of the systems considered is not taking into account.</p>
</sec>
<sec id="s4">
<label>4</label><title>Methods</title>
<sec id="s4a">
<label>4.1</label><title>Virtual environment</title>
<p>We setup our virtual environment using the Unity3D Engine (editor version: 2021.3.16f1; Silicon). The three-dimensional laboratory environment was adapted from an existing template purchased on the Unity Store (<xref ref-type="bibr" rid="c1">3DEverything, 2022</xref>). Our virtual agent’s movement is constrained in a two-dimensional square placed inside the three-dimensional space, much like a moving area for an animal in real-world experiments. The agent randomly moves around the square area and captures first-person pictures of the environment across locations and heading directions.</p>
<sec id="s4a1">
<title>Environment specifications</title>
<p>The specifications for the environment, measured in Unity units, are as follows: the lab room has a height of 3.17 units, a width of 6.29 units, and a depth of 10.81 units. The moving area, which is a distinct part of the environment, has a height of 0.25 units, a width of 2 units, and a depth of 2 units. Additionally, the moving area is located at a specific location whose relative distances from the center to the lab room boundaries are: 3.59 units to the left wall, 5.35 units to the right wall, 5.55 units to the bottom wall, and 2.80 units to the front wall.</p>
<p>The agent is only allowed to move within the squared moving area. For simplicity, we discretize reachable locations by the agent in the area as a 2D grid where the agent can only move between points on the grid. We use a 17×17 grid (289 unique locations).</p>
</sec>
<sec id="s4a2">
<title>Visual input</title>
<p>To collect visual inputs from the virtual environment, the agent randomly moves along the specified n-by-n grid in the squared area. We denote l as the total number of unique locations on the grid. The grid is evenly spaced. At each location, the agent turns around and captures m number of frames, each separated by a fixed angle (e.g., 15 degrees per frame). Each frame obtained by the agent is processed by a DNN (up to a given layer). Resulting outputs at the same location are considered independent data-points, each forming a long 1D vector. Repeating this procedure for all locations yields a M-by-F data matrix <bold>X</bold> where M = l × m and F is the number of feature variables. Each feature variable represents a neuron on the output layer of the DNN.</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label><title>Spatial decoding</title>
<p>To evaluate whether representations in the deep neural networks (DNN) optimized for object recognition on natural images encode spatial information, we set up a series of spatial decoding tasks where we use fixed network representations across various hidden layers to decode spatial location (i.e., coordinates), head-direction (i.e., rotation degrees) and distance (i.e., Euclidean) to the nearest border using visual inputs of views from unvisited locations and/or directions. We formulate spatial decoding as a prediction problem where we fit a set of linear regression models on representations of visual views to predict location, direction and distance to nearest borders. We define the nearest border to the agent as the border that has the shortest Euclidean distance to the agent’s location regardless of the agent’s heading direction.</p>
<sec id="s4b1">
<title>Decoder training and testing</title>
<p>To fit and test spatial decoders, we randomly sample locations in the environment (subject to a sampling rate). When a location is sampled for training, we consider views of different rotations as independent data-points. For example, a location decoder is fit to predict a vector <bold>t<sub>i</sub></bold> = [t<italic><sub>x</sub></italic>, t<italic><sub>y</sub></italic>, t<italic><sub>r</sub></italic>, t<italic><sub>b</sub></italic>] where t<italic><sub>x</sub></italic>, t<italic><sub>y</sub></italic> are the x, y coordinates, t<italic><sub>r</sub></italic> is the direction, t<italic><sub>b</sub></italic> is the shortest distance to a border, given a view respectively. For the entire training set (<bold>X<sub>i</sub></bold>, <bold>T<sub>i</sub></bold>) where <bold>T<sub>i</sub></bold> = {<bold>t<sub>1</sub></bold>, <bold>t<sub>2</sub></bold>, …, <bold>t<sub>i</sub></bold>}, we can write the decoder as
<disp-formula id="eqn1">
<graphic xlink:href="575026v1_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>where <bold>W<sub>i</sub></bold> are the coefficients and <bold>b<sub>i</sub></bold> are the intercepts. Decoder weights (both coefficients and intercepts) are optimized to minimize the mean squared error between the true and predicted targets.</p>
</sec>
<sec id="s4b2">
<title>Feature representation and selection</title>
<p>We consider a number of representations for visual inputs. Given a DNN model, we train separate decoders using representations across different layers. We apply L<sub>2</sub> regularization on the decoder weights to reduce overfitting. Additionally, we consider different exclusion strategies (see <xref rid="s4d" ref-type="sec">Sec. 4.4</xref>) where unit representations are selectively removed based on their firing profiles (e.g., the number of place fields a unit contains).</p>
</sec>
<sec id="s4b3">
<title>Performance measure</title>
<p>We report the normalized decoding error on the heldout set of location, head-direction and nearest border decodings for each representation at a given sampling rate. The normalized decoding error reflects the deviation of the model’s prediction on location, heading direction or distance to the nearest wall relative to the ground truth (i.e., in terms of physical unit of space or angle in the virtual environment) and normalized with respect to the maximal length of the moving area (2 units in both depth and width). To reflect variance of the average error, we compute a two-sided 95% confidence interval of the average using a bootstrapping approach over locations.</p>
</sec>
<sec id="s4b4">
<title>Baselines</title>
<p>We compare decoding performance achieved by our agent to baselines where the agent decodes location, head-direction and distance to border based on fixed policies independent to visual inputs. Specifically, we establish two baselines where the agent simply either predict randomly within the legal bound or predict the centre position of the room (when predicting location), predict 90 degrees (when predicting rotation) and predict the distance from the centre of the room (when predicting nearest border distance).</p>
</sec>
</sec>
<sec id="s4c">
<label>4.3</label><title>Profiling spatial properties of model units</title>
<p>To gain a more intuitive understanding of the kinds of model units that are most useful for spatial decoding tasks, we profile all model units from the layers we consider based on their spatial patterns of firing. We consider the most common cell types that are found to encode spatial information as defined in the neuroscience literature, which include place cells, border cells and head-direction cells. For each cell type, we track a number of measures defined below.</p>
<sec id="s4c1">
<title>Place cells</title>
<p>In our investigation, the identification of place-cell-like model units hinges on several key characteristics of unit firing. Firstly, we examine the number of place fields exhibited by each unit, defining a qualified field as one that spans 2D space ranging from 10 pixels to half the size of the environment, as outlined in <xref ref-type="bibr" rid="c49">Tanni et al. (2022)</xref>. Additionally, we consider the maximal activation of each field, providing insights into the intensity and significance of unit firing.</p>
</sec>
<sec id="s4c2">
<title>Head-direction cells</title>
<p>Following <xref ref-type="bibr" rid="c2">Banino et al. (2018)</xref>, the degree of directional tuning exhibited by each unit was assessed using the length of the resultant vector of the directional activity map. In our case, there is an activity map spanning entire 2D space for each direction. Vectors corresponding to each direction of an activity map were created:
<disp-formula id="eqn2">
<graphic xlink:href="575026v1_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>where α and β are, respectively, the angle and average intensity (irrespective of spatial locations) of direction i in the activity map. These vectors were averaged to generate a mean resultant vector:
<disp-formula id="eqn3">
<graphic xlink:href="575026v1_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>and the length of the resultant vector calculated as the magnitude of ⃗r. We used 24 angular directions uniformly spaced.</p>
</sec>
<sec id="s4c3">
<title>Border cells</title>
<p>Border score definition is based on <xref ref-type="bibr" rid="c2">Banino et al. (2018)</xref> where for each of the four walls in the square enclosure, the average activation for that wall, b<italic><sub>i</sub></italic>, was compared to the average centre activity c obtaining a border score for that wall, and the maximum was used as the border-score for the unit:
<disp-formula id="eqn4">
<graphic xlink:href="575026v1_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>where b<italic><sub>i</sub></italic> is the mean activation for bins within d<italic><sub>b</sub></italic> distance from the i-th wall and c the average activity for bins further than d<italic><sub>b</sub></italic> bins from any wall (d<italic><sub>b</sub></italic> = 3). Units with border score &gt; 0.5 are considered border-like.</p>
</sec>
</sec>
<sec id="s4d">
<label>4.4</label><title>Spatial decoding with unit exclusion</title>
<p>To delve deeper into how different units within the DNN models contribute to the linear decoding of spatial information, we employ two complementary analyses. Model units are selectively excluded subject to criteria detailed below. Spatial decoders are re-trained on the rest of the model units following the same procedure as before (<xref rid="s4b" ref-type="sec">sec. 4.2</xref>).</p>
<sec id="s4d1">
<title>Excluding units by spatial profile</title>
<p>In this first analysis, we selectively exclude units based on their firing profiles, which fall into the various spatial unit categories mentioned earlier. We initially rank all units in descending order according to their specific spatial measures (e.g., field count) and then systematically increase the exclusion rate, assessing the impact on the corresponding downstream tasks as we go. For example, we progressively exclude the top n% of units with the most place-like characteristics for varying values of n, examining the relationship between exclusion and performance of location decoding. This process is carried out separately for each unit type and their associated tasks we have defined, and we also include a control group where an equivalent number of units are excluded at random.</p>
</sec>
<sec id="s4d2">
<title>Excluding units by task-relevance</title>
<p>Our second exclusion analysis focuses on identifying the task-relevance of each model unit by utilizing previously learned decoders and their coefficient magnitudes. Essentially, units with larger coefficients are deemed more crucial for decoding, while those with smaller or zero coefficients are considered less relevant. Similar to the exclusion by spatial profile, we gradually increase the exclusion rate by selectively removing units based on the magnitude of their decoder coefficients. Additionally, we conduct a control analysis where an equal number of units are randomly chosen for exclusion. We then retrain linear decoders with the remaining units and assess their performance on downstream spatial tasks.</p>
</sec>
</sec>
</sec>
<sec id="s5">
<title>Data and code availability</title>
<p>The code for simulations and analyses are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/don-tpanic/Space">https://github.com/don-tpanic/Space</ext-link>.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by ESRC (ES/W007347/1), Wellcome Trust (WT106931MA), and a Royal Society Wolfson Fellowship (18302) to B.C.L., and the Medical Research Council UK (MC UU 00030/7) and a Leverhulme Trust Early Ca-reer Fellowship (Leverhulme Trust, Isaac Newton Trust: SUAI/053 G100773, SUAI/056 G105620, ECF-2019-110) to R.M.M.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><collab>3DEverything</collab></person-group> (<year>2022</year>) <article-title>Hospital Laboratory | 3D Interior | Unity Asset Store</article-title>. URL <ext-link ext-link-type="uri" xlink:href="https://assetstore.unity.com/packages/3d/props/interior/hospital-laboratory-54382">https://assetstore.unity.com/packages/3d/props/interior/hospital-laboratory-54382</ext-link></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Banino</surname> <given-names>A</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name>, <string-name><surname>Uria</surname> <given-names>B</given-names></string-name>, <string-name><surname>Blundell</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lillicrap</surname> <given-names>T</given-names></string-name>, <string-name><surname>Mirowski</surname> <given-names>P</given-names></string-name>, <string-name><surname>Pritzel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chadwick</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Degris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Modayil</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wayne</surname> <given-names>G</given-names></string-name>, <string-name><surname>Soyer</surname> <given-names>H</given-names></string-name>, <string-name><surname>Viola</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>B</given-names></string-name>, <string-name><surname>Goroshin</surname> <given-names>R</given-names></string-name>, <string-name><surname>Rabinowitz</surname> <given-names>N</given-names></string-name>, <string-name><surname>Pascanu</surname> <given-names>R</given-names></string-name>, <string-name><surname>Beattie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sadik</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gaffney</surname> <given-names>S</given-names></string-name>, <string-name><surname>King</surname> <given-names>H</given-names></string-name>, <string-name><surname>Kavukcuoglu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hassabis</surname> <given-names>D</given-names></string-name>, <string-name><surname>Hadsell</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kumaran</surname> <given-names>D</given-names></string-name></person-group> (<year>2018</year>) <article-title>Vector-based navigation using grid-like representations in artificial agents</article-title>. <source>Nature</source> <volume>557</volume>(<issue>7705</issue>):<fpage>429</fpage>–<lpage>433</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41586-018-0102-6</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/s41586-018-0102-6">http://www.nature.com/articles/s41586-018-0102-6</ext-link></mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Cueva</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Wei</surname> <given-names>XX</given-names></string-name></person-group> (<year>2018</year>) <article-title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</article-title>. <source>arXiv</source>, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1803.07770">http://arxiv.org/abs/1803.07770</ext-link></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diehl</surname> <given-names>GW</given-names></string-name>, <string-name><surname>Hon</surname> <given-names>OJ</given-names></string-name>, <string-name><surname>Leutgeb</surname> <given-names>S</given-names></string-name>, <string-name><surname>Leutgeb</surname> <given-names>JK</given-names></string-name></person-group> (<year>2017</year>) <article-title>Grid and Nongrid Cells in Medial Entorhinal Cortex Represent Spatial Location and Environmental Features with Complementary Coding Schemes</article-title>. <source>Neuron</source> <volume>94</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>92.e6,</lpage> DOI <pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.004</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627317301873">https://linkinghub.elsevier.com/retrieve/pii/S0896627317301873</ext-link></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Dosovitskiy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kolesnikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Weissenborn</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zhai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Unterthiner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Dehghani</surname> <given-names>M</given-names></string-name>, <string-name><surname>Minderer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Heigold</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gelly</surname> <given-names>S</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Houlsby</surname> <given-names>N</given-names></string-name></person-group> (<year>2020</year>) <article-title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title>.<source>arXiv</source>, DOI <pub-id pub-id-type="doi">10.48550/arxiv.2010.11929</pub-id></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dupret</surname> <given-names>D</given-names></string-name>, <string-name><surname>O’Neill</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pleydell-Bouverie</surname> <given-names>B</given-names></string-name>, <string-name><surname>Csicsvari</surname> <given-names>J</given-names></string-name></person-group> (<year>2010</year>) <article-title>The reorganization and reactivation of hippocampal maps predict spatial memory performance</article-title>. <source>Nature Neuroscience</source> <volume>13</volume>(<issue>8</issue>):<fpage>995</fpage>–<lpage>1002</lpage>, DOI <pub-id pub-id-type="doi">10.1038/nn.2599</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn.2599">https://www.nature.com/articles/nn.2599</ext-link></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ebitz</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Hayden</surname> <given-names>BY</given-names></string-name></person-group> (<year>2021</year>) <article-title>The population doctrine in cognitive neuroscience</article-title>. <source>Neuron</source> <volume>109</volume>(<issue>19</issue>):<fpage>3055</fpage>–<lpage>3068</lpage>, DOI <pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.011</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627321005213">https://linkinghub.elsevier.com/retrieve/pii/S0896627321005213</ext-link></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eichenbaum</surname> <given-names>H</given-names></string-name></person-group> (<year>2015</year>) <article-title>Perspectives on 2014 Nobel Prize</article-title>. <source>Hippocampus</source> <volume>25</volume>:<fpage>679</fpage>–<lpage>681</lpage>, DOI <pub-id pub-id-type="doi">10.1002/hipo.22445</pub-id></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eichenbaum</surname> <given-names>H</given-names></string-name></person-group> (<year>2018</year>) <article-title>Barlow versus Hebb: When is it time to abandon the notion of feature detectors and adopt the cell assembly as the unit of cognition?</article-title> <source>Neuroscience Letters</source> <volume>680</volume>:<fpage>88</fpage>–<lpage>93</lpage>, DOI <pub-id pub-id-type="doi">10.1016/j.neulet.2017.04.006</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0304394017302975">https://linkinghub.elsevier.com/retrieve/pii/S0304394017302975</ext-link></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eickenberg</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name></person-group> (<year>2017</year>) <article-title>Seeing it all: Convolutional network layers map the function of the human visual system</article-title>. <source>NeuroImage</source> <volume>152</volume>:<fpage>184</fpage>–<lpage>194</lpage>, DOI <pub-id pub-id-type="doi">10.1016/J.NEUROIMAGE.2016.10.001</pub-id></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franzius</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sprekeler</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wiskott</surname> <given-names>L</given-names></string-name></person-group> (<year>2007a</year>) <article-title>Slowness and Sparseness Lead to Place, Head-Direction, and Spatial-View Cells</article-title>. <source>PLoS Computational Biology</source> <volume>3</volume>(<issue>8</issue>):<fpage>e166</fpage>, DOI <pub-id pub-id-type="doi">10.1371/journal.pcbi.0030166</pub-id></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Franzius</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vollgraf</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wiskott</surname> <given-names>L</given-names></string-name></person-group> (<year>2007b</year>) <article-title>From grids to places</article-title>. <source>Journal of Computational Neuroscience</source> <volume>22</volume>(<issue>3</issue>):<fpage>297</fpage>–<lpage>299</lpage>, DOI <pub-id pub-id-type="doi">10.1007/s10827-006-0013-7</pub-id>,</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fukushima</surname> <given-names>K</given-names></string-name></person-group> (<year>1980</year>) <article-title>Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title>. <source>Biological Cybernetics</source> <volume>36</volume>(<issue>4</issue>):<fpage>193</fpage>–<lpage>202</lpage>, DOI <pub-id pub-id-type="doi">10.1007/BF00344251</pub-id>,</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grieves</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Jeffery</surname> <given-names>KJ</given-names></string-name></person-group> (<year>2017</year>) <article-title>The representation of space in the brain</article-title>. <source>Behavioural Processes</source> <volume>135</volume>:<fpage>113</fpage>–<lpage>131</lpage>, DOI <pub-id pub-id-type="doi">10.1016/j.beproc.2016.12.012</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0376635716302480">https://linkinghub.elsevier.com/retrieve/pii/S0376635716302480</ext-link></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grieves</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Jedidi-Ayoub</surname> <given-names>S</given-names></string-name>, <string-name><surname>Mishchanchuk</surname> <given-names>K</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>A</given-names></string-name>, <string-name><surname>Renaudineau</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jeffery</surname> <given-names>KJ</given-names></string-name></person-group> (<year>2020</year>) <article-title>The place-cell representation of volumetric space in rats</article-title>. <source>Nature Communications</source> <volume>11</volume>(<issue>1</issue>):<fpage>789</fpage>, DOI <pub-id pub-id-type="doi">10.1038/s41467-020-14611-7</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-020-14611-7">https://www.nature.com/articles/s41467-020-14611-7</ext-link></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Güçclü</surname> <given-names>U</given-names></string-name>, <string-name><surname>van Gerven</surname> <given-names>MAJ</given-names></string-name></person-group> (<year>2015</year>) <article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>(<issue>27</issue>):<fpage>10005</fpage>–<lpage>10014</lpage>, DOI <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hafting</surname> <given-names>T</given-names></string-name>, <string-name><surname>Fyhn</surname> <given-names>M</given-names></string-name>, <string-name><surname>Molden</surname> <given-names>S</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name></person-group> (<year>2005</year>) <article-title>Microstructure of a spatial map in the entorhinal cortex</article-title>. <source>Nature</source> <volume>436</volume>(<issue>7052</issue>):<fpage>801</fpage>–<lpage>806</lpage>, DOI 10. 1038/nature03721, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nature03721">https://www.nature.com/articles/nature03721</ext-link></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hales</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schlesiger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Leutgeb</surname> <given-names>J</given-names></string-name>, <string-name><surname>Squire</surname> <given-names>L</given-names></string-name>, <string-name><surname>Leutgeb</surname> <given-names>S</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>R</given-names></string-name></person-group> (<year>2014</year>) <article-title>Medial Entorhinal Cortex Lesions Only Partially Disrupt Hippocampal Place Cells and Hippocampus-Dependent Place Memory</article-title>. <source>Cell Reports</source> <volume>9</volume>(<issue>3</issue>):<fpage>893</fpage>–<lpage>901</lpage>, DOI <pub-id pub-id-type="doi">10.1016/j.celrep.2014.10.009</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S2211124714008638">https://linkinghub.elsevier.com/retrieve/pii/S2211124714008638</ext-link></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartley</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lever</surname> <given-names>C</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>N</given-names></string-name>, <string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name></person-group> (<year>2014</year>) <article-title>Space in the brain: how the hippocampal formation supports spatial cognition</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>369</volume>(<issue>1635</issue>):<fpage>20120510</fpage>, DOI <pub-id pub-id-type="doi">10.1098/rstb.2012.0510</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/doi/">https://royalsocietypublishing.org/doi/</ext-link></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J</given-names></string-name></person-group> (<year>2016</year>) <article-title>Deep residual learning for image recognition. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</article-title>, <source>IEEE Computer Society</source>, vol 2016-Decem, pp 770–778, DOI <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hebb</surname> <given-names>D</given-names></string-name></person-group> (<year>1949</year>) <source>The Organizationof Behavior: A Neuropsychological Theory, 2002nd edn</source>. <publisher-name>Psychology Press</publisher-name>, <publisher-loc>New York</publisher-loc></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hollup</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Molden</surname> <given-names>S</given-names></string-name>, <string-name><surname>Donnett</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name></person-group> (<year>2001</year>) <article-title>Accumulation of Hippocampal Place Fields at the Goal Location in an Annular Watermaze Task</article-title>. <source>The Journal of Neuroscience</source> <volume>21</volume>(<issue>5</issue>):<fpage>1635</fpage>–<lpage>1644</lpage>, DOI 10. 1523/JNEUROSCI.21-05-01635.2001, URL <pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-05-01635.2001</pub-id></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hubel</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Wiesel</surname> <given-names>TN</given-names></string-name></person-group> (<year>1959</year>) <article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title>. <source>The Journal of Physiology</source> <volume>148</volume>(<issue>3</issue>):<fpage>574</fpage>–<lpage>591</lpage>, DOI <pub-id pub-id-type="doi">10.1113/JPHYSIOL.1959.SP006308</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Høydal Skytøen</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Andersson</surname> <given-names>SO</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name></person-group> (<year>2019</year>) <article-title>Objectvector coding in the medial entorhinal cortex</article-title>. <source>Nature</source> <volume>568</volume>(<issue>7752</issue>):<fpage>400</fpage>– <lpage>404</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41586-019-1077-7</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41586-019-1077-7">https://www.nature.com/articles/s41586-019-1077-7</ext-link></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name></person-group> (<year>2014</year>) <article-title>Deep Supervised, but Not Unsupervised</article-title>, <source>Models May Explain IT Cortical Representation. PLoS Computational Biology</source> <volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>, DOI <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id>,</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>GE</given-names></string-name></person-group> (<year>2012</year>) <source>ImageNet classification with deep convolutional neural networks</source>. <publisher-loc>In</publisher-loc>: <publisher-name>Advances in Neural Information Processing Systems</publisher-name>, vol 2, pp 1097–1105</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kropff</surname> <given-names>E</given-names></string-name>, <string-name><surname>Treves</surname> <given-names>A</given-names></string-name></person-group> (<year>2008</year>) <article-title>The emergence of grid cells: Intelligent design or just adaptation?</article-title> <source>Hippocampus</source> <volume>18</volume>(<issue>12</issue>):<fpage>1256</fpage>–<lpage>1269</lpage>, DOI <pub-id pub-id-type="doi">10.1002/hipo.20520</pub-id>,</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latuske</surname> <given-names>P</given-names></string-name>, <string-name><surname>Toader</surname> <given-names>O</given-names></string-name>, <string-name><surname>Allen</surname> <given-names>K</given-names></string-name></person-group> (<year>2015</year>) <article-title>Interspike Intervals Reveal Functionally Distinct Cell Populations in the Medial Entorhinal Cortex</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>(<issue>31</issue>):<fpage>10963</fpage>–<lpage>10976</lpage>, DOI <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0276-15.2015</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lever</surname> <given-names>C</given-names></string-name>, <string-name><surname>Burton</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jeewajee</surname> <given-names>A</given-names></string-name>, <string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>N</given-names></string-name></person-group> (<year>2009</year>) <article-title>Boundary Vector Cells in the Subiculum of the Hippocampal Formation</article-title>. <source>The Journal of Neuroscience</source> <volume>29</volume>(<issue>31</issue>):<fpage>9771</fpage>–<lpage>9777</lpage>, DOI <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1319-09.2009</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Long</surname> <given-names>X</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>SJ</given-names></string-name></person-group> (<year>2021</year>) <article-title>A novel somatosensory spatial navigation system outside the hippocampal formation</article-title>. <source>Cell Research</source> <volume>31</volume>(<issue>6</issue>):<fpage>649</fpage>–<lpage>663</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41422-020-00448-8</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Long</surname> <given-names>X</given-names></string-name>, <string-name><surname>Deng</surname> <given-names>B</given-names></string-name>, <string-name><surname>Cai</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>ZS</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>SJ</given-names></string-name></person-group> (<year>2021</year>) <article-title>A compact spatial map in V2 visual cortex. preprint</article-title>, <source>Neuroscience</source>, DOI <pub-id pub-id-type="doi">10.1101/2021.02.11.430687</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McKenzie</surname> <given-names>S</given-names></string-name>, <string-name><surname>Robinson</surname> <given-names>NTM</given-names></string-name>, <string-name><surname>Herrera</surname> <given-names>L</given-names></string-name>, <string-name><surname>Churchill</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Eichenbaum</surname> <given-names>H</given-names></string-name></person-group> (<year>2013</year>) <article-title>Learning Causes Reorganization of Neuronal Firing Patterns to Represent Related Experiences within a Hippocampal Schema</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>(<issue>25</issue>):<fpage>10243</fpage>–<lpage>10256</lpage>, DOI <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0879-13.2013</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McMahon</surname> <given-names>E</given-names></string-name>, <string-name><surname>Isik</surname> <given-names>L</given-names></string-name></person-group> (<year>2023</year>) <article-title>Seeing social interactions</article-title>. <source>Trends in Cognitive Sciences</source> <volume>27</volume>(<issue>12</issue>):<fpage>1165</fpage>–<lpage>1179</lpage>, DOI <pub-id pub-id-type="doi">10.1016/j.tics.2023.09.001</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1364661323002486">https://linkinghub.elsevier.com/retrieve/pii/S1364661323002486</ext-link></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McNaughton</surname> <given-names>L</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Markus</surname> <given-names>J</given-names></string-name></person-group> (<year>1991</year>) “<article-title>DeadReckoning,”kmdmark Learning, and the Sense of Direction: A Neurophysiological and Computational Hypothesis</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>3</volume>:<fpage>190</fpage>–<lpage>202</lpage></mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mok</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Love</surname> <given-names>BC</given-names></string-name></person-group> (<year>2019</year>) <article-title>A non-spatial account of place and grid cells based on clustering models of concept learning</article-title>. <source>Nature Communications</source> <volume>10</volume>(<fpage>1</fpage>), DOI <pub-id pub-id-type="doi">10.1038/S41467-019-13760-8</pub-id> <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6908717/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6908717/</ext-link></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mok</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Love</surname> <given-names>BC</given-names></string-name></person-group> (<year>2023</year>) <article-title>A multilevel account of hippocampal function in spatial and concept learning: Bridging models of behavior and neural assemblies</article-title>. <source>Science Advances</source> <volume>9</volume>(<issue>29</issue>):<elocation-id>eade6903</elocation-id>, DOI <pub-id pub-id-type="doi">10.1126/sciadv.ade6903</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>, <string-name><surname>Kropff</surname> <given-names>E</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name></person-group> (<year>2008</year>) <article-title>Place Cells, Grid Cells, and the Brain’s Spatial Representation System</article-title>. <source>Annual Review of Neuroscience</source> <volume>31</volume>(<issue>1</issue>):<fpage>69</fpage>– <lpage>89</lpage>, DOI <pub-id pub-id-type="doi">10.1146/annurev.neuro.31.061307.090723</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nayebi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Attinger</surname> <given-names>A</given-names></string-name>, <string-name><surname>Campbell</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Hardcastle</surname> <given-names>K</given-names></string-name>, <string-name><surname>Low</surname> <given-names>II</given-names></string-name>, <string-name><surname>Mallory</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Mel</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Sorscher</surname> <given-names>B</given-names></string-name>, <string-name><surname>Williams</surname> <given-names>AH</given-names></string-name>, <string-name><surname>Ganguli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Giocomo</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name></person-group> (<year>2021</year>) <article-title>Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks. In: Advances in Neural Information Processing Systems</article-title>, <source>Neuroscience</source>, DOI <pub-id pub-id-type="doi">10.1101/2021.10.30.466617</pub-id>,.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dostrovsky</surname> <given-names>J</given-names></string-name></person-group> (<year>1971</year>) <article-title>The hippocampus as a spatial map</article-title>. <source>Preliminary evidence from unit activity in the freely-moving rat. Brain Research</source> <volume>34</volume>(<issue>1</issue>):<fpage>171</fpage>–<lpage>175</lpage>, DOI <pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/0006899371903581">https://linkinghub.elsevier.com/retrieve/pii/0006899371903581</ext-link></mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Olshausen</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Field</surname> <given-names>DJ</given-names></string-name></person-group> (<year>2006</year>) <chapter-title>What Is the Other 85 Percent of V1 Doing?</chapter-title> In: <person-group person-group-type="editor"><string-name><surname>Van Hemmen</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name></person-group> (eds) <source>23 Problems in Systems Neuroscience</source>, <edition>1st edn</edition>, <publisher-name>Oxford University Press</publisher-name> <publisher-loc>New York</publisher-loc>, pp <fpage>182</fpage>–<lpage>212</lpage>, DOI <pub-id pub-id-type="doi">10.1093/acprof:oso/9780195148220.003.0010</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ormond</surname> <given-names>J</given-names></string-name>, <string-name><surname>O’Keefe</surname> <given-names>J</given-names></string-name></person-group> (<year>2022</year>) <article-title>Hippocampal place cells have goaloriented vector fields during navigation</article-title>. <source>Nature</source> <volume>607</volume>(<issue>7920</issue>):<fpage>741</fpage>–<lpage>746</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41586-022-04913-9</pub-id>,.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quiroga</surname> <given-names>RQ</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kreiman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Koch</surname> <given-names>C</given-names></string-name>, <string-name><surname>Fried</surname> <given-names>I</given-names></string-name></person-group> (<year>2005</year>) <article-title>Invariant visual representation by single neurons in the human brain</article-title>. <source>Nature</source> <volume>435</volume>(<issue>7045</issue>):<fpage>1102</fpage>– <lpage>1107</lpage>, DOI <pub-id pub-id-type="doi">10.1038/nature03687</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saleem</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Diamanti</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Fournier</surname> <given-names>J</given-names></string-name>, <string-name><surname>Harris</surname> <given-names>KD</given-names></string-name>, <string-name><surname>Carandini</surname> <given-names>M</given-names></string-name></person-group> (<year>2018</year>) <article-title>Coherent encoding of subjective spatial position in visual cortex and hippocampus</article-title>. <source>Nature</source> <volume>562</volume>:<fpage>124</fpage>–<lpage>127</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41586-018-0516-1</pub-id></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sarel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Finkelstein</surname> <given-names>A</given-names></string-name>, <string-name><surname>Las</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ulanovsky</surname> <given-names>N</given-names></string-name></person-group> (<year>2017</year>) <article-title>Vectorial representation of spatial goals in the hippocampus of bats</article-title>. <source>Science</source> <volume>355</volume>(<issue>6321</issue>):<fpage>176</fpage>– <lpage>180</lpage>, DOI <pub-id pub-id-type="doi">10.1126/science.aak9589</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Simonyan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname> <given-names>A</given-names></string-name></person-group> (<year>2015</year>) <article-title>Very Deep Convolutional Networks for LargeScale Image Recognition</article-title>. <source>arXiv</source>, URL <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</ext-link></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simonyan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Vedaldi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zisserman</surname> <given-names>A</given-names></string-name></person-group> (<year>2014</year>) <article-title>Deep inside convolutional networks: Visualising image classification models and saliency maps</article-title>. <source>2nd International Conference on Learning Representations, ICLR 2014 Workshop Track Proceedings pp 1–8</source></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stachenfeld</surname> <given-names>KL</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name></person-group> (<year>2017</year>) <article-title>The hippocampus as a predictive map</article-title>. <source>Nature Neuroscience</source> <volume>20</volume>(<issue>11</issue>):<fpage>1643</fpage>–<lpage>1653</lpage>, DOI <pub-id pub-id-type="doi">10.1038/nn.4650</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Ebbesen</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Sanguinetti-Scheck</surname> <given-names>JI</given-names></string-name>, <string-name><surname>Preston-Ferrer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gundlfinger</surname> <given-names>A</given-names></string-name>, <string-name><surname>Winterer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Beed</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ray</surname> <given-names>S</given-names></string-name>, <string-name><surname>Naumann</surname> <given-names>R</given-names></string-name>, <string-name><surname>Schmitz</surname> <given-names>D</given-names></string-name>, <string-name><surname>Brecht</surname> <given-names>M</given-names></string-name>, <string-name><surname>Burgalossi</surname> <given-names>A</given-names></string-name></person-group> (<year>2015</year>) <article-title>Anatomical Organization and Spatiotemporal Firing Patterns of Layer 3 Neurons in the Rat Medial Entorhinal Cortex</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>(<issue>36</issue>):<fpage>12346</fpage>–<lpage>12354</lpage>, DOI <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0696-15.2015</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanni</surname> <given-names>S</given-names></string-name>, <string-name><surname>De Cothi</surname> <given-names>W</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name></person-group> (<year>2022</year>) <article-title>State transitions in the statistically stable place cell population correspond to rate of perceptual change</article-title>. <source>Current Biology</source> <volume>32</volume>(<issue>16</issue>):<fpage>3505</fpage>–<lpage>3514.e7,</lpage> DOI <pub-id pub-id-type="doi">10.1016/j.cub.2022.06.046</pub-id>, URL <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0960982222010089">https://linkinghub.elsevier.com/retrieve/pii/S0960982222010089</ext-link></mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taube</surname> <given-names>J</given-names></string-name>, <string-name><surname>Muller</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ranck</surname> <given-names>J</given-names></string-name></person-group> (<year>1990</year>) <article-title>Head-direction cells recorded from the postsubiculum in freely moving rats</article-title>. <source>II. Effects of environmental manipulations. The Journal of Neuroscience</source> <volume>10</volume>(<issue>2</issue>):<fpage>436</fpage>–<lpage>447</lpage>, DOI <pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-02-00436.1990</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsao</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sugar</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lu</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Knierim</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name></person-group> (<year>2018</year>) <article-title>Integrating time from experience in the lateral entorhinal cortex</article-title>. <source>Nature</source> <volume>561</volume>(<issue>7721</issue>):<fpage>57</fpage>–<lpage>62</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41586-018-0459-6</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Vaswani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname> <given-names>N</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gomez</surname> <given-names>AN</given-names></string-name>, <string-name><surname>Kaiser</surname> <given-names>L</given-names></string-name>, <string-name><surname>Polosukhin</surname> <given-names>I</given-names></string-name></person-group> (<year>2017</year>) <source>Attention is all you need</source>. <publisher-loc>In</publisher-loc>: <publisher-name>Advances in Neural Information Processing Systems</publisher-name>, Neural information processing systems foundation, vol 2017-Decem, pp 5999–6009</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whishaw</surname> <given-names>IQ</given-names></string-name>, <string-name><surname>Tomie</surname> <given-names>JA</given-names></string-name></person-group> (<year>1997</year>) <article-title>Perseveration on place reversals in spatial swimming pool tasks: Further evidence for place learning in hippocampal rats</article-title>. <source>Hippocampus</source> <volume>7</volume>(<issue>4</issue>):<fpage>361</fpage>–<lpage>370</lpage>, DOI <pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1997)7:4&lt;361::AID-HIPO2&gt;3.0.CO;2-M</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whishaw</surname> <given-names>IQ</given-names></string-name>, <string-name><surname>McKenna</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Maaswinkel</surname> <given-names>H</given-names></string-name></person-group> (<year>1997</year>) <article-title>Hippocampal lesions and path integration</article-title>. <source>Current Opinion in Neurobiology</source> <volume>7</volume>(<issue>2</issue>):<fpage>228</fpage>–<lpage>234</lpage>, DOI <pub-id pub-id-type="doi">10.1016/S0959-4388(97)80011-6</pub-id>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whittington</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Muller</surname> <given-names>TH</given-names></string-name>, <string-name><surname>Mark</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>G</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>N</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TE</given-names></string-name></person-group> (<year>2020</year>) <article-title>The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation</article-title>. <source>Cell</source> <volume>183</volume>(<issue>5</issue>):<fpage>1249</fpage>–<lpage>1263</lpage>, DOI <pub-id pub-id-type="doi">10.1016/J.CELL.2020.10.024</pub-id></mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname> <given-names>DLK</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>H</given-names></string-name>, <string-name><surname>Cadieu</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Seibert</surname> <given-names>D</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name></person-group> (<year>2014</year>) <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>8624</lpage>, DOI <pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeman</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Ritchie</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Bracci</surname> <given-names>S</given-names></string-name>, <string-name><given-names>Op</given-names> <surname>de Beeck H</surname></string-name></person-group> (<year>2020</year>) <article-title>Orthogonal Representations of Object Shape and Category in Deep Convolutional Neural Networks and Human Visual Cortex</article-title>. <source>Scientific Reports</source> <year>2020</year> <volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>12</lpage>, DOI <pub-id pub-id-type="doi">10.1038/s41598-020-59175-0</pub-id>.</mixed-citation></ref>
</ref-list>
<sec>
<fig id="figs1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.1:</label>
<caption><title>VGG-16 (untrained).</title>
<p>Linear decoders trained with representations from various layers of the untrained VGG-16 achieve low errors across sampling rates; though not as good as its trained version. Mid-to-advanced layers show superior performance than early layers. As more locations are sampled for training the linear decoders, overall decoding performance improves. All model layers can decode better than two visual-invariant baselines.</p></caption>
<graphic xlink:href="575026v1_figs1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.2:</label>
<caption><title>ResNet-50 (trained).</title>
<p>Linear decoders trained with representations from various layers of the ResNet-50 pretrained on object recognition achieve low errors across sampling rates. Mid-to-advanced layers show superior performance than early layers. The penultimate layer decoding performance did not improve as much as the intermediate layers as more locations are sampled for training the linear decoders. All model layers can decode better than two visual-invariant baselines.</p></caption>
<graphic xlink:href="575026v1_figs2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.3:</label>
<caption><title>ResNet-50 (untrained).</title>
<p>Similar to ResNet-50 pretrained on images, the untrained counterpart can effectively decode spatial knowledge related to location, heading direction and distance to borders. All layers decoder better than the baseline decoders which do not rely on visual signals of the environment.</p></caption>
<graphic xlink:href="575026v1_figs3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.4:</label>
<caption><title>ViT-B/16 (trained).</title>
<p>Linear decoders trained on different layers of the pretrained ViT model show very similar decoding performance. Overall the decoding performance is much better than the two baseline decoders which do not incorporate visual signals.</p></caption>
<graphic xlink:href="575026v1_figs4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.5:</label>
<caption><title>ViT-B/16 (untrained).</title>
<p>Linear decoders trained on the untrained ViT model also achieve accurate decoding performance on location, heading direction and distance to the nearest border. Similar to the trained version, all layers considered in our analysis achieve comparable performance and outperform the baselines.</p></caption>
<graphic xlink:href="575026v1_figs5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.6:</label>
<caption><title>Distribution of different spatial unit types across layers of perceptual models of object recognition.</title></caption>
<graphic xlink:href="575026v1_figs6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs7" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.7:</label>
<caption><title>Examples of model units exhibiting spatial characteristics.</title></caption>
<graphic xlink:href="575026v1_figs7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs8" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.8:</label>
<caption><title>Excluding units with the strongest spatial profiles had minimal impact on spatial knowledge (ResNet-50).</title></caption>
<graphic xlink:href="575026v1_figs8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs9" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.9:</label>
<caption><title>Excluding units by task-relevance affects spatial decoding performance (ResNet-50).</title></caption>
<graphic xlink:href="575026v1_figs9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs10" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.10:</label>
<caption><title>Excluding units with the strongest spatial profiles had minimal impact on spatial knowledge (ViT-B/16).</title></caption>
<graphic xlink:href="575026v1_figs10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figs11" position="float" orientation="portrait" fig-type="figure">
<label>Figure S.11:</label>
<caption><title>Excluding units by task-relevance affects spatial decoding performance (ViT-B/16).</title></caption>
<graphic xlink:href="575026v1_figs11.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study uses a deep neural network approach to challenge the role of spatially selective neurons like place, head or border cells for position decoding. The findings are <bold>important</bold> as they suggest that such functional cell types may emerge naturally from object recognition in complex visual environments, but are neither necessary, nor particularly critical for position decoding. However, direct evidence supporting this conclusion remains <bold>incomplete</bold>.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
This study investigated spatial representations in deep feedforward neural network models (DDNs) that were often used in visual tasks. The authors create a three-dimensional virtual environment, and let a simulated agent randomly forage in a smaller two-dimensional square area. The agent &quot;sees&quot; images of the room within its field of view from different locations and heading directions. These images were processed by DDNs. Analyzing model neurons in DDNs, they found response properties similar to those of place cells, border cells and head direction cells in various layers of deep nets. A linear readout of network activity can recover key spatial variables. In addition, after removing neurons with strong place/border/head direction selectivity, one can still decode these spatial variables from the remaining neurons in the DNNs. Based on these results, the authors argue that that the notion of functional cell types in spatial cognition is misleading.</p>
<p>Strengths:</p>
<p>
This paper contains interesting and original ideas, and I enjoy reading it. Most previous studies (e.g., Banino, Nature, 2018; Cueva &amp; Wei, ICLR, 2018; Whittington et al, Cell, 2020) using deep network models to investigate spatial cognition mainly relied on velocity/head rotation inputs, rather than vision (but see Franzius, Sprekeler, Wiskott, PLoS Computational Biology, 2007). Here, the authors find that, under certain settings, visual inputs alone may contain enough information about the agent's location, head direction and distance to the boundary, and such information can be extracted by DNNs. If confirmed, this is potentially an interesting and important observation.</p>
<p>Weaknesses:</p>
<p>
While the findings reported here are interesting, it is unclear whether they are the consequence of the specific model setting, and how well they would generalize. Furthermore, I feel the results are over-interpreted. There are major gaps between the results actually shown and the claim about the &quot;superfluousness of cell types in spatial cognition&quot;. Evidence directly supporting the overall conclusion seems to be weak at the moment.</p>
<p>Major concerns:</p>
<p>(1) The authors reported that, in their model setting, most neurons throughout the different layers of CNNs show strong spatial selectivity. This is interesting and perhaps also surprising. It would be useful to test/assess this prediction directly based on existing experimental results. It is possible that the particular 2-d virtual environment used is special. The results will be strengthened if similar results hold for other testing environments.</p>
<p>In particular, examining the pictures shown in Fig. 1A, it seems that local walls of the 'box' contain strong oriented features that are distinct across different views. Perhaps the response of oriented visual filters can leverage these features to uniquely determine the spatial variable. This is concerning because this is a very specific setting that is unlikely to generalize.</p>
<p>(2) Previous experimental results suggest that various function cell types discovered in rodent navigation circuits persist in dark environments. If we take the modeling framework presented in this paper literally, the prediction would be that place cells/head direction cells should go away in darkness. This implies that key aspects of functional cell types in the spatial cognition are missing in the current modeling framework. This limitation needs to be addressed or explicitly discussed.</p>
<p>(3) Place cells/border cell/ head direction cells are mostly studied in the rodent's brain. For rodents, it is not clear whether standard DNNs would be good models of their visual systems. It is likely that rodent visual system would not be as powerful in processing visual inputs as the DNNs used in this study.</p>
<p>(4) The overall claim that those functional cell types defined in spatial cognition are superfluousness seems to be too strong based on the results reported here. The paper only studied a particular class of models, and arguably, the properties of these models have a major gap to those of real brains. Even though, in the DNN models simulated in this particular virtual environment, (i) most model neurons have strong spatial selectivity; (ii) removing model neurons with the strongest spatial selectivity still retain substantial spatial information, why this is relevant to the brain? The neural circuits may operate in a very different regime. Perhaps a more reasonable interpretation of the results would be: these results raise the possibility that those strongly selective neurons observed in the brain may not be essential for encoding certain features, as something like this is observed in certain models. It is difficult to draw definitive conclusions about the brain based on the results reported.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors aim at challenging the relevance of cell populations with characteristic selectivity for specific aspects of navigation (e.g. place cells, head direction and border cells) in the processing of spatial information. Their claim is that such cells naturally emerge in any system dealing with the estimation of position in an environment, without the need for a special involvement of these cells in the computations. In particular the work shows how when provided with spatial error signals, networks designed for invariant object recognition spontaneously organize the activity in their hidden layers into a mixture of spatially selective cells, some of them passing classification criteria for place, head direction or border cells. Crucially, these cells are not necessary for position decoding, nor are they the most informative when it comes to the performance of the network in reconstructing spatial position from visual scenes. These results lead the authors to claim that focusing on the classification of specific cell types is hindering rather than helping advancement in the understanding of spatial cognition. In fact they claim that the attention should rather be pointed at understanding highly-dimensional population coding, regardless of its direct interpretability or its appeal to human observers.</p>
<p>Strengths:</p>
<p>
Methodologically the paper is consistent and convincingly support the author claims regarding the role of cell types in coding for spatial aspects of cognition. It is also interesting how the authors leverage on established machine learning systems to provide a sort of counter-argument to the use of such techniques to establish a parallel between artificial and biological neural representations. In the recent past similar applications of artificial neural networks to spatial navigation have been directed at proving the importance of specific neural substrates (take for example Banino et al. 2018 for grid cells), while in this case the same procedure is used to unveil them as epiphenomena, so general and unspecific to be of very limited use in understanding the actual functioning of the neural system. I am quite confident that this stance regarding the role of place cells and co. could gather large sympathy and support in the greater part of the neuroscience community, or at least among the majority of theoretical neuroscientists with some interest in the hippocampus and higher cognition.</p>
<p>Weaknesses:</p>
<p>
My criticism of the paper can be articulated in three main points:</p>
<p>
- What about grid cells? Grid cells are notably not showing up in the analyses of the paper. But they surely can be considered as the 'mother' of all tailored spatial cells of the hippocampal formation. Are they falling outside the author's assessment of the importance of this kind of cells? Some discussion of the place grid cells occupy in the vision of the authors would greatly help.</p>
<p>
- The network used in the paper is still guided by a spatial error signal, and the network is trained to minimize spatial decoding error. In a sense, although object classfication networks are not designed for spatial navigation, one could say that the authors are in some way hacking this architecture and turning it into a spatial navigation one through learning. I wonder if their case could be strengthened by devising a version of their experiment based on some form of self-supervised or unsupervised learning.</p>
<p>
- The last point is more about my perception of the community studying hippocampal functions, rather than being directed at the merits of the paper itself. My question is whether the paper is fighting an already won battle. That is whether the focus on the minute classification of response profiles of cells in the hippocampus is in fact already considered an 'old' approach, very useful for some initial qualitative assessments but of limited power when asked to provide deeper insight into the functioning of hippocampal computations (or computations of any other brain circuit).</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
In this paper, the authors demonstrate the inevitably of the emergence of some degree of spatial information in sufficiently complex systems, even those that are only trained on object recognition (i.e. not &quot;spatial&quot; systems). As such, they present an important null hypothesis that should be taken into consideration for experimental design and data analysis of spatial tuning and its relevance for behavior.</p>
<p>Strengths:</p>
<p>
The paper's strengths include the use of a large multi-layer network trained in a detailed visual environment. This illustrates an important message for the field: that spatial tuning can be a result of sensory processing. While this is a historically recognized and often-studied fact in experimental neuroscience, it is made more concrete with the use of a complex sensory network. Indeed, the manuscript is a cautionary tale for experimentalists and computational researchers alike against blindly applying and interpreting metrics without adequate controls.</p>
<p>Weaknesses:</p>
<p>
However, the work has a number of significant weaknesses. Most notably: the degree and quality of spatial tuning is not analyzed to the standards of evidence historically used in studies of spatial tuning in the brain, and the authors do not critically engage with past work that studies the sensory influences of these cells; there are significant issues in the authors' interpretation of their results and its impact on neuroscientific research; the ability to linearly decode position from a large number of units is not a strong test of spatial information, nor is it a measure of spatial cognition; and the authors make strong but unjustified claims as to the implications of their results in opposition to, as opposed to contributing to, work being done in the field.</p>
<p>The first weakness is that the degree and quality of spatial tuning that emerges in the network is not analyzed to the standards of evidence that have been used in studies of spatial tuning in the brain. Specifically, the authors identify place cells, head direction cells, and border cells in their network and their conjunctive combinations. However, these forms of tuning are the most easily confounded by visual responses, and it's unclear if their results will extend to forms of spatial tuning that are not. Further, in each case, previous experimental work to further elucidate the influence of sensory information on these cells has not been acknowledged or engaged with.</p>
<p>For example, consider the head direction cells in Figure 3C. In addition to increased activity in some directions, these cells also have a high degree of spatial nonuniformity, suggesting they are responding to specific visual features of the environment. In contrast, the majority of HD cells in the brain are only very weakly spatially selective, if at all, once an animal's spatial occupancy is accounted for (Taube et al 1990, JNeurosci). While the preferred orientation of these cells are anchored to prominent visual cues, when they rotate with changing visual cues the entire head direction system rotates together (cells' relative orientation relationships are maintained, including those that encode directions facing AWAY from the moved cue), and thus these responses cannot be simply independent sensory-tuned cells responding to the sensory change) (Taube et al 1990 JNeurosci, Zugaro et al 2003 JNeurosci, Ajbi et al 2023).</p>
<p>As another example, the joint selectivity of detected border cells with head direction in Figure 3D suggests that they are &quot;view of a wall from a specific angle&quot; cells. In contrast, experimental work on border cells in the brain has demonstrated that these are robust to changes in the sensory input from the wall (e.g. van Wijngaarden et al 2020), or that many of them are not directionally selective (Solstad et al 2008).</p>
<p>The most convincing evidence of &quot;spurious&quot; spatial tuning would be the emergence of HD-independent place cells in the network, however, these cells are a small minority (in contrast to hippocampal data, Thompson and Best 1984 JNeurosci, Rich et al 2014 Science), the examples provided in Figure 3 are significantly more weakly tuned than those observed in the brain, and the metrics used by the authors to quantify place cell tuning are not clearly defined in the methods, but do not seem to be as stringent as those commonly used in real data. (e.g. spatial information, Skaggs et al 1992 NeurIPS).</p>
<p>Indeed, the vast majority of tuned cells in the network are conjunctively selective for HD (Figure 3A). While this conjunctive tuning has been reported, many units in the hippocampus/entorhinal system are *not* strongly hd selective (Muller et al 1994 JNeurosci, Sangoli et al 2006 Science, Carpenter et al 2023 bioRxiv). Further, many studies have been done to test and understand the nature of sensory influence (e.g. Acharya et al 2016 Cell), and they tend to have a complex relationship with a variety of sensory cues, which cannot readily be explained by straightforward sensory processing (rev: Poucet et al 2000 Rev Neurosci, Plitt and Giocomo 2021 Nat Neuro). E.g. while some place cells are sometimes reported to be directionally selective, this directional selectivity is dependent on behavioral context (Markus et al 1995, JNeurosci), and emerges over time with familiarity to the environment (Navratiloua et al 2012 Front. Neural Circuits). Thus, the question is not whether spatially tuned cells are influenced by sensory information, but whether feed-forward sensory processing alone is sufficient to account for their observed turning properties and responses to sensory manipulations.</p>
<p>These issues indicate a more significant underlying issue of scientific methodology relating to the interpretation of their result and its impact on neuroscientific research. Specifically, in order to make strong claims about experimental data, it is not enough to show that a control (i.e. a null hypothesis) exists, one needs to demonstrate that experimental observations are quantitatively no better than that control.</p>
<p>Where the authors state that &quot;In summary, complex networks that are not spatial systems, coupled with environmental input, appear sufficient to decode spatial information.&quot; what they have really shown is that it is possible to decode *some degree* of spatial information. This is a null hypothesis (that observations of spatial tuning do not reflect a &quot;spatial system&quot;), and the comparison must be made to experimental data to test if the so-called &quot;spatial&quot; networks in the brain have more cells with more reliable spatial info than a complex-visual control.</p>
<p>Further, the authors state that &quot;Consistent with our view, we found no clear relationship between cell type distribution and spatial information in each layer. This raises the possibility that &quot;spatial cells&quot; do not play a pivotal role in spatial tasks as is broadly assumed.&quot; Indeed, this would raise such a possibility, if 1) the observations of their network were indeed quantitatively similar to the brain, and 2) the presence of these cells in the brain were the only evidence for their role in spatial tasks. However, 1) the authors have not shown this result in neural data, they've only noticed it in a network and mentioned the POSSIBILITY of a similar thing in the brain, and 2) the &quot;assumption&quot; of the role of spatially tuned cells in spatial tasks is not just from the observation of a few spatially tuned cells. But from many other experiments including causal manipulations (e.g. Robinson et al 2020 Cell, DeLauilleon et al 2015 Nat Neuro), which the authors conveniently ignore. Thus, I do not find their argument, as strongly stated as it is, to be well-supported.</p>
<p>An additional weakness is that linear decoding of position is not a strong test, nor is it a measure of spatial cognition. The ability to decode position from a large number of weakly tuned cells is not surprising. However, based on this ability to decode, the authors claim that &quot;'spatial' cells do not play a privileged role in spatial cognition&quot;. To justify this claim, the authors would need to use the network to perform e.g. spatial navigation tasks, then investigate the network's ability to perform these tasks when tuned cells were lesioned.</p>
<p>Finally, I find a major weakness of the paper to be the framing of the results in opposition to, as opposed to contributing to, the study of spatially tuned cells. For example, the authors state that &quot;If a perception system devoid of a spatial component demonstrates classically spatially-tuned unit representations, such as place, head-direction, and border cells, can &quot;spatial cells&quot; truly be regarded as 'spatial'?&quot; Setting aside the issue of whether the perception system in question does indeed demonstrate spatially-tuned unit representations comparable to those in the brain, I ask &quot;Why not?&quot; This seems to be a semantic game of reading more into a name then is necessarily there. The names (place cells, grid cells, border cells, etc) describe an observation (that cells are observed to fire in certain areas of an animal's environment). They need not be a mechanistic claim (that space &quot;causes&quot; these cells to fire) or even, necessarily, a normative one (these cells are &quot;for&quot; spatial computation). This is evidenced by the fact that even within e.g. the place cell community, there is debate about these cells' mechanisms and function (eg memory, navigation, etc), or if they can even be said to serve only a single function. However, they are still referred to as place cells, not as a statement of their function but as a history-dependent label that refers to their observed correlates with experimental variables. Thus, the observation that spatially tuned cells are &quot;inevitable derivatives of any complex system&quot; is itself an interesting finding which *contributes to*, rather than contradicts, the study of these cells. It seems that the authors have a specific definition in mind when they say that a cell is &quot;truly&quot; &quot;spatial&quot; or that a biological or artificial neural network is a &quot;spatial system&quot;, but this definition is not stated, and it is not clear that the terminology used in the field presupposes their definition.</p>
<p>In sum, the authors have demonstrated the existence of a control/null hypothesis for observations of spatially-tuned cells. However, 1) It is not enough to show that a control (null hypothesis) exists, one needs to test if experimental observations are no better than control, in order to make strong claims about experimental data, 2) the authors do not acknowledge the work that has been done in many cases specifically to control for this null hypothesis in experimental work or to test the sensory influences on these cells, and 3) the authors do not rigorously test the degree or source of spatial tuning of their units.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99047.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Luo</surname>
<given-names>Xiaoliang</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5297-2114</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mok</surname>
<given-names>Robert M</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7261-9257</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Love</surname>
<given-names>Bradley C</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7883-7076</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the reviewers for their engagement and constructive comments. This provisional response aims to clarify key misconceptions, address major criticisms, and outline our revision plans.</p>
<p>A primary concern of the reviewers appears to be our model's limitations in addressing a broad range of empirical findings. This, however, misinterprets our core contribution. Our work centers on a cautionary tale that before advocating for newly discovered cell types and their purported special roles in spatial cognition—an approach prevalent in the field—such claims must be tested against alternative (null) hypotheses that may contradict intuitive expectations. We present such an alternative hypothesis regarding spatial cells and their assumed privileged roles. We show that key findings in the field - spatial “cell types”,  arise in a set of null models without spatial grounding (including untrained variants) despite the models not being a model for spatial processing, and we also found that they had no privileged role for representing spatial information.</p>
<p>Our proposal is not a new model attempting to explain the brain, and therefore we do not aim to capture every empirical finding. Indeed, we would not expect an object recognition model (and its untrained variant) with no explicit spatial grounding to account for all phenomena in spatial cognition. This underscores our key point: if there exists a basic, spatially agnostic model that can explain certain degrees of empirical findings using criteria from the literature (i.e. place, head-direction and border cells), what implications does this have for the more complex theories and models proposed as underlying mechanisms of special cell types?</p>
<p>Regarding concerns about the limited scope and generalizability of our setting, we will clarify that we considered multiple DNN architectures, both trained and untrained, on multiple decoding tasks (position, head direction, and nearest-wall distance). We plan to extend our experiments further as detailed in the revision plan below.</p>
<p>Further, there was a methodological concern about using a linear decoder on a fixed DNN for spatial decoding tasks being a form of &quot;hacking&quot;. However, linear readout is standard practice in neuroscience to characterize information available in a neural population. Moreover, our tests on untrained networks also showed spatial decoding capabilities, suggesting it's not solely due to the linear readout.</p>
<p>For our full revision plan:</p>
<p>(1) We will revise the manuscript to better reflect these above points, clarifying our paper's stance and improving the writing to reduce misconceptions.</p>
<p>(2) We will address individual public reviews in more detail.</p>
<p>(3) We intend to address key reviewer recommendations, focusing on better situating our work within the broader context of the existing literature whilst emphasizing the null hypothesis perspective.</p>
<p>(4) In general, we will consider additional aspects of the literature and conduct new experiments to strengthen the relevance of our work to existing work. We highlight a number of potential experiments which we believe can address reviewer concerns:</p>
<p>a. Blurring the visual inputs to DNNs to match rodent perception.</p>
<p>b. Vary environmental settings to verify whether our findings are more</p>
<p>generalizable (which we predict to be the case).</p>
<p>c. Vary the environment to assess remapping effects, which will strengthen the</p>
<p>connection of our work to the literature.</p>
</body>
</sub-article>
</article>