<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">98005</article-id>
<article-id pub-id-type="doi">10.7554/eLife.98005</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98005.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Epidemiology and Global Health</subject>
</subj-group>
</article-categories><title-group>
<article-title>Forecasting the spatial spread of an Ebola epidemic in real-time: comparing predictions of mathematical models and experts</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6206-7134</contrib-id>
<name>
<surname>Munday</surname>
<given-names>James D</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>james.munday@bsse.ethz.ch</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0737-5679</contrib-id>
<name>
<surname>Rosello</surname>
<given-names>Alicia</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9179-2917</contrib-id>
<name>
<surname>Edmunds</surname>
<given-names>W John</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n2">$</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2842-3406</contrib-id>
<name>
<surname>Funk</surname>
<given-names>Sebastian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n2">$</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a0jsq62</institution-id><institution>Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene and Tropical Medicine</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a0jsq62</institution-id><institution>Department of Infectious Disease Epidemiology, London School of Hygiene and Tropical Medicine</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a28rw58</institution-id><institution>Department of Biosystems Science and Engineering, ETH Zürich</institution></institution-wrap>, <city>Zürich</city>, <country country="CH">Switzerland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Cooper</surname>
<given-names>Ben S</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Walczak</surname>
<given-names>Aleksandra M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>CNRS</institution>
</institution-wrap>
<city>Paris</city>
<country country="FR">France</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally</p></fn>
<fn id="n2" fn-type="equal"><label>$</label><p>These authors contributed equally</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-08-06">
<day>06</day>
<month>08</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-07-30">
<day>30</day>
<month>07</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP98005</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-12">
<day>12</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-15">
<day>15</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.14.24304285"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-08-06">
<day>06</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98005.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.98005.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98005.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98005.1.sa0">Reviewer #2 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Munday et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Munday et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-98005-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Ebola virus disease outbreaks can often be controlled, but require rapid response efforts frequently with profound operational complexities. Mathematical models can be used to support response planning, but it is unclear if models improve the prior understanding of experts.</p>
<p>We performed repeated surveys of Ebola response experts during an outbreak. From each expert we elicited the probability of cases exceeding four thresholds between two and 20 cases in a set of small geographical areas in the following calendar month. We compared the predictive performance of these forecasts to those of two mathematical models with different spatial interaction components.</p>
<p>An ensemble combining the forecasts of all experts performed similarly to the two models. Experts showed stronger bias than models forecasting two-case threshold exceedance. Experts and models both performed better when predicting exceedance of higher thresholds. The models also tended to be better at risk-ranking areas than experts.</p>
<p>Our results support the use of models in outbreak contexts, offering a convenient and scalable route to a quantified situational awareness, which can provide confidence in or to call into question existing advice of experts. There could be value in combining expert opinion and modelled forecasts to support the response to future outbreaks.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Ebola</kwd>
<kwd>Forecast evaluation</kwd>
<kwd>Expert elicitation</kwd>
<kwd>Mathematical modelling</kwd>
<kwd>Outbreak response</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="disclosures">
<title>Author Declarations</title><p>I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.</p><p>Yes</p><p>The details of the IRB/oversight body that provided approval or exemption for the research described are given below:</p><p>LSHTM ethics approval was obtained for this study (reference: 17633). Signed informed consent was taken from experts willing to participate and their verbal consent was requested again at the beginning of each elicitation.</p><p>I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.</p><p>Yes</p><p>I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).</p><p>Yes</p><p>I have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.</p><p>Yes</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have revised the paper inline with reviewers recommendations via peer review at eLife. eLife require us to update the medRXiv preprint version too.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Background</title>
<p>Following the initial emergence in 1976 in Zaire (now the Democratic Republic of the Congo, DRC)[<xref ref-type="bibr" rid="c1">1</xref>], epidemics of Ebola Virus Disease (EVD) have occurred, on average, every 12 - 24 months[<xref ref-type="bibr" rid="c2">2</xref>]. EVD is a viral haemorrhagic fever first caused by the Ebola Zaire virus (EZV), with a case fatality rate of 25-90%[<xref ref-type="bibr" rid="c3">3</xref>]. A major outbreak in North-Eastern provinces of DRC between 2018-2020 resulted in over 3300 reported cases and over 2100 deaths [<xref ref-type="bibr" rid="c4">4</xref>](<xref rid="fig1" ref-type="fig">Figure 1</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>The extent of the 2018-2020 ebola outbreak in north-eastern DRC and areas included in our study.</title>
<p>A) Daily incidence in north-eastern DRC between August 2018 and March 2020. Grey points show days prior to the study period, coloured points show days within the study period (November 2019 - March 2020), hue indicates month. B) Shows the total number of cases of ebola recorded in each Health Zone. C) Number of cases in each month and health zone during the period covered by this study, health zones outlined in red show all health zones affected by the entire epidemic.</p></caption>
<graphic xlink:href="24304285v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Transmission of EZV occurs mainly through direct contact during the symptomatic phase of infection; therefore, isolation of infected individuals with strict infection control, contact tracing, and safe burials have been key to controlling past EVD outbreaks[<xref ref-type="bibr" rid="c5">5</xref>], although setting specific challenges can hamper containment efforts [<xref ref-type="bibr" rid="c6">6</xref>]. More recently, vaccination has also become a tool for outbreak control, with two vaccines now licensed for use[<xref ref-type="bibr" rid="c7">7</xref>][<xref ref-type="bibr" rid="c8">8</xref>].</p>
<p>EVD outbreaks typically occur in resource-poor settings where limited communication and poor accessibility make logistics of surveillance and vaccination campaigns challenging. Understanding the spatial risk of future spread is therefore useful to allow response teams to focus efforts on high-risk areas. Mathematical and statistical models have been used extensively to forecast the spread of infectious diseases, including EVD[<xref ref-type="bibr" rid="c9">9</xref>]. Such models rely on a combination of statistical inference based on epidemiological data and information about the mechanisms underlying the dynamics of infection. However, the dynamics of EVD are frequently governed by changing contextual factors which are challenging to forecast quantitatively. For example, violent conflicts or flooding can seriously hinder, interrupt, or even reverse the impact of containment efforts [<xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c10">10</xref>]. Moreover, changes in healthcare capacity and health seeking behaviour of patients can strengthen or weaken efforts to reduce transmission [<xref ref-type="bibr" rid="c11">11</xref>]. The timing and impact of these factors is notoriously difficult to predict using mathematical models.</p>
<p>Models are used by epidemic response experts to support decision making in the field. In addition to models, experts also make judgments as to the future spread of the virus based on their interpretation of the current status of the outbreak combined with their knowledge of other less tangible factors such as the geography, climate (eg. seasonal variation in accessibility of particular areas) and soft intelligence about the escalation of conflict in areas which may as a result, be harder to access by response teams. There are clear costs and benefits to human- made and modelled-based forecasts. Whereas models are objectively based on observations of the past outbreak dynamics and current case data, experts have additional knowledge of the complex factors surrounding the outbreak response. It is therefore difficult to assess the impact mathematical models have on decision making, how much modelled forecasts differ from those made by experts in the field and whether either modelled or human forecasts are systematically more accurate or useful. Moreover, the knowledge of experts in the field of EVD epidemiology, with a good understanding of the geographical area of study may provide an invaluable resource that is currently underused in forecasting.</p>
<p>Previous studies have aimed to establish the relative performance of humans and models in predicting infectious disease spread in human populations, particularly in the context of acute respiratory infections such as Influenza and SARS-CoV-2. Three studies have evaluated the predictions of humans against models, explicitly. The first of these evaluated short-term forecasts and season-wide predictions of reports of influenza-like-illness (ILI) in the United States of America (USA) [<xref ref-type="bibr" rid="c12">12</xref>] and two studies [<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c14">14</xref>] compared short-term forecasts of cases of and deaths from COVID-19, firstly in Germany and Poland and secondly in the United Kingdom (UK). All three studies found that humans tended to perform better than the mathematical and statistical models selected for comparison when predicting cases. However, the COVID- focussed studies found the human ensembles performed worse than the ensemble prediction of the models when predicting deaths - These results were maintained when only self-declared ‘experts’ were included in the forecasts. A number of other studies recorded expert predictions without comparison to mathematical models. A study conducted early in the COVID-19 pandemic [<xref ref-type="bibr" rid="c15">15</xref>] evaluated the relative ability of laypeople and experts to predict the course of the UK epidemic over the first calendar year. The study found that both experts and laypeople typically under-predicted the impact overall, however experts’ forecasts were more accurate and better calibrated than laypeople. A study of expert predictions in the United States of America [<xref ref-type="bibr" rid="c16">16</xref>] evaluated their weekly forecasts of case incidence and total deaths in the first year against a pooled ensemble of all predictions. The study found that the ensemble outperformed every expert individually over the period of the study. A similar study surveyed experts regarding the total number of cases and deaths from MPox in the USA during 2022 [<xref ref-type="bibr" rid="c17">17</xref>], however these predictions are yet to be evaluated. Overall, these studies provide evidence that human predictions can play a valuable role in epidemiological prediction, providing a comparator and complementary method to mathematical and statistical modelling.</p>
<p>In this paper we extend the use of expert forecasters to predict spatial risk of transmission in the context of a local outbreak. We made monthly forecasts of the geographic spread of Ebola Virus Disease (EVD) from November 2019 to March 2020 during the declining phase of the 2018-2020 outbreak in the Democratic Republic of the Congo (DRC) using both expert predictions collected through regular interviews and with two spatially explicit computational transmission models with different spatial interaction assumptions: a gravity model and an adjacency model (where transmission can only occur between contiguous regions), see the methods section for details. Alongside supporting situational awareness, these forecasts were motivated by an aim to inform site selection for a planned vaccine trial. The objective was to identify areas that had seen no cases yet and thus were not already being supported by vaccination and other interventions, but were at high risk of still becoming affected by the EVD outbreak, thus allowing estimation of efficacy [<xref ref-type="bibr" rid="c18">18</xref>]. Here we evaluate the performance of the forecasts and select ensembles of the methods in predicting continued transmission and flare- ups of EVD in health zones (HZs) close to the affected area. We further study variation in forecast quality against a selection of factors related to local demography, case history and forecast implementation.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Expert elicitation</title>
<p>Experts in EVD epidemiology with knowledge of the local geography speaking English or French were identified originally by convenience sampling. The pool of experts was then expanded through recommendation from the identified experts (snowball sampling). This approach was best suited to capture the expertise of individuals who were most often temporarily based in the field.</p>
<p>A pilot study was carried out in November 2019. Subsequently, monthly interviews were held over WhatsApp in December 2019, January 2020, February 2020, and March 2020. All interviews were scripted. The main biases of this type of study (availability bias, representativeness bias, overconfidence, motivational bias, anchoring on past estimates) were briefly discussed during the first interview.</p>
<p>Experts also were provided with an interactive map of the outbreak area and surrounding health zones (HZ) showing the number of total cases during the outbreak and during the two preceding weeks for reference (supplementary figure S1). HZ were numbered to facilitate communication with the experts.</p>
<p>Experts were asked to estimate the number of reported probable and confirmed cases they would expect per HZ during the following month using the online MATCH Uncertainty Elicitation Tool [<xref ref-type="bibr" rid="c19">19</xref>] (supplementary figure S1). Through this platform, the experts and the researcher (AR) interacted in real time. The “roulette” (chips and bins) method was used.</p>
<p>Experts were instructed to place a total of 20 chips over the available bins (0-1 cases, 2-4 cases, …, 48-50 cases). Therefore, each chip represented for the expert a 5% probability that the number of cases was in the bin where the chip was placed. This process aimed to capture the uncertainty surrounding the expert’s estimates.</p>
<p>The experts were asked to estimate the number of reported cases they would expect in the HZ where there had been 1 or more cases in the 2 preceding weeks, as well as Goma (<xref rid="fig2" ref-type="fig">Figure 2</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Health zones included in the model and Expert Elicitation survey.</title>
<p>A shows the provinces around the affected area, and included in the transmission model, the red box shows the area detailed in panel B. B shows the health zones included at least once in the Expert Elicitation survey we conducted.</p></caption>
<graphic xlink:href="24304285v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The experts were then asked to identify any additional HZ where they would predict 1 or more cases during the following month with &gt;5% probability, and to also estimate the number of reported cases they would expect in these HZ. In the pilot study, carried out in October 2019, experts were asked to forecast the number of cases they expected during November 2019 in 10 HZs: Beni, Goma, Kalunguta, Katwa, Lolwa, Mabalako, Mambasa, Mandima, Nyankunde, and Oicha.</p>
</sec>
<sec id="s2b" sec-type="ethics-statement">
<title>Ethics</title>
<p>LSHTM ethics approval was obtained for this study (reference: 17633). Signed informed consent was taken from experts willing to participate and their verbal consent was requested again at the beginning of each elicitation.</p>
</sec>
<sec id="s2c">
<title>Modelling framework</title>
<p>In parallel with the expert elicitation programme, we developed a modelling framework to forecast spatial risk of infection. In the framework, incidence of cases is forecast in each Health Zone based on historical case reports. The model was formed of two components, the autoregressive component, and the spatial component.
<disp-formula id="eqn1">
<graphic xlink:href="24304285v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The auto-regressive component modelled the rate of infections in a particular health zone i, on day t, to be proportional to the number of cases in the same health zone (i) between dates t- (D+L) and t-D, where L is the estimated latent period and D is the estimated infectious period. The spatial component accounts for transmission between health zones, where rate of infection was proportional to the cases in each other Health Zone (i.e. ∀j j≠i) and moderated by a pairwise specific factor defined by a spatial kernel <italic>w<sub>ij</sub></italic> We used two spatial kernels, both of which use proximity of health zones to each other and their respective population size, <italic>P<sub>i</sub></italic> and <italic>P<sub>j</sub></italic>. Firstly, the gravity model which treats interaction in an analogous way to Newtonian gravity with population size in place of mass, such that interaction reduces distance, <italic>d</italic>, raised to a power, <italic>k</italic>.
<disp-formula id="eqn2">
<graphic xlink:href="24304285v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Secondly, we applied a model with adjacency-based interaction. In this model only adjacent HZs can interact. The strength of interaction between HZs is proportional to the product of their population sizes.
<disp-formula id="eqn3">
<graphic xlink:href="24304285v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<disp-formula id="eqn4">
<graphic xlink:href="24304285v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<p>Cases were modelled as Poisson distributed such that:
<disp-formula id="eqn5">
<graphic xlink:href="24304285v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
To forecast cases, we fitted the spatiotemporal model to historical data from the 60 days prior to the date the forecast was made, accounting for cases in health zones in seven regions (169 HZs) centred on the location of the epidemic; Nord-Kivu, Ituri, Tshopo, Maniema, Sud-Kivu, Haut-Uele, Bas-Uele. We fit the model using the No U-Turn Sampling (NUTS) method for Hamiltonian Monte Carlo with Stan [<xref ref-type="bibr" rid="c20">20</xref>], a probabilistic programming framework. We estimated <italic>α</italic> and <italic>γ</italic>, which vary the contribution of within-health-zone and between-health- zone transmission. We also estimated k, which determines how rapidly transmission rate decays with distance in the spatial component of the model. We sampled parameters from the resultant joint posterior distribution to simulate daily incidence in all HZs in the seven regions, up to and including the last day of the following month. We performed 1000 iterations for each forecast date. We then extracted the full distribution of the number of cases incident within the calendar month of interest. Forecasts were made using data up to the last day of the month prior to the forecast period.</p>
</sec>
<sec id="s2d">
<title>Enslemble forecasts</title>
<p>Ensemble forecasts were calculated as an average of the probabilities attributed by the members of the ensemble. For the expert ensemble the arithmetic mean was calculated across all experts with equal weighting. Similarly the model ensemble used the unweighted mean of the model forecasts. For the mixed (model and expert) ensemble, the mean was weighted such that the combined weight of the experts forecasts and the combined weight of the models forecasts were equal.</p>
</sec>
<sec id="s2e">
<title>Quantification of risk and forecast evaluation</title>
<p>To compare the model and the expert forecasts and score them according to the eventual true number of cases we calculated the probability attributed to cases over four thresholds, &gt;=2, &gt;=6, &gt;=10 and &gt;=20 cases.</p>
<p>We evaluated the forecasts using the Brier Score, a proper scoring rule which quantifies how accurate a forecast or a group of forecasts are when compared to true data after the event. The Brier score, BS, is defined as the square of the difference between the probability of observing an event and the observation <italic>o<sub>i</sub></italic> status, which takes a value 1 or 0 for cases observed and none observed respectively. We calculated this for multiple (N) forecasts by taking the mean of the individual forecast scores.
<disp-formula id="eqn6">
<graphic xlink:href="24304285v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
We also quantified the general bias and calibration of the forecasts by considering the hazard rate predicted by each forecast, which we calculated as the sum of probabilities attributed to exceeding each threshold. This gives the number of HZs the forecast ‘expected’ to cross the threshold in each month. To quantify the bias of each set of predictions, we took the difference between the hazard rate and the actual number of HZs that exceeded each threshold in each month. We refer to this as the <italic>hazard gap</italic> (HG).
<disp-formula id="eqn7">
<graphic xlink:href="24304285v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where <italic>p</italic>(<italic>c</italic>&gt;<italic>cthresh</italic>) isthe probability of threshold exceedance and <italic>o</italic>(<italic>c</italic>&gt;<italic>cthresh</italic>) is the observation status of the threshold exceedence 1 if true 0 if false.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Expert panel and health zones included in survey</title>
<p>Over the study period, we conducted a total of 40 interviews with 15 experts, three of which took place during the pilot phase (November 2019). <xref rid="fig3" ref-type="fig">Figure 3</xref> shows the timeline of the expert elicitations.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Timeline of the expert elicitation.</title>
<p>Each point shows the date of the interview of the expert labelled to obtain forecasts for the following month. Colour indicates the month for which the forecast was made, the forecast windows are highlighted with a shaded band of the same colour.</p></caption>
<graphic xlink:href="24304285v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Eight experts worked at the World Health Organization, four at the London School of Hygiene &amp; Tropical Medicine, two for Médecins Sans Frontières, and one at the DRC Ministry of Health.</p>
<p>Most experts (10/15) had more than five years of experience working in infectious disease epidemiology. About half of the interviews (21 of 40) were conducted with experts that were in the outbreak area (defined HZs affected by EVD or Goma, the site of the international response base) or had been there within 2 weeks of the interview. Four experts had never been in the outbreak area.</p>
<p>Between December 2024 and March 2025, eight to ten experts provided monthly forecasts for 4 to 11 health zones (HZs), estimating the probability that reported cases would exceed various thresholds (2, 6, and 20 cases). Expert predictions showed variable accuracy across months and zones.</p>
<p>Experts correctly identified Mabalako as the highest-risk HZ in December. They attributed an average 82% probability of exceeding 2 cases; Mabalako reported 38 cases that month, exceeding all thresholds, although the probability assigned to exceeding the higher thresholds was similar to that of Beni (3 cases). Other zones with moderate cases—Kalanguta (5), and Mambasa (4)—were assigned lower probabilities and generally did not exceed higher case thresholds. Interestingly, Mandima, which had no reported cases, was assigned a relatively high probability (72%) of exceeding 2 cases, indicating some overestimation in this zone.</p>
<p>In January, Beni and Mabalako again accounted for most cases (22 and 11, respectively) and were recognized as high risk by the experts. However, experts underestimated the scale of the outbreak in Beni, assigning 0% probability to exceeding 20 cases there, despite this threshold being surpassed. Mandima continued to be forecasted as high risk, though no cases were reported. Predictions for other zones such as Oicha and Biena suggested moderate risk, but no cases were confirmed.</p>
<p>Of the 11 nominated HZs, only Beni reported confirmed cases in February (9), exceeding the 6 case threshold. Experts collectively assigned a 70% probability to this event. Similar probabilities assigned to Mabalako (60%), where no cases were reported, following cases in the prior two months. Several other zones were predicted to exceed 2 cases, yet reported none.</p>
<p>No cases were reported in any health zone during March. Experts broadly anticipated this, with only one expert assigning over 50% probability of exceeding 2 cases in any HZ. Beni had the highest average assigned risk at 33%.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Expert elicitation results and accuracy of predictions.</title>
<p>Only the HZs that were rated by all experts are included here. Results are shown as probabilities (vertical axes) that a given health zone (horizontal panels) exceeds a given threshold (horizontal axes) according to the experts (box plots) or models (square / diamond for gravity and adjacency models, respectively) across different months (vertical panels). Health zone / month combinations where the given thresholds were exceeded are marked in cyan, and ones where they weren’t in red.</p></caption>
<graphic xlink:href="24304285v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3b">
<title>Performance evaluation</title>
<p>We evaluated the forecasts using the Brier score. The overall scores of individual experts varied between 0 and 0.6 across the four thresholds. Collectively, the experts scored best at the highest threshold (20 cases) and worst for forecasts of the lowest threshold (2 cases). The models also performed better at higher thresholds than low thresholds, but the difference was less pronounced. Overall, the gravity model ranked best amongst all forecasts at the 2 case threshold. It also ranked best for this threshold in the month of February and consistently in the top half of forecasts in December and January, however performed comparatively poorly in March, ranking higher than only one of the experts. The adjacency model also performed better than the experts overall for the 2 case threshold. Related to this, including the models improved the ensemble forecast. Although the gravity model performed better than the adjacency model for higher thresholds, together the models performed similarly to the expert ensemble forecast overall. In January and February the gravity model performed well compared to the adjacency model and the expert ensemble, however in March both models performed particularly badly compared to the experts for all thresholds. None of the experts performed consistently well relative to the others, experts 3 and 10 performed best for the 2 case threshold, whereas experts 13 and 14 did best for higher thresholds.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Evaluation of forecasts made by the experts, models, and ensembles.</title>
<p>A shows the overall Brier Score for each expert, model and ensemble (calculated over all forecasts included in the study). In B each panel shows the Brier score across all health zones for each month (vertical) at each case threshold (Horizontal). Coloured points show each expert score, the violin plot shows their distribution. The grey hollow points show the model scores, the yellow points show the ensemble scores (circles show experts alone, squares show models alone and triangles show experts and models with 50% weight given to each). C Shows the ranking of each expert and model in terms of forecast performance</p></caption>
<graphic xlink:href="24304285v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To evaluate how the different forecasts may impact decision making we ranked the health zones for each month, based on the probability of exceeding each threshold of cases forecast by each ensemble and by the model alone (supplementary figure S6). In general, the model and the ensembles all ranked health zones that did reach the threshold highly. In some cases the model performed better, ranking health zones that did meet the threshold higher than the experts, specifically ranking Beni Higher than Mandima in higher thresholds (&gt;=6 and 10 cases) for the forecast of January, where Beni ultimately had cases and Mandima did not, in that month. Considering the models separately, the gravity model performed better than the adjacency model in general, with the adjacency model occasionally performing worse than the experts when ranking the HZs. This was clearest in the forecasts of November and January.</p>
</sec>
<sec id="s3c">
<title>Bias and calibration in forecasts</title>
<p>We evaluated the bias in each forecast type by considering the hazard gap between forecasts and actual cases. We found that experts systematically forecasted higher risk of the lowest threshold (&gt;=2 cases) than was warranted, but tended to forecast lower risk of exceeding the highest threshold (&gt;=20 cases) than was borne out across all HZs (<xref rid="fig6" ref-type="fig">Figure 6</xref>). When calculated across all months, this bias was present in 12 of the 15 experts. The models did not show clear consistent bias in either direction.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6</label>
<caption><title>Bias and calibration of forecasts.</title>
<p>Panels show the Hazards gap difference between the Hazard rate (expected number of exceedances across all health zones) for each threshold attributed by the forecast and the actual number of Health Zones that exceeded the associated threshold. Each panel shows one forecast (expert or model) in each month. The bottom row shows the same for each forecast calculated over the entire study period.</p></caption>
<graphic xlink:href="24304285v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3d">
<title>Forecasting flare-ups</title>
<p>In addition to the health zones presented to all experts, each expert was able to nominate health zones, which they deemed at risk. Experts nominated seven further HZs to forecast in December, four in January, four in February and one in March (<xref rid="tbl1" ref-type="table">Table 1</xref>).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Experts and health zones included in each round of the survey. The left part of the table details the experts interviewed (highlighted in green) the health zones included in the main survey in each month. In addition, the right part of the table details the health zones nominated by experts and the number of experts that nominated each one.</title></caption>
<graphic xlink:href="24304285v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>The only two HZs not included in the default list but exceeding 2 cases were Butembo and Katwa. These were nominated by 2 of the 10 experts (4 and 10) both attributing 50% chance of exceeding 2 cases. In contrast, Oicha and Komanda had the most nominations with 7 and 5 each, and 6 and 4 of the 10 experts interviewed allotted greater than 5% chance of 2 or more cases in December, with attributed probabilities ranging between 5% and 95% chance of crossing the 2 case threshold.</p>
<p>In January, six of the eight experts interviewed nominated Butembo (1,2,4,5,9, and 11 with probabilities of 5% to 85% of crossing the 2 case threshold) while three of them also nominated Katwa (2 and 9 giving 85% and 5 giving 5%). Kalunguta and Manguredjipa were also nominated by one expert each, 5 with 10% and 11 with 50% respectively. None of the nominated health zones crossed the threshold in January.</p>
<p>In February six of the ten experts interviewed (2, 4, 5, 11, 12 and 14) nominated Oicha with probabilities between 10% and 95% of crossing the 2 case threshold. Four (2, 4, 5 and 11) nominated Biena with probabilities between 10% and 95% of exceedance. Experts 4 and 8 also nominated Vuhovi attributing 55% and 20% probability of threshold exceedance respectively. Expert 3 nominated Lolwa alone but gave no probability of exceeding 1 case. No HZs not included in the interview as default crossed the 2 case threshold in February.</p>
<p>In March three (4, 8 and 11) of the eight experts interviewed gave probabilities of 35%, 50% and 15% of exceeding the 2-case threshold respectively. No HZs not included in the interview as default crossed the 2 case threshold in March.</p>
<p>To compare the model with the experts we included all HZs modelled and attributed all HZs not nominated by experts an exceedance probability of 0%. To allow comparison, we also set all HZs given a probability of lower than 5% to 0% for both the gravity and adjacency models.</p>
<p>When considering the Brier Score (<xref rid="fig7" ref-type="fig">Figure 7</xref>), we found that the gravity model performed comparably to some experts when forecasting for December, and February. The adjacency model performed worse than all the experts in every month except February. In every month the ensemble of experts did better than the models and including the models in the ensembles reduced performance.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7</label>
<caption><title>Evaluation of forecasts made in health zones not included in the main survey.</title>
<p>Each panel (right to left) shows the Brier score across all health zones for each month. Coloured points show each expert score, density plot shows their overall distribution. The red points show the model scores, the yellow points show the ensemble scores (squares show experts alone, crosses show experts and models with 50% weight given to each).</p></caption>
<graphic xlink:href="24304285v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>We compared forecasts of the geographic spread of Ebola made by experts, with those made using a modelling framework. Since the outbreak dynamics of Ebola are highly sensitive to the changeable context in which they take place, mathematical models and expert opinions are expected to have different strengths and weaknesses, with models benefiting from objective inference from previous observations and experts able to utilise detailed knowledge about the outbreak and the changing surrounding context to make informed projections of risk. By interviewing experts and asking them to forecast risk in a structured way, we were able to compare the performance of their forecasts against those made with well-established modelling approaches in a quantifiable and robust way.</p>
<p>Overall, the forecasts made by the group of experts as a whole performed similarly to those of the model, with a few consistent exceptions. The model performed better than the experts when considering the lowest threshold in four of the five months covered by the survey, but performance was more comparable for the higher thresholds. The model also performed marginally better when ranking health zones by risk of ongoing transmission, indicating that use of a model may improve prioritisation of health zones when attributing resources.</p>
<p>We found that both methods performed better when considering higher case thresholds. This is likely to be due to a combination of bias in both forecast types towards under prediction of cases and the fact that there were few instances where the higher thresholds were reached.</p>
<p>Although individual experts frequently out-performed ensembles in individual instances, no individual expert outperformed the ensembles overall. This supports the practice of considering predictions from a range of experts over a smaller number of more specialist or experienced experts. The models tended to perform similarly to the ensemble representing more consistent performance across all forecasts.</p>
<p>Experts tended to be more biased than models, especially at low case thresholds with a tendency to over-predict cases to a greater degree than models. This bias reduced rapidly as the case threshold increased. This may be interpreted as over cautiousness from the experts regarding potential for geographic spread of the virus but confidence that transmission could be contained quickly. This trend reflects a pattern amongst previous introductions into new health zones earlier in the pandemic, where a small number of cases were reported, but the local outbreak was quickly stopped (<xref rid="fig1" ref-type="fig">Figure 1</xref>).</p>
<p>To our knowledge, our study is the first to record experts’ assessment of geographical risk at a local level during an epidemic and the first comparison of outbreak response experts’ predictions to those of models in real-time. Although direct comparison is not possible, our results lead to conclusions that are broadly similar to those from previous studies [<xref ref-type="bibr" rid="c12">12</xref>–<xref ref-type="bibr" rid="c14">14</xref>], however each of these studies found that ensemble expert forecasts performed better than the comparison models, whereas our study found no clear performance difference. This may suggest that experts are better at predicting simple time series than geographic distribution of cases. However, we cannot view these findings independently from the different survey designs or study contexts.</p>
<p>There are a number of important limitations to consider when interpreting our results. The context within which we conducted the study has important implications for interpretation. Due to the timing and logistics of setting up the questionnaire, the study only began in the closing phase of the epidemic, whereas the relative performance of experts and models may differ during different phases of the epidemic. For example, in the early phase where dynamics are driven more by infectious transmission than established response practices, or during the peak where changes in intervention strategy may be more influential. The stage of the epidemic also meant that there was a substantial trend towards ‘negative’ results (i.e. no threshold exceedance), which is likely to favour some forecasting methods over others.</p>
<p>Additionally, experts were not all interviewed on the same day and interviews occurred several days before the beginning of the month they were forecasting. In some cases experts were interviewed up to 2 weeks prior to the beginning of the month. This means that the information available differed both between experts and with the model, which was run considering data up to the first day of each month. This reduces our ability to compare models to experts directly, however, it could also be argued that this is a ‘built-in’ factor which represents the inherent challenge of eliciting predictions from experts. In addition, the interview process for experts was quite taxing and required a phone call - which can cause scheduling challenges during an epidemic response. It may be that other methods, with less arduous and more flexible data entry would improve responses.</p>
<p>Our analysis represents the comparison of expert forecasts to only two specific forecasting models. There are a great range of models that could have been applied in this context which may have differed in performance to those we used. We chose these models for convenience since we were applying them to the outbreak at the time of the interviews. It is also possible that some of the experts involved in the study had ingested results from our model, which were available through our online dashboard, or other models being used at the time.</p>
<p>Since our findings, like those of similar studies, suggest that models and experts perform comparably in this context, there is an argument that models have no value in informing expert decision making. It can be argued, however, that models remain useful in outbreak response.</p>
<p>Firstly, while the models performed similarly to the ensemble forecasts of the experts, there was no individual expert that performed consistently better than the models. Secondly, models are much more easily scaled and generalised making them simple to deploy in new contexts and to adapt as epidemics grow. Expert interviews are time-consuming and often inconvenient, especially in the context of outbreak response activities, which are characteristically fast paced. Models therefore offer a more convenient route to a quantified insight, which from our results, performs comparably to the way groups of experts may think. Finally, there are ways to combine both methods. For example, in the event that expert forecasts can be garnered, joint ensembles can capture information from both the expert and modelled forecasts. Further, we suggest that models can offer a role in aiding decision making by providing confidence in or calling into question expert advice that is being considered.</p>
</sec>
<sec id="s5">
<title>Conclusions</title>
<p>Our analysis evaluated performance of experts and models when forecasting the spatial spread of Ebola, representing the first such study incorporating local geographic distribution and the first to focus on an epidemic in a resource poor setting. We found that forecasts made by experts and models performed comparably overall, but experts tended to be slightly more biased towards predicting that a small number of cases would persist. The results support the use of models in outbreak response and provide insight into how models and expert opinion could be combined when tackling future epidemics.</p>
</sec>

</body>
<back>
<sec id="s7" sec-type="data-availability">
<title>Code and data availability</title>
<p>All data and code used to process the expert interview responses can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/epiforecasts/Ebola-Expert-Interviews">https://github.com/epiforecasts/Ebola-Expert-Interviews</ext-link>. The forecasts were performed using the EpiCastR package <ext-link ext-link-type="uri" xlink:href="https://github.com/epiforecasts/EpiCastR">https://github.com/epiforecasts/EpiCastR</ext-link>. The code used for the analysis and scoring of the forecasts can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/epiforecasts/Ebola-Expert-Ellicitation">https://github.com/epiforecasts/Ebola-Expert-Ellicitation</ext-link></p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We would like to thank Xavier de Radiguès, Neale Batra, Nabil Tabbal, Mathias Mossoko, Chris Jarvis, Thibaut Jombart, Denis Ardiet, Michel Van Herp, Silimane Ngoma, Olivier le Polain, Esther van Kleef, Noé Guinko, and Amy Gimma as well as 2 other experts who preferred to remain anonymous, for their participation as experts in this study. We would also like to thank David Smith, Thibaut Jombart, Chris Jarvis, Flavio Finger, and Anton Camacho for their helpful advice in conducting this survey.</p>
</ack>
<sec id="d1e876" sec-type="additional-information">
<title>Additional information</title>
<sec id="s8">
<title>Author contributions</title>
<p>AR and WJE conceived and designed the interviews. JDM and SF conceived and designed the modelling framework and the evaluation of forecasts. AR conducted the expert interviews and prepared the interview data for comparison. JDM implemented the model and performed the formal forecast evaluations. JDM, AR, WJE and SF interpreted the results. JDM and AR wrote the manuscript. JDM, AR, WJE and SF edited the manuscript.</p>
</sec>
<sec id="s9">
<title>Funding statement</title>
<p>This study was partly funded by the Department of Health and Social Care using UK Aid funding and is managed by the National Institute for Health and Care Research (VEEPED: PR-OD-1017- 20002; AR and WJE). This study was partly funded by the Wellcome Trust (210758/Z/18/Z : JDM and SF). The views expressed in this publication are those of the authors and not necessarily those of the funders.</p>
</sec>
</sec>
<sec id="suppd1e876" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="d1e867">
<label>Supplementary Materials</label>
<media xlink:href="supplements/304285_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Report of an International Commission</collab></person-group>. <year>1978</year> <article-title>Ebola haemorrhagic fever in Zaire, 1976</article-title>. <source>Bull. World Health Organ</source>. <volume>56</volume>, <fpage>271</fpage>–<lpage>293</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosello</surname> <given-names>A</given-names></string-name> <etal>et al.</etal></person-group> <year>2015</year> <article-title>Ebola virus disease in the Democratic Republic of the Congo, 1976-2014</article-title>. <source>eLife</source> <volume>4</volume>. (doi:<pub-id pub-id-type="doi">10.7554/eLife.09015</pub-id>)</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>World Health Organization</collab></person-group><source>Ebola virus disease</source>. <ext-link ext-link-type="uri" xlink:href="https://www.who.int/news-room/fact-sheets/detail/ebola-virus-disease">https://www.who.int/news-room/fact-sheets/detail/ebola-virus-disease</ext-link> <date-in-citation content-type="access-date">19 December 2023</date-in-citation>. <year>no date</year></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="report"><person-group person-group-type="author"><collab>WHO Regional Office for Africa</collab></person-group>. <year>2020</year> <source>Outbreaks and Emergencies Bulletin, Week 5: 27 January - 02 February 2020</source>. <publisher-name>WHO </publisher-name>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kucharski</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Camacho</surname> <given-names>A</given-names></string-name>, <string-name><surname>Flasche</surname> <given-names>S</given-names></string-name>, <string-name><surname>Glover</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Edmunds</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Funk</surname> <given-names>S</given-names></string-name></person-group>. <year>2015</year> <article-title>Measuring the impact of Ebola control measures in Sierra Leone</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>. <volume>112</volume>, <fpage>14366</fpage>–<lpage>14371</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adongo</surname> <given-names>PB</given-names></string-name>, <string-name><surname>Tabong</surname> <given-names>PT-N</given-names></string-name>, <string-name><surname>Asampong</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ansong</surname> <given-names>J</given-names></string-name>, <string-name><surname>Robalo</surname> <given-names>M</given-names></string-name>, <string-name><surname>Adanu</surname> <given-names>RM</given-names></string-name></person-group>. <year>2016</year> <article-title>Preparing towards Preventing and Containing an Ebola Virus Disease Outbreak: What Socio-cultural Practices May Affect Containment Efforts in Ghana?</article-title> <source>PLoS Negl. Trop. Dis</source>. <volume>10</volume>, <fpage>e0004852</fpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><collab>World Health Organization</collab></person-group> <source>WHO prequalifies Ebola vaccine, paving the way for its use in high-risk countries</source>. <ext-link ext-link-type="uri" xlink:href="https://www.who.int/news/item/12-11-2019-who-prequalifies-ebola-vaccine-paving-the-way-for-its-use-in-high-risk-countries">https://www.who.int/news/item/12-11-2019-who-prequalifies-ebola-vaccine-paving-the-way-for-its-use-in-high-risk-countries</ext-link> <date-in-citation content-type="access-date">20 December 2023</date-in-citation>. <year>no date</year></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woolsey</surname> <given-names>C</given-names></string-name>, <string-name><surname>Geisbert</surname> <given-names>TW</given-names></string-name></person-group>. <year>2021</year> <article-title>Current state of Ebola virus vaccines: A snapshot</article-title>. <source>PLoS Pathog</source>. <volume>17</volume>, <fpage>e1010078</fpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chowell</surname> <given-names>G</given-names></string-name>, <string-name><surname>Viboud</surname> <given-names>C</given-names></string-name>, <string-name><surname>Simonsen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Merler</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vespignani</surname> <given-names>A</given-names></string-name></person-group>. <year>2017</year> <article-title>Perspectives on model forecasts of the 2014-2015 Ebola epidemic in West Africa: lessons and the way forward</article-title>. <source>BMC Med</source>. <volume>15</volume>, <fpage>42</fpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wannier</surname> <given-names>SR</given-names></string-name> <etal>et al.</etal></person-group> <year>2019</year> <article-title>Estimating the impact of violent events on transmission in Ebola virus disease outbreak, Democratic Republic of the Congo, 2018-2019</article-title>. <source>Epidemics</source> <volume>28</volume>, <fpage>100353</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Funk</surname> <given-names>S</given-names></string-name> <etal>et al.</etal></person-group> <year>2017</year> <article-title>The impact of control strategies and behavioural changes on the elimination of Ebola from Lofa County, Liberia</article-title>. <source>Philos. Trans. R. Soc. Lond. B Biol. Sci</source>. <volume>372</volume>. (doi:<pub-id pub-id-type="doi">10.1098/rstb.2016.0302</pub-id>)</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Farrow</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Brooks</surname> <given-names>LC</given-names></string-name>, <string-name><surname>Hyun</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tibshirani</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Burke</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Rosenfeld</surname> <given-names>R</given-names></string-name></person-group>. <year>2017</year> <article-title>A human judgment approach to epidemiological forecasting</article-title>. <source>PLoS Comput. Biol</source>. <volume>13</volume>, <fpage>e1005248</fpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bosse</surname> <given-names>NI</given-names></string-name> <etal>et al.</etal></person-group> <year>2022</year> <article-title>Comparing human and model-based forecasts of COVID-19 in Germany and Poland</article-title>. <source>PLoS Comput. Biol</source>. <volume>18</volume>, <fpage>e1010405</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bosse</surname> <given-names>NI</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bracher</surname> <given-names>J</given-names></string-name>, <string-name><surname>van Leeuwen</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cori</surname> <given-names>A</given-names></string-name>, <string-name><surname>Funk</surname> <given-names>S</given-names></string-name></person-group>. <year>2023</year> <article-title>Human judgement forecasting of COVID-19 in the UK</article-title>. <source>Wellcome Open Res</source>. <volume>8</volume>, <fpage>416</fpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Recchia</surname> <given-names>G</given-names></string-name>, <string-name><surname>Freeman</surname> <given-names>ALJ</given-names></string-name>, <string-name><surname>Spiegelhalter</surname> <given-names>D</given-names></string-name></person-group>. <year>2021</year> <article-title>How well did experts and laypeople forecast the size of the COVID-19 pandemic?</article-title> <source>PLoS One</source> <volume>16</volume>, <fpage>e0250935</fpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAndrew</surname> <given-names>T</given-names></string-name>, <string-name><surname>Reich</surname> <given-names>NG</given-names></string-name></person-group>. <year>2022</year> <article-title>An expert judgment model to predict early stages of the COVID-19 pandemic in the United States</article-title>. <source>PLoS Comput. Biol</source>. <volume>18</volume>, <fpage>e1010485</fpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McAndrew</surname> <given-names>T</given-names></string-name> <etal>et al.</etal></person-group> <year>2022</year> <article-title>Early human judgment forecasts of human monkeypox, May 2022</article-title>. <source>Lancet Digit Health</source> <volume>4</volume>, <fpage>e569</fpage>–<lpage>e571</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Watson-Jones</surname> <given-names>D</given-names></string-name> <etal>et al.</etal></person-group> <year>2022</year> <article-title>Protocol for a phase 3 trial to evaluate the effectiveness and safety of a heterologous, two-dose vaccine for Ebola virus disease in the Democratic Republic of the Congo</article-title>. <source>BMJ Open</source> <volume>12</volume>, <fpage>e055596</fpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morris</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Oakley</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Crowe</surname> <given-names>JA</given-names></string-name> </person-group><year>2014</year> <article-title>A web-based tool for eliciting probability distributions from experts</article-title>. <source>Environmental Modelling &amp; Software</source> <volume>52</volume>, <fpage>1</fpage>–<lpage>4</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>Stan Team</collab></person-group>. <year>2012</year> <source>Stan Modeling Language User’s Guide and Reference Manual</source>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98005.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Cooper</surname>
<given-names>Ben S</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This manuscript provides <bold>valuable</bold> evidence comparing the performance of mathematical models and opinions from experts engaged in outbreak response in forecasting the spatial spread of an Ebola epidemic. The evidence supporting the conclusions is <bold>convincing</bold>. It will be of interest to disease modellers, infectious disease epidemiologists, policy-makers, and those who need to inform policy-makers during an outbreak.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98005.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Munday, Rosello, and colleagues compared predictions from a group of experts in epidemiology with predictions from two mathematical models on the question of how many Ebola cases would be reported in different geographical zones over the next month. Their study ran from November 2019 to March 2020 during the Ebola virus outbreak in Democratic Republic of the Congo. Their key result concerned predicted numbers of cases in a defined set of zones. They found that neither the ensemble of models nor the group of experts produced consistently better predictions. Similarly, neither model performed consistently better than the other, and no expert's predictions were consistently better than the others'. Experts were also able to specify other zones in which they expected to see cases in the next month. For this part of the analysis, experts consistently outperformed the models. In March, the final month of the analysis, the models' accuracy was lower than in other months, and consistently poorer than the experts' predictions.</p>
<p>A strength of the analysis is use of consistent methodology to elicit predictions from experts during an outbreak that can be compared to observations, and that are comparable to predictions from the models. Results were elicited for a specified group of zones, and experts were also able to suggest other zones that were expected to have diagnosed cases. This likely replicates the type of advice being sought by policymakers during an outbreak.</p>
<p>A potential weakness is that the authors included only two models in their ensemble. Ensembles of greater numbers of models might tend to produce better predictions. The authors do not address whether a greater number of models could outperform the experts.</p>
<p>The elicitation was performed in four months near the end of the outbreak. The authors address some of the implications of this. A potential challenge for the transferability of this result is that the experts' understanding of local idiosyncrasies in transmission may have improved over the course of the outbreak. The model did not have this improvement over time. The comparison of models to experts may therefore not be applicable to early stages of an outbreak when expert opinions may be less well-tuned.</p>
<p>This research has important implications for both researchers and policy-makers. Mathematical models produce clearly-described predictions that will later be compared to observed outcomes. When model predictions differ greatly from observations, this harms trust in the models, but alternative forms of prediction are seldom so clearly articulated or accurately assessed. If models are discredited without proper assessment of alternatives then we risk losing a valuable source of information that can help guide public health responses. From an academic perspective, this research can help to guide methods for combining expert opinion with model outputs, such as considering how experts can inform models' prior distributions and how model outputs can inform experts' opinions.</p>
<p>Comments on revisions:</p>
<p>I am grateful to the authors for their responses to my previous comments. I think their updates have made the paper much clearer. I do not think the updates change the opinions already given in the public review so I have not modified it.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98005.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The manuscript by Munday et al. presents real-time predictions of geographic spread during an Ebola epidemic in north-eastern DRC. Predictions were elicited from individual experts engaged in outbreak response and from two mathematical models. The authors found comparable performance between experts and models overall, although the models outperformed experts in a few dimensions.</p>
<p>Both individual experts and mathematical models are commonly used to support outbreak response, but the relative strengths of each information source are rarely quantified. The manuscript presents an in-depth analysis of the accuracy and decision-relevance of the information provided by each source individually and in combination for a real-time outbreak response effort.</p>
<p>While this paper presents an important and unique comparison, forecast performance is known to be inconsistent and unpredictable across many dimensions such as pathogen, location, forecasting target, and phase of the outbreak. Thus, as the authors note, continuing to replicate such studies will be important for verifying the robustness of their conclusions in other contexts.</p>
<p>Comments on revisions:</p>
<p>I have no further comments. I commend the authors for an interesting and important contribution.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98005.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Munday</surname>
<given-names>James D</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6206-7134</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Rosello</surname>
<given-names>Alicia</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0737-5679</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Edmunds</surname>
<given-names>W John</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9179-2917</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Funk</surname>
<given-names>Sebastian</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2842-3406</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Munday, Rosello, and colleagues compared predictions from a group of experts in epidemiology with predictions from two mathematical models on the question of how many Ebola cases would be reported in different geographical zones over the next month. Their study ran from November 2019 to March 2020 during the Ebola virus outbreak in the Democratic Republic of the Congo. Their key result concerned predicted numbers of cases in a defined set of zones. They found that neither the ensemble of models nor the group of experts produced consistently better predictions. Similarly, neither model performed consistently better than the other, and no expert's predictions were consistently better than the others. Experts were also able to specify other zones in which they expected to see cases in the next month. For this part of the analysis, experts consistently outperformed the models. In March, the final month of the analysis, the models' accuracy was lower than in other months and consistently poorer than the experts' predictions.</p>
<p>A strength of the analysis is the use of consistent methodology to elicit predictions from experts during an outbreak that can be compared to observations, and that are comparable to predictions from the models. Results were elicited for a specified group of zones, and experts were also able to suggest other zones that were expected to have diagnosed cases. This likely replicates the type of advice being sought by policymakers during an outbreak.</p>
<p>A potential weakness is that the authors included only two models in their ensemble. Ensembles of greater numbers of models might tend to produce better predictions. The authors do not address whether a greater number of models could outperform the experts.</p>
<p>The elicitation was performed in four months near the end of the outbreak. The authors address some of the implications of this. A potential challenge to the transferability of this result is that the experts' understanding of local idiosyncrasies in transmission may have improved over the course of the outbreak. The model did not have this improvement over time. The comparison of models to experts may therefore not be applicable to the early stages of an outbreak when expert opinions may be less welltuned.</p>
<p>This research has important implications for both researchers and policy-makers. Mathematical models produce clearly-described predictions that will later be compared to observed outcomes. When model predictions differ greatly from observations, this harms trust in the models, but alternative forms of prediction are seldom so clearly articulated or accurately assessed. If models are discredited without proper assessment of alternatives then we risk losing a valuable source of information that can help guide public health responses. From an academic perspective, this research can help to guide methods for combining expert opinion with model outputs, such as considering how experts can inform models' prior distributions and how model outputs can inform experts' opinions.</p>
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>The manuscript by Munday et al. presents real-time predictions of geographic spread during an Ebola epidemic in north-eastern DRC. Predictions were elicited from individual experts engaged in outbreak response and from two mathematical models. The authors found comparable performance between experts and models overall, although the models outperformed experts in a few dimensions.</p>
<p>Strengths:</p>
<p>Both individual experts and mathematical models are commonly used to support outbreak response but rarely used together. The manuscript presents an in-depth analysis of the accuracy and decision-relevance of the information provided by each source individually and in combination.</p>
<p>Weaknesses:</p>
<p>A few minor methodological details are currently missing.</p>
</disp-quote>
<p>We thank the reviewers for taking the time to consider our paper and for their positive reflections and suggestions for our study. We recognise and endorse their characterisation of the study in the public reviews and are greatful for their interest and support for this work.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>I initially found Table 1 difficult to interpret. In the final two columns, the rows relate to each other but in the other columns, rows within months don't relate to each other. Could this be made clearer?</p>
</disp-quote>
<p>Thank you for your helpful suggestion. We agree that this is a little confusing and have now added vertical dividers to the table to indicate which parts of the table relate to each other.</p>
<disp-quote content-type="editor-comment">
<p>In Figure 1A, the colours are the same as in the colour-bar for Figure 1B but don't have the same meaning. Could different colours be used or could Figure 1A have its own colour-bar to aid clarity?</p>
</disp-quote>
<p>Thank you for your query. The colours are not the same pallette, but we appreciate that they look very similar. To help the reader we have changed the colour palette of panel A and added a legend to the left.</p>
<disp-quote content-type="editor-comment">
<p>In Figure 3, can labels for each expert be aligned horizontally, rather than moving above and below the timeline each month?</p>
</disp-quote>
<p>Thank you for your perspective on this. We made the concious dicision to desplay the experts in this way as it allows the timeline to be presented in a shorter horizontal space. We appreciate that others may prefer a different design, but we are happy with this one.</p>
<disp-quote content-type="editor-comment">
<p>On lines 292 and 293, the authors state that experts were less confident that case numbers would cross higher thresholds. It seems that this would be inevitable given the number of cases is cumulative. Could this be clarified, please?</p>
</disp-quote>
<p>Thank you for raising this point. We agree that this wording is confusing. We have now reworked the entire section in response to another reviewer. The equivalent section now reads:</p>
<p>Experts correctly identified Mabalako as the highest-risk HZ in December. They attributed an average 82% probability of exceeding 2 cases; Mabalako reported 38 cases that month, exceeding all thresholds, although the probability assigned to exceeding the higher thresholds was similar to that of Beni (3 cases)</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>(1) Some methodological details seem to be missing. Most importantly, the results present multiple ensembles (experts, models, and both), but I can't seem to find anywhere in the Methods that details how these ensembles are calculated. Also, I think it would be useful to define the variables in each equation. It would have been easier to connect the equations to the description if the variables were cited explicitly in the text.</p>
</disp-quote>
<p>Thank you for pointing out these omissions. We have included the following paragraph to detail how ensemble forecasts were calculated.</p>
<p>“Enslemble forecasts</p>
<p>Ensemble forecasts were calculated as an average of the probabilities attributed by the members of the ensemble. For the expert ensemble the arithmetic mean was calculated across all experts with equal weighting. Similarly the model ensemble used the unweighted mean of the model forecasts. For the mixed (model and expert) ensemble, the mean was weighted such that the combined weight of the experts forecasts and the combined weight of the models forecasts were equal.”</p>
<disp-quote content-type="editor-comment">
<p>(2) Overall, I think the results provide a strong analysis of model vs. expert performance. However, some sections were highly detailed (e.g., the text usually discusses results for every month and all health zones), which clouded my ability to see the salient points. For example, I found it difficult to follow all the details about expert/model predictions vs. observations in the &quot;Expert panel and health zones...&quot; subsection; instead, the graphical illustration of predictions vs. observations in Figure 4 was much easier to interpret. Perhaps some of these details could be trimmed or moved to the supplementary material.</p>
</disp-quote>
<p>Thank you for your honest feedback on this point. We have shortened this section to highlight the key points that we feel are the most important. We have also simplified the text where we discuss the health zones nominated by experts.</p>
<disp-quote content-type="editor-comment">
<p>(3) Figure 5C is a nice visualization of the fallibility of relying on a single individual expert (or model). I wonder if it would be useful to summarize these results into the probability that a randomly selected expert outperforms a single model. Is it the case that a single expert is more unreliable than a single model? The discussion emphasizes the importance of ensembles and compares a single model to an ensemble of experts, but eliciting predictions from multiple experts may not always be possible.</p>
</disp-quote>
<p>Thank you for raising this. We agree that this is an important point that eliciting expert opinions is not a trivial task and should not be taken for granted. We agree with the principle of your suggestion that it would be useful to understand how the models compare to indevidual experts. We don’t however believe that an additional analysis would add sufficiently more information than already shown in Figure 5, which already displays the full distribution of indevidual experts for each month and threshold. If you would like to try this analysis yourself, the relevant data (the indevidual score for each combination of expert, threshold, heal zone and month) is included in the github repo (<ext-link ext-link-type="uri" xlink:href="https://github.com/epiforecasts/Ebola-Expert-Elicitation/blob/main/outputs/indevidual_results_with_scores.csv">https://github.com/epiforecasts/Ebola-Expert-Elicitation/blob/main/outputs/indevidual_results_with_scores.csv</ext-link>).</p>
<disp-quote content-type="editor-comment">
<p>Minor comments:</p>
<p>(1) Figure 2: the color scales in each panel are meant to represent different places, correct? The figure might be easier to interpret if the colors used were different.</p>
</disp-quote>
<p>Thank you for bringing this to our attention. We have now changed the palette of panel A to differ from panel B.</p>
<disp-quote content-type="editor-comment">
<p>(2) Equation 7: is o(c&gt;c_thresh) meant to be the indicator function (i.e. 1 if c&gt;c_thresh) and 0 otherwise)?</p>
</disp-quote>
<p>Thanks for raising this. The function o is the same as in the previous equation – an observation count function. We appreciate that this is not immediately clear so have added a sentence to explain the notation after the equation.</p>
<disp-quote content-type="editor-comment">
<p>(3) Table 1: a brief description of the column headers would be useful.</p>
</disp-quote>
<p>Thank you for the suggestion. We have now extended the table caption to include more description of the columns.</p>
<p>“Table 1: Experts and health zones included in each round of the survey. The left part of the table details the experts interviewed (highlighted in green) the health zones included in the main survey in each month. In addition, the right part of the table details the health zones nominated by experts and the number of experts that nominated each one.”</p>
</body>
</sub-article>
</article>