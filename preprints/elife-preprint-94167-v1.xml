<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">94167</article-id>
<article-id pub-id-type="doi">10.7554/eLife.94167</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94167.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Pan-cortical 2-photon mesoscopic imaging and neurobehavioral alignment in awake, behaving mice</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7053-4740</contrib-id>
<name>
<surname>Vickers</surname>
<given-names>Evan D.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9803-8335</contrib-id>
<name>
<surname>McCormick</surname>
<given-names>David A.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Institute of Neuroscience, University of Oregon</institution>, Eugene, OR</aff>
<aff id="a2"><label>2</label><institution>Professor, Department of Biology</institution></aff>
<aff id="a3"><label>3</label><institution>Director, Institute of Neuroscience</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Grunwald Kadow</surname>
<given-names>Ilona C</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Bonn</institution>
</institution-wrap>
<city>Bonn</city>
<country>Germany</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Huguenard</surname>
<given-names>John R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University School of Medicine</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Corresponding author; email: <email>davidmc@uoregon.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-01-23">
<day>23</day>
<month>01</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP94167</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-11-06">
<day>06</day>
<month>11</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-10-24">
<day>24</day>
<month>10</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.19.563159"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Vickers &amp; McCormick</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Vickers &amp; McCormick</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-94167-v1.pdf"/>
<abstract>
<title>Abstract</title><p>The flow of neural activity across the neocortex during active sensory discrimination is constrained by task-specific cognitive demands, movements, and internal states. During behavior, the brain appears to sample from a broad repertoire of activation motifs. Understanding how these patterns of local and global activity are selected in relation to both spontaneous and task-dependent behavior requires in-depth study of densely sampled activity at single neuron resolution across large regions of cortex. In a significant advance toward this goal, we developed procedures to record mesoscale 2-photon Ca<sup>2+</sup> imaging data from two novel <italic>in vivo</italic> preparations that, between them, allow simultaneous access to nearly all of the mouse dorsal and lateral neocortex. As a proof of principle, we aligned neural activity with both behavioral primitives and high-level motifs to reveal the existence of large populations of neurons that coordinated their activity across cortical areas with spontaneous changes in movement and/or arousal. The methods we detail here facilitate the identification and exploration of widespread, spatially heterogeneous neural ensembles whose activity is related to diverse aspects of behavior.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>The number for David A. McCormick's R35 was incorrectly indicated, there was a numeral accidently pasted in the middle of the Rastermap display in Figure 6, the links for the public github page were incorrectly indicated / not hyperlinked, and the period following the first author's middle initial was left out of the title page. Also, the supplementary methods were included as a word doc instead of a pdf.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">https://github.com/vickerse1/mesoscope_spontaneous</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Recent advances in large-scale neural recording technology, such as widefield imaging (<xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>; <xref ref-type="bibr" rid="c12">Gallero-Salas et al 2021</xref>; <xref ref-type="bibr" rid="c11">Esmaeili et al, 2021</xref>), large field-of-view (FOV) 2-photon (2p) imaging (<xref ref-type="bibr" rid="c38">Sofroniew et al, 2016</xref>; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>; Yu et al, 2021), and Neuropixels high-density extracellular electrophysiology recordings (<xref ref-type="bibr" rid="c21">Jun et al, 2017</xref>; <xref ref-type="bibr" rid="c40">Steinmetz et al, 2019</xref>) have allowed for rapid advancement in our understanding of the relationships between brain-wide neural activity and both spontaneous and task-engaged behavior in mice. For example, these techniques can now be deployed in recently developed behavioral paradigms that allow for the separation of decision, choice, and response periods in 2-alternative forced choice (2-AFC) discrimination tasks (Guo and Svoboda, 2014), the standardization of training and performance analysis across laboratories (The International Brain Laboratory et al, 2021; eLife), and the separation of context-dependent rule representation and choice in a working memory task (Wu and Shadlen, 2020).</p>
<p>One of the striking features of neocortical neuronal activity is how strongly changes in behavioral state, such as task engagement, movement, or arousal, affect the spontaneous and evoked activity of neurons within visual, auditory, somatosensory, motor and other cortical regions (<xref ref-type="bibr" rid="c30">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="c25">McGinley et al, 2015</xref>; <xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>). However, this is not to say that these effects are uniform across the cerebral cortex (<xref ref-type="bibr" rid="c28">Morandell et al, 2023</xref>; <xref ref-type="bibr" rid="c50">Wang et al, 2023</xref>). Neurons exhibit diversity in the dependence of their activities on arousal and behavioral state both between and within cortical areas (<xref ref-type="bibr" rid="c30">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="c25">McGinley et al, 2015</xref>; Shimoaka et al, 2018), and these areas are active at different times during a rewarded task (<xref ref-type="bibr" rid="c35">Salkoff et al, 2020</xref>; <xref ref-type="bibr" rid="c11">Esmaeili et al, 2021</xref>; <xref ref-type="bibr" rid="c12">Gallero-Salas et al 2021</xref>). Furthermore, the arousal dependence of membrane potential across cortical areas has been shown to be diverse and predictable by a temporally filtered readout of pupil diameter and walking speed (Shimoaka et al, 2018). This makes simultaneous recording of multiple cortical areas essential for comparison of the dependence of their neural activity on arousal/movement, because combining multiple recording sessions with pupil dilations and walking bouts of different durations will necessarily obscure the true spatial correlation structures.</p>
<p>Areas involved in sensory decision making are often far from each other (<xref ref-type="bibr" rid="c12">Gallero-Salas et al, 2021</xref>) and can exhibit coordinated state-dependent changes in functional coupling (<xref ref-type="bibr" rid="c7">Clancy et al, 2019</xref>). Also, multimodal sensory information is multiplexed and combined as it ascends across the cortical hierarchy (<xref ref-type="bibr" rid="c8">Coen et al, 2023</xref>). For these reasons, understanding the brain activity underlying optimal performance during multimodal, task-engaged behavior will require dense sampling of many brain areas at single neuron resolution across lateral, dorsal, and frontal cortices simultaneously at a temporal resolution high enough to describe both spontaneous behavioral state transitions and the neural dynamics relevant for a given task.</p>
<p>Dense intra-cortical sampling at a fixed depth/cortical layer across many areas is not possible with current Neuropixels probes (<xref ref-type="bibr" rid="c39">Steinmetz et al, 2021</xref>), 1-photon widefield imaging can be contaminated by neuropil and hemodynamic signal (<xref ref-type="bibr" rid="c51">Waters, 2020</xref>; Valley et al, 2021) and does not achieve single cell resolution (but see <xref ref-type="bibr" rid="c55">Yoshida et al, 2018</xref>; <xref ref-type="bibr" rid="c22">Kauvar et al, 2020</xref>), and standard 2-photon (2p) imaging is limited by scanning speed (see <xref ref-type="bibr" rid="c15">Gong et al, 2015</xref>) and field of view spatial extent (FOV; i.e. simultaneously imageable, either contiguous or non-contiguous, regions; <xref ref-type="bibr" rid="c1">Allen et al, 2017</xref>; <xref ref-type="bibr" rid="c17">Hattori et al, 2019</xref>; but see <xref ref-type="bibr" rid="c56">Yu et al, 2020</xref>). Recent advancements in cranial window preparations have enabled imaging over large portions of the dorsal cortex (<xref ref-type="bibr" rid="c23">Kim et al, 2016</xref>; <xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv). However, simultaneous 2-photon imaging over a large cortical area including both dorsal and lateral regions, particularly in a preparation that allows for simultaneous imaging of the major primary sensory cortices (auditory, visual, and somatosensory) and frontal choice/motor areas (M1, M2) in awake, behaving mice has not been previously shown.</p>
<p>Here, we developed a set of new techniques and integrated them with existing technologies in order to overcome the limitations of current state-of-the-art methods and provide the first pan-cortical 2p assays at single-cell resolution in awake, behaving mice. To achieve this, we designed custom 3D-printed titanium headposts, mounting devices, adapters, and cranial windows, and modified (“Crystal Skull”; <xref ref-type="bibr" rid="c23">Kim et al, 2016</xref>; <xref rid="fig1" ref-type="fig">Figs. 1a, d, S1c</xref> upper, e) or developed (“A1/V1/M2” or “temporo-parietal”; <xref rid="fig1" ref-type="fig">Figs. 1b, e, S1a, f</xref>) two <italic>in vivo</italic> surgical preparations, which we will henceforth refer to as the “dorsal mount” and “side mount”, respectively. These two novel preparations enabled mesoscale 2-photon imaging (2p-RAM mesoscope, Thorlabs; <xref ref-type="bibr" rid="c38">Sofroniew et al, 2016</xref>) of up to ∼7500 individual neurons simultaneously at ∼3 Hz across a 5 x 5 mm FOV (<xref rid="fig2" ref-type="fig">Figs. 2</xref>, <xref rid="fig3" ref-type="fig">3</xref>, <xref rid="fig2" ref-type="fig">S2</xref>; Suppl movies S4, S5; Protocol 1), or up to 800 neurons combined across four 660 x 660 µm FOVs at ∼10 Hz (Fig. S2d, e; Protocol 1; see Methods).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Technical adaptations for dual-mount <italic>in vivo</italic> pan-cortical imaging with the Thorlabs 2p-RAM mesoscope.</title>
<p><bold>a)</bold> Mesoscope behavioral apparatus for the dorsal mount preparation. Mouse is mounted upright on a running wheel with headpost fixed to dual adjustable support arms mounted on vertical posts (1” diameter). Behavior cameras are fixed at front left, front right, and center posterior, with ultraviolet and infrared light-emitting diodes aligned with goose-neck supports parallel to right and left cameras. <bold>b)</bold> Mesoscope behavioral apparatus for side mount preparation. Same as in (a), except that the mouse is rotated 22.5 degrees to its left so that the objective lens (at angle 0) is orthogonal to its center-of-mass of preparation in the right cortical hemisphere. The objective can rotate +/- 20 degrees medio-laterally, if needed, to optimize imaging of any portion of the cortex under the cranial window. The right behavior camera is positioned more posterior and lower than in a), to allow for imaging of the eye under the acute angle formed with the horizontal light shield, shown in c). <bold>c)</bold> Mouse running on wheel with side mount preparation receiving visual stimulation from an LED screen positioned on the left side, with linear motor-positioned dual lick-spouts in place and 3D printed vertical light shield (wok 2) attached to rim of flat shield (wok 1) to block extraneous light from entering the objective. <bold>d)</bold> Overhead view of dorsal mount preparation with 3D printed titanium headpost, custom cranial window, and Allen CCF aligned to paraformaldehyde-fixed mouse brain. Motor region = light green, somatosensory = dark green, visual = dark blue, retrosplenial = light blue. Olfactory bulbs (anterior) at top, cerebellum (posterior) at bottom of image. Note ridge along perimeter of headpost for fitted horizontal light shield (wok 1) attachment. <bold>e)</bold> Rotated dorsal view (22.5 degrees right) of side mount preparation with 3D printed titanium headpost, custom cranial window, and Allen CCF aligned to paraformaldehyde-fixed mouse brain. Auditory region shown in light blue, ventral and anterior to visual areas, and ventral and posterior to somatosensory areas (right side of image is lateral/ventral, and left side of headpost perimeter is medial/dorsal). Other regions shown with the same color scheme as in d). UV = ultraviolet, IR = infrared, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, V1 = primary visual cortex, A1 = primary auditory cortex.</p></caption>
<graphic xlink:href="563159v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Widefield-imaging based multimodal mapping (MMM) and CCF alignment, ROI detection, and areal assignment of GCaMP6s<sup>+</sup> excitatory neurons for the dorsal mount preparation.</title>
<p><bold>a)</bold> Mean projection of 30 s, 470 nm excitation epifluorescence widefield movie of vasculature performed through a dorsal mount cranial window. Midline indicated by a dashed white line. <bold>b)</bold> Mean 1 s baseline subtracted dF/F images of 1 s responses to ∼20-30 repetitions of left-side presentations of a 5-40 kHz auditory tone cloud (auditory TC; left), visual full field (FF) isoluminant Gaussian noise stimulation (center), and 100 ms x 5 Hz burst whisker deflection (right) in an awake dorsal mount preparation mouse. <bold>c)</bold> Example 2-step CCF alignment to dorsal mount preparation, performed in Python on MMM masked blood-vessel image (upper left), rotated outline CCF (upper right), and Suite2p region of interest (ROI) output image containing exact position of all 2p imaged neurons (lower right). Yellow outlines show the area of masks created from the thresholded mean dF/F image for, in this example, repeated full-field visual stimulation. In step 1 (top row), the CCF is transformed and aligned to the MMM image using a series of user-selected points (blue points with red numbered labels, set to matching locations on both left and right images) defining a bounding-box and known anatomical and functional locations. In step 2 (bottom row), the same process is applied to transformation and alignment of Suite2p ROIs onto the MMM using user-selected points defining a bounding box and corresponding to unique, identifiable blood-vessel positions and/or intersections. Finally, a unique CCF area name and number are assigned to each Suite2p ROI (i.e. neuron) by applying the double reverse-transformation from Suite2p cell center location coordinates, to MMM, to CCF. <bold>d)</bold> CCF-aligned Suite2p ROIs from an example dorsal mount preparation with 7328 neurons identified in a single 30 min session from a spontaneously behaving mouse. TC = tone cloud, FF = full-field, defl = deflection, dF/F = change in fluorescence over baseline fluorescence, CCF = Allen common coordinate framework version 3.0, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, V1 = primary visual cortex, A = anterior, P = posterior, R = right, L = left.</p></caption>
<graphic xlink:href="563159v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Widefield-imaging based multimodal mapping (MMM) and CCF alignment, ROI detection, and areal assignment of GCaMP6s<sup>+</sup> excitatory neurons for the side mount preparation.</title>
<p><bold>a)</bold> Mean projection of 30 s, 470 nm excitation fluorescence widefield movie of vasculature performed through a side mount cranial window. Midline indicated by a dashed white line. <bold>b)</bold> Mean 1 s baseline subtracted dF/F images of 1 s responses to ∼20-30 repetitions of left-side presentations of a 5-40 kHz auditory tone cloud (auditory TC; left), visual full field (FF) isoluminant Gaussian noise stimulation (center), and 100 ms x 5 Hz burst whisker deflection (right) in an example side mount preparation mouse under 1.5% isoflurane anesthesia. <bold>c)</bold> Example 2-step CCF alignment to side mount preparation, performed in Python on MMM masked blood-vessel image (upper left), rotated outline CCF (upper right), and Suite2p region of interest (ROI) output image containing exact position of all 2p imaged neurons (bottom right). Yellow outlines show the area of masks from thresholded mean dF/F image for repeated auditory, full-field visual, and/or whisker stimulation. In step 1 (top row), the CCF is transformed and aligned to the MMM image using a series of user-selected points (blue points with red numbered labels, set to matching locations on both left and right images) defining a bounding-box and known anatomical and functional locations. In step 2 (bottom row), the same process is applied to transformation and alignment of Suite2p ROIs onto the MMM using user-selected points defining a bounding box and corresponding to unique, identifiable blood-vessel positions and/or intersections. Finally, a unique CCF area name and number are assigned to each Suite2p ROI (i.e. neuron) by applying the double reverse-transformation from Suite2p cell center location coordinates, to MMM, to CCF. <bold>d)</bold> CCF-aligned Suite2p ROIs from an example side mount preparation with 4782 neurons identified in a single 70 min session from a mouse performing our 2-alternative forced choice (2-AFC) auditory discrimination task. TC = tone cloud, FF = full-field, defl = deflection, dF/F = change in fluorescence over baseline fluorescence, CCF = Allen common coordinate framework version 3.0, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, A1 = primary auditory cortex, V1 = primary visual cortex, A = anterior, P = posterior, R = right, L = left.</p></caption>
<graphic xlink:href="563159v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In addition, we designed a custom, LabView-controlled behavioral setup with up to 3 high-speed body and face cameras (<xref rid="fig1" ref-type="fig">Figs. 1a</xref>, b; Suppl movies S1), allowing us to compare widespread, densely sampled, high-resolution neural activity to movement and behavioral arousal state variation in head-fixed, awake, ambulatory (i.e. running atop a cylindrical wheel) mice under conditions of spontaneous behavior (<xref rid="fig4" ref-type="fig">Figs. 4</xref>-<xref rid="fig6" ref-type="fig">6</xref>, S3-S7), passive sensory stimulation (<xref rid="fig2" ref-type="fig">Figs. 2</xref>, <xref rid="fig3" ref-type="fig">3</xref>, <xref rid="fig2" ref-type="fig">S2</xref>), or 2-alternative forced choice (2-AFC) task engagement (<xref rid="fig1" ref-type="fig">Figs. 1c</xref>, Protocol 1). We present here detailed methods, along with example recording sessions during spontaneous behavior from both dorsal and side mount preparations, to demonstrate the feasibility of widespread 2-photon cortical neuronal imaging simultaneously with behavioral monitoring.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>The relationship between spontaneous behavioral measures and neural activity across the dorsal cortex is globally heterogeneous.</title>
<p><bold>a)</bold> Rastermap (top), first principal component (PC1; middle), and second principal component (PC2, bottom) sorting of normalized, rasterized, neuropil subtracted dF/F neural activity for 3096 cells from a single 20 min duration example dorsal mount 2-photon imaging session. Each row in the display corresponds to a “superneuron”, or average activity of 50 adjacent neurons in the Rastermap sort (<xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv). Activity for each superneuron is separately z-scored, then displayed as low activity in blue, intermediate activity in green, and maximum activity level in yellow (standard viridis color look-up table). Red and blue arrowheads show alignment of walking and whisking bouts, respectively, to neural activity. White dashed box inserts indicate dual selections highlighted in panels c) and d). Scale bar shows color look-up map for each separately z-scored, individually displayed superneuron in the raster displays. <bold>b)</bold> Behavioral arousal primitives of walk speed, whisker motion energy, and pupil diameter shown temporally aligned to the rasterized neural activity traces in a), directly above. Horizontal black bar indicates time of expanded inset in Fig. S4. <bold>c)</bold> Expanded insets of top and bottom fifths of rasterized PC1 sorting from the middle segment of panel a), with mean dF/F activity traces shown above each. Red and blue arrowheads indicate the same walking and whisking bouts, respectively, as in a) and b). <bold>d)</bold> Normalized density of neurons in each CCF area belonging to two example Rastermap sorted groups (d1, left, red: MIN=0%, MAX=25%; d2, right, blue: MIN=0%, MAX=13%), with rasterized activities shown in corresponding labeled white dashed boxes in the top segment of panel a). The neurons present in selected Rastermap groups are shown as Suite2p footprint outlines. The type of behavioral arousal primitive activity typically concurrent with high neural activity for each Rastermap group is shown below each Rastermap group’s CCF density map (i.e. walk and whisk for group d1, left (red), and whisk for group d2, right (blue). <bold>e)</bold> Normalized mean correlations of neural activity and walk speed (left, red; mean: MIN=0.01, MAX=0.03) and whisker motion energy (right, blue; mean: MIN=0.02, MAX=0.08) per CCF area (i.e. average correlation of all neurons in each area) are shown for this example session. Mean walk speed correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.001, median t(3095)=3.7, single-sample t-test; python: scipy.stats.ttest_1samp) for 15 of the 19 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were right VISam, right SSp_ll, left VIS_rl, and left AUDd. Mean whisker motion energy correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.001, median t(3095)=4.4, single-sample t-test) for 16 of the 19 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were right VISam, left VISam, and left VISrl. PC = principal component, ME = motion energy, au = arbitrary units, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, V1 = primary visual cortex.</p></caption>
<graphic xlink:href="563159v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>The relationship between spontaneous behavioral measures and neural activity across the lateral cortex is regionally patterned.</title>
<p><bold>a)</bold> Rastermap (top), first principal component (PC1; middle), and second principal component (PC2, bottom) sortings of normalized, rasterized, neuropil subtracted dF/F neural activity for 5678 cells from a single 90 min duration example side mount 2-photon imaging session. Each row in the display corresponds to a “superneuron”, or average of 50 adjacent neurons in the Rastermap sort (<xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv). Low activity (z-scored) in blue, intermediate activity in green, maximum activity level in yellow. Red and blue arrowheads show alignment of walking and whisking bouts, respectively, to neural activity. White dashed box inserts indicate selections highlighted in panels c) and d). Scale bar shows color look-up map for each separately z-scored, individually displayed superneuron in the raster displays. <bold>b)</bold> Behavioral arousal primitives of walk speed, whisker motion energy, and pupil diameter shown temporally aligned to the rasterized neural activity traces in a), directly above. <bold>c)</bold> Expanded insets of top and bottom fifths of rasterized PC1 sorting from the middle segment of panel a), with mean activity traces shown above each. Red and blue arrowheads indicate the same walking and whisking bouts, respectively, as in a) and b). Horizontal black bar indicates time of expanded inset shown in Fig. S5. <bold>d)</bold> Normalized density of neurons in each CCF area belonging to two example Rastermap sorted groups (d1, left, red: MIN=0%, MAX=34%; d2, right, blue: MIN=0%, MAX=32%), with rasterized activities shown in corresponding labeled white dashed boxes in the top segment of panel a). Only cells in selected Rastermap groups are shown. The type of behavioral arousal primitive (i.e. walk and whisk, left, in red; whisk, right, in blue) that was typically concurrent with high neural activity is shown below each Rastermap group’s CCF density map. <bold>e)</bold> Normalized mean correlations of neural activity and walk speed (left, red; mean: MIN=0.006, MAX=0.061; standard deviation: MIN=0.000, MAX=0.029) and whisker motion energy (right, blue; mean: MIN=0.009, MAX=0.079; standard deviation: MIN=0.000, MAX=0.043) per CCF area for this example session. Mean walk speed correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.001, median t(5677)=4.4, single-sample t-test; python: scipy.stats.ttest_1samp) for 20 of the 24 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were left VISp, right SSpn, right AUDpo, and right TEa. Mean whisker motion energy correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.001, median t(5677)=7.0, single-sample t-test) for 21 of the 24 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were right SSpn, right AUDpo, and right TEa. PC = principal component, ME = motion energy, au = arbitrary units, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, V1 = primary visual cortex, A1 = primary auditory cortex.</p></caption>
<graphic xlink:href="563159v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Alignment of activity sorted neural ensembles and spontaneous behavioral motifs reveals sparse, distributed encoding of arousal measures.</title>
<p><bold>a)</bold> Manual identification of high-level, qualitative behaviors (twitch, green dashed box; whisk, blue dashed box; walk, red dashed box; pupil oscillate, gray dashed box), aligned to sets of raw, unfiltered behavioral motifs (B-SOiD) extracted from pose-estimates (DeepLabCut) for a single example session. Numbered B-SOiD motifs (trained on a set of 4 sessions from the same mouse, motif # indicated by the position of the thick blue line relative to the y-axis at each time point) are color-coded (horizontal bars) across the entire 90-minute session. <bold>b)</bold> High level behaviors (dashed colored boxes, i-iv) and BSOiD motifs from a) are vertically temporally aligned to the behavioral primitive movement and arousal measures of walk speed, whisker motion energy, and pupil diameter. Expanded alignments of high level behaviors ii (whisk) and iii (walk) are shown in Fig S7a, b. Red arrowheads indicate temporal alignment positions for multiple walk bouts across BSOiD motifs (a), behavioral arousal primitives (b), and Rastermap sorted neural data (c). <bold>c)</bold> Rastermap sorted, z-scored, rasterized, neuropil subtracted dF/F neural activity (10th percentile baseline, rolling 30 s window) from the same side mount session, temporally aligned to the behavioral data directly above. Numbers at left (R0-R7) indicate Rastermap motif numbers, selected manually “by-eye” for this session, and horizontal dashed white lines on raster indicate separation between neighboring Rastermap motifs. Each row in the display corresponds to a “superneuron”, or average of 50 adjacent neurons in the Rastermap sort (<xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv). Colored, dashed boxes indicate alignment of high-level behaviors with neural activity epochs from the same example session. White dashed ovals indicate areas of Rastermap group-aligned active neural ensembles during periods of defined high-level behaviors. The 2-photon sampling rate for this session was 3.38 Hz. <bold>d)</bold> Normalized mean activity traces for all neurons in each neural ensemble indicated in (c) (white dashed ovals). Each trace also corresponds and is aligned to the indicated Rastermap groups. <bold>e)</bold> Color-coded neuron densities per CCF area corresponding to Rastermap groups shown in c), shown grouped by qualitative high-level behaviors, as indicated in a-b). Cross-indexing with neurobehavioral alignments (white dashed ovals) in c) allows for visualization of spatial distribution of neurons active during identified high-level behaviors (a) consisting of defined patterns of behavioral primitive movement and arousal measures (b). Corresponding maximum cell density percentages in each CCF area (white to red scale bar, top right) for each normalized Rastermap (0-7) heatmap color lookup table are 31.8, 24.0, 8.0, 11.4, 24.1, 38.4, 22.9, and 33.2%, respectively (left to right, top row followed by second row). This list of cell density percentages, therefore, corresponds to that of the CCF area filled with the darkest shade of red in each Rastermap group. White CCF areas in each Rastermap group are areas where no cells of that group were found, or where the total number of cells was less than 20 and therefore the density estimate was deemed unreliable and not reported. ME = motion energy, au = arbitrary units, CCF = common coordinate framework.</p></caption>
<graphic xlink:href="563159v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Mice</title>
<p>To image cortical activity, we used both male and female, 8-50 week old, CaMKII-tTA x tetO-GCaMP6s (provided by Cris Niell, University of Oregon), CaMKII-Cre x Ai148 (GCaMP6f), and CaMKII-Cre x Ai162 (GCaMP6s) mice (Jackson Labs, Allen Institute). Neural activity was typically imageable between 30 and 180 days post-windowing. Habituation to the experimental setup was performed for 2-3 days prior to 2p data acquisition during spontaneous behavior sessions.</p>
</sec>
<sec id="s2b">
<title>Surgeries</title>
<p>See Protocols II and III, and Supplementary Methods and Materials.</p>
</sec>
<sec id="s2c">
<title>Behavioral videography</title>
<p>See Supplementary Methods and Materials, Overall Workflow, Protocol I. See Supplementary Movies S1.</p>
</sec>
<sec id="s2d">
<title>Widefield imaging and multimodal mapping</title>
<p>See Supplementary Methods and Materials, Overall Workflow, Protocol I, and Multimodal Mapping. See Supplementary Movies S2 and S3.</p>
</sec>
<sec id="s2e">
<title>2-photon imaging and ROI extraction</title>
<p>See Supplementary Methods and Materials, ScanImage 2-photon acquisition. 2p imaging was performed with a Spectra Physics Mai Tai HP-244 tunable femtosecond laser and the Thorlabs Multiphoton Mesoscope. We controlled imaging with ScanImage (2018-2021; Vidrio) running in Matlab, with GPU-enabled online z-axis motion correction. Behavioral synchronization was achieved by triggering 2p acquisition with the first right body camera frameclock signal. Left and posterior cameras were triggered by the same signal, but were subject to variable LabView runtime-related delays on the order of up to ∼10 ms and, occasionally, dropped frames. Exposure times for both left and right body cameras, as well as frame-clock times for all three body cameras, were recorded in Spike2 (CED Power1401) as continuous waveforms and events, respectively, to allow for temporal alignment of recorded videos. We used the ScanImage function SI render to combine image strips from large field-of-view (FOV) sessions, and in some cases we used custom Suite2p Matlab scripts (from Carsen Stringer) to align small FOVs from the same or different z-planes in a single session. Rigid and non-rigid motion correction were performed in Suite2p, along with region of interest (ROI) detection and classifier screening of ROIs likely to be neurons. Fluorescence intensity was calculated as changes in fluorescence divided by baseline fluorescence (dF/F), where F was calculated, pixel by pixel, using the ∼30-60 s (100-200 frame) rolling 10th or 15th percentile baseline of the neuropil subtracted mean ROI pixel intensity (F-0.7*Fneu). Traces of dF/F were then truncated to match the length of acquired behavioral data and resampled (python: scipy.signal.resample for upsampling, or numpy: slicing for downsampling), along with behavioral data, to 10 Hz for further analysis. ROIs with a Suite2p classifier-assigned cell probability less than 0.5 were excluded from further analysis.</p>
</sec>
<sec id="s2f">
<title>Pose-tracking analysis (DeepLabCut)</title>
<p>An example DeepLabCut (DLC) (<ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut">https://github.com/DeepLabCut/DeepLabCut</ext-link>; <xref ref-type="bibr" rid="c24">Mathis et al, 2018</xref>) annotated video with 66 labeled points is shown in Suppl Movie S1 (bottom right). Video snippets were concatenated, cropped, truncated, and aligned to 2p frames with FFMPEG under the Linux subsystem for Windows (Ubuntu 20.04; Windows 10 Enterprise 2004) or Ubuntu 18.04 running on local network GPU clusters. In the example session shown in <xref rid="fig5" ref-type="fig">Figs. 5</xref>, 6 and S7, we analyzed the right Teledyne Dalsa camera video (30 Hz, 1020 x 760 pixel with 2x2 binning, 1” x 1.8” sensor, 90 min video, ffmpeg crop compression to 525.12 MB), which was natively aligned to the 2p acquisition by a common trigger with frameclock and exposure times recorded by a CED 1401 data acquisition device in Spike2 software (Cambridge Electronic Design), running 650K iterations with the resnet_50 model in DLC (see labeled video, Suppl Movie S1, side mount).</p>
</sec>
<sec id="s2g">
<title>Behavioral motif analysis (BSOiD)</title>
<p>Pose-tracking data outputs from DLC in the form of comma-separated value files were loaded into B-SOiD (<ext-link ext-link-type="uri" xlink:href="https://github.com/YttriLab/B-SOID">https://github.com/YttriLab/B-SOID</ext-link>; Hsu and Yttri, 2021) along with concatenated, cropped, and spatially downsampled (FFMPEG) right-camera videos (∼20-90 minutes, ∼100 MB to 1 GB per video). B-SOiD uniform manifold approximation and projection (UMAP) embedding training was performed on between 1 and 4 sessions from the same mouse. Between 20 and 60 labeling space dimensions were typically assigned per session, with between 75 and 98% of features confidently assigned to between ∼2-15 motifs per session. Minimum bout time was set at 900 ms, and minimum cumulative motif time per session was set at ∼1-5 minutes.</p>
<p>Pixel-wise analysis of cumulative motion energy showed that B-SOiD-identified motifs differed in terms of body-region localized basic movement patterns in a manner that was consistent with human-annotated categorization labels. Thresholding of cumulative motion energy by z-score further confirmed this and made the following kinematic patterns evident: i) In asymmetric pose motifs, a clear region of low movement (dark pixels) is visible in the chest region between the left and right shoulders that is either absent or not as prominent in high-arousal and symmetric pose motifs); ii) Enhanced nose and mouth movement was associated with both high arousal walking and non-walking states; and iii) Increased whisker movement was associated with both high arousal walking and non-walking states.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>Recent work with large-scale imaging (e.g. widefield and mesoscale 2p) and electrophysiology (e.g. Neuropixels) recording technologies has suggested that a significant percentage of the variance of neural activity across neocortex is accounted for by rapid spontaneous fluctuations in arousal and self-directed movement during both spontaneous behavior and task performance (<xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>; <xref ref-type="bibr" rid="c40">Steinmetz et al, 2019</xref>; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>; <xref ref-type="bibr" rid="c20">Jacobs et al, 2020</xref>; <xref ref-type="bibr" rid="c35">Salkoff et al, 2020</xref>; <xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv). Given that arousal state- and movement-related activity appears to be a ubiquitous feature of cortical activity across all regions, it would be advantageous to develop methodologies that allow for simultaneous, single neuron resolution, contiguous monitoring of neuronal activity during second-to-second movements and changes in arousal, within both spontaneous and trained behaviors. Here, we harness new imaging and analysis technologies in order to both address this methodological gap and provide a proof of principle test of these methods by examining the relationship of behavioral arousal and movement to detailed spatial patterns of neural activity across the dorsal and lateral neocortex.</p>
<sec id="s3a">
<title>Large field of view 2-photon imaging in behaving mice</title>
<p>In order to simultaneously monitor the activity of cortical neurons across a large area (∼25 mm<sup>2</sup>, or up to 36 mm<sup>2</sup> in some cases) of the dorsal cortex in awake, behaving mice, we chose 2-photon imaging of GCaMP6s fluorescence using existing 2-photon random access mesoscope (Thorlabs) imaging technology (2p-RAM; <xref ref-type="bibr" rid="c38">Sofroniew et al, 2016</xref>). We chose this form of data collection for use in head-fixed mice because it allows for: 1) rapid scanning (i.e. it uses a resonant scanner coupled to a virtually conjugated galvo pair) over a large (5x5 mm fully corrected, or up to 6x6 mm imageable), field-curvature corrected field of view (FOV); 2) subcellular resolution in the z-axis to avoid region-of-interest (ROI) contamination by neuropil and neighboring cells (0.61 x 0.61 x 4.25 µm xyz point-spread function at 970 nm laser excitation wavelength); 3) correction for aberrations at all wavelengths between 900-1070 nm, to allow for combined imaging of multiple fluorophores. We achieved all three of these objectives in head-fixed mice that are ambulatory, able to clearly view visual monitors on both sides, and readily able to lick two water spouts (left, right) for 2-alternative forced choice responses, with left, right, and rear videography to monitor each mouse’s face, pupil and body.</p>
<p>The neuronal imaging methodology we chose (see below) is not the only one available. For example, another promising and recently developed alternative implementation, Diesel 2p, allows for similarly flexible and large FOV imaging at single cell resolution, but with a significantly higher z-axis point-spread function (∼8-10 µm; Yu and Smith, 2021). Despite this drawback, its use of dual scan engines allows for true simultaneous imaging of different cortical areas, while the 2p RAM mesoscope (Thorlabs) only accomplishes this by rapid jumping (∼1 ms) between FOVs. Both systems offer excellent corrected field curvatures on the order of +/- 25 µm over ∼5 mm.</p>
<p>Several primary design constraints needed to be met to achieve our goal of monitoring neuronal activity over a large portion of the mouse dorsal cortex. These ranged from aspects of the surgical preparation, mounting, and imaging, to neural recording (<xref rid="fig1" ref-type="fig">Figs. 1</xref>, S1, Suppl Movies S1, Protocols II-III). To increase the extent of the neocortex over which we could monitor neural activity, we developed two distinct headpost/cranial window preparations. In the first, the dorsal mount, the mouse is head-fixed upright on a cylindrical running wheel and the objective is vertical (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). This allows for monitoring of neural activity over large regions of both cerebral hemispheres, from the posterior aspects of visual cortex to the anterior aspects of motor cortex, and laterally to the dorsal-most aspects of auditory cortex, depending on the placement of the 2p mesoscope objective (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>). The second preparation, the side mount, required the head of the mouse to be rotated 22.5° to the left (or right, not shown), with the microscope objective vertical or tilted 1-5 degrees to the right, so as to extend the 5x5 mm imaging FOV to include the auditory cortex and other neighboring ventral/lateral cortical areas (<xref rid="fig1" ref-type="fig">Fig. 1b, e</xref>).</p>
<p>We observed that mice adapt well to head tilt in the side mount preparation, and will readily whisk, walk or run, and learn to perform lick response tasks in this configuration. Keeping the microscope objective in a vertical or near-vertical orientation, and rotating the mouse’s head (instead of rotating the objective), significantly improved the manageability of the water meniscus between the objective and the cranial window. However, if needed, the objective of the Thorlabs mesoscope may be rotated laterally up to +/- 20° for direct access to more ventral cortical areas, for example if one wants to use a smaller, flat cortical window that requires the objective to be positioned orthogonally to the target region.</p>
<p>Imaging in this novel side mount preparation was restricted to one hemisphere and a thin medial strip (∼1 mm wide) of the contralateral hemisphere. This allowed for imaging from roughly the anterio-medial areas of V1 to medial and medio-lateral secondary regions of motor cortex (or slightly farther, if the anterior-posterior axis of the brain is oriented along the diagonal of the 5 x 5 mm FOV). This anterior-posterior reach was similar to that observed in the dorsal mount preparation. However, the side mount windowing and the accompanying mounting procedure significantly increased our ability to image neural activity in lateral (ventral) cortical aspects, including auditory, somatosensory, and association cortical regions (<xref rid="fig1" ref-type="fig">Fig. 1e</xref>), notably without the need for substantial rotation of the objective.</p>
<p>To achieve 2p neuronal imaging from broad cortical regions, we created large and stable, custom (designed in AutoDesk Inventor) 3D-printed titanium (laser sintered powder with active cooling and shot-peened post-processing) headposts (<xref rid="fig1" ref-type="fig">Figs. 1d, 1e</xref>, S1e, S1f, Suppl Design Files; <ext-link ext-link-type="uri" xlink:href="http://i.materialise.com">i.materialise.com</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://sculpteo.com">sculpteo.com</ext-link>) and glass (0.21 mm thick, Schott D263T) cranial windows (<xref rid="fig1" ref-type="fig">Figs. 1d</xref>, <xref rid="fig1" ref-type="fig">1e</xref>, S1a, c, d, e, and f; Suppl Movies S1; Suppl Design files: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">https://github.com/vickerse1/mesoscope_spontaneous</ext-link>; <ext-link ext-link-type="uri" xlink:href="http://Labmaker.org">Labmaker.org</ext-link> for dorsal mount; TLC International for cutting, and GlasWerk for bending, for the side mount and early prototypes of the dorsal mount). We incorporated these custom made headposts and cranial windows into the dorsal mount (<xref rid="fig1" ref-type="fig">Figs. 1a, d, Fig</xref>. S1c upper, S1d upper left, S1e), and side mount (<xref rid="fig1" ref-type="fig">Figs. 1b, e, S1a, c</xref> lower, d lower left and right, and upper right, S1f) preparations.</p>
<p>To mimic the curvature of the brain and reduce tissue compression, which is a problem with flat coverslips with a diameter larger than ∼3 mm, cranial windows were curved by heating pre-cut (TLC International) glass pieces over 9 or 10 mm bend-radius molds (GlasWerk). The bend radius (i.e. radius of half cylinder mold over which the melted glass is bent to achieve its curved shape) was fixed at 10 mm for <ext-link ext-link-type="uri" xlink:href="http://Labmaker.org">Labmaker.org</ext-link> dorsal mount windows (Fig. S1e, right; Suppl Movie S1 dorsal mount; early attempts with 11 or 12 mm bend radii failed), or, for windows custom designed in collaboration with GlasWerk (Fig.S1f, right; Suppl Movie S1 side mount), the bend radius was set at either 9 mm, for a tight fit to ventral auditory areas, or at 10 mm, to enable simultaneous imaging of the entire preparation at a single focal depth.</p>
<p>Within a field of view of 5x5 mm, curvature of the brain, especially in the medial-lateral direction, is a significant problem. To partially compensate for brain curvature, online field- curvature correction (FCC) was applied in ScanImage (Vidrio Technologies, MBF Biosciences) during mesoscope image acquisition. In addition, complementary fast-z “sawtooth” or “step” corrections were applied when all ROIs were acquired at the same or different z-planes, respectively (see <xref ref-type="bibr" rid="c38">Sofroniew et al, 2016</xref>). Despite this, small uncorrected discrepancies in the depth of the imaging plane below the pial surface may have remained, especially along the long-axis of acquisition ROIs (i.e. mediolateral). It is likely that optical effects of the curvature of the glass partially compensated for these differences in targeted imaging depth, although we did not quantify this effect. In cases where we imaged multiple small ROIs, nominal imaging depth was adjusted in an attempt to maintain a constant relative cortical layer depth (i.e. depth below the pial surface; ∼200 LJm offset due to brain curvature over 2.5 mm mediolateral distance, symmetric across the center axis of the window).</p>
<p>The mouse was restrained by fixation of the headpost with two countersunk screws to either a fixed (Fig. S1b, top) or adjustable aluminum support arm (custom; <xref rid="fig1" ref-type="fig">Figs. 1a, b, S1b</xref>, bottom) and mounting apparatus (<xref rid="fig1" ref-type="fig">Figs. 1a</xref>, b). For the dorsal mount preparation, two headpost support arms, one on each side of the head, were used (<xref rid="fig1" ref-type="fig">Fig. 1a, S1c, top</xref>; outer screw of both headpost wings used) while for the side mount preparation, a single, left side support arm was sufficient (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>, S1c, bottom; inner and outer screws in the left headpost wing were both used). It was important to fix the headpost mounting arms to the top of the airtable with 1 inch diameter vertical mounting posts (Thorlabs RS6P8, RSH4, PF175), making sure all joints were clean of debris and tightened securely, to reduce movement artifacts.</p>
<p>Both preparations were found to be relatively free of vibration and movement artifacts and allowed for micro-adjustments (i.e. pitch, yaw, and roll) of the mouse relative to both the objective and the running wheel (<xref rid="fig1" ref-type="fig">Fig. 1a, b, c</xref>). Increased positioning flexibility of the support-arm assembly was achieved through either of 2 ways: 1) by positioning it on a distal base-attached ball-joint mount (Thorlabs, SL20 articulating base; not shown); 2) through the use of a custom adjustable headpost support arm that was proximally adjusted around a ball-and-joint assembly (Fig. S1b, bottom). This adjustable support arm operated through the use of a custom wrench and reverse-threaded (left-handed) nut for initial coarse tightening, and dual micro-clamps for rotational stabilization during tightening. Four micro-hex wrench driven set-screws, two on either side of the ball-and-joint fitting, were used to achieve the final, fully locked state for imaging. The adjustable support arm was found to be superior to the base-attached ball joint mount in allowing for micro-positioning of the animal in relation to the running wheel, objective, and lick spouts after initial fixation (mounting) of the headstage to the support bar.</p>
<p>In order to perform 2-photon neuronal imaging, the large 2p-RAM water-immersion objective (∼10x net magnification, 0.6 NA, ∼1.3 kg, 12 mm diameter tip, 25.6 mm shaft) must be positioned between 2.2 and 2.8 mm (i.e. ∼working distance minus window thickness) from the curved glass surface of the cranial window while maintaining a stable water meniscus. Providing a base for the water meniscus while also blocking incident light entry was achieved by creating a custom 3D printed plastic light shield (AutoDesk Inventor, MakerGear M3 printer, PLA) that was attached to the protruding, fitted rim of the headpost with 170 FAST CURE Sylgard (“wok one”; <xref rid="fig1" ref-type="fig">Fig. 1c</xref>, S1d). A second light shield (“wok two”) served to further block light entry into the objective and was attached to the base of the lower light shield (“wok one”) by fitting of a U-shaped profile over a vertically protruding single edge profile along the perimeter of the first, lower light shield (“wok one”; <xref rid="fig1" ref-type="fig">Fig. 1c</xref>). Together, these two custom-printed light shields prevented incident light (from the video stimulus monitor, the ultraviolet LEDs used for controlling the baseline and dynamic range of pupil diameter, and the infrared LEDs used to illuminate the mouse) from entering the imaging objective and thereby either contaminating or tripping the photomultiplier tubes (PMTs). Line-of-sight for the animal to the video stimulus monitor was retained, and this design allowed for free vertical and rotational (over a limited range) movement of the objective lens, which was contacted directly only by the water meniscus.</p>
<p>Direct left, right, and posterior camera angles, or “lines of sight”, were preserved to enable reliable recording and proper illumination of pupil diameter, whisker pad movement, and the movement of other body parts such as the nose, mouth, ear, paws, and tail (<xref rid="fig1" ref-type="fig">Fig. 1a, b</xref>; see Suppl Movies S1). Adjustable positioning of two conductance-based lick spouts for 2-alternative forced choice (2-AFC) task performance (lick left, lick right) and reward delivery was achieved with a rapidly translatable motorized linear stage (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>; 63.7 ms total travel time over 7 mm; Zaber Technologies). This system allowed for rapid withdrawal of lick spouts between trials, and presentation of the lick spouts during the response period of each trial.</p>
</sec>
<sec id="s3b">
<title>Widefield imaging and optogenetic stimulation in the 2p-RAM mesoscope</title>
<p>Examining neuronal activity at the single cell level and relating it to stimulus-evoked activity observed at the widefield level (<xref rid="fig2" ref-type="fig">Figs. 2</xref>, 3), as well as manipulating regions of the cortex through localized light delivery and optogenetics (see Suppl Movie S7), required precise optical alignment across all three of these methods. First, a standardized cortical map needed to be fitted onto images of skull landmarks and the cortical vasculature by multimodal sensory widefield mapping (<xref rid="fig2" ref-type="fig">Figs. 2</xref>, 3; a,b, c, top). Then, a method was needed for transferring this cortical map onto the field of view of the mesoscope so that each individual neuron could be assigned to a cortical area for subsequent analyses (<xref rid="fig2" ref-type="fig">Figs. 2</xref>, 3; c, bottom, d). Finally, the spatial coordinate system of the 1p opto-stimulation laser routed in through an auxiliary light-path of the mesoscope needed to be aligned to that of the 2p laser so that specific cortical subregions could be targeted for optogenetic inhibition by light activated, ChR2-mediated excitation of parvalbumin interneurons in Thy1-RGECO x PV-Cre x Ai32 mice (Suppl Movie S7).</p>
<p>In order to align 2p maps of single neuron activity with functional 1p cortical maps determined through pseudo-widefield imaging (i.e. combined reflected and fluorescence light imaging with a standard format CCD camera), we designed a method for reliably aligning a standardized cortical map to an image of the cortical vasculature pattern, which can be directly visualized with both widefield and 2p imaging techniques for each brain (see Protocol V). The epifluorescence light source (Excelitas) for pseudo-widefield imaging and visualization of vasculature for targeting of the 2p FOV was controllable by a dial, shutter, and remote foot-pedal for optimal ease of positioning the objective in the water meniscus at a distance of ∼2.2 mm from the headpost and cranial window (<xref rid="fig1" ref-type="fig">Figs. 1c</xref>, and S1d, top and bottom right).</p>
<p>This technique required several modifications of the auxiliary light-paths of the Thorlabs mesoscope. For switchable blue/green widefield imaging and 2p imaging, we used a 469/35 Semrock excitation filter, 466/40 Semrock dichroic (1st, top cube), and mirror (2nd, bottom cube). For this standard configuration, which we used with GCaMP6s mice, an electronically positionable dichroic (T580-160, Chroma) was positioned in the light-path just upstream of the mesoscope objective to enable the user to switch on-the-fly between widefield imaging (i.e. dichroic in light path) and stand-alone 2p imaging (i.e. dichroic out of light path). Pseudo-widefield images (ThorCam) and 2p images (ScanImage) were aligned to be parfocal so that the user could switch directly from the vasculature image at a desired CCF location to the 2p image.</p>
<p>For the combination of pseudo-widefield imaging and rapid, targetable optogenetic 1-photon (1p) stimulation with concurrent 2p imaging, which we used with PV-Cre x Ai32 x Thy1-RGECO mice (Suppl Movie S7), we established dual coupling of a broadband fluorescence light-source (Excelitas) and a 473 nm laser driver (SF4C 473, Thorlabs) to converging, dual input auxiliary light paths of the Thorlabs mesoscope via two in-series switchable filter cubes (DFM1T1 cube, Thorlabs). The 473 nm laser driver was connected to a single open loop, high speed buffer (50 LD, Thorlabs), and targeted to the coordinate system of the 2p laser with a grid-calibrated, auxiliary galvo-galvo scanner (GVSM002, Thorlabs). This allowed both orange epifluorescence excitation light and collimated light from the blue 1p laser to reach the mouse via auxiliary light paths, and for 2p imaging and blue 1p laser optogenetic stimulation to occur pseudo-simultaneously, in that an electronically controlled shutter was positioned to block the light path to the PMT and interrupt image acquisition only during the brief presentation of the optogenetic stimulus (∼100 ms; see Suppl Movie S7).</p>
</sec>
<sec id="s3c">
<title>Reduction of resonant scanner noise</title>
<p>Resonant scanners in 2p microscopes emit intense sound at the resonant mirror frequency, which is well within the hearing range of mice (∼12.5 kHz emitted in the Thorlabs mesoscope). Because scanning precision requires the scanner to remain at a stable, elevated temperature, the scanner must remain on during the entire experimental session (i.e. up to ∼2 hrs per mouse). The unattenuated or native high-frequency background noise generated by the resonant scanner causes stress to both mice and experimenters, and can prevent mice from achieving maximum performance in auditory mapping, spontaneous activity sessions, auditory stimulus detection, and auditory discrimination sessions/tasks.</p>
<p>To reduce this acoustic noise, we encased the resonant scanner and attached light path tubes with a custom 3-dimensional (3D)-printed assembly containing dense interior insulating foam (see Suppl Materials: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">https://github.com/vickerse1/mesoscope_spontaneous</ext-link>; “3D scanning and honeycomb patterned nylon print” (University of Oregon Innovation Disclosure #DIS-23-001, US provisional patent application UOR-145-PROV). It was critical to use 3D scanning of encased components in the design of the noise reduction shield so that the assembly would closely follow all of the surface contours and fit accurately in spaces with tight tolerances. The result of this encasement was a large reduction in resonance scanner sound, from ∼60 dB to ∼5 dB measured at the head of the mouse (Bruel and Kjaer ¼” pressure filled microphone 4938-A-011; i.e. below the mouse hearing threshold at 12.5 kHz of roughly 15 dB). By comparison, encasements designed using standard 3D-printing techniques (i.e. not based on a 3D scan) were only able to achieve a noise reduction of ∼30 dB, to a level still audible to both mice and humans.</p>
</sec>
<sec id="s3d">
<title>Cortical alignment to the common coordinate framework v3.0 (CCF) map</title>
<p>Interpretation of the diversity of activity of thousands of neurons simultaneously identifiable with the mesoscope requires proper cortical areal localization, including that of areas far from primary sensory cortices. Such mapping is not routinely possible directly on the 2-photon mesoscope when performing neuronal-level imaging, due to both the heterogeneity of responses within primary sensory cortices at the single neuron level, and to animal-to-animal variations in the spatial extent of cortical regions (<xref ref-type="bibr" rid="c10">de Vries et al, 2020</xref>; <xref ref-type="bibr" rid="c3">Bimbard et al, 2023</xref>). To facilitate the assignment of neurons to cortical areas, we sought to align the Allen Institute Common Coordinate Framework (CCF; <xref ref-type="bibr" rid="c49">Wang et al, 2020</xref>) to our 2p neuronal imaging results, using blood vessels, skull landmarks, and widefield imaging responses as intermediaries.</p>
<p>Alignment of the CCF to the dorsal surface of cortex and subsequent image-stack registration based on widefield imaging Ca<sup>2+</sup> fluorescence data has been performed elsewhere with a variety of techniques. These include the use of skull landmarks (e.g. bregma and lambda; <xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>), responses to unimodal sensory stimuli (Gallero-Helmchen, 2021), visual field mapping (<xref ref-type="bibr" rid="c58">Zhuang et al, 2017</xref>), and/or autocorrelation maps of spontaneous activity (<xref ref-type="bibr" rid="c33">Peters et al, 2021</xref>). Few studies, however, have performed such alignments with rotated cortex (i.e. imaging lateral portions of the cortex at an angle; but, see <xref ref-type="bibr" rid="c11">Esmaeili et al, 2021</xref>).</p>
<p>To summarize our approach, we used a technique where, for both the dorsal mount (<xref rid="fig2" ref-type="fig">Figs. 2</xref>, S2a) and side mount (<xref rid="fig3" ref-type="fig">Figs. 3</xref>, S2b) preparations, we first imaged 1p widefield GCaMP6s responses to unimodal passive sensory stimulation (e.g. visual, auditory, and somatosensory) to create a multimodal map (MMM) consisting of multiple sensory area masks on top of the cortical vasculature. We then overlaid skull landmarks (e.g. bregma and lambda), imaged with reflected green light, onto the MMM. The Allen CCF was then warped onto the vasculature/skull/MMM overlay, and the mean 2p image was warped onto the MMM by vasculature alignment. Finally, each neural ROI in the 2p image was assigned to a position in the MMM x-y coordinate system and a corresponding CCF area based on the final overall alignment of the MMM, CCF, and 2p image. The following sections describe these steps in more detail.</p>
</sec>
<sec id="s3e">
<title>Widefield multimodal mapping</title>
<p>To create the multimodal map, each mouse was put under light isoflurane anesthesia (∼1.5%) after head post surgery, but before implantation of the cranial window, and exposed to 5 minutes each of full-field visual (vertical and horizontal stationary grating patches; 0.16 cpd, 30 deg; <xref ref-type="bibr" rid="c26">Michaiel et al, 2019</xref>), tone-cloud auditory (a series of overlapping 30 ms duration tones randomly selected from a frequency range of 5-40 kHz and presented at 100 Hz; <xref ref-type="bibr" rid="c54">Xiong et al, 2015</xref>), and piezo-driven whisker deflection (or, in some cases, forelimb and trunk stimulation; see Gallero-Helmchen, 2021). For whisker deflection, a 5 Hz burst of 100 ms duration was used, consisting of posterior to anterior sweeps. The whisker deflector was a custom 3D-printed triangular polylactic acid (PLA) piece mounted on a 21-gauge needle and attached to a PL140.11 piezo-actuator with epoxy glue. It was actuated with a Physik-Instrumente controller driven by a custom pulse sequence generated in Spike2 and delivered from an analog output of a CED Power 1401, with an inter-stimulus interval of ∼10 s (<xref rid="fig2" ref-type="fig">Figs. 2b</xref>, right, and 3b, right; see also Suppl Movies S2 and S3).</p>
<p>Averaged, baseline-subtracted dF/F responses (<xref rid="fig2" ref-type="fig">Figs. 2b</xref>, <xref rid="fig3" ref-type="fig">3b</xref>) were then thresholded, masked, outlined, and layered onto the blood vessel image (<xref rid="fig2" ref-type="fig">Figs. 2a</xref>, c, <xref rid="fig3" ref-type="fig">3a</xref>, c, S2a, b) along with bregma and lambda skull landmarks (Fig. S1a) from 530 nm (green) reflected-light skull-imaging using a custom protocol/macro in Fiji (ImageJ; <xref rid="fig2" ref-type="fig">Figs. 2c</xref>, <xref rid="fig3" ref-type="fig">3c</xref>, S2a, b). This multimodal mapping procedure and alignment to the CCF and skull landmarks was repeated if significant changes in vasculature pattern occurred over the days/weeks of the experiment. More precise maps of visual cortical areas can be achieved through visual field mapping using a topographic stimulus consisting of a bar sweeping in azimuth or elevation (<xref ref-type="bibr" rid="c13">Garrett et al, 2014</xref>). This technique was not applied here, because our goal was global alignment of the Allen Institute cortex-wide CCF to our dorsal or side mount views, and not precise alignment to visual subareas <italic>per se</italic>.</p>
</sec>
<sec id="s3f">
<title>Co-alignment of 2p image to vasculature and CCF</title>
<p>The MMM was aligned to an overlay of the rotated CCF edges and region of interest (ROI) masks using custom Python code and the built-in function “PiecewiseAffineTransform”, given a user-supplied series of bounding-box and alignment points common to both images (<xref rid="fig2" ref-type="fig">Figs. 2c</xref> (upper, part 1), 3c (upper, part 1)). A second, similar alignment was then performed between the multimodal map and all neural ROI locations (i.e. Suite2p-identified neurons) relative to the 2p image plane; here, vasculature is inferred from the pattern of gaps (e.g. vascular “shadows”) in the spatial distribution of neurons (<xref rid="fig2" ref-type="fig">Figs. 2c</xref> (lower, part 2), 3c (lower, part 2)), and can be confirmed by parfocal pseudo-widefield (i.e. combined reflected and epifluorescence light) imaging directly on the 2p Thorlabs mesoscope. Note that in 2p imaging below the pial surface, the effective/apparent width of the vasculature, or its “shadow”, is significantly larger than that of the actual blood vessel and its width increases with depth. The transforms resulting from each of these two alignments were then applied in serial (i.e. multimodal map to CCF, then multimodal map to 2p neural image; <xref rid="fig2" ref-type="fig">Figs. 2c</xref> and <xref rid="fig3" ref-type="fig">3c</xref>) to overlay the neural image directly onto the cortical map, using outlines, and to assign a unique CCF area identifier (i.e. name and ID number) to each Suite2p-extracted, 2p-imaged neuron using CCF masks (<xref rid="fig2" ref-type="fig">Figs. 2d</xref>, <xref rid="fig3" ref-type="fig">3d</xref>).</p>
</sec>
<sec id="s3g">
<title>2-photon imaging across broad regions of the dorsal and lateral cortex in behaving mice</title>
<p>Previous 2-photon imaging studies have demonstrated that arousal and/or orofacial/body movement can explain a significant proportion of the variance in spontaneous and sensory evoked neural responses in visual and other restricted dorsal cortical regions (<xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>). Here, as a proof of principle, we examined the generality of this finding by simultaneously monitoring neuronal activity across broad regions of the dorsal and lateral cortex with our dorsal and side mount preparations. Previous studies suggest that there may be both commonalities as well as heterogeneity in the effects of changes in arousal/movement on spontaneous and sensory-evoked responses in different cortical areas (<xref ref-type="bibr" rid="c25">McGinley et al, 2015</xref>; <xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>). For example, running typically enhances the gain of visually evoked responses in the mouse primary visual cortex (<xref ref-type="bibr" rid="c30">Niell and Stryker, 2010</xref>), while it significantly decreases that of evoked auditory responses in primary auditory cortex (<xref ref-type="bibr" rid="c57">Zhou et al, 2014</xref>; <xref ref-type="bibr" rid="c25">McGinley et al, 2015</xref>).</p>
<p>Assessing such potential differences between functionally distinct, and potentially distant, cortical areas requires simultaneous imaging across multiple cortical regions while maintaining adequate imaging speed, quality, and resolution. To achieve this level of 2p imaging across several millimeters of cerebral cortex (e.g. from visual to motor or auditory cortical areas), we first needed to optimize the parameters of our mesoscope imaging methods and protocols.</p>
<p>Conventional 2-photon imaging using a preparation similar to our dorsal mount (“crystal skull”; <xref ref-type="bibr" rid="c23">Kim et al., 2016</xref>) previously employed serial acquisition of ∼1 x 1 mm FOVs (<xref ref-type="bibr" rid="c23">Kim et al, 2016</xref>). Other 2p Thorlabs mesoscope imaging studies have used acquisition protocols targeting z-stacks of 600 x 600 µm FOVs in barrel cortex (3 planes at 7 Hz, 1.17 µm/pixel; <xref ref-type="bibr" rid="c32">Peron et al, 2015</xref>), 900 x 935 µm FOVs in visual cortex (11 planes at ∼1 Hz; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>), and three adjacent 600 x 1800 µm FOVs in CA1 hippocampus (<xref ref-type="bibr" rid="c43">Sun et al, 2023</xref>; bioRxiv). Here, with our dorsal and side mount preparations, we were able to routinely image between 2000 and 7600 neurons per session over up to ∼25 mm<sup>2</sup> of dorsal cortex simultaneously at ∼3 Hz with a resolution of 5 µm/pixel in GCaMP6s mice (combined total in both preparations including all large-FOV sessions: N=17 mice, n=91 sessions, ∼350,000 neurons; Fig. S2e, Suppl Movie S4).</p>
<p>In order to achieve spatially broad and dense sampling, while monitoring changing activity in each neuron as frequently as possible, we typically tiled 7 mediolaterally-aligned 5000 x 660 µm FOVs (i.e. mechanical scanning along a long-axis oriented mediolaterally, combined with fast resonant-scanning across multiple short-axes oriented anterior-posterior) over either posterior-medial, posterior-lateral, or anterior dorsal, cortex at a pial depth of ∼200-300 µm (cortical layers 2/3). Bidirectional scanning (with a scan-phase correction of between 0.9-1.0) and field-curvature correction were typically enabled, with a 1 ms “flyback” and “frame” time, which are defined in ScanImage as the time delay for retargeting of the laser from the end of one line or frame acquisition to the beginning of the next, respectively. Here, there is a trade-off between longer flyback times, which reduce imaging speed but minimize positioning errors at the beginning of each scan line or ROI, and shorter values, which accelerate overall imaging acquisition speed but increase these positioning errors.</p>
<p>We typically imaged GCaMP6s fluorescence in awake, behaving mice at an excitation wavelength of 920 nm with 60-95 mW power calibrated at the pia over 20-90 minute sessions, with little to no bleaching or change in baseline fluorescence. Suite2p reliably extracted neurons with &gt;0.9 classifier probabilities, and nearly completely (i.e. to less than 0.1 pixels) removed x-y movement artifacts with uncorrected, raw principal components of up to ∼1.3 pixels (i.e. up to ∼15% of a 20 to 40 µm, or 4 to 8 pixels, diameter neuron at an imaging resolution of 0.2 µm/pixel, reduced to less than 0.5 µm or 1-2% of the diameter of a neuron after combined rigid and non-rigid motion correction).</p>
<p>To further control for z-movement artifacts, whose correction is intractable post-acquisition under normal scanning conditions, we also occasionally used GPU-enabled online fast-z motion correction (42/52 large FOV side mount sessions, and 9/39 large FOV dorsal mount sessions, used online z-correction) with ScanImage (Vidrio Technologies, LLC). This was only possible, in our hands, during multi-frame imaging when all frames were acquired at the same z-level. Quality of post-hoc motion-corrected dF/F traces and correlations with behavioral variables did not appear, by eye, to be different under these conditions (Fig. S3a vs b), as both sessions contained similar mixtures of neurons with walk-bout dependent, and walk-bout independent, activity, for example. Because the z-motion correction did not seem to lead to large qualitative differences in neural activity and its relationship to behavioral primitives, both types of session are used interchangeably here.</p>
<p>In order to confirm neural region-of-interest (ROI) selection (i.e. each neuron as identified by a spatiotemporal footprint in Suite2p), increase scan speed, and correct our cortical layer 2/3 targeting for the effects of brain curvature, we also routinely imaged in random access mode with 4-6 small FOVs (660-1320 x 660 µm) positioned over visual (V1), somatosensory (S1), retrosplenial (RSpl), posterior parietal (PPC), and/or motor (MOs_pm, MOs_am, and/or MOs_al - ALM) cortices at independently controlled z-levels, with an imaging speed of 4-10 Hz and a resolution of 1.0 – 2.0 µm/pixel (see Protocol I; Fig. S2d, e). These sessions yielded hundreds of high quality neurons per FOV, at a density roughly twice that of the lower resolution sessions (Fig. S2c vs. d).</p>
<p>Uncontrolled image motion in behaving animals is a significant impediment to image quality (<xref ref-type="bibr" rid="c31">Pachitariu et al, 2016</xref>). We have taken significant care in our headpost design and fixation (see <xref rid="fig1" ref-type="fig">Figs. 1</xref> a-b, S1 a-f, and Supplementary Methods and Materials) so that our preparations are highly stable, such that motion-corrected or “registered” movies showed very little to no residual movement in either large FOV, low-resolution, or multiple small FOV, high-resolution sessions (Suppl Movies S5, S7). Furthermore, movement-related activity across neighboring detected ROIs in large FOV, low-resolution sessions was correlated but non-identical upon detailed examination, and differed from dF/F Ca<sup>2+</sup> fluorescence fluctuations detected in nearby non-ROI and blood-vessel regions of similar cross-sectional area (see Suppl Movies S5, S7). In theory, movement artifact signals should be relatively time-locked or nearly synchronous across cortical regions during vigorous walking or movement. However, we did not observe this in our preparations.</p>
<p>These findings, therefore, gave us confidence that neural activity changes detected during spontaneous movements of the mouse were due to real changes in Ca<sup>2+</sup> fluorescence within each neuron, and not to movement of the brain causing the neuron to shift into or out of each Suite2p-defined ROI and leading to artifactual fluorescence transients (see Suppl Movies S5 and S7, which use blood vessels and inactive fluorophore/neuropil, respectively, as examples of the lack of significant movement-driven artifactual transients in GCaMP6s(-) structures in our preparations). In other studies performed using the same setup and methods as those applied here, we have demonstrated that non-activity dependent fluorescent markers, such as the expression of mCherry in cholinergic or noradrenergic axons, or autofluorescent blebs, exhibit no significant movement-related changes in fluorescence, indicating that movement artifacts are minimal in our preparations (<xref ref-type="bibr" rid="c9">Collins et al, 2023</xref>). It should be noted that axonal diameters are on the order of ∼1 µm or less - which is significantly smaller than the diameter of the neuronal cell bodies (∼15-30 µm) imaged here. Axons, therefore, are a more stringent test of the stability of our imaging preparations.</p>
</sec>
<sec id="s3h">
<title>Modulation of neural activity by spontaneous movements and arousal as observed with the dorsal mount preparation</title>
<p>Recent work has shown that sorting fluorescence based 2p GCaMP cortical neuronal activity with an algorithm called Rastermap is a powerful method to reveal clusters of cells that exhibit similar patterns of activity (<xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv). An example of such sorting of our data for a dorsal mount session of 3096 neurons is shown in <xref rid="fig4" ref-type="fig">Figure 4a</xref> (top). This method for one-dimensional nonlinear embedding works by sorting and clustering of ROIs (i.e. neurons) by similarity of neural activity patterns across multiple timescales, using k-means clustering, sorting by asymmetric similarity as defined by peak cross-correlations at non-negative timelags, and upsampling of individual cluster activities in principal components space to allow sorting within clusters (<xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv). Here, sorting with Rastermap revealed stark clustering of neurons with clearly distinguishable activity patterns that exhibited heterogeneous relationships to the behavioral primitive arousal/movement variables of walk speed, whisker motion energy, and pupil diameter (<xref rid="fig4" ref-type="fig">Fig. 4a</xref> top, b; Suppl Movie S4).</p>
<p>The activity generated by these thousands of neurons was clearly not random, with sub-groups of cells exhibiting similar activity patterns. Many neurons exhibited activity that appeared to be coupled to walking/whisking (cf. <xref rid="fig4" ref-type="fig">Fig. 4a</xref> top, b). It has previously been shown that the first principal component of cortical neuronal activity is highly correlated with movement (<xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>). Indeed, sorting our neurons based on degree of activity loading onto their first two principal components (i.e. neurons at the top of the sort have the largest amount of activity accounted for by the first principal component of activity across the entire population) revealed several distinct patterns of activity. The first principal component, PC1 (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>, middle) demonstrated a clear relationship to walking/whisking, such that some neurons were strongly activated in association with movement (e.g. <xref rid="fig4" ref-type="fig">Fig. 4a</xref>, c1 in PC1, middle, and 4c, top), while other neurons appeared to be deactivated during movement bouts (e.g. <xref rid="fig4" ref-type="fig">Fig. 4a</xref>, c2 in PC1, middle, and 4c, bottom).</p>
<p>Closer examination revealed that some Rastermap and PC1 clusters were active primarily during walking (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>, top, <xref rid="fig1" ref-type="fig">d1</xref>, <xref rid="fig4" ref-type="fig">Fig. 4a</xref>, middle, c1, and <xref rid="fig4" ref-type="fig">Fig. 4d</xref>1) or whisking, with either transient or sustained activity, while others were either active during low arousal periods (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>, middle, c2) or appeared to display a combination of transient whisking-related activity (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>, top, <xref rid="fig2" ref-type="fig">d2</xref>, and <xref rid="fig4" ref-type="fig">Fig 4d</xref>, right; see Fig. S4). In the illustrated example, alignment of marked increases and decreases in bulk population neural activity to eight brief walking bouts and corresponding whisking bouts and pupil dilations (red and blue arrowheads, respectively; <xref rid="fig4" ref-type="fig">Fig. 4a-c</xref>) was more clearly seen following ROI sorting by first and second principal components (<xref rid="fig4" ref-type="fig">Fig. 4a</xref> middle and bottom; PC1 and PC2, respectively, <xref rid="fig4" ref-type="fig">Fig. 4c</xref>) than in Rastermap sorting (<xref rid="fig4" ref-type="fig">Fig. 4a</xref> top).</p>
<p>Interestingly, separation of the top and bottom fifth of neurons in the example data set sorted by PC1 (<xref rid="fig4" ref-type="fig">Fig. 4a</xref> middle, c1 and c2, respectively; <xref rid="fig4" ref-type="fig">Fig. 4c</xref> top and bottom, respectively) showed large groups of neurons with clear positive (<xref rid="fig4" ref-type="fig">Fig. 4c</xref> (1), top) and negative (<xref rid="fig4" ref-type="fig">Fig. 4c</xref> (2), bottom) correlations with arousal/movement, as demonstrated by alignment of rasterized neural activity with walk and whisker transients, and local maxima in pupil diameter (see <xref rid="fig4" ref-type="fig">Fig. 4b</xref>). Specifically, neurons with high PC1 values (<xref rid="fig4" ref-type="fig">Fig. 4</xref> c1) showed elevated activity during periods of increased arousal/movement, while neurons with low PC1 values (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>2) showed depressed activity levels during these periods. Subsets of these neurons showed either elevated basal activity with suppression during periods of high activity corresponding to arousal and/or movement (<xref rid="fig4" ref-type="fig">Fig. 4c</xref> (2), top of Rastermap), or transient increased activity leading up to movement/arousal (<xref rid="fig4" ref-type="fig">Fig. 4c</xref> (2), bottom of Rastermap), followed by transient suppression (see Suppl Movie S4). Examination of individual “raw” dF/F traces confirmed that these differences were not due to artifacts of z-scoring or superneuron averaging (<xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv) in the PC and Rastermap sorted displays (not shown).</p>
</sec>
<sec id="s3i">
<title>Spatial distribution of neurons related to movement/arousal</title>
<p>Performing simple correlations between dF/F in each neuron with either whisker movements, walking, or pupil diameter revealed a broad range of values (Fig. S6b; whisk -0.4 to +0.6, pupil -0.6 to +0.6, and walk -0.2 to +0.4; same mouse as <xref rid="fig4" ref-type="fig">Fig.4</xref>, but a different session). The population distributions of these correlations were highly skewed (positive skew for whisk and walk, negative skew for pupil diameter) with modes at small positive values (Fig. S6a, b). Plotting the spatial location of neurons by correlation value within the cortical map area revealed a broad distribution across the dorsal cortex of neurons whose activity was either strongly negatively or positively (or in between) correlated with walking, whisking, and/or pupil diameter, inclusively across every CCF subregion imaged (Fig. S6a-c).</p>
<p>At the local spatial scale (Fig. S6a, colored dots indicate individual neurons), neurons whose GCaMP6s activity was strongly positively correlated with movement or pupil diameter could often be found near neurons whose activity exhibited a wide variety of correlations, from strongly positive to strongly negative. Plotting the average correlation value for neurons within a CCF region revealed modest yet statistically significant (see Fig. S6c, legend) spatial heterogeneities (<xref rid="fig4" ref-type="fig">Figs. 4e</xref>, S6) that were evident both in each of two different single sessions (<xref rid="fig4" ref-type="fig">Figs. 4e</xref>; S6a, b) and across sessions (Fig. S6c, mean of 8 sessions from the same mouse).</p>
<p>To examine the spatial distribution of different Rastermap groups, we created heatmaps of the density of cells in each CCF area that belong to a particular Rastermap group (i.e. percent of cells in each region belonging to that Rastermap group; <xref rid="fig4" ref-type="fig">Fig. 4d</xref>; only areas with at least 20 cells were considered). A Rastermap group with increased activity in relationship to walking and whisking bouts (Rastermap cell group d1 in <xref rid="fig4" ref-type="fig">Fig. 4a, top, and</xref> S4a) had a CCF density distribution ranging from 0 to 25% and was concentrated in MOs, SSn, VISrl, and VISa (CCF neural density map; <xref rid="fig4" ref-type="fig">Fig. 4d</xref>, left). A second example Rastermap group, d2 (<xref rid="fig4" ref-type="fig">Figs. 4a, top, and</xref> S4c) exhibited increases in fluorescence in relationship to whisking, but not as strikingly during bouts of walking. This cell group had a CCF density distribution ranging from 0 to 13% and was concentrated in SSun, SSb, and AUDd (<xref rid="fig4" ref-type="fig">Fig. 4d</xref>, right).</p>
<p>Next, we compared the proportion of neurons within each CCF that exhibited large positive correlations with walking speed and whisker movement energy. In this example dorsal mount preparation these walk and whisk related neurons formed the largest relative proportion of cells in the CCF areas MOs, SSll, and SSn (<xref rid="fig4" ref-type="fig">Fig. 4e</xref> left and right, respectively). Mean correlations for each CCF area were generally statistically significantly greater than zero (see <xref rid="fig4" ref-type="fig">Fig. 4e</xref>, legend).</p>
</sec>
<sec id="s3j">
<title>Modulation of neural activity by spontaneous movements and arousal as observed with the side mount preparation</title>
<p>To extend the potential diversity of neural activity patterns during spontaneous behavior, and to examine the generality of the observation of movement/arousal related neuronal activity to the lateral cortex, we monitored neuronal activity in spontaneously behaving mice with our lateral window preparation. We imaged ∼85 sessions in the side mount preparation (20-90 minutes each, including both large- and multi-FOV sessions) across 11 GCaMP6s mice, for a total of ∼320,000 neurons (Fig. S2e; Suppl Movie S6). In an example imaging session (<xref rid="fig5" ref-type="fig">Fig. 5</xref>) of a mouse implanted with our custom 3D-printed titanium side mount headpost (<xref rid="fig1" ref-type="fig">Figs. 1e</xref>, S1f) and 10 mm radius bend cranial window (<xref rid="fig1" ref-type="fig">Figs. 1e, S1a, d, f</xref>; Suppl Movie S1), we were able to image 5678 neurons at 5 x 5 µm/pixel resolution over a combined 5.0 x 4.62 mm FOV (<xref rid="fig5" ref-type="fig">Fig. 5</xref>).</p>
<p>As in the dorsal mount (<xref rid="fig4" ref-type="fig">Fig. 4</xref>), Rastermap sorting of neurons across this single side mount example session (<xref rid="fig5" ref-type="fig">Fig. 5</xref>) was performed in order to further explore the broad range of relationships between movement/arousal measures and neural activity. As in the dorsal mount preparation, Rastermap sorting showed non-random patterns of neuronal activity that were often related to movement/arousal measures. Here, we identify and further characterize both a cluster with increased activity in relationship to bouts of walking/whisking (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, top, cell group d1; see Fig. S5; a, b), and another cell cluster whose activity appeared, by eye, to be more related to whisking than walking, <italic>per se</italic> (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, top, cell group d2; see Fig. S5; c, b). Interestingly, these two groups consisted of ∼300 neurons each spread across the right lateral cortex, with some neurons nearby in the same CCF areas, and others in distant, functionally distinct regions (<xref rid="fig5" ref-type="fig">Fig. 5d</xref> left, right).</p>
<p>Sorting by the first principal component (PC1) of activity, as in our dorsal mount preparation (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>, middle), revealed that a majority of imaged neurons exhibited activity related to bouts of walking/whisking (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, middle; walk bouts indicated by red arrowheads, whisk bouts indicated by blue arrowheads; see Suppl Movie S6). The top fifth of these movement-activated neurons showed activity nearly completely dominated by walk-related signals (<xref rid="fig5" ref-type="fig">Figs. 5a</xref>, middle, c1, and 5c, top, (1)), while the bottom fifth showed activity negatively correlated with walking (<xref rid="fig5" ref-type="fig">Figs. 5a</xref>, middle, c2, and 5c, bottom, (2)). Sorting by the second principal component (PC2), on the other hand, appeared to show increases in activity more closely aligned to the onset of whisking movements (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, bottom). Interestingly, some neurons in PC2 exhibited sustained inter-walk bout activation (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>, bottom).</p>
<p>As in our dorsal mount example session (Fig. S6, a-c), we examined the overall spatial pattern of correlations in a different side mount example session with 3897 recorded neurons (Fig. S6, d-f). Here, correlations of individual neuron activity with whisker motion energy, pupil diameter, and walk speed ranged from -0.6 to +0.6 (Fig. S6d, e; whisk -0.4 to +0.6, pupil -0.4 to +0.4, and walk -0.2 to +0.5) with positively skewed distributions (except for pupil diameter) and modes at small positive values (Fig. S6d, e). Strong heterogeneity of these correlations, as in the dorsal preparation example (Fig. S6, a-c), was evident at both local (Fig. S6d, e, single session) and regional (Fig. S6f, mean of 8 sessions from the same mouse) spatial scales.</p>
<p>Examining the regional variations in mean correlations between neural activity and walk speed (Fig S6f, bottom, in red), pupil diameter (Fig S6f, middle, in gray) and whisker motion energy (<xref rid="fig6" ref-type="fig">Fig 6Sf</xref>, top, in blue) showed that neurons with high correlations to arousal/movement tended to concentrate in M1 and somatosensory upper and lower limb (as one might expect for movement related activity) and dorsal auditory areas in this example session (dark red, gray, and dark blue, respectively), while neurons with lower correlations tended to concentrate in somatosensory barrel, nose, and mouth regions (light red, gray, and blue, respectively). As in the dorsal preparation (Fig. S6c), the mean correlations across 8 sessions from this example side mount mouse were in general statistically significantly greater than zero (see Fig. S6f, legend).</p>
</sec>
<sec id="s3k">
<title>Behavioral video analysis and neurobehavioral alignment</title>
<p>From the above results, we can see that there are clear, but complex, relationships between behavior and neuronal activity across the dorsal and lateral cortex of the mouse. Principal component analysis of this activity suggests that one major factor is variations in behavioral state, as indicated by movement/arousal (see also <xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>; <xref ref-type="bibr" rid="c40">Steinmetz et al, 2019</xref>; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>). Indeed, previous studies have demonstrated that the use of dozens or more of the top principal components of the motion energy of facial movement videos is beneficial in explaining variance in visual cortical activity (<xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>). However, beyond the first few principal components of facial movement, the precise behavioral meaning of the subsequent components are difficult to discern (<xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>).</p>
<p>Here we took a different approach by using a multi-step semi-automated analysis pipeline to identify higher order behavioral motifs and to examine the relationship of these motifs to the diversity of patterns of neural activity observed in the cortex.</p>
<p>To test the feasibility of this approach, we first created pose estimates consisting of (x,y) positions of a set of user-labeled body-parts and joints from our left, right, and posterior mouse videos using DeepLabCut (DLC; <xref ref-type="bibr" rid="c24">Mathis et al, 2018</xref>; see Suppl Movies S1). These pose estimate outputs were used for unsupervised machine learning extraction of behavioral motifs by an algorithm called “Behavioral segmentation of open field in DeepLabCut” (“B-SOiD”; Hsu and Yttri, 2021; see Suppl Movie S6). To identify high-level patterns of spontaneous head-fixed mouse behavior, we next performed manifold embedding (“UMAP”) and cluster identification (“HDBSCAN”) of DLC pose tracking outputs in BSOiD. A random-forests machine classifier was trained with these clusters to be able to predict, or identify, behavioral motifs in any test session with the same set or subset of labeled body part pose estimates (i.e. x-y positions for a certain number of body parts with the same names), either within or between mice.</p>
<p>In an example side mount session (same session as <xref rid="fig5" ref-type="fig">Fig. 5</xref>), we identified 16 BSOiD behavioral motifs that exhibited good coarse and fine alignment to the dynamics of behavioral arousal primitives (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>), including walking, and to Rastermap groups from the same session. Here, BSOiD was trained on four sessions from the same mouse across three days, including the target session itself, with minimum cumulative motif times set to 1% of total video time or ∼100 s, and then “tested” or used to label individual video frames from the target session. We found that, in some cases, training on individual sessions resulted in poor identification of rare and or brief behaviors, such as walking (which occurs rarely in some sessions; not shown).</p>
<p>Examination of four types of qualitatively identified behavioral epochs (“twitch”, “whisk”, “walk”, and “pupil oscillate”) at a finer temporal scale showed good alignment of BSOiD motifs with fine-scale features of spontaneous head-fixed mouse behavior at the level of movement and arousal primitives (<xref rid="fig6" ref-type="fig">Figs. 6a, b, Fig</xref>. S7). For example, balancing wheel movements during “twitch” seem to align well to flickering into BSOiD motifs 8 and 11, while flickering into motif 13 during “walk” appears to correspond well to individual periods of high walk speed during the extended walking bout (see Fig S7b). The qualitative labels that best describe these behaviors (“twitch”, <xref rid="fig6" ref-type="fig">Fig. 6a-c</xref>; “whisk”, <xref rid="fig6" ref-type="fig">Figs. 6a-c</xref>, S7; “walk”, <xref rid="fig6" ref-type="fig">Figs. 6a-c</xref>, S7b; “pupil oscillate”, <xref rid="fig6" ref-type="fig">Fig. 6a-c</xref>) did not appear to align with or be clearly explained by simple “first-order” interaction effects of movement and arousal primitives (i.e. whisk/walk/pupil x ON/OFF), despite the fact that the above-mentioned, closer, albeit preliminary, analysis of fine-scale behavioral features revealed precise and reliable alignments. This suggests that these methods will yield novel and important insights into the organization of neurobehavioral alignment across cortex, because other factors that are currently poorly understood appear to be contributing to the alignments.</p>
<p>Alignment of the bulk activity of all neurons in each Rastermap group with BSOiD motif transitions for this example session (<xref rid="fig6" ref-type="fig">Fig. 6a-c</xref>, colored, vertically-oriented dashed boxes) showed differential alignment with the above-mentioned four high-level, manually identified qualitative behaviors (“twitch”, “whisk”, “walk”, and “pupil oscillate”). The occurrences of these behaviors, then, were clearly aligned with both behavioral movement/arousal primitives (<xref rid="fig6" ref-type="fig">Fig. 6b</xref>) and the activity of putative, qualitatively segmented neural ensembles (<xref rid="fig6" ref-type="fig">Fig. 6c-d</xref>) that displayed varied spatial distributions across defined CCF areas of the side mount preparation (<xref rid="fig6" ref-type="fig">Fig. 6e</xref>). For example, “whisk”, which corresponds to whisking in the absence of walking (<xref rid="fig6" ref-type="fig">Fig. 6a, b</xref>), with relatively elevated pupil diameter, aligns with strong activity in Rastermap group 5 (<xref rid="fig6" ref-type="fig">Fig. 6c-d</xref>, white dashed oval corresponding to Rastermap group #5, green/blue), which has a higher density of cells in M1, M2, and somatosensory upper limb, mouth, and nose CCF areas (<xref rid="fig6" ref-type="fig">Fig. 6e</xref>, R5).</p>
<p>We expect that, in general, the predictive contribution of high level behavioral motifs relative to movement and arousal primitives will be greater both in lateral areas, such as A1, in non-primary CCF areas further up the cortical hierarchy, such as secondary sensory cortices and M2 (<xref ref-type="bibr" rid="c50">Wang et al, 2023</xref>; bioRxiv), and over intermediate and longer timescales of activity aligned with purposeful and/or goal oriented movements organized at higher levels of the behavioral hierarchy (Mimica et al, 2023).</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<sec id="s4a">
<title>Overview and main findings</title>
<p>We developed novel headpost designs, surgical procedures, 3D-printed accessories, experimental protocols, and analysis pipelines that allowed us to routinely perform 2p imaging of up to 7500 neurons simultaneously across ∼25 mm<sup>2</sup> of either parieto-occipital (“dorsal mount”) or temporo-parietal (“side mount”) cortex in awake, behaving mice, while simultaneously monitoring an array of global arousal state variables. Correlations between neural activity and arousal/movement were broadly, but heterogeneously, represented across both local and regional spatial scales. Preliminary analysis showed that this rich dataset could be used to segregate specific mappings between high-level, qualitatively identified behaviors (e.g. “twitch”, “whisk”, “walk”, and “pupil oscillate”; <xref rid="fig6" ref-type="fig">Fig. 6a</xref>), each consisting of a persistent pattern of robustly identifiable, high-level behavioral motifs and low-level behavioral arousal, with movement primitives (i.e. walk, whisker, and pupil; <xref rid="fig6" ref-type="fig">Fig. 6b</xref>) and spatially localized neural activity clusters (i.e. “Rastermap” groups; <xref rid="fig6" ref-type="fig">Fig. 6c-e</xref>). Further analysis of such mappings will both allow for the fine dissection of patterned behaviors and neural activity, and for the development of a powerful framework for the sophisticated prediction of performance in mice engaged in a variety of multimodal sensory discrimination tasks (see <xref ref-type="bibr" rid="c19">Hulsey et al, 2023</xref>; bioRxiv, and Wang et al, 2023; bioRxiv).</p>
<p>Taken together, these findings show the strong potential of our novel methods for further elucidation of the principles of pan-cortical neurobehavioral alignment at the level of densely sampled individual neurons. For example, the spatial distributions of densities of neurons in each Rastermap (<xref ref-type="bibr" rid="c42">Stringer et al, 2023</xref>, bioRxiv) group across CCF areas were partially overlapping, yet strongly dissociable (see <xref rid="fig4" ref-type="fig">Figs. 4d</xref>, <xref rid="fig5" ref-type="fig">5d</xref>, <xref rid="fig6" ref-type="fig">6e</xref>). This suggested that dynamic, recurring patterns of arousal-dependent reorganization might act to enable activity mode switching between various distributed, cortical functional “communities.” Each of these communities, then, could be specialized for different forms of activity related to various aspects of spontaneous behavior or task performance. This topic should be further explored with principled statistical techniques for recursive, top-down hierarchical community detection (Li et al, 2020).</p>
<p>In summary, we have shown here that it is feasible to monitor individual neuronal activity across broad expanses of the cerebral cortex and to perform neurobehavioral alignment of high resolution behavioral arousal state motifs and pan-cortical, activity-clustered neural ensembles in awake, behaving mice. Furthermore, our preliminary findings suggest a detailed alignment between both arousal/movement primitives and high-level behavioral motifs that appears to vary across broad regions of cortex (see <xref rid="fig6" ref-type="fig">Fig. 6</xref>). These findings extend those of earlier studies that showed widespread encoding of behavioral state during spontaneous behavior in visual cortex (<xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>; 2p imaging and Neuropixels recordings) and encoding of uninstructed movements during task performance both across dorsal cortex (<xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>; 1p widefield imaging across dorsal cortex, 2p imaging in restricted FOVs in primary visual and secondary motor cortex) and brain-wide during task performance (at low volumetric sampling density; <xref ref-type="bibr" rid="c40">Steinmetz et al, 2019</xref>).</p>
<p>In particular, our findings are consistent with those of <xref ref-type="bibr" rid="c41">Stringer et al (2019)</xref> in that we observe widespread, strong correlations of cortical neural activity with the behavioral state variables of movement (facial and locomotor) and pupil diameter. However, we imaged larger areas of cortex simultaneously, including medial, anterior, and lateral cortical areas, and at a higher overall imaging frequency (∼3 Hz vs. ∼1 Hz). This may have contributed to our ability to observe enhanced heterogeneity across these non-primary cortical areas, including in neurons whose activities were strongly suppressed during periods of increased arousal/movement. Higher concentrations of such neurons were evident over lateral regions of cortex such as primary auditory and somatosensory nose and mouse regions, and in anterior regions including secondary motor cortex.</p>
<p>We also showed that high-level behaviors, manually identified and aligned to clusters of repeating motifs by UMAP embedding (BSOiD; Hsu and Yttri, 2021), aligned well with patterns of neural activity identified by Rastermap corresponding to neural ensembles with non-uniform spatial distributions (<xref rid="fig6" ref-type="fig">Fig. 6</xref>). This suggests that encoding of behavioral state across cortex may occur at multiple levels of the behavioral hierarchy, in addition to that of “low-level” behavioral arousal/movement primitives such as pupil diameter, whisker motion energy, and walking speed (see Mimica et al, 2023).</p>
<p>Our finding that neurons with no correlation to arousal/movement, or with a large negative correlation, are also widespread across cortex, suggests that multiple mechanisms or pathways may exist in parallel to distribute information about behavioral state across the brain. Interestingly, as evidenced by our discovery with Rastermap of neural ensembles exhibiting diverse response kinetics and polarities (<xref rid="fig4" ref-type="fig">Figs. 4a</xref>, top, <xref rid="fig4" ref-type="fig">4d</xref>, <xref rid="fig5" ref-type="fig">5a</xref>, top, 5d, and 6c-e), these pathways may be activated with different temporal dynamics simultaneously across various brain areas (<xref ref-type="bibr" rid="c36">Shimaoka et al, 2018</xref>), thus making our approach for simultaneous recording of many areas one of the keys to potentially understanding the nature of their neuronal activity.</p>
<p>Additional experiments are warranted to uncover the mechanistic basis of such differences across large populations of cortical neurons associated with changes in activity level during periods of fluctuating arousal/engagement. For example, correlations between spiking and movement/arousal under 2p imaging, both as shown here and in previous studies (<xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>), and Neuropixels recordings (<xref ref-type="bibr" rid="c40">Steinmetz et al, 2019</xref>; <xref ref-type="bibr" rid="c41">Stringer et al, 2019</xref>), appear not to mirror those observed with widefield voltage imaging of membrane potential (<xref ref-type="bibr" rid="c36">Shimaoka et al, 2018</xref>; especially in VIS_lm and SS_b). This suggests the existence of a complex interaction between movement/arousal related changes in membrane potential and spiking that may vary across cortical areas.</p>
<p>In general, our preparations allow for the observation of diverse pan-cortical neural populations that likely communicate with each other in a transient or sustained manner during alternating periods of low and high movement/arousal and drive different patterns of global functional connectivity. Furthermore, the apparent variability of these neurobehavioral alignments suggests that interactions between cognitive and arousal/movement encodings may also occur during spontaneous behavior, in addition to during task performance (<xref ref-type="bibr" rid="c29">Musall et al, 2019</xref>), and that the degree of this interaction may depend on the cortical area.</p>
</sec>
<sec id="s4b">
<title>Advantages and disadvantages of our approach</title>
<p>A main advantage of our approach is the flexibility afforded by the combination of our two surgical preparations (see Protocols II, III), in terms of the sheer number of cortical areas spanning distant regions that can be recorded simultaneously at single cell resolution under 2p imaging. The interoperability of our preparations across widefield 1p and Thorlabs mesoscope 2p imaging rigs (see Protocol IV) will allow for the development of an experimental pipeline that first surveys widespread activity at rapid timescales (widefield) and then investigates the activity of large subregions in a serial manner at intermediate timescales (Thorlabs mesoscope). In addition, we are currently working on modifying these preparations to allow combined 2p imaging and whole-cell electrophysiological recordings (i.e. using a Thorlabs Bergamo II microscope) and/or widefield imaging and high-density electrophysiological recordings (i.e. Neuropixels; <xref ref-type="bibr" rid="c21">Jun et al, 2017</xref>; <xref ref-type="bibr" rid="c33">Peters et al, 2021</xref>), so that a single preparation in each mouse can be used to acquire data across multiple spatiotemporal scales during spontaneous and task-engaged epochs.</p>
<p>A drawback of our current preparations is that they require use of a water-immersion objective with a relatively short working distance (∼2.4-3.0 mm). This limits access for simultaneous electrophysiological recordings during mesoscale imaging, even though the same preparations could be used for simultaneous imaging and electrophysiological recordings on parallel acquisition rigs (i.e. widefield and/or standard 2p). Interestingly, recent advances in mesoscale FOV 2p microscopy (Diesel 2P; <xref ref-type="bibr" rid="c56">Yu et al, 2020</xref>) allow for the use of large working distance (∼8 mm) air-immersion objectives. In tandem with our headpost, surgical, behavioral, and analytic protocols, this could enable simultaneous Neuropixels, whole-cell electrophysiology, and mesoscale single-cell resolution imaging experiments. Such an approach would allow integration of neural data from multiple brain depths and areas, as well as across multiple spatiotemporal scales.</p>
<p>Another limitation of our current methodologies as presented here is that our analyses were limited to neurons acquired at a single cortical depth, and at an imaging rate that did not take full advantage of the speed of the Ca<sup>2+</sup> indicator that we used (i.e. GCaMP6s; we estimate that imaging at ∼10 Hz would sample all available information with a rise time of ∼200 ms and decay time of ∼1.2 s; <xref ref-type="bibr" rid="c6">Chen et al, 2013</xref>). In the future we will expand our analyses to sessions, which we have already recorded, acquired at ∼5-10 Hz over 4-6 FOVs (e.g. V1, A1, M2, RSpl, and SS_m/_n cortical areas; Fig. S2e), with resolutions ranging from 0.2 to 1 µm per pixel (Fig. S2d). Use of Dual Plane Imaging, a recently developed Thorlabs technology (developed together with the Allen Institute) that enables multiplexing of acquisition at more than one z-plane with no loss of imaging speed, with our preparations, in particular for use while pseudo-simultaneously imaging multiple small FOVs, would enable further in-depth examination of rapid fluctuations in neurobehavioral alignment structure across cortex in more than one cortical layer.</p>
</sec>
<sec id="s4c">
<title>Future directions</title>
<p>Reliability of cortical area identification and alignment in our preparation is enhanced due to standardization of headposts and cranial windows, and the combined use of skull landmark and cortical vasculature alignment in a rigorous, semi-automated widefield multimodal sensory mapping protocol. This has allowed us to acquire an extensive, standardized neurobehavioral data set (∼200 hours of spontaneous behavior with 30 Hz high resolution face and body video from three cameras, ∼1,000,000 total neurons at ∼3-10 Hz 2p acquisition rates imaged at ∼1000 x 1000 pixels across ∼250 sessions, total combined across both preparations and both FOV imaging configurations in 17 mice; see Fig. S2e).</p>
<p>In the future, we hope to expand our analyses of this extensive dataset to include identification of joint Rastermap/BSOiD (or keypoint-MOSEQ motif “syllable”; <xref ref-type="bibr" rid="c52">Weinreb et al, 2023</xref>; bioRxiv) transitions, and to perform further precision neurobehavioral alignment, using other methodologies such as hierarchical state-space models (Lindermann, S; <ext-link ext-link-type="uri" xlink:href="https://github.com/lindermanlab/ssm">https://github.com/lindermanlab/ssm</ext-link>), and factorial HMMs (<xref ref-type="bibr" rid="c14">Ghahramani and Jordan, 1997</xref>). These methods will allow increased accuracy over multiple behavioral timescales and the ability to predict transition or change points between different behavioral and/or neural activity states.</p>
<p>We will also examine changes in pan-cortical functional connectivity between communities of neurons with similar activity relationships associated with changes in behavioral state with statistical techniques such as Vector Autoregressive Union of Intersections (VA-UoI; <xref ref-type="bibr" rid="c2">Balasubramanian et al, 2020</xref>, <xref ref-type="bibr" rid="c34">Ruiz et al, 2020</xref>, Bouchard et al, 2022) that allow us to rigorously test for and remove putative but “false positive” connections based on coincidental activity correlations. Together, these analysis tools, when combined with the experimental techniques demonstrated here, will enable both identification of patterns of neurobehavioral alignment between arousal/movement and neural activity that are strongly predictive of high levels of behavioral performance, and the direct probing control of these patterns with targeted optogenetic manipulations.</p>
</sec>
</sec>
<sec id="d1e1474" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1553">
<label>Supplementary Figures, Protocols, Methods, and Materials</label>
<media xlink:href="supplements/563159_file02.pdf"/>
</supplementary-material>
<supplementary-material id="d1e1560">
<label>Suppl_Movie_S1_dorsal_mount</label>
<media xlink:href="supplements/563159_file03.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1567">
<label>Suppl_Movie_S1_side_mount</label>
<media xlink:href="supplements/563159_file04.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1574">
<label>Suppl_Movie_S2_visual</label>
<media xlink:href="supplements/563159_file05.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1582">
<label>Suppl_Movie_S2_whisker</label>
<media xlink:href="supplements/563159_file06.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1589">
<label>Suppl_Movie_S3_auditory</label>
<media xlink:href="supplements/563159_file07.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1596">
<label>Suppl_Movie_S3_visual</label>
<media xlink:href="supplements/563159_file08.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1603">
<label>Suppl_Movie_S3_whisker</label>
<media xlink:href="supplements/563159_file09.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1610">
<label>Suppl_Movie_S4</label>
<media xlink:href="supplements/563159_file10.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1617">
<label>Suppl_Movie_S5</label>
<media xlink:href="supplements/563159_file11.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1625">
<label>Suppl_Movie_S6</label>
<media xlink:href="supplements/563159_file12.mp4"/>
</supplementary-material>
<supplementary-material id="d1e1632">
<label>Suppl_Movie_S7</label>
<media xlink:href="supplements/563159_file13.mp4"/>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Allen</surname>, <given-names>W. E.</given-names></string-name> <etal>et al.</etal> <article-title>Global Representations of Goal-Directed Behavior in Distinct Cell Types of Mouse Neocortex</article-title>. <source>Neuron</source> <volume>94</volume>, <fpage>891</fpage>–<lpage>907</lpage>.e6 (<year>2017</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Balasubramanian</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Scaling of Union of Intersections for Inference of Granger Causal Networks from Observational Data. in 2020</article-title> <source>IEEE International Parallel and Distributed Processing Symposium (IPDPS)</source> <fpage>264</fpage>–<lpage>273</lpage> (IEEE, <year>2020</year>). doi:<pub-id pub-id-type="doi">10.1109/IPDPS47924.2020.00036</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Bimbard</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> <article-title>Behavioral origin of sound-evoked activity in mouse visual cortex</article-title>. <source>Nat Neurosci</source> <volume>26</volume>, <fpage>251</fpage>–<lpage>258</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Borisovska</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>McGinley</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Bensen</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Westbrook</surname>, <given-names>G. L</given-names></string-name>. <article-title>Loss of olfactory cell adhesion molecule reduces the synchrony of mitral cell activity in olfactory glomeruli: Olfactory cell adhesion molecule and olfactory bulb synchrony</article-title>. <source>The Journal of Physiology</source> <volume>589</volume>, <fpage>1927</fpage>–<lpage>1941</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="book"><string-name><surname>Bouchard</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Bujan</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Roosta-Khorasani</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ubaru</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pabhat, Snijders</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Mao</surname>, <given-names>J-H</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Mahoney</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Bhattacharyya</surname>, <given-names>S.</given-names></string-name> <source>Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction.. 31st Conference on Neural Information Processing Systems (NIPS</source> <year>2017</year>). <publisher-name>Long Beach, CA</publisher-name>. <publisher-loc>USA</publisher-loc>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Chen</surname>, <given-names>T.-W.</given-names></string-name> <etal>et al.</etal> <article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title>. <source>Nature</source> <volume>499</volume>, <fpage>295</fpage>–<lpage>300</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Clancy</surname>, <given-names>K. B.</given-names></string-name>, <string-name><surname>Orsolic</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Mrsic-Flogel</surname>, <given-names>T. D</given-names></string-name>. <article-title>Locomotion-dependent remapping of distributed cortical networks</article-title>. <source>Nat Neurosci</source> <volume>22</volume>, <fpage>778</fpage>–<lpage>786</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Coen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sit</surname>, <given-names>T. P. H.</given-names></string-name>, <string-name><surname>Wells</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>K. D</given-names></string-name>. <article-title>Mouse frontal cortex mediates additive multisensory decisions</article-title>. <source>Neuron</source> <issue>S0896627323003811</issue> (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2023.05.008</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Collins</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Francis</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Emanuel</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>McCormick</surname>, <given-names>D. A</given-names></string-name>. <article-title>Cholinergic and noradrenergic axonal activity contains a behavioral-state signal that is coordinated across the dorsal cortex</article-title>. <source>eLife</source> <volume>12</volume>, <fpage>e81826</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>de Vries</surname>, <given-names>S. E. J.</given-names></string-name> <etal>et al.</etal> <article-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title>. <source>Nat Neurosci</source> <volume>23</volume>, <fpage>138</fpage>–<lpage>151</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Esmaeili</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal> <article-title>Rapid suppression and sustained activation of distinct cortical regions for a delayed sensory-triggered motor response</article-title>. <source>Neuron</source> <volume>109</volume>, <fpage>2183</fpage>–<lpage>2201</lpage>.e9 (<year>2021</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Gallero-Salas</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>Sensory and Behavioral Components of Neocortical Signal Flow in Discrimination Tasks with Short-Term Memory</article-title>. <source>Neuron</source> <volume>109</volume>, <fpage>135</fpage>–<lpage>148</lpage>.e6 (<year>2021</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Garrett</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Nauhaus</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Marshel</surname>, <given-names>J. H.</given-names></string-name> &amp; <string-name><surname>Callaway</surname>, <given-names>E. M</given-names></string-name>. <article-title>Topography and Areal Organization of Mouse Visual Cortex</article-title>. <source>J. Neurosci</source>. <volume>34</volume>, <fpage>12587</fpage>–<lpage>12600</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name> &amp; <string-name><surname>Jordan</surname>, <given-names>M. I</given-names></string-name>. <article-title>Factorial Hidden Markov Models</article-title>. <source>Machine Learning?</source>, <fpage>1</fpage>–<lpage>31</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Gong</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>High-speed recording of neural spikes in awake mice and flies with a fluorescent voltage sensor</article-title>. <source>Science</source> <volume>350</volume>, <fpage>1361</fpage>–<lpage>1366</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Guo</surname>, <given-names>Z. V.</given-names></string-name> <etal>et al.</etal> <article-title>Flow of Cortical Activity Underlying a Tactile Decision in Mice</article-title>. <source>Neuron</source> <volume>81</volume>, <fpage>179</fpage>–<lpage>194</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Hattori</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Danskin</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Babic</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Mlynaryk</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Komiyama</surname>, <given-names>T</given-names></string-name>. <article-title>Area-Specificity and Plasticity of History-Dependent Value Coding During Learning</article-title>. <source>Cell</source> <volume>177</volume>, <fpage>1858</fpage>–<lpage>1872</lpage>.e15 (<year>2019</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Hsu</surname>, <given-names>A. I.</given-names></string-name> &amp; <string-name><surname>Yttri</surname>, <given-names>E. A</given-names></string-name>. <article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title>. <source>Nat Commun</source> <volume>12</volume>, <fpage>5188</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Hulsey</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Zumwalt</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Mazzucato</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>McCormick</surname>, <given-names>D. A.</given-names></string-name> &amp; <string-name><surname>Jaramillo</surname>, <given-names>S</given-names></string-name>. <source>Decision-making dynamics are predicted by arousal and uninstructed movements</source>. http://biorxiv.org/lookup/doi/10.1101/2023.03.02.530651&lt;x&gt; (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.03.02.530651</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Jacobs</surname>, <given-names>E. A. K.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Peters</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>K. D</given-names></string-name>. <article-title>Cortical State Fluctuations during Sensory Decision Making</article-title>. <source>Current Biology</source> <volume>30</volume>, <fpage>4944</fpage>–<lpage>4955</lpage>.e7 (<year>2020</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Jun</surname>, <given-names>J. J.</given-names></string-name> <etal>et al.</etal> <article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title>. <source>Nature</source> <volume>551</volume>, <fpage>232</fpage>–<lpage>236</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Kauvar</surname>, <given-names>I. V.</given-names></string-name> <etal>et al.</etal> <article-title>Cortical Observation by Synchronous Multifocal Optical Sampling Reveals Widespread Population Encoding of Actions</article-title>. <source>Neuron</source> <volume>107</volume>, <fpage>351</fpage>–<lpage>367</lpage>.e19 (<year>2020</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Kim</surname>, <given-names>T. H.</given-names></string-name> <etal>et al.</etal> <article-title>Long-Term Optical Access to an Estimated One Million Neurons in the Live Mouse Cortex</article-title>. <source>Cell Reports</source> <volume>17</volume>, <fpage>3385</fpage>–<lpage>3394</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat Neurosci</source> <volume>21</volume>, <fpage>1281</fpage>–<lpage>1289</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>McGinley</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>David</surname>, <given-names>S. V.</given-names></string-name> &amp; <string-name><surname>McCormick</surname>, <given-names>D. A</given-names></string-name>. <article-title>Cortical Membrane Potential Signature of Optimal States for Sensory Signal Detection</article-title>. <source>Neuron</source> <volume>87</volume>, <fpage>179</fpage>–<lpage>192</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Michaiel</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Parker</surname>, <given-names>P. R. L.</given-names></string-name> &amp; <string-name><surname>Niell</surname>, <given-names>C. M</given-names></string-name>. <article-title>A Hallucinogenic Serotonin-2A Receptor Agonist Reduces Visual Response Gain and Alters Temporal Dynamics in Mouse V1</article-title>. <source>Cell Reports</source> <volume>26</volume>, <fpage>3475</fpage>–<lpage>3483</lpage>.e4 (<year>2019</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Mimica</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal> <article-title>Behavioral decomposition reveals rich encoding structure employed across neocortex in rats</article-title>. <source>Nat Commun</source> <volume>14</volume>, <fpage>3947</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Morandell</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Del Rio</surname>, <given-names>R. T.</given-names></string-name> &amp; <string-name><surname>Schneider</surname>, <given-names>D. M</given-names></string-name>. <source>Movement-related modulation in mouse auditory cortex is widespread yet locally diverse</source>. http://biorxiv.org/lookup/doi/10.1101/2023.07.03.547560&lt;x&gt; (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.07.03.547560</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Musall</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Juavinett</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Gluf</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Churchland</surname>, <given-names>A. K</given-names></string-name>. <article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title>. <source>Nature Neuroscience</source> <volume>22</volume>, <fpage>1677</fpage>– <lpage>1686</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Niell</surname>, <given-names>C. M.</given-names></string-name> &amp; <string-name><surname>Stryker</surname>, <given-names>M. P</given-names></string-name>. <article-title>Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex</article-title>. <source>Neuron</source> <volume>65</volume>, <fpage>472</fpage>–<lpage>479</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Pachitariu</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal> <source>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</source>. http://biorxiv.org/lookup/doi/10.1101/061507&lt;x&gt; (<year>2016</year>) doi:<pub-id pub-id-type="doi">10.1101/061507</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Peron</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Freeman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Iyer</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Guo</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Svoboda</surname>, <given-names>K</given-names></string-name>. <article-title>A Cellular Resolution Map of Barrel Cortex Activity during Tactile Behavior</article-title>. <source>Neuron</source> <volume>86</volume>, <fpage>783</fpage>–<lpage>799</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Peters</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Fabre</surname>, <given-names>J. M. J.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> &amp; <string-name><surname>Carandini</surname>, <given-names>M</given-names></string-name>. <article-title>Striatal activity topographically reflects cortical activity</article-title>. <source>Nature</source> <volume>591</volume>, <fpage>420</fpage>–<lpage>425</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Ruiz</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Bhattacharyya</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Balasubramanian</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Bouchard</surname>, <given-names>K</given-names></string-name>. <article-title>Sparse and Low-bias Estimation of High Dimensional Vector Autoregressive Models</article-title>. <source>Proceedings of Machine Learning Research</source> <volume>120</volume>, <fpage>1</fpage>–<lpage>10</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Salkoff</surname>, <given-names>D. B.</given-names></string-name>, <string-name><surname>Zagha</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>McCarthy</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>McCormick</surname>, <given-names>D. A</given-names></string-name>. <article-title>Movement and Performance Explain Widespread Cortical Activity in a Visual Detection Task</article-title>. <source>Cerebral Cortex</source> <volume>30</volume>, <fpage>421</fpage>–<lpage>437</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Shimaoka</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> &amp; <string-name><surname>Carandini</surname>, <given-names>M</given-names></string-name>. <article-title>Effects of Arousal on Mouse Sensory Cortex Depend on Modality</article-title>. <source>Cell Reports</source> <volume>22</volume>, <fpage>3160</fpage>–<lpage>3167</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Shimaoka</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Harris</surname>, <given-names>K. D.</given-names></string-name> &amp; <string-name><surname>Carandini</surname>, <given-names>M</given-names></string-name>. <article-title>The impact of bilateral ongoing activity on evoked responses in mouse cortex</article-title>. <source>eLife</source> <volume>8</volume>, <fpage>e43533</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Sofroniew</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Flickinger</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>King</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Svoboda</surname>, <given-names>K</given-names></string-name>. <article-title>A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</article-title>. <source>eLife</source> <volume>5</volume>, <fpage>e14472</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name> <etal>et al.</etal> <article-title>Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings</article-title>. <source>Science</source> <volume>372</volume>, eabf4588 (<year>2021</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Steinmetz</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Zatka-Haas</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Carandini</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Harris</surname>, <given-names>K. D</given-names></string-name>. <article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title>. <source>Nature</source> <volume>576</volume>, <fpage>266</fpage>–<lpage>273</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> <article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title>. <source>Science</source> <volume>364</volume>, eaav7893 (<year>2019</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Stringer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zhong</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Syeda</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Du</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Pachitariu</surname>, <given-names>M</given-names></string-name>. <source>Rastermap: a discovery method for neural population recordings</source>. http://biorxiv.org/lookup/doi/10.1101/2023.07.25.550571&lt;x&gt; (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.07.25.550571</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Sun</surname>, <given-names>W.</given-names></string-name>, <etal>et al.</etal> <source>Learning produces a hippocampal cognitive map in the form of an orthogonalized state machine</source>. http://biorxiv.org/lookup/doi/10.1101/2023.08.03.551900&lt;x&gt; (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.08.03.551900</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Takita</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> <article-title>Chemistry of bleomycin. XXI. Metal-complex of bleomycin and its implication for the mechanism of bleomycin action</article-title>. <source>J Antibiot (Tokyo</source>) <volume>31</volume>, <fpage>1073</fpage>–<lpage>1077</lpage> (<year>1978</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Takita</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> <article-title>Chemistry of bleomycin. XXI. Metal-complex of bleomycin and its implication for the mechanism of bleomycin action</article-title>. <source>J Antibiot (Tokyo</source>) <volume>31</volume>, <fpage>1073</fpage>–<lpage>1077</lpage> (<year>1978</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><collab>The International Brain Laboratory</collab> <etal>et al.</etal> <source>Standardized and reproducible measurement of decision-making in mice. eLife</source> <volume>10</volume>, <fpage>e63711</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Valley</surname>, <given-names>M. T.</given-names></string-name> <etal>et al.</etal> <article-title>Separation of hemodynamic signals from GCaMP fluorescence measured with wide-field imaging</article-title>. <source>Journal of Neurophysiology</source> <volume>123</volume>, <fpage>356</fpage>–<lpage>366</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Vesuna</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Deep posteromedial cortical rhythm in dissociation</article-title>. <source>Nature</source> <volume>586</volume>, <fpage>87</fpage>–<lpage>94</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Q.</given-names></string-name> <etal>et al.</etal> <article-title>The Allen Mouse Brain Common Coordinate Framework: A 3D Reference Atlas</article-title>. <source>Cell</source> <volume>181</volume>, <fpage>936</fpage>–<lpage>953</lpage>.e20 (<year>2020</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Z. A.</given-names></string-name>, <etal>et al.</etal> <source>Not everything, not everywhere, not all at once: a study of brain-wide encoding of movement</source>. http://biorxiv.org/lookup/doi/10.1101/2023.06.08.544257&lt;x&gt; (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.06.08.544257</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Waters</surname>, <given-names>J</given-names></string-name>. <article-title>Sources of widefield fluorescence from the brain</article-title>. <source>eLife</source> <volume>9</volume>, <fpage>e59841</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Weinreb</surname>, <given-names>C.</given-names></string-name>, <etal>et al.</etal> <source>Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics</source>. http://biorxiv.org/lookup/doi/10.1101/2023.03.16.532307&lt;x&gt; (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.03.16.532307</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>Z.</given-names></string-name> <etal>et al.</etal> <article-title>Context-Dependent Decision Making in a Premotor Circuit</article-title>. <source>Neuron</source> <volume>106</volume>, <fpage>316</fpage>–<lpage>328</lpage>.e6 (<year>2020</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Xiong</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Znamenskiy</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Zador</surname>, <given-names>A. M</given-names></string-name>. <article-title>Selective corticostriatal plasticity during acquisition of an auditory discrimination task</article-title>. <source>Nature</source> <volume>521</volume>, <fpage>348</fpage>–<lpage>351</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Yoshida</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal> <article-title>In vivo wide-field calcium imaging of mouse thalamocortical synapses with an 8 K ultra-high-definition camera</article-title>. <source>Sci Rep</source> <volume>8</volume>, <fpage>8324</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Yu</surname>, <given-names>C.-H.</given-names></string-name>, <string-name><surname>Stirman</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hira</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S. L</given-names></string-name>. <source>Diesel2p mesoscope with dual independent scan engines for flexible capture of dynamics in distributed neural circuitry</source>. http://biorxiv.org/lookup/doi/10.1101/2020.09.20.305508&lt;x&gt; (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.1101/2020.09.20.305508</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Scaling down of balanced excitation and inhibition by active behavioral states in auditory cortex</article-title>. <source>Nat Neurosci</source> <volume>17</volume>, <fpage>841</fpage>–<lpage>850</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Zhuang</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>An extended retinotopic map of mouse cortex</article-title>. <source>eLife</source> <volume>6</volume>, <fpage>e18372</fpage> (<year>2017</year>).</mixed-citation></ref>
</ref-list>
<ack>
<title>Acknowledgements.</title>
<p>We thank Luca Mazzucato for helpful comments on the manuscript and Paul Steffan, Lawrence Scatena, and Daniel Hulsey for technical assistance. We thank Jack Waters for contributing CCF outlines and masks for the side mount rotated view cortical map. We thank Kazi Rafizullah, John Boosinger, and Eowyn Boosinger for helping conceive, design, and build the mesoscope resonant scanner noise reduction shield. We also thank Elliott Abe, David Wyrick, Shreya Saxena, Matthew Kaufman, Alexander Hsu, Caleb Weinreb, Jens Tillmann, and Carsen Stringer for assistance with coding and setting up preliminary data analysis for both neural and behavioral data, and the teams at Vidrio ScanImage and Thorlabs for technical assistance with software and hardware related to imaging acquisition, respectively. This work was supported by NIH grants R35NS097287 (DAM) and R01NS118461 (DAM).</p>
</ack>
<sec id="s5">
<title>Author contributions</title>
<p>E.D.V and D.A.M. conceived and designed the study. E.D.V. performed the hardware and protocol design, surgeries, data acquisition, and data analysis. E.D.V and D.A.M. wrote the paper.</p>
</sec>
<sec id="s6">
<title>Supplementary Files</title>
<p>All supplementary analysis code, 3D-printable files, and other design files are publicly available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">https://github.com/vickerse1/mesoscope_spontaneous</ext-link>. Please cite this work if you use them.</p>
</sec>
<sec id="s7">
<title>Declaration of interests</title>
<p>The authors declare no competing interests.</p>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94167.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Grunwald Kadow</surname>
<given-names>Ilona C</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Bonn</institution>
</institution-wrap>
<city>Bonn</city>
<country>Germany</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> paper presents a thoroughly detailed methodology for mesoscale-imaging of extensive areas of the cortex, either from a top or lateral perspective, in behaving mice. While the examples of scientific results to be derived with this method are in the preliminary stages, they offer promising and stimulating insights. Overall, the method and results presented are <bold>convincing</bold> and will be of interest to neuroscientists focused on cortical processing in rodents.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94167.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors introduce two preparations for observing large-scale cortical activity in mice during behavior. Alongside this, they present intriguing preliminary findings utilizing these methods. This paper is poised to be an invaluable resource for researchers engaged in extensive cortical recording in behaving mice.</p>
<p>Strengths:</p>
<p>
-Comprehensive methodological detailing:</p>
<p>
The paper excels in providing an exceptionally detailed description of the methods used. This meticulous documentation includes a step-by-step workflow, complemented by thorough workflow, protocols, and a list of materials in the supplementary materials.</p>
<p>-Minimal movement artifacts:</p>
<p>
A notable strength of this study is the remarkably low movement artifacts. To further underscore this achievement, a more robust quantification across all subjects, coupled with benchmarking against established tools (such as those from suite2p), would be beneficial.</p>
<p>Insightful preliminary data and analysis:</p>
<p>
The preliminary data unveiled in the study reveal interesting heterogeneity in the relationships between neural activity and detailed behavioral features, particularly notable in the lateral cortex. This aspect of the findings is intriguing and suggests avenues for further exploration.</p>
<p>Weaknesses:</p>
<p>
-Clarification about the extent of the method in the title and text:</p>
<p>
The title of the paper, using the term &quot;pan-cortical,&quot; along with certain phrases in the text, may inadvertently suggest that both the top and lateral view preparations are utilized in the same set of mice. To avoid confusion, it should be explicitly stated that the authors employ either the dorsal view (which offers limited access to the lateral ventral regions) or the lateral view (which restricts access to the opposite side of the cortex). For instance, in line 545, the phrase &quot;lateral cortex with our dorsal and side mount preparations&quot; should be revised to &quot;lateral cortex with our dorsal or side mount preparations&quot; for greater clarity.</p>
<p>-Comparison with existing methods:</p>
<p>
A more detailed contrast between this method and other published techniques would add value to the paper. Specifically, the lateral view appears somewhat narrower than that described in Esmaeili et al., 2021; a discussion of this comparison would be useful. Furthermore, the number of neurons analyzed seems modest compared to recent papers (50k) - elaborating on this aspect could provide important context for the readers.</p>
<p>-Discussion of methodological limitations:</p>
<p>
The limitations inherent to the method, such as the potential behavioral effects of tilting the mouse's head, are not thoroughly examined. A more comprehensive discussion of these limitations would enhance the paper's balance and depth.</p>
<p>-Preliminary nature of results:</p>
<p>
The results are at a preliminary stage; for example, the B-soid analysis is based on a single mouse, and the validation data are derived from the training data set. The discrepancy between the maps in Figures 5e and 6e might indicate that a significant portion of the map represents noise. An analysis of variability across mice and a method to assign significance to these maps would be beneficial.</p>
<p>-Analysis details:</p>
<p>
More comprehensive details on the analysis would be beneficial for replicability and deeper understanding. For instance, the statement &quot;Rigid and non-rigid motion correction were performed in Suite2p&quot; could be expanded with a brief explanation of the underlying principles, such as phase correlation, to provide readers with a better grasp of the methodologies employed.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94167.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>
The authors present a comprehensive technical overview of the challenging acquisition of large-scale cortical activity, including surgical procedures and custom 3D-printed headbar designs to obtain neural activity from large parts of the dorsal or lateral neocortex. They then describe technical adjustments for stable head fixation, light shielding, and noise insulation in a 2-photon mesoscope and provide a workflow for multisensory mapping and alignment of the obtained large-scale neural data sets in the Allen CCF framework. Lastly, they show different analytical approaches to relate single-cell activity from various cortical areas to spontaneous activity by using visualization and clustering tools, such as Rastermap, PCA-based cell sorting, and B-SOID behavioral motif detection.</p>
<p>The study contains a lot of useful technical information that should be of interest to the field. It tackles a timely problem that an increasing number of labs will be facing as recent technical advances allow the activity measurement of an increasing number of neurons across multiple areas in awake mice. Since the acquisition of cortical data with a large field of view in awake animals poses unique experimental challenges, the provided information could be very helpful to promote standard workflows for data acquisition and analysis and push the field forward.</p>
<p>Strengths:</p>
<p>
The proposed methodology is technically sound and the authors provide convincing data to suggest that they successfully solved various problems, such as motion artifacts or high-frequency noise emissions, during 2-photon imaging. Overall, the authors achieved their goal of demonstrating a comprehensive approach for the imaging of neural data across many cortical areas and providing several examples that demonstrate the validity of their methods and recapitulate and further extend some recent findings in the field.</p>
<p>Weaknesses:</p>
<p>
Most of the descriptions are quite focused on a specific acquisition system, the Thorlabs Mesoscope, and the manuscript is in part highly technical making it harder to understand the motivation and reasoning behind some of the proposed implementations. A revised version would benefit from a more general description of common problems and the thought process behind the proposed solutions to broaden the impact of the work and make it more accessible for labs that do not have access to a Thorlabs mesoscope. A better introduction of some of the specific issues would also promote the development of other solutions in labs that are just starting to use similar tools.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94167.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>
In their manuscript, Vickers and McCormick have demonstrated the potential of leveraging mesoscale two-photon calcium imaging data to unravel complex behavioural motifs in mice. Particularly commendable is their dedication to providing detailed surgical preparations and corresponding design files, a contribution that will greatly benefit the broader neuroscience community as a whole. The quality of the data is high, but it is not clear whether this is available to the community, some datasets should be deposited. More importantly, the authors have acquired activity-clustered neural ensembles at an unprecedented spatial scale to further correlate with high-level behaviour motifs identified by B-SOiD. Such an advancement marks a significant contribution to the field. While the manuscript is comprehensive and the analytical strategy proposed is promising, some technical aspects warrant further clarification. Overall, the authors have presented an invaluable and innovative approach, effectively laying a solid foundation for future research in correlating large-scale neural ensembles with behavioural. The implementation of a custom sound insulator for the scanner is a great idea and should be something implemented by others.</p>
<p>This is a methods paper, but there is no large diagram that shows how all the parts are connected, communicating, and triggering each other. This is described in the methods, but a visual representation would greatly benefit the readers looking to implement something similar. The authors should cite sources for the claims stated in lines 449-453 and cite the claim of the mouse's hearing threshold mentioned in lines 463. No stats for the results shown in Figure 6e, it would be useful to know which of these neural densities for all areas show a clear statistical significance across all the behaviors. While I understand that this is a methods paper, it seems like the authors are aware of the literature surrounding large neuronal recordings during mouse behavior. Indeed, in lines 178-179, the authors mention how a significant portion of the variance in neural activity can be attributed to changes in &quot;arousal or self-directed movement even during spontaneous behavior.&quot;. Why then did the authors not make an attempt at a simple linear model that tries to predict the activity of their many thousands of neurons by employing the multitude of regressors at their disposal (pupil, saccades, stimuli, movements, facial changes, etc). These models are straightforward to implement, and indeed it would benefit this work if the model extracts information on par with what is known from the literature.</p>
<p>Specific strengths and weaknesses with areas to improve:</p>
<p>The paper should include an overall cartoon diagram that indicates how the various modules are linked together for the sampling of both behaviour and mesoscale GCAMP. This is a methods paper, but there is no large diagram that shows how all the parts are connected, communicating, and triggering each other.</p>
<p>The paper contains many important results regarding correlations between behaviour and activity motifs on both the cellular and regional scales. There is a lot of data and it is difficult to draw out new concepts. It might be useful for readers to have an overall figure discussing various results and how they are linked to pupil movement and brain activity. A simple linear model that tries to predict the activity of their many thousands of neurons by employing the multitude of regressors at their disposal (pupil, saccades, stimuli, movements, facial changes, etc) may help in this regard.</p>
<p>Previously, widefield imaging methods have been employed to describe regional activity motifs that correlate with known intracortical projections. Within the authors' data it would be interesting to perhaps describe how these two different methods are interrelated, they do collect both datasets. Surprisingly, such macroscale patterns are not immediately obvious from the authors' data. Some of this may be related to the scaling of correlation patterns or other factors. Perhaps there still isn't enough data to readily see these and it is too sparse.</p>
<p>In lines 71-71, the authors described some disadvantages of one-photon widefield imaging including the inability to achieve single-cell resolution. However, this is not true. In recent years, the combination of better surgical preparations, camera sensors, and genetically encoded calcium indicators has enabled the acquisition of single-cell data even using one-photon widefield imaging methods. These methods include miniscopes (Cai et al., 2016), multi-camera arrays (Hope et al., 2023), and spinning disks (Xie et al., 2023).</p>
<p>Cai, Denise J., et al. &quot;A shared neural ensemble links distinct contextual memories encoded close in time.&quot; Nature 534.7605 (2016): 115-118.</p>
<p>
Hope, James, et al. &quot;Brain-wide neural recordings in mice navigating physical spaces enabled by a cranial exoskeleton.&quot; bioRxiv (2023).</p>
<p>
Xie, Hao, et al. &quot;Multifocal fluorescence video-rate imaging of centimetre-wide arbitrarily shaped brain surfaces at micrometric resolution.&quot; Nature Biomedical Engineering (2023): 1-14.</p>
<p>The authors' claim of achieving optical clarity for up to 150 days post-surgery with their modified crystal skull approach is significantly longer than the 8 weeks (approximately 56 days) reported in the original study by Kim et al. (2016). Since surgical preparations are an integral part of the manuscript, it may be helpful to provide more details to address the feasibility and reliability of the preparation in chronic studies. A series of images documenting the progression optical quality of the window would offer valuable insight.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.94167.1.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Vickers</surname>
<given-names>Evan D.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7053-4740</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>McCormick</surname>
<given-names>David A.</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9803-8335</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<disp-quote content-type="editor-comment">
<p>eLife assessment</p>
<p>This valuable paper presents a thoroughly detailed methodology for mesoscale-imaging of extensive areas of the cortex, either from a top or lateral perspective, in behaving mice. While the examples of scientific results to be derived with this method are in the preliminary stages, they offer promising and stimulating insights. Overall, the method and results presented are convincing and will be of interest to neuroscientists focused on cortical processing in rodents.</p>
</disp-quote>
<p>Authors’ Response:  We thank the reviewers for the helpful and constructive comments.  They have helped us plan for significant improvements to our manuscript.  Our preliminary response and plans for revision are indicated below.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors introduce two preparations for observing large-scale cortical activity in mice during behavior. Alongside this, they present intriguing preliminary findings utilizing these methods. This paper is poised to be an invaluable resource for researchers engaged in extensive cortical recording in behaving mice.</p>
<p>Strengths:</p>
<p>-Comprehensive methodological detailing:</p>
<p>The paper excels in providing an exceptionally detailed description of the methods used. This meticulous documentation includes a step-by-step workflow, complemented by thorough workflow, protocols, and a list of materials in the supplementary materials.</p>
<p>-Minimal movement artifacts:</p>
<p>A notable strength of this study is the remarkably low movement artifacts. To further underscore this achievement, a more robust quantification across all subjects, coupled with benchmarking against established tools (such as those from suite2p), would be beneficial.</p>
</disp-quote>
<p>Authors’ Response:  This is a good suggestion.  Since we used suite2p for our data analysis, and have records of the fast-z correction applied by the microscope, we can supply these as quantifications of movement corrections that were applied across our sample of mice.  We hope to supply this information as a supplement in the revised manuscript.</p>
<p>Currently, we have chosen to show that the corrected, post- suite2p registration movement artifacts are very close to zero. We will revise the manuscript with clear descriptions of methods that we have found important, such as fully tightening all mounting devices, utilizing the air table properly, implanting the cranial window with proper, even pressure across its entire extent, and mounting the mouse so that it is not too close or far from the surface of the running wheel.</p>
<disp-quote content-type="editor-comment">
<p>Insightful preliminary data and analysis:</p>
<p>The preliminary data unveiled in the study reveal interesting heterogeneity in the relationships between neural activity and detailed behavioral features, particularly notable in the lateral cortex. This aspect of the findings is intriguing and suggests avenues for further exploration.</p>
<p>Weaknesses:</p>
<p>-Clarification about the extent of the method in the title and text:</p>
<p>The title of the paper, using the term &quot;pan-cortical,&quot; along with certain phrases in the text, may inadvertently suggest that both the top and lateral view preparations are utilized in the same set of mice. To avoid confusion, it should be explicitly stated that the authors employ either the dorsal view (which offers limited access to the lateral ventral regions) or the lateral view (which restricts access to the opposite side of the cortex). For instance, in line 545, the phrase &quot;lateral cortex with our dorsal and side mount preparations&quot; should be revised to &quot;lateral cortex with our dorsal or side mount preparations&quot; for greater clarity.</p>
</disp-quote>
<p>Authors’ Response:  We will revise the manuscript so that it is clear that we made use of two imaging configurations for the 2-photon mesoscope data and the benefits and limitations of these two preparations.  The dorsal mount and the side mount each have their advantages and disadvantages, but together form a powerful tool for imaging much of the dorsal and lateral cortex in awake, behaving mice.</p>
<disp-quote content-type="editor-comment">
<p>-Comparison with existing methods:</p>
<p>A more detailed contrast between this method and other published techniques would add value to the paper. Specifically, the lateral view appears somewhat narrower than that described in Esmaeili et al., 2021; a discussion of this comparison would be useful.</p>
</disp-quote>
<p>Authors’ Response:  We will modify the manuscript so that a more detailed comparison with other published techniques is included.  The preparation by Esmaeili et al. 2021 has some similarities, but also differences, from our preparation. Our preliminary reading is that their through-the-skull field of view is approximately the same as our through-the-skull field of view that exists between our first (headpost implantation) and second (window implantation) surgeries, although our preparation appears to include more anterior areas both near to and on the contralateral side of the midline.  We will compare these preparations more accurately in the revised manuscript.</p>
<p>If you compare the imageable extent of our cranial window for mesoscale 2-photon imaging to that of their through-the-skull widefield preparation, which is a bit of an “apples to oranges” comparison, then you are likely correct that their field of view is larger than ours, if you are referring to our 10 mm radius-bend glass. However, use of our 9 mm radius bend glass (i.e. a tighter bend) allows us to image additional ventral auditory areas. We could show an example of this, perhaps, although we did not make as much use of this alternative window in the large FOV experiments, because the increased curvature of the glass relative to the 10 mm radius bend window prevents imaging of the entire preparation in a single 2-photon z-plane. With the 9 mm radius bend glass we mostly imaged in the multiple, small FOV configuration (see Fig. S2).</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, the number of neurons analyzed seems modest compared to recent papers (50k) - elaborating on this aspect could provide important context for the readers.</p>
</disp-quote>
<p>Authors’ response:  With respect to the “modest” number of neurons analyzed (between 2000 and 8000 neurons per session for our dorsal and side mount preparations with medians near 4500; See Fig. S2e) we would like to point out that factors such as use of dual-plane imaging or multiple imaging planes, different mouse lines, use of different duration recording sessions (see our Fig S2c), use of different imaging speeds and resolutions (see our Fig S2d), use of different Suite2p run-time parameters, and inclusion or areas with blood vessels and different neuron cell densities, may all impact the count of total analyzed neurons. We could provide additional documentation of these issues, but we would like to point out that, in our case, we were not trying to maximize neuron count at the expense of other factors such as imaging speed and total spatial FOV extent.</p>
<disp-quote content-type="editor-comment">
<p>-Discussion of methodological limitations:</p>
<p>The limitations inherent to the method, such as the potential behavioral effects of tilting the mouse's head, are not thoroughly examined. A more comprehensive discussion of these limitations would enhance the paper's balance and depth.</p>
</disp-quote>
<p>Authors’ Response:  Our mice readily adapted to the 22.5 degree head tilt and learned to perform 2-alternative forced choice (2-AFC) auditory and visual tasks in this situation (Hulsey et al, 2024; Cell Reports).  The advantages and limitations of such a rotation of the mouse, and possible ways to alleviate these limitations, as detailed in the following paragraphs, will be discussed more thoroughly in the revised manuscript.</p>
<p>One can look at Supplementary Movie 1 for examples of the relatively similar behavior between the dorsal mount (not rotated) and side mount (rotated) preparations. We do not have behavioral data from mice that were placed in both configurations.  Our preliminary comparison across mice indicates that side and dorsal mount mice show similar behavioral variability.</p>
<p>It was in general important to make sure that the distance between the wheel and all four limbs was similar for both preparations. In particular, careful attention must be paid to the positioning of the front limbs in the side mount mice so that they are not too high off the wheel. This can be accomplished by a slight forward angling of the left support arm for side mount mice.</p>
<p>Although it would in principle be nearly possible to image the side mount preparation in the same optical configuration that we do without rotating the mouse, by rotating the objective to 20 degrees to the right, we found that the last 2-3 degrees of missing rotation (our preparation is rotated 22.5 degrees left, which is more than the full available 20 degrees rotation of the objective), along with several other factors, made this undesirable. First, it was very difficult to image auditory areas without the additional flexibility to rotate the objective more laterally. Second, it was difficult or impossible to attach the horizontal light shield and to establish a water meniscus with the objective fully rotated. One could use gel instead (which we found to be optically inferior to water), but without the horizontal light shield, the UV and IR LEDs can reach the PMTs via the objective and contaminate the image or cause tripping of the PMT. Third, imaging the right pupil and face of the mouse is difficult to impossible under these conditions because the camera would need the same optical access angle as the objective, or would need to be moved down toward the air table and rotated up 20 degrees, in which case its view would be blocked by the running wheel and other objects mounted on the air table.</p>
<disp-quote content-type="editor-comment">
<p>-Preliminary nature of results:</p>
<p>The results are at a preliminary stage; for example, the B-soid analysis is based on a single mouse, and the validation data are derived from the training data set. The discrepancy between the maps in Figures 5e and 6e might indicate that a significant portion of the map represents noise. An analysis of variability across mice and a method to assign significance to these maps would be beneficial.</p>
</disp-quote>
<p>Authors’ Response:  In this methods paper, we have chosen to supply proof of principle examples, without a complete analysis of animal-to-animal variance. The dataset for this paper contains both neural and behavioral data for 91 sessions across 18 mice from both dorsal and side mount preparations. The complete analysis of this dataset exceeds the capacity of the present study.  We will include more individual examples in the revised version, along with data showing the amount of  between session and across mouse variance.  We will include in the revised manuscript a comparison of the stability of B-SOiD measures across sessions, as a demonstration of what may be expected with this method.</p>
<disp-quote content-type="editor-comment">
<p>-Analysis details:</p>
<p>More comprehensive details on the analysis would be beneficial for replicability and deeper understanding. For instance, the statement &quot;Rigid and non-rigid motion correction were performed in Suite2p&quot; could be expanded with a brief explanation of the underlying principles, such as phase correlation, to provide readers with a better grasp of the methodologies employed.</p>
</disp-quote>
<p>Authors’ Response:  We are revising the manuscript to give more detail without reducing readability, so as to increase clarity of presentation.  Since this is a methods paper, we are modifying the manuscript to include more details and clear explanations so that the reader may replicate our methods and results.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors present a comprehensive technical overview of the challenging acquisition of large-scale cortical activity, including surgical procedures and custom 3D-printed headbar designs to obtain neural activity from large parts of the dorsal or lateral neocortex. They then describe technical adjustments for stable head fixation, light shielding, and noise insulation in a 2-photon mesoscope and provide a workflow for multisensory mapping and alignment of the obtained large-scale neural data sets in the Allen CCF framework. Lastly, they show different analytical approaches to relate single-cell activity from various cortical areas to spontaneous activity by using visualization and clustering tools, such as Rastermap, PCA-based cell sorting, and B-SOID behavioral motif detection.</p>
</disp-quote>
<p>Authors’ Response:  Thank you for this excellent summary of the scope of our paper.</p>
<disp-quote content-type="editor-comment">
<p>The study contains a lot of useful technical information that should be of interest to the field. It tackles a timely problem that an increasing number of labs will be facing as recent technical advances allow the activity measurement of an increasing number of neurons across multiple areas in awake mice. Since the acquisition of cortical data with a large field of view in awake animals poses unique experimental challenges, the provided information could be very helpful to promote standard workflows for data acquisition and analysis and push the field forward.</p>
</disp-quote>
<p>Authors’ Response:  We very much support the idea that our work here will contribute to the development of standard workflows across the field including multiple approaches to large-scale neural recordings.</p>
<disp-quote content-type="editor-comment">
<p>Strengths:</p>
<p>The proposed methodology is technically sound and the authors provide convincing data to suggest that they successfully solved various problems, such as motion artifacts or high-frequency noise emissions, during 2-photon imaging. Overall, the authors achieved their goal of demonstrating a comprehensive approach for the imaging of neural data across many cortical areas and providing several examples that demonstrate the validity of their methods and recapitulate and further extend some recent findings in the field.</p>
<p>Weaknesses:</p>
<p>Most of the descriptions are quite focused on a specific acquisition system, the Thorlabs Mesoscope, and the manuscript is in part highly technical making it harder to understand the motivation and reasoning behind some of the proposed implementations. A revised version would benefit from a more general description of common problems and the thought process behind the proposed solutions to broaden the impact of the work and make it more accessible for labs that do not have access to a Thorlabs mesoscope. A better introduction of some of the specific issues would also promote the development of other solutions in labs that are just starting to use similar tools.</p>
</disp-quote>
<p>Authors’ Response:  We will re-write the motivation behind the study to clarify the general problems that are being addressed.  As the 2-photon imaging component of these experiments were performed on a Thorlabs mesoscope, the imaging details will necessarily deal specifically with this system. We will briefly compare the methods and results from our Thorlabs system to that of other systems, based on what we are able to glean from the literature on their strengths and weaknesses.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary</p>
<p>In their manuscript, Vickers and McCormick have demonstrated the potential of leveraging mesoscale two-photon calcium imaging data to unravel complex behavioural motifs in mice. Particularly commendable is their dedication to providing detailed surgical preparations and corresponding design files, a contribution that will greatly benefit the broader neuroscience community as a whole. The quality of the data is high, but it is not clear whether this is available to the community, some datasets should be deposited. More importantly, the authors have acquired activity-clustered neural ensembles at an unprecedented spatial scale to further correlate with high-level behaviour motifs identified by B-SOiD. Such an advancement marks a significant contribution to the field. While the manuscript is comprehensive and the analytical strategy proposed is promising, some technical aspects warrant further clarification. Overall, the authors have presented an invaluable and innovative approach, effectively laying a solid foundation for future research in correlating large-scale neural ensembles with behaviour. The implementation of a custom sound insulator for the scanner is a great idea and should be something implemented by others.</p>
</disp-quote>
<p>Authors’ Response:  Thank you for the kind words.</p>
<p>We intend to make the data set used in making our main figures available to the public, perhaps using FigShare, so that they may check the validity of the methods and analysis.  We intend to release a complete data set to the public as a Dandiset on the DANDI archive in conjunction with a second in-depth analysis paper that is currently in preparation.</p>
<disp-quote content-type="editor-comment">
<p>This is a methods paper, but there is no large diagram that shows how all the parts are connected, communicating, and triggering each other. This is described in the methods, but a visual representation would greatly benefit the readers looking to implement something similar.</p>
</disp-quote>
<p>Authors’ Response:  This is an excellent suggestion.  We will include a workflow diagram in the revised manuscript for the methods, data collection, and analysis.</p>
<disp-quote content-type="editor-comment">
<p>The authors should cite sources for the claims stated in lines 449-453 and cite the claim of the mouse's hearing threshold mentioned in lines 463.</p>
</disp-quote>
<p>Authors’ Response:  For the claim stated in lines 449-453, “The unattenuated or native high-frequency background noise generated by the resonant scanner causes stress to both mice and experimenters, and can prevent mice from achieving maximum performance in auditory mapping, spontaneous activity sessions, auditory stimulus detection, and auditory discrimination sessions/tasks,” we can provide the following references: (i) for mice: Sadananda et al, 2008 (“Playback of 22-kHz and 50-kHz ultrasonic vocalizations induces differential c-fos expression in rat brain”, Neuroscience Letters, Vol 435, Issue 1, p 17-23), and (ii) for humans: Fletcher et al, 2018 (“Effects of very high-frequency sound and ultrasound on humans. Part I: Adverse symptoms after exposure to audible very-high frequency sound”, J Acoust Soc A, 144, 2511-2520). We will include these references in the revised paper.</p>
<p>For line 463, “i.e. below the mouse hearing threshold at 12.5 kHz of roughly 15 dB”, we can provide the following reference: Zheng et al, 1999 (“Assessment of hearing in 80 inbred strains of mice by ABR threshold analyses”, Vol 130, Issues 1-2, p 94-107). We will also include this reference in the paper. Thank you for identifying these citation omissions.</p>
<disp-quote content-type="editor-comment">
<p>No stats for the results shown in Figure 6e, it would be useful to know which of these neural densities for all areas show a clear statistical significance across all the behaviors.</p>
</disp-quote>
<p>Authors’ Response:  There are two statistical comparisons that we feel may be useful to add to the single session data displayed in this figure, in order to address the point that you raise. The first would allow us to assess whether for each Rastermap group, the distribution of neuron densities across CCF areas differs from a null, uniform distribution. The second would allow us to examine differences between Rastermap groups associated with different qualitative behaviors in order to know with which patterns of neural activity they are reliably associated.</p>
<p>For the first comparison, we could provide a statistic similar to what we provide for Fig. S6c and f, in which for each CCF area we compare the observed mean correlation values to a null of 0, or, in this case, the population densities of each Rastermap group for each CCF area to a null value equal to the total number of CCF areas divided by the total number of recorded neurons for that group (i.e. a Rastermap group with 500 neurons evenly distributed across ~30 CCF areas would contain ~17 neurons (or ~6% density) per CCF area.) Our current figure legend states that the maximum of the scale bar look-up value (reds) for each group ranges from ~8% to 32%. So indeed, adding these significances would be informative in this case.</p>
<p>For the second comparison, we could compare the density of neurons for each CCF area across Rastermap groups for this session. For example, it may be the case that the density of neurons in primary and secondary visual areas belonging to Rastermap groups that predominate during the “walk” behavior is higher than in the Rastermap group that predominates during the “whisk” behavior, or that the density of neurons in the “whisk” and “twitch” Rastermap groups in primary and secondary motor areas is higher than in the Rastermap groups that are active during the “walk” and “oscillate” behaviors.</p>
<p>Such a comparison should in fact be robust to Rastermap group variability across sessions and mice, as long as the same qualitative behaviors recur. However, our current qualitative methods for discretization of the Rastermap groups likely limits our ability to extend such an analysis accurately across our entire dataset. We are pursuing more rigorous analysis methods in this vein for our second, results oriented paper.</p>
<disp-quote content-type="editor-comment">
<p>While I understand that this is a methods paper, it seems like the authors are aware of the literature surrounding large neuronal recordings during mouse behavior. Indeed, in lines 178-179, the authors mention how a significant portion of the variance in neural activity can be attributed to changes in &quot;arousal or self-directed movement even during spontaneous behavior.&quot; Why then did the authors not make an attempt at a simple linear model that tries to predict the activity of their many thousands of neurons by employing the multitude of regressors at their disposal (pupil, saccades, stimuli, movements, facial changes, etc). These models are straightforward to implement, and indeed it would benefit this work if the model extracts information on par with what is known from the literature.</p>
</disp-quote>
<p>Authors’ Response:  This is an excellent suggestion, but beyond the scope of the current methods paper.  We are following up this methods paper with an in depth analysis of neural activity and corresponding behavior across the cortex during spontaneous and trained behaviors, but this analysis goes well beyond the scope of the present manuscript. Here, we prefer to present examples of the types of  results that can be expected to be obtained using our methods, and how these results compare with those obtained by others in the field.</p>
<disp-quote content-type="editor-comment">
<p>Specific strengths and weaknesses with areas to improve:</p>
<p>The paper should include an overall cartoon diagram that indicates how the various modules are linked together for the sampling of both behaviour and mesoscale GCAMP. This is a methods paper, but there is no large diagram that shows how all the parts are connected, communicating, and triggering each other.</p>
</disp-quote>
<p>Authors’ Response:  This is an excellent suggestion and will be included in the revised manuscript, so that readers can more readily follow our workflow, data collection, and analysis.</p>
<disp-quote content-type="editor-comment">
<p>The paper contains many important results regarding correlations between behaviour and activity motifs on both the cellular and regional scales. There is a lot of data and it is difficult to draw out new concepts. It might be useful for readers to have an overall figure discussing various results and how they are linked to pupil movement and brain activity. A simple linear model that tries to predict the activity of their many thousands of neurons by employing the multitude of regressors at their disposal (pupil, saccades, stimuli, movements, facial changes, etc) may help in this regard.</p>
</disp-quote>
<p>Authors’ Response:  This is an excellent suggestion, but beyond the scope of the present methods paper.  Such an analysis is a significant undertaking with such large and heterogeneous datasets, and we provide proof-of-principle data here so that the reader can understand the type of data to be expected using our methods.  We hope to provide a more complete analysis of data obtained using our methodology in the near future in a second manuscript.</p>
<p>However, we may be amenable to including preliminary linear model fit results, as supplementary material, for the two example sessions highlighted in this paper (i.e. the one dorsal mount session in Fig. 4, and the one side mount session shown in Figs. 5 and 6).</p>
<disp-quote content-type="editor-comment">
<p>Previously, widefield imaging methods have been employed to describe regional activity motifs that correlate with known intracortical projections. Within the authors' data it would be interesting to perhaps describe how these two different methods are interrelated -they do collect both datasets. Surprisingly, such macroscale patterns are not immediately obvious from the authors' data. Some of this may be related to the scaling of correlation patterns or other factors. Perhaps there still isn't enough data to readily see these and it is too sparse.</p>
</disp-quote>
<p>Authors’ Response:  Unfortunately, we are unable to directly compare widefield GCaMP6s activity with mesoscope 2-photon GCaMP6s activity.  During widefield data acquisition, animals were stimulated with visual, auditory, or somatosensory stimuli, while 2-photon mesoscope data collection occurred during spontaneous changes in behavioral state, without sensory stimulation.  The suggested comparison is, indeed, an interesting project for the future.</p>
<disp-quote content-type="editor-comment">
<p>In lines 71-71, the authors described some disadvantages of one-photon widefield imaging including the inability to achieve single-cell resolution. However, this is not true. In recent years, the combination of better surgical preparations, camera sensors, and genetically encoded calcium indicators has enabled the acquisition of single-cell data even using one-photon widefield imaging methods. These methods include miniscopes (Cai et al., 2016), multi-camera arrays (Hope et al., 2023), and spinning disks (Xie et al., 2023).</p>
<p>Cai, Denise J., et al. &quot;A shared neural ensemble links distinct contextual memories encoded close in time.&quot; Nature 534.7605 (2016): 115-118.</p>
<p>Hope, James, et al. &quot;Brain-wide neural recordings in mice navigating physical spaces enabled by a cranial exoskeleton.&quot; bioRxiv (2023).</p>
<p>Xie, Hao, et al. &quot;Multifocal fluorescence video-rate imaging of centimetre-wide arbitrarily shaped brain surfaces at micrometric resolution.&quot; Nature Biomedical Engineering (2023): 1-14.</p>
</disp-quote>
<p>Authors’ Response:  We will correct these statements and incorporate these, and other relevant, references.  There are advantages and disadvantages to each chosen technique, such as ease of use, field of view, accuracy, speed, etc., and we will highlight a few of these without an extensive literature review.</p>
<p>Even the best one-photon imaging techniques typically have ~10-20 micrometer resolution in xy (we image at 5 micrometer resolution for our large FOV configuration, but the xy point-spread function for the Thorlabs mesoscope is 0.61 x 0.61 micrometers in xy with 970 nm excitation) and undefined z-resolution (4.25 micrometers for Thorlabs mesoscope).  A coarser resolution increases the likelihood  that activity data from neighboring cells may contaminate the fluorescence observed from imaged neurons. Reducing the FOV and using sparse expression of the indicator lessens this overlap problem.</p>
<p>We do appreciate these recent advances, however, particularly for use in cases where more rapid imaging is desired over a large field of view (CCD acquisition can be much faster than that of standard 2-photon galvo-galvo or even galvo-resonant scanning, as the Thorlabs mesoscope uses). This being said, there are few currently available genetically encoded Ca2+ sensors that are able to measure fluctuations faster than ~10 Hz, which is a speed achievable on the Thorlabs 2-photon mesoscope with our techniques using the “small, multiple FOV” method (Fig. S2d, e).</p>
<disp-quote content-type="editor-comment">
<p>The authors' claim of achieving optical clarity for up to 150 days post-surgery with their modified crystal skull approach is significantly longer than the 8 weeks (approximately 56 days) reported in the original study by Kim et al. (2016). Since surgical preparations are an integral part of the manuscript, it may be helpful to provide more details to address the feasibility and reliability of the preparation in chronic studies. A series of images documenting the progression optical quality of the window would offer valuable insight.</p>
</disp-quote>
<p>Authors’ Response:  As you suggest, we will include images and data demonstrating the average changes in the window preparation, as well as the degree of variability and a range of outcome scenarios that we observed over the prolonged time periods of our study.  We will also include methodological details that we found were useful for facilitating long term use of these preparations.</p>
</body>
</sub-article>
</article>