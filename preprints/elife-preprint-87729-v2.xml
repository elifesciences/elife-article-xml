<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">87729</article-id>
<article-id pub-id-type="doi">10.7554/eLife.87729</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87729.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A generative model of electrophysiological brain responses to stimulation: an approach to study perceptual stability</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9650-2229</contrib-id>
<name>
<surname>Vidaurre</surname>
<given-names>Diego</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Center for Functionally Integrative Neuroscience, Department of Clinical Medicine, Aarhus University</institution>, 8000 <country>(Denmark)</country></aff>
<aff id="a2"><label>2</label><institution>Department of Psychiatry, Oxford University</institution>, OX3 7JX <country>(UK)</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Behrens</surname>
<given-names>Timothy E</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
<city>Oxford</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Corresponding email: <email>dvidaurre@cfin.au.dk</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-05-31">
<day>31</day>
<month>05</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2023-12-21">
<day>21</day>
<month>12</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP87729</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-03-29">
<day>29</day>
<month>03</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-05-20">
<day>20</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.03.522583"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-05-31">
<day>31</day>
<month>05</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87729.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.87729.1.sa2">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.87729.1.sa1">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.87729.1.sa0">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.87729.1.sa3">Author Response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Vidaurre</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Vidaurre</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-87729-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Each brain response to a stimulus is, to a large extent, unique. However this variability, our perceptual experience feels stable. Standard decoding models, which utilise information across several areas to tap into stimuli representation and processing, are fundamentally based on averages. Therefore, they can focus precisely on the features that are most stable across stimulus presentations. But which are these features exactly is difficult to address in the absence of a generative model of the signal. Here, I introduce <italic>genephys</italic>, a generative model of brain responses to stimulation publicly available as a Python package that, when confronted with a decoding algorithm, can reproduce the structured patterns of decoding accuracy that we observe in real data. Using this approach, I characterise how these patterns may be brought about by the different aspects of the signal, which in turn may translate into distinct putative neural mechanisms. In particular, the model shows that the features in the data that support successful decoding —and, therefore, likely reflect stable mechanisms of stimulus representation— have an oscillatory component that spans multiple channels, frequencies and latencies of response; and an additive, slower response with a specific (cross-frequency) relation to the phase of the oscillatory component. At the individual trial level, still, responses are found to be highly variable, which can be due to various factors including phase noise and probabilistic activations.</p>
</abstract>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>I have made changes to clarify the actual purpose of the paper, which was not to mimic a specific TGM, but to characterise the main features that we generally see across studies in the field; and added a new experimental section.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>There are virtually infinite manners by which a constant stimulus can impinge into the sensorium of an animal. For instance, our noses have many receptors that can sense a given odorant molecule, but only a small subset of those are excited each time the odour is perceived (<xref ref-type="bibr" rid="c1">Axel, 1995</xref>). Similarly, photons hit and excite photoreceptors in the retina randomly and sparsely for a given presented image (<xref ref-type="bibr" rid="c7">Dowling, 1987</xref>). An important aspect of sensation and perception is that never the exact same receptors are excited every time we perceive and, still, our perceptual experiences feel quite stable. So the brain must have a way to transit from lack of invariance at the microscopic sensory level toward invariance at the macroscopic level, which ultimately supports the invariant aspects of conscious perception and behaviour. However, we observe significant variability in the brain responses at the trial level (<xref ref-type="bibr" rid="c24">Stein, et al., 2005</xref>; <xref ref-type="bibr" rid="c19">McIntosh, et al., 2008</xref>; <xref ref-type="bibr" rid="c11">Garrett, et al., 2013</xref>), including at the earliest layers of the perceptual hierarchy (<xref ref-type="bibr" rid="c5">Croner, et al., 1993</xref>; <xref ref-type="bibr" rid="c9">Freeman, 1978</xref>) —that is, each perceptual experience is associated with a unique neural trajectory that does not repeat. How the gap between stability in subjective perception and the changing nature of brain responses is bridged is an important question in neuroscience. Here, I investigate the distributed aspects of brain activity that are most stable across experimental repetitions, and are therefore most likely to relate to stable perceptual experiences.</p>
<p>Decoding analysis uses multivariate machine learning algorithms to predict the identity of the observed stimulus from brain data (<xref ref-type="bibr" rid="c13">Haxby, et al., 2014</xref>; <xref ref-type="bibr" rid="c26">Stokes, et al., 2015</xref>). This way, per time point, it estimates a function of the data that maximally discriminate between conditions, as well as a temporal measure of accuracy that reflects how much information the data convey about the stimuli per time point. The assessment, via decoding accuracy, of how the discriminative space changes throughout the trial offers a view of the properties of stimuli representation and processing. However, it is not straightforward to know what specific aspects of the signal cause the patterns of decoding accuracy that we observe in perceptual experiments. Without this capacity, it is hard to link these patterns to actual neural mechanisms.</p>
<p>To gain insight into what aspects of the signal underpin decoding accuracy, and therefore the most stable aspects of stimulus processing, I introduce a generative model of multichannel electrophysiological activity (e.g. EEG or MEG) that, under no stimulation, exhibits chaotic phasic and amplitude fluctuations; and that, when stimulated, responds by manipulating certain aspects of the data, such as ongoing phase or signal amplitude, in a stimulus-specific manner. Specifically, in every trial, each channel may or may not respond to stimulation, according to a certain probability. In the model, when a channel responds, it can do it in different ways: (i) by phase-resetting the ongoing oscillation to a given target phase and then entraining to a given frequency, (ii) by an additive oscillatory response independent of the ongoing oscillation, (iii) by modulating the amplitude of the stimulus-relevant oscillations, or (iv) by an additive non-oscillatory (slower) response. This (not exhaustive) list of effects was considered given previous literature (<xref rid="c23" ref-type="bibr">Shah, et al., 2004</xref>; <xref ref-type="bibr" rid="c18">Mazaheri &amp; Jensen, 2006</xref>; <xref rid="c17" ref-type="bibr">Makeig, et al., 2002</xref>; <xref ref-type="bibr" rid="c28">Vidaurre, et al., 2021</xref>), and each effect may be underpinned by distinct neural mechanisms. For example, it is not completely clear the extent to which stimulus processing is sustained by oscillations, and disentangling these effects can help resolving this question. I named this model <italic>genephys</italic> by <italic>gen</italic>erative model of <italic>e</italic>mpirical electro<italic>phys</italic>iological signals. Genephys is empirical in the sense that it purely accounts for features in the signal that are observable, without making assumptions about the underlying neurobiological causes; that is, it can generate signals that, with the right parametrisation, can share empirical properties with real data. In particular, when confronted with a decoding algorithm, the data generated by this model can show patterns of decoding accuracy with similar characteristics to what we observe in real data.</p>
<p>Given the effects that we observe in the stimulus processing literature, and using an example of visual perception as a reference, I observed that two different mechanisms can produce realistic decoding results as we see in real perception: either phase-resetting to a stimulus-specific phase followed by frequency entrainment, or an additive oscillation (unrelated to the ongoing oscillations) with a stimulus-specific phase. Either way, a cross-frequency coupling effect is also necessary, where an additive slower response holds a specific phasic relation with the oscillatory (faster) response. Furthermore, the stimulus-related oscillation needs to span multiple channels, and have a diversity of frequencies and latencies of response across channels. Other experimental paradigms, including motor tasks and decision making, can be investigated with g<italic>enephys</italic>, which is publicly available as a Python package in PyPI<sup><xref ref-type="fn" rid="fn1">1</xref><xref ref-type="fn" rid="fn2">2</xref></sup>.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<p>A generative model of empirical electrophysiological signals: <italic>genephys</italic></p>
<p>While the system is unperturbed, <italic>genephys</italic> is based on sampling spontaneously varying instantaneous frequency and amplitude (i.e. square root of power) time series, <italic>f</italic><sup>rest</sup>and <italic>a</italic><sup>rest</sup> respectively, that are analytically combined to form the sampled signal <italic>x</italic>. Amplitude and frequency are allowed to oscillate within ranges <italic>r</italic><sup><italic>f</italic></sup>and <italic>r</italic><sup><italic>a</italic></sup>. Instantaneous frequency here refers to angular frequency, from which we can obtain the ordinary frequency in Hertz as <inline-formula><inline-graphic xlink:href="522583v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>F</italic> is the sampling frequency of the signal. Both <italic>f</italic><sup>rest</sup> and <italic>a</italic><sup>rest</sup> are sampled separately from autoregressive processes of order one, endowing them with chaotic, non-oscillatory dynamics. Specifically, given autoregressive parameters <italic>b</italic><sup><italic>f</italic></sup>, <italic>b</italic><sup><italic>a</italic></sup> &lt; 1, and Gaussian-noise variables <inline-formula><inline-graphic xlink:href="522583v4_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, I generate <italic>f</italic><sup>rest</sup> and <italic>a</italic><sup>rest</sup> for a given channel as
<disp-formula id="ueqn1">
<graphic xlink:href="522583v4_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Without stimulation, a phase time series <italic>φ</italic><sup>rest</sup> is then built as
<disp-formula id="ueqn2">
<graphic xlink:href="522583v4_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Then, given some Gaussian-distributed measurement noise <italic>ϵ</italic><sub><italic>t</italic></sub> with standard deviation <italic>σ</italic><sub><italic>ϵ</italic></sub>, I build <italic>x</italic> in absence of stimulation) as
<disp-formula id="ueqn3">
<graphic xlink:href="522583v4_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This process is done separately per channel and per trial. Note that, under no stimulation, the channel time series are (asymptotically) uncorrelated. We can think of them as dipoles in brain space. We can induce correlations for instance by projecting these time series onto a higher-dimensional space, which we can consider to be in sensor space, or by using correlated noise. Altogether, this generates chaotic oscillatory data relatively akin to real data.</p>
<p>When a stimulus <italic>k</italic> is presented at time point <italic>τ</italic> of the trial, a perturbation is introduced into the system on the <italic>p</italic> channels that are stimulus-relevant (which can be a subset of the total number of channels). When a channel is not relevant, it does not respond; when it is relevant, it responds to the stimulus with a given probability:
<disp-formula id="ueqn4">
<graphic xlink:href="522583v4_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where <italic>θ</italic><sub><italic>j</italic></sub> is a hyperparameter representing a channel-specific probability. In simpler words, relevant channels might respond in some trials, but not in others. In all the simulations, I set all channel probabilities to a single value, <italic>θ</italic><sub><italic>j</italic></sub> = <italic>θ</italic>, but other configurations are possible.</p>
<p>Given some stimulus presentation at time point <italic>τ</italic>, a channel may respond:</p>
<list list-type="simple">
<list-item><label>-</label><p>By phase-resetting to a condition-specific target phase, and then, when the target phase is reached, by frequency entraining to a given target frequency.</p></list-item>
<list-item><label>-</label><p>By adding a transient (damped), oscillatory response with a condition-specific phase, amplitude and frequency.</p></list-item>
<list-item><label>-</label><p>By increasing the (absolute) amplitude of the ongoing oscillation following stimulus presentation (with no phase effect).</p></list-item>
<list-item><label>-</label><p>By adding a transient, non-oscillatory response with a condition-specific amplitude.</p></list-item>
</list>
<p>The timing of the effect is controlled by a response function. I use an double logarithmic response function, asymmetric around the time of maximum effect <italic>t</italic><sub>max</sub>. For the left side (i.e. before <italic>t</italic><sub>max</sub>),this function is parametrised by: <italic>δ</italic><sub>1</sub>, reflecting the latency of the response in number of time points; and <italic>δ</italic><sub>2</sub>, reflecting how many time points it takes the logarithmic function to go from zero to its maximum before it changes to the right-side logarithmic function. Therefore <italic>t</italic><sub>max</sub> = <italic>δ</italic><sub>1</sub>+ <italic>δ</italic><sub>2</sub> + <italic>τ</italic>. I introduce some noise in <italic>δ</italic> per trial to make the timing of the response to vary, as per <inline-formula><inline-graphic xlink:href="522583v4_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="522583v4_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is a model hyperparameter. This latency noise could be fixed for all channels (absolute stochastic latency) or has a value per channel (relative stochastic latency; (<xref ref-type="bibr" rid="c28">Vidaurre, et al., 2021</xref>)). It is also possible to use different response function parametrisations per channel. For example, we can induce a diversity of latencies for the different channels by using different values of <italic>δ</italic><sub>1</sub> per channel, so that some channels (e.g. those more closely related to primary sensory areas) respond earlier than others (e.g. those related to associative areas). Once the activation reaches its maximum at <italic>t</italic><sub>max</sub>, the decay is also parametrised by a logarithmic function with a parameter <italic>δ</italic><sub>3</sub>, reflecting how many time points it takes the logarithmic function to go from its maximum (at <italic>t</italic><sub>max</sub>) to zero. A different response function can be used for each type of effect, which can be combined. In the <bold>Supplemental Information</bold> there is a full mathematical specification of the response function.</p>
<p>With respect to the phase-reset and frequency entrainment effect, the phase reset occurs before <italic>t</italic><sub>max</sub>, when the target phase is reached. Given condition or stimulus <italic>k</italic>, for each trial and channel,
<disp-formula id="ueqn5">
<graphic xlink:href="522583v4_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where, for each trial, <italic>φ</italic><sup><italic>k</italic></sup> is randomly sampled from a von Mises distribution with mean equal the target phase <inline-formula><inline-graphic xlink:href="522583v4_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and standard deviation <italic>σ</italic><sub><italic>φ</italic></sub>; ∇<sub><italic>t</italic></sub> is the polar gradient between the target phase and the ongoing phase <italic>φ</italic><sub><italic>t</italic>-1</sub>; and <italic>g</italic><sub><italic>t</italic></sub> is the value taken by the response function at time point <italic>t</italic>. After <italic>t</italic><sub>max</sub>, phase-resetting is over, and the phase entrainment period starts,
<disp-formula id="ueqn6">
<graphic xlink:href="522583v4_ueqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The system then entrains to a possibly stimulus-specific, possibly channel-specific, target frequency <italic>f</italic><sup><italic>k</italic></sup>, with a strength that logarithmically decreases as <italic>t</italic> moves away from <italic>t</italic><sub>max</sub>.</p>
<p>With respect to the additive oscillatory response, we consider a sinusoidal oscillator, which is damped by the action of the response function <italic>g</italic><sub><italic>t</italic></sub>:
<disp-formula id="ueqn7">
<graphic xlink:href="522583v4_ueqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, <italic>α</italic><sup><italic>k</italic></sup> reflects the amplitude of the additive oscillation, <italic>ω</italic><sup><italic>k</italic></sup> its frequency, and <italic>γ</italic><sup><italic>k</italic></sup> its phase.</p>
<p>For the amplitude modulation, I apply a multiplying factor to the ongoing amplitude time series:
<disp-formula id="ueqn8">
<graphic xlink:href="522583v4_ueqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>m</italic><sup><italic>k</italic></sup> is a proportional increment; for example, <italic>m</italic><sup><italic>k</italic></sup> = 0.1 would produce an increment of 10% in amplitude at <italic>t</italic><sub>max</sub>.</p>
<p>With respect to the additive non-oscillatory response, we have:
<disp-formula id="ueqn9">
<graphic xlink:href="522583v4_ueqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>s</italic><sup><italic>k</italic></sup> is sampled from a Gaussian distribution, <inline-formula><inline-graphic xlink:href="522583v4_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="522583v4_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is stimulus-specific. Given these elements, the signal is built as
<disp-formula id="ueqn10">
<graphic xlink:href="522583v4_ueqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
This model can be trivially extended to have correlated noise <italic>ϵ</italic><sub><italic>t</italic></sub> across channels.</p>
<p><xref rid="fig1" ref-type="fig">Figure 1</xref>. shows two examples of how the effect of stimulation looks for one trial and one channel. The left panel corresponds to a phase resetting plus frequency entrainment effect, and the middle panel corresponds to an additive oscillation; both are accompanied by an additive non-oscillatory response. Here, the sampled signal <italic>x</italic> is shown in blue on top, and the phase <italic>φ</italic>, frequency <italic>f</italic>, amplitude <italic>a</italic>, additive non-oscillatory response <italic>z</italic>, and additive oscillatory response <italic>y</italic> are shown in red underneath. For comparison, the right panel shows real magnetoencephalography data from a passive visual experiment where a number of images are shown to the subjects at a rate of one image per second (<xref ref-type="bibr" rid="c4">Cichy, et al., 2016</xref>); the measured (filtered) signal is shown in blue, while the red curves correspond to analytical phase, frequency and amplitude (computed via the Hilbert transform on the filtered signal). <bold><xref rid="tbl1" ref-type="table">Table 1</xref></bold> summarises all the hyperparameters that configure the model.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title><italic>genephys</italic> configuration hyperparameters.</title></caption>
<graphic xlink:href="522583v4_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Left and middle: single-trial example of the generated signal (in blue) and its constitutive components (in red): instantaneous phase, frequency and amplitude, as well as an additive non-oscillatory and oscillatory response components; the left panel reflects a phase resetting plus frequency entrainment effect, while the middle panel corresponds to an additive oscillatory response. Right: real (filtered) magnetoencephalography data collected during passive stimuli viewing; the red curves are the analytically computed phase, frequency and amplitude (via the Hilbert transform).</p></caption>
<graphic xlink:href="522583v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2a">
<title>Decoding accuracy to characterise structured invariance</title>
<p>One possible approach to characterise the stable aspects of brain responses to stimulation is the analysis of average evoked responses potentials or fields (ERP/F) (<xref ref-type="bibr" rid="c6">Dawson, 1954</xref>; <xref ref-type="bibr" rid="c25">Steven &amp; Kappenman, 2011</xref>; <xref ref-type="bibr" rid="c21">Pfurtscheller &amp; Lopes da Silva, 1999</xref>). However, this approach is limited because perceptual experiences emerge from activity patterns across multiple brain areas acting in a distributed fashion, whereas ERP/Fs are separately evaluated channel by channel. Also, ERP/F analyses are not concerned with what aspects of the signal carry specific information about the stimulus —i.e. they are not predictive of the stimulus (<xref ref-type="bibr" rid="c15">Kragel, et al., 2018</xref>).</p>
<p>I instead focus on decoding, which finds, in a multivariate fashion, patterns of differential activity between conditions across channels and throughout time (<xref ref-type="bibr" rid="c12">Grootswagers, et al., 2017</xref>). I use linear discriminant analysis (LDA), which estimates a projection or subspace in the data that maximally discriminate between conditions. Throughout the trial, this set of projections reflects information about the dynamics of stimulus processing.</p>
<p>As the read-out of decoding analysis, I use the temporal generalisation matrix (TGM), a <italic>T</italic> × <italic>T</italic> matrix of decoding accuracies (where <italic>T</italic> is the number of time points in the trial), such that one decoding model is trained per time point and tested on every time point of the trial using cross-validation (<xref ref-type="bibr" rid="c14">King &amp; Dehaene, 2014</xref>). The diagonal of the TGM reflects how well we can decode information time point by time point, while the off-diagonal shows how well decoding models generalize to time points different from those where they were trained. As illustrated in <bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold>, where I show a real-data TGM from a subject performing a simple visual task (<xref ref-type="bibr" rid="c4">Cichy, et al., 2016</xref>), the TGM exhibits some characteristic features that we often see throughout the literature. First, there is a strong diagonal band that is relatively narrow early in the trial, often surrounded by areas of worse-than-baseline accuracy. Then, the accuracy on the diagonal becomes progressively wider and weaker, expanding until relatively late in the trial. Also, there are bands of higher-than-baseline accuracy stemming vertically and horizontally from the time points of maximum accuracy (after a brief period of depression), which often show oscillatory behaviour throughout the band. Below, I explore <italic>genephys’</italic> configuration space in relation to its ability to produce patterns of decoding accuracy similar to these that we observe in real data.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Example of an empirical TGM, where the different characteristic features have been highlighted.</p></caption>
<graphic xlink:href="522583v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>I confronted <italic>genephys</italic> to classification-based decoding analysis to characterise how brain activity carries stimulus-specific information. In each simulation, I generated 10 data sets per combination of parameters, each with <italic>N</italic>=250 trials and <italic>T</italic>=250 time points per trial (1s for a sampling frequency of 250Hz). The number of channels is 32. Only one endogenous oscillation was sampled, with (angular) frequencies spontaneously ranging between 0.01 and 0.25π (0.4– 10Hz). I computed a TGM per run and took the average across runs.</p>
<p>For reference, I considered magnetoencephalography (MEG) data recorded while participants viewed object images across 118 categories, as presented in (<xref ref-type="bibr" rid="c4">Cichy, et al., 2016</xref>). Each image category was presented 30 times. Presentation occurred during the first 500ms of the trial, and trials were 1s long, sampled at 250Hz. The multi-channel sensor-space data, epoched around the presentation of each visual stimulus, can be used to train a decoder to predict which visual image is being presented (<xref ref-type="bibr" rid="c4">Cichy, et al., 2016</xref>; <xref ref-type="bibr" rid="c28">Vidaurre, et al., 2021</xref>). Here, I decoded whether the image corresponded to an animate category (a dog) or inanimate (a pencil). <bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold> shows a TGM for an example subject, where some archetypal characteristics are highlighted. In the experiments below, specifically, I focus on the strong narrow diagonal at the beginning of the trial, the broadening of accuracy later in the trial, and the vertical/horizontal bars of higher-than-chance accuracy. Importantly, this specific example in <bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold> is only meant as a reference, and therefore I did not optimise the model hyperparameters to this TGM (except in the last subsection), or showed any quantitative metric of similarity.</p>
<sec id="s3a">
<title>Oscillatory components underlying the observed decoding patterns</title>
<p>In real data, we often see oscillatory patterns in the TGM, indicating that the subspace of brain activity that carries information about the stimulus must have oscillatory elements. At least two distinct mechanisms may be behind this phenomenon: first, an ongoing oscillation might reset its phase and then entrain to a given frequency in a stimulus-specific fashion; second, a stimulus-specific oscillatory response might by added to the signal after stimulus presentation on top of the existing ongoing oscillation. Essentially, the difference between the two is that, in the phase-resetting case, the ongoing oscillations are altered; while in the other case the additive oscillation coexists with the ongoing oscillations. Next, I use <italic>genephys</italic> together with decoding analysis to compare between these two alternatives, showing that both can produce decoding patterns similar to what we observe empirically in real experiments.</p>
<p>In the simulations, all 32 channels convey information but with a relatively low activation probability (<italic>θ</italic> = 1/6). For phase-resetting and frequency entrainment, I considered a diverse range of entrainment frequencies (between 0.1 and 0.2 in angular frequency) and latencies of response (<italic>δ</italic><sub>1</sub> = 0–160ms) across channels. For the additive oscillatory response, I considered a similar range of frequencies and latencies of response across channels. (I will show below that channel stochasticity and frequency diversity are both important to produce realistic decoding patterns). The difference between the two fictitious stimuli lied in their different target frequencies (that is, <inline-formula><inline-graphic xlink:href="522583v4_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for phase-resetting and <italic>γ</italic><sup><italic>k</italic></sup> for the additive oscillation; see specification of the model in <bold>Methods</bold>). I also included an additive non-oscillatory response with a stimulus-specific amplitude, which (as I will also show later) is important to produce realistic decoding results. I did not optimise the parameters of the model to reflect the fine details from real data TGMs, since TGMs vary across subjects (see <bold><xref rid="figS1" ref-type="fig">Supplementary Figure 1</xref></bold>) and experimental paradigms (<xref ref-type="bibr" rid="c14">King &amp; Dehaene, 2014</xref>).</p>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref>. shows TGMs (top) together with one-channel ERPs (where each stimulus is represented by a different tonality of blue or red; bottom) for the sampled signal <italic>x</italic> and its various constitutive elements: ongoing phase <italic>φ</italic>, frequency <italic>f</italic>, amplitude <italic>a</italic>, additive non-oscillatory response <italic>z</italic>, and, when applicable, an additive oscillatory response <italic>y</italic>. The left panels show results for phase reset plus frequency entrainment, where we can see an effect on the ongoing phase. The middle panels show results for the additive oscillation; here, there is no effect on the ongoing phase, and, instead, the additive oscillatory response <italic>y</italic> is shown at the bottom. The right panels show an example from real data, where phase, frequency and amplitude were computed analytically using the Hilbert transform on the filtered data.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Examples of two configurations based on phase-resetting and frequency entrainment (left) and additive oscillatory responses (middle), shown together with results obtained from real data (right). The top panels show temporal generalisation matrices (TGM) from standard decoding analysis. The bottom panels show average evoked responses (ERP/F) for the sampled signal (blue) and its components (red); the two stimuli are represented with different tonalities of blue or red.</p></caption>
<graphic xlink:href="522583v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Although the exact details differ from the right panels of the figure, both types of effects produce patterns reproducing the characteristic signatures of real data. These include the strong diagonal, the vertical/horizontal bands of high generalisation accuracy, and the broadening of accuracy in later stages of the trial. Note that phase resetting plus frequency entrainment is, everything else held constant, a stronger effect than the additive oscillatory response. This is because, for the latter, the ongoing oscillations (here, non-stimulus specific) can interfere with the phase of the additive response, impeding cross-trial phase locking throughout the trial. For phase-resetting, on the other hand, the ongoing oscillation <italic>is</italic> the effect and there is no interference. In this particular example, anyway, the range for the additive oscillation (0.1–0.2) was much narrower than that of the ongoing oscillation (0.01–0.25π), making the interference more unlikely; that is, the ongoing oscillation phase is approximately averaged out, and treated as noise by the decoding algorithm.</p>
</sec>
<sec id="s3b">
<title>The distribution of stimulus-specific information spans multiple channels and is stochastic</title>
<p>Next, I use <italic>genephys</italic> to show that the stimulus-specific information spans a large number of channels, and do so stochastically. I focus on the additive oscillatory response effect, which, as shown in the previous section, can produce comparable results to phase-resetting plus frequency entrainment.</p>
<p>I varied the number of relevant channels <italic>p</italic> as well as the probability of activation <italic>θ</italic> for the relevant channels, so that the subset of channels that activate is different for every trial. <bold><xref rid="fig4" ref-type="fig">Figure 4</xref></bold> shows the TGMs for various combinations of <italic>p</italic> and <italic>θ</italic>, from a configuration where there are few relevant channels that always respond (<italic>p</italic> = 1, <italic>θ</italic> = 1) to another where there are many relevant channels that only respond sparsely (<italic>p</italic> = 32, <italic>θ</italic> = 1/8). As previously, channels have diverse frequencies and latencies of response. An additive non-oscillatory response is also included as an effect.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Having more dimensions (channels) carrying stimulus-specific information, but with a larger degree of stochasticity, produces more realistic decoding patterns than having fewer dimensions with a lower degree of stochasticity. Here stochasticity referred to the channels’ probability of activation.</p></caption>
<graphic xlink:href="522583v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Contrary to empirical TGMs (which have a relatively stylised diagonal), having only a few relevant channels (first three panels of <bold><xref rid="fig4" ref-type="fig">Figure 4</xref></bold>) produces unrealistically geometric patterns. This indicates that, in real data, the subspace of the data that contains information about the stimulus needs to be multi-dimensional; that is, that the amount of relevant channels must be relatively high (as in the fourth panel of <bold><xref rid="fig4" ref-type="fig">Figure 4</xref></bold>). However, the contribution of each channel must entail some sort of noise or instability (in this case expressed by having probabilistic activations), or else the decoding accuracy becomes unrealistically perfect. In summary, the subspace of brain activity that carries out information about the condition is highly stochastic at the single trial level.</p>
</sec>
<sec id="s3c">
<title>Frequency entrainment span multiple frequencies and latencies</title>
<p>In the previous sections, I showed that a noisy, additive oscillation effect (or, alternatively, a phase-reset plus frequency entrainment effect) across multiple channels can generate decoding patterns as we see in real data. Next, I demonstrate that the effect must span a diversity of frequencies across channels, as well as a diversity of latencies of response. In real data, frequency diversity can be expressed as, for example, a gradient of frequencies from primary towards more associative areas; while latency diversity could reflect phenomena such that primary areas responding earlier than associative areas.</p>
<p>For frequency diversity, each channel is endowed with a different effect-related frequency, such that frequencies are not multiples of each other (specifically, they have different values of <italic>ω</italic><sup><italic>k</italic></sup> between 0.1 and 0.2 in angular frequency; see <bold>Methods</bold>). For latency diversity, channels do not respond simultaneously but with different values of <italic>δ</italic><sub>1</sub> between 0 and 120ms. <bold><xref rid="fig5" ref-type="fig">Figure 5</xref></bold> shows a two-by-two design. On the left column, I set <italic>genephys</italic> to have a uniform frequency of response (i.e. all channels have the same frequency of response), while on the right column it uses a diversity of frequencies. On the top row, we have a uniform latency of response (i.e. all channels have the same latency of response), whereas on the bottom row we have a diversity of latencies of response. See <bold><xref rid="fig2" ref-type="fig">Supplemental Figure 2</xref></bold> for a similar result for phase-resetting followed by frequency entrainment.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Oscillatory responses to stimulation occur in a diversity of frequencies and latencies across channels. Top row, single latency of response; bottom row, diverse latencies across channels. Left column, single frequency; right column, diverse frequencies across channels. Bottom-right is the most realistic TGM.</p></caption>
<graphic xlink:href="522583v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>As mentioned, real data normally yield a relatively tight band of high decoding accuracy along the diagonal, often accompanied from contiguous parallel bands of below-baseline accuracy. Critically, the fact that we do not typically observe a chequer pattern means that the trajectory of phase across channels does not repeat itself periodically. If it did, it would show patterns as in the top-left panel, where the uniformity of frequencies and latencies gives rise to an unrealistically regular pattern —such that a decoder trained at a certain time point will become equally accurate again after one cycle at the entrained frequency. Having a diversity of latencies but not of frequencies produces another regular pattern consisting of alternating, parallel bands of higher/lower than baseline accuracy. This, shown in the bottom-left panel, is not what we see in real data either. Having a diversity of frequencies but not of latencies gets us closer to a realistic pattern, as we see in the top-right panel. Finally, having diversity of both frequencies and latencies produces the most realistic pattern, as we can see by comparing to the examples in <bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold> and <bold><xref rid="fig1" ref-type="fig">Supplemental Figure 1</xref></bold>, and many others throughout the literature (<xref ref-type="bibr" rid="c14">King &amp; Dehaene, 2014</xref>). Similar conclusions can be drawn from <bold><xref rid="fig2" ref-type="fig">Supplemental Figure 2</xref></bold> for phase-resetting plus frequency entrainment.</p>
<p>In summary, these results show that it is not only important for the stimulus-relevant subspace of activity to be spatially high-dimensional, but also temporally high-dimensional.</p>
</sec>
<sec id="s3d">
<title>A slower additive component is coupled with the oscillatory response</title>
<p>In real data, we normally see a broadening of decoding accuracy in the TGM as time progresses throughout the trial (see <bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold>). This is often interpreted as neural representations becoming more stable at latter stages of the trial, which is putatively linked to memory encoding and higher cognitive processes. In <bold><xref rid="fig6" ref-type="fig">Figure 6</xref></bold>, I show that this effect can be reproduced on synthetic data through the addition of a slowly-progressing, non-oscillatory response.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><p>An additive non-oscillatory response is needed to produce realistic TGMs with a broadening of decoding accuracy at later stages of the trial. <bold>A</bold>: By increasing the strength of the non-oscillatory response, the broadening of accuracy becomes more prominent. <bold>B</bold>: Changing the nature of the phasic relationship between the slower and the faster (oscillatory component) greatly influences the TGM, from having all channels in-phase (left) towards having all channels anti-phase (right).</p></caption>
<graphic xlink:href="522583v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Specifically, I set up a response function such that, after stimulus presentation, the non-oscillatory additive response ramps up to a stimulus-specific target value in about 100s, and then slowly decays to finally vanish at around 800ms. Also, I modulate the strength of the oscillatory response using values of <inline-formula><inline-graphic xlink:href="522583v4_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (see <bold>Methods</bold>) that differ between the two stimuli by a magnitude that ranges from 0.0 (no difference) to 1.0 (for reference, the examples in the previous figures had a difference of 0.5). As seen in <bold><xref rid="fig6" ref-type="fig">Figure 6A</xref></bold>, the strength of decoding accuracy grows as the difference in the slow response increases.</p>
<p>Another feature commonly seen on real data are the vertical and horizontal bars of high accuracy stemming from the time point of maximum accuracy (see <bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold>), which is sometimes interpreted as evidence of stimulus representation recurrence in the brain. I show in <bold><xref rid="fig6" ref-type="fig">Figure 6B</xref></bold> that this feature emerges from a phase coupling between the oscillatory component and the slower component (with respect to the stimuli). For example, following the notation established in <bold>Methods</bold>, an in-phase relationship means that the sign of <inline-formula><inline-graphic xlink:href="522583v4_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> at <italic>t</italic><sub>max</sub> is the same than the sign of <inline-formula><inline-graphic xlink:href="522583v4_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Note that the sign of <inline-formula><inline-graphic xlink:href="522583v4_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> will likely be maintained for most of the trial since this component is slow, therefore creating the effect in the TGM. That was the case for all panels of <bold><xref rid="fig6" ref-type="fig">Figure 6A</xref></bold>. For <bold><xref rid="fig6" ref-type="fig">Figure 6B</xref></bold>, while keeping <inline-formula><inline-graphic xlink:href="522583v4_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> (as in the middle panel of <bold><xref rid="fig6" ref-type="fig">Figure 6A</xref></bold>), I varied the phase consistency between the oscillatory and the non-oscillatory components. In the leftmost panel, the oscillatory and the non-oscillatory components are in-phase for all channels, while in the rightmost panel, they are anti-phase for all channels; in between, 25%, 50% and 75% of the channels are in-phase (and 75%, 50% and 25% are anti-phase, respectively). As observed, the type of phase consistency between the oscillatory and the non-oscillatory component has a strong impact on the TGM. In particular, the in-phase relation bears the most consistent patterns with the considered real data.</p>
</sec>
<sec id="s3e">
<title>Amplitude increases modulate the size of the effect</title>
<p>Are modulations in the amplitude of the stimulus-specific oscillation necessary for the effects we observe in real data? They are not, but they can enhance the already existing patterns.</p>
<p>I generated data sets with an additive oscillatory component effect. In each of them, I applied a different amount of amplitude enhancement, from no increase to a five-fold increase. These did not entail any effect on the signals’ phase. I considered two characteristic features of the TGM: (i) the diagonal, and (ii) a vertical slice stemming from the time point of maximum accuracy. <bold><xref rid="fig7" ref-type="fig">Figure 7</xref></bold> shows the results. As observed, the main features are present in all runs regardless of the size of the amplitude effect, although they were more prominent for higher amplitude modulations. An amplitude modulation on a phase-resetting effect causes a similar effect (not shown). Note that an amplitude effect without a phase effect would not result in any phase locking across trials, and therefore could not lead to any significant decoding accuracy.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Enhancing the amplitude of the stimulus-relevant oscillation is not strictly necessary to produce realistic TGMs, but it can enlarge the effects. Two features of the TGM are highlighted: the diagonal, and a vertical slice at the time point of maximum accuracy.</p></caption>
<graphic xlink:href="522583v4_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3f">
<title>Fitting an empirical TGM</title>
<p>The previous analyses were descriptive in the sense that they did not quantify how much the generated TGMs resembled a specific empirical TGM. This was deliberate, because empirical TGMs vary across subjects and experiments, and I aimed at characterising them as generally as possible by looking at some characteristic features in broad terms. For example, while TGMs typically have a strong diagonal and horizontal/vertical bars of high accuracy, questions such as when these effects emerge and for how long are highly dependent on the experimental paradigm. For the same reason, I did not optimise the model hyperparameters, limiting myself to observing the behaviour of the model across some characteristic configurations. But, often, one’s interests are more specific. Then, it would be interesting to optimise some key hyperparameters of the model to maximise the correlation with a particular empirical TGM.</p>
<p>To illustrate how a data set could be more explicitly considered, I took an empirical TGM computed from the visual experiment in (<xref ref-type="bibr" rid="c4">Cichy, et al., 2016</xref>). Specifically, this is a cross-average TGM (over ten subjects) obtained from decoding animate versus inanimate stimuli. Using an additive oscillatory response, and fixing the rest of the hyperparameters to a sensible configuration (the one that produced the middle panel in <bold><xref rid="fig6" ref-type="fig">Figure 6A</xref></bold>), I varied, within a reasonable range, two parameters of the response function that control the temporal shape of the effect: the rise slope <italic>δ</italic><sub>2</sub> and the fall slope <italic>δ</italic><sub>3</sub>; see <bold>Methods</bold>. From each pair of parameters, I generated ten data sets and computed ten TGMs; I then correlated the average of these with the empirical TGM from real data. <bold><xref rid="fig8" ref-type="fig">Figure 8</xref></bold> shows a heatmap of the resulting correlations. For a specific configuration (<italic>δ</italic><sub>2</sub> = 0.06<italic>s δ</italic><sub>3</sub> = 0.09s), the correlation peaks at <italic>r</italic>=0.7.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><p>Correlation between an empirical TGM and the sampled TGM (averaged across ten runs of the simulation) across a range of configurations of the response function, which defines the temporal dynamics of the effect. The maximum correlation with the empirical TGM is slightly over 0.7</p></caption>
<graphic xlink:href="522583v4_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>Brain’s responses to even the simplest stimuli are characteristically variable. This is not surprising, given the brain’s plasticity, and the fact that its endogenous activity is ever-changing. It also speaks to the brain’s degeneracy (<xref ref-type="bibr" rid="c8">Edelman &amp; Gally, 2001</xref>) —i.e. that there are many possible neural trajectories that can achieve a single purpose. How this variability translates into behaviour and experience is however an open question. Anyhow, it seems reasonable that, at some level, brain responses must keep some invariant aspects so that our perceptual experiences remain stable. Here, I investigated, using a novel generative model, the most stable aspects of brain responses to stimuli as seen through the lens of decoding accuracy, which is, by definition, based on averaging. Previous work has analysed the nature of brain responses to perceptual stimulation using average evoked responses (<xref rid="c22" ref-type="bibr">Sauseng, et al., 2007</xref>), arguing either for a predominant role of additive responses (<xref rid="c23" ref-type="bibr">Shah, et al., 2004</xref>; <xref ref-type="bibr" rid="c18">Mazaheri &amp; Jensen, 2006</xref>) or phase-resetting (<xref rid="c17" ref-type="bibr">Makeig, et al., 2002</xref>). These studies looked at the average response to a given stimulus, but did not investigate what aspects of the data carry information about the identity of the stimulus; this is the focus of this paper and the goal of genephys, the proposed model.</p>
<p>Genephys has different available types of effect, including phase resets, additive damped oscillations, amplitude modulations, and non-oscillatory responses. All of these elements, which may relate to distinct neurobiological mechanisms, are configurable and can be combined to generate a plethora of TGMs that, in turn, can be contrasted to specific empirical TGMs. This way, we can gain insight on what mechanisms might be at play in a given task.</p>
<p>The demonstrations here are not meant to be tailored to a specific data set, and are, for the most part, intentionally qualitative. TGMs do vary across experiments and subjects; and the hyperparameters of the model can be explicitly optimised to specific scientific questions, data sets, and even individuals. In order to explore the space of configurations effectively, an automatic optimisation of the hyperparameter space using, for instance, Bayesian optimisation (<xref ref-type="bibr" rid="c16">Lorenz, et al., 2017</xref>) could be advantageous. This may lead to the identification of very specific (spatial, spectral and temporal) features in the data that may be neurobiologically interpreted.</p>
<p>Importantly, the list of effects that I have explored here is not exhaustive. For example, I have considered additive oscillatory responses in the form of sinusoidal waves. Another possibility could be to have additive oscillatory responses that are non-linear, i.e. with a tendency to spend more time in certain phases (for example, having wider peaks than throughs); in this case, even in the absence of phase-locking between trials, we could potentially have a significant evoked response due to phase asymmetry (Nikulin, et al., 2007). For simplicity, also, I have considered independent sources of activity that exhibit correlations only through the induced effect. In practice, brain areas are in constant communication even in the absence of stimulation. Alternatives where sources are modelled as coupled oscillators (<xref ref-type="bibr" rid="c2">Breakspear, et al., 2010</xref>; <xref rid="c3" ref-type="bibr">Cabral, et al., 2014</xref>), or where there is correlated noise, are also possible. Also for simplicity, I have considered that every channel has one endogenous fundamental frequency only; in practice, multiple ongoing frequencies coexist and interact, potentially affecting the TGM if they are not completely averaged out across trials. Finally, the inherent stochasticity of the stimulus-specific space of activity can take various forms; here, I have explored a probabilistic activation of the channels, but others, such as noise in the phase distribution, are also possible and can also be simulated using <italic>genephys</italic>.</p>
<p>Also importantly, I have shown that standard decoding analysis can differentiate between these explanations only to some extent. For example, the effects induced by phase-resetting and the use of additive oscillatory components are not enormously different in terms of the resulting TGMs. In future work, alternatives to standard decoding analysis and TGMs might be used to disentangle these sources of variation (<xref rid="c27" ref-type="bibr">Vidaurre, et al., 2019</xref>).</p>
<p>Overall, the results obtained from applying <italic>genephys</italic> suggest that the stable aspects of brain activity regarding stimulus processing comprise phasic modulations of an oscillatory component coupled with a slower component, with an important role played by the nature of this coupling. The subspace of brain activity induced by the effect is high-dimensional in both the spatial domain (it must span many channels) and the frequency domain (it must involve a great diversity of frequencies and exhibit a diversity of latencies). This effect may be accompanied by an amplitude modulation. Above and beyond these average patterns, the stimulus-specific subspace of brain responses remains highly stochastic at the trial level.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>DV is supported by a Novo Nordisk Foundation Emerging Investigator Fellowship (NNF19OC-0054895) and an ERC Starting Grant (ERC-StG-2019-850404). I also thank the Wellcome Trust for support (106183/Z/14/Z, 215573/Z/19/Z).</p>
</ack>
<sec id="s5">
<title>Conflicts of interest</title>
<p>None</p>
</sec>
<sec id="s6">
<title>Code accessibility</title>
<p>The model is available as a Python package in PyPI and Github.</p>
</sec>
<ref-list>
<title>Bibliography</title>
<ref id="c1"><mixed-citation publication-type="other"><string-name><surname>Axel</surname>, <given-names>R.</given-names></string-name>, <year>1995</year>. <article-title>The molecular logic of smell</article-title>. <source>Scientific American</source>, pp. <fpage>154</fpage>–<lpage>159</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="other"><string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Heitmann</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Daffertshofer</surname>, <given-names>A.</given-names></string-name>, <year>2010</year>. <article-title>Generative models of cortical oscillations: neurobiological implications of the Kuramoto model</article-title>. <source>Frontiers in human neuroscience</source>, p. <fpage>190</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Cabral</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal>, <year>2014</year>. <article-title>Exploring mechanisms of spontaneous functional connectivity in MEG: How delayed network interactions lead to structured amplitude envelopes of band-pass filtered oscillations</article-title>. <source>NeuroImage</source>, pp. <fpage>423</fpage>–<lpage>435</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="other"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Oliva</surname>, <given-names>A.</given-names></string-name>, <year>2016</year>. <article-title>Similarity-based fusion of MEG and fMRI reveals spatio-temporal dynamics in human cortex during visual object recognition</article-title>. <source>Cerebral Cortex</source>, pp. <fpage>3563</fpage>–<lpage>3579</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="other"><string-name><surname>Croner</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Purpura</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Kaplan</surname>, <given-names>E.</given-names></string-name>, <year>1993</year>. <article-title>Response variability in retinal ganglion cells of primates</article-title>. <source>Proceedings of the National Academy of Sciences</source>, pp. <fpage>8128</fpage>–<lpage>8130</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="other"><string-name><surname>Dawson</surname>, <given-names>G. D.</given-names></string-name>, <year>1954</year>. <article-title>A summation technique for the detection of small evoked potentials</article-title>. <source>Electroencephalography &amp; clinical neurophysiology</source>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="book"><string-name><surname>Dowling</surname>, <given-names>J. E.</given-names></string-name>, <year>1987</year>. <source>The retina: an approachable part of the brain</source>. <publisher-name>s.l.:Harvard University Press</publisher-name>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="other"><string-name><surname>Edelman</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Gally</surname>, <given-names>J.</given-names></string-name>, <year>2001</year>. <article-title>Degeneracy and complexity in biological systems</article-title>. <source>Proceedings of the National Academy of Sciences</source>, pp. <fpage>13763</fpage>–<lpage>13768</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="other"><string-name><surname>Freeman</surname>, <given-names>W. J.</given-names></string-name>, <year>1978</year>. <article-title>Spatial properties of an EEG event in the olfactory bulb and cortex</article-title>. <source>Electroencephalography and Clinical Neurophysiology</source>, pp. <fpage>586</fpage>–<lpage>605</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="book"><string-name><surname>Freeman</surname>, <given-names>W. J.</given-names></string-name>, <year>2000</year>. <source>How brains make up their minds</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Columbia University Press</publisher-name>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="other"><string-name><surname>Garrett</surname>, <given-names>D. D.</given-names></string-name> <etal>et al.</etal>, <year>2013</year>. <article-title>Moment-to-moment brain signal variability: A next frontier in human brain mapping?</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, pp. <fpage>610</fpage>–<lpage>624</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="other"><string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wardle</surname>, <given-names>S. G.</given-names></string-name> &amp; <string-name><surname>Carl</surname>, <given-names>T. A.</given-names></string-name>, <year>2017</year>. <article-title>Decoding Dynamic Brain Patterns from Evoked Responses: A Tutorial on Multivariate Pattern Analysis Applied to Time Series Neuroimaging Data</article-title>. <source>Journal of Cognitive Neuroscience</source>, Issue <issue>29</issue>, p. <fpage>677</fpage>–<lpage>697</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="other"><string-name><surname>Haxby</surname>, <given-names>J. V.</given-names></string-name>, <string-name><surname>Connolly</surname>, <given-names>A. C.</given-names></string-name> &amp; <string-name><surname>Guntupalli</surname>, <given-names>S.</given-names></string-name>, <year>2014</year>. <article-title>Decoding Neural Representational Spaces Using Multivariate Pattern Analysis</article-title>. <source>Annual review of neuroscience</source>, pp. <fpage>435</fpage>–<lpage>456</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="other"><string-name><surname>King</surname>, <given-names>J.-R.</given-names></string-name> &amp; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name>, <year>2014</year>. <article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title>. <source>Trends in cognitive sciences</source>, pp. <fpage>203</fpage>–<lpage>210</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="other"><string-name><surname>Kragel</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Koban</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Feldman Barrett</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name>, <year>2018</year>. <article-title>Representation, pattern information, and brain signatures: from neurons to neuroimaging</article-title>. <source>Neuron</source>, pp. <fpage>257</fpage>–<lpage>273</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="other"><string-name><surname>Lorenz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hampshire</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Leech</surname>, <given-names>R.</given-names></string-name>, <year>2017</year>. <article-title>Neuroadaptive Bayesian optimization and hypothesis testing</article-title>. <source>Trends in Cognitive Sciences</source>, pp. <fpage>155</fpage>–<lpage>167</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="other"><string-name><surname>Makeig</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal>, <year>2002</year>. <article-title>Dynamic Brain Sources of Visual Evoked Responses</article-title>. <source>Science</source>, pp. <fpage>690</fpage>–<lpage>694</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Mazaheri</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name>, <year>2006</year>. <article-title>Posterior α activity is not phase-reset by visual stimuli</article-title>. <source>Proceedings of the National Academy of Sciences</source>, pp. <fpage>2948</fpage>–<lpage>2952</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="other"><string-name><surname>McIntosh</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Kovacevic</surname>, <given-names>N.</given-names></string-name> &amp; <string-name><surname>Itier</surname>, <given-names>R. J. J.</given-names></string-name>, <year>2008</year>. <article-title>Increased Brain Signal Variability Accompanies Lower Behavioral Variability in Development</article-title>. <source>PLOS Computational Biology</source>, p. <fpage>e1000106</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="other"><string-name><surname>Nikulin</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal>, <year>2007</year>. <article-title>A novel mechanism for evoked responses in the human brain</article-title>. <source>European Journal of Neuroscience</source>, pp. <fpage>3146</fpage>–<lpage>3154</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="other"><string-name><surname>Pfurtscheller</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Lopes da Silva</surname>, <given-names>F.</given-names></string-name>, <year>1999</year>. <article-title>Event-related EEG/MEG synchronization and desynchronization: basic principles</article-title>. <source>Clinical Neurophysiology</source>, pp. <fpage>1842</fpage>–<lpage>1857</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="other"><string-name><surname>Sauseng</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal>, <year>2007</year>. <article-title>Are event-related potential components generated by phase resetting of brain oscillations? A critical discussion</article-title>. <source>Neuroscience</source>, pp. <fpage>1435</fpage>–<lpage>1444</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="other"><string-name><surname>Shah</surname>, <given-names>A. S.</given-names></string-name> <etal>et al.</etal>, <year>2004</year>. <article-title>Neural Dynamics and the Fundamental Mechanisms of Event-related Brain Potentials</article-title>. <source>Cerebral Cortex</source>, p. <fpage>476</fpage>–<lpage>483</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="other"><string-name><surname>Stein</surname>, <given-names>R. B.</given-names></string-name>, <string-name><surname>Gossen</surname>, <given-names>E. R.</given-names></string-name> &amp; <string-name><surname>Jones</surname>, <given-names>K. E.</given-names></string-name>, <year>2005</year>. <article-title>Neuronal variability: noise or part of the signal?</article-title>. <source>Nature Reviews Neuroscience</source>, p. <fpage>389</fpage>–<lpage>397</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="book"><string-name><surname>Steven</surname>, <given-names>L. J.</given-names></string-name> &amp; <string-name><surname>Kappenman</surname>, <given-names>S. E.</given-names></string-name>, <year>2011</year>. <source>The Oxford handbook of event-related potential components</source>. <publisher-name>s.l.:Oxford university press</publisher-name>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="other"><string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Wolff</surname>, <given-names>M. J.</given-names></string-name> &amp; <string-name><surname>Spaak</surname>, <given-names>E.</given-names></string-name>, <year>2015</year>. <article-title>Decoding Rich Spatial Information with High Temporal Resolution</article-title>. <source>Trends in Cognitive Sciences</source>, pp. <fpage>636</fpage>–<lpage>638</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="other"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal>, <year>2019</year>. <article-title>Temporally unconstrained decoding reveals consistent but timevarying stages of stimulus processing</article-title>. <source>Cerebral Cortex</source>, pp. <fpage>863</fpage>–<lpage>874</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="other"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, <year>2021</year>. <article-title>Dissociable Components of Information Encoding in Human Perception</article-title>. <source>Cerebral Cortex</source>, pp. <fpage>5664</fpage>–<lpage>5675</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s7">
<title>Supplemental methods: The response function</title>
<p>Assuming the stimulus was presented at time point <italic>τ</italic> within a given trial, and that both <italic>t</italic> and <italic>τ</italic> are expressed in number of time points, the response function is asymmetric around <italic>t</italic><sub>max</sub>, when the function takes the value 1.0 from both sides. In the experiments above, both left and right parts are logarithmic. Mathematically,
<disp-formula id="ueqn11">
<graphic xlink:href="522583v4_ueqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where</p>
<list list-type="order">
<list-item><p><italic>δ</italic><sub>1</sub>reflects the latency of the response in number of time points,</p></list-item>
<list-item><p><italic>δ</italic><sub>2</sub> is how many time points it takes the left logarithmic function to go from 0.0 to 1.0,</p></list-item>
<list-item><p><italic>δ</italic><sub>3</sub> is how many time points it takes the right logarithmic function to go from 1.0 to 0.0,</p></list-item>
<list-item><p><italic>t</italic><sub>max</sub> = <italic>δ</italic><sub>1</sub> + <italic>δ</italic><sub>2</sub> + <italic>τ</italic> is the time point of maximum response (i.e. the changing point between the two functions),</p></list-item>
<list-item><p><italic>C</italic><sub>1</sub>,<italic>C</italic><sub>2</sub>,<italic>T</italic><sub>1</sub>,<italic>T</italic><sub>2</sub> are normalisation constants chosen such that the logarithmic functions are bounded between 0.0 and 1.0, and G(<italic>t</italic><sub>max</sub>; <italic>τ</italic>) =1.0 from both sides.</p></list-item>
<list-item><p><italic>ς</italic><sub>1</sub> and <italic>ς</italic><sub>2</sub> determine the shape of the logarithmic functions (here chosen to 2 and 4 respectively).</p></list-item>
</list>
<p>Note that, thanks to the normalisation constants, both the left and the right side of the response function take values between 0.0 and 1.0. There is also the possibility of using an exponential function —which is faster to decay— in either side, but I did not use it in the experiments. For example, in the left part, this would take the form:
<disp-formula id="ueqn12">
<graphic xlink:href="522583v4_ueqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>C</italic><sub>3</sub> and <italic>C</italic><sub>4</sub> are normalisation constants.</p>
</sec>
<sec id="s8">
<title>Supplemental figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplemental Figure 1:</label>
<caption><p>TGMs from six real data example subjects performing a passive viewing paradigm</p></caption>
<graphic xlink:href="522583v4_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplemental Figure 2:</label>
<caption><p>Phase reset plus frequency entrainment effect. Top row, single latency of response; bottom row, diverse latencies across channels; left column, single frequency; right column; diverse frequencies across channels.</p></caption>
<graphic xlink:href="522583v4_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<fn-group>
<fn id="fn1">
<label><sup>1</sup></label>
<p><ext-link ext-link-type="uri" xlink:href="https://github.com/vidaurre/genephys">https://github.com/vidaurre/genephys</ext-link></p></fn>
<fn id="fn2">
<label><sup>2</sup></label>
<p><ext-link ext-link-type="uri" xlink:href="https://genephys-doc.readthedocs.io/en/latest/">https://genephys-doc.readthedocs.io/en/latest/</ext-link></p></fn>
</fn-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87729.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Luo</surname>
<given-names>Huan</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a <bold>valuable</bold> finding on developing a state-of-the-art generative model of brain electrophysiological signals to explain temporal decoding matrices widely used in cognitive neuroscience. The evidence supporting the authors' claims is <bold>convincing</bold>. The results will be strengthened by providing more clear mappings between neurobiological mechanisms and signal generators in the model. The work will be of interest to cognitive neuroscientists using electrophysiological recordings.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87729.2.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>With genephys, the author provides a generative model of brain responses to stimulation. This generative model allows to mimic specific parameters of a brain response at the sensor level, to test the impact of those parameters on critical analytic methods utilized on real M/EEG data. Specifically, they compare the decoding output for differently set parameters to the decoding pattern observed in a classical passive viewing study in terms of the resulting temporal generalization matrix (TGM). They identify that the correspondence between the mimicked and the experimental TGM to depend on an oscillatory component that spans multiple channels, frequencies, and latencies of response; and an additive, slower response with a specific (cross-frequency) relation to the phase of the oscillatory, faster component.</p>
<p>A strength of the article is that it considers the complexity of neural data that contribute to the findings obtained in stimulation experiments. An additional strength is the provision of a Python package that allows scientists to explore the potential contribution of different aspects of neural signals to obtained experimental data and thereby to potentially test their theoretical assumptions critical parameters that contribute to their experimental data.</p>
<p>A weakness of the paper is that the power of the model is illustrated for only one specific set of parameters, added in a stepwise manner and the comparison to on specific empirical TGM, assumed to be prototypical; And that this comparison remains descriptive. (That is could a different selection of parameters lead to similar results and is there TGM data which matches these settings less well.) It further remained unclear to me, which implications may be drawn from the generative model, following from the capacities to mimic this specific TGM (i) for more complex cases, such as the comparison between experimental conditions, and (ii) about the complex nature of neural processes involved.</p>
<p>Towards this end I would appreciate (i) a more profound explanation of the conclusions that can be drawn from this specific showcase, including potential limitations, as well as wider considerations of how scientists may empower the generative model to (ii) understand their experimental data better and (iii) which added value the model may have in understanding the nature of underlaying brain mechanism (rather than a mere technical characterization of sensor data).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87729.2.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This paper introduces a new model that aims to explain the generators of temporal decoding matrices (TGMs) in terms of underlying signal properties. This is important because TGMs are regularly used to investigate neural mechanisms underlying cognitive processes, but their interpretation in terms of underlying signals often remains unclear. Furthermore, neural signals are often variant over different instances of stimulation despite behaviour being relatively stable. The author aims to tackle these concerns by developing a generative model of electrophysiological data and then showing how different parameterizations can explain different features of TGMs. The developed technique is able to capture empirical observations in terms of fundamental signal properties. Specifically, the model shows that complexity is necessary in terms of spatial configuration, frequencies and latencies to obtain a TGM that is comparable to empirical data.</p>
<p>The major strength of the paper is that the novel technique has the potential to further our understanding of the generators of electrophysiological signals which are an important way to understand brain function. The paper clearly outlines how the method can be used to capture empirical data. Furthermore, the used techniques are state-of-the-art and the developed model is publicly shared in open source code.</p>
<p>On the other hand, there is no unambiguous mapping between neurobiological mechanisms and different signal generators, making it hard to draw firm conclusions about neural underpinnings based on this analysis.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87729.2.sa3</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Vidaurre</surname>
<given-names>Diego</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9650-2229</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1:</bold></p>
<p>A weakness of the paper is that the power of the model is illustrated for only one specific set of parameters, added in a stepwise manner and the comparison to one specific empirical TGM, assumed to be prototypical; And that this comparison remains descriptive. (That is could a different selection of parameters lead to similar results and is there TGM data which matches these settings less well.)</p>
</disp-quote>
<p>The fact that the comparisons in the paper are descriptive is a central point of criticism from both reviewers. As mentioned in my preliminary response, I intentionally did not optimise the model to a specific TGM or show an explicit metric of fitness. As I now explicitly mention in the new experimental section of the paper:</p>
<p>“The previous analyses were descriptive in the sense that they did not quantify how much the generated TGMs resembled a specific empirical TGM. This was deliberate, because empirical TGMs vary across subjects and experiments, and I aimed at characterising them as generally as possible by looking at some characteristic features in broad terms. For example, while TGMs typically have a strong diagonal and horizontal/vertical bars of high accuracy, questions such as when these effects emerge and for how long are highly dependent on the experimental paradigm. For the same reason, I did not optimise the model hyperparameters, limiting myself to observing the behaviour of the model across some characteristic configurations”</p>
<p>And, in the Discussion:</p>
<p>“The demonstrations here are not meant to be tailored to a specific data set, and are, for the most part, intentionally qualitative. TGMs do vary across experiments and subjects; and the hyperparameters of the model can be explicitly optimised to specific scientific questions, data sets, and even individuals. In order to explore the space of configurations effectively, an automatic optimisation of the hyperparameter space using, for instance, Bayesian optimisation (Lorenz, et al., 2017) could be advantageous. This may lead to the identification of very specific (spatial, spectral and temporal) features in the data that may be neurobiologically interpreted.”</p>
<p>Nonetheless, it is possible to fit the model to a specific TGMs by using a explicit metric of fitness. For illustration, this is what I did in the new experimental section Fitting and empirical TGM, where I used correlation with an empirical TGM to optimise two temporal parameters: the rise slope and the fall slope. As can be seen in the Figure 8, the correlation with the empirical TGM was as high as 0.7, even though I did not fit the other parameters of the model. As mentioned in the paragraph above, more sophisticated techniques such as Bayesian optimisation might be necessary for a more exhaustive exploration, but this would be beyond the scope of the current paper.</p>
<p>I would also like to point out that fitting the parameters in a step-wise manner was a necessity for interpretation. I suggest to think of the way we use F-tests in regression analyses as a comparison: if we want to know how important a feature is, we compare the model with and without this feature and see how much we loss.</p>
<disp-quote content-type="editor-comment">
<p>It further remained unclear to me, which implications may be drawn from the generative model, following from the capacities to mimic this specific TGM (i) for more complex cases, such as the comparison between experimental conditions, and (ii) about the complex nature of neural processes involved.</p>
</disp-quote>
<p>Following on the previous points, the object of this paper (besides presenting the model and the associated toolbox) was not to mimic a specific TGM, but to characterise the main features that we generally see across studies in the field. To clarify this, I have added Figure 2 (previously a Supplemental Information figure), and added the following to the Results section:</p>
<p>“Figure 2 shows a TGM for an example subject, where some archetypal characteristics are highlighted. In the experiments below, specifically, I focus on the strong narrow diagonal at the beginning of the trial, the broadening of accuracy later in the trial, and the vertical/horizontal bars of higher-than-chance accuracy. Importantly, this specific example in Figure 2 is only meant as a reference, and therefore I did not optimise the model hyperparameters to this TGM (except in the last subsection), or showed any quantitative metric of similarity.”</p>
<p>I mention the possibility of using the model to explore more complex cases in the Introduction, although doing so here would be out of scope:</p>
<p>“Other experimental paradigms, including motor tasks and decision making, can be investigated with genephys”</p>
<disp-quote content-type="editor-comment">
<p>Towards this end, I would appreciate (i) a more profound explanation of the conclusions that can be drawn from this specific showcase, including potential limitations, as well as wider considerations of how scientists may empower the generative model to (ii) understand their experimental data better and (iii) which added value the model may have in understanding the nature of underlying brain mechanism (rather than a mere technical characterization of sensor data).</p>
</disp-quote>
<p>To better illustrate how to use genephys to explore a specific data set, I have added a section (Fitting an empirical TGM) where I show how to fit specific hyperparameters to an empirical TGM in a simple manner.</p>
<p>In the Introduction, I briefly mentioned:</p>
<p>“This (not exhaustive) list of effects was considered given previous literature (Shah, et al., 2004; Mazaheri &amp; Jensen, 2006; Makeig, et al., 2002; Vidaurre, et al., 2021), and each effect may be underpinned by distinct neural mechanisms. For example, it is not completely clear the extent to which stimulus processing is sustained by oscillations, and disentangling these effects can help resolving this question”</p>
<p>In the Discussion, I have further commented:</p>
<p>“Genephys has different available types of effect, including phase resets, additive damped oscillations, amplitude modulations, and non-oscillatory responses. All of these elements, which may relate to distinct neurobiological mechanisms, are configurable and can be combined to generate a plethora of TGMs that, in turn, can be contrasted to specific empirical TGMs. This way, we can gain insight on what mechanisms might be at play in a given task.</p>
<p>The demonstrations here are not meant to be tailored to a specific data set, and are, for the most part, intentionally qualitative. TGMs do vary across experiments and subjects; and the hyperparameters of the model can be explicitly optimised to specific scientific questions, data sets, and even individuals. In order to explore the space of configurations effectively, an automatic optimisation of the hyperparameter space using, for instance, Bayesian optimisation (Lorenz, et al., 2017) could be advantageous. This may lead to the identification of very specific (spatial, spectral and temporal) features in the data that may be neurobiologically interpreted. “</p>
<disp-quote content-type="editor-comment">
<p>On p. 15 &quot;Having a diversity of frequencies but not of latencies produces another regular pattern consisting of alternating, parallel bands of higher/lower than baseline accuracy. This, shown in the bottom left panel, is not what we see in real data either. Having a diversity of latencies but not of frequencies gets us closer to a realistic pattern, as we see in the top right panel.&quot; The terms frequency and latency seem to be confused.</p>
</disp-quote>
<p>The Reviewer is right. I have corrected this now. Thank you.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2:</bold></p>
<p>The results of comparisons between simulations and real data are not always clear for an inexperienced reader. For example, the comparisons are qualitative rather than quantitative, making it hard to draw firm conclusions. Relatedly, it is unclear whether the chosen parameterizations are the only/best ones to generate the observed patterns or whether others are possible. In the case of the latter, it is unclear what we can actually conclude about underlying signal generators. It would have been different if the model was directly fitted to empirical data, maybe of different cognitive conditions. Finally, the neurobiological interpretation of different signal properties is not discussed. Therefore, taken together, in its currently presented form, it is unclear how this method could be used exactly to further our understanding of the brain.</p>
</disp-quote>
<p>This critique coincides with that of Reviewer 1. In the current version, I made more clear the fact that I am not fitting a specific empirical TGM and why, and that, instead, I am referring to general features that appear broadly throughout the literature. See more detailed changes below.</p>
<p>Regarding whether the chosen parameterizations are the only/best ones to generate the observed patterns, the Discussion reflects this limitation:</p>
<p>“Also importantly, I have shown that standard decoding analysis can differentiate between these explanations only to some extent. For example, the effects induced by phase-resetting and the use of additive oscillatory components are not enormously different in terms of the resulting TGMs. In future work, alternatives to standard decoding analysis and TGMs might be used to disentangle these sources of variation (Vidaurre, et al., 2019). ”</p>
<p>And</p>
<p>“Importantly, the list of effects that I have explored here is not exhaustive …”</p>
<p>Of course, since the list of signal features I have explored is not exhaustive, it cannot be claimed without a doubt that these features are the ones generating the properties we observe in real TGMs. The model, however, is a step forward in that direction, as it provides us with a tool to at least rule out some causes.</p>
<disp-quote content-type="editor-comment">
<p>Firstly, it was not entirely clear to me from the introduction what gap exactly the model is supposed to fill: is it about variance in neural responses in general, about which signal properties are responsible for decoding, or about capturing stability of signals? It seems like it does all of these, but this needs to be made clearer in the introduction. It would be helpful to emphasize exactly what insights the model can provide that are unable to be obtained with the current methods.</p>
</disp-quote>
<p>I have now made this explicit in in the Introduction, as suggested:</p>
<p>“To gain insight into what aspects of the signal underpin decoding accuracy, and therefore the most stable aspects of stimulus processing, I introduce a generative model”</p>
<p>To help illustrating what insights the model can provide, I have added the following sentence as an example:</p>
<p>“For example, it is not completely clear the extent to which stimulus processing is sustained by oscillations, and disentangling these effects can help resolving this question.”</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, I was unclear on why these specific properties were chosen (lines 71 to 78). Is there evidence from neuroscience to suggest that these signal properties are especially important for neural processing? Or, if the logic has more to do with signal processing, why are these specific properties the most important to include?</p>
</disp-quote>
<p>To clarify this the text now reads:</p>
<p>“In the model, when a channel responds, it can do it in different ways: (i) by phase-resetting the ongoing oscillation to a given target phase and then entraining to a given frequency, (ii) by an additive oscillatory response independent of the ongoing oscillation, (iii) by modulating the amplitude of the stimulus-relevant oscillations, or (iv) by an additive non-oscillatory (slower) response. This (not exhaustive) list of effects was considered given previous literature (Shah, et al., 2004; Mazaheri &amp; Jensen, 2006; Makeig, et al., 2002; Vidaurre, et al., 2021), and each effect may be underpinned by distinct neural mechanisms”</p>
<disp-quote content-type="editor-comment">
<p>The general narrative and focus of the paper could also be improved. It might help to start off with an outline of what the goal is at the start of the paper and then explicitly discuss how each of the steps works toward that goal. For example, I got the idea that the goal was to capture specific properties of an empirical TGM. If this was the case, the empirical TGM could be placed in the main body of the text as a reference picture for all simulated TGMs. For each simulation step, it could be emphasized more clearly exactly which features of the TGM is captured and what that means for interpreting these features in real data.</p>
</disp-quote>
<p>Thank you. To clarify the purpose of the paper better, I have brought Figure 2 to the front (before a Supplementary Figure), and in the first part of Results I have now added:</p>
<p>“Figure 2 shows a TGM for an example subject, where some archetypal characteristics are highlighted. In the experiments below, specifically, I focus on the strong narrow diagonal at the beginning of the trial, the broadening of accuracy later in the trial, and the vertical/horizontal bars of higher-than-chance accuracy. Importantly, this specific example in Figure 2 is only meant as a reference, and therefore I did not optimise the model hyperparameters to this TGM (except in the last subsection), or showed any quantitative metric of similarity. ”</p>
<p>I have enunciated the goals more clearly in the Introduction:</p>
<p>“To gain insight into what aspects of the signal underpin decoding accuracy, and therefore the most stable aspects of stimulus processing, …”</p>
<disp-quote content-type="editor-comment">
<p>Relatedly, it would be good to connect the various signal properties to possible neurobiological mechanisms. I appreciate that the author tries to remain neutral on this in the introduction, but I think it would greatly increase the implications of the analysis if it is made clearer how it could eventually help us understand neural processes.</p>
</disp-quote>
<p>The Reviewer is right in pointing out that I preferred to remain neutral on this. While I have still kept that tone of neutrality throughout the paper, I have now included the following sentence as an example of a neurobiological question that could be investigated with the model:</p>
<p>“For example, it is not completely clear the extent to which stimulus processing is sustained by oscillations, and disentangling these effects can help resolving this question.”</p>
<p>And, more generally,</p>
<p>“Genephys has different available types of effect, including phase resets, additive damped oscillations, amplitude modulations, and non-oscillatory responses. All of these elements, which may relate to distinct neurobiological mechanisms, are configurable and can be combined to generate a plethora of TGMs that, in turn, can be contrasted to specific empirical TGMs. This way, we can gain insight on what mechanisms might be at play in a given task. ”</p>
<disp-quote content-type="editor-comment">
<p>Line 57: this sentence is very long, making it hard to follow, could you break up into smaller parts?</p>
</disp-quote>
<p>Thank you. The sentence is fragmented now.</p>
<disp-quote content-type="editor-comment">
<p>Please replace angular frequencies with frequencies in Hertz for clarity.</p>
</disp-quote>
<p>Here I have preferred to stick to angular frequencies because it is more general than if I talk about Hertz, because that would entail having a specific sampling frequency. I think doing so would create confusion precisely of the sorts that I am trying to clarify in this revision: that is, that these results are not specific of one TGM but reflect general features that we see broadly in the literature.</p>
<p>There are quite some types throughout the paper, please recheck</p>
<p>Thank you. I have revised and have made my best to clear them out.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.87729.2.sa4</article-id>
<title-group>
<article-title>Author Response</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Vidaurre</surname>
<given-names>Diego</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9650-2229</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1:</bold></p>
<p>A weakness of the paper is that the power of the model is illustrated for only one specific set of parameters, added in a stepwise manner and the comparison to one specific empirical TGM, assumed to be prototypical; And that this comparison remains descriptive. (That is could a different selection of parameters lead to similar results and is there TGM data which matches these settings less well.)</p>
</disp-quote>
<p>The fact that the comparisons in the paper are descriptive is a central point of criticism from both reviewers. As mentioned in my preliminary response, I intentionally did not optimise the model to a specific TGM or show an explicit metric of fitness. As I now explicitly mention in the new experimental section of the paper:</p>
<p>“The previous analyses were descriptive in the sense that they did not quantify how much the generated TGMs resembled a specific empirical TGM. This was deliberate, because empirical TGMs vary across subjects and experiments, and I aimed at characterising them as generally as possible by looking at some characteristic features in broad terms. For example, while TGMs typically have a strong diagonal and horizontal/vertical bars of high accuracy, questions such as when these effects emerge and for how long are highly dependent on the experimental paradigm. For the same reason, I did not optimise the model hyperparameters, limiting myself to observing the behaviour of the model across some characteristic configurations”</p>
<p>And, in the Discussion:</p>
<p>“The demonstrations here are not meant to be tailored to a specific data set, and are, for the most part, intentionally qualitative. TGMs do vary across experiments and subjects; and the hyperparameters of the model can be explicitly optimised to specific scientific questions, data sets, and even individuals. In order to explore the space of configurations effectively, an automatic optimisation of the hyperparameter space using, for instance, Bayesian optimisation (Lorenz, et al., 2017) could be advantageous. This may lead to the identification of very specific (spatial, spectral and temporal) features in the data that may be neurobiologically interpreted.”</p>
<p>Nonetheless, it is possible to fit the model to a specific TGMs by using a explicit metric of fitness. For illustration, this is what I did in the new experimental section Fitting and empirical TGM, where I used correlation with an empirical TGM to optimise two temporal parameters: the rise slope and the fall slope. As can be seen in the Figure 8, the correlation with the empirical TGM was as high as 0.7, even though I did not fit the other parameters of the model. As mentioned in the paragraph above, more sophisticated techniques such as Bayesian optimisation might be necessary for a more exhaustive exploration, but this would be beyond the scope of the current paper.</p>
<p>I would also like to point out that fitting the parameters in a step-wise manner was a necessity for interpretation. I suggest to think of the way we use F-tests in regression analyses as a comparison: if we want to know how important a feature is, we compare the model with and without this feature and see how much we loss.</p>
<disp-quote content-type="editor-comment">
<p>It further remained unclear to me, which implications may be drawn from the generative model, following from the capacities to mimic this specific TGM (i) for more complex cases, such as the comparison between experimental conditions, and (ii) about the complex nature of neural processes involved.</p>
</disp-quote>
<p>Following on the previous points, the object of this paper (besides presenting the model and the associated toolbox) was not to mimic a specific TGM, but to characterise the main features that we generally see across studies in the field. To clarify this, I have added Figure 2 (previously a Supplemental Information figure), and added the following to the Results section:</p>
<p>“Figure 2 shows a TGM for an example subject, where some archetypal characteristics are highlighted. In the experiments below, specifically, I focus on the strong narrow diagonal at the beginning of the trial, the broadening of accuracy later in the trial, and the vertical/horizontal bars of higher-than-chance accuracy. Importantly, this specific example in Figure 2 is only meant as a reference, and therefore I did not optimise the model hyperparameters to this TGM (except in the last subsection), or showed any quantitative metric of similarity.”</p>
<p>I mention the possibility of using the model to explore more complex cases in the Introduction, although doing so here would be out of scope:</p>
<p>“Other experimental paradigms, including motor tasks and decision making, can be investigated with genephys”</p>
<disp-quote content-type="editor-comment">
<p>Towards this end, I would appreciate (i) a more profound explanation of the conclusions that can be drawn from this specific showcase, including potential limitations, as well as wider considerations of how scientists may empower the generative model to (ii) understand their experimental data better and (iii) which added value the model may have in understanding the nature of underlying brain mechanism (rather than a mere technical characterization of sensor data).</p>
</disp-quote>
<p>To better illustrate how to use genephys to explore a specific data set, I have added a section (Fitting an empirical TGM) where I show how to fit specific hyperparameters to an empirical TGM in a simple manner.</p>
<p>In the Introduction, I briefly mentioned:</p>
<p>“This (not exhaustive) list of effects was considered given previous literature (Shah, et al., 2004; Mazaheri &amp; Jensen, 2006; Makeig, et al., 2002; Vidaurre, et al., 2021), and each effect may be underpinned by distinct neural mechanisms. For example, it is not completely clear the extent to which stimulus processing is sustained by oscillations, and disentangling these effects can help resolving this question”</p>
<p>In the Discussion, I have further commented:</p>
<p>“Genephys has different available types of effect, including phase resets, additive damped oscillations, amplitude modulations, and non-oscillatory responses. All of these elements, which may relate to distinct neurobiological mechanisms, are configurable and can be combined to generate a plethora of TGMs that, in turn, can be contrasted to specific empirical TGMs. This way, we can gain insight on what mechanisms might be at play in a given task.</p>
<p>The demonstrations here are not meant to be tailored to a specific data set, and are, for the most part, intentionally qualitative. TGMs do vary across experiments and subjects; and the hyperparameters of the model can be explicitly optimised to specific scientific questions, data sets, and even individuals. In order to explore the space of configurations effectively, an automatic optimisation of the hyperparameter space using, for instance, Bayesian optimisation (Lorenz, et al., 2017) could be advantageous. This may lead to the identification of very specific (spatial, spectral and temporal) features in the data that may be neurobiologically interpreted. “</p>
<disp-quote content-type="editor-comment">
<p>On p. 15 &quot;Having a diversity of frequencies but not of latencies produces another regular pattern consisting of alternating, parallel bands of higher/lower than baseline accuracy. This, shown in the bottom left panel, is not what we see in real data either. Having a diversity of latencies but not of frequencies gets us closer to a realistic pattern, as we see in the top right panel.&quot; The terms frequency and latency seem to be confused.</p>
</disp-quote>
<p>The Reviewer is right. I have corrected this now. Thank you.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2:</bold></p>
<p>The results of comparisons between simulations and real data are not always clear for an inexperienced reader. For example, the comparisons are qualitative rather than quantitative, making it hard to draw firm conclusions. Relatedly, it is unclear whether the chosen parameterizations are the only/best ones to generate the observed patterns or whether others are possible. In the case of the latter, it is unclear what we can actually conclude about underlying signal generators. It would have been different if the model was directly fitted to empirical data, maybe of different cognitive conditions. Finally, the neurobiological interpretation of different signal properties is not discussed. Therefore, taken together, in its currently presented form, it is unclear how this method could be used exactly to further our understanding of the brain.</p>
</disp-quote>
<p>This critique coincides with that of Reviewer 1. In the current version, I made more clear the fact that I am not fitting a specific empirical TGM and why, and that, instead, I am referring to general features that appear broadly throughout the literature. See more detailed changes below.</p>
<p>Regarding whether the chosen parameterizations are the only/best ones to generate the observed patterns, the Discussion reflects this limitation:</p>
<p>“Also importantly, I have shown that standard decoding analysis can differentiate between these explanations only to some extent. For example, the effects induced by phase-resetting and the use of additive oscillatory components are not enormously different in terms of the resulting TGMs. In future work, alternatives to standard decoding analysis and TGMs might be used to disentangle these sources of variation (Vidaurre, et al., 2019). ”</p>
<p>And</p>
<p>“Importantly, the list of effects that I have explored here is not exhaustive …”</p>
<p>Of course, since the list of signal features I have explored is not exhaustive, it cannot be claimed without a doubt that these features are the ones generating the properties we observe in real TGMs. The model, however, is a step forward in that direction, as it provides us with a tool to at least rule out some causes.</p>
<disp-quote content-type="editor-comment">
<p>Firstly, it was not entirely clear to me from the introduction what gap exactly the model is supposed to fill: is it about variance in neural responses in general, about which signal properties are responsible for decoding, or about capturing stability of signals? It seems like it does all of these, but this needs to be made clearer in the introduction. It would be helpful to emphasize exactly what insights the model can provide that are unable to be obtained with the current methods.</p>
</disp-quote>
<p>I have now made this explicit in in the Introduction, as suggested:</p>
<p>“To gain insight into what aspects of the signal underpin decoding accuracy, and therefore the most stable aspects of stimulus processing, I introduce a generative model”</p>
<p>To help illustrating what insights the model can provide, I have added the following sentence as an example:</p>
<p>“For example, it is not completely clear the extent to which stimulus processing is sustained by oscillations, and disentangling these effects can help resolving this question.”</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, I was unclear on why these specific properties were chosen (lines 71 to 78). Is there evidence from neuroscience to suggest that these signal properties are especially important for neural processing? Or, if the logic has more to do with signal processing, why are these specific properties the most important to include?</p>
</disp-quote>
<p>To clarify this the text now reads:</p>
<p>“In the model, when a channel responds, it can do it in different ways: (i) by phase-resetting the ongoing oscillation to a given target phase and then entraining to a given frequency, (ii) by an additive oscillatory response independent of the ongoing oscillation, (iii) by modulating the amplitude of the stimulus-relevant oscillations, or (iv) by an additive non-oscillatory (slower) response. This (not exhaustive) list of effects was considered given previous literature (Shah, et al., 2004; Mazaheri &amp; Jensen, 2006; Makeig, et al., 2002; Vidaurre, et al., 2021), and each effect may be underpinned by distinct neural mechanisms”</p>
<disp-quote content-type="editor-comment">
<p>The general narrative and focus of the paper could also be improved. It might help to start off with an outline of what the goal is at the start of the paper and then explicitly discuss how each of the steps works toward that goal. For example, I got the idea that the goal was to capture specific properties of an empirical TGM. If this was the case, the empirical TGM could be placed in the main body of the text as a reference picture for all simulated TGMs. For each simulation step, it could be emphasized more clearly exactly which features of the TGM is captured and what that means for interpreting these features in real data.</p>
</disp-quote>
<p>Thank you. To clarify the purpose of the paper better, I have brought Figure 2 to the front (before a Supplementary Figure), and in the first part of Results I have now added:</p>
<p>“Figure 2 shows a TGM for an example subject, where some archetypal characteristics are highlighted. In the experiments below, specifically, I focus on the strong narrow diagonal at the beginning of the trial, the broadening of accuracy later in the trial, and the vertical/horizontal bars of higher-than-chance accuracy. Importantly, this specific example in Figure 2 is only meant as a reference, and therefore I did not optimise the model hyperparameters to this TGM (except in the last subsection), or showed any quantitative metric of similarity. ”</p>
<p>I have enunciated the goals more clearly in the Introduction:</p>
<p>“To gain insight into what aspects of the signal underpin decoding accuracy, and therefore the most stable aspects of stimulus processing, …”</p>
<disp-quote content-type="editor-comment">
<p>Relatedly, it would be good to connect the various signal properties to possible neurobiological mechanisms. I appreciate that the author tries to remain neutral on this in the introduction, but I think it would greatly increase the implications of the analysis if it is made clearer how it could eventually help us understand neural processes.</p>
</disp-quote>
<p>The Reviewer is right in pointing out that I preferred to remain neutral on this. While I have still kept that tone of neutrality throughout the paper, I have now included the following sentence as an example of a neurobiological question that could be investigated with the model:</p>
<p>“For example, it is not completely clear the extent to which stimulus processing is sustained by oscillations, and disentangling these effects can help resolving this question.”</p>
<p>And, more generally,</p>
<p>“Genephys has different available types of effect, including phase resets, additive damped oscillations, amplitude modulations, and non-oscillatory responses. All of these elements, which may relate to distinct neurobiological mechanisms, are configurable and can be combined to generate a plethora of TGMs that, in turn, can be contrasted to specific empirical TGMs. This way, we can gain insight on what mechanisms might be at play in a given task. ”</p>
<disp-quote content-type="editor-comment">
<p>Line 57: this sentence is very long, making it hard to follow, could you break up into smaller parts?</p>
</disp-quote>
<p>Thank you. The sentence is fragmented now.</p>
<disp-quote content-type="editor-comment">
<p>Please replace angular frequencies with frequencies in Hertz for clarity.</p>
</disp-quote>
<p>Here I have preferred to stick to angular frequencies because it is more general than if I talk about Hertz, because that would entail having a specific sampling frequency. I think doing so would create confusion precisely of the sorts that I am trying to clarify in this revision: that is, that these results are not specific of one TGM but reflect general features that we see broadly in the literature.</p>
<p>There are quite some types throughout the paper, please recheck</p>
<p>Thank you. I have revised and have made my best to clear them out.</p>
</body>
</sub-article>
</article>