<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">99545</article-id>
<article-id pub-id-type="doi">10.7554/eLife.99545</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.99545.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Efficient coding in biophysically realistic excitatory-inhibitory spiking networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2920-2717</contrib-id>
<name>
<surname>Koren</surname>
<given-names>Veronika</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>koren.veronika@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4467-5988</contrib-id>
<name>
<surname>Blanco Malerba</surname>
<given-names>Simone</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5422-3723</contrib-id>
<name>
<surname>Schwalger</surname>
<given-names>Tilo</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1700-8909</contrib-id>
<name>
<surname>Panzeri</surname>
<given-names>Stefano</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>s.panzeri@uke.de</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>Institute of Neural Information Processing, Center for Molecular Neurobiology (ZMNH), University Medical Center Hamburg-Eppendorf (UKE)</institution></institution-wrap>, <city>Hamburg</city>, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v4gjf40</institution-id><institution>Institute of Mathematics, Technische Universität Berlin</institution></institution-wrap>, <city>Berlin</city>, <country>Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ewdps05</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin</institution></institution-wrap>, <city>Berlin</city>, <country>Germany</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing Interest Statement: The authors have declared no competing interest.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-26">
<day>26</day>
<month>07</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-01-22">
<day>22</day>
<month>01</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP99545</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-05-28">
<day>28</day>
<month>05</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-27">
<day>27</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.24.590955"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-07-26">
<day>26</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.99545.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.99545.1.sa4">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99545.1.sa3">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99545.1.sa2">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.99545.1.sa1">Reviewer #3 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.99545.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Koren et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Koren et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-99545-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>The principle of efficient coding posits that sensory cortical networks are designed to encode maximal sensory information with minimal metabolic cost. Despite the major influence of efficient coding in neuroscience, it has remained unclear whether fundamental empirical properties of neural network activity can be explained solely based on this normative principle. Here, we derive the structural, coding, and biophysical properties of excitatory-inhibitory recurrent networks of spiking neurons that emerge directly from imposing that the network minimizes an instantaneous loss function and a time-averaged performance measure enacting efficient coding. We assumed that the network encodes a number of independent stimulus features varying with a time scale equal to the membrane time constant of excitatory and inhibitory neurons. The optimal network has biologically-plausible biophysical features, including realistic integrate-and-fire spiking dynamics, spike-triggered adaptation, and a non-specific excitatory external input. The excitatory-inhibitory recurrent connectivity between neurons with similar stimulus tuning implements feature-specific competition, similar to that recently found in visual cortex. Networks with unstructured connectivity cannot reach comparable levels of coding efficiency. The optimal ratio of excitatory vs inhibitory neurons and the ratio of mean inhibitory-to-inhibitory vs excitatory-to-inhibitory connectivity are comparable to those of cortical sensory networks. The efficient network solution exhibits an instantaneous balance between excitation and inhibition. The network can perform efficient coding even when external stimuli vary over multiple time scales. Together, these results suggest that key properties of biological neural networks may be accounted for by efficient coding.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Neural coding</kwd>
<kwd>Efficient coding</kwd>
<kwd>Spiking neural networks</kwd>
<kwd>Recurrent Neural Networks</kwd>
<kwd>Population coding</kwd>
<kwd>Spike-triggered adaptation</kwd>
<kwd>Integrate-and-fire neuron</kwd>
<kwd>Excitatory-Inhibitory balance</kwd>
<kwd>Connectivity</kwd>
<kwd>Optimality</kwd>
</kwd-group>

<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Major revision of text and figures with additional analyses and text clarification.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Information about the sensory world is represented in the brain through the dynamics of neural population activity <sup><xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref></sup>. One prominent theory about the principles that may guide the design of neural computations for sensory function is efficient coding <sup><xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c5">5</xref></sup>. This theory posits that neural computations are optimized to maximize the information that neural systems encode about sensory stimuli while at the same time limiting the metabolic cost of neural activity. Efficient coding has been highly influential as a normative theory of how networks are organized and designed to optimally process natural sensory stimuli in visual<sup><xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref>,<xref ref-type="bibr" rid="c11">11</xref></sup>, auditory <sup><xref ref-type="bibr" rid="c12">12</xref></sup> and olfactory sensory pathways <sup><xref ref-type="bibr" rid="c13">13</xref></sup>.</p>
<p>The first normative neural network models<sup><xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c10">10</xref></sup> designed with efficient coding principles had at least two major levels of abstractions. First, neural dynamics was greatly simplified, ignoring the spiking nature of neural activity. Instead, biological networks often encode information through millisecond-precise spike timing <sup><xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. Second, these earlier contributions mostly considered encoding of static sensory stimuli, whereas the sensory environment changes continuously at multiple timescales and the dynamics of neural networks encodes these temporal variations of the environment <sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c24">24</xref></sup>.</p>
<p>Recent years have witnessed a considerable effort and success in laying down the mathematical tools and methodology to understand how to formulate efficient coding theories of neural networks with more biological realism <sup><xref ref-type="bibr" rid="c25">25</xref></sup>. This effort has established the incorporation of recurrent connectivity <sup><xref ref-type="bibr" rid="c26">26</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>, of spiking neurons, and of time-varying stimulus inputs <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>. In these models, the efficient coding principle has been implemented by designing networks whose activity maximizes the encoding accuracy, by minimizing the error between a desired representation and a linear readout of network’s activity, subject to a constraint on the metabolic cost of processing. This double objective is captured by a loss function that trades off encoding accuracy and metabolic cost. The minimization of the loss function is performed through a greedy approach, by assuming that a neuron will emit a spike only if this will decrease the loss. This, in turn, yields a set of leaky integrate-and-fire (LIF) neural equations<sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c29">29</xref></sup>, which can also include biologically plausible non-instantaneous synaptic delays <sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>. Although most initial implementations did not respect Dale’s law, further studies analytically derived efficient networks of excitatory (E) and inhibitory (I) spiking neurons that respect Dale’s law <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup> and included spike-triggered adaptation <sup><xref ref-type="bibr" rid="c38">38</xref></sup>. These networks take the form of generalized leaky integrate-and-fire (gLIF) models neurons, which are realistic models of neuronal activity <sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup> and capable of accurately predicting real neural spike times <italic>in vivo</italic> <sup><xref ref-type="bibr" rid="c42">42</xref></sup>. Efficient spiking models thus have the potential to provide a normative theory of neural coding through spiking dynamics of E-I circuits <sup><xref ref-type="bibr" rid="c43">43</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c44">44</xref></sup> with high biological plausibility.</p>
<p>However, despite the major progress described above, we still lack a thorough characterization of which structural, coding, biophysical and dynamical properties of excitatory-inhibitory recurrent spiking neural networks directly relate to efficient coding. Previous studies only rarely made predictions that could be quantitatively compared against experimentally measurable biological properties. As a consequence, we still do not know which, if any, fundamental properties of cortical networks emerge directly from efficient coding.</p>
<p>To address the above questions, we systematically analyze our biologically plausible efficient coding model of E and I neurons that respects Dale’s law <sup><xref ref-type="bibr" rid="c38">38</xref></sup>. We make concrete predictions about experimentally measurable structural, coding and dynamical features of neurons that arise from efficient coding. We systematically investigate how experimentally measurable emergent dynamical properties, including firing rates, trial-to-trial spiking variability of single neurons and E-I balance <sup><xref ref-type="bibr" rid="c45">45</xref></sup>, relate to network optimality. We further analyze how the organization of the connectivity arising by imposing efficient coding relates to the anatomical and effective connectivity recently reported in visual cortex, which suggests competition between excitatory neurons with similar stimulus tuning. We find that several key and robustly found empirical properties of cortical circuits match those of our efficient coding network. This lends support to the notion that efficient coding may be a design principle that has shaped the evolution of cortical circuits and that may be used to conceptually understand and interpret them.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Assumptions and emergent structural properties of the efficient E-I network derived from first principles</title>
<p>We study the properties of a spiking neural network in which the dynamics and structure of the network are analytically derived starting from first principles of efficient coding of sensory stimuli. The model relies on a number of assumptions, described next.</p>
<p>The network responds to <italic>M</italic> time-varying features of a sensory stimulus, <bold><italic>s</italic></bold>(<italic>t</italic>) = [<italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub><italic>M</italic></sub> (<italic>t</italic>)] (e.g., for a visual stimulus, contrast, orientation, etc.) received as inputs from an earlier sensory area. We model each feature <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>) as an independent Ornstein–Uhlenbeck (OU) processes (see Methods). The network’s objective is to compute a leaky integration of sensory features; the target representations of the network, <bold><italic>x</italic></bold>(<italic>t</italic>), is defined as
<disp-formula id="eqn1">
<graphic xlink:href="590955v2_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>τ</italic> a characteristic integration time-scale (<xref rid="fig1" ref-type="fig">Fig. 1A(i)</xref>). We assumed leaky integration of sensory features for consistency with previous theoretical models <sup><xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>. This assumption stems from the finding that, in many cases, integration of sensory evidence by neurons is well described by an exponential kernel <sup><xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref></sup>. Additionally, a leaky integration of neural activity with an exponential kernel implemented in models of neural activity readout often explains well perceptual discrimination results <sup><xref ref-type="bibr" rid="c48">48</xref>,<xref ref-type="bibr" rid="c49">49</xref>,<xref ref-type="bibr" rid="c50">50</xref></sup>. This suggests that the assumption of leaky integration of sensory evidence, though possibly simplistic, captures relevant aspects of neural computations.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Structural and dynamical properties of the efficient E-I spiking network.</title>
<p><bold>(A) (i)</bold> Encoding of a target signal representing the evolution of a stimulus feature (top) with one E (middle) and one I spiking neuron (bottom). The target signal <italic>x</italic>(<italic>t</italic>) integrates the input signal <italic>s</italic>(<italic>t</italic>). The readout of the E neuron tracks the target signal and the readout of the I neuron tracks the readout of the E neuron. Neurons spike to bring the readout of their activity closer to their respective target. Each spike causes a jump of the readout, with the sign and the amplitude of the jump being determined by neuron’s tuning parameters. <bold>(ii)</bold> Schematic of the matrix of tuning parameters. Every neuron is selective to all stimulus features (columns of the matrix), and all neurons participate in encoding of every feature (rows).</p>
<p><bold>(B)(i)</bold> Schematic of the network with E (red) and I (blue) cell type. E neurons are driven by the stimulus features while I neurons are driven by the activity of E neurons. E and I neurons are connected through recurrent connectivity matrices. <bold>(ii)</bold> Schematic of E (red) and I (blue) synaptic interactions. Arrows represent the direction of the tuning vector of each neuron. Only neurons with similar tuning are connected and the connection strength is proportional to the tuning similarity.</p>
<p><bold>(C)(i)</bold> Schematic of similarity of tuning vectors (tuning similarity) in a 2-dimensional space of stimulus features. <bold>(ii)</bold> Synaptic strength as a function of tuning similarity.</p>
<p><bold>(D)</bold> Coding and dynamics in a simulation trial. Top three rows show the signal (black), the E estimate (red) and the I estimate (blue) for each of the three stimulus features. Below are the spike trains. In the bottom row, we show the average instantaneous firing rate (in Hz).</p>
<p><bold>(E)</bold> Top: Example of the target signal (black) and the E estimate in three simulation trials (colors) for one stimulus feature. Bottom: Distribution (across time) of the time-dependent bias of estimates in E and I cell type.</p>
<p><bold>(F)</bold> Left: Distribution of time-averaged firing rates in E (top) and I neurons (bottom). Black traces are fits with log-normal distribution. Right: Distribution of coefficients of variation of interspike intervals for E and I neurons.</p>
<p><bold>(G)</bold> Distribution (across neurons) of time-averaged synaptic inputs to E (left) and I neurons (right). In E neurons, the mean of distributions of inhibitory and of net synaptic inputs are very close.</p>
<p><bold>(H)</bold> Sum of synaptic inputs over time in a single E (top) and I neuron (bottom) in a simulation trial.</p>
<p><bold>(I)</bold> Distribution (across neurons) of Pearson’s correlation coefficients measuring the correlation of synaptic inputs <italic>A</italic><sup><italic>Ey</italic></sup> and <italic>A</italic><sup><italic>Iy</italic></sup> (as defined in Methods, <xref ref-type="disp-formula" rid="eqn43">Eq. (43)</xref>) in single E (red) and I (blue) neurons. All statistical results (E-F, H-I) were computed on 10 simulation trials of 10 second duration. For model parameters, see <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The network is composed of two neural populations of excitatory (E) and inhibitory (I) neurons, defined by their postsynaptic action which respects Dale’s law. For each population, <italic>y</italic> ∈ {<italic>E, I</italic>}, we define a population readout of each feature,<inline-formula><inline-graphic xlink:href="590955v2_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, as a filtered weighted sum of spiking activity of neurons in the population,
<disp-formula id="eqn2">
<graphic xlink:href="590955v2_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v2_inline1a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the spike train of neuron <italic>i</italic> of type <italic>y</italic> and <inline-formula><inline-graphic xlink:href="590955v2_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the vector of decoding weights of the neuron for features <italic>k</italic> = 1, …, <italic>M</italic> (<xref rid="fig1" ref-type="fig">Fig. 1A(ii)</xref>). We assume that every neuron encodes multiple (<italic>M</italic> &gt; 1) stimulus features and that the encoding of every stimulus is distributed among neurons. As a result of the optimization, the decoding weights of the neurons are equivalent to the neuron’s stimulus tuning parameters (see Methods <sup><xref ref-type="bibr" rid="c43">43</xref></sup>). We sampled tuning parameters uniformly from a <italic>M</italic> -dimensional hypersphere with unit radius, giving tuning vectors with unit length to all neurons (see Methods). To control the amount of inhibition in the network, we then multiplied the tuning vectors of I neurons with a factor <italic>d</italic> &gt; 1, homogeneously across all I neurons. Normalization of decoding vectors preserves the heterogeneity of decoding weights across neurons, which may benefit coding efficiency <sup><xref ref-type="bibr" rid="c51">51</xref></sup>.</p>
<p>Following previous work <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup>, we impose that E and I neurons have distinct normative objectives and we define specific loss functions relative to each neuron type. To implement at the same time, as requested by efficient coding, the constraints of faithful stimulus representation with limited computational resources <sup><xref ref-type="bibr" rid="c52">52</xref></sup>, we define the loss functions of the population <italic>y</italic> ∈ {<italic>E, I</italic>} as a weighted sum of a time-dependent encoding error and time-dependent metabolic cost:
<disp-formula id="eqn3">
<graphic xlink:href="590955v2_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>We refer to <italic>β</italic>, the parameter controlling the relative importance of the metabolic cost over the encoding error, as the metabolic constant of the network. We hypothesize that population readouts of E neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, track the target representations, <bold><italic>x</italic></bold>(<italic>t</italic>), and the population readouts of I neurons,<inline-formula><inline-graphic xlink:href="590955v2_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, track the population readouts of E neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, by minimizing the squared error between these quantities <sup><xref ref-type="bibr" rid="c38">38</xref></sup> (see also <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c53">53</xref></sup> for related approaches). Furthermore, we hypothesize the metabolic cost to be proportional to the instantaneous estimate of network’s firing frequency. We thus define the variables of loss functions in <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref> as
<disp-formula id="eqn4">
<graphic xlink:href="590955v2_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v2_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <italic>y</italic> ∈ {<italic>E, I</italic>}, is the low-pass filtered spike train of neuron <italic>i</italic> (single neuron readout) with time constant <inline-formula><inline-graphic xlink:href="590955v2_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, proportional to the instantaneous firing rate of the neuron: <inline-formula><inline-graphic xlink:href="590955v2_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We then impose the following condition for spiking: a neuron emits a spike at time <italic>t</italic> only if this decreases the loss function of its population (<xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>) in the immediate future. The condition for spiking also includes a noise term (Methods) accounting for sources of stochasticity in spike generation <sup><xref ref-type="bibr" rid="c54">54</xref></sup> which include the effect of non-specific inputs from the rest of the brain.</p>
<p>We derived the dynamics and network structure of a spiking network that instantiates efficient coding (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>, see Methods). The derived dynamics of the subthreshold membrane potential <inline-formula><inline-graphic xlink:href="590955v2_inline8a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline8b.gif" mime-subtype="gif" mimetype="image"/></inline-formula> obey the equations of the generalized leaky integrate and fire (gLIF) neuron
<disp-formula id="eqn5">
<graphic xlink:href="590955v2_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v2_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="590955v2_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are synaptic current, spike-triggered adaptation current and non-specific external current, respectively, <italic>R</italic><sub><italic>m</italic></sub> is the membrane resistance and <inline-formula><inline-graphic xlink:href="590955v2_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula>is the resting potential. This dynamics is complemented with a fire-and-reset rule: when the membrane potential reaches the firing threshold <italic>ϑ</italic><sup><italic>y</italic></sup>, a spike is fired and <inline-formula><inline-graphic xlink:href="590955v2_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is set to the reset potential <italic>V</italic> <sup>reset,<italic>y</italic></sup>. The analytical solution in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> holds for any number of neurons (with at least 1 neuron in each population) and predicts an optimal spike pattern to encode the presented external stimulus. Following previous work <sup><xref ref-type="bibr" rid="c28">28</xref></sup> in which physical units were assigned to derived mathematical expressions to interpret them as biophysical variables, we express computational variables (target stimuli in <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>, population readouts in <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref> and the metabolic constant in <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>), with physical units in such a way that all terms of the biophysical model (<xref ref-type="disp-formula" rid="eqn5">Eq. 5</xref>) have realistic physical units.</p>
<p>The synaptic currents in E neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, consist of feedforward currents, obtained as stimulus features <bold><italic>s</italic></bold>(<italic>t</italic>) weighted by the tuning weights of the neuron, and of recurrent inhibitory currents (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>). Synaptic currents in I neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, consist of recurrent excitatory and inhibitory currents. Note that there are no recurrent connections between E neurons, a consequence of our assumption of no across-feature interaction in the leaky integration of stimulus features (<xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>). This assumption is likely to be simplistic even for early sensory cortices <sup><xref ref-type="bibr" rid="c55">55</xref></sup>. However, in other studies we found that many properties of efficient networks implementing leaky integration hold also when input features are linearly mixed during integration <sup><xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c25">25</xref></sup>.</p>
<p>The optimization of the loss function yielded structured recurrent connectivity (<xref rid="fig1" ref-type="fig">Fig. 1B(ii)-C</xref>). Synaptic strength between two neurons is proportional to their tuning similarity, forming like-to-like connectivity, if the tuning similarity is positive; otherwise the synaptic weight is set to zero (<xref rid="fig1" ref-type="fig">Fig. 1C (ii)</xref>) to ensure that Dale’s law is respected. A connectivity structure in which the synaptic weight is proportional to pairwise tuning similarity is consistent with some empirical observations in visual cortex<sup><xref ref-type="bibr" rid="c56">56</xref></sup> and has been suggested by previous models <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c57">57</xref></sup>. Such connectivity organization is also suggested by across-neuron influence measured with optogenetic perturbations of visual cortex <sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup>. While such connectivity structure is the result of optimization, the rectification of the connectivity that enforces Dale’s law does not emerge from imposing efficient coding, but from constraining the space of solutions to biologically plausible networks. Rectification also sets the overall connection probability to 0.5, consistent with experimental results <sup><xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup>. (For a study of how efficient coding would be implemented if the above Dale’s law constraint were removed and each neuron were free to have either an inhibitory or excitatory effect depending on the postsynaptic target, see Supplementary Text 1 and <xref rid="figS1" ref-type="fig">Supplementary Fig. S1A-E</xref>.)</p>
<p>The spike-triggered adaptation current of neuron <italic>i</italic> in population <italic>y</italic>, <inline-formula><inline-graphic xlink:href="590955v2_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, is proportional to its low-pass filtered spike train. This current realizes spike-frequency adaptation or facilitation depending on the difference between the time constants of population and single neuron readout (see Results section “Weak or no spike-triggered adaptation optimizes network efficiency”).</p>
<p>Finally, non-specific external currents <inline-formula><inline-graphic xlink:href="590955v2_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula> have a constant mean that depends on the parameter <italic>β</italic>, and fluctuations that arise from the noise with strength <italic>σ</italic> in the condition for spiking. The relative weight of the metabolic cost over the encoding error, <italic>β</italic>, controls how the network responds to feedforward stimuli, by modulating the mean of the non-specific synaptic currents incoming to all neurons. Together with the noise strength <italic>σ</italic>, these two parameters set the non-specific synaptic currents to single neurons that are homogeneous across the network and akin to the background synaptic input discussed in <sup><xref ref-type="bibr" rid="c62">62</xref></sup>. By allowing a large part of the distance between the resting potential and the threshold to be taken by the non-specific current, we found a biologically plausible set of optimally efficient model parameters (<xref ref-type="table" rid="tbl1">Table 1</xref>) including the firing threshold at about 20 mV from the resting potential, which is within the experimental ballpark <sup><xref ref-type="bibr" rid="c63">63</xref></sup>, and average synaptic strengths of 0.75 mV (E-I and I-E synapses) and 2.25 mV (I-I synapses), which are consistent with measurements in sensory cortex<sup><xref ref-type="bibr" rid="c61">61</xref></sup>. An optimal network without non-specific currents can be derived (see Methods, <xref ref-type="disp-formula" rid="eqn25">Eq. 25</xref>), but its parameters are not consistent with biology (see Supplementary Text 2 and Supplementary Table S1). The non-specific currents can be interpreted as synaptic currents that are modulated by largerscale variables, such as brain states (see section “Non-specific currents regulate network coding properties”).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Table of default model parameters for the efficient E-I network</title>
<p>Parameters above the double horizontal line are the minimal set of parameters needed to simulate model equations (<xref ref-type="disp-formula" rid="eqn29a">Eqs. 29a</xref>-<xref ref-type="disp-formula" rid="eqn29h">29h</xref> in Methods). Parameters below the double horizontal line are biophysical parameters, derived from the same model equations and from model parameters listed above the horizontal line. Parameters <italic>N</italic> <sup><italic>E</italic></sup>, <italic>M, τ</italic> and <inline-formula><inline-graphic xlink:href="590955v2_inline160.gif" mime-subtype="gif" mimetype="image"/></inline-formula> were chosen for their biological plausibility and computational simplicity. Parameters <italic>N</italic> <sup><italic>I</italic></sup>, <inline-formula><inline-graphic xlink:href="590955v2_inline161.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <italic>σ</italic>, ratio of mean E-I to I-I synaptic connectivity and <italic>β</italic> are parameters that maximize network efficiency (see the section “Criterion for determining model parameters” in Methods). The metabolic constant <italic>β</italic> and the noise strength <italic>σ</italic> are interpreted as global network parameters and are for this reason assumed to be the same across the E and I population, e.g., <italic>β</italic><sup><italic>E</italic></sup> = <italic>β</italic><sup><italic>I</italic></sup> = <italic>β</italic> and <italic>σ</italic><sup><italic>E</italic></sup> = <italic>σ</italic><sup><italic>I</italic></sup> = <italic>σ</italic> (see <xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>). The connection probability of <italic>p</italic><sup><italic>xy</italic></sup> = 0.5 is the consequence of rectification of the connectivity (see <xref ref-type="disp-formula" rid="eqn24">Eq. 24</xref> in Methods).</p></caption>
<graphic xlink:href="590955v2_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>To summarize, the analytical derivation of an optimally efficient network includes gLIF neurons <sup><xref ref-type="bibr" rid="c64">64</xref>,<xref ref-type="bibr" rid="c42">42</xref>,<xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c65">65</xref>,<xref ref-type="bibr" rid="c66">66</xref></sup>, a distributed code with linear mixed selectivity to the input stimuli <sup><xref ref-type="bibr" rid="c67">67</xref>,<xref ref-type="bibr" rid="c68">68</xref></sup>, spike-triggered adaptation, structured synaptic connectivity, and a non-specific external current akin to background synaptic input.</p>
</sec>
<sec id="s2b">
<title>Encoding performance and neural dynamics in an optimally efficient E-I network</title>
<p>The equations for the E-I network of gLIF neurons in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> optimize the loss functions at any given time and for any set of parameters. In particular, the network equations have the same analytical form for any positive value of the metabolic constant <italic>β</italic>. To find a set of parameters that optimizes the overall performance, we minimized the loss function averaged over time and trials. We then optimized the parameters by setting the metabolic constant <italic>β</italic> such that the encoding error weights 70 % and the metabolic error weights 30 % of the average loss, and by choosing all other parameters such as to minimize numerically the average loss (see Methods).</p>
<p>The numerical optimization was performed by simulating a model of 400 E and 100 I units, a network size relevant for computations within one layer of a cortical microcolumn <sup><xref ref-type="bibr" rid="c69">69</xref></sup>. The set of model parameters that optimized network efficiency is detailed in <xref ref-type="table" rid="tbl1">Table 1</xref>. Unless otherwise stated, we will use the optimal parameters of <xref ref-type="table" rid="tbl1">Table 1</xref> in all simulations and only vary parameters detailed in the figure axes.</p>
<p>With optimally efficient parameters, population readouts closely tracked the target signals (<xref rid="fig1" ref-type="fig">Fig. 1D</xref>, M=3, <italic>R</italic><sup><xref ref-type="bibr" rid="c2">2</xref></sup> = [0.95, 0.97] for E and I neurons, respectively). When stimulated by our 3-dimensional time-varying feedforward input, the optimal E-I network provided a precise estimator of target signals (<xref rid="fig1" ref-type="fig">Fig. 1E</xref>, top). The average estimation bias (<italic>B</italic><sup><italic>E</italic></sup> and <italic>B</italic><sup><italic>I</italic></sup>, see Methods) of the network minimizing the encoding error was close to zero (<italic>B</italic><sup><italic>E</italic></sup> = 0.02 and <italic>B</italic><sup><italic>I</italic></sup> = 0.03) while the bias of the network minimizing the average loss (and optimizing efficiency) was slightly larger and negative (<italic>B</italic><sup><italic>E</italic></sup> = -0.15 and <italic>B</italic><sup><italic>I</italic></sup>=-0.34), but still small compared to the stimulus amplitude (<xref rid="fig1" ref-type="fig">Fig. 1E</xref>, bottom, <xref rid="figS1" ref-type="fig">Supplementary figure S1F</xref>). Time- and trial-averaged encoding error (RMSE) and metabolic cost (MC, see Methods) were comparable in magnitude (<italic>RMSE</italic> = [3.5, 2.4], <italic>MC</italic> = [4.4, 2.8] for E and I), but with smaller error and lower cost in I, leading to a better performance in I (average loss of 2.5) compared to E neurons (average loss of 3.7). We report both the encoding error and the metabolic cost throughout the paper, so that readers can evaluate how these performance measures may generalize when weighting differently the error and the metabolic cost.</p>
<p>Next, we examined the emergent dynamical properties of an optimally efficient E-I network. I neurons had higher average firing rates compared to E neurons, consistently with observations in cortex <sup><xref ref-type="bibr" rid="c70">70</xref></sup>. The distribution of firing rates was well described by a log-normal distribution (<xref rid="fig1" ref-type="fig">Fig. 1F</xref>, left), consistent with distributions of cortical firing observed empirically <sup><xref ref-type="bibr" rid="c71">71</xref></sup>. Neurons fired irregularly, with mean coefficient of variation (CV) slightly smaller than 1 (<xref rid="fig1" ref-type="fig">Fig. 1F</xref>, right; CV= [0.97, 0.95] for E and I neurons, respectively), compatible with cortical firing<sup><xref ref-type="bibr" rid="c72">72</xref></sup>. We assessed E-I balance in single neurons through two complementary measures. First, we calculated the <italic>average</italic> (global) balance of E-I currents by taking the time-average of the net sum of synaptic inputs (shortened to net synaptic input <sup><xref ref-type="bibr" rid="c73">73</xref></sup>). Second, we computed the <italic>instantaneous</italic> <sup><xref ref-type="bibr" rid="c74">74</xref></sup> (also termed detailed <sup><xref ref-type="bibr" rid="c45">45</xref></sup>) E-I balance as the Pearson correlation (<italic>ρ</italic>) over time of E and I currents received by each neuron (see Methods).</p>
<p>We observed an excess inhibition in both E and I neurons, with a negative net synaptic input in both E and I cells (<xref rid="fig1" ref-type="fig">Fig. 1H</xref>), indicating an inhibition-dominated network according to the criterion of average balance <sup><xref ref-type="bibr" rid="c73">73</xref></sup>. In E neurons, net synaptic current is the sum of the feedforward current and recurrent inhibition and the mean of the net current is close to the mean of the inhibitory current, because feedforward inputs have vanishing mean. Furthermore, we found a moderate instantaneous balance <sup><xref ref-type="bibr" rid="c75">75</xref></sup>, stronger in I compared to E cell type (<xref rid="fig1" ref-type="fig">Fig. 1G,I</xref>, <italic>ρ</italic> = [0.44, 0.25], for I and E neurons, respectively), similar to levels measured empirically in rat visual cortex <sup><xref ref-type="bibr" rid="c76">76</xref></sup>.</p>
<p>We determined optimal model parameters by optimizing one parameter at a time. To independently validate the so obtained optimal parameter set (reported in <xref ref-type="table" rid="tbl1">Table 1</xref>), we varied all six model parameters explored in the paper with Monte-Carlo random joint sampling (10.000 random samples), uniformly within a biologically plausible parameter range for each parameter (<xref ref-type="table" rid="tbl2">Table 2</xref>). We did not find any parameter configuration with lower average loss than the setting in <xref ref-type="table" rid="tbl1">Table 1</xref> (<xref rid="fig2" ref-type="fig">Fig. 2A-B</xref>) when using the weighting of the encoding error with metabolic cost between 0.4 <italic>&lt; g</italic><sub><italic>L</italic></sub> <italic>&lt;</italic> 0.81 (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>). The three parameter settings that came the closest to our configuration on <xref ref-type="table" rid="tbl1">Table 1</xref> had stronger noise but also stronger metabolic constant than our configuration (<xref ref-type="table" rid="tbl3">Table 3</xref>). The second, third and fourth configurations had longer time constants of both E and I single neurons. Ratios of E-I neuron numbers and of I-I to E-I connectivity in the second, third and fourth best configuration were either jointly increased or decreased with respect to our optimal configuration. This suggests that joint covariations in parameters may influence the network’s optimality. While our finite Monte-Carlo random sampling does not fully prove the global optimality of the configuration in <xref ref-type="table" rid="tbl1">Table 1</xref>, it shows that it is highly efficient.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Table of parameter ranges for Monte-Carlo sampling.</title>
<p>Minimum and maximum of the uniform distributions from which we randomly drew parameters during Monte-Carlo random sampling.</p></caption>
<graphic xlink:href="590955v2_tbl2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><title>Table of best four parameter settings from Monte-Carlo sampling.</title>
<p>Best four parameter settings out of 10000 tested settings. The performance was evaluated using trial- and time-averaged loss. Each parameter setting was evaluated on 20 trials, with each trial using an independent realization of tuning parameters, noise in the non-specific current and initial conditions for the integration of the membrane potentials.</p></caption>
<graphic xlink:href="590955v2_tbl3.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Monte-Carlo joint random sampling on six model parameters.</title>
<p><bold>(A)</bold> Distribution of the trial-averaged loss, with weighting <italic>g</italic><sub><italic>L</italic></sub>=0.7, from 10000 random simulations and using 20 simulation trials of duration of 1 second for each parameter configuration. The red cross marks the average loss of the parameter setting in <xref ref-type="table" rid="tbl1">Table 1</xref>. Inset: The average loss of the parameter setting in <xref ref-type="table" rid="tbl1">Table 1</xref> (red cross) and of the first- and second-best parameter settings from the random search.</p>
<p><bold>(B)</bold> Distribution of the average loss across 20 simulation trials for the parameter setting in <xref ref-type="table" rid="tbl1">Table 1</xref> (red) and for the first four ranked points according to the trial-averaged loss in A. Stars indicate a significant two-tailed t-test against the distribution in red (*** indicate <italic>p</italic> &lt; 0.001).</p>
<p><bold>(C)</bold> Same as in <bold>A</bold>, for different values of weighting of the error with the cost <italic>g</italic><sub><italic>L</italic></sub>. Parameters for all plots are in <xref ref-type="table" rid="tbl1">Table 1</xref></p></caption>
<graphic xlink:href="590955v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2c">
<title>Competition across neurons with similar stimulus tuning emerging in efficient spiking networks</title>
<p>We next explored coding properties emerging from recurrent synaptic interactions between E and I populations in the optimally efficient networks.</p>
<p>An approach that has recently provided empirical insight into local recurrent interactions is measuring effective connectivity with cellular resolution. Recent effective connectivity experiments photostimulated single E neurons in primary visual cortex and measured its effect on neighbouring neurons, finding that the photostimulation of an E neuron led to a decrease in firing rate of similarly tuned close-by neurons <sup><xref ref-type="bibr" rid="c58">58</xref></sup>. This effective lateral inhibition <sup><xref ref-type="bibr" rid="c26">26</xref></sup> between E neurons with similar tuning to the stimulus implements competition between neurons for the representation of stimulus features <sup><xref ref-type="bibr" rid="c58">58</xref></sup>. Since our model instantiates efficient coding by design and because we removed connections between neurons with different selectivity, we expected that our network implements lateral inhibition and would thus give comparable effective connectivity results in simulated photostimulation experiments.</p>
<p>To test this prediction, we simulated photostimulation experiments in our optimally efficient network. We first performed experiments in the absence of the feedforward input to ensure all effects are only due to the recurrent processing. We stimulated a randomly selected single target E neuron and measured the change in the instantaneous firing rate from the baseline firing rate, Δ<italic>z</italic><sub><italic>i</italic></sub>(<italic>t</italic>), in all the other I and E neurons (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, left). The photostimulation was modeled as an application of a constant depolarising current with a strength parameter, <italic>a</italic><sub><italic>p</italic></sub>, proportional to the distance between the resting potential and the firing threshold (<italic>a</italic><sub><italic>p</italic></sub> = 0 means no stimulation, while <italic>a</italic><sub><italic>p</italic></sub> = 1 indicates photostimulation at the firing threshold). We quantified the effect of the simulated photostimulation of a target E neuron on other E and I neurons, distinguishing neurons with either similar or different tuning with respect to the target neuron (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, right; <xref rid="figS2" ref-type="fig">Supplementary Fig. S2A-D</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Mechanism of lateral excitation/inhibition in the efficient spiking network.</title>
<p><bold>(A)</bold> Left: Schematic of the E-I network and of the stimulation and measurement in a perturbation experiment. Right: Schematic of the propagation of the neural activity between E and I neurons with similar tuning.</p>
<p><bold>(B)</bold> Trial and neuron-averaged deviation of the firing rate from the baseline, for the population of I (top) and E (bottom) neurons with similar (magenta) and different tuning (gray) to the target neuron. The stimulation strength corresponded to an increase in the firing rate of the stimulated neuron by 28.0 Hz.</p>
<p><bold>(C)</bold> Scatter plot of the tuning similarity vs. effective connectivity to the target neuron. Red line marks zero effective connectivity and magenta line is the least-squares line. Stimulation strength was <italic>a</italic><sub><italic>p</italic></sub> = 1.</p>
<p><bold>(D)</bold> Correlation of membrane potentials vs. the tuning similarity in E (top) and I cell type (bottom), for the efficient E-I network (left), for the network where each E neuron receives independent instead of shared stimulus features (middle), and for the network with unstructured connectivity (right). In the model with unstructured connectivity, elements of each connectivity matrix were randomly shuffled. We quantified voltage correlation using the (zero-lag) Pearson’s correlation coefficient, denoted as <inline-formula><inline-graphic xlink:href="590955v2_inline148.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, for each pair of neurons.</p>
<p><bold>(E)</bold> Average cross-correlogram (CCG) of spike timing with strongly similar (orange), weakly similar (green) and different tuning (black). Statistical results (<bold>B-E</bold>) were computed on 100 simulation trials. The duration of the trial in <bold>D-E</bold> was 1 second. Parameters for all plots are in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The photostimulation of the target E neuron increased the instantaneous firing rate of similarlytuned I neurons and reduced that of other similarly-tuned E neurons (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>). We quantified the effective connectivity as the difference between the time-averaged firing rate of the recorded cell in presence or absence of the photostimulation of the targeted cell, measured during perturbation and up to 50 ms after. We found positive effective connectivity on I and negative effective connectivity on E neurons with similar tuning to the target neuron, with a positive correlation between tuning similarity and effective connectivity on I neurons and a negative correlation on E neurons (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>). We confirmed these effects of photostimulation in presence of a weak feedforward input (<xref rid="figS2" ref-type="fig">Supplementary Fig. S2E</xref>), similar to the experiments of Ref <sup><xref ref-type="bibr" rid="c58">58</xref></sup> in which photostimulation was applied during the presentation of visual stimuli with weak contrast. Thus, the optimal network replicates the preponderance of negative effective connectivity between E neurons and the dependence of its strength on tuning similarity found in <sup><xref ref-type="bibr" rid="c58">58</xref></sup>.</p>
<p>In summary, lateral excitation of I neurons and lateral inhibition of E neurons with similar tuning is an emerging coding property of the efficient E-I network, which recapitulates competition between neurons with similar stimulus tuning found in visual cortex <sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup>. An intuition of why this competition implements efficient coding is that the E neuron that fires first activates I neurons with similar tuning. In turn, these I neurons inhibit all similarly tuned E neurons (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, right), preventing them to generate redundant spikes to encode the sensory information that has already been encoded by the first spike. Suppression of redundant spiking reduces metabolic cost without reducing encoded information <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>.</p>
<p>While perturbing the activity of E neurons in our model qualitatively reproduces empirically observed lateral inhibition among E neurons <sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup>, these experiments have also reported positive effective connectivity between E neurons with very similar stimulus tuning. Our intuition is that our simple model cannot reproduce this finding because it lacks E-E connectivity.</p>
<p>To explore further the consequences of E-I interactions for stimulus encoding, we next investigated the dynamics of lateral inhibition in the optimal network driven by the feedforward sensory input but without perturbing the neural activity. Previous work has established that efficient spiking neurons may present strong correlations in the membrane potentials, but only weak correlations in the spiking output, because redundant spikes are prevented by lateral in-hibition <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>. We investigated voltage correlations in pairs of neurons within our network as a function of their tuning similarity. Because the feedforward inputs are shared across E neurons and weighted by their tuning parameters, they cause strong positive voltage correlations between E-E neuronal pairs with very similar tuning and strong negative correlations between pairs with very different (opposite) tuning (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>, top-left). Voltage correlations between E-E pairs vanished regardless of tuning similarity when we made the feedforward inputs independent across neurons (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>, top-middle), showing that the dependence of voltage correlations on tuning similarity occurs because of shared feedforward inputs. In contrast to E neurons, I neurons do not receive feedforward inputs and are driven only by similarly tuned E neurons (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>, right). This causes positive voltage correlations in I-I neuronal pairs with similar tuning and vanishing correlations in neurons with different tuning (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>, bottom-left). Such dependence of voltage correlations on tuning similarity disappears when removing the structure from the E-I synaptic connectivity (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>, bottom-right).</p>
<p>In contrast to voltage correlations, and as expected by previous studies <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>, the coordination of spike timing of pairs of E neurons (measured with cross-correlograms or CCGs) was very weak (<xref rid="fig3" ref-type="fig">Fig. 3E</xref>). For I-I and E-I neuronal pairs, the peaks of CCGs were stronger than those observed in E-E pairs, but they were present only at very short lags (lags <italic>&lt;</italic> 1 ms). This confirms that recurrent interactions of the efficient E-I network wipe away the effect of membrane potential correlations at the spiking output level, and shows information processing with millisecond precision in these networks <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>.</p>
</sec>
<sec id="s2d">
<title>The effect of structured connectivity on coding efficiency and neural dynamics</title>
<p>The analytical solution of the optimally efficient E-I network predicts that recurrent synaptic weights are proportional to the tuning similarity between neurons. We next investigated the role of such connectivity structure by comparing the behavior of an efficient network with an unstructured E-I network, similar to the type studied in previous works <sup><xref ref-type="bibr" rid="c77">77</xref>,<xref ref-type="bibr" rid="c78">78</xref>,<xref ref-type="bibr" rid="c23">23</xref></sup>. We removed the connectivity structure by randomly permuting synaptic weights across neuronal pairs (see Methods). Such shuffling destroys the relationship between tuning similarity and synaptic strength (as shown in <xref rid="fig1" ref-type="fig">Fig. 1C(ii)</xref>) while it preserves Dale’s law and the overall distribution of connectivity weights.</p>
<p>We found that shuffling the connectivity structure significantly altered the efficiency of the network (<xref rid="fig4" ref-type="fig">Fig. 4A-B</xref>), neural dynamics (<xref rid="fig4" ref-type="fig">Fig. 4C-D, F-H</xref>) and lateral inhibition (<xref rid="fig4" ref-type="fig">Fig. 4I</xref>). In particular, structured networks differ from unstructured ones by showing better encoding performance (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>), lower metabolic cost (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>), weaker variance of the membrane potential over time (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>), lower firing rates (<xref rid="fig4" ref-type="fig">Fig. 4D</xref>) and weaker average (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>) and instantaneous balance (<xref rid="fig4" ref-type="fig">Fig. 4G</xref>) of synaptic inputs. However, we found only a small difference in the variability of spiking between structured and unstructured networks (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>). While these results are difficult to test experimentally due to the difficulty of manipulating synaptic connectivity structures <italic>in vivo</italic>, they highlight the importance of the connectivity structure for cortical computations.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Effects of connectivity structure on coding efficiency, neural dynamics and lateral inhibition.</title>
<p><bold>(A)</bold> Left: Root mean squared error (RMSE) in networks with structured and randomly shuffled recurrent connectivity. Random shuffling consisted of a random permutation of the elements within each of the three (E-I, I-I, I-E) connectivity matrices. Right: Distribution of decoding weights after training the decoder on neural activity from the structured network (green), and a sample from uniform distribution as typically used in the optimal network.</p>
<p><bold>(B)</bold> Metabolic cost in structured and shuffled networks with matched average balance. The average balance of the shuffled network was matched with the one of the structured network by changing the following parameters: stimulus by factor of 0.88. <inline-formula><inline-graphic xlink:href="590955v2_inline149.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and by decreasing the amplitude of the OU</p>
<p><bold>(C)</bold> Standard deviation of the membrane potential (in mV) for networks with structured and unstructured connectivity. Distributions are across neurons.</p>
<p><bold>(D)</bold> Average firing rate of E (top) and I neurons (bottom) in networks with structured and unstructured connectivity.</p>
<p><bold>(E)</bold> Same as in <bold>D</bold>, showing the coefficient of variation of spiking activity in a network responding to a constant stimulus.</p>
<p><bold>(F)</bold> Same as in <bold>D</bold>, showing the average net synaptic input, a measure of average imbalance.</p>
<p><bold>(G)</bold> Same as in <bold>D</bold>, showing the time-dependent correlation of synaptic inputs, a measure of instantaneous balance.</p>
<p><bold>(H)</bold> Voltage correlation in E-E (top) and I-I neuronal pairs (bottom) for the four cases of unstructured connectivity (colored dots) and the equivalent result in the structured network (grey dots). We show the results for pairs with similar tuning. <bold>(I)</bold> Scatter plot of effective connectivity versus tuning similarity to the photostimulated E neuron in shuffled networks. The title of each plot indicates the connectivity matrix that has been shuffled. The magenta line is the least-squares regression line and the photostimulation is at threshold (<italic>a</italic><sub><italic>p</italic></sub> = 1.0). Results were computed using 200 (<bold>A-G</bold>) and 100 (<bold>H-I</bold>) simulation trials of 1 second duration. Parameters for all plots are in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We also compared structured and unstructured networks about their relation between pairwise voltage correlations and tuning similarity, by randomizing connections within a single connectivity type (E-I, I-I or I-E) or within all these three connectivity types at once (“all”). We found the structure of E-I connectivity to be crucial for the linear relation between voltage correlations and tuning similarity in pairs of I neurons (<xref rid="fig4" ref-type="fig">Fig. 4H</xref>, magenta).</p>
<p>Finally, we analyzed how the structure in recurrent connectivity influences lateral inhibition that we observed in efficient networks. We found that the dependence of lateral inhibition on tuning similarity vanishes when the connectivity structure is fully removed (<xref rid="fig4" ref-type="fig">Fig. 4I</xref>, “all” on the right plot), thus showing that connectivity structure is necessary for lateral inhibition. While networks with unstructured E-I and I-E connectivity still show inhibition in E neurons upon single neuron photostimulation (because of the net inhibitory effect of recurrent connectivity; <xref rid="figS3" ref-type="fig">Supplementary Fig. S3F</xref>), this inhibition was largely unspecific to tuning similarity (<xref rid="fig4" ref-type="fig">Fig. 4I</xref>, “E-I” and “I-E”). Unstructured connectivity decreased the correlation between tuning similarity and effective connectivity from <italic>r</italic> = [0.31, −0.54] in E and I neurons in a structured network to <italic>r</italic> = [0.02, −0.13] and <italic>r</italic> = [0.57, 0.11] in networks with unstructured E-I and I-E connectivity, respectively. Removing the structure in I-I connectivity, in contrast, increased the correlation between effective connectivity and tuning similarity in E neurons (<italic>r</italic> = [0.30, −0.65], <xref rid="fig4" ref-type="fig">Fig. 4I</xref>, second from the left), showing that lateral inhibition takes place irrespectively of the I-I connectivity structure.</p>
<p>Previous empirical <sup><xref ref-type="bibr" rid="c56">56</xref></sup> and theoretical work has established the necessity of strong E-I-E synaptic connectivity for lateral inhibition<sup><xref ref-type="bibr" rid="c57">57</xref>,<xref ref-type="bibr" rid="c79">79</xref></sup>. To refine this understanding, we asked what is the minimal connectivity structure necessary to qualitatively replicate empirically observed lateral inhibition. We did so by considering a simpler connectivity rule than the one obtained from first principles. We assumed neurons to be connected (with random synaptic efficacy) if their tuning vectors are similar <inline-formula><inline-graphic xlink:href="590955v2_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and unconnected otherwise <inline-formula><inline-graphic xlink:href="590955v2_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, relaxing the precise proportionality relationship between tuning similarity and synaptic weights (as on <xref rid="fig1" ref-type="fig">Fig. 1C(ii)</xref>). We found that networks with such simpler connectivity respond to activity perturbation in a qualitatively similar way as the optimal network (<xref rid="figS2" ref-type="fig">Supplementary Fig. S2F</xref>) and still replicate experimentally observed activity profiles in <sup><xref ref-type="bibr" rid="c58">58</xref></sup>.</p>
<p>While optimally structured connectivity predicted by efficient coding is biologically plausible, it may be difficult to realise it exactly on a synapse-by-synapse basis in biological networks. We verified the robustness of the model to small deviations from the optimal synaptic weights by adding a random jitter, proportional to the synaptic strength, to all synaptic connections (see Methods). The encoding performance and neural activity were barely affected by weak and moderate levels of such perturbation (<xref rid="figS3" ref-type="fig">Supplementary Fig. S3 G-H</xref>), demonstrating that the network is robust against random jittering of the optimal synaptic weights.</p>
<p>In summary, we found that some aspects of recurrent connectivity structure, such as the like-to-like organization, are crucial to achieve efficient coding. Instead, for other aspects there is considerable flexibility; the proportionality between tuning similarity and synaptic weights is not crucial for efficiency and small random jitter of optimal weights has only minor effects. Structured E-I and I-E, but not I-I connectivity, is necessary for implementing experimentally observed pattern of lateral inhibition whose strength is modulated by tuning similarity.</p>
</sec>
<sec id="s2e">
<title>Weak or no spike-triggered adaptation optimizes network efficiency</title>
<p>We next investigated the role of within-neuron feedback triggered by each spike, <inline-formula><inline-graphic xlink:href="590955v2_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, that emerges from the optimally efficient solution (<xref ref-type="disp-formula" rid="eqn5">Eq. 5</xref>). A previous study <sup><xref ref-type="bibr" rid="c33">33</xref></sup> showed that spike-triggered adaptation, together with structured connectivity, redistributes the activity from highly excitable neurons to less excitable neurons, leaving the population readout invariant. Here, we address model efficiency in presence of adapting or facilitating feedback as well as differential effects of adaptation in E and I neurons.</p>
<p>The spike-triggered within-neuron feedback <inline-formula><inline-graphic xlink:href="590955v2_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> has a time constant equal to that of the single neuron readout <inline-formula><inline-graphic xlink:href="590955v2_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (E neurons) and <inline-formula><inline-graphic xlink:href="590955v2_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (I neurons). The strength of the current is proportional to the difference in inverse time constants of single neuron and population readouts, <inline-formula><inline-graphic xlink:href="590955v2_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This spike-triggered current is negative, giving spike-triggered adaptation <sup><xref ref-type="bibr" rid="c40">40</xref></sup>, if the single-neuron readout has longer time constant than the population readout <inline-formula><inline-graphic xlink:href="590955v2_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, or positive, giving spike-triggered facilitation, if the opposite is true <inline-formula><inline-graphic xlink:href="590955v2_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="table" rid="tbl4">Table 4</xref>). We expected that network efficiency would benefit from spike-triggered adaptation, because accurate encoding requires fast temporal dynamics of the population readouts, to capture fast fluctuations in the target signal, while we expect a slower dynamics in the readout of single neuron’s firing frequency, <inline-formula><inline-graphic xlink:href="590955v2_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, a process that could be related to homeostatic regulation of single neuron’s firing rate <sup><xref ref-type="bibr" rid="c80">80</xref>,<xref ref-type="bibr" rid="c81">81</xref></sup>. In our optimal E-I network we indeed found that optimal coding efficiency is achieved in absence of within-neuron feedback or with weak adaptation in both cell types (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). The optimal set of time constants <inline-formula><inline-graphic xlink:href="590955v2_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula> only weakly depended on the weighting of the encoding error with the metabolic cost <italic>g</italic><sub><italic>L</italic></sub> (<xref rid="figS4" ref-type="fig">Supplementary Fig. S4A</xref>). We note that adaptation in E neurons promotes efficient coding because it enforces every spike to be error-correcting, while a spike-triggered facilitation in E neurons would lead to additional spikes that might be redundant and reduce network efficiency. Contrary to previously proposed models of adaptation in LIF neurons<sup><xref ref-type="bibr" rid="c39">39</xref>,<xref ref-type="bibr" rid="c82">82</xref></sup>, the strength and the time constant of adaptation in our model are not independent, but they both depend on <inline-formula><inline-graphic xlink:href="590955v2_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, with larger <inline-formula><inline-graphic xlink:href="590955v2_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> yielding both longer and stronger adaptation.</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4.</label>
<caption><title>Relation of time constants of single-neuron and population readout set an adaptation or a facilitation current.</title>
<p>The population readout that evolves on a faster (slower) time scale than the single neuron readout determines a spike-triggered adaptation (facilitation) in its own cell type.</p></caption>
<graphic xlink:href="590955v2_tbl4.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Adaptation, network coding efficiency and excitation-inhibition balance.</title>
<p><bold>(A)</bold> The encoding error (left), metabolic cost (middle) and average loss (right) as a function of single neuron time constants <inline-formula><inline-graphic xlink:href="590955v2_inline150.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (E neurons) and <inline-formula><inline-graphic xlink:href="590955v2_inline151.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (I neurons), in units of ms. These parameters set the sign, the strength, as well as the time constant of the feedback current in E and I neurons. Best performance (lowest average loss) is obtained in the top right quadrant, where the feedback current is spike-triggered adaptation in both E and I neurons. The performance measures are computed as a weighted sum of the respective measures across the E and I populations with equal weighting for E and I. All measures are plotted on the scale of the natural logarithm for better visibility.</p>
<p><bold>(B)</bold> Top: Log-log plot of the RMSE of the E (red) and the I (blue) estimates as a function of the time constant of the single neuron readout of E neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline152.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, in the regime with spike-triggered adaptation. Feedback current in I neurons is set to 0. Bottom: Same as on top, as a function of <inline-formula><inline-graphic xlink:href="590955v2_inline153.gif" mime-subtype="gif" mimetype="image"/></inline-formula> while the feedback current in E neurons is set to 0.</p>
<p><bold>(C)</bold> Same as in <bold>B</bold>, showing the average loss.</p>
<p><bold>(D)</bold> Same as in <bold>B</bold>, showing the firing rate.</p>
<p><bold>(E)</bold> Firing rate in E (left) and I neurons (right), as a function of time constants <inline-formula><inline-graphic xlink:href="590955v2_inline154.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline155.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p><bold>(F)</bold> Same as in <bold>E</bold>, showing the coefficient of variation.</p>
<p><bold>(G)</bold> Same as <bold>E</bold>, showing the average net synaptic input, a measure of average imbalance.</p>
<p><bold>(H)</bold> Same as <bold>E</bold>, showing the average net synaptic input, a measure of instantaneous balance. All statistical results were computed on 100 simulation trials of 1 second duration. For other parameters, see <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To gain insights on the differential effect of adaptation in E vs I neurons, we set the adaptation in one cell type to 0 and vary the strength of adaptation in the other cell type by varying the time constant of the single neuron readout. With adaptation in E neurons (and no adaptation in I), we observed a slow increase of the encoding error in E neurons, while the encoding error increased faster with adaptation in I neurons (<xref rid="fig5" ref-type="fig">Fig. 5B</xref>). Similarly, network efficiency increased slowly with adaptation in E and faster with adaptation in I neurons (<xref rid="fig5" ref-type="fig">Fig. 5C</xref>), thus showing that adaptation in E neurons decreases less the performance compared to the adaptation in I neurons. With increasing adaptation in E neurons, the firing rate in E neurons decreased (<xref rid="fig5" ref-type="fig">Fig. 5D</xref>), leading to E estimates with smaller amplitude. Because E estimates are target signals for I neurons and because weaker E signals imply weaker drive to I neurons, average loss of the I population decreased by increasing adaptation in E neurons (<xref rid="fig5" ref-type="fig">Fig. 5C</xref> top, blue trace).</p>
<p>Firing rates and variability of spiking were sensitive to the strength of adaptation. As expected, adaptation in E neurons caused a decrease in the firing levels in both cell types (<xref rid="fig5" ref-type="fig">Fig. 5D-E</xref>). In contrast, adaptation in I neurons decreased the firing rate in I neurons, but increased the firing rate in E neurons, due to a decrease in the level of inhibition. Furthermore, adaptation decreased the variability of spiking, in particular in the cell type with strong adaptation (<xref rid="fig5" ref-type="fig">Fig. 5F</xref>), a well-known effect of spike-triggered adaptation in single neurons <sup><xref ref-type="bibr" rid="c82">82</xref></sup>.</p>
<p>In regimes with adaptation, time constants of single neuron readout <inline-formula><inline-graphic xlink:href="590955v2_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula> influenced the average balance (<xref rid="fig5" ref-type="fig">Fig. 5G</xref>) as well as the instantaneous balance (<xref rid="fig5" ref-type="fig">Fig. 5H</xref>) in E and I cell type. To gain a better understanding of the relationship between adaptation, E-I interactions and network optimality, we measured the instantaneous and time-averaged E-I balance while varying the adaptation parameters and studied their relation with the loss. By increasing adaptation in E neurons, the average imbalance got weaker in E neurons (<xref rid="fig5" ref-type="fig">Fig. 5G</xref>, left), but stronger in I neurons (<xref rid="fig5" ref-type="fig">Fig. 5G</xref>, right). Regimes with precise average balance in both cell types were suboptimal (compare <xref rid="fig5" ref-type="fig">Fig. 5A</xref>, right and <xref rid="fig5" ref-type="fig">G</xref>), while regimes with precise instantaneous balance were highly efficient (compare <xref rid="fig5" ref-type="fig">Fig. 5A</xref>, right and <xref rid="fig5" ref-type="fig">H</xref>).</p>
<p>To test how well the average balance and the instantaneous balance of synaptic inputs predict network efficiency, we concatenated the column-vectors of the measured average loss and of the average imbalance in each cell type and computed the Pearson correlation between these quantities. The correlation between the average imbalance and the average loss was weak in the E cell type (<italic>r</italic> = 0.16) and close to zero in the I cell type (<italic>r</italic> = 0.02), suggesting almost no relation between efficiency and average imbalance. In contrast, the average loss was negatively correlated with the instantaneous balance in both E (<italic>r</italic> = −0.35) and in I cell type (<italic>r</italic> = −0.45), showing that instantaneous balance of synaptic inputs is positively correlated with network efficiency. When measured for varying levels of spike-triggered adaptation, unlike the average balance of synaptic inputs, the instantaneous balance is thus mildly predictive of network efficiency.</p>
<p>In summary, our results show that the absence of within-neuron feedback or the presence of weak and short-lasting spike-triggered adaptation in both E and I neurons are optimally efficient solutions, and predict that information coding would be more efficient with adaptation than with facilitation. Assuming that our I neurons describe parvalbumin-positive interneurons, our results suggest that the weaker adaptation in I compared to E neurons, reported empirically<sup><xref ref-type="bibr" rid="c60">60</xref></sup>, may be beneficial for the network’s encoding efficiency.</p>
<p>Spike-triggered adaptation in our model captures adaptive processes in single neurons that occur on time scales lasting from a couple of milliseconds to tens of milliseconds after each spike. However, spiking in biological neurons triggers adaptation on multiple time scales, including much slower time scales on the order of seconds or tens of seconds <sup><xref ref-type="bibr" rid="c83">83</xref></sup>. Our model does not capture adaptive processes on these longer time scales (but see <sup><xref ref-type="bibr" rid="c33">33</xref></sup>).</p>
</sec>
<sec id="s2f">
<title>Non-specific currents regulate network coding properties</title>
<p>In our derivation of the optimal network, we obtained a non-specific external current (in the following, non-specific current) <inline-formula><inline-graphic xlink:href="590955v2_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Non-specific current captures all synaptic currents that are unrelated and unspecific to the stimulus features. This non-specific term collates effects of synaptic currents from neurons untuned to the stimulus <sup><xref ref-type="bibr" rid="c84">84</xref>,<xref ref-type="bibr" rid="c85">85</xref></sup>, as well as synaptic currents from other brain areas. It can be conceptualized as the background synaptic activity that provides a large fraction of all synaptic inputs to both E and I neurons in cortical networks <sup><xref ref-type="bibr" rid="c86">86</xref></sup>, and which may modulate feedforward-driven responses by controlling the distance between the membrane potential and the firing threshold <sup><xref ref-type="bibr" rid="c62">62</xref></sup>. Likewise, in our model, the non-specific current does not directly convey information about the feedforward input features, but influences the network dynamics.</p>
<p>Non-specific current comprises mean and fluctuations (see Methods). The mean is proportional to the metabolic constant <italic>β</italic> and its fluctuations reflect the noise that we included in the condition for spiking. Since <italic>β</italic> governs the trade-off between encoding error and metabolic cost (<xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>), higher values of <italic>β</italic> imply that more importance is assigned to the metabolic efficiency than to coding accuracy, yielding a reduction in firing rates. In the expression for the non-specific current, we found that the mean of the current is negatively proportional to the metabolic constant <italic>β</italic> (see Methods). Because the non-specific current is typically depolarizing, this means that increasing <italic>β</italic> yields a weaker non-specific current and increases the distance between the mean membrane potential and the firing threshold. Thus, an increase of the metabolic constant is expected to make the network less responsive to the feedforward signal.</p>
<p>We found the metabolic constant <italic>β</italic> to significantly influence the spiking dynamics (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>). The optimal efficiency was achieved for non-zero levels of the metabolic constant (<xref rid="fig6" ref-type="fig">Fig. 6B</xref>), with the mean of the non-specific current spanning more than half of the distance between the resting potential and the threshold (<xref ref-type="table" rid="tbl1">Table 1</xref>). Stronger weighting of the loss of I compared to E neurons and stronger weighting of the error compared to the cost yielded weaker optimal metabolic constant (<xref rid="figS4" ref-type="fig">Supplementary Fig. S4B</xref>). Metabolic constant modulated the firing rate as expected, with the firing rate in E and I neurons decreasing with the increasing of the metabolic constant (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>, top). It also modulated the variability of spiking, as increasing the metabolic constant decreased the variability of spiking in both cell types (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>, bottom). Furthermore, it modulated the average balance and the instantaneous balance in opposite ways: larger values of <italic>β</italic> led to regimes that had stronger average balance, but weaker instantaneous balance (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>). We note that, even with suboptimal values of the metabolic constant, the neural dynamics remained within biologically relevant ranges.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>State-dependent coding and dynamics are controlled by non-specific currents.</title>
<p><bold>(A)</bold> Spike trains of the efficient E-I network in one simulation trial, with different values of the metabolic constant <italic>β</italic>. The network received identical stimulus across trials.</p>
<p><bold>(B)</bold> Top: RMSE of E (red) and I (blue) estimates as a function of the metabolic constant. Bottom: Normalized average metabolic cost and average loss as a function of the metabolic constant. Black arrow indicates the minimum loss and therefore the optimal metabolic constant.</p>
<p><bold>(C)</bold> Average firing rate (top) and the coefficient of variation of the spiking activity (bottom), as a function of the metabolic constant. Black arrow marks the metabolic constant leading to optimal network efficiency in <bold>B</bold>.</p>
<p><bold>(D)</bold> Average imbalance (top) and instantaneous balance (bottom) balance as a function of the metabolic constant.</p>
<p><bold>(E)</bold> Same as in <bold>A</bold>, for different values of the noise strength <italic>σ</italic>.</p>
<p><bold>(F)</bold> Same as in <bold>B</bold>, as a function of the noise strength. The noise is a Gaussian random process, independent over time and across neurons.</p>
<p><bold>(G)</bold> Same as <bold>C</bold>, as a function of the noise strength.</p>
<p><bold>(H)</bold> Top: Same as in <bold>D</bold>, as a function of the noise strength.</p>
<p><bold>(I)</bold> The encoding error measured as RMSE (left), the metabolic cost (middle) and the average loss (right) as a function of the metabolic constant <italic>β</italic> and the noise strength <italic>σ</italic>. Metabolic constant and noise strength that are optimal for the single parameter search (in <bold>B</bold> and <bold>F</bold>) are marked with a red cross in the figure on the right. For plots in <bold>B-D</bold> and <bold>F-I</bold>, we computed and averaged results over 100 simulation trials with 1 second duration. For other parameters, see <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The fluctuation part of the non-specific current, modulated by the noise strength <italic>σ</italic> that we added in the definition of spiking rule for biological plausibility (see Methods), strongly affected the neural dynamics as well (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>). The optimal performance was achieved with non-vanishing noise levels (<xref rid="fig6" ref-type="fig">Fig. 6F</xref>), similarly to previous work showing that the noise prevents excessive network synchronization that would harm performance <sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c87">87</xref></sup>. The optimal noise strength depended on the weighting of the error with the cost, with strong weighting of the error predicting stronger noise intensity (<xref rid="figS4" ref-type="fig">Supplementary Fig. S4C</xref>).</p>
<p>The average firing rate of both cell types, as well as the variability of spiking in E neurons, increased with noise strength (<xref rid="fig6" ref-type="fig">Fig. 6G</xref>), and some level of noise in the non-specific inputs was necessary to establish the optimal level of spiking variability. Nevertheless, we measured significant levels of spiking variability already in the absence of noise, with a coefficient of variation of about 0.8 in E and 0.9 in I neurons (<xref rid="fig6" ref-type="fig">Fig. 6G</xref>, bottom). This indicates that the recurrent network dynamics generates substantial variability even in absence of an external source of noise. The average and instantaneous balance of synaptic currents exhibited a non-linear behavior as a function of noise strength (<xref rid="fig6" ref-type="fig">Fig. 6H</xref>). Due to decorrelation of membrane potentials by the noise, instantaneous balance in I neurons decreased with increasing noise strength (<xref rid="fig6" ref-type="fig">Fig. 6H</xref>, bottom).</p>
<p>Next, we investigated the joint impact of the metabolic constant and the noise strength on network optimality. We expect these two parameters to be related, because larger noise strength requires stronger metabolic constant to prevent the activity of the network to be dominated by noise. We thus performed a 2-dimensional parameter search (<xref rid="fig6" ref-type="fig">Fig. 6I</xref>). As expected, the optima of the metabolic constant and the noise strength were positively correlated. A weaker noise required lower metabolic constant, and-vice-versa. While achieving maximal efficiency at non-zero levels of the metabolic cost and noise (see <xref rid="fig6" ref-type="fig">Fig. 6I</xref>) might seem counterintuitive, we speculate that such setting is optimal because some noise in the non-specific current prevents over-synchronization and over-regularity of firing that would harm efficiency, similarly to what was shown in previous works <sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c87">87</xref></sup>. In the presence of noise, a non-zero metabolic constant is needed to suppress inefficient spikes purely induced by noise that do not contribute to coding and increase the error. This gives rise to a form of stochastic resonance, where an optimal level of noise is helpful to detect the signal coming from the feedforward currents.</p>
<p>In summary, non-specific external currents derived in our optimal solution have a major effect on coding efficiency and on neural dynamics. In qualitative agreement with empirical measurements <sup><xref ref-type="bibr" rid="c86">86</xref>,<xref ref-type="bibr" rid="c62">62</xref></sup>, our model predicts that more than half of the average distance between the resting potential and firing threshold is accounted for by non-specific synaptic currents. Similarly to previous theoretical work <sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>, we find that some level of external noise, in the form of a random fluctuation of the non-specific synaptic current, is beneficial for network efficiency. This remains a prediction for experiments.</p>
</sec>
<sec id="s2g">
<title>Optimal ratio of E-I neuron numbers and of the mean I-I to E-I synaptic efficacy coincide with biophysical measurements</title>
<p>Next, we investigated how coding efficiency and neural dynamics depend on the ratio of the number of E and I neurons (<italic>N</italic> <sup><italic>E</italic></sup> : <italic>N</italic> <sup><italic>I</italic></sup> or E-I ratio) and on the relative synaptic strengths between E-I and I-I connections.</p>
<p>Efficiency objectives (<xref ref-type="disp-formula" rid="eqn3">Eq. 3</xref>) are based on population, rather than single-neuron activity. Our efficient E-I network thus realizes a computation of the target representation that is distributed across multiple neurons (<xref rid="fig7" ref-type="fig">Fig. 7A</xref>). Following previous reports <sup><xref ref-type="bibr" rid="c37">37</xref></sup>, we predict that, if the number of neurons within the population decreases, neurons have to fire more spikes to achieve an optimal population readout because the task of tracking the target signal is distributed among fewer neurons. To test this prediction, we varied the number of I neurons while keeping the number of E neurons constant. As predicted, a decrease of the number of I neurons (and thus an increase in the ratio of the number of E to I neurons) caused a linear increase in the firing rate of I neurons, while the firing rate of E neurons stayed constant (<xref rid="fig7" ref-type="fig">Fig. 7B</xref>, top). However, the variability of spiking and the average synaptic inputs remained relatively constant in both cell types as we varied the E-I ratio (<xref rid="fig7" ref-type="fig">Fig. 7B</xref>, bottom, <xref rid="fig7" ref-type="fig">C</xref>), indicating a compensation for the change in the ratio of E-I neuron numbers through adjustment in the firing rates. These results are consistent with the observation in neuronal cultures of a linear change in the rate of postsynaptic events but unchanged postsynaptic current in either E and I neurons for variations in the E-I neuron number ratio <sup><xref ref-type="bibr" rid="c88">88</xref></sup>.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Optimal ratios of E-I neuron numbers and of mean I-I to E-I efficacy.</title>
<p><bold>(A)</bold> Schematic of the effect of changing the number of I neurons on firing rates of I neurons. As encoding of the stimulus is distributed among more I neurons, the number of spikes per I neuron decreases.</p>
<p><bold>(B)</bold> Average firing rate as a function of the ratio of the number of E to I neurons. Black arrow marks the optimal ratio.</p>
<p><bold>(C)</bold> Average net synaptic input in E neurons (top) and in I neurons (bottom).</p>
<p><bold>(D)</bold> Top: Encoding error (RMSE) of the E (red) and I (blue) estimates, as a function of the ratio of E-I neuron numbers. Bottom: Same as on top, showing the cost and the average loss. Black arrow shows the minimum of the loss, indicating the optimal parameter.</p>
<p><bold>(E)</bold> Top: Optimal ratio of the number of E to I neurons as a function of the weighting of the average loss of E and I cell type (using the weighting of the error and cost of 0.7 and 0.3, respectively). Bottom: Same as on top, measured as a function of the weighting of the error and the cost when computing the loss. (The weighting of the losses of E and I neurons is 0.5.) Black triangles mark weightings that we typically used.</p>
<p><bold>(F)</bold> Schematic of the readout of the spiking activity of E (red) and I population (blue) with equal amplitude of decoding weights (left) and with stronger decoding weight in I neuron (right). Stronger decoding weight in I neurons results in a stronger effect of spikes on the readout, leading to less spikes by the I population.</p>
<p><bold>(G-H)</bold> Same as in <bold>D</bold> and <bold>B</bold>, as a function of the ratio of mean I-I to E-I efficacy.</p>
<p><bold>(I)</bold> Average imbalance (top) and instantaneous balance (bottom) balance, as a function of the ratio of mean I-I to E-I efficacy.</p>
<p><bold>(J)</bold> The encoding error (RMSE; left) the metabolic cost (middle) and the average loss (right) as a function of the ratio of E-I neuron numbers and the ratio of mean I-I to E-I connectivity. The optimal ratios obtained with single parameter search (in <bold>D</bold> and <bold>G</bold>) are marked with a red cross. All statistical results were computed on 100 simulation trials of 1 second duration. For other parameters, see <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></caption>
<graphic xlink:href="590955v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The ratio of the number of E to I neurons had a significant influence on coding efficiency. We found a unique minimum of the encoding error of each cell type, while the metabolic cost increased linearly with the ratio of the number of E and I neurons (<xref rid="fig7" ref-type="fig">Fig. 7D</xref>). We found the optimal ratio of E to I neuron numbers to be in range observed experimentally in cortical circuits (<xref rid="fig7" ref-type="fig">Fig. 7D</xref>, bottom, black arrow, <italic>N</italic> <sup><italic>E</italic></sup> : <italic>N</italic> <sup><italic>I</italic></sup> = 3.75 : 1; <sup><xref ref-type="bibr" rid="c89">89</xref></sup>). The optimal ratio depends on the weighting of the error with the cost, decreasing when increasing the cost of firing (<xref rid="fig7" ref-type="fig">Fig. 7E</xref>, bottom). Also the encoding error (RMSE) alone, without considering the metabolic cost, predicted optimal ratio of the number of E to I neurons within a plausible physiological range, <italic>N</italic> <sup><italic>E</italic></sup> : <italic>N</italic> <sup><italic>I</italic></sup> = [3.75 : 1, 5.25 : 1], with stronger weightings of the encoding error by I neurons predicting higher ratios (<xref rid="fig7" ref-type="fig">Fig. 7E</xref>, top).</p>
<p>Next, we investigated the impact of the strength of E and I synaptic efficacy (EPSPs and IPSPs). As evident from the expression for the population readouts (<xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref>), the magnitude of tuning parameters (which are also decoding weights) determines the amplitude of jumps of the population readout caused by spikes (<xref rid="fig7" ref-type="fig">Fig. 7F</xref>). The larger these weights are, the larger is the impact of spikes on the population signals.</p>
<p>E and I synaptic efficacies depend on the tuning parameters. We parametrized the distribution of tuning parameters as uniform distributions centered at zero, but allowed the spread of distributions in E and I neurons (<inline-formula><inline-graphic xlink:href="590955v2_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) to vary across E and I cell type (Methods). In the optimally efficient network, as found analytically (Methods section “Dynamic equations for the membrane potentials”), the E-I connectivity is the transpose of the of the I-E connectivity, which implies that these connectivities are exactly balanced and have the same mean. We also showed analytically that by parametrizing tuning parameters with uniform distributions, the scaling of synaptic connectivity of E-I (equal to I-E) and I-I connectivity is controlled by the variance of tuning parameters of the pre and postsynaptic population as follows: <inline-formula><inline-graphic xlink:href="590955v2_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Using these insights, we were able to analytically evaluate the mean E-I and I-I synaptic efficacy (see Methods section “Parametrization of synaptic connectivity”).</p>
<p>We next searched for the optimal ratio of the mean I-I to E-I efficacy as the parameter that maximizes network efficiency. Network efficiency was maximized when such ratio was about 3 to 1 (<xref rid="fig7" ref-type="fig">Fig. 7G</xref>). Our results suggest the maximum E-I and I-E synaptic efficacy, averaged across neuronal pairs, of 0.75 mV, and the maximal I-I efficacy of 2.25 mV, values that are consistent with empirical measurements in the primary sensory cortex<sup><xref ref-type="bibr" rid="c90">90</xref>,<xref ref-type="bibr" rid="c60">60</xref>,<xref ref-type="bibr" rid="c61">61</xref></sup>. The optimal ratio of mean I-I to E-I connectivity decreased when the error was weighted more with respect to the metabolic cost (<xref rid="figS4" ref-type="fig">Supplementary Fig. S4D</xref>).</p>
<p>Similarly to the ratio of E-I neuron numbers, a change in the ratio of mean E-I to I-E synaptic efficacy was compensated for by a change in firing rates, with stronger I-I synapses leading to a decrease in the firing rate of I neurons (<xref rid="fig7" ref-type="fig">Fig. 7H</xref>, top). Conversely, weakening the E-I (and I-E) synapses resulted in an increase in the firing rate in E neurons (<xref rid="figS4" ref-type="fig">Supplementary Fig. S4E-F</xref>).</p>
<p>This is easily understood by considering that weakening the E-I and I-E synapses activates less strongly the lateral inhibition in E neurons (<xref rid="fig3" ref-type="fig">Fig. 3</xref>) and thus leads to an increase in the firing rate of E neurons. We also found that single neuron variability remained almost unchanged when varying the ratio of mean I-I to E-I efficacy (<xref rid="fig7" ref-type="fig">Fig. 7H</xref>, bottom) and the optimal ratio yielded optimal levels of average and instantaneous balance of synaptic inputs, as found previously (<xref rid="fig7" ref-type="fig">Fig. 7I</xref>). The instantaneous balance monotonically decreased with increasing ratio of I-I to E-I efficacy (<xref rid="fig7" ref-type="fig">Fig. 7I</xref>, bottom, <xref rid="figS4" ref-type="fig">Supplementary Fig. S4G</xref>).</p>
<p>Further, we tested the co-dependency of network optimality on the above two ratios with a 2-dimensional parameter search. We expected a positive correlation of network performance as a function of these two parameters, because both of them regulate the level of instantaneous E-I balance in the network. We found that the lower ratio of E-I neuron numbers indeed predicts a lower ratio of the mean I-I to E-I connectivity (<xref rid="fig7" ref-type="fig">Fig. 7J</xref>). This is because fewer E neurons bring less excitation in the network, thus requiring less inhibition to achieve optimal levels of instantaneous balance. The co-dependency of the two parameters in affecting network optimality might be informative as to why E-I neuron number ratios may vary across species (for example, it is reported to be 2:1 in human cortex <sup><xref ref-type="bibr" rid="c91">91</xref></sup> and 4:1 in mouse cortex). Our model predicts that lower E-I neuron number ratios require weaker mean I-I to E-I connectivity.</p>
<p>In summary, our analysis suggests that optimal coding efficiency is achieved with four times more E neurons than I neurons and with mean I-I synaptic efficacy about 3 times stronger than the E-I and I-E efficacy. The optimal network has less I than E neurons, but the impact of spikes of I neurons on the population readout is stronger, also suggesting that spikes of I neurons convey more information.</p>
</sec>
<sec id="s2h">
<title>Dependence of efficient coding and neural dynamics on the stimulus statistics</title>
<p>We further investigated how the network’s behavior depends on the timescales of the input stimulus features. We manipulated the stimulus timescales by changing the time constants of <italic>M</italic> = 3 OU processes. The network efficiently encoded stimulus features when their time constants varied between 1 and 200 ms, with stable encoding error, metabolic cost (<xref rid="fig8" ref-type="fig">Fig. 8A</xref>) and neural dynamics (<xref rid="figS5" ref-type="fig">Supplementary Fig. S5 A-B</xref>). To examine if the network can efficiently encode also stimuli that evolve on different timescales, we tested its performance in response to <italic>M</italic> = 3 input variables, each with a different timescale. We kept the timescale of the first variable constant at <inline-formula><inline-graphic xlink:href="590955v2_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula> ms, while we varied the time constants of the other two keeping the time constant of the third twice as long as that of the second. We found excellent performance of the network in response to such stimuli that was stable across timescales (<xref rid="fig8" ref-type="fig">Fig. 8B</xref>). The prediction that the network can encode information effectively over a wide range of time scales can be tested experimentally, by measuring the sensory information encoded by the activity of a set of neurons while varying the sensory stimulus timescales over a wide range.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Dependence of efficient coding and neural dynamics on stimulus parameters and comparison of E-I versus one cell type model architecture.</title>
<p><bold>(A)</bold> Top: Root mean squared error (RMSE) of E estimates (red) and I estimates (blue), as a function of the time constant (in ms) of stimulus features. The time constant <italic>τ</italic><sub><italic>s</italic></sub> is the same for all stimulus features. Bottom: Same as on top, showing the metabolic cost (MC) of E and I cell type.</p>
<p><bold>(B)</bold> Left: Mean squared error between the targets and their estimates for every stimulus feature (marked as dimensions), as a function of time constants of OU stimuli in E population (top) and in I population (bottom). In the first dimension, the stimulus feature has a time constant fixed at 10 ms, while the second and third feature increase their time constants from left to right. The time constant of the third stimulus feature (x-axis on the bottom) is the double of the time constant of the second stimulus feature (x-axis on top). Right: Same as on the left, showing the RMSE that was averaged across stimulus features (top), and the metabolic cost (bottom) in E (red) and I (blue) populations.</p>
<p><bold>(C)</bold> Top: Same as in <bold>A</bold> top, measured as a function of the number of stimulus features <italic>M</italic>. Bottom: Normalized cost and the average loss as a function of the number of stimulus features. Black arrow marks the minimum loss and the optimal parameter <italic>M</italic>.</p>
<p><bold>(D)</bold> Top: Optimal number of encoded variables (stimulus features) as a function of weighting of the losses of E and I population. The weighting of the error with the cost is 0.7. Bottom: Same as on top, as a function of the weighting of the error with the cost and with equal weighting of losses of E and I populations.</p>
<p><bold>(E)</bold> Tuning curves of 10 example E (left) and I neurons (right). We computed tuning curves using <italic>M</italic> =3 stimulus features that were constant over time. We varied the amplitude of the first stimulus feature <italic>s</italic><sub>1</sub>, while two other stimulus features were kept fixed.</p>
<p><bold>(F)</bold> Distribution of the selectivity index across E (red) and I neurons (blue).<bold>(G)</bold> Root mean squared error (left) and metabolic cost (right) in E and I populations in the E-I model and in the 1CT model. The distribution is across 100 simulation trials.</p>
<p><bold>(H)</bold> Left: Average loss in the E population of the E-I model and of the 1CT model. The distribution is across 100 simulation trials. Right: Average loss in the E population of the E-I models and in the 1CT model as a function of the weighting <italic>g</italic><sub><italic>L</italic></sub>, averaged across trials.</p>
<p><bold>(I)</bold> Firing rate in the 1CT model as a function of the metabolic constant. All statistical results were computed on 100 simulation trials of 1 second duration. For other parameters of the E-I model see <xref ref-type="table" rid="tbl1">Table 1</xref>, and for the 1CT model see Supplementary Table S2.</p></caption>
<graphic xlink:href="590955v2_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We next examined network performance while varying the timescale of targets <italic>τ</italic><sub><italic>x</italic></sub> (see <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>). Because we assumed that the target time constants equal the membrane time constant of E and I neurons (<italic>τ</italic><sub><italic>x</italic></sub> = <italic>τ</italic> <sup><italic>E</italic></sup> = <italic>τ</italic> <sup><italic>I</italic></sup> = <italic>τ</italic>), it is not surprising that the best performance was achieved when these time constants were similar (<xref rid="figS5" ref-type="fig">Supplementary Fig. S5C</xref>). Firing rates, firing variability and the average and instantaneous balance did not change appreciably with this time constant (<xref rid="figS5" ref-type="fig">Supplementary Fig. S5D-E</xref>).</p>
<p>Next, we tested how the network’s behavior changed when we varied the number of stimulus features <italic>M</italic>. Because all other parameters were optimized using <italic>M</italic> = 3, the encoding error of E (RMSE<sup><italic>E</italic></sup>) and I neurons (RMSE<sup><italic>I</italic></sup>) achieved a minimum around this value (<xref rid="fig8" ref-type="fig">Fig. 8C</xref>, top). The metabolic cost increased monotonically with <italic>M</italic> (<xref rid="fig8" ref-type="fig">Fig. 8C</xref>, bottom). The number of features that optimized network efficiency (and minimized the average loss) depended on <italic>g</italic><sub><italic>L</italic></sub>, with stronger penalty of firing yielding a smaller optimal number of features. Increasing <italic>M</italic> beyond the optimal number resulted in a gentle monotonic increase in firing rates for both E and I neurons, and it increased the average E-I balance and weakened the instantaneous balance (<xref rid="figS5" ref-type="fig">Supplementary Fig. S5F-G</xref>).</p>
<p>We next characterized the tuning and the stimulus selectivity of E and I neurons. E neurons receive a feedforward current, which is expected to make them stimulus-selective, while I neurons receive synaptic inputs from E neurons through dense E-I connectivity. We measured stimulus tuning by computing tuning curves for each neuron in response to <italic>M</italic> =3 constant stimulus features (see Methods). Similarly to previous work <sup><xref ref-type="bibr" rid="c37">37</xref></sup>, tuning curves of both E and I neurons were strongly heterogeneous (<xref rid="fig8" ref-type="fig">Fig. 8E</xref>). We tested if the selectivity differs across E and I cell types. We computed a selectivity index for each neuron as the stimulus-response gain (average change in the firing rate in response to a small change in the stimulus divided by the stimulus change size, see Methods), and found that E and I neurons had similar mean stimulus selectivity (<italic>p</italic> = 0.418, two-sample t-test; <xref rid="fig8" ref-type="fig">Fig. 8F</xref>). Thus, I neurons, despite not receiving direct feedforward inputs and acquiring stimulus selectivity only through structured E-I connections, are tuned to the input stimuli as strongly as the E neurons.</p>
</sec>
<sec id="s2i">
<title>Comparison of E-I and one cell type model architecture for coding efficiency and robustness</title>
<p>Neurons in the brain are either excitatory or inhibitory. To understand how differentiating E and I neurons benefits efficient coding, we compared the properties of our efficient E-I network with an efficient network with a single cell type (1CT). The 1CT model can be seen as a simplification of the E-I model (see Supplementary Text 1) and has been derived and analyzed in previous studies <sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c92">92</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>. We compared the average encoding error (RMSE), the average metabolic cost (MC), and the average loss (see Supplementary Text 3) of the E-I model against the one cell type (1CT) model. Compared to the 1CT model, the E-I model exhibited a higher encoding error and metabolic cost in the E population, but a lower encoding error and metabolic cost in the I population (<xref rid="fig8" ref-type="fig">Fig. 8G</xref>). The 1CT model can perform similar computations as the E-I network. Instead of an E neuron directly providing lateral inhibition to its neighbor (<xref rid="figS1" ref-type="fig">Supplementary Fig. S1A-C</xref>), it goes through an interneuron in the E-I model (<xref rid="fig1" ref-type="fig">Fig. 1A(i)</xref> and <xref rid="fig1" ref-type="fig">B</xref>). We compared quantitatively the efficiency of the two models and found that the 1CT model is slightly more efficient than the E-I model, consistently for different weighting of the error with the cost (<xref rid="fig8" ref-type="fig">Fig. 8H</xref>).</p>
<p>We further compared the robustness of firing rates to changes in the metabolic constant of the two models. Consistently with previous studies <sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>, firing rates in the 1CT model were highly sensitive to variations in the metabolic constant (<xref rid="fig8" ref-type="fig">Fig. 8I</xref>, note the logarithmic scale on the y-axis), with a superexponential growth of the firing rate with the inverse of the metabolic constant in regimes with metabolic cost lower than optimal. This is in contrast to the E-I model, whose firing rates exhibited lower sensitivity to the metabolic constant, and never exceeded physiological limits (<xref rid="fig6" ref-type="fig">Fig. 6C</xref>, top). Because our E-I model does not incorporate a saturating input-output function that constrains the range of firing as in <sup><xref ref-type="bibr" rid="c34">34</xref></sup>, the ability of the E-I model to maintain firing rates within biologically plausible limits emerges as a highly desirable dynamic property.</p>
<p>In summary, although the optimal E-I model is slightly less efficient than the optimal 1CT model, it does not enter into states of physiologically unrealistic firing rates when the metabolic constant is lower than the optimal one.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We analyzed the structural, dynamical and coding properties that emerge in networks of spiking neurons that implement efficient coding. We demonstrated that efficient recurrent E-I networks form highly accurate representations of stimulus features with biologically plausible parameters, biologically plausible neural dynamics, instantaneous E-I balance and like-to-like connectivity structure leading to lateral inhibition. The network can implement efficient coding with stimulus features varying over a wide range of timescales and when encoding even multiple such features. Here we discuss the implications of these findings.</p>
<p>By a systematic study of the model, we determined the model parameters that optimize network efficiency. The optimal parameters (including the ratio between the number of E and I neurons, the ratio of I-I to E-I synaptic efficacy and parameters of non-specific currents) were consistent with parameters measured empirically in cortical circuits, and generated plausible spiking dynamics. This result lends credibility to the hypothesis that cortical networks might be designed for efficient coding and may operate close to optimal efficiency, as well as provides a solid intuition about what specific parameter ranges (e.g. higher numbers of E than I neurons) may be good for. With moderate deviations from the optimal parameters, efficient networks still exhibited realistic dynamics and close-to-efficient coding, suggesting that the optimal operational point of such networks is relatively robust. We also found that optimally efficient analytical solution derives generalized LIF (gLIF) equations for neuron models<sup><xref ref-type="bibr" rid="c38">38</xref></sup>. While gLIF <sup><xref ref-type="bibr" rid="c82">82</xref>,<xref ref-type="bibr" rid="c41">41</xref></sup> and LIF <sup><xref ref-type="bibr" rid="c77">77</xref>,<xref ref-type="bibr" rid="c78">78</xref></sup> models are reasonably biologically plausible and are widely used to model and study spiking neural network dynamics, it was unclear how their parameters affect network-level information coding. Our study provides a principled way to determine uniquely the parameter values of gLIF networks that are optimal for efficient information encoding. Studying the dynamics of gLIF networks with such optimal parameters thus provides a direct link between optimal coding and neural dynamics. Moreover, our formalism provides a framework for the optimization of neural parameters that can in principles be used not only for neural network models that study brain function but also for the design of artificial neuromorphic circuits that perform information coding computations <sup><xref ref-type="bibr" rid="c93">93</xref>,<xref ref-type="bibr" rid="c94">94</xref></sup>.</p>
<p>Our model generates a number of insights about the role of structured connectivity in efficient information processing. A first insight is that I neurons develop stimulus feature selectivity because of the structured recurrent connectivity. This is in line with recent reports of stimulus feature selectivity of inhibitory neurons, including in primary visual cortex <sup><xref ref-type="bibr" rid="c95">95</xref>,<xref ref-type="bibr" rid="c96">96</xref>,<xref ref-type="bibr" rid="c97">97</xref></sup>. A second insight is that a network with structured connectivity shows stronger average and instantaneous E-I balance, as well as significantly lower variance of membrane potentials compared to an equivalent network with randomly organized connections. This implies that the connectivity structure is not only crucial for coding efficiency, but also influences the dynamical regime of the network. A third insight is that the structured network exhibits both lower encoding error and lower firing rates compared to unstructured networks, thus achieving higher efficiency. Our analysis of the effective connectivity created by the efficient connectivity structure shows that this structure sharpens stimulus representations, reduces redundancy and increases metabolic efficiency by implementing feature-specific competition, that is a negative effective connectivity between E neurons with similar stimulus tuning, as proposed by recent theories <sup><xref ref-type="bibr" rid="c30">30</xref></sup> and experiments <sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup> of computations in visual cortex.</p>
<p>Our model gives insights on what would be minimal requirements for a biological network to implement efficient coding. The network has to have structured E-I and I-E connectivity and weak and short-lasting or no spike-triggered adaptation. Further, at least half of the distance between the resting potential and the threshold should be provided by a stochastic external current that is unrelated to the feedforward stimuli. Finally, the network should have a ratio of E to I neuron numbers in the range of about 2:1 to 4:1 and the ratio of average I-I to E-I connectivity in the range of about 2:1 to 3:1, with smaller E-I neuron number ratios implying smaller average I-I to E-I connectivity ratios.</p>
<p>Our study gives insights into how structured connectivity between E and I neurons affects the dynamics of E-I balancing and how this relates to information coding. Previous work <sup><xref ref-type="bibr" rid="c32">32</xref></sup> proposed that the E-I balance in efficient spiking networks operates on a finer time scale than in classical balanced E-I networks with random connectivity <sup><xref ref-type="bibr" rid="c78">78</xref></sup>. However, theoretical attempts to determine the levels of instantaneous E-I balance that are optimal for coding are rare <sup><xref ref-type="bibr" rid="c98">98</xref></sup>. Consistent with the general idea put forth in <sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c53">53</xref></sup>, we here showed that moderate levels of E-I balance are optimal for coding, and that too strong levels of instantaneous E-I balance are detrimental to coding efficiency. Our results predict that structured E-I-E connectivity is necessary for optimal levels of instantaneous E-I balance. Finally, the E-I-E structured connectivity that we derived supports optimal levels of instantaneous E-I balance and causes desynchronization of the spiking output. Such intrinsically generated desynchronization is a desirable network property that in previously proposed models could only be achieved by the less plausible addition of strong noise to each neuron <sup><xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>.</p>
<p>Our result that network efficiency depends gently on the number of neurons is consistent with previous findings that demonstrated robustness of efficient networks to neuronal loss <sup><xref ref-type="bibr" rid="c37">37</xref></sup> and robustness of efficient spiking to the number of neurons <sup><xref ref-type="bibr" rid="c99">99</xref></sup>. Building on these studies, we additionally documented how the optimal ratio of the number of E to I neurons relates to the optimal ratio of average I-I to E-I connectivity. In particular, our analysis predicts that the optima of these two ratios are positively correlated. This might give insights into the diversity of ratios of E-I neuron number ratios observed across species <sup><xref ref-type="bibr" rid="c91">91</xref></sup>.</p>
<p>We found that our efficient network, optimizing the representation of a leaky integration of stimulus features, does not require recurrent E-E connections. This is compatible with the relatively sparse levels of recurrent E-E connections in primary visual cortex<sup><xref ref-type="bibr" rid="c100">100</xref></sup>, with the majority of E-E synapses suggested to be long-range <sup><xref ref-type="bibr" rid="c101">101</xref></sup>. Nevertheless, a limitation of our study is that it did not investigate the computations that could be made by E-E connections. Future studies could address the role of recurrent excitatory synapses that implement efficient coding computations beyond leaky integration, such as linear <sup><xref ref-type="bibr" rid="c38">38</xref></sup> or non-linear mixing of stimulus features <sup><xref ref-type="bibr" rid="c92">92</xref></sup>. Investigating such networks would also allow addressing whether biologically plausible efficient networks exhibit criticality, as suggested by<sup><xref ref-type="bibr" rid="c102">102</xref></sup>.</p>
<p>A more realistic mapping of efficient coding onto biological networks would also entail including multiple types of inhibitory neurons <sup><xref ref-type="bibr" rid="c103">103</xref></sup>, which could provide additional insights into how interneuron diversity serves information coding. Further limitations of our study to be addressed in future work include a more realistic implementation of the feedforward current. In our implementation, the feedforward current is simply a sum of uncorrelated stimulus features. However, in biological circuits, the feedforward input is a series of complex synaptic inputs from upstream circuits. A more detailed implementation of feedforward inputs, coupled with recurrent E-E synapses, might influence the levels of the instantaneous balance, in particular in E neurons, and have an impact on network efficiency. Moreover, we here did not explore cases where the same stimulus feature has multiple time scales. Finally, we note that efficient encoding might be the primary normative objective in sensory areas, while areas supporting high-level cognitive tasks might include other computational objectives, such as efficient transmission of information downstream to generate reliable behavioral outputs <sup><xref ref-type="bibr" rid="c104">104</xref>,<xref ref-type="bibr" rid="c105">105</xref>,<xref ref-type="bibr" rid="c106">106</xref>,<xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c107">107</xref></sup>. It would thus be important to understand how networks could simultaneously optimize or trade off different objectives.</p>
</sec>
</body>
<back>
<sec id="s5">
<title>Methods</title>
<sec id="s5a">
<title>Overview of the current approach and of differences with previous approaches</title>
<p>In the following, we present a detailed derivation of the E-I spiking network implementing the efficient coding principle. The analytical derivation is based on previous works on efficient coding with spikes <sup><xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup>, and in particular on our recent work <sup><xref ref-type="bibr" rid="c38">38</xref></sup>. While these previous works analytically derived feedforward and recurrent transmembrane currents in leaky integrate-and fire neuron models, they did not contain any synaptic currents unrelated to feedforward and recurrent processing. Non-specific synaptic currents were suggested to be important for an accurate description of coding and dynamics in cortical networks <sup><xref ref-type="bibr" rid="c62">62</xref></sup>. In the model derivation that follows, we also derived non-specific external current from efficiency objectives.</p>
<p>Moreover, we here revisited the derivation of physical units in efficient spiking networks. We built on a previous work <sup><xref ref-type="bibr" rid="c28">28</xref></sup> that assigned physical units to mathematical expressions that correspond to membrane potentials, firing thresholds, etc. Here, we instead assigned physical units to the computational variables such as the target signals and the population readouts, and then derived units of the membrane potentials and firing thresholds.</p>
<p>With this model, we aim to describe neural dynamics and computation in early sensory cortices such as the primary visual cortex in rodents, even though many principles of the model developed here could be relevant throughout the brain.</p>
</sec>
<sec id="s5b">
<title>Introducing variables of the model</title>
<p>We consider two types of neurons, excitatory neurons <italic>E</italic> and inhibitory neurons <italic>I</italic>. We denote as <italic>N</italic> <sup><italic>E</italic></sup> and <italic>N</italic> <sup><italic>I</italic></sup> the number of <italic>E</italic>-cells and <italic>I</italic>-cells, respectively. The spike train of neuron <italic>i</italic> of type <italic>y</italic> ∈ {<italic>E, I</italic>}, <italic>i</italic> = 1, 2, …, <italic>N</italic> <sup><italic>y</italic></sup>, is defined as a sum of Dirac delta functions,
<disp-formula id="eqn6">
<graphic xlink:href="590955v2_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v2_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the time of the <italic>α</italic>-th spike of that neuron, defined as a time point at which the membrane potential of neuron <italic>i</italic> crosses the firing threshold.</p>
<p>We define the readout of the spiking activity of neuron <italic>i</italic> of type <italic>y</italic> (in the following, “single neuron readout”) as a leaky integration of its spike train,
<disp-formula id="eqn7">
<graphic xlink:href="590955v2_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>λ</italic> denoting the inverse time constant. This way, the quantity <inline-formula><inline-graphic xlink:href="590955v2_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula> represents an estimate of the instantaneous firing rate of neuron <italic>i</italic>.</p>
<p>We denote as <bold><italic>s</italic></bold>(<italic>t</italic>) := [<italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub><italic>M</italic></sub> (<italic>t</italic>)]<sup>⊤</sup> the set of <italic>M</italic> dynamical features of the external stimulus (in the following, stimulus features) which are transmitted to the network through a feedforward sensory pathway. The stimulus features have the unit of the square root of millivolt, (<italic>mV</italic>)<inline-formula><inline-graphic xlink:href="590955v2_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The target signal is then obtained through a leaky integration of the feedforward variable, <bold><italic>s</italic></bold>(<italic>t</italic>)<sup><xref ref-type="bibr" rid="c29">29</xref></sup>, with inverse time constant <italic>λ</italic>, as
<disp-formula id="eqn8">
<graphic xlink:href="590955v2_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <bold><italic>x</italic></bold>(<italic>t</italic>) := [<italic>x</italic><sub>1</sub>(<italic>t</italic>), …, <italic>x</italic><sub><italic>M</italic></sub> (<italic>t</italic>)]<sup>⊤</sup> the vector of <italic>M</italic> target signals. Furthermore, we define a linear population readout of the spiking activity of E and I neurons
<disp-formula id="eqn9">
<graphic xlink:href="590955v2_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v2_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the vector of estimates of cell type <italic>y</italic> and <inline-formula><inline-graphic xlink:href="590955v2_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula>in units of <inline-formula><inline-graphic xlink:href="590955v2_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Here, each neuron <italic>i</italic> of type <italic>y</italic> is associated with a vector <inline-formula><inline-graphic xlink:href="590955v2_inline42.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of <italic>M</italic> tuning parameters representing the decoding weight of neuron <italic>i</italic> with respect to the <italic>M</italic> population readouts in <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref>. These decoding vectors can be combined in the <italic>M</italic> × <italic>N</italic> <sup><italic>y</italic></sup> matrix <inline-formula><inline-graphic xlink:href="590955v2_inline43.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The rows of this matrix define the patterns of decoding weights <inline-formula><inline-graphic xlink:href="590955v2_inline44.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for each signal dimension <italic>k</italic> = 1, …, <italic>M</italic>.</p>
</sec>
<sec id="s5c">
<title>Loss functions</title>
<p>We assume that the activity of a population <italic>y</italic> ∈ {<italic>E, I</italic>} is set so as to minimize a time-dependent encoding error and a time-dependent metabolic cost:
<disp-formula id="eqn10">
<graphic xlink:href="590955v2_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>β</italic><sup><italic>y</italic></sup> <italic>&gt;</italic> 0 in units of mV the Lagrange multiplier which controls the weight of the metabolic cost relative to the encoding error. The time-dependent encoding error is defined as the squared distance between the targets and their estimates, and the role of estimates is assigned to the population readouts <inline-formula><inline-graphic xlink:href="590955v2_inline45.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In E neurons, the targets are defined as the target signals <bold><italic>x</italic></bold>(<italic>t</italic>), and their estimators are the population readouts of the spiking activity of E neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline46.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In I neurons, the targets are defined as the population readouts of E neurons <inline-formula><inline-graphic xlink:href="590955v2_inline47.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and their estimators are the population readouts of I neurons <inline-formula><inline-graphic xlink:href="590955v2_inline48.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Furthermore, the time-dependent metabolic cost is proportional to the squared estimate of the instantaneous firing rate, summed across neurons from the same population. Following these assumptions, we define the variables of loss functions in <xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref> as
<disp-formula id="eqn11">
<graphic xlink:href="590955v2_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>We use a quadratic metabolic cost because it promotes the distribution of spiking across neurons <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. In particular, the loss function of I neurons, L<sup><italic>I</italic></sup>(<italic>t</italic>) implies the relevance of the approximation: <inline-formula><inline-graphic xlink:href="590955v2_inline49.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (see <italic>ϵ</italic><sup><italic>I</italic></sup> in the <xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>), which will be used in what follows.</p>
</sec>
<sec id="s5d">
<title>When shall a neuron spike?</title>
<p>We minimize the loss function by positing that neuron <italic>i</italic> of type <italic>y</italic> ∈ {<italic>E, I</italic>} emits a spike as soon as its spike decreases the loss function of its population <italic>y</italic> in the immediate future<sup><xref ref-type="bibr" rid="c38">38</xref></sup>. We also define <italic>t</italic><sup>−</sup> and <italic>t</italic><sup>+</sup> as the left- and right-sided limits of a spike time <inline-formula><inline-graphic xlink:href="590955v2_inline50.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, respectively. Thus, at the spike time, the following jump condition must hold:
<disp-formula id="eqn12">
<graphic xlink:href="590955v2_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v2_inline51.gif" mime-subtype="gif" mimetype="image"/></inline-formula>in units of mV. Here, the arguments <italic>t</italic><sup>−</sup> and <italic>t</italic><sup>+</sup> denote the left- and right-sided limits of the respected functions at time <italic>t</italic>. Furthermore, we added a noise term on the right-hand side of the <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> in order to consider the stochastic nature of spike generation in biological networks <sup><xref ref-type="bibr" rid="c54">54</xref></sup>. A convenient choice for the noise <inline-formula><inline-graphic xlink:href="590955v2_inline52.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the Ornstein-Uhlenbeck process obeying
<disp-formula id="eqn13">
<graphic xlink:href="590955v2_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v2_inline53.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is a Gaussian white noise with auto-covariance function ⟨<italic>η</italic><sub><italic>i</italic></sub>(<italic>t</italic>)<italic>η</italic><sub><italic>j</italic></sub>(<italic>t</italic><sup><italic>′</italic></sup>)⟩ = <italic>δ</italic><sub><italic>ij</italic></sub><italic>δ</italic>(<italic>t</italic> − <italic>t</italic><sup><italic>′</italic></sup>). The process <inline-formula><inline-graphic xlink:href="590955v2_inline54.gif" mime-subtype="gif" mimetype="image"/></inline-formula> has zero mean and auto-covariance function <inline-formula><inline-graphic xlink:href="590955v2_inline55.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="590955v2_inline56.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the variance of the noise.</p>
<p>By applying the condition for spiking in <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> using <italic>y</italic> = <italic>E</italic> and <italic>y</italic> = <italic>I</italic>, respectively, we get
<disp-formula id="eqn14">
<graphic xlink:href="590955v2_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>According to the definitions in Eqs. (7) and (9), if neuron <italic>i</italic> fires a spike at time <inline-formula><inline-graphic xlink:href="590955v2_inline57.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, it causes a jump of its own filtered spike train (but not of other neurons <italic>j</italic>≠<italic>i</italic>), as well as of the population readout of the population it belongs to. Therefore, when neuron <italic>i</italic> fires a spike, we have for a given neuron <italic>j</italic> and a given population readout <italic>k</italic>:
<disp-formula id="eqn15a">
<graphic xlink:href="590955v2_eqn15a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn15b">
<graphic xlink:href="590955v2_eqn15b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>By inserting <xref ref-type="disp-formula" rid="eqn15a">Eq. (15a)</xref>-<xref ref-type="disp-formula" rid="eqn15b">(15b)</xref> in <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref>, we find that neuron <italic>i</italic> of type <italic>y</italic> should fire a spike if the following condition holds:
<disp-formula id="eqn16a">
<graphic xlink:href="590955v2_eqn16a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v2_inline58.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the squared length of the tuning vector of neuron <italic>i</italic> of type <italic>y</italic>. These equations tell us when the neuron <italic>i</italic> of type <italic>E</italic> and <italic>I</italic>, respectively, emits a spike, and are similar to the ones derived in previous works <sup><xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>. In addition to what has been found in these previous works, we here also find that each term on the left- and right-hand side in the <xref ref-type="disp-formula" rid="eqn16a">Eq 16a</xref> has the physical units of millivolts.</p>
<p>We note that the expression derived from the minimization of the loss function of E neurons in the top row of <xref ref-type="disp-formula" rid="eqn16a">Eq. (16a)</xref> is independent of the activity of I neurons, and would thus lead to the E population being unconnected with the I population. In order to derive a recurrently connected E-I network, the activity of E neurons must depend on the activity of I neurons. We impose this property by using the approximation of estimates that holds under the assumption of efficient coding in I neurons (see <italic>ϵ</italic><sup><italic>I</italic></sup> in the <xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>), <inline-formula><inline-graphic xlink:href="590955v2_inline59.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This yields the following conditions:
<disp-formula id="eqn16b">
<graphic xlink:href="590955v2_eqn16b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>We now define new variables <inline-formula><inline-graphic xlink:href="590955v2_inline60.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline61.gif" mime-subtype="gif" mimetype="image"/></inline-formula>as proportional to the left- and the right-hand side of these expressions,
<disp-formula id="eqn17">
<graphic xlink:href="590955v2_eqn17.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>The variables <inline-formula><inline-graphic xlink:href="590955v2_inline62.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline63.gif" mime-subtype="gif" mimetype="image"/></inline-formula>are interpreted as the membrane potential and the firing threshold of neuron <italic>i</italic> of cell type <italic>y</italic> ∈ {<italic>E, I</italic>}.</p>
</sec>
<sec id="s5e">
<title>Dynamic equations for the membrane potentials</title>
<p>In this section we develop the exact dynamic equations of the membrane potentials <inline-formula><inline-graphic xlink:href="590955v2_inline64.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>y</italic> ∈ {<italic>E, I</italic>} according to the efficient coding assumption. We rewrite <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> in vector notation as
<disp-formula id="eqn18">
<graphic xlink:href="590955v2_eqn18.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v2_inline65.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the vector of spike trains for <italic>N</italic> <sup><italic>y</italic></sup> neurons of cell type <italic>y</italic> ∈ {<italic>E, I</italic>}.</p>
<p>In the case of E neurons, the time-derivative of the membrane potential <inline-formula><inline-graphic xlink:href="590955v2_inline66.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref>, is obtained as
<disp-formula id="eqn19">
<graphic xlink:href="590955v2_eqn19.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>By inserting the dynamic equations of the target signal <inline-formula><inline-graphic xlink:href="590955v2_inline67.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, its estimate <inline-formula><inline-graphic xlink:href="590955v2_inline68.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn18">Eq. 18</xref>) and of the single neuron readout <inline-formula><inline-graphic xlink:href="590955v2_inline69.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn7">Eq. 7</xref> in the case <italic>y</italic> = <italic>E</italic>), we get
<disp-formula id="eqn20">
<graphic xlink:href="590955v2_eqn20.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where in the last line we used the definition of <inline-formula><inline-graphic xlink:href="590955v2_inline69a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from the <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref>.</p>
<p>In the case of I neurons, the time derivative of the membrane potential <inline-formula><inline-graphic xlink:href="590955v2_inline70.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref> is
<disp-formula id="eqn21">
<graphic xlink:href="590955v2_eqn21.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>By inserting the dynamic equations of the population readouts of E neurons <inline-formula><inline-graphic xlink:href="590955v2_inline71.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and of the I neurons <inline-formula><inline-graphic xlink:href="590955v2_inline72.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn18">Eq. 18</xref>) and of the single neuron readout <inline-formula><inline-graphic xlink:href="590955v2_inline73.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref ref-type="disp-formula" rid="eqn7">Eq. 7</xref> in the case <italic>y</italic> = <italic>I</italic>), we get
<disp-formula id="eqn22">
<graphic xlink:href="590955v2_eqn22.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where in the last line we used the definition of <inline-formula><inline-graphic xlink:href="590955v2_inline74.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref>.</p>
</sec>
<sec id="s5f">
<title>Leaky integrate-and-fire neurons</title>
<p>The terms on the right-hand-side in <xref ref-type="disp-formula" rid="eqn20">Eqs. (20)</xref> and <xref ref-type="disp-formula" rid="eqn22">(22)</xref> can be interpreted as transmembrane currents. The last term in these equations, <inline-formula><inline-graphic xlink:href="590955v2_inline75.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, can be interpreted as a current instantaneously resetting the membrane potential upon reaching the firing threshold<sup><xref ref-type="bibr" rid="c28">28</xref></sup>. Indeed, when the membrane potential reaches the threshold, it triggers a spike and causes a jump of the membrane potential by an amount −<italic>β</italic><sup><italic>y</italic></sup>; this realizes resetting of the membrane potential which is equivalent to the resetting rule of integrate-and-fire neurons <sup><xref ref-type="bibr" rid="c64">64</xref>,<xref ref-type="bibr" rid="c66">66</xref></sup>. Thus, by taking into account the resetting mechanism and defining the time constants of population and single neuron readout <italic>τ</italic> := <italic>λ</italic><sup>−1</sup> and <inline-formula><inline-graphic xlink:href="590955v2_inline76.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, we can rewrite <xref ref-type="disp-formula" rid="eqn20">Eqs. (20)</xref> and <xref ref-type="disp-formula" rid="eqn22">(22)</xref> as a leaky integrate-and-fire neuron model,
<disp-formula id="eqn23">
<graphic xlink:href="590955v2_eqn23.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>In the <xref ref-type="disp-formula" rid="eqn23">Eq. 23</xref> we wrote explicitly the terms <inline-formula><inline-graphic xlink:href="590955v2_inline77.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, which cor-respond to the synaptic projections of <italic>N</italic> <sup><italic>x</italic></sup> presynaptic neurons of type <italic>x</italic> to the postsynaptic neuron <italic>i</italic> of type <italic>y</italic>, with the quantity <inline-formula><inline-graphic xlink:href="590955v2_inline78.gif" mime-subtype="gif" mimetype="image"/></inline-formula> denoting the synaptic weight. We note that, in the case of I neurons, the element with <italic>j</italic> = <italic>i</italic> describes an autapse, i.e., a projection of a neuron with itself; this term is equal to <inline-formula><inline-graphic xlink:href="590955v2_inline79.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and thus contributes to the resetting of the neuron <italic>i</italic>.</p>
</sec>
<sec id="s5g">
<title>Imposing Dale’s principle on synaptic connectivity</title>
<p>We now examine the synaptic terms in <xref ref-type="disp-formula" rid="eqn23">Eq. (23)</xref>. As a first remark, we see that synaptic weights depend on tuning parameters <inline-formula><inline-graphic xlink:href="590955v2_inline79a.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For the sake of generality we drew tuning parameters <inline-formula><inline-graphic xlink:href="590955v2_inline80.gif" mime-subtype="gif" mimetype="image"/></inline-formula> from a normal distribution with vanishing mean, which yielded both positive and negative values of <inline-formula><inline-graphic xlink:href="590955v2_inline81.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This has the desirable consequence that a spike of a neuron with a positive tuning parameter in signal dimension <italic>k</italic>, <inline-formula><inline-graphic xlink:href="590955v2_inline82.gif" mime-subtype="gif" mimetype="image"/></inline-formula> pulls the estimate, <inline-formula><inline-graphic xlink:href="590955v2_inline83.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, up, while a spike of a neuron with <inline-formula><inline-graphic xlink:href="590955v2_inline84.gif" mime-subtype="gif" mimetype="image"/></inline-formula> pulls the estimate down, allowing population readouts to track both positive and negative fluctuations of the target signal on a fast time scale.</p>
<p>Another consequence of synaptic connectivity in the <xref ref-type="disp-formula" rid="eqn23">Eq. (23)</xref> is that the synaptic weight between a presynaptic neuron <italic>j</italic> of type <italic>x</italic> and a postsynaptic neuron <italic>i</italic> of type <italic>y</italic> is symmetric and depends on the similarity of tuning vectors of the presynaptic and the postsynaptic neuron: <inline-formula><inline-graphic xlink:href="590955v2_inline85.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The sign of this scalar product is positive between neurons with similar tuning and negative between neurons with different tuning (and zero when the two tuning vectors are orthogonal). Thus, for a presynaptic neuron <italic>j</italic> of type <italic>x</italic>, the synaptic weights of its outgoing connections can be both positive and negative, because some of its postsynaptic neurons have similar tuning to the neuron <italic>j</italic> while others have different tuning. This is inconsistent with Dale’s principle <sup><xref ref-type="bibr" rid="c108">108</xref></sup>, which postulates that a particular neuron can only have one type of effect on postsynaptic neurons (excitatory or inhibitory), but never both. To impose this constraint in our model, we set synaptic weights between neurons with different tuning (i.e., <inline-formula><inline-graphic xlink:href="590955v2_inline86.gif" mime-subtype="gif" mimetype="image"/></inline-formula>) to zero. To this end, we define the rectified connectivity matrices,
<disp-formula id="eqn24">
<graphic xlink:href="590955v2_eqn24.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with (<italic>x, y</italic>) ∈ {(<italic>E, I</italic>), (<italic>I, I</italic>), (<italic>I, E</italic>)} and [<italic>a</italic>]<sub>+</sub> ≡ max(0, <italic>a</italic>) a rectified linear function. Note that there are no direct synaptic connections between E neurons. Since the elements of the matrix <italic>J</italic><sup><italic>yx</italic></sup> are all non-negative, it is the sign in front of the synaptic term in the <xref ref-type="disp-formula" rid="eqn23">Eq. (23)</xref> that determines the sign of the synaptic current between neurons <italic>i</italic> and <italic>j</italic>. The synaptic current is excitatory if the sign is positive, and inhibitory if the sign is negative.</p>
<p>It is also interesting to note that rectification affects the rank of connectivity matrices. Without rectification, the product in <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref> yields a connectivity matrix with rank smaller or equal to the number of input features to the network, <italic>M</italic>, similarly as in previous works <sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c92">92</xref></sup>. Since typically the number of input features is much smaller than the number of neurons, i.e., <italic>M &lt;&lt; N</italic> <sup><italic>y</italic></sup>, this would give a low-rank connectivity matrix. However, rectification in <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref>, necessary to ensure Dale’s principle in presence of positive and negative tuning parameters, typically results in a substantial increase of the rank of the connectivity matrix.</p>
<p>Using the synaptic connectivity defined in <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref>, we rewrite the network dynamics from <xref ref-type="disp-formula" rid="eqn23">Eq. (23)</xref> as:
<disp-formula id="eqn25">
<graphic xlink:href="590955v2_eqn25.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>These equations express the neural dynamics which minimizes the loss functions (<xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref>) in terms of a generalized leaky integrate-and-fire model with E and I cell types, and are consistent with Dale’s principle.</p>
<p>In principle, it is possible to use the same strategy as for the E-I network to enforce Dale’s principle in model with one cell type (introduced by <sup><xref ref-type="bibr" rid="c28">28</xref></sup>). To do so, we constrained the recurrent connectivity of the model with a single cell type from <sup><xref ref-type="bibr" rid="c36">36</xref></sup> by keeping only connections between neurons with similar tuning vectors and setting other connections to 0 (see Supplementary Text 1). This led to a network of only inhibitory neurons, a type of network model which is less relevant for the description of biological networks.</p>
</sec>
<sec id="s5h">
<title>Model with resting potential and an external current</title>
<p>In the model given by the <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref> the resting potential is equal to zero. In order to account for biophysical values of the resting potential and to introduce an implementation of the metabolic constant that is consistent with neurobiology, we add a constant value to the dynamical equations of the membrane potentials <inline-formula><inline-graphic xlink:href="590955v2_inline87.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the firing thresholds <inline-formula><inline-graphic xlink:href="590955v2_inline88.gif" mime-subtype="gif" mimetype="image"/></inline-formula>and the reset potentials <inline-formula><inline-graphic xlink:href="590955v2_inline89.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This does not change the spiking dynamics of the model, as what matters to correctly infer the efficient spiking times of neurons is the distance between the membrane potential and the threshold.</p>
<p>Furthermore, in the same equations, the role of the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> as a biophysical quantity is questionable. The metabolic constant <italic>β</italic><sup><italic>y</italic></sup> is an important parameter that weights the metabolic cost over the encoding error in the objective functions (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>). On the level of computational objectives, the metabolic constant naturally controls firing rates, as it allows the network to fire more or less spikes to correct for a certain encoding error. A flexible control of the firing rates is a desirable property, as gives the possibility to potentially capture different dynamical regimes of efficient spiking networks <sup><xref ref-type="bibr" rid="c36">36</xref></sup>. In the spiking model we developed thus far (<xref ref-type="disp-formula" rid="eqn25">Eq. 25</xref>), similarly to previous efficient spiking models <sup><xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup>, the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> controls the firing threshold. In neurobiology, however, strong changes to the firing threshold that would reflect metabolic constraints of the network are not plausible. We thus searched for an implementation of the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> that is consistent with neurobiology.</p>
<p>The condition for threshold crossing of the neuron <italic>i</italic> can be written by <xref ref-type="disp-formula" rid="eqn25">Eq. (25)</xref> as
<disp-formula id="eqn26">
<graphic xlink:href="590955v2_eqn26.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>c</italic> an arbitrary constant in units of millivolts. In <xref ref-type="disp-formula" rid="eqn26">Eq. (26)</xref> we added a constant <italic>c/</italic>2 and a resting potential <inline-formula><inline-graphic xlink:href="590955v2_inline90.gif" mime-subtype="gif" mimetype="image"/></inline-formula> on the left- and right-hand side of the firing rule. Moreover, we shifted the noise and the dependency on the parameter <italic>β</italic> from the firing threshold to the membrane potential. Thus, we assumed that the firing threshold is independent of the metabolic constant and the noise, and we instead assumed the dependence on the metabolic constant and noise in the membrane potentials.</p>
<p>We now define new variables for <italic>y</italic> ∈ {<italic>E, I</italic>}:
<disp-formula id="eqn27">
<graphic xlink:href="590955v2_eqn27.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and rewrite the model in <xref ref-type="disp-formula" rid="eqn25">Eq. 25</xref> in these new variables
<disp-formula id="eqn28">
<graphic xlink:href="590955v2_eqn28.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="590955v2_inline91.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline92.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are the independent Gaussian white noise processes defined in <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref> above. We note that all terms on the right-hand side of <xref ref-type="disp-formula" rid="eqn28">Eq. (28)</xref> have the desired units of mV. The model in <xref ref-type="disp-formula" rid="eqn28">Eq. (28)</xref> is an efficient E-I spiking network with improved compatibility with neurobiology. We have expressed two new terms in the membrane potentials of E and I neurons, one dependent on the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> and one on the noise that we assumed in the condition for spiking (see <xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref>). We will group these two terms to define an external current, a current that is well known in spiking models of neural dynamics <sup><xref ref-type="bibr" rid="c41">41</xref></sup>.</p>
</sec>
<sec id="s5i">
<title>Efficient generalized leaky integrate-and-fire neuron model</title>
<p>Finally, we rewrite the model from <xref ref-type="disp-formula" rid="eqn28">Eq. (28)</xref> in a compact form in terms of transmembrane currents, and discuss their biological interpretation. The efficient coding with spikes is realized by the following model for the neuron <italic>i</italic> of type <italic>y</italic> ∈ {<italic>E, I</italic>}:
<disp-formula id="eqn29a">
<graphic xlink:href="590955v2_eqn29a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>R</italic><sub><italic>m</italic></sub> the current resistance. The leak current,
<disp-formula id="eqn29b">
<graphic xlink:href="590955v2_eqn29b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>τ</italic> = <italic>R</italic><sub><italic>m</italic></sub><italic>C</italic><sub><italic>m</italic></sub> and <italic>C</italic><sub><italic>m</italic></sub> the capacitance of the neural membrane <sup><xref ref-type="bibr" rid="c64">64</xref></sup>, arose by assuming the same time constant for the target signals <bold><italic>x</italic></bold>(<italic>t</italic>) and estimates <inline-formula><inline-graphic xlink:href="590955v2_inline93.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline94.gif" mime-subtype="gif" mimetype="image"/></inline-formula>(see <xref ref-type="disp-formula" rid="eqn8">Eqs. 8</xref> and <xref ref-type="disp-formula" rid="eqn18">18</xref>). We see that the passive membrane time constant <italic>τ</italic> = <italic>λ</italic><sup>−1</sup> can be traced back to the time constant of the population read-out in <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref>. The synaptic currents are defined as
<disp-formula id="eqn29c">
<graphic xlink:href="590955v2_eqn29c.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we note the presence of a feedforward current to E neurons,
<disp-formula id="eqn29d">
<graphic xlink:href="590955v2_eqn29d.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
which consist in a linear combination of the stimulus features <bold><italic>s</italic></bold>(<italic>t</italic>) weighted by the decoding weights <inline-formula><inline-graphic xlink:href="590955v2_inline95.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The stimulus features can be traced back to the definition of the target signals in <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>. This current emerges in E neurons, as a consequence of having the target signal <bold><italic>x</italic></bold>(<italic>t</italic>) in the loss function of the E population (see <xref ref-type="disp-formula" rid="eqn10">Eqs. 10</xref>-<xref ref-type="disp-formula" rid="eqn11">11</xref>). I neurons do not receive the feedforward current because their loss function does not contain the target signal.</p>
<p>The current providing within-neuron feedback triggered by each spike,
<disp-formula id="eqn29e">
<graphic xlink:href="590955v2_eqn29e.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
was recently recovered <sup><xref ref-type="bibr" rid="c38">38</xref></sup>. This current has the kinetics of the single neuron readout <inline-formula><inline-graphic xlink:href="590955v2_inline96.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (i.e., low-pass filtered spike train). Its sign depends on the relation between the time constant of the population readout <italic>τ</italic> = <italic>λ</italic><sup>−1</sup> and single neuron readout <inline-formula><inline-graphic xlink:href="590955v2_inline97.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, because the metabolic constant <italic>β</italic><sup><italic>y</italic></sup> is non-negative by definition (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>). If the single neuron readout is slower than the population readout, <inline-formula><inline-graphic xlink:href="590955v2_inline98.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, within-neuron feedback is negative, and can thus be interpreted as spike-triggered <italic>adaptation</italic>. On the contrary, if the single neuron readout is faster than the population readout, <inline-formula><inline-graphic xlink:href="590955v2_inline99.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the within-neuron feedback is positive and can thus be interpreted as spike-triggered <italic>facilitation</italic>. In a special case where the time constant of the single neuron and population readout are assumed to be equal, within-neuron feedback vanishes.</p>
<p>Finally, we here derived the non-specific external current:
<disp-formula id="eqn29f">
<graphic xlink:href="590955v2_eqn29f.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
that captures the ensemble of non-specific synaptic currents received by each single neuron. The non-specific current has a homogeneous mean across all neurons of the same cell type, and a neuron-specific fluctuation. The mean of the non-specific current can be traced back to the weighting of the metabolic cost over the encoding error in model objectives (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>), while the fluctuation can be traced back to the noise strength that we assumed in the condition for spiking (<xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref>). The non-specific external current might arise because of synaptic inputs from other brain areas than the brain area that delivers feedforward projections to the E-I network we consider here, or it might result from synaptic activity of neurons that are part of the local network, but are not tuned to the feedforward input <sup><xref ref-type="bibr" rid="c85">85</xref></sup>.</p>
<p>We also recall the fast and slower time scales of single neuron activity:
<disp-formula id="eqn29g">
<graphic xlink:href="590955v2_eqn29g.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
and the connectivity matrices
<disp-formula id="eqn29h">
<graphic xlink:href="590955v2_eqn29h.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>The structure of synaptic connectivity is fully determined by the similarity of tuning vectors of the presynaptic and the postsynaptic neurons (<inline-formula><inline-graphic xlink:href="590955v2_inline100.gif" mime-subtype="gif" mimetype="image"/></inline-formula>and <inline-formula><inline-graphic xlink:href="590955v2_inline101.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, respectively), while the distribution of synaptic connectivity weights is fully determined by the distribution of tuning parameters <inline-formula><inline-graphic xlink:href="590955v2_inline102.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
</sec>
<sec id="s5j">
<title>Stimulus features</title>
<p>We define stimulus features <bold><italic>s</italic></bold>(<italic>t</italic>) as a set of <italic>k</italic> = 1, …, <italic>M</italic> independent Ornstein-Uhlenbeck processes with vanishing mean, standard deviation <italic>σ</italic><sup><italic>s</italic></sup> and the correlation time <inline-formula><inline-graphic xlink:href="590955v2_inline103.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,
<disp-formula id="eqn30">
<graphic xlink:href="590955v2_eqn30.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>If not mentioned otherwise, we use the following parameters, identical across stimulus features: <italic>σ</italic><sup><italic>s</italic></sup> = 2 (mV)<sup>1<italic>/</italic>2</sup> and <inline-formula><inline-graphic xlink:href="590955v2_inline104.gif" mime-subtype="gif" mimetype="image"/></inline-formula> ms. Variables <italic>η</italic><sub><italic>k</italic></sub>(<italic>t</italic>) are independent Gaussian white noise processes with zero mean and covariance function ⟨<italic>η</italic><sub><italic>k</italic></sub>(<italic>t</italic>)<italic>η</italic><sub><italic>l</italic></sub>(<italic>t</italic><sup><italic>′</italic></sup>)⟩ = <italic>δ</italic><sub><italic>kl</italic></sub><italic>δ</italic>(<italic>t</italic> − <italic>t</italic><sup><italic>′</italic></sup>). These variables should not be confused with the Gaussian white noises <inline-formula><inline-graphic xlink:href="590955v2_inline105.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn28">Eq. (28)</xref>.</p>
</sec>
<sec id="s5k">
<title>Parametrization of synaptic connectivity</title>
<p>In the efficient E-I model, synaptic weights <inline-formula><inline-graphic xlink:href="590955v2_inline106.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are parametrized by tuning parameters <inline-formula><inline-graphic xlink:href="590955v2_inline107.gif" mime-subtype="gif" mimetype="image"/></inline-formula> through <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref>. The total number of synapses in the E-I, I-I and I-E connectivity matrices (including silent synapses with zero synaptic weight) is <italic>n</italic><sub>syn</sub> = 2<italic>N</italic> <sup><italic>E</italic></sup><italic>N</italic> <sup><italic>I</italic></sup> + (<italic>N</italic> <sup><italic>I</italic></sup>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, while the number of tuning parameters is <italic>n</italic><sub><italic>w</italic></sub> = <italic>M</italic> (<italic>N</italic> <sup><italic>E</italic></sup> + <italic>N</italic> <sup><italic>I</italic></sup>). Because the number of stimulus features <italic>M</italic> is expected to be much smaller than the number of E or I neurons, the number of tuning parameters <italic>n</italic><sub><italic>w</italic></sub> is much smaller than the number of synapses <italic>n</italic><sub>syn</sub>.</p>
<p>We can achieve a further substantial decrease in the number of free parameters by using a parametric distribution of tuning parameters <inline-formula><inline-graphic xlink:href="590955v2_inline108.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We have set the tuning parameters following a normal distribution and found that excellent performance can be achieved with random draws of tuning parameters from the normal distribution, thus without searching for a specific set of tuning parameters. This drastically decreased the number of free parameters relative to synaptic weights to only a handful of parameters that determine the distributions of tuning parameters.</p>
<p>Given <italic>M</italic> features, we sample tuning parameters, <inline-formula><inline-graphic xlink:href="590955v2_inline109.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, with <italic>i</italic> = 1, …, <italic>N</italic> <sup><italic>y</italic></sup>, <italic>y</italic> ∈ {<italic>E, I</italic>}, as random points uniformly distributed on a <italic>M</italic> -dimensional sphere of radius <inline-formula><inline-graphic xlink:href="590955v2_inline110.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We obtain this by sampling, for each neuron, a vector of <italic>M</italic> i.i.d. standard Gaussian random variables,<inline-formula><inline-graphic xlink:href="590955v2_inline111.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="590955v2_inline112.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and normalizing the vector such as to have length equal to <inline-formula><inline-graphic xlink:href="590955v2_inline113.gif" mime-subtype="gif" mimetype="image"/></inline-formula> <sup><xref ref-type="bibr" rid="c109">109</xref></sup>,
<disp-formula id="eqn31">
<graphic xlink:href="590955v2_eqn31.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>This ensures that the length of tuning vectors <inline-formula><inline-graphic xlink:href="590955v2_inline114.gif" mime-subtype="gif" mimetype="image"/></inline-formula> in <xref ref-type="disp-formula" rid="eqn31">Eq. (31)</xref> is homogeneous across neurons of the same cell type, i.e., <inline-formula><inline-graphic xlink:href="590955v2_inline115.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Parameters <inline-formula><inline-graphic xlink:href="590955v2_inline115a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline115b.gif" mime-subtype="gif" mimetype="image"/></inline-formula> determine the heterogeneity (spread) of tuning parameters.</p>
<p>By combining <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref> and <xref ref-type="disp-formula" rid="eqn31">Eq. (31)</xref>, we obtain the synaptic weights, <inline-formula><inline-graphic xlink:href="590955v2_inline116.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, as a function of the angle,<inline-formula><inline-graphic xlink:href="590955v2_inline117.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, between the tuning vectors of presynaptic neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline118.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and postsynaptic neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline119.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,
<disp-formula id="eqn32">
<graphic xlink:href="590955v2_eqn32.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>In the <italic>M</italic> = 3 dimensional case, we have that the distribution of the angle between two vectors is <inline-formula><inline-graphic xlink:href="590955v2_inline120.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="590955v2_inline121.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Thus, the average strength of synaptic weights between the pre- and the postsynaptic population can be calculated as
<disp-formula id="eqn33">
<graphic xlink:href="590955v2_eqn33.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>Thus, the upper bound for the synaptic weight between cell types <italic>x</italic> and <italic>y</italic> is simply
<disp-formula id="eqn34">
<graphic xlink:href="590955v2_eqn34.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>From the <xref ref-type="disp-formula" rid="eqn33">Eq. (33)</xref>, we have that the mean E-I connectivity is equal to the mean I-E connectivity, <inline-formula><inline-graphic xlink:href="590955v2_inline122.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. As we consider the ratio of the mean connectivity between I-I and E-I connections, we find that it is given by the following:
<disp-formula id="eqn35">
<graphic xlink:href="590955v2_eqn35.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
</sec>
<sec id="s5l">
<title>Performance measures</title>
<sec id="s5l1">
<title>Average encoding error and average metabolic cost</title>
<p>The definition of the time-dependent loss functions (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>) induces a natural choice for the performance measure: the mean squared error (MSE) between the targets and their estimators for each cell type. In the case of the E population, the time-dependent encoding error is captured by the variable <italic>ϵ</italic><sup><italic>E</italic></sup>(<italic>t</italic>) in the <xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref> and in case of I population it is captured by <italic>ϵ</italic><sup><italic>I</italic></sup>(<italic>t</italic>) defined in the same equation. We used the root MSE (RMSE), a standard measure for the performance of an estimator <sup><xref ref-type="bibr" rid="c41">41</xref></sup>. For the cell type <italic>y</italic> ∈ {<italic>E, I</italic>} in trial <italic>q</italic>, the RMSE is measured as
<disp-formula id="eqn36">
<graphic xlink:href="590955v2_eqn36.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with ⟨<italic>z</italic><sub><italic>q</italic></sub>(<italic>t</italic>)⟩<sub><italic>t,q</italic></sub> denoting the time- and trial-average.</p>
<p>Following the definition of the time-dependent metabolic cost in the loss functions (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>), we measured the average metabolic cost in a trial <italic>q</italic> for the cell type <italic>y</italic> ∈ {<italic>E, I</italic>} as
<disp-formula id="eqn37">
<graphic xlink:href="590955v2_eqn37.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with time-dependent metabolic cost <italic>κ</italic><sup><italic>y</italic></sup>(<italic>t</italic>) as in model’s objectives (<xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>) and ⟨<italic>z</italic><sub><italic>q</italic></sub>(<italic>t</italic>)⟩<sub><italic>t,q</italic></sub> the time- and trial-average. The square root was taken to have the same scale as for the RMSE (see <xref ref-type="disp-formula" rid="eqn36">Eq. 36</xref>).</p>
</sec>
<sec id="s5l2">
<title>The bias of the estimator</title>
<p>The MSE can be decomposed into the bias and the variance of the estimator. The time-dependent bias of estimates <inline-formula><inline-graphic xlink:href="590955v2_inline123.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <italic>y</italic> ∈ {<italic>E, I</italic>}, were evaluated for each time point over <italic>q</italic> = 1, …, <italic>Q</italic> trials. The time-dependent bias in input dimension <italic>k</italic> = 1, …, <italic>M</italic> is defined as
<disp-formula id="eqn38a">
<graphic xlink:href="590955v2_eqn38a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with ⟨<italic>z</italic><sub><italic>q</italic></sub>(<italic>t</italic>)⟩<sub><italic>q</italic></sub> the trial-averaged realization at time <italic>t</italic>. To have an average measure of the encoding bias, we averaged the bias of estimators over time and over input dimensions:
<disp-formula id="eqn38b">
<graphic xlink:href="590955v2_eqn38b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>The averaging over time and input dimensions is justified because <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>) are independent realizations of the Ornstein-Uhlenbeck process (see Eq.30) with vanishing mean and with the same time constant, and variance across input dimensions.</p>
</sec>
<sec id="s5l3">
<title>Criterion for determining optimal model parameters</title>
<p>The equations of the E-I spiking network in <xref ref-type="disp-formula" rid="eqn29a">Eqs. 29a</xref>-<xref ref-type="disp-formula" rid="eqn29h">29h</xref> (Methods), derived from the instantaneous loss functions, give efficient coding solutions valid for any set of parameter values. However, to choose parameters values in simulated data in a principled way, we performed a numerical optimization of the performance function detailed below. Numerical optimization gave the set of optimal parameters listed in <xref ref-type="table" rid="tbl1">Table 1</xref>. When testing the efficient E-I model with simulations, we used the optimal parameters in <xref ref-type="table" rid="tbl1">Table 1</xref> and changed only the parameters plotted in the figure axes on a figure-by-figure basis.</p>
<p>To estimate the optimal set of parameters <italic>θ</italic> = <italic>θ</italic><sup>*</sup>, we performed a grid search on each parameter <italic>θ</italic><sub><italic>i</italic></sub> while keeping all other parameters fixed as specified in <xref ref-type="table" rid="tbl1">Table 1</xref>. While varying the parameters, we measured a weighted sum of the time- and trial-averaged encoding error and metabolic cost. For each cell type <italic>y</italic> ∈ {<italic>E, I</italic>}, we computed
<disp-formula id="eqn39a">
<graphic xlink:href="590955v2_eqn39a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with ⟨<italic>z</italic><sub><italic>q</italic></sub>(<italic>t</italic>)⟩<sub><italic>t,q</italic></sub> the average over time and over trials and with <italic>ϵ</italic><sup><italic>y</italic></sup>(<italic>t</italic>) and <italic>κ</italic><sup><italic>y</italic></sup>(<italic>t</italic>) as in model’s objectives (<xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref>), where <italic>g</italic><sub><italic>L</italic></sub> ∈ [0, 1] is a weighting factor.</p>
<p>To optimize the performance measure, we used a value of <italic>g</italic><sub><italic>L</italic></sub> = 0.7. The parameter <italic>g</italic><sub><italic>L</italic></sub> in the Eq. (39a) regulates the relative importance of the average encoding error over the average metabolic cost. Since the performance measure in Eq. (39a) is closely related to the average over time and trials of the instantaneous loss function (<xref ref-type="disp-formula" rid="eqn10">Eq. 10</xref>) where the parameter <italic>β</italic> regulates the relative weight of instantaneous encoding error over the metabolic cost, setting <italic>g</italic><sub><italic>L</italic></sub> is effectively achieved by setting <italic>β</italic>.</p>
<p>The optimal parameter set <italic>θ</italic> = <italic>θ</italic><sup>*</sup> reported in <xref ref-type="table" rid="tbl1">Table 1</xref> is the parameter set that minimizes the sum of losses across E and I cell type
<disp-formula id="eqn39b">
<graphic xlink:href="590955v2_eqn39b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>For visualization of the behavior of the average metabolic cost (<xref ref-type="disp-formula" rid="eqn37">Eq. 37</xref>) and average loss (<xref ref-type="disp-formula" rid="eqn39a">Eq. 39a</xref>) across a range of a specific parameter <italic>θ</italic><sub><italic>i</italic></sub>, we summed these measures across the E and I cell type and normalized them across the range of tested parameters.</p>
<p>The exact neural dynamics and performance of our model depends on the realizations of random variables which describe the the tuning parameters <inline-formula><inline-graphic xlink:href="590955v2_inline124.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the Gaussian noise in the non-specific currents <inline-formula><inline-graphic xlink:href="590955v2_inline125.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, and the initial conditions of the membrane potential <inline-formula><inline-graphic xlink:href="590955v2_inline126.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, that were ran-domly drawn from a normal distribution in each simulation trial. To capture the performance of a “typical” network, we iterated the performance measures across trials with different realizations of these random variables, and averaged the performance measures across trials. We typically used 100 simulation trials for each parameter value.</p>
</sec>
</sec>
<sec id="s5m">
<title>Functional activity measures</title>
<sec id="s6">
<title>Tuning similarity</title>
<p>The pairwise tuning similarity was measured as the cosine similarity <sup><xref ref-type="bibr" rid="c110">110</xref></sup>, defined as:
<disp-formula id="eqn40">
<graphic xlink:href="590955v2_eqn40.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <inline-formula><inline-graphic xlink:href="590955v2_inline127.gif" mime-subtype="gif" mimetype="image"/></inline-formula>the length of the tuning vector in Euclidean space and <italic>α</italic> the angle between the tuning vectors <inline-formula><inline-graphic xlink:href="590955v2_inline128.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline129.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<sec id="s7">
<title>Cross-correlograms of spike timing</title>
<p>The time-dependent coordination of spike timing was measured with the cross-correlogram (CCG) of spike trains, corrected for stimulus-driven coincident spiking. The raw cross-correlogram (CCG) for neuron <italic>i</italic> of cell type <italic>y</italic> and neuron <italic>j</italic> of cell type <italic>x</italic> was measured as follows:
<disp-formula id="eqn41a">
<graphic xlink:href="590955v2_eqn41a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>q</italic> = 1, …, <italic>Q</italic> simulation trials with identical stimulus and <italic>T</italic> the duration of the trial. We subtracted from the raw CCG the CCG of trial-invariant activity. To evaluate the trial-invariant cross-correlogram, we first computed the peri-stimulus time histogram (PSTH) for each neuron as follows:
<disp-formula id="eqn41b">
<graphic xlink:href="590955v2_eqn41b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>The trial-invariant CCG was then evaluated as the cross-correlation function of PSTHs between neurons <italic>i</italic> and <italic>j</italic>,
<disp-formula id="eqn41c">
<graphic xlink:href="590955v2_eqn41c.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>Finally, the temporal coordination of spike timing was computed by subtracting the correction term from the raw CCG:
<disp-formula id="eqn41d">
<graphic xlink:href="590955v2_eqn41d.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
</sec>
<sec id="s8">
<title>Average imbalance of synaptic inputs</title>
<p>We considered time and trial-averaged synaptic inputs to each E and I neuron <italic>i</italic> in trial <italic>q</italic>, evaluated as:
<disp-formula id="eqn42">
<graphic xlink:href="590955v2_eqn42.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with synaptic currents to E neurons <inline-formula><inline-graphic xlink:href="590955v2_inline130.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and to I neurons <inline-formula><inline-graphic xlink:href="590955v2_inline131.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as in Eq. (29c). Synaptic inputs were measured in units of mV. We reported trial-averages of the net synaptic inputs from the <xref ref-type="disp-formula" rid="eqn42">Eq. (42)</xref>.</p>
</sec>
<sec id="s9">
<title>Instantaneous balance of synaptic inputs</title>
<p>We measured the instantaneous balance of synaptic inputs as the Pearson correlation of time-dependent synaptic inputs incoming to the neuron <italic>i</italic>. For those synaptic inputs that are defined as weighted delta-spikes (for which the Pearson correlation is not well defined; see <xref ref-type="disp-formula" rid="eqn29c">Eq. 29c</xref>), we convolved spikes with a synaptic filter <inline-formula><inline-graphic xlink:href="590955v2_inline132.gif" mime-subtype="gif" mimetype="image"/></inline-formula>,
<disp-formula id="eqn43">
<graphic xlink:href="590955v2_eqn43.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where we used the expression for the feedforward synaptic current from the <xref ref-type="disp-formula" rid="eqn29d">Eq. (29d)</xref>. Note that the feedforward synaptic current is already already low-pass filtered (see <xref ref-type="disp-formula" rid="eqn30">Eq. 30</xref>). Using synaptic inputs from the <xref ref-type="disp-formula" rid="eqn43">Eq. 43</xref>, we computed the Pearson correlation of synaptic inputs in-coming to single E neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline133.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>i</italic> = 1, …, <italic>N</italic> <sup><italic>E</italic></sup>, and to single I neurons, <inline-formula><inline-graphic xlink:href="590955v2_inline134.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for <italic>i</italic> = 1, …, <italic>N</italic> <sup><italic>I</italic></sup>. The coefficients were then averaged across trials.</p>
</sec>
<sec id="s10">
<title>Tuning curves and selectivity index</title>
<p>The selectivity index of a neuron captures the change in neuron’s firing rate in response to a change in the stimulus. We first evaluated the tuning curve of each neuron by measuring the firing rate of the neuron <inline-formula><inline-graphic xlink:href="590955v2_inline135.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, as a function of the amplitude of the stimulus feature <italic>s</italic><sub>1</sub>. The firing rate was evaluated from the network response to <italic>M</italic> = 3 stimulus features that were constant over time. We varied the first stimulus feature <italic>s</italic><sub>1</sub> from strongly negative (<italic>s</italic><sub>1</sub> = −5) to strongly positive values (<italic>s</italic><sub>1</sub> = <italic>s</italic><sub>max</sub> = 5), while the two other features were kept at an intermediate positive value (<italic>s</italic><sub>2</sub> = <italic>s</italic><sub>3</sub> = 1.6). Note that with all three features at such intermediate value (<italic>s</italic><sub>1</sub> = <italic>s</italic><sub>2</sub> = <italic>s</italic><sub>3</sub> = 1.6), the average firing rate was about 8 Hz in E and 12 Hz in I neurons. To evaluate the tuning curve of a neuron, we measured its firing rate in 100 simulation trials of 1 second duration, for each value of the stimulus feature <italic>s</italic><sub>1</sub>.</p>
<p>To evaluate the sensitivity index, we normalized the tuning curve of the neuron with its maximal value,
<disp-formula id="eqn44">
<graphic xlink:href="590955v2_eqn44a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>We then computed the sensitivity index as the average absolute change of the normalized firing rate with the change in the stimulus:
<disp-formula id="eqn44b">
<graphic xlink:href="590955v2_eqn44b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
</sec>
<sec id="s11">
<title>Perturbation experiments</title>
<sec id="s11a">
<title>Perturbation of neural activity</title>
<p>Empirical studies <sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup> suggested experiments with perturbation of neural activity that estimate functional connectivity in recurrently connected neural networks. Here, we detail the procedure on how we performed similar experiments on simulated neural networks. To evaluate the functional connectivity between pairs of neurons, we measured the effect of activation of a single E neuron (“target” neuron) on the activity of other neurons. We stimulated a randomly chosen E neuron with a depolarizing input, capturing the effect of photostimulation in empirical studies <sup><xref ref-type="bibr" rid="c58">58</xref>,<xref ref-type="bibr" rid="c59">59</xref></sup>, and measured the deviation of the firing rate from the baseline in all other neurons.</p>
<p>The time-dependent deviation of the firing rate from the baseline for neuron <italic>i</italic> of type <italic>y</italic> ∈ {<italic>E, I</italic>} was computed as <inline-formula><inline-graphic xlink:href="590955v2_inline136.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the estimate of the instantaneous firing rate and <inline-formula><inline-graphic xlink:href="590955v2_inline137.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the average spontaneous firing rate of the neuron <italic>i</italic>. The target neuron received a constant depolarizing current during 50 ms and the effect of its activity on other neurons was measured during a time window of [0, 100] ms with respect to the onset of the stimulation. The functional connectivity between the target neuron and every other neuron in the network was then computed as the time average of the variable <inline-formula><inline-graphic xlink:href="590955v2_inline138.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. To isolate the functional effect of recurrent connections on firing rate changes, we performed these experiments in a network without external stimuli, setting <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>) = 0 ∀<italic>t, k</italic>.</p>
</sec>
<sec id="s11b">
<title>Removal of connectivity structure</title>
<p>To better understand the effect of optimally structured recurrent connectivity (as given by the <xref ref-type="disp-formula" rid="eqn24">Eq. 24</xref>) on network’s activity and efficiency, we compared networks with and without the connectivity structure. To fully remove the connectivity structure, we randomly permuted, without repetition, recurrent connectivity weights between all neuronal pairs of all the three recurrent connectivity matrices. This was achieved by shuffling entries within each recurrent connectivity matrix. This procedure preserves all properties of the distribution of connectivity weights and only removes the connectivity structure. Shuffling of connections was iterated across 200 simulation trials, with each trial implementing a different random permutation of the connectivity. Dale’s law is preserved by such manipulation.</p>
<p>To compare the performance of models with structured and unstructured connectivity (as reported on <xref rid="fig4" ref-type="fig">Fig. 4A</xref>), we collected the low-pass filtered spiking activity in networks with and without connectivity structure. We used this neural activity to train a linear decoder with least squares method that minimizes the Euclidean distance between target signals and a linear read-out of low-pass filtered spikes. The output of the training was a set of linear coefficients akin to decoding weights <inline-formula><inline-graphic xlink:href="590955v2_inline139.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We used these decoding weights estimated by the decoder to weight spikes in a held-out validation set. The performance was measured with root mean squared error (RMSE) between target signals and their estimates in the validation set. The training set comprised 70 % of trials (140 trials), and the validation test comprised the remaining 30 % of trials (60 trials).</p>
<p>To compare networks with and without connectivity structure about their metabolic cost, firing rate, variability of spiking and the E-I balance (<xref rid="fig4" ref-type="fig">Fig. 4B-G</xref>), we performed these measures in networks with and without connectivity structure and plotted their distributions across 200 simulation trials. For the comparison of the metabolic cost (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>), we additionally matched the network with and without the connectivity structure about their mean net synaptic input to E and I neurons, to see if the difference in the metabolic cost between structured and unstructured networks persists after such matching. For the comparison of the coefficient of variation in structured and unstructured networks (<xref rid="fig4" ref-type="fig">Fig. 4E</xref>), we used a constant stimulus instead of the OU stimulus, to exclude possible effects of a time-dependent variations of the stimulus on the variability of spiking. Constant stimulus was homogeneous across all stimulus dimensions, <italic>s</italic><sub><italic>k</italic></sub>(<italic>t</italic>) = 1.6, ∀<italic>k</italic> = 1, …, <italic>M</italic>. The amplitude of the constant stimulus was set such that the average firing rate in response to the constant stimulus matched the firing rate in response to the OU stimulus.</p>
<p>For the comparison of the voltage correlations and the effective connectivity between structured and unstructured networks (<xref rid="fig4" ref-type="fig">Fig. 4H-I</xref>), we additionally permuted individual connectivity (sub)matrices. This gave four cases, namely, permuted E-I, I-I, I-E, and “all”, with “all” meaning that all three recurrent connectivity matrices have been randomly permuted.</p>
<p>We also tested networks where the connectivity structure was not fully but only partially removed. There, we limited random permutation of synaptic weights to pairs of neurons that already had a connection in the structured network. By the <xref ref-type="disp-formula" rid="eqn24">Eq. 24</xref>, connected neurons are those with positive tuning similarity, i.e., neuronal pairs for which the following holds: <inline-formula><inline-graphic xlink:href="590955v2_inline140.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, with tuning similarity as in <xref ref-type="disp-formula" rid="eqn40">Eq. 40</xref>. We compared partially unstructured networks with structured networks by plotting measures of neural activity in structured and partially unstructured networks across 200 simulation trials (<xref rid="figS3" ref-type="fig">Fig. S3B-E</xref>).</p>
</sec>
<sec id="s11c">
<title>Perturbation of connectivity</title>
<p>To test the robustness of the model to random perturbations of synaptic weights (<xref rid="figS3" ref-type="fig">Fig. S3G-H</xref>), we applied a random jitter to optimally efficient recurrent synaptic connectivity weights. The random jitter was proportional to the optimal synaptic weight, <inline-formula><inline-graphic xlink:href="590955v2_inline141.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>σ</italic><sub><italic>J</italic></sub> is the strength of the perturbation and <inline-formula><inline-graphic xlink:href="590955v2_inline141a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> are independent standard normal random variables. All three recurrent connectivity matrices (E-I, I-I and I-E) were randomly perturbed at once.</p>
</sec>
<sec id="s11d">
<title>Computer simulations</title>
<p>We ran computer simulations with Matlab R2023b (Mathworks). The membrane equation for each neuron was integrated with Euler integration scheme with the time step of <italic>dt</italic> = 0.02 ms.</p>
<p>The simulation of the E-I network with 400 E units and 100 I units for an equivalent of 1 second of neural activity lasted approximately 1.65 seconds on a laptop.</p>
</sec>
</sec></sec>
</sec>
</sec>
<sec id="s12">
<title>Supplementary material</title>
<sec id="s12a">
<title>Supplementary text 1: Derivation of the one cell type model</title>
<p>An efficient spiking model network with one cell type (1CT) has been developed previously <sup><xref ref-type="bibr" rid="c28">28</xref></sup>, and properties of the 1CT model where the computation is assumed to be the leaky integration of inputs has been addressed in a number of previous studies <sup><xref ref-type="bibr" rid="c29">29</xref>,<xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c43">43</xref></sup>. Compared to the efficient E-I model, the 1CT model can be seen as a simplification, and can be treated similarly to the E-I model, which is what we demonstrate in this section.</p>
<p>As the name of the model suggests, all neurons in the 1CT model are of the same cell type, and we have <italic>i</italic> = 1, …, <italic>N</italic> such neurons. We can then use the definitions in Eqs. (6) - (9) (now without the index <italic>y</italic>) and a loss function similar to the one in <sup><xref ref-type="bibr" rid="c36">36</xref></sup>, but with only one (quadratic) regularizer
<disp-formula id="eqn60">
<graphic xlink:href="590955v2_eqnS_1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>β</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup> <italic>&gt;</italic> 0. The encoding error of the one cell type model minimizes the squared distance between the target signal <bold><italic>x</italic></bold>(<italic>t</italic>) and the estimate <inline-formula><inline-graphic xlink:href="590955v2_inline142.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. As we apply the condition for spiking as for the E-I network (<xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref> without the index <italic>y</italic>) and follow the same steps as for the E-I network, we get
<disp-formula id="eqn61">
<graphic xlink:href="590955v2_eqnS_2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with <italic>ξ</italic><sub><italic>i</italic></sub>(<italic>t</italic>) the noise at the condition for spiking. Same as in the E-I model, we define the noise as an Ornstein-Uhlenbeck process with zero mean, obeying
<disp-formula id="eqn62">
<graphic xlink:href="590955v2_eqnS_3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>η</italic><sub><italic>i</italic></sub> is a Gaussian white noise and <italic>λ</italic> = <italic>τ</italic><sup>−1</sup> is the inverse time constant of the process. We now define proxies of the membrane potential and the firing threshold as
<disp-formula id="eqn63">
<graphic xlink:href="590955v2_eqnS_4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>Differentiating the proxy of the membrane potential <italic>u</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and rewriting the model as an integrate- and-fire neuron, we get
<disp-formula id="eqn64">
<graphic xlink:href="590955v2_eqnS_5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>We now proceed in the same way as with the E-I model and define new variables
<disp-formula id="eqn65">
<graphic xlink:href="590955v2_eqnS_6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>In these new variables, we can rewrite the membrane equation of the 1CT model as follows:
<disp-formula id="eqn66">
<graphic xlink:href="590955v2_eqnS_7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>Finally, we rewrite the model with a more compact notation of a leaky integrate-and-fire neuron model with transmembrane currents,
<disp-formula id="eqn67">
<graphic xlink:href="590955v2_eqnS_8a.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
with currents
<disp-formula id="eqn68">
<graphic xlink:href="590955v2_eqnS_8b.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>Note that the model with one cell type does not obey Dale’s law, since the same neuron sends to its postsynaptic targets excitatory and inhibitory currents, depending on the tuning similarity of the presynaptic and the postsynaptic neuron <bold><italic>w</italic></bold><sub><italic>i</italic></sub> and <bold><italic>w</italic></bold><sub><italic>j</italic></sub> (Eq. S.8b). In particular, if the pre- and postsynaptic neurons have similar selectivity <inline-formula><inline-graphic xlink:href="590955v2_inline143.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the recurrent interaction is inhibitory, and if the neurons have different selectivity <inline-formula><inline-graphic xlink:href="590955v2_inline144.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the interaction is excitatory. Simply put, neurons with similar selectivity inhibit each other while neurons with different selectivity excite each other <sup><xref ref-type="bibr" rid="c36">36</xref></sup>.</p>
<p>Dale’s law can be imposed to the 1CT model the same way as in the E-I model, by removing synaptic interactions between neurons with different selectivity with rectification of the connectivity matrix,
<disp-formula id="eqn69">
<graphic xlink:href="590955v2_eqnS_9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>However, this manipulation results in a network with only inhibitory recurrent synaptic interactions, and thus a network of only inhibitory neurons. Network with only inhibitory interactions is less relevant for the description of recurrently connected biological networks.</p>
</sec>
<sec id="s12b">
<title>Supplementary text 2: Parameters of the E-I model without non-specific currents</title>
<p>Our analytical derivation in <xref ref-type="disp-formula" rid="eqn25">Eq. 25</xref> suggested an efficient E-I model that is simpler with respect to the E-I model studied in this contribution, as it does not have non-specific synaptic currents. Optimal (computational) model parameters of such simpler model, listed above the double line in Table S1, are by definition identical to the full E-I model listed in <xref ref-type="table" rid="tbl1">Table 1</xref>. However, the model without non-specific synaptic currents differs from the full E-I model about the distance between the resting potential and the threshold. In the simpler model, this distance is lower</p>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><title>Table of optimal model parameters for the efficient E-I network without non-specific synaptic currents.</title></caption>
<graphic xlink:href="590955v2_tblS1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>As in <xref ref-type="table" rid="tbl1">Table 1</xref>, for the E-I model without non-specific currents. The model is defined in <xref ref-type="disp-formula" rid="eqn25">Eq. 25</xref>. compared to the full E-I model, and is not consistent with empirically measured distance, which is about 20 mV <sup><xref ref-type="bibr" rid="c63">63</xref></sup>.</p>
<p>A simple way to increase the distance between the resting potential and the firing threshold is to introduce a constant that multiplies all mathematical terms in the <xref ref-type="disp-formula" rid="eqn25">Eq. 25</xref>. While this allows to achieve biologically plausible values for the distance between the resting potential and the threshold, it leads to values of mean recurrent synaptic connectivity <inline-formula><inline-graphic xlink:href="590955v2_inline145.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="590955v2_inline146.gif" mime-subtype="gif" mimetype="image"/></inline-formula> that are stronger than typically reported in the empirical literature <sup><xref ref-type="bibr" rid="c61">61</xref></sup>.</p>
</sec>
<sec id="s12c">
<title>Supplementary text 3: Analysis of the one cell type model and comparison with the E-I model</title>
<p>We re-derived the 1CT model as a simplification of the E-I network (Supplementary Text 1, Supplementary Fig. S1A-B), with objective function of the same form as <italic>L</italic><sup><italic>E</italic></sup> and by allowing a single type of neurons sending both excitatory and inhibitory synaptic currents to their post-synaptic targets (<xref rid="figS1" ref-type="fig">Supplementary Fig. S1C</xref>). Similarly to the E-I model, also the 1CT model exhibits structured connectivity, with synaptic strength depending on the tuning similarity between the presynaptic and the postsynaptic neuron. Pairs of neurons with stronger tuning similarity (dissimilarity) have stronger mutual inhibition (excitation); see <xref rid="figS1" ref-type="fig">Supplementary Fig. S1D</xref>.</p>
<p>We compared the coding performance of the E-I model with that of a fully connected 1CT model. Both models received the same set of stimulus features and performed the same computation. In the 1CT model, tuning parameters were drawn from the same distribution as used for the E neurons in the E-I model. We used the same membrane time constant <italic>τ</italic> in both models, while the metabolic constants (<italic>β</italic> of the E-I model and <italic>β</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup> of the 1CT model) and the noise intensity (<italic>σ</italic> of the E-I model and <italic>σ</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup> of the 1CT model) were chosen such as to optimize the average loss for each model (<xref rid="fig6" ref-type="fig">Fig. 6B</xref> for E-I model, Supplementary Fig. S1F-G for 1CT model). Parameters of the 1CT model are listed in the Supplementary Table S2. A qualitative comparison of the E-I and the 1CT model showed that with optimal parameters, both models accurately tracked multiple target signals (<xref rid="fig1" ref-type="fig">Fig. 1G</xref> and <xref rid="figS1" ref-type="fig">Supplementary Fig. S1E</xref>).</p>
<p>To compare the performance of the E-I and the 1CT models also quantitatively, we measured the average encoding error (RMSE), metabolic cost (MC) and loss of each model. The RMSE and the MC in the 1CT model were measured as in <xref ref-type="disp-formula" rid="eqn36">Eq. 36</xref> and <xref ref-type="disp-formula" rid="eqn37">37</xref>, while the average loss of each model was evaluated as follows:
<disp-formula id="eqn70">
<graphic xlink:href="590955v2_eqnS_10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>Unless mentioned otherwise, we weighted stronger the encoding error compared to the metabolic cost and used <italic>g</italic><sub><italic>L</italic></sub> = 0.7.</p>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Table S2.</label>
<caption><title>Table of default model parameters for the efficient network with one cell type.</title></caption>
<graphic xlink:href="590955v2_tblS2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>The parameters <italic>N, M, τ</italic> and <inline-formula><inline-graphic xlink:href="590955v2_inline147.gif" mime-subtype="gif" mimetype="image"/></inline-formula> were chosen identical to the E-I network (see <xref ref-type="table" rid="tbl1">Table 1</xref> in the main text). Parameters <italic>σ</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup> and <italic>β</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup> were determined as values that maximize network efficiency (see section “Performance measures” in the main text).</p>
</sec>
</sec>
<sec id="s13">
<title>Supplementary Figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><title>Efficient spiking model with one cell type and the encoding bias of the E-I network.</title>
<p><bold>(A)</bold> Schematic of efficient coding with a single spiking neuron. In this toy example, the neuron has a positive decoding weight and responds to a single stimulus feature <italic>s</italic>(<italic>t</italic>) (top). The target signal <italic>x</italic>(<italic>t</italic>) (bottom, black) integrates the stimulus feature from top. The neuron spikes to keep the readout of its activity <inline-formula><inline-graphic xlink:href="590955v2_inline156.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (magenta) close to the target.</p>
<p><bold>(B)</bold> Schematic of an efficient 1CT model. The target signal is computed from the stimulus feature. The spiking network estimates the targets by generating population readouts of its spiking activity.</p>
<p><bold>(C)</bold> Schematic of excitatory (red) and inhibitory (blue) synaptic interactions in the 1CT model. Neurons with similar selectivity inhibit each other (blue), while neurons with different selectivity excite each other (red). The same neuron is sending excitatory and inhibitory synaptic outputs, which is not consistent with Dale’s law.</p>
<p><bold>(D)</bold> Strength of recurrent synapses as a function of pair-wise tuning similarity.</p>
<p><bold>(E)</bold> Simulation of the model with 1CT. Top three rows show the target (black), and the estimate (magenta) in each of the 3 input dimensions.</p>
<p><bold>(F)</bold> Top left: The target (black) and the estimate (red) of the E population in the first signal dimension (in response to the first stimulus feature <italic>s</italic><sub>1</sub>(<italic>t</italic>)). The estimate is averaged across 100 trials, with trials varying about the initial conditions and the noise in the membrane potential. Bottom left: Same as on top left, for the I population. Right: Time-dependent bias of estimates in E (top) and I (bottom) population in each of the three stimulus dimensions.</p>
<p><bold>(G)</bold> Left: Root mean squared error (RMSE) as a function of the metabolic constant <italic>β</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Right: Normalized metabolic cost (green) and normalized average loss (black) as a function of the metabolic constant <italic>β</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup>. The black arrow denotes the minimum of the loss and thus the optimal parameter <italic>β</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup>.</p>
<p><bold>(H)</bold> Same as in <bold>G</bold>, measured as a function of the noise intensity <italic>σ</italic><sup><xref ref-type="bibr" rid="c1">1</xref></sup>.</p>
<p>Results in F,G and H were computed in 100 simulation trials of duration of 1 second. For other parameters, see <xref ref-type="table" rid="tbl1">Table 1</xref> (E-I model) and Table S2 (1CT model). This figure is related to <xref rid="fig1" ref-type="fig">Fig. 1</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v2_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Tuning similarity and its relation to lateral excitation/inhibition.</title>
<p><bold>(A)</bold> Pair-wise tuning similarity for all pairs of E neurons. Tuning similarity is measured as cosine similarity of decoding vectors between the target neuron and every other E neuron.</p>
<p><bold>(B)</bold> Histogram of tuning similarity across all E-E pairs shown in <bold>A</bold>.</p>
<p><bold>(C)</bold> Tuning similarity to a single, randomly selected target neuron. Tuning similarity to a target neuron corresponds to a vector from the tuning similarity matrix in <bold>A</bold>. We sorted the tuning similarity to target from the smallest to the biggest value. Neurons with negative similarity are grouped as neurons with different tuning, while neurons with positive tuning similarity are grouped as neurons with similar tuning.</p>
<p><bold>(D)</bold> Histogram of tuning similarity of E neurons to the target neuron shown in <bold>C</bold>. With distribution of tuning parameters that is symmetric around zero as used in our study, any choice of the target neuron gives approximately the same number of neurons with similar and different selectivity.</p>
<p><bold>(E)</bold> Top: Trial and neuron-averaged deviation of the instantaneous firing rate from the baseline in presence of weak feedforward stimulus. We show the ± standard error of the mean (SEM) of neurons with similar (orange) and different tuning (gray) to the target neuron. The photo-stimulation intensity is at threshold (<italic>a</italic><sub><italic>p</italic></sub> = 1.0). The feedforward stimulus was received by all E neurons and it induced, together with the external current, the mean firing rates of 7.3 Hz and 13.5 Hz in E and I neurons, respectively. Bottom: Scatter plot of the tuning similarity versus effective connectivity. Magenta line marks the least-squares line.</p>
<p><bold>(F)</bold> Same as in <bold>E</bold>, for the network with partial (fine-grained) removal of connectivity structure. Partial removal of connectivity structure is achieved by shuffling the synaptic weights among pairs of neurons with similar tuning (for which <inline-formula><inline-graphic xlink:href="590955v2_inline157.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For model parameters, see <xref ref-type="table" rid="tbl1">Table 1</xref>. This figure is related to <xref rid="fig3" ref-type="fig">Fig. 3</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v2_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Effect of removal of connectivity structure and of jittering of synaptic weights.</title>
<p><bold>(A)</bold> Distribution of decoding weights after training a linear decoder on neural activity generated by the network without connectivity structure.</p>
<p><bold>(B)</bold> RMSE in E (top) and I neurons (bottom) in networks with partial removal of connectivity structure. Partial removal of connectivity structure is achieved by limiting the permutation of synaptic connectivity to neuronal pairs with similar tuning, e.g. to neuronal pairs for which the following is true: <inline-formula><inline-graphic xlink:href="590955v2_inline158.gif" mime-subtype="gif" mimetype="image"/></inline-formula>.</p>
<p><bold>(C)</bold> Same as in <bold>B</bold>, showing the average metabolic cost on spiking.</p>
<p><bold>(D)</bold> Same as in <bold>B</bold>, showing the average net synaptic input, a measure of the average E-I balance.</p>
<p><bold>(E)</bold> Same as in <bold>B</bold>, showing the correlation of synaptic inputs, a measure of instantaneous balance.</p>
<p><bold>(F)</bold> Average deviation of the instantaneous firing rate from the baseline for the population of I (top) and E (bottom) neurons in networks with fully removed structure in E-I (left), I-E (middle) and in all connectivity matrices (right). We show the mean ± SEM for neurons with similar (ochre) and different (green) tuning to the target neuron. The mean traces of the network with structured connectivity are shown for comparison, with magenta and gray for similar and different tuning, respectively.</p>
<p><bold>(G)</bold> Top: The RMSE (top) in E and I cell type, as a function of the strength of perturbation of the synaptic connectivity by random jittering. Bottom: Same as on top, showing the normalized metabolic cost (green) and average loss (black).</p>
<p><bold>(H)</bold> Target signals, E estimates and I estimates in three input dimensions (three top rows), spike trains (fourth row) and the instantaneous estimate of the firing rate of E and I populations (bottom) in a simulation trial, with significant jitter of recurrent connectivity (jittering strength of 0.5, see Methods). In spite of a relatively strong jittering, the network shows excellent encoding of the target signal. All statistical results were computed in 100 simulation trials of duration of 1 second. Other parameters are in <xref ref-type="table" rid="tbl1">Table 1</xref>. This figure is related to <xref rid="fig4" ref-type="fig">Fig. 4</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v2_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption><title>Dependence of optimal parameters on weighting of the encoding error and the metabolic cost and analysis of mean ratio of I-I to E-I connectivity by varying the number of E neurons.</title>
<p><bold>(A)</bold> Optimal set of time constants of E and I neurons <inline-formula><inline-graphic xlink:href="590955v2_inline159.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for different weightings between the error and the cost when computing the loss. Optimal time constants show little dependency on this weighting.</p>
<p><bold>(B)</bold> Top: Optimal metabolic constant as a function of the weighting of the average loss of E and I cell type. Bottom: Same as on top, as a function of the weighting between the error and the cost. Black triangles mark weightings that are typically used to estimate optimal model efficiency.</p>
<p><bold>(C)</bold> Same as in <bold>B</bold>, as a function of noise intensity.</p>
<p><bold>(D)</bold> Same as in <bold>B</bold>, as a function of the optimal ratio of I-I to E-I connectivity. This analysis was performed by varying the number of I neurons while the number of E neurons stays fixed.</p>
<p><bold>(E)</bold> Top: Encoding error (RMSE) of the E (red) and I (blue) estimates as a function of mean I-I to E-I connectivity. The ratio was varied by changing the number of E neurons and keeping the number of I neurons fixed at a value specified in <xref ref-type="table" rid="tbl1">Table 1</xref>. Bottom: Same as on top, showing the normalized cost and average loss.</p>
<p><bold>(F)</bold> Same as in <bold>E</bold>, showing, the average firing rate (top), and average coefficient of variation (bottom) in E and I cell type.</p>
<p><bold>(G)</bold> Same as in <bold>E</bold>, showing the average imbalance and instantaneous balance of synaptic currents in E and I neurons.</p>
<p><bold>(H)</bold> Same as in <bold>D</bold>, for the optimal ratio measured by varying the number of E neurons. All results were computed in 100 trials of duration of 1 second for each trial. For other parameters, see <xref ref-type="table" rid="tbl1">Table 1</xref>. This figure is related to <xref rid="fig7" ref-type="fig">Fig. 7</xref> in the main paper.</p></caption>
<graphic xlink:href="590955v2_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5.</label>
<caption><title>Effect of stimulus properties on efficient neural coding and dynamics.</title>
<p><bold>(A)</bold> Average firing rate (top), and average coefficient of variation (bottom) in E and I cell type, as a function of the time constant of the stimulus features <italic>τ</italic><sub><italic>s</italic></sub>. All stimulus features have the same time constant.</p>
<p><bold>(B)</bold> Average imbalance (top) and instantaneous balance (bottom) as a function of the time constant of stimuli <italic>τ</italic><sub><italic>s</italic></sub>.</p>
<p><bold>(C)</bold> Top: RMSE of E (red) and I (blue) estimates as a function of the time constant of the targets <italic>τ</italic><sub><italic>x</italic></sub>. All targets have the same time constant. Middle: Metabolic cost in the E and I population. Bottom: Average loss in the E and I population. Black arrow indicates the minimum loss and therefore the optimal time constant.</p>
<p><bold>(D-E)</bold> Same as in <bold>A-B</bold>, as a function of the time constant of the targets <italic>τ</italic><sub><italic>x</italic></sub>.</p>
<p><bold>(F-G)</bold> Same as in <bold>A-B</bold>, as a function of the number of encoded variables <italic>M</italic>. All results were computed in 100 trials of duration of 1 second. For parameters, see <xref ref-type="table" rid="tbl1">Table 1</xref>. This figure is related to <xref rid="fig8" ref-type="fig">Fig. 8</xref> in the main paper.</p>
</caption>
<graphic xlink:href="590955v2_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgements</title>
<p>V.K. and T.S. thank Tatiana Engel for her contribution to the discussion of results and for her comments on an earlier version of the manuscript. This project was supported by funding from Technische Universität Berlin (“Equal Opportunity Program” to VK), by Internal Research Funding of Technische Universität Berlin (to TS),, by NIH Brain Initiative (grants U19 NS107464, R01 NS109961, R01 NS108410 to SP), and the Simons Foundation for Autism Research Initiative (SFARI; grant 982347 to SP), and by the European Union’s Horizon 2023 research and innovation program under the Marie Sk-lodowska-Curie Actions (grant 101152984 ASTRONET to SBM).</p>
</ack>
<sec id="d1e2957" sec-type="additional-information">
<title>Additional information</title>
<sec id="s4">
<title>Code availability</title>
<p>The complete computer code for reproducing the results can be downloaded anonymously from a public GitHub repository <ext-link ext-link-type="uri" xlink:href="https://github.com/VeronikaKoren/efficient_EI">https://github.com/VeronikaKoren/efficient_EI</ext-link>.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>DePasquale</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Memmesheimer</surname>, <given-names>R.-M.</given-names></string-name></person-group> <article-title>Building functional networks of spiking model neurons</article-title>. <source>Nature Neuroscience</source> <volume>19</volume>, <fpage>350</fpage>–<lpage>355</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thalmeier</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Uhlmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kappen</surname>, <given-names>H. J.</given-names></string-name> &amp; <string-name><surname>Memmesheimer</surname>, <given-names>R.-M.</given-names></string-name></person-group> <article-title>Learning universal computations with spikes</article-title>. <source>PLoS Comput Biol</source> <volume>12</volume>, <fpage>e1004895</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barlow</surname>, <given-names>H. B.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Possible principles underlying the transformation of sensory messages</article-title>. <source>Sensory communication</source> <volume>1</volume>, <fpage>217</fpage>–<lpage>233</lpage> (<year>1961</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> &amp; <string-name><surname>Field</surname>, <given-names>D. J.</given-names></string-name></person-group> <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source> <volume>381</volume>, <fpage>607</fpage>–<lpage>609</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Chalk</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Efficiency turns the table on neural encoding, decoding and noise</article-title>. <source>Current Opinion in Neurobiology</source> <volume>37</volume>, <fpage>141</fpage>–<lpage>148</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Atick</surname>, <given-names>J. J.</given-names></string-name></person-group> <article-title>Could information theory provide an ecological theory of sensory processing?</article-title> <source>Network: Computation in neural systems</source> <volume>3</volume>, <fpage>213</fpage>–<lpage>251</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> &amp; <string-name><surname>Field</surname>, <given-names>D. J.</given-names></string-name></person-group> <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title> <source>Vision research</source> <volume>37</volume>, <fpage>3311</fpage>–<lpage>3325</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name> &amp; <string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name></person-group> <article-title>Natural image statistics and neural representation</article-title>. <source>Annual review of neuroscience</source> <volume>24</volume>, <fpage>1193</fpage>–<lpage>1216</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vinje</surname>, <given-names>W. E.</given-names></string-name> &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title>. <source>Science</source> <volume>287</volume>, <fpage>1273</fpage>–<lpage>1276</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> &amp; <string-name><surname>Field</surname>, <given-names>D. J.</given-names></string-name></person-group> <article-title>Sparse coding of sensory inputs</article-title>. <source>Current opinion in neurobiology</source> <volume>14</volume>, <fpage>481</fpage>–<lpage>487</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>Z.</given-names></string-name></person-group> <source>Understanding vision: theory, models, and data</source> (<publisher-name>Oxford University Press, USA</publisher-name>, <year>2014</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewicki</surname>, <given-names>M. S.</given-names></string-name></person-group> <article-title>Efficient coding of natural sounds</article-title>. <source>Nature Neuroscience</source> <volume>5</volume>, <fpage>356</fpage>–<lpage>363</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koulakov</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rinberg</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Sparse incomplete representations: A potential role of olfactory granule cells</article-title>. <source>Neuron</source> <volume>72</volume>, <fpage>124</fpage>–<lpage>136</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Rieke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>de Ruyter van Steveninck</surname>, <given-names>R. R.</given-names></string-name> &amp; <string-name><surname>Warland</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Reading a neural code</article-title>. <source>Science</source> <volume>252</volume>, <fpage>1854</fpage>–<lpage>1857</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Rieke</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Reliability and information transmission in spiking neurons</article-title>. <source>Trends in neurosciences</source> <volume>15</volume>, <fpage>428</fpage>–<lpage>434</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Schultz</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Lebedev</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Diamond</surname>, <given-names>M. E.</given-names></string-name></person-group> <article-title>The role of spike timing in the coding of stimulus location in rat somatosensory cortex</article-title>. <source>Neuron</source> <volume>29</volume>, <fpage>769</fpage>–<lpage>777</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nemenman</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Lewen</surname>, <given-names>G. D.</given-names></string-name>, <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>de Ruyter van Steveninck</surname>, <given-names>R. R.</given-names></string-name></person-group> <article-title>Neural coding of natural stimuli: information at sub-millisecond resolution</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>, <fpage>e1000025</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Millisecond encoding precision of auditory cortex neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>107</volume>, <fpage>16976</fpage>–<lpage>16981</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ince</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Neural codes formed by small and temporally precise populations in auditory cortex</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>, <fpage>18277</fpage>–<lpage>18287</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Sensory neural codes using multiplexed temporal scales</article-title>. <source>Trends in neurosciences</source> <volume>33</volume>, <fpage>111</fpage>–<lpage>120</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fairhall</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Lewen</surname>, <given-names>G. D.</given-names></string-name>, <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>de Ruyter van Steveninck</surname>, <given-names>R. R.</given-names></string-name></person-group> <article-title>Efficiency and ambiguity in an adaptive neural code</article-title>. <source>Nature</source> <volume>412</volume>, <fpage>787</fpage>–<lpage>792</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wark</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Fairhall</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rieke</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>Timescales of inference in visual adaptation</article-title>. <source>Neuron</source> <volume>61</volume>, <fpage>750</fpage>–<lpage>761</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mazzoni</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>Encoding of naturalistic stimuli by local field potential spectra in networks of excitatory and inhibitory neurons</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>, <fpage>e1000239</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mlynarski</surname>, <given-names>W. F.</given-names></string-name> &amp; <string-name><surname>Hermundstadt</surname>, <given-names>A. M.</given-names></string-name></person-group> <article-title>Efficient and adaptive sensory codes</article-title>. <source>Nature Neuroscience</source> <volume>24</volume>, <fpage>998</fpage>–<lpage>1009</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koren</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Bondanelli</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Computational methods to study information processing in neural circuits</article-title>. <source>Computational and Structural Biotechnology Journal</source> <volume>21</volume>, <fpage>910</fpage>–<lpage>922</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lochmann</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Ernst</surname>, <given-names>U. A.</given-names></string-name> &amp; <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Perceptual inference predicts contextual modulations of sensory responses</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>, <fpage>4179</fpage>–<lpage>4195</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Rozell</surname>, <given-names>C. J.</given-names></string-name></person-group> <article-title>Visual nonclassical receptive field effects emerge from sparse coding in a dynamical system</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>, <fpage>e1003191</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boerlin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name> &amp; <string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>, <fpage>e1003258</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bourdoukan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Barrett</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name></person-group> <article-title>Learning optimal spike-based representations</article-title>. <source>Advances in neural information processing systems</source> <volume>25</volume>, <fpage>2285</fpage>–<lpage>2293</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moreno-Bote</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Drugowitsch</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Causal inference and explaining away in a spiking network</article-title>. <source>Scientific Reports</source> <volume>5</volume>, <fpage>17531</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chalk</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gutkin</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Neural oscillations as a signature of efficient coding in the presence of synaptic delays</article-title>. <source>Elife</source> <volume>5</volume>, <elocation-id>e13824</elocation-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name></person-group> <article-title>Efficient codes and balanced networks</article-title>. <source>Nature Neuroscience</source> <volume>19</volume>, <fpage>375</fpage>–<lpage>382</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gutierrez</surname>, <given-names>G. J.</given-names></string-name> &amp; <string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Population adaptation in efficient balanced networks</article-title>. <source>Elife</source> <volume>8</volume>, <elocation-id>e46926</elocation-id> (<year>2019</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kadmon</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Timcheck</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Predictive coding in balanced neural networks with noise, chaos and delays</article-title>. In  <conf-name>Advances in Neural Information Processing Systems</conf-name>, vol. <volume>33</volume>, <fpage>16677</fpage>–<lpage>16688</lpage> (<publisher-name>Curran Associates, Inc</publisher-name>., <year>2020</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buxó</surname>, <given-names>C. E. R.</given-names></string-name> &amp; <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name></person-group> <article-title>Poisson balanced spiking networks</article-title>. <source>PLoS Comput Biol</source> <volume>16</volume>, <fpage>e1008261</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koren</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Computational account of spontaneous activity as a signature of predictive coding</article-title>. <source>PLoS Comput Biol</source> <volume>13</volume>, <fpage>e1005355</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barrett</surname>, <given-names>D. G.</given-names></string-name>, <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name></person-group> <article-title>Optimal compensation for neuron loss</article-title>. <source>Elife</source> <volume>5</volume>, <elocation-id>e12454</elocation-id> (<year>2016</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Koren</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Biologically plausible solutions for spiking networks with efficient coding</article-title>. In  <conf-name>Advances in Neural Information Processing Systems</conf-name>, vol. <volume>35</volume>, <fpage>20607</fpage>–<lpage>20620</lpage> (<publisher-name>Curran Associates, Inc</publisher-name>., <year>2022</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brette</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Adaptive exponential integrate-and-fire model as an effective description of neuronal activity</article-title>. <source>Journal of neurophysiology</source> <volume>94</volume>, <fpage>3637</fpage>–<lpage>3642</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mensi</surname>, <given-names>S.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Parameter extraction and classification of three cortical neuron types reveals two distinct adaptation mechanisms</article-title>. <source>Journal of neurophysiology</source> <volume>107</volume>, <fpage>1756</fpage>–<lpage>1775</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Kistler</surname>, <given-names>W. M.</given-names></string-name>, <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Paninski</surname>, <given-names>L.</given-names></string-name></person-group> <source>Neuronal dynamics: From single neurons to networks and models of cognition</source> (<publisher-name>Cambridge University Press</publisher-name>, <year>2014</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jolivet</surname>, <given-names>R.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>The quantitative single-neuron modeling competition</article-title>. <source>Biological cybernetics</source> <volume>99</volume>, <fpage>417</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brendel</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Bourdoukan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Vertechi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name> &amp; <string-name><surname>Denéve</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Learning to represent signals spike by spike</article-title>. <source>PLoS Comput Biol</source> <volume>16</volume>, <fpage>e1007692</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Podlaski</surname>, <given-names>W. F.</given-names></string-name> &amp; <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name></person-group> <article-title>Approximating nonlinear functions with latent bound-aries in low-rank excitatory-inhibitory spiking networks</article-title>. <source>Neural Computation</source> <volume>36</volume>, <fpage>803</fpage>–<lpage>857</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vogels</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Sprekeler</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Zenke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title>. <source>Science</source> <volume>334</volume>, <fpage>1569</fpage>–<lpage>1573</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scott</surname>, <given-names>B. B.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Fronto-parietal cortical circuits encode accumulated evidence with a diversity of timescales</article-title>. <source>Neuron</source> <volume>95</volume>, <fpage>385</fpage>–<lpage>398</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Danskin</surname>, <given-names>B. P.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Exponential history integration with diverse temporal scales in retrosplenial cortex supports hyperbolic behavior</article-title>. <source>Science advances</source> <volume>9</volume>, <fpage>eadj4897</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gold</surname>, <given-names>J. I.</given-names></string-name> &amp; <string-name><surname>Shadlen</surname>, <given-names>M. N.</given-names></string-name></person-group> <article-title>Neural computations that underlie decisions about sensory stimuli</article-title>. <source>Trends in cognitive sciences</source> <volume>5</volume>, <fpage>10</fpage>–<lpage>16</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Usher</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>McClelland</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>The time course of perceptual choice: the leaky, competing accumulator model</article-title>. <source>Psychological review</source> <volume>108</volume>, <fpage>550</fpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chong</surname>, <given-names>E.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Manipulating synthetic optogenetic odors reveals the coding logic of olfactory perception</article-title>. <source>Science</source> <volume>368</volume>, <fpage>eaba2357</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeldenrust</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gutkin</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Denéve</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Efficient and robust coding in heterogeneous recurrent networks</article-title>. <source>PLoS Comput Biol</source> <volume>17</volume>, <fpage>e1008673</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tavoni</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Balasubramanian</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Gold</surname>, <given-names>J. I.</given-names></string-name></person-group> <article-title>What is optimal in optimal inference?</article-title> <source>Current Opinion in Behavioral Sciences</source> <volume>29</volume>, <fpage>117</fpage>–<lpage>126</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Denève</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Alemi</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Bourdoukan</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>The brain as an efficient and robust adaptive learner</article-title>. <source>Neuron</source> <volume>94</volume>, <fpage>969</fpage>–<lpage>977</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Faisal</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Selen</surname>, <given-names>L. P.</given-names></string-name> &amp; <string-name><surname>Wolpert</surname>, <given-names>D. M.</given-names></string-name></person-group> <article-title>Noise in the nervous system</article-title>. <source>Nature reviews neuroscience</source> <volume>9</volume>, <fpage>292</fpage>–<lpage>303</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Emanuel</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Lehnert</surname>, <given-names>B. P.</given-names></string-name>, <string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name> &amp; <string-name><surname>Ginty</surname>, <given-names>D. D.</given-names></string-name></person-group> <article-title>Cortical responses to touch reflect subcortical integration of ltmr signals</article-title>. <source>Nature</source> <volume>600</volume>, <fpage>680</fpage>–<lpage>685</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Znamenskiy</surname>, <given-names>P.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Functional specificity of recurrent inhibition in visual cortex</article-title>. <source>Neuron</source> <volume>112</volume>, <fpage>991</fpage>–<lpage>1000</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sadeh</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Theory of neuronal perturbome in cortical networks</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>117</volume>, <fpage>26966</fpage>–<lpage>26976</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chettih</surname>, <given-names>S. N.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name></person-group> <article-title>Single-neuron perturbations reveal feature-specific competition in V1</article-title>. <source>Nature</source> <volume>567</volume>, <fpage>334</fpage>–<lpage>340</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oldenburg</surname>, <given-names>I. A.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>The logic of recurrent circuits in the primary visual cortex</article-title>. <source>Nature Neuroscience</source> <volume>27</volume>, <fpage>1</fpage>–<lpage>11</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pala</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Petersen</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>In-vivo measurement of cell-type-specific synaptic connectivity and synaptic transmission in layer 2/3 mouse barrel cortex</article-title>. <source>Neuron</source> <volume>85</volume>, <fpage>68</fpage>–<lpage>75</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Campagnola</surname>, <given-names>L.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Local connectivity and synaptic dynamics in mouse and human neocortex</article-title>. <source>Science</source> <volume>375</volume>, <fpage>eabj5861</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Destexhe</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Rudolph</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Paré</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>The high-conductance state of neocortical neurons in vivo</article-title>. <source>Nature Reviews Neuroscience</source> <volume>4</volume>, <fpage>739</fpage>–<lpage>751</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Constantinople</surname>, <given-names>C. M.</given-names></string-name> &amp; <string-name><surname>Bruno</surname>, <given-names>R. M.</given-names></string-name></person-group> <article-title>Deep cortical layers are activated directly by thalamus</article-title>. <source>Science</source> <volume>340</volume>, <fpage>1591</fpage>–<lpage>1594</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burkitt</surname>, <given-names>A. N.</given-names></string-name></person-group> <article-title>A review of the integrate-and-fire neuron model: I. homogeneous synaptic input</article-title>. <source>Biological cybernetics</source> <volume>95</volume>, <fpage>1</fpage>–<lpage>19</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwalger</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Deger</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Towards a theory of cortical columns: From spiking neurons to interacting neural populations of finite size</article-title>. <source>PLoS Comput. Biol</source>. <volume>13</volume>, <fpage>e1005507</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Harkin</surname>, <given-names>E. F.</given-names></string-name>, <string-name><surname>Béïque</surname>, <given-names>J.-C.</given-names></string-name> &amp; <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name></person-group> <chapter-title>A user’s guide to generalized integrate-and-fire models</chapter-title>. In <source>Computational Modelling of the Brain: Modelling Approaches to Cells, Circuits and Networks</source>, <fpage>69</fpage>–<lpage>86</lpage> (<publisher-name>Springer</publisher-name>, <year>2021</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name></person-group> <article-title>The code for facial identity in the primate brain</article-title>. <source>Cell</source> <volume>169</volume>, <fpage>1013</fpage>–<lpage>1028</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaufman</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Churchland</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Ryu</surname>, <given-names>S. I.</given-names></string-name> &amp; <string-name><surname>Shenoy</surname>, <given-names>K. V.</given-names></string-name></person-group> <article-title>Cortical activity in the null space: permitting preparation without movement</article-title>. <source>Nature Neuroscience</source> <volume>17</volume>, <fpage>440</fpage>–<lpage>448</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lefort</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tomm</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sarria</surname>, <given-names>J.-C. F.</given-names></string-name> &amp; <string-name><surname>Petersen</surname>, <given-names>C. C.</given-names></string-name></person-group> <article-title>The excitatory neuronal network of the C2 barrel column in mouse primary somatosensory cortex</article-title>. <source>Neuron</source> <volume>61</volume>, <fpage>301</fpage>–<lpage>316</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neske</surname>, <given-names>G. T.</given-names></string-name>, <string-name><surname>Patrick</surname>, <given-names>S. L.</given-names></string-name> &amp; <string-name><surname>Connors</surname>, <given-names>B. W.</given-names></string-name></person-group> <article-title>Contributions of diverse excitatory and inhibitory neurons to recurrent network activity in cerebral cortex</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>, <fpage>1089</fpage>–<lpage>1105</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buzsáki</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Mizuseki</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>The log-dynamic brain: how skewed distributions affect network operations</article-title>. <source>Nature Reviews Neuroscience</source> <volume>15</volume>, <fpage>264</fpage>–<lpage>278</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Softky</surname>, <given-names>W. R.</given-names></string-name> &amp; <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>The highly irregular firing of cortical cells is inconsistent with temporal integration of random epsps</article-title>. <source>Journal of neuroscience</source> <volume>13</volume>, <fpage>334</fpage>–<lpage>350</lpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahmadian</surname>, <given-names>Y.</given-names></string-name> &amp; <string-name><surname>Miller</surname>, <given-names>K. D.</given-names></string-name></person-group> <article-title>What is the dynamical regime of cerebral cortex?</article-title> <source>Neuron</source> <volume>109</volume>, <fpage>3373</fpage>–<lpage>3391</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Okun</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Lampl</surname>, <given-names>I.</given-names></string-name></person-group> <article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title>. <source>Nature Neuroscience</source> <volume>11</volume>, <fpage>535</fpage>–<lpage>537</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xue</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Atallah</surname>, <given-names>B. V.</given-names></string-name> &amp; <string-name><surname>Scanziani</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Equalizing excitation–inhibition ratios across visual cortical neurons</article-title>. <source>Nature</source> <volume>511</volume>, <fpage>596</fpage>–<lpage>600</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Andoni</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Priebe</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>A spontaneous state of weakly correlated synaptic excitation and inhibition in visual cortex</article-title>. <source>Neuroscience</source> <volume>247</volume>, <fpage>364</fpage>–<lpage>375</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brunel</surname>, <given-names>N.</given-names></string-name></person-group> <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>Journal of computational neuroscience</source> <volume>8</volume>, <fpage>183</fpage>–<lpage>208</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Renart</surname>, <given-names>A.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>The asynchronous state in cortical circuits</article-title>. <source>science</source> <volume>327</volume>, <fpage>587</fpage>–<lpage>590</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mackwood</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Naumann</surname>, <given-names>L. B.</given-names></string-name> &amp; <string-name><surname>Sprekeler</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>Learning excitatory-inhibitory neuronal assemblies in recurrent networks</article-title>. <source>Elife</source> <volume>10</volume>, <elocation-id>e59715</elocation-id> (<year>2021</year>).</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name> &amp; <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name></person-group> <article-title>Synaptic plasticity: taming the beast</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>, <fpage>1178</fpage>–<lpage>1183</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name> &amp; <string-name><surname>Nelson</surname>, <given-names>S. B.</given-names></string-name></person-group> <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nature reviews neuroscience</source> <volume>5</volume>, <fpage>97</fpage>–<lpage>107</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwalger</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Lindner</surname>, <given-names>B.</given-names></string-name></person-group> <article-title>Patterns of interval correlations in neural oscillators with adaptation</article-title>. <source>Front. Comput. Neurosci</source>. <volume>7</volume>, <fpage>164</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pozzorini</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mensi</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Gerstner</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>Temporal whitening by power-law adaptation in neocortical neurons</article-title>. <source>Nature Neuroscience</source> <volume>16</volume>, <fpage>942</fpage>–<lpage>948</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levy</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name> &amp; <string-name><surname>MacLean</surname>, <given-names>J. N.</given-names></string-name></person-group> <article-title>Network analysis of murine cortical dynamics implicates untuned neurons in visual stimulus coding</article-title>. <source>Cell Reports</source> <volume>31</volume>, <fpage>107483</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Zylberberg</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>The role of untuned neurons in sensory information coding</article-title>. <source>BioRxiv</source> <fpage>134379</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Destexhe</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Paré</surname>, <given-names>D.</given-names></string-name></person-group> <article-title>Impact of network activity on the integrative properties of neocortical pyramidal neurons in vivo</article-title>. <source>Journal of neurophysiology</source> <volume>81</volume>, <fpage>1531</fpage>–<lpage>1547</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Timcheck</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kadmon</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Boahen</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>Optimal noise level for coding with tightly balanced networks of spiking neurons in the presence of transmission delays</article-title>. <source>PLoS Comput Bioly</source> <volume>18</volume>, <fpage>e1010593</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sukenik</surname>, <given-names>N.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Neuronal circuits overcome imbalance in excitation and inhibition by adjusting connection numbers</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>118</volume>, <fpage>e2018459118</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Markram</surname>, <given-names>H.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Interneurons of the neocortical inhibitory system</article-title>. <source>Nature reviews neuroscience</source> <volume>5</volume>, <fpage>793</fpage>–<lpage>807</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cossell</surname>, <given-names>L.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Functional organization of excitatory synaptic strength in primary visual cortex</article-title>. <source>Nature</source> <volume>518</volume>, <fpage>399</fpage>–<lpage>403</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fang</surname>, <given-names>R.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Conservation and divergence of cortical cell organization in human and mouse revealed by merfish</article-title>. <source>Science</source> <volume>377</volume>, <fpage>56</fpage>–<lpage>62</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Alemi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Machens</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Deneve</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Slotine</surname>, <given-names>J.-J.</given-names></string-name></person-group> <article-title>Learning nonlinear dynamics in efficient, balanced spiking networks using local plasticity rules</article-title>. In <conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name>, vol. <volume>32</volume>, doi: <pub-id pub-id-type="doi">10.1609/aaai.v32i1.11320</pub-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roy</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jaiswal</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Panda</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Towards spike-based machine intelligence with neuromorphic computing</article-title>. <source>Nature</source> <volume>575</volume>, <fpage>607</fpage>–<lpage>617</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schuman</surname>, <given-names>C. D.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Opportunities for neuromorphic computing algorithms and applications</article-title>. <source>Nature Computational Science</source> <volume>2</volume>, <fpage>10</fpage>–<lpage>19</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Najafi</surname>, <given-names>F.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Excitatory and inhibitory subnetworks are equally selective during decision-making and emerge simultaneously during learning</article-title>. <source>Neuron</source> <volume>105</volume>, <fpage>165</fpage>–<lpage>179</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Runyan</surname>, <given-names>C. A.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Response features of parvalbumin-expressing interneurons suggest precise roles for subtypes of inhibition in visual cortex</article-title>. <source>Neuron</source> <volume>67</volume>, <fpage>847</fpage>–<lpage>857</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuan</surname>, <given-names>A. T.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Synaptic wiring motifs in posterior parietal cortex support decisionmaking</article-title>. <source>Nature</source> <volume>627</volume>, <fpage>367</fpage>–<lpage>373</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Engelken</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Goedeke</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>A time-resolved theory of information encoding in recurrent neural networks</article-title>. In <conf-name>Advances in Neural Information Processing Systems</conf-name>, vol. <volume>35</volume>, <fpage>35490</fpage>–<lpage>35503</lpage> (<publisher-name>Curran Associates, Inc</publisher-name>., <year>2022</year>).</mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calaim</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Dehmelt</surname>, <given-names>F. A.</given-names></string-name>, <string-name><surname>Gonçalves</surname>, <given-names>P. J.</given-names></string-name> &amp; <string-name><surname>Machens</surname>, <given-names>C. K.</given-names></string-name></person-group> <article-title>The geometry of robustness in spiking neural networks</article-title>. <source>Elife</source> <volume>11</volume>, <elocation-id>e73276</elocation-id> (<year>2022</year>).</mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seeman</surname>, <given-names>S. C.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Sparse recurrent excitatory connectivity in the microcircuit of the adult mouse and human cortex</article-title>. <source>Elife</source> <volume>7</volume>, <elocation-id>e37349</elocation-id> (<year>2018</year>).</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stepanyants</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Martinez</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Ferecskó</surname>, <given-names>A. S.</given-names></string-name> &amp; <string-name><surname>Kisvárday</surname>, <given-names>Z. F.</given-names></string-name></person-group> <article-title>The fractions of short-and long-range connections in the visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>, <fpage>3555</fpage>–<lpage>3560</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Safavi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Chalk</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> &amp; <string-name><surname>Levina</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Signatures of criticality in efficient coding networks</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>121</volume>, <fpage>e2302730121</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname>, <given-names>N. R.</given-names></string-name>, <string-name><surname>Runyan</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>F. L.</given-names></string-name> &amp; <string-name><surname>Sur</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Division and subtraction by distinct cortical inhibitory networks in vivo</article-title>. <source>Nature</source> <volume>488</volume>, <fpage>343</fpage>–<lpage>348</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Valente</surname>, <given-names>M.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Correlations enhance the behavioral readout of neural population activity in association cortex</article-title>. <source>Nature Neuroscience</source> <volume>24</volume>, <fpage>975</fpage>–<lpage>986</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Panzeri</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Moroni</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Safaai</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>C. D.</given-names></string-name></person-group> <article-title>The structures and functions of correlations in neural population codes</article-title>. <source>Nature Reviews Neuroscience</source> <volume>23</volume>, <fpage>551</fpage>–<lpage>567</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c106"><label>106.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manning</surname>, <given-names>T. S.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Transformations of sensory information in the brain suggest changing criteria for optimality</article-title>. <source>PLoS Comput Biol</source> <volume>20</volume>, <fpage>e1011783</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blanco Malerba</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Micheli</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Woodford</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Azeredo da Silveira</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Jointly efficient encoding and decoding in neural populations</article-title>. <source>PLoS Comput Biol</source> <volume>20</volume>, <fpage>e1012240</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c108"><label>108.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Whittaker</surname>, <given-names>V.</given-names></string-name></person-group> <chapter-title>What is Dale’s principle?</chapter-title> In <person-group person-group-type="editor"><string-name><surname>Osborne</surname>, <given-names>N. N.</given-names></string-name></person-group> (ed.) <source>Dale’s Principle and Communication Between Neurones</source>, <fpage>1</fpage>–<lpage>5</lpage> (<publisher-name>Pergamon</publisher-name>, <year>1983</year>).</mixed-citation></ref>
<ref id="c109"><label>109.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muller</surname>, <given-names>M. E.</given-names></string-name></person-group> <article-title>A note on a method for generating points uniformly on n-dimensional spheres</article-title>. <source>Communications of the ACM</source> <volume>2</volume>, <fpage>19</fpage>–<lpage>20</lpage> (<year>1959</year>).</mixed-citation></ref>
<ref id="c110"><label>110.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Luo</surname>, <given-names>C.</given-names></string-name> <etal>et al..</etal></person-group> <article-title>Cosine normalization: Using cosine similarity instead of dot product in neural networks</article-title>. In <conf-name>Artificial Neural Networks and Machine Learning–ICANN 2018: 27th International Conference on Artificial Neural Networks</conf-name>, vol. <volume>I</volume> <issue>27</issue>, <fpage>382</fpage>–<lpage>391</lpage> (<publisher-name>Springer</publisher-name>, <year>2018</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study offers a <bold>valuable</bold> treatment of how the population of excitatory and inhibitory neurons integrates principles of energy efficiency in their coding strategies. The <bold>convincing</bold> analysis provides a comprehensive characterisation of the model, highlighting the structured connectivity between excitatory and inhibitory neurons. The role of the many free parameters are discussed and studied in depth.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Koren et al. derive and analyse a spiking network model optimised to represent external signals using the minimum number of spikes. Unlike most prior work using a similar setup, the network includes separate populations of excitatory and inhibitory neurons. The authors show that the optimised connectivity has a like-to-like structure, which leads to the experimentally observed phenomenon of feature competition. The authors also examine how various (hyper)parameters-such as adaptation timescale, the excitatory-to-inhibitory cell ratio, regularization strength, and background current-affect the model. These findings add biological realism to a specific implementation of efficient coding. They show that efficient coding explains, or at least is consistent with, multiple experimentally observed properties of excitatory and inhibitory neurons.</p>
<p>As discussed in the first round of reviews, the model's ability to replicate biological observations such as the 4:1 ratio of excitatory vs. inhibitory neurons hinges on somewhat arbitrary hyperparameter choices. Although this may limit the model's explanatory power, the authors have made significant efforts to explore how these parameters influence their model. It is an empirical question whether the uncovered relationships between, e.g., metabolic cost and the fraction of excitatory neurons are biologically relevant.</p>
<p>The revised manuscript is also more transparent about the model's limitations, such as the lack of excitatory-excitatory connectivity. Further improvements could come from explicitly acknowledging additional discrepancies with biological data, such as the widely reported weak stimulus tuning of inhibitory neurons in the primary sensory cortex of untrained animals.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this work, the authors present a biologically plausible, efficient E-I spiking network model and study various aspects of the model and its relation to experimental observations. This includes a derivation of the network into two (E-I) populations, the study of single-neuron perturbations and lateral-inhibition, the study of the effects of adaptation and metabolic cost, and considerations of optimal parameters. From this, they conclude that their work puts forth a plausible implementation of efficient coding that matches several experimental findings, including feature-specific inhibition, tight instantaneous balance, a 4 to 1 ratio of excitatory to inhibitory neurons, and a 3 to 1 ratio of I-I to E-I connectivity strength.</p>
<p>Strengths:</p>
<p>While many network implementations of efficient coding have been developed, such normative models are often abstract and lacking sufficient detail to compare directly to experiments. The intention of this work to produce a more plausible and efficient spiking model and compare it with experimental data is important and necessary in order to test these models. In rigorously deriving the model with real physical units, this work maps efficient spiking networks onto other more classical biophysical spiking neuron models. It also attempts to compare the model to recent single-neuron perturbation experiments, as well as some long-standing puzzles about neural circuits, such as the presence of separate excitatory and inhibitory neurons, the ratio of excitatory to inhibitory neurons, and E/I balance. One of the primary goals of this paper, to determine if these are merely biological constraints or come from some normative efficient coding objective, is also important. Lastly, though several of the observations have been reported and studied before, this work arguably studies them in more depth, which could be useful for comparing more directly to experiments.</p>
<p>Weaknesses:</p>
<p>This work is the latest among a line of research papers studying the properties of efficient spiking networks. Many of the characteristics and findings here have been discussed before, thereby limiting the new insights that this work can provide. Thus, the conclusions of this work should be considered and understood in the context of those previous works, as the authors state. Furthermore, the number of assumptions and free parameters in the model, though necessary to bring the model closer to biophysical reality, make it more difficult to understand and to draw clear conclusions from. As the authors state, many of the optimality claims depend on these free parameters, such as the dimensionality of the input signal (M=3), the relative weighting of encoding error and metabolic cost, and several others. This raises the possibility that it is not the case that the set of biophysical properties measured in the brain are accounted for by efficient coding, but rather that theories of efficient coding are flexible enough to be consistent with this regime. With this in mind, some of the conclusions made in the text may be overstated and should be considered in this light.</p>
<p>Conclusions, Impact, and additional context:</p>
<p>Notions of optimality are important for normative theories, but they are often studied in simple models with as few free parameters as possible. Biophysically detailed and mechanistic models, on the other hand, will often have many free parameters by their very nature, thereby muddying the connection to optimality. This tradeoff is an important concern in neuroscientific models. Previous efficient spiking models have often been criticized for their lack of biophysically-plausible characteristics, such as large synaptic weights, dense connectivity, and instantaneous communication. This work is an important contribution in showing that such networks can be modified to be much closer to biophysical reality without losing their essential properties. Though the model presented does suffer from complexity issues which raise questions about its connections to &quot;optimal&quot; efficient coding, the extensive study of various parameter dependencies offers a good characterization of the model and puts its conclusions in context.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In their paper the authors tackle three things at once in a theoretical model: how can spiking neural networks perform efficient coding, how can such networks limit the energy use at the same time, and how can this be done in a more biologically realistic way than previous work.</p>
<p>They start by working from a long-running theory on how networks operating in a precisely balanced state can perform efficient coding. First, they assume split networks of excitatory (E) and inhibitory (I) neurons. The E neurons have the task to represent some lower dimensional input signal, and the I neurons have the task to represent the signal represented by the E neurons. Additionally, the E and I populations should minimize an energy cost represented by the sum of all spikes. All this results in two loss functions for the E and I populations, and the networks are then derived by assuming E and I neurons should only spike if this improves their respective loss. This results in networks of spiking neurons that live in a balanced state, and can accurately represent the network inputs.</p>
<p>They then investigate in depth different aspects of the resulting networks, such as responses to perturbations, the effect of following Dale's law, spiking statistics, the excitation (E)/inhibition (I) balance, optimal E/I cell ratios, and others. Overall, they expand on previous work by taking a more biological angle on the theory and show the networks can operate in a biologically realistic regime.</p>
<p>Strengths:</p>
<p>* The authors take a much more biological angle on the efficient spiking networks theory than previous work, which is an essential contribution to the field</p>
<p>
* They make a very extensive investigation of many aspects of the network in this context, and do so thoroughly</p>
<p>
* They put sensible constraints on their networks, while still maintaining the good properties these networks should have</p>
<p>Weaknesses:</p>
<p>* One of the core goals of the paper is to make a more biophysically realistic network than previous work using similar optimization principles. One of the important things they consider is a split into E and I neurons. While this works fine, and they consider the coding consequences of this, it is not clear from an optimization perspective why the split into E and I neurons and following Dale's law would be beneficial. This would be out of scope for the current paper however.</p>
<p>
* The theoretical advances in the paper are not all novel by themselves, as most of them (in particular the split into E and I neurons and the use of biophysical constants) had been achieved in previous models. However, the authors discuss these links thoroughly and do more in-depth follow-up experiments with the resulting model.</p>
<p>Assessment and context:</p>
<p>Overall, although much of the underlying theory is not necessarily new, the work provides an important addition to the field. The authors succeeded well in their goal of making the networks more biologically realistic, and incorporate aspects of energy efficiency. For computational neuroscientists this paper is a good example of how to build models that link well to experimental knowledge and constraints, while still being computationally and mathematically tractable. For experimental readers the model provides a clearer link of efficient coding spiking networks to known experimental constraints and provides a few predictions.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.99545.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Koren</surname>
<given-names>Veronika</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2920-2717</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Blanco Malerba</surname>
<given-names>Simone</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4467-5988</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Schwalger</surname>
<given-names>Tilo</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5422-3723</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Panzeri</surname>
<given-names>Stefano</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1700-8909</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>eLife Assessment</bold></p>
<p>This study offers a useful treatment of how the population of excitatory and inhibitory neurons integrates principles of energy efficiency in their coding strategies. The analysis provides a comprehensive characterisation of the model, highlighting the structured connectivity between excitatory and inhibitory neurons. However, the manuscript provides an incomplete motivation for parameter choices. Furthermore, the work is insufficiently contextualized within the literature, and some of the findings appear overlapping and incremental given previous work.</p>
</disp-quote>
<p>We are genuinely grateful to the Editors and Reviewers for taking time to provide extremely valuable suggestions and comments, which will help us to substantially improve our paper. We decided to do our very best to implement all suggestions, as detailed in the point-by-point rebuttal letter below. We feel that our paper has improved considerably as a result.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary: Koren et al. derive and analyse a spiking network model optimised to represent external signals using the minimum number of spikes. Unlike most prior work using a similar setup, the network includes separate populations of excitatory and inhibitory neurons. The authors show that the optimised connectivity has a like-to-like structure, leading to the experimentally observed phenomenon of feature competition. They also characterise the impact of various (hyper)parameters, such as adaptation timescale, ratio of excitatory to inhibitory cells, regularisation strength, and background current. These results add useful biological realism to a particular model of efficient coding. However, not all claims seem fully supported by the evidence. Specifically, several biological features, such as the ratio of excitatory to inhibitory neurons, which the authors claim to explain through efficient coding, might be contingent on arbitrary modelling choices. In addition, earlier work has already established the importance of structured connectivity for feature competition. A clearer presentation of modelling choices, limitations, and prior work could improve the manuscript.</p>
</disp-quote>
<p>Thanks for these insights and for this summary of our work.</p>
<disp-quote content-type="editor-comment">
<p>Major comments:</p>
<p>(1) Much is made of the 4:1 ratio between excitatory and inhibitory neurons, which the authors claim to explain through efficient coding. I see two issues with this conclusion: (i) The 4:1 ratio is specific to rodents; humans have an approximate 2:1 ratio (see Fang &amp; Xia et al., Science 2022 and references therein); (ii) the optimal ratio in the model depends on a seemingly arbitrary choice of hyperparameters, particularly the weighting of encoding error versus metabolic cost. This second concern applies to several other results, including the strength of inhibitory versus excitatory synapses. While the model can, therefore, be made consistent with biological data, this requires auxiliary assumptions.</p>
</disp-quote>
<p>We now describe better the ratio of numbers of E and I neurons found in real data, as suggested. The first submission already contained an analysis of how the optimal ratio of E vs I neuron numbers depends in our model on the relative weighting of the loss of E and I neurons and on the relative weighting of the encoding error vs the metabolic cost in the loss function (see Fig. 7E). We revised the text on page 12 describing Fig. 7E.</p>
<p>To allow readers to form easily a clear idea of how the weighting of the error vs the cost may influence the optimal network configuration, we now present how optimal parameters depend on the weighting in a systematic way, by always including this type of analysis when studying all other model parameters (time constants of single E and I neurons, noise intensity, metabolic constant, ratio of mean I-I to E-I connectivity). These results are shown on the Supplementary Fig. S4 A-D and H, and we comment briefly on each of them in Results sections (pages 9, 10, 11 and 12) that analyze each of these parameters.</p>
<p>Following this Reviewer’s comment, we now included a joint analysis of network performance relative to the ratio of E-I neuron numbers and the ratio of mean I-I to E-I connectivity (Fig. 7J). We found a positive correlation between optima values of these two ratios. This implies that a lower ratio of E-I neuron numbers, such as a 2:1 ratio in human cortex mentioned by the reviewer, predicts lower optimal ratio of I-I to E-I connectivity and thus weaker inhibition in the network. We made sure that this finding is suitably described in revision (page 13).</p>
<disp-quote content-type="editor-comment">
<p>(2) A growing body of evidence supports the importance of structured E-I and I-E connectivity for feature selectivity and response to perturbations. For example, this is a major conclusion from the Oldenburg paper (reference 62 in the manuscript), which includes extensive modelling work. Similar conclusions can be found in work from Znamenskiy and colleagues (experiments and spiking network model; bioRxiv 2018, Neuron 2023 (ref. 82)), Sadeh &amp; Clopath (rate network; eLife, 2020), and Mackwood et al. (rate network with plasticity; eLife, 2021). The current manuscript adds to this evidence by showing that (a particular implementation of) efficient coding in spiking networks leads to structured connectivity. The fact that this structured connectivity then explains perturbation responses is, in the light of earlier findings, not new.</p>
</disp-quote>
<p>We agree that the main contribution of our manuscript in this respect is to show how efficient coding in spiking networks can lead to structured connectivity implementing lateral inhibition similar to that proposed in the recent studies mentioned by the Reviewer. We apologize if this was not clear enough in the previous version. We streamlined the presentation to make it clearer in revision.  We nevertheless think it useful to report the effects of perturbations within this network because these results give information about how lateral inhibition works in our network. Thus, we kept presenting it in the revised version, although we de-emphasized and simplified its presentation. We now give more emphasis to the novelty of the derivation of this connectivity rule from the principles of efficient coding (pages 4 and 6). We also describe better (page 8) what the specific results of our simulated perturbation experiments add to the existing literature.</p>
<disp-quote content-type="editor-comment">
<p>(3) The model's limitations are hard to discern, being relegated to the manuscript's last and rather equivocal paragraph. For instance, the lack of recurrent excitation, crucial in neural dynamics and computation, likely influences the results: neuronal time constants must be as large as the target readout (Figure 4), presumably because the network cannot integrate the signal without recurrent excitation. However, this and other results are not presented in tandem with relevant caveats.</p>
</disp-quote>
<p>We improved the Limitations paragraph in Discussion, and also anticipated caveats in tandem with results when needed, as suggested.</p>
<p>We now mention the assumption of equal time constants between the targets and readouts in the Abstract.</p>
<p>We now added the analysis of the network performance and dynamics as a function of the time constant of the target (t<sub>x</sub>) to the Supplementary Fig S5 (C-E). These results are briefly discussed in text on page 13. The only measure sensitive to t<sub>x</sub> is the encoding error of E neurons, with a minimum at t<sub>x</sub> =9 ms, while I neurons and metabolic cost show no dependency. Firing rates, variability of spiking as well as the average and instantaneous balance show no dependency on t<sub>x</sub>. We note that t<sub>x</sub> = t, with t=1/l the time constant of the population readout (Eq. 9), is an assumption we use when we derive the model from the efficiency objective (Eq. 18 to 23). In our new and preliminary work (Koren, Emanuel, Panzeri, Biorxiv 2024), we derived a more general class of models where this assumption is relaxed, which gives a network with E-E connectivity that adapts to the time constant of the stimulus. Thus, the reviewer is correct in the intuition that the network requires E-E connectivity to better integrate target signals with a different time constant than the time constant of the membrane. We now better emphasize this limitation in Discussion (page 16).</p>
<disp-quote content-type="editor-comment">
<p>(4) On repeated occasions, results from the model are referred to as predictions claimed to match the data. A prediction is a statement about what will happen in the future – but most of the “predictions” from the model are actually findings that broadly match earlier experimental results, making them “postdictions”.</p>
<p>This distinction is important: compared to postdictions, predictions are a much stronger test because they are falsifiable. This is especially relevant given (my impression) that key parameters of the model were tweaked to match the data.</p>
</disp-quote>
<p>We now comment on every result from the model as either matching earlier experimental results, or being a prediction for experiments.</p>
<p>In Section “Assumptions and emergent properties of the efficient E-I network derived from first principles”, we report (page 4) that neural networks have connectivity structure that relates to tuning similarity of neurons (postdiction).</p>
<p>In Section “Encoding performance and neural dynamics in an optimally efficient E-I network” we report (page 5) that in a network with optimal parameters, I neurons have higher firing rate than E neurons (postdiction), that single neurons show temporally correlated synaptic currents (postdiction) and that the distribution of firing rates across neurons is log-normal (postdiction).</p>
<p>In Section “Competition across neurons with similar stimulus tuning emerging in efficient spiking networks” we report (page 6)  that the activity perturbation of E neurons induces lateral inhibition on other E neurons, and that the strength of lateral inhibition depends on tuning similarity (postdiction). We show that activity perturbation of E neurons induces lateral excitation in I neurons (prediction). We moreover show that the specific effects of the perturbation of neural activity rely on structured E-I-E connectivity (prediction for experiments, but similar result in Sadeh and Clopath, 2020). We show strong voltage correlations but weak spike-timing correlations in our network (prediction for experiments, but similar result in Boerlin et al. 2013).</p>
<p>In Section “The effect of structured connectivity on coding efficiency and neural dynamics”, we report (page 7) that our model predicts a number of differences between networks with structured and unstructured (random) connectivity. In particular, structured networks differ from unstructured ones by showing better encoding performance, lower metabolic cost, weaker variance over time in the membrane potential of each neuron, lower firing rates and weaker average and instantaneous balance of synaptic currents.</p>
<p>In Section “Weak or no spike-triggered adaptation optimizes network efficiency”, we report (page 9) that our model predicts better encoding performance in networks with adaptation compared to facilitation. Our results suggest that adaptation should be stronger in E compared to I (PV+) neurons (postdiction). In the same section, we report (page 10) that our results suggest that the instantaneous balance is a better predictor of model efficiency than average balance (prediction).</p>
<p>In Section “Non-specific currents regulate network coding properties”, we report (page 10) that our model predicts that more than half of the distance between the resting potential and firing threshold is taken by external currents that are unrelated to feedforward processing (postdiction). We also report (page 11) that our model predicts that moderate levels of uncorrelated (additive) noise is beneficial for efficiency (prediction for experiments, but similar results in Chalk et al., 2016, Koren et al., 2017, Timcheck et al. 2022).</p>
<p>In Section “Optimal ratio of E-I neuron numbers and of mean I-I to E-I synaptic efficacy coincide with biophysical measurements”, we predict the optimal ratio of E to I neuron numbers to be 4:1 (postdiction) and the optimal ratio of mean I-I to E-I connectivity to be 3:1 (postdiction). Further, we report (page 13) that our results predict that a decrease in the ratio of E-I neuron numbers is accompanied with the decrease in the ratio of mean I-I to E-I connectivity.</p>
<p>Finally, in Section “Dependence of efficient coding and neural dynamics on the stimulus statistics”, we report (page 13) that our model predicts that the efficiency of the network has almost no dependence on the time scale of the stimulus (prediction).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>In this work, the authors present a biologically plausible, efficient E-I spiking network model and study various aspects of the model and its relation to experimental observations. This includes a derivation of the network into two (E-I) populations, the study of single-neuron perturbations and lateral-inhibition, the study of the effects of adaptation and metabolic cost, and considerations of optimal parameters. From this, they conclude that their work puts forth a plausible implementation of efficient coding that matches several experimental findings, including feature-specific inhibition, tight instantaneous balance, a 4 to 1 ratio of excitatory to inhibitory neurons, and a 3 to 1 ratio of I-I to E-I connectivity strength. It thus argues that some of these observations may come as a direct consequence of efficient coding.</p>
<p>Strengths:</p>
<p>While many network implementations of efficient coding have been developed, such normative models are often abstract and lacking sufficient detail to compare directly to experiments. The intention of this work to produce a more plausible and efficient spiking model and compare it with experimental data is important and necessary in order to test these models.</p>
<p>In rigorously deriving the model with real physical units, this work maps efficient spiking networks onto other more classical biophysical spiking neuron models. It also attempts to compare the model to recent single-neuron perturbation experiments, as well as some longstanding puzzles about neural circuits, such as the presence of separate excitatory and inhibitory neurons, the ratio of excitatory to inhibitory neurons, and E/I balance. One of the primary goals of this paper, to determine if these are merely biological constraints or come from some normative efficient coding objective, is also important.</p>
<p>Though several of the observations have been reported and studied before (see below), this work arguably studies them in more depth, which could be useful for comparing more directly to experiments.</p>
</disp-quote>
<p>Thanks for these insights and for the kind words of appreciation of the strengths of our work.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>Though the text of the paper may suggest otherwise, many of the modeling choices and observations found in the paper have been introduced in previous work on efficient spiking models, thereby making this work somewhat repetitive and incremental at times. This includes the derivation of the network into separate excitatory and inhibitory populations, discussion of physical units, comparison of voltage versus spike-timing correlations, and instantaneous E/I balance, all of which can be found in one of the first efficient spiking network papers (Boerlin et al. 2013), as well as in subsequent papers. Metabolic cost and slow adaptation currents were also presented in a previous study (Gutierrez &amp; Deneve 2019). Though it is perfectly fine and reasonable to build upon these previous studies, the language of the text gives them insufficient credit.</p>
</disp-quote>
<p>We indeed built our work on these important previous studies, and we apologize if this was not clear enough. We thus improved the text to make sure that credit to previous studies is more precisely and more clearly given (see detailed reply for the list of changes made).</p>
<p>To facilitate the understanding on how we built on previous work, we expanded the comparison of our results with the results of Boerlin et al. (2013) about voltage correlations and uncorrelated spiking (page 7), comparison with the derivation of physical units of Boerlin et al. (2013) (page 3), discussion of how results on the ratio of the number of E to I neurons relate  to Calaim et al (2022) and Barrett et al. (2016) (page 16), and comment on the previous work by Gutierrez and Deneve about adaptation (page 8).</p>
<disp-quote content-type="editor-comment">
<p>Furthermore, the paper makes several claims of optimality that are not convincing enough, as they are only verified by a limited parameter sweep of single parameters at a time, are unintuitive and may be in conflict with previous findings of efficient spiking networks. This includes the following.</p>
<p>Coding error (RMSE) has a minimum at intermediate metabolic cost (Figure 5B), despite the fact that intuitively, zero metabolic cost would indicate that the network is solely minimizing coding error and that previous work has suggested that additional costs bias the output.</p>
<p>Coding error also appears to have a minimum at intermediate values of the ratio of E to I neurons (effectively the number of I neurons) and the number of encoded variables (Figures 6D, 7B). These both have to do with the redundancy in the network (number of neurons for each encoded variable), and previous work suggests that networks can code for arbitrary numbers of variables provided the redundancy is high enough (e.g., Calaim et al. 2022).</p>
<p>Lastly, the performance of the E-I variant of the network is shown to be better than that of a single cell type (1CT: Figure 7C, D). Given that the E-I network is performing a similar computation as to the 1CT model but with more neurons (i.e., instead of an E neuron directly providing lateral inhibition to its neighbor, it goes through an interneuron), this is unintuitive and again not supported by previous work. These may be valid emergent properties of the E-I spiking network derived here, but their presentation and description are not sufficient to determine this.</p>
</disp-quote>
<p>With regard to the concern that our previous analyses considered optimal parameter sets determined with a sweep of a single parameter at a time, we have addressed this issue in two ways. First, we presented (Figure 6I and 7J and text on pages 11 and 13) results of joint sweeps of variations of pairs of parameters whose joint variations are expected to influence optimality in a way that cannot be understood varying one parameter at a time. These new analyses complement the joint parameter sweep of the time constants of single E and I neurons (t<sub>r</sub><sup>E</sup> and t<sub>r</sub><sup>I</sup>) that has already been presented in Fig. 5A (former Fig. 4A). Second, we conducted, within a reasonable/realistic range of possible variations of each individual parameter, a Monte-Carlo random joint sampling (10000 simulations with 20 trials each) of all 6 model parameters that we explored in the paper. We presented these new results on Fig. 2 and discuss it on pages 5-6.</p>
<p>The Reviewer is correct in stating that the error (RMSE) exhibits a counterintuitive minimum as a function of the metabolic constant despite the fact that, intuitively, for vanishing metabolic constant the network is solely minimizing the coding error (Fig. 6B). In our understanding, this counterintuitive finding is due to the presence of noise in the membrane potential dynamics. In the presence of noise, a non-vanishing metabolic constant is needed to suppress “inefficient” spikes purely induced by noise that do not contribute to coding and increase the error. This gives rise to a form of “stochastic resonance”, where the noise improves detection of the signal coming from the feedforward currents. We note that the metabolic constant and the noise variance both appear in the non-specific external current (Eq. 29f in Methods), and, thus, a covariation in their optimal values is expected. Indeed, we find that the optimal metabolic constant monotonically increases as a function of the noise variance, with stronger regularization (larger beta) required to compensate for larger variability (larger sigma) (Fig. 6I). Finally, we note that a moderate level of noise (which, in turn, induces a non-trivial minimum of the coding error as a function of beta) in the network is optimal. The beneficial effect of moderate levels of noise on performance in networks with efficient coding has been shown in different contexts in previous work (Chalk et al. 2016, Koren and Deneve, 2017). The intuition is that the noise prevents the excessive synchronization of the network and insufficient single neuron variability that decrease the performance. The points above are now explained in the revised text on page 11.</p>
<p>The Reviewer is also correct in stating that the network exhibits an optimal performance for intermediate values of the number of I neurons and the number of encoded features. In our understanding, the optimal number of encoded features of M=3 arises simply because all the other parameters were optimized for those values of M. The purpose of those analyses was not to state that a network optimally encodes only a given number of features, but how a network whose parameters are optimized for a given M perform reasonably well when M is varied. We clarify this on page 13 of Results in Discussion on page 16. In the same Discussion paragraph we refer also to the results of Calaim et al mentioned by the Reviewer.</p>
<p>To address the concern about the comparison of efficiency between the E-I and the 1CT model, we took advantage of the Reviewer’s suggestions to consider this issue more deeply. In revision, we now compare the efficiency of the 1CT model with the E population of the E-I model (Fig. 8H). This new comparison changes the conclusion about which model is more efficient, as it shows the 1CT model is slightly more efficient than the E-I model. Nevertheless, the E-I model performance is more robust to small variations of optimal parameters, e.g., it exhibits biologically plausible firing rates for non-optimal values of the metabolic constant. See also the reply to point 3 of the Public Review of Reviewer 2 for more detail. We added these results and the ensuing caveats for the interpretation of this comparison on Page 14, and also revised the title of the last subsection of Results.</p>
<disp-quote content-type="editor-comment">
<p>Alternatively, the methodology of the model suggests that ad hoc modeling choices may be playing a role. For example, an arbitrary weighting of coding error and metabolic cost of 0.7 to 0.3, respectively, is chosen without mention of how this affects the results. Furthermore, the scaling of synaptic weights appears to be controlled separately for each connection type in the network (Table 1), despite the fact that some of these quantities are likely linked in the optimal network derivation. Finally, the optimal threshold and metabolic constants are an order of magnitude larger than the synaptic weights (Table 1). All of these considerations suggest one of the following two possibilities. One, the model has a substantial number of unconstrained parameters to tune, in which case more parameter sweeps would be necessary to definitively make claims of optimality. Or two, parameters are being decoupled from those constrained by the optimal derivation, and the optima simply corresponds to the values that should come out of the derivation.</p>
</disp-quote>
<p>We thank the reviewer for bringing about these important questions.</p>
<p>In the first submission, we presented both the encoding error and the metabolic cost separately as a function of the parameters, so that readers could get an understanding of how stable optimal parameters would be to the change of the relative weighting of encoding error and metabolic cost. We specified this in Results (page 5) and we kept presenting separately encoding and metabolic terms in the revision.</p>
<p>However, we agree that it is important to present the explicit quantification on how the optimal parameters may depend on g<sub>L</sub>. In the first submission, we showed the analysis for all possible weightings in case of two parameters for which we found this analysis was the most relevant – the ratio of neuron numbers (Fig. 7E, Fig. 6E in first submission) and the optimal number of input features M (see last paragraph on page 13 and Fig. 8D). We now show this analysis also for the rest of studied model parameters in the Supplementary Fig. S4 (A-D and H). This is discussed on pages 9, 10,11 and 12.</p>
<p>With regard to the concern that the scaling of synaptic weights should not be controlled separately for each connection type in the network, we agree and we would like to clarify that we did not control such scaling separately. Apologies if this was not clear enough. From the optimal analytical solution, we obtained that the connectivity scales with the standard deviation of decoding weights (s<sub>w</sub><sup>E</sup> and s<sub>w</sub><sup>I</sup>) of the pre and postsynaptic populations (Methods, Eq. 32). We studied the network properties as a function of the ratio of average I-I to E-I connectivity (Fig. 7 F-I; Supplementary Fig. S4 D-H), which is equivalent to the ratio of standard deviations s<sub>w</sub><sup>I</sup> /s<sub>w</sub><sup>E</sup> (see Methods, Eq. 35). We clarified this in text on page 12.</p>
<p>Next, it is correct that our synaptic weights are an order of magnitude smaller than the metabolic constant. We analysed a simpler version of the network that has the coding and dynamics identical to our full model (Methods, Eq. 25) but without the external currents. We found that the optimal parameters determining the firing threshold in such a simpler network were biologically implausible (see Supplementary Text 2 and Supplementary Table S1). We considered as another simple solution the rescaling of the synaptic efficacy such as to have biologically plausible threshold. However, that gave implausible mean synaptic efficacy (see Supplementary Text 2).  Thus, to be able to define a network with biologically plausible firing threshold and mean synaptic efficacy, we introduced the non-specific external current. After introducing such current, we were able to shift the firing threshold to biologically plausible values while keeping realistic values of mean synaptic efficacy. Biologically plausible values for the firing threshold are around 15 -– 20 mV above the resting potential (Constantinople and Bruno, 2013), which is the value that we have in our model. A plausible value for the average synaptic strength is between a fraction of one millivolt to a couple of millivolts (Constantinople &amp; Bruno, 2013, Campagnola et al. 2022), which also corresponds to values that the synaptic weights take. The above results are briefly explained in the revised text on page 4.</p>
<p>Finally, to study the optimality of the network when changing multiple parameters at a time, we added a new analysis with Monte-Carlo random joint sampling (10.000 parameter sets with 20 trials for each set) of all 6 model parameters that we explored in the paper. We compared (Fig 2) the so-obtained results of each simulation with those obtained from the understanding gained from varying one or two parameters at a time (optimal parameters reported in Table 1 and used throughout the paper).  We found (Fig. 2) that the optimal configuration in Table 1 was never improved by any other simulations we performed, and that the first three random simulations that came the closest to the optimal one of Table 1 had stronger noise intensity but also stronger metabolic cost than the configuration on Table 1. The second, third and fourth configurations had longer time constants of both E and I single neurons (adaptation time constants). Ratio of E-I neuron numbers and of I-I to E-I connectivity in the second, third and fourth best configuration were either jointly increased or decreased with respect to our configuration. These results are reported on Fig. 2 and in Tables 2-3 and they are discussed in Results (page 5).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>In their paper the authors tackle three things at once in a theoretical model: how can spiking neural networks perform efficient coding, how can such networks limit the energy use at the same time, and how can this be done in a more biologically realistic way than previous work?</p>
<p>They start by working from a long-running theory on how networks operating in a precisely balanced state can perform efficient coding. First, they assume split networks of excitatory (E) and inhibitory (I) neurons. The E neurons have the task to represent some lower dimensional input signal, and the I neurons have the task to represent the signal represented by the E neurons. Additionally, the E and I populations should minimize an energy cost represented by the sum of all spikes. All this results in two loss functions for the E and I populations, and the networks are then derived by assuming E and I neurons should only spike if this improves their respective loss. This results in networks of spiking neurons that live in a balanced state, and can accurately represent the network inputs.</p>
<p>They then investigate in-depth different aspects of the resulting networks, such as responses to perturbations, the effect of following Dale's law, spiking statistics, the excitation (E)/inhibition (I) balance, optimal E/I cell ratios, and others. Overall, they expand on previous work by taking a more biological angle on the theory and showing the networks can operate in a biologically realistic regime.</p>
<p>Strengths:</p>
<p>(1) The authors take a much more biological angle on the efficient spiking networks theory than previous work, which is an essential contribution to the field.</p>
<p>(2) They make a very extensive investigation of many aspects of the network in this context, and do so thoroughly.</p>
<p>(3) They put sensible constraints on their networks, while still maintaining the good properties these networks should have.</p>
</disp-quote>
<p>Thanks for this summary and for these kind words of appreciation of the strengths of our work.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) The paper has somewhat overstated the significance of their theoretical contributions, and should make much clearer what aspects of the derivations are novel. Large parts were done in very similar ways in previous papers. Specifically: the split into E and I neurons was also done in Boerlin et al (2008) and in Barrett et al (2016). Defining the networks in terms of realistic units was already done by Boerlin et al (2008). It would also be worth it to discuss Barrett et al (2016) specifically more, as there they also use split E/I networks and perform biologically relevant experiments.</p>
</disp-quote>
<p>We improved the text to make sure that credit to previous studies is more precisely and more clearly given (see rebuttal to the specific suggestions of Reviewer 2 for a full list).</p>
<p>We apologize if this was not clear enough in the previous version.</p>
<p>With regard to the specific point raised here about the E-I split, we revised the text on page 2. With regard to the realistic units, we revised the text on page 3. Finally, we commented on relation between our results and results of the study by Barrett et al. (2016) on page 16.</p>
<disp-quote content-type="editor-comment">
<p>(2) It is not clear from an optimization perspective why the split into E and I neurons and following Dale's law would be beneficial. While the constraints of Dale's law are sensible (splitting the population in E and I neurons, and removing any non-Dalian connection), they are imposed from biology and not from any coding principles. A discussion of how this could be done would be much appreciated, and in the main text, this should be made clear.</p>
</disp-quote>
<p>We indeed removed non-Dalian connections because Dale’s law is a major constraint for biological plausibility. Our logic was to consider efficient coding within the space of networks that satisfy this (and other) biological plausibility constraints. We did not intend to claim that removing the non-Dalian connections was the result of an analytical optimization. We clarified this in revision (page 4).</p>
<disp-quote content-type="editor-comment">
<p>(3) Related to the previous point, the claim that the network with split E and I neurons has a lower average loss than a 1 cell-type (1-CT) network seems incorrect to me. Only the E population coding error should be compared to the 1-CT network loss, or the sum of the E and I populations (not their average). In my author recommendations, I go more in-depth on this point.</p>
</disp-quote>
<p>We carefully considered these possibilities and decided to compare only the E population of the E-I model with the 1-CT model. On Fig.8G (7C of the first submission), E neurons have a slightly higher error and cost compared to the 1CT network. In the revision, we compared the loss of E neurons of the E-I model with the loss of the 1-CT model. Using such comparison, we found that the 1CT network has lower loss and is more efficient compared to E neurons of the E-I model. We revised Figure 8H and text on page 14 to address this point.</p>
<disp-quote content-type="editor-comment">
<p>(4) While the paper is supposed to bring the balanced spiking networks they consider in a more experimentally relevant context, for experimental audiences I don't think it is easy to follow how the model works, and I recommend reworking both the main text and methods to improve on that aspect.</p>
</disp-quote>
<p>We tried to make the presentation of the model more accessible to a non-computational audience in the revised paper. We carefully edited the text throughout to make it as accessible as possible.</p>
<disp-quote content-type="editor-comment">
<p>Assessment and context:</p>
<p>Overall, although much of the underlying theory is not necessarily new, the work provides an important addition to the field. The authors succeeded well in their goal of making the networks more biologically realistic, and incorporating aspects of energy efficiency. For computational neuroscientists, this paper is a good example of how to build models that link well to experimental knowledge and constraints, while still being computationally and mathematically tractable. For experimental readers, the model provides a clearer link between efficient coding spiking networks to known experimental constraints and provides a few predictions.</p>
</disp-quote>
<p>Thanks for these kind words. We revised the paper to make sure that these points emerge more clearly and in a more accessible way from the revised paper.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>Referring to the major comments:</p>
<p>(1) Be upfront about particular modelling choices and why you made them; avoid talk of a &quot;striking/surprising&quot;, etc. ability to explain data when this actually requires otherwise-arbitrary choices and auxiliary assumptions. Ideally, this nuance is already clear from the abstract.</p>
</disp-quote>
<p>We removed all the &quot;striking/surprising&quot; and similar expressions from the text.</p>
<p>We added to the Abstract the assumption of equal time constants of the stimulus and of the membrane of E and I neurons and the assumption of the independence of encoded stimulus features.</p>
<p>In revision, we performed additional analyses (joint parameter sweeps, Monte-Carlo joint sampling of all 6 model parameters) providing additional evidence that the network parameters in Table 1 capture reasonably well the optimal solution. These are reported on Figs. 2, 6I and 7J and in Results (pages 5, 11 and 13). See rebuttal to weaknesses of the public review of the Referee 2 for details.</p>
<disp-quote content-type="editor-comment">
<p>(2) Make even more of an effort to acknowledge prior work on the importance of structured E-I and I-E connectivity.</p>
</disp-quote>
<p>We have revised the text (page 4) to better place our results within previous work on structured E-I and I-E connectivity.</p>
<disp-quote content-type="editor-comment">
<p>(3) Be clear about the model's limitations and mention them throughout the text. This will allow readers to interpret your results appropriately.</p>
</disp-quote>
<p>We now comment more on model's limitations, in particular the simplifying assumption about the network's computation (page 16), the lack of E-E connectivity (page 3), the absence of long-term adaptation (page 10), and the simplification of only having one type of inhibitory neurons (page 16).</p>
<disp-quote content-type="editor-comment">
<p>(4) Present your &quot;predictions&quot; for what they are: aspects of the model that can be made consistent with the existing data after some fitting. Except in the few cases where you make actual predictions, which deserve to be highlighted.</p>
</disp-quote>
<p>We followed the suggestion of the reviewer and distinguished cases where the model is consistent with the data (postdictions) from actual predictions, where empirical measurements are not available or not conclusive. We compiled a list of predictions and postdictions in response to the point 4 of Reviewer 1. In revision, we now comment about every property of the model as either reproducing a known property of biological networks (postdiction) or being a prediction. We improved the text in Results on pages 4, 5, 6, 7, 9, 10, 11, 12 and 13 to accommodate these requests.</p>
<disp-quote content-type="editor-comment">
<p>Minor comments and recommendations</p>
<p>It's a sizable list, but most can be addressed with some text edits.</p>
<p>(1) The image captions should give more details about the simulations and analyses, particularly regarding sample sizes and statistical tests. In Figure 5, for example, it is unclear if the lines represent averages over multiple signals and, if so, how many. It's probably not a single realization, but if it is, this might explain the otherwise puzzling optimal number of three stimuli. Box plots visualize the distribution across simulation trials, but it's not clear how many. In Figure 7d, a star suggests statistical significance, but the caption does not mention the test or its results; the y-axis should also have larger limits.</p>
</disp-quote>
<p>All statistical results were computed on 100 or 200 simulation trials, depending on the figure, with duration of the trial of 1 second of simulated time. To compute statistical results in Fig. 1, we used 10 trials with duration of 10 seconds for each trial. Each trial consisted of M independent realizations of Ornstein-Uhlenbeck (OU) processes as stimuli, independent noise in the membrane potential and an independent draw of tuning parameters, such that the results are general over specific realization of these random variables. Realizations of the OU processes were independent across stimulus dimensions and across trials. We added this information in the caption of each figure.</p>
<p>The optimal number of M=3 stimuli is the result of measuring the performance of the network in 100 simulation trials (for each parameter value), thus following the same procedure as for all other parameters. Boxplots on Fig. 8G-H were also generated from results computed in 100 simulation trials, which we have now specified in the caption of the figure, together with the statistical test used for assessing the significance (twotailed t-test). We also enlarged the limits of Fig. 8H (7D in the previous version).</p>
<disp-quote content-type="editor-comment">
<p>(2) The Oldenburg paper (reference 62) finds suppression of all but nearby neurons in response to two- photon stimulation of small neural ensembles (instead of single neurons, as in Chettih &amp; Harvey). This isn't perfectly consistent with the model's results, even though the Oldenburg experiments seem more relevant given the model's small size, and strong connectivity/high connection probability between similarly tuned neurons. What might explain the potential mismatch?</p>
</disp-quote>
<p>We sincerely apologize for not having been precise enough on this point when comparing our model against Chettih &amp; Harvey and Oldenburg et al. We corrected the sentence (page 6) to remove the claim that our model reproduces both.</p>
<p>We speculate that the discrepancy between perturbing our model and the Oldenburg data may arise from the lack of E-E connectivity in our model. Synaptic connections between E neurons with similar selectivity could create an enhancement instead of suppression between neuronal pairs with very similar tuning. We added a sentence about this in the section with perturbation experiments “Competition across neurons with similar stimulus tuning emerging in efficient spiking networks” (page 7) where we discuss this limitation of our model. We feel that this example shows the utility to derive some perturbation results from our model, as not all networks with some degree of lateral inhibition will show the same perturbation results. Comparing our model's perturbation with real data perturbation results has thus some value to better appreciate strengths and limitations of our approach.</p>
<disp-quote content-type="editor-comment">
<p>(3) &quot;Previous studies optogenetically stimulated E neurons but did not determine whether the recorded neurons were excitatory or inhibitory &quot; (p. 11). I believe Oldenburg et al. did specifically image excitatory neurons.</p>
</disp-quote>
<p>The reviewer is correct about Oldenburg et al. imaging specifically excitatory neurons. We have revised this part of the Discussion (page 15).</p>
<disp-quote content-type="editor-comment">
<p>(4) The authors write that efficiency is particularly achieved where adaptation is stronger in E compared to I neurons (p. 7; Figure 4). Although this would be consistent with experimental data (the I neurons in the model seem akin to fast-spiking Pv+ cells), I struggle to see it in the figure. Instead, it seems like there are roughly two regimes. If either of the neuronal timescales is faster than the stimulus timescale, the optimisation fails. If both are at least as slow, optimisation succeeds.</p>
</disp-quote>
<p>We agree with the reviewer that the adaptation properties of our inhibitory neurons are compatible with Pv+ cells. What is essential for determining the dynamical regime of the network is less the relation to the time constant of the stimulus (t<sub>x</sub>) but rather the relation between the time constant of the population readout (t, which is also the membrane time constant) and the time constant of the single neuron (t<sub>r</sub><sup>y</sup> for y=E and y=I; see Eq. 23, 25 or 29e). The relation between t and t<sub>r</sub><sup>y</sup> determines if single neurons generate spike-triggered adaptation (t<sub>r</sub><sup>y</sup> &gt; t) or spike-triggered facilitation (t<sub>r</sub><sup>y</sup> &lt; t; see Table 4). In regimes with facilitation in either E or I neurons (or both), the network performance strongly deteriorates compared to regimes with adaptation (Fig. 5A).</p>
<p>Beyond adaptation leading to better performance, we also found different effects of adaptation in E and I neurons. We acknowledge that the difference of these effects was difficult to see from the Fig. 4B in the first submission. We have now replotted results from previously shown Fig. 4B to focus on the adaptation regime only, (since the Fig. 5A already establishes that this is the regime with better performance). We also added figures showing the differential effect of adaptation in E and I cell type on the firing rate and on the average loss (Fig. 5C-D). Fig. 5B and C (top plots) show that with adaptation in E neurons, the error and the loss increase more slowly than with adaptation in I neurons. Moreover, the firing rate in both cell types decreases with adaptation in E neurons, while this is not the case with adaptation in I neurons (Fig. 5D). These results are added to the figure panels specified above and discussed in text on page 9.</p>
<p>To clarify the relation between neuronal and stimulus timescale, we now also added the analysis of network performance as a function of the time constant of the stimulus t<sub>x</sub> (Supplementary Fig. S5 C-E). We found that the model's performance is optimal when the time constant of the stimulus is close to the membrane time constant t. This result is expected, because the equality of these time constants was imposed in our analytical derivation of the model (t<sub>x</sub>  = t). We see a similar decrease in performance for values of t<sub>x</sub>  that are faster and slower with respect to the membrane time constant (Supplementary Fig. S5C, top). These results are added to the figure panels specified above and discussed in text on page 13.</p>
<disp-quote content-type="editor-comment">
<p>(5) A key functional property of cortical interneurons is their lower stimulus selectivity. Does the model replicate this feature?</p>
</disp-quote>
<p>We think that whether I neurons are less selective than E neurons is still an open question. A number of recent empirical studies reported that the selectivity of I neurons is comparable to the selectivity of E neurons (see., e.g., Kuan et al. Nature 2024, Runyan et al. Neuron 2010, Najafi et al. Neuron 2020). In our model, the optimal solution prescribes a precise structure in recurrent connectivity (see Eq. 24 and Fig. 1C(ii)) and structured connectivity endows I neurons with stimulus selectivity. To show this, we added plots of example tuning curves and the distribution of the selectivity index across E and I neurons (Fig. 8E-F) and described these new results in Results (page 14). Tuning curves in our network were similar to those computed in a previous work that addressed stimulus tuning in efficient spiking networks (Barrett et al. 2016). We evaluated tuning curves using M=3 constant stimulus features and we varied one of the features while the two others were kept fixed. We provided details on how the tuning curves and the selectivity index were computed in a new Methods subsection (“Tuning curves and selectivity index”) on page 50.</p>
<disp-quote content-type="editor-comment">
<p>(6) The final panels of Figure 4 are presented as an approach to test the efficiency of biological networks. The authors seem to measure the instantaneous (and time-averaged) E-I balance while varying the adaptation parameter and then correlate this with the loss. If that is indeed the approach (it's difficult to tell), this doesn't seem to suggest a tractable experiment. Also, the conclusion is somewhat obvious: the tighter the single neuron balance, the fewer unnecessary spikes are fired. I recommend that the authors clearly explain their analysis and how they envision its application to biological data.</p>
</disp-quote>
<p>We indeed measured the instantaneous (and time-averaged) E-I balance while varying the adaptation parameters and then correlating this with the loss. We did not want to imply that the latter panels of Figure 4 are a means to test the efficiency or biological networks or that we are suggesting new and possibly unfeasible experiments. We see it as a way to better conceptually understand how spike triggered adaptation helps the network’s coding efficiency, by tightening the E I balance in a way that it reduces the number of unnecessary spikes. We apologize if the previous text was confusing in this respect.   We have now removed the initial paragraph of former Results Subsection (including removing the subsection title) and added new text about different effect of adaptation in E and I neurons on Page 9. We also thoroughly revised Figure 5.</p>
<disp-quote content-type="editor-comment">
<p>(7) The external stimuli are repeatedly said to vary (or be tracked) across &quot;multiple time scales&quot;, which might inadvertently be interpreted as (i) a single stimulus containing multiple timescales or (ii) simultaneously presented stimuli containing different timescales. These scenarios are potential targets for efficient coding through neuronal adaptation (reference 21 in the manuscript and Pozzorini et al. Nat. Neuro. 2013), but they are not addressed in the current model. I recommend the authors clarify their statements regarding timescales (and if they're up for it, acknowledge this as a limitation).</p>
</disp-quote>
<p>We thank the reviewer for bringing up this interesting point. To address the second point raised by the Reviewer (simultaneously presented stimuli containing multiple timescales), we performed new analyses to test the model with simultaneously presented stimuli that have different timescales. We found that the model encodes efficiently such stimuli.  We tested the case with a 3-dimensional stimulus where each dimension is an Ornstein-Uhlenbeck process with a different time constant. More precisely, we kept the time constant in the first dimension fixed (at 10 ms), and varied the time constant in the second and third dimension such that the time constant in the third dimension is doubled with respect to the second dimension. We plotted the encoding error in every stimulus dimension for E and I neurons (Fig. 8B, left plot) as well as the encoding error and the metabolic cost averaged across stimulus dimensions (Fig. 8B, right plot). The results are briefly described with text on page 13.</p>
<p>Regarding the case i) (single stimulus containing multiple timescales), we considered two possibilities. One possibility is that timescales of the stimulus are separable, and in this case a single stimulus containing several time scales can be decomposed in several stimuli with a single time scale each. As we assign a new set of weights for each dimension of the decomposed stimulus, this case is similar to the case ii) that we already addressed. Another possibility is that timescales of the stimulus cannot be separated. This case is not covered in the present analysis and we listed it among the limitations of the model. We revised the text (page 13) around the question of multiple time scales and included the citation of Pozzorini et al. (2013).</p>
<disp-quote content-type="editor-comment">
<p>(8) It is claimed that the model uses a mixed code to represent signals, citing reference 47 (Rigotti et al., Nature 2013). But whereas the model seems to use linear mixed selectivity, the Rigotti reference highlights the virtues of nonlinear mixed selectivity. In my understanding, a linearly mixed code does not enjoy the same benefits since it’s mathematically equivalent to a non-mixed code (simply rotate the readout matrix). I recommend that the authors clarify the type of selectivity used by their model and how it relates to the paper(s) they cite.</p>
</disp-quote>
<p>The reviewer is correct that our selectivity is a linear mixing of input variables, and differs from the selectivity in Rigotti et al. (2013) which is non-linear. We revised the sentence on page 4 to clarify better that the mixed selectivity we consider is linear and we removed Rigotti’s citation.</p>
<disp-quote content-type="editor-comment">
<p>(9) Reference 46 is cited as evidence that leaky integration of sensory features is a relevant computation for sensory areas. I don’t think this is quite what the reference shows. Instead, it finds certain morphological and electrophysiological differences between single pyramidal neurons in the primary visual cortex compared to the prefrontal cortex. Reference 46’ then goes on to speculate that these are differences relevant to sensory computation. This may seem like a quibble, but given the centrality of the objectivee function in normative theories, I think it's important to clarify why a particular objective is chosen.</p>
</disp-quote>
<p>We agree that our reference of Amatrudo et al was not the best reference and that the previous text was confusing. We thus tried to improve on its clarity. We looked at the previous theoretical efficient coding papers introducing this leaky integration and we could not find in the previous theoretical work a justification of this assumption based on experimental papers. However, there is evidence that neurons in sensory structures, and in cortical association areas respond to time varying sensory evidence by summing stimuli over time with a weight that decreases steadily going back in time from the time of firing, which suggests that neurons integrate time-varying sensory features. In many cases, these integration kernels decay approximately exponentially going back in time, and several models explaining successfully perceptual readouts of neural activity work assuming leaky integration. This suggests that the mathematical approximation of leaky integration of sensory evidence, though possibly simplistic, is reasonable.  We revised the text in this respect (page 2).</p>
<disp-quote content-type="editor-comment">
<p>(10) The definition of the objective function uses beta as a tuning parameter, but later parts of the text and figures refer to a parameter g_L which might only be introduced in the convex combination of Eq. 40a.</p>
</disp-quote>
<p>This is correct. Parameter optimization has been performed on a weighted sum of the average encoding error and cost as given by the Eq. 39a (40a in first submission), with the weighting g<sub>L</sub> for the error versus the cost, and not the beta that is part of the objective in Eq.10. The convex combination in Eq. 39a allowed us to find a set of optimal parameters that is within biologically realistic parameter ranges, which includes realistic values for the firing threshold. The average encoding error and metabolic cost (the two terms on the right-hand side of Eq. 39a, without weighting with g<sub>L</sub>) in our network are of the same order (see Fig 8G for the E-I model where these values are plotted separately for the optimal network). Weighing the cost with optimal beta that is in the range of ~10 would have yielded a network that optimizes almost exclusively the metabolic cost and would bias the results towards solutions with poor encoding accuracy.</p>
<p>To document more fully how the choice of weighting of the error with the cost (g<sub>L</sub>) affects the optimal parameters, we now added new analysis (Fig. 8D and Supplementary Fig. S4 A-D and H) showing optimal parameters as a function of this weighting. We commented on these results in the text on pages 9-11 and 12. For further details, please see also the reply to point 1 or Reviewer 1.</p>
<disp-quote content-type="editor-comment">
<p>(11) Figure 1J: &quot;In E neurons, the distribution of inhibitory and of net synaptic inputs overlap&quot;. In my understanding, they are in fact identical, and this is by construction. It might help the reader to state this.</p>
</disp-quote>
<p>We apologize for an unclear statement. In E neurons, net synaptic current is the sum of the feedforward current and of recurrent inhibition (Eq. 29c and Eq. 42). With our choice of tuning parameters that are symmetric around zero and with stimulus features that have vanishing mean, the mean of the feedforward current is close to zero. Because of this, the mean of the net current is negative and is close to the mean of the inhibitory current. We have clarified this in the text (page 5).</p>
<disp-quote content-type="editor-comment">
<p>(12) A few typos:</p>
<p>-  p1. &quot;Minimizes the encoding accuracy&quot; should be &quot;maximizes...&quot;</p>
<p>-  p1: &quot;as well the progress&quot; should be something like &quot;as well as the progress&quot;</p>
<p>-  p.11 In recorded neurons where excitatory or inhibitory. &quot;, &quot;where&quot; should be &quot;were&quot; - Fig3: missing parentheses (B)</p>
<p>-  Fig4B: the 200 ticks on the y-scale are cut off.</p>
<p>-  Panel Fig. 5a: &quot;stimulus&quot; should be &quot;stimuli&quot;.</p>
<p>-  Ref 24 &quot;Efficient andadaptive sensory codes&quot; is missing a space.</p>
<p>-  p. 26: &quot;requires&quot; should be &quot;required&quot;.</p>
<p>-  On several occasions, the article &quot;the&quot; is missing.</p>
</disp-quote>
<p>We thank the reviewer for kindly pointing out the typos that we now corrected.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>I would like to give the authors more details about the two main weaknesses discussed above, so that they may address specific points in the paper. First, there is the relation to previous work. Several published articles have presented very similar results to those discussed here, including references 5, 26, 28, 32, 33, 42, 43, 48, and an additional reference not cited by the authors (Calaim et al. 2022 eLife e73276). This includes:</p>
<p>(1) Derivation of an E-I efficient spiking network, which is found in refs. 28, 42, 43, and 48. This is not reflected in the text: e.g., &quot;These previous implementations, however, had neurons that did not respect Dale's law&quot; (Introduction, pg. 1); &quot;Unlike previous approaches (28, 48), we hypothesize that E and I neurons have distinct normative objectives...&quot;. The authors should discuss how their derivation compares to these.</p>
</disp-quote>
<p>We have now fully clarified on page 3 that our model builds on the seminal previous works that introduced E-I networks with efficient coding (Supplementary text in Boerlin et al. 2013, Chalk et al. 2016, Barrett et al. 2016).</p>
<disp-quote content-type="editor-comment">
<p>(2) Inclusion of a slow adaptation current: I believe this also appears in a previous paper (Gutierrez &amp; Deneve 2019, ref. 33) in almost the exact same form, and is again not reflected in the text: &quot;The strength of the current is proportional to the difference in inverse time constants ... and is thus absent in previous studies assuming that these time constants are equal (... ref. 33). Again, the authors should compare their derivation to this previous work.</p>
</disp-quote>
<p>We thank the reviewer for pointing this out. We sincerely apologize if our previous version did not recognize sufficiently clearly that the previous work of Gutierrez and Deneve (eLife 2019; ref 33) introduced first the slow adaptation current that is similar to spike-triggered adaptation in our model. We have made sure that the revised text recognizes it more clearly. We also explained better what we changed or added with respect to this previous work (see revised text on page 8).</p>
<p>The work by Gutierrez and Deneve (2019) emphasizes the interplay between single neuron property (an adapting current in single neurons) and network property (networklevel coding through structured recurrent connections). They use a network that does not distinguish E and I neurons. Our contribution instead focuses on the adaptation in an E-I network. To improve the presentation following the Reviewer’s comment, we now better emphasize the differential effect of adaptation in E and in I neurons in revision (Fig. 5 B-D). Moreover, Gutierrez and Deneve studied the effect of adaptation on slower time scales (1 or 2 seconds) while we study the adaptation on a finer time scale of tens of milliseconds. The revised text detailed this is reported on Page 8.</p>
<disp-quote content-type="editor-comment">
<p>(3) Background currents and physical units: Pg. 26: &quot;these models did not contain any synaptic current unrelated to feedforward and recurrent processing&quot; and &quot;Moreover previous models on efficient coding did not thoroughly consider physical units of variables&quot; - this was briefly described in ref. 28 (Boerlin et al. 2013), in which the voltage and threshold are transformed by adding a common constant, and additional aspects of physical units are discussed.</p>
</disp-quote>
<p>It is correct that Boerlin et al (2013) suggested adding a common constant to introduce physical units. We now revised the text to make clearer the relation between our results and the results of Boerlin et al. (2013) (page 3). In our paper, we built on Boerlin et al. (2013) and assigned physical units to computational variables that define the model's objective (the targets, the estimates, the metabolic constant, etc.). We assigned units to computational variables in such a way that physical variables (such as membrane potential, transmembrane currents, firing thresholds and resets) have the correct physical units.  We have now clarified how we derived physical units in the section of Results where we introduce the biophysical model (page 3) and specified how this derivation relates to the results in Boerlin et al. (2013).</p>
<disp-quote content-type="editor-comment">
<p>(4) Voltage correlations, spike correlations, and instantaneous E/I balance: this was already pointed out in Boerlin et al. 2013 (ref 28; from that paper: &quot;Despite these strong correlations of the membrane potentials, the neurons fire rarely and asynchronously&quot;) and others including ref. 32. The authors mention this briefly in the Discussion, but it should be more prominent that this work presents a more thorough study of this well-known characteristic of the network.</p>
</disp-quote>
<p>We agree that it would be important to comment on how our results relate to these results in Boerlin et al. (2013). It is correct that in Boerlin et al. (2013) neurons have strong correlations in the membrane potentials, but fire asynchronously, similarly to what we observe in our model. However, asynchronous dynamics in Boerlin et al. (2013) strongly depends on the assumption of instantaneous synaptic transmission and time discretization, with a “one spike per time bin” rule in numerical implementation. This rule enforces that at most one spike is fired in each time bin, thus actively preventing any synchronization across neurons. If this rule is removed, their network synchronizes, unless the metabolic constant is strong enough to control such synchronization to bring it back to asynchronous regime (see ref. 36). Our implementation does not contain any specific rule that would prevent synchronization across neurons. We now cite the paper by Boerlin and colleagues and briefly summarize this discussion when we describe the result of Fig. 3D on page 7.</p>
<disp-quote content-type="editor-comment">
<p>(5) Perturbations and parameters sweep: I found one previous paper on efficient spiking networks (Calaim et al. 2022) which the authors did not cite, but appears to be highly relevant to the work presented here. Though the authors perform different perturbations from this previous study, they should ideally discuss how their findings relate to this one. Furthermore, this previous study performs extensive sweeps over various network parameters, which the authors might discuss here, when relevant. For example, on pg. 8, the authors write “We predict that, if number of neurons within the population decreases, neurons have to fire more spikes to achieve an optimal population readout” – this was already shown in Calaim et al. 2022 Figure 5, and the authors should mention if their results are consistent.</p>
</disp-quote>
<p>We apologize for not being aware of Calaim et al. (2022) when we submitted the first version of our paper. This important study is now cited in the revised version. We have now, as suggested, performed sweeps of multiple parameters inspired by the work of Calaim. This new analysis is described extensively in reply to Weaknesses in the Public Review of reviewer 2 and is found in Fig 2, 6I and 7J and described on pages 5,11 and 13.</p>
<p>The Reviewer is also correct that the compensation mechanism that applies when changing the ratio of E-I neuron numbers is similar to the one described in Barrett et al. (2016) and related to our claim “if number of neurons within the population decreases, neurons have to fire more spikes to achieve an optimal population readout”. We have now added (page 11) that this prediction is consistent with the finding of Barrett et al. (2016).</p>
<p>With regard to the dependence of optimal coding properties on the number of neurons, we have tried to better describe similarities and differences with our work and that of Calaim et al as well as with the work of Barrett et al. (2016) which reports highly relevant results. These additional considerations are summarized in a paragraph in Discussion (page 16).</p>
<disp-quote content-type="editor-comment">
<p>(6) Overall, the authors should distinguish which of their results are novel, which ones are consistent with previous work on efficient spiking networks, and which ones are consistent in general with network implementations of efficient and sparse coding. In many of the above cases, this manuscript goes into much more depth and study of each of the network characteristics, which is interesting and commendable, but this should be made clear. In clarifying the points listed above, I hope that the authors can better contextualize their work in relation to previous studies, and highlight what are the unique characteristics of the model presented here.</p>
</disp-quote>
<p>We made a number of clarifications of the text to provide better contextualization of our model within existing literature and to credit more precisely previous publications. This includes commenting on previous studies that introduced separate objective functions of E and I neurons (page 2), spike-triggered adaptation (page 8), physical units (page 3), and changes in the number of neurons in the network (page 16).</p>
<disp-quote content-type="editor-comment">
<p>Next, there are the claims of optimal parameters. As explained on pg. 35 (criterion for determining optimal model parameters), it appears to me that they simply vary each parameter one at a time around the optimal value. This argument appears somewhat circular, as they would need to know the optimal parameters before starting this sweep. In general, I find these optimality considerations to be the most interesting and novel part of the paper, but the simulations are relatively limited, so I would ask the authors to either back them up with more extensive parameter sweeps that consider covariations in different parameters simultaneously (as in Calaim et al. 2022). Furthermore, the authors should make sure that they are not breaking any of the required relationships between parameters necessary for the optimization of the loss function. Again, some of the results (such as coding error not being minimized with zero metabolic cost) suggests that there might be issues here.</p>
</disp-quote>
<p>We thank the reviewer for this insightful suggestion. We have now added a joint sweep of all relevant model parameters using Monte-Carlo parameter search with 10.000 iterations. We randomly drew parameter configurations from predetermined parameter ranges that are detailed in the newly added Table 2. Parameters were sampled from a uniform distribution. We varied all the six model parameters studied in the paper (metabolic constant, noise intensity, time constant of single E and I neurons, ratio of E to I neurons and ratio of the mean I-I to E-I connectivity).  We now present these results on a new Figure 2. We did not find any set of parameters with lower loss than the parameters in Table 1 when the weighting of the error with the cost was in the following range: 0.4&lt;g<sub>L</sub>&lt;0.81 (Fig. 2C). While our large but finite Monte-Carlo random sampling does not fully prove that the configuration we selected as optimal (on Table 1) is a global optimum, it shows that this configuration is highly efficient. Further, and as detailed in the rebuttal to the Weaknesses of the Public Review of Referee 2, analyses of the near optimal solutions are compatible with the notion (resulting from the join parameter sweep studies that we added to Figures 6 and 7) that network optimality may be influenced by joint covariations in parameters. These new results are reported in Results (page 5, 11 and 13) and in Figure 2, 6I an 7J.</p>
<disp-quote content-type="editor-comment">
<p>Some more specific points:</p>
<p>(1) In general, I find it difficult to understand the scaling of the RMSE, cost, and loss values in Figures 4-7. Why are RMSE values in the range of 1-10, whereas loss and cost values are in the range of 0-1? Perhaps the authors can explicitly write the values of the RMSE and loss for the simulation in Figure 1G as a reference point.</p>
</disp-quote>
<p>Encoding error (RMSE), metabolic cost (MC) and average loss for a well performing network are within the range of 1-10 (see Fig. 8G or 7C in the first submission). To ease the visualization of results, we normalized the cost and the loss on Figs. 6-8 in order to plot them on the same figure (while the computation of the optima is done following the Eq. 39 and is without normalization). We have now explicitly written the values of RMSE, MC and the average loss (non-normalized) for the simulation in Fig. 1D on page 5, as suggested by the reviewer. We have also revised Fig. 4 and now show the absolute and not the relative values of the RMSE and the MC (metabolic cost).</p>
<disp-quote content-type="editor-comment">
<p>(2) Optimal E-I neuron ratio of 4:1 and efficacy ratio of 3:1: besides being unintuitive in relation to previous work, are these two optimal settings related to one another? If there are 4x more excitatory neurons than inhibitory neurons, won't this affect the efficacy ratio of the weights of the two populations? What happens if these two parameters are varied together?</p>
</disp-quote>
<p>Thanks for this insightful point. Indeed, the optima of these two parameters are interdependent and positively correlated - if we decrease the E-I neuron ratio, the optimal efficacy ratio decreases as well. To better show this relation we added figures with 2dimensional parameter search (Fig. 7J) where we varied jointly the two ratios. The red cross on the right figure marks the optimal ratios used as optimal parameters in our study. These finding are discussed on page 13.</p>
<disp-quote content-type="editor-comment">
<p>(3) Optimal dimensionality of M=[1,4]: Again, previous work (Calaim et al. 2022) would suggest that efficient spiking networks can code for arbitrary dimensional signals, but that performance depends on the redundancy in the network - the more neurons, the better the coding. From this, I don't understand how or why the authors find a minimum in Figure 7B. Why does coding performance get worse for small M?</p>
</disp-quote>
<p>We optimized all model parameters with M=3 and this is the reason why M=3 is the optimal number of inputs when we vary this parameter. Our network shows a distinct minimum of the encoding error as a function of the stimulus dimensionality for both E and I neurons (Fig. 8C, top). This minimum is reflected in the minimum of the average loss (Fig. 8C, bottom). The minimum of the loss is shifted (or biased) by the metabolic cost, with strong weighting of the cost lowering the optimal number of inputs. This is discussed on pages 13-14.</p>
<disp-quote content-type="editor-comment">
<p>Here are a list of other, more minor points, that the authors can consider addressing to make the results and text more clear:</p>
<p>(1) Feedforward efficient coding models: in the introduction (pg. 1) and discussion (pg. 11) it is mentioned that early efficient coding models, such as that of Olshausen &amp; Field 96, were purely feedforward, which I believe to be untrue (e.g., see Eq. 2 of O&amp;F 96). Later models made this even more explicit (Rozell et al. 2008). Perhaps the authors can either clarify what they meant by this, or downplay this point.</p>
</disp-quote>
<p>We sincerely apologize for the oversight present in the previous version of the text. We agree with the reviewer that the model in Olshausen and Field (1996) indeed defines a network with recurrent connections, and the same type of recurrent connectivity has been used by Rozell et al. (2008, 2013). The structure of the connectivity in Olshausen and Field (as well as in Rozell et al (2008)) is closely related to the structure of connectivity that we derived in our model. We have corrected the text in the introduction (page 1) to remove these errors.</p>
<disp-quote content-type="editor-comment">
<p>(2) Pg. 2 - The authors state: &quot;We draw tuning parameters from a normal distribution...&quot;, but in the methods, it states that these are then normalized across neurons, so perhaps the authors could add this here, or rephrase it to say that weights are drawn uniformly on the hypersphere.</p>
</disp-quote>
<p>We rephrased the description of how weights were determined (page 2).</p>
<disp-quote content-type="editor-comment">
<p>(3) Pg. 2 - &quot;We hypothesize the time-resolved metabolic cost to be proportional to the estimate of a momentary firing rate of the neural population&quot; - from what I can see, this is not the usual population rate, which would be an average or sum of rates across the population.</p>
</disp-quote>
<p>Indeed, the time-dependent metabolic cost is not the population rate (in the sense of the sum of instantaneous firing rates across neurons), but is proportional to it by a factor of 1/t. More precisely, we can define the instantaneous estimate of the firing rate of a single neuron i as z<sub>i</sub>(t) = 1/t<sub>r</sub> r<sub>i</sub>(t) with r<sub>i</sub>(t) as in Eq. 7. We have clarified this in the revised text on page 3.</p>
<disp-quote content-type="editor-comment">
<p>(4) Pg. 3: &quot;The synaptic strength between two neurons is proportional to their tuning similarity if the tuning similarity is positive&quot; - based on the figure and results, this appears to be the case for I-E, E-I, and I-I connections, but not for E-E connections. This should be clarified in the text. Furthermore, one reference given in the subsequent sentence (Ko et al. 2011, ref. 51), is specifically about E-E connections, so doesn't appear to be relevant here.</p>
</disp-quote>
<p>We have now specified that the Eq. 24 does not describe E-E connections. We also agree that the reference (Ko et al. 2011) did not adequately support our claim and we thus removed it and revised the text on page 3 accordingly.</p>
<disp-quote content-type="editor-comment">
<p>(5) Pg. 3: &quot;the relative weight of the metabolic cost over the encoding error controls the operating regime of the network&quot; and &quot;and an operating regime controlled by the metabolic constant&quot; - what do you mean by operating regime here?</p>
</disp-quote>
<p>We used the expression “operating regime” in the sense of a dynamical regime of the network.  However, we agree that this expression may be confusing and we removed it in revision.</p>
<disp-quote content-type="editor-comment">
<p>(6) Pg. 3: &quot;Previous studies interpreted changes of the metabolic constant beta as changes to the firing thresholds, which has less biological plausibility&quot; - can the authors explain why this is less plausible, or ideally provide a reference for it?</p>
</disp-quote>
<p>In biological networks, global variables such as brain state can strongly modulate the way neural networks respond to a feedforward stimulus. These variables influence neural activity in at least two distinct ways. One is by changing non-specific synaptic inputs to neurons, which is a network-wide effect (Destexhe and Pare, Nature Reviews Neurosci. 2003). This is captured in our model by changing the strength of the mean and fluctuations in the external currents. Beyond modulating synaptic currents, another way of modulating neural activity is by changing cell-intrinsic factors that modulate the firing threshold in biological neurons (Pozzorini et al. 2013). Previous studies on spiking networks with efficient coding interpreted the effect of the metabolic constant as changes to the firing threshold (Koren and Deneve, 2017, Gutierrez and Deneve 2019), which corresponds to cell-intrinsic factors. Here we instead propose that the metabolic constant modulates the neural activity by changing the non-specific synaptic input, homogeneously across all neurons in the network. Interpreting the metabolic constant as setting the mean of the non-specific synaptic input was necessary in our model to find an optimal set of parameters (as in Table 1) that is also biologically plausible. We revised the text accordingly (page 4).</p>
<disp-quote content-type="editor-comment">
<p>(7) Pg. 4: Competition across neurons: since the model lacks E-E connectivity, it seems trivial to conclude that there is competition through lateral inhibition, and it can be directly determined from the connectivity. What is gained from running these perturbation experiments?</p>
</disp-quote>
<p>We agree that a reader with a good understanding of sparse / efficient coding theory can tell that there is competition across neurons with similar tuning already from the equation for the recurrent connectivity (Eq. 24). However, we presume that not all readers can see this from the equations and that it is worth showing this with simulations.</p>
<p>Following the reviewer's comment, we have now downplayed the result about the model manifesting lateral inhibition in general on page 6. We have also removed its extensive elaboration in Discussion.</p>
<p>One reason to run perturbation experiments was to test to what extent the optimal model qualitatively replicates empirical findings, in particular, single neuron perturbation experiments in Chettih and Harvey, 2019, without specifically tuning any of the model parameters. We found that the model reproduces qualitatively the main empirical findings, without tuning the model to replicate the data. We revised the text on page 5 accordingly.</p>
<p>Further reason to run these experiments was to refine predictions about the minimal amount of connectivity structure that generates perturbation response profiles that are qualitatively compatible with empirical observations. To establish this, we did perturbation experiments while removing the connectivity structure of a particular connectivity sub-matrices (E-I, I-I or I-E; Fig. S3 F). This allowed us to determine which connectivity matrix has to be structured to observe results that qualitatively match empirical findings. We found that the structure of E-I and I-E connectivity is necessary, but not the structure of I-I connectivity. Finally, we tested partial removal of the connectivity structure where we replaced the precise (and optimal) connectivity structure and imposed a simpler connectivity rule. In the optimal connectivity, the connection strength is proportional to the tuning similarity. A simpler connectivity rule, in contrast, only specifies that neurons with similar tuning share a connection, and beyond this the connection strength is random. Running perturbation experiments in such a network obeying a simpler connectivity rule still qualitatively replicated empirical results from Chettih and Harvey (2019). This is shown on the Supplementary Fig. S2F on described on page 8.</p>
<disp-quote content-type="editor-comment">
<p>(8) Pg. 4: &quot;the optimal E-I network provided a precise and unbiased estimator of the multidimensional and time-dependent target signal&quot; - from previous work (e.g., Calaim et al. 2022), I would guess that the estimator is indeed biased by the metabolic cost. Why is this not the case here? Did you tune the output weights to remove this bias?</p>
</disp-quote>
<p>Output weights were not tuned to remove the bias. On Fig. 1H in the first submission we plotted the bias for the network that minimizes the encoding error. We forgot to specify this in the text and figure caption, for which we apologize. We now replaced this figure with a new one (Fig. 1E) where we plot the bias of the network minimizing the average loss (with parameters as in Table 1). The bias of the network minimizing the error is close to zero, B^E = 0.02 and B^I = 0.03.  The bias of the network minimizing the loss is stronger and negative, B^E = -0.15 and B^I=-0.34. In the text of Results, we now report the bias of both networks (i.e., optimizing the encoding error and optimizing the loss). We also added a plot showing trial-averaged estimates and a time-dependent bias in each stimulus dimension (Supplementary figure S1 F). Note that the network minimizing the encoding error requires a lower metabolic constant (β = 6) than the network optimizing the loss (β=14), however, the optimal metabolic cost in both networks is nonzero. We revised the text and explained these points on page 5.</p>
<disp-quote content-type="editor-comment">
<p>(9) Pg. 4: &quot;The distribution of firing rates was well described by a log-normal distribution&quot; - I find this quite interesting, but it isn't clear to me how much this is due to the simulation of a finitetime noisy input. If the neurons all have equal tuning on the hypersphere, I would expect that the variability in firing is primarily due to how much the input correlates with their tuning. If this is true, I would guess that if you extend the duration of the simulation, the distribution would become tighter. Can you confirm that this is the stationary distribution of the firing rates?</p>
</disp-quote>
<p>We now simulated the network with longer simulation time (10 seconds of simulated time instead of 2 seconds used previously) and also iterated the simulation across 10 trials to report a result that is general across random draws of tuning parameters (previously a single set of tuning parameters was used). The reviewer is correct that the distribution of firing rates of E neurons has become tighter with longer simulation time, but distributions remain log-normal. We also recomputed the coefficient of variation (CV) using the same procedure. We updated these plots on Fig. 1F.</p>
<disp-quote content-type="editor-comment">
<p>(10) Pg. 4: &quot;We observed a strong average E-I balance&quot; - based on the plots in Figure 1J, the inputs appear to be inhibition-dominated, especially for excitatory neurons. So by what criterion are you calling this strong average balance?</p>
</disp-quote>
<p>The reviewer is correct about the fact that the net synaptic input to single neurons in our optimal network shows excess inhibition and the network is inhibition-dominated, so we revised this sentence (page 5) accordingly.</p>
<disp-quote content-type="editor-comment">
<p>(11) Pg. 4: Stronger instantaneous balance in I neurons compared to E neurons - this is curious, and I have two questions: (1) can the authors provide any intuition or explanation for why this is the case in the model? and (2) does this relate to any literature on balance that might suggest inhibitory neurons are more balanced than excitatory neurons?</p>
</disp-quote>
<p>In our model, I neurons receive excitatory and inhibitory synaptic currents through synaptic connections that are precisely structured. E neurons receive structured inhibition and a feedforward current. The feedforward current consists of M=3 independent OU processes projected on the tuning vectors of E neurons w<sub>i</sub><sup>E</sup>. We speculate that because the synaptic inhibition and feedforward current are different processes and the 3 OU inputs are independent, it is harder for E neurons to achieve the instantaneous balance that would be as precise as in I neurons. While we think that the feedforward current in our model reflects biologically plausible sensory processing, it is not a mechanistic model of feedforward processing. In biological neurons, real feedforward signals are implemented as a series of complex feedforward synaptic inputs from downstream areas, while the feedforward current in our model is a sum of stimulus features, and is thus a simplification of a biological process that generates feedforward signals. We speculate that a mechanistic implementation of the feedforward current could increase the instantaneous balance in E neurons.  Furthermore, the presence of EE connections could potentially also increase the instantaneous balance in E neurons. We revised the Discussion about these important questions that lie on the side of model limitations and could be advanced in future work. We could not find any empirical evidence directly comparing the instantaneous balance in E versus I neurons.  We have reported these considerations in the revised Discussion (page 16).</p>
<disp-quote content-type="editor-comment">
<p>(12) Pg. 5, comparison with random connectivity: &quot;Randomizing E-I and I-E connectivity led to several-fold increases in the encoding error as well as to significant increases in the metabolic cost&quot; and Discussion, pg. 11: &quot;the structured network exhibits several fold lower encoding error compared to unstructured networks&quot;: I'm wondering if these comparisons are fair. First, regarding activity changes that affect the metabolic cost - it is known that random balanced networks can have global activity control, so it is not straightforward that randomizing the connectivity will change the metabolic cost. What about shuffling the weights but keeping an average balance for each neuron's input weights? Second, regarding coding error, it is trivial that random weights will not map onto the correct readout. A fairer comparison, in my opinion, would at least be to retrain the output weights to find the best-fitting decoder for the threedimensional signal, something more akin to a reservoir network.</p>
</disp-quote>
<p>Thank you for raising these interesting questions. The purpose of comparing networks with and without connectivity structure was to observe causal effects of the connectivity structure on the neural activity. We agree that the effect on the encoding error is close to trivial, because shuffling of connectivity weights decouples neural dynamics from decoding weights. We have carefully considered Reviewer's suggestions to better compare the performance of structured and unstructured networks.</p>
<p>In reply to the first point, we followed the reviewer's suggestion and compared the optimal network with a shuffled network that matched the optimal network in its average balance. This was achieved by increasing the metabolic constant, decreasing the noise intensity and slightly decreasing the feedforward stimulus (we did not find a way to match the net current in both cell types by changing a single parameter). As we compared the metabolic cost between the optimal and the shuffled network with matched average balance, we still found lower metabolic cost in the optimal network, even though the difference was now smaller. We replaced Fig. 3B from the first submission with these new results in Fig. 4B and commented on them in the text (page 7).</p>
<p>In reply to the second point, we followed reviewer’s suggestion and compared the encoding error (RMSE) of the optimal network and the network with shuffled connectivity where decoding weights are trained such as to optimally reconstruct the target signal. As suggested, we now analyzed the encoding error of the networks using decoding weights trained on the set of spike trains generated by the network using linear least square regression to minimize the decoding error. For a fair and quantitative comparison and because we did not train decoding weights of our structured model, we performed this same analysis using spike trains generated by networks with structured and shuffled recurrent connectivity. We found that the encoding error is smaller in the E population and much smaller in the I population in the structured compared to the random network. Decoding weights found numerically in the optimal network approach uniform distribution of weights that we used in our model (Fig. 4A, right). In contrast, decoding weights obtained from the random network do not converge to a uniform distribution, but instead form a much sparser distribution, in particular in I neurons (Supplementary Fig. S3 A). These additional results reported in the above mentioned figures are discussed in text on page 14.</p>
<disp-quote content-type="editor-comment">
<p>(13) Pg. 5: &quot;a shift from mean-driven to fluctuation-driven spiking&quot; and Pg. 11 &quot;a network structured as in our efficient coding solution operates in a dynamical regime that is more stimulus-driven, compared to an unstructured network that is more fluctuation driven&quot; - I would expect that the balanced condition dictates that spiking is always fluctuation driven. I'm wondering if the authors can clarify this.</p>
</disp-quote>
<p>We agree with the reviewer that networks with and without connectivity structure are fluctuation-driven, because in a mean-driven network the mean current must be suprathreshold (Ahmadian and Miller, 2021), which is not the case of either network. We removed the claim of the change from mean to fluctuation driven regime in the revised paper. We are grateful to the Reviewer for helping us tighten the elaboration of our findings.</p>
<disp-quote content-type="editor-comment">
<p>(14) Pg. 5: &quot;suggesting that variability of spiking is independent of the connectivity structure&quot; - the literature of balanced networks argues against this. Is this not simply because you have a noisy input? Can you test this claim?</p>
</disp-quote>
<p>We thank the reviewer for the suggestion. We tested this claim by measuring the coefficient of variation in networks receiving a constant stimulus. In particular, we set the same strength in each of the M=3 stimulus dimensions and set the stimulus amplitude such as to match the firing rate of the optimal network in response to the OU stimulus. We computed the coefficient of variation in 200 simulation trials.  The removal of connectivity structure did not cause significant change of the coefficient of variation in a network driven by a constant stimulus (Fig. 4E). These additional results are discussed in text on page 7.</p>
<p>We also taken the suggestion about variability of spiking being independent of the connectivity structure. We removed this claim in the revision, because we only tested a couple of specific cases where the connectivity is structured with respect to tuning similarity (fully structured, fully unstructured and partially unstructured networks). This is not exhaustive of all possible structures that recurrent connectivity may have.</p>
<disp-quote content-type="editor-comment">
<p>(15) Pg. 6: &quot;we also removed the connectivity structure only partially, keeping like-to-like connectivity structure and removing all structure beyond like-to-like&quot; - can you clarify what this means, perhaps using an equation? What connectivity structure is there besides like-to-like?</p>
</disp-quote>
<p>In the optimal model, the strength of the synapse between a pair of neurons is proportional to the tuning similarity of the two neurons, Y<sub>ij</sub> proportional to J<sub>ij</sub> for Y<sub>ij</sub> &gt;0 (see Eq. 24 and Fig. 1C(ii)). Besides networks with optimal connectivity, we also tested networks with a simpler connectivity rule. Such a simpler rule prescribes a connection if the pair of neurons has similar tuning (Y<sub>ij</sub> &gt;0), and no connection otherwise. The strength of the connection following this simpler connectivity rule is otherwise random (and not proportional to pairwise tuning similarity Y<sub>ij</sub> as it is in the optimal network). We clarified this in the revision (page 8), also by avoiding the term “like-to-like” for the second type of networks, which could indeed be prone to confusion.</p>
<disp-quote content-type="editor-comment">
<p>(16) Pgs. 6-7: &quot;we indeed found that optimal coding efficiency is achieved with weak adaptation in both cell types&quot; and &quot;adaptation in E neurons promotes efficient coding because it enforces every spike to be error- correcting&quot; - this was not clear to me. First, it appears as though optimal efficiency is achieved without adaptation nor facilitation, i.e., when the time constants are all equal. Indeed, this is what is stated in Table 1. So is there really a weak adaptation present in the optimal case? Second, it seems that the network already enforces each spike to be errorcorrecting without adaptation, so why and how would adaptation help with this?</p>
</disp-quote>
<p>We agree with the Reviewer that the network without adaptation in E and I neurons is already optimal. It is also true that most spikes in an optimal network should already be error-correcting (besides some spikes that might be caused by the noise). However, regimes with weak adaptation in E neurons remain close to optimality. Spike-triggered facilitation, meanwhile, ads spikes that are unnecessary and decrease network efficiency. We revised the Fig.5 (Fig. 4 in first submission) and replaced 2-dimensional plots in Fig.4 C-F with plots that show the differential effect of adaptation in E neurons (top) and in I neurons (bottom plots) for the measures of the encoding error (RMSE), the efficiency (average loss) and the firing rate (Fig. 5B-D). On the new Fig. 5C it is evident that the loss of E and I population grows slowly with adaptation in E neurons (top) while it grows faster with adaptation in I neurons (bottom). These considerations are explained in revised text on page 9.</p>
<disp-quote content-type="editor-comment">
<p>(17) Pg. 7: &quot;adaptation in E neurons resulted in an increase of the encoding error in E neurons and a decrease in I neurons&quot; - it would be nice if the authors could provide any explanation or intuition for why this is the case. Could it perhaps be because the E population has fewer spikes, making the signal easier to track for the I population?</p>
</disp-quote>
<p>We agree that this could indeed be the case. We commented on it in revision (page 9).</p>
<disp-quote content-type="editor-comment">
<p>(18) Pg. 7: &quot;The average balance was precise...with strong adaptation in E neurons, and it got weaker when increasing the adaptation in I neurons (Figure 4E)&quot; - I found the wording of this a bit confusing. Didn't the balance get stronger with larger I time constants?</p>
</disp-quote>
<p>By increasing the time constant of I neurons, the average imbalance got weaker (closer to zero) in E neurons (Fig. 5G, left), but stronger (further away from zero) in I neurons (Fig. 5G, right). We have revised the text on page 9 to make this clearer.</p>
<disp-quote content-type="editor-comment">
<p>(19) Pg. 7: Figure 4F is not directly described in the text.</p>
</disp-quote>
<p>We have now added text (page 9) commenting on this figure in revision.</p>
<disp-quote content-type="editor-comment">
<p>(20) Pg. 8: &quot;indicating that the recurrent network dynamics generates substantial variability even in the absence of variability in the external current&quot; -- how does this observation relate to your earlier claim (which I noted above) that &quot;variability of spiking is independent of connectivity structure&quot;?</p>
</disp-quote>
<p>We agree that the claim about variability of spiking being independent of connectivity structure was overstated and we thus removed it. The observation that we wanted to report is that both structured and unstructured networks have very similar levels of variability of spiking of single neurons. The fact that much of the variability of the optimal network is generated by recurrent connections is not incompatible. We revised the related text (page 11) for clarity.</p>
<disp-quote content-type="editor-comment">
<p>(21) Pg. 9: &quot;We found that in the optimally efficient network, the mean E-I and I-E synaptic efficacy are exactly balanced&quot; - isn't this by design based on the derivation of the network?</p>
</disp-quote>
<p>True, the I-E connectivity matrix is the transpose of the E-I connectivity matrix, and their means are the same by the analytical solution. This however remains a finding of our study. We have clarified this in the revised text (page 12).</p>
<disp-quote content-type="editor-comment">
<p>(22) Pg. 30, eq. 25: the authors should verify if they include all possible connectivity here, or if they exclude EE connectivity beforehand.</p>
</disp-quote>
<p>We now specify that the equation for recurrent connectivity (Eq. 24, Eq. 25 in first submission) does not include the E-E connectivity in the revised text (page 41).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>Essential</p>
<p>(1)  Currently, they measure the RMSE and cost of the E and I population separately, and the 1CT model. Then, they average the losses of the E and I populations, and compare that to the 1CT model, with the conclusion that the 1CT model has a higher average loss. However, it seems to me that only the E population should be compared to the 1CT model. The I population loss determines how well the I population can represent the E population representation (which it can do extremely well). But the overall coding accuracy of the network of the input signal itself is only represented by the E population. Even if you do combine the E and I losses, they should be summed, not averaged. I believe a more fair conclusion would be that the E/I networks have generally slightly worse performance because of needing to follow Dale's law, but are still highly efficient and precise nonetheless. Of course, I might be making a critical error somewhere above, and happy to be convinced otherwise!</p>
</disp-quote>
<p>We carefully considered the reviewer's comment and tested different ways of combining the losses of the E and I population. We decided to follow the reviewer's suggestion and to compare the loss of the E population of the E-I model with the loss of the one cell type model. As evident already from the Fig. 8G, such comparison indeed changes the result to make the 1CT model more efficient. Also, the sum of losses of E and I neurons results in the 1CT model being more efficient than the E-I model. Note, however, the robustness of the E-I model to changes in the metabolic constant (Fig. 6C, top). The firing rates of the E-I model stay within physiological ranges for any value of the metabolic constant, while the firing rate of the 1CT model skyrocket for the metabolic constant that is lower than optimal (Fig. 8I).</p>
<p>We added to Results (page 14) a summary of these findings.</p>
<disp-quote content-type="editor-comment">
<p>(2) The methods and main text should make much clearer what aspects of the derivation are novel, and which are not novel (see review weaknesses for specifics).</p>
</disp-quote>
<p>We specified these aspects, as discussed in more detail in the above reply to point 4 of the public review of Reviewer 1.</p>
<disp-quote content-type="editor-comment">
<p>Request:</p>
<p>If possible, I would like to see the code before publication and give recommendations on that (is it easy to parse and reproduce, etc.)</p>
</disp-quote>
<p>We are happy to share the computer code with the reviewer and the community. We added a link to our public repository containing the computer code that we used for simulations and analysis to the preprint and submission (section “Code availability” on page 17).</p>
<p>Suggestions:</p>
<disp-quote content-type="editor-comment">
<p>(1) I believe that for an eLife audience, the main text is too math-heavy at the beginning, and it could be much simplified, or more effort could be made to guide the reader through the math.</p>
</disp-quote>
<p>We tried to do our best to improve the clarity of description of mathematical expressions in the main text.</p>
<disp-quote content-type="editor-comment">
<p>(2) Generally vector notation makes network equations for spiking neurons much clearer and easier to parse, I would recommend using that throughout the paper (and not just in the supplementary methods).</p>
</disp-quote>
<p>We now use vector notation throughout the paper whenever we think that this improves the intelligibility of the text.</p>
<disp-quote content-type="editor-comment">
<p>(3) In the discussion or at the end of the results adding a clear section summarizing what the minimal requirements or essential assumptions are for biological networks to implement this theory would be helpful for experimentalists and theorists alike.</p>
</disp-quote>
<p>We have added such a section in Discussion (page 15).</p>
<disp-quote content-type="editor-comment">
<p>(5) I think the title is a bit too cumbersome and hard to parse. Might I suggest something like 'Efficient coding and energy use in biophysically realistic excitatory-inhibitory spiking networks' or 'Biophysically constrained excitatory-inhibitory spiking networks can efficiently implement efficient coding'.</p>
</disp-quote>
<p>We followed reviewer’s suggestion and changed the title to “Efficient coding in biophysically realistic excitatory-inhibitory spiking networks.”</p>
<disp-quote content-type="editor-comment">
<p>(6) How the connections were shuffled exactly was not clear to me in how it was described now. Did they just take the derived connectivity, and shuffle the connections around? I recommend a more explicit methods section on it (I might have missed it).</p>
</disp-quote>
<p>Indeed, the connections of the optimal network were randomly shuffled, without repetition, between all neuronal pairs of a specific connectivity matrix. This allows to preserve all properties of the distribution of connectivity weights and only removes the structure of the connectivity, which is precisely what we wanted to test. We now added a section in Methods (“Removal of connectivity structure”) on pages 51-52 where we explain how the connectivity structure is removed.</p>
<disp-quote content-type="editor-comment">
<p>(7) Figure 1 sub-panel ordering was confusing to read (first up down, then left right). Not sure if re- arranging is possible, but perhaps it could be A, B, and C at the top, with subsublabels (i) and (ii). Might become too busy though.</p>
</disp-quote>
<p>We followed this suggestion and rearranged the Fig. 1 as suggested by the reviewer.</p>
<disp-quote content-type="editor-comment">
<p>(8) Equation 3 in the main text should specify that 'y' stands for either E or I.</p>
</disp-quote>
<p>This has been specified in the revision (page 3).</p>
<disp-quote content-type="editor-comment">
<p>(9) Figure 1D shows a rough sketch of the types of connectivities that exist, but I would find it very useful to also see the actual connection strengths and the effect of enforcing Dale's law.</p>
</disp-quote>
<p>We revised this figure (now Fig. 1B (ii)) and added connection strengths as well as a sketch of a connection that was removed because of Dale’s law.</p>
<disp-quote content-type="editor-comment">
<p>(10) The main text mentions how the readout weights are defined (normal distributions), but I think this should also be mentioned in the methods.</p>
</disp-quote>
<p>Agreed. We indeed had Methods section “Parametrization of synaptic connectivity (page 46), where we explain how readout weights are defined. We apologize if a call on this section was not salient enough in the first submission. We made sure that the revised main text contains a clear pointer to this Methods section for details.</p>
<disp-quote content-type="editor-comment">
<p>(11) The text seems to mix ‘decoding weights’ and ‘readout weights’.</p>
</disp-quote>
<p>Thanks for this suggestion to use consistent language. We opted for ‘decoding weights’ and removed ‘readout weights’.</p>
<disp-quote content-type="editor-comment">
<p>(12) The way the paper is written makes it quite hard to parse what are new experimental predictions, and what results reproduce known features. I wonder if some sort of 'box' is possible with novel predictions that experimentalists could easily look at and design an experiment around.</p>
</disp-quote>
<p>We now revised the text. We clarified for every property of the model if this property is a prediction of facts that were not yet experimentally tested or if it accounts for previously observed properties of biological neurons. Please see the reply to point 4 of Reviewer 1.</p>
<disp-quote content-type="editor-comment">
<p>(13) Typo's etc.:</p>
<p>Page 5 bottom -- (&quot;all&quot;) should have one of the quotes change direction (common latex typo, seems to be the only place with the issue).</p>
</disp-quote>
<p>We thank the reviewer for pointing out this typo that has been removed in revision.</p>
</body>
</sub-article>
</article>