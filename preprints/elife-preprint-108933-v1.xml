<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">108933</article-id>
<article-id pub-id-type="doi">10.7554/eLife.108933</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.108933.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>EEG decodability of facial expressions and their stereoscopic depth cues in immersive virtual reality</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3985-2481</contrib-id>
<name>
<surname>Klotzsche</surname>
<given-names>Felix</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>klotzsche@cbs.mpg.de</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6965-9012</contrib-id>
<name>
<surname>Nasim</surname>
<given-names>Ammara</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0958-501X</contrib-id>
<name>
<surname>Hofmann</surname>
<given-names>Simon M</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2604-2404</contrib-id>
<name>
<surname>Villringer</surname>
<given-names>Arno</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6082-3859</contrib-id>
<name>
<surname>Nikulin</surname>
<given-names>Vadim</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5266-3445</contrib-id>
<name>
<surname>Sommer</surname>
<given-names>Werner</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4442-5778</contrib-id>
<name>
<surname>Gaebler</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences, Department of Neurology</institution></institution-wrap>, <city>Leipzig</city>, <country country="DE">Germany</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Humboldt-Universität zu Berlin, Department of Psychology</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033n9gh91</institution-id><institution>Carl von Ossietzky Universität Oldenburg</institution></institution-wrap>, <city>Oldenburg</city>, <country country="DE">Germany</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tbr6331</institution-id><institution>Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute (HHI), Department of Artificial Intelligence</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0145fw131</institution-id><institution>Department of Physics and Life Science Imaging Center, Hong Kong Baptist University</institution></institution-wrap>, <city>Hong Kong</city>, <country country="CN">China</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00bw8d226</institution-id><institution>Faculty of Education, National University of Malaysia</institution></institution-wrap>, <city>Bangi</city>, <country country="MY">Malaysia</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2104-4988</contrib-id>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/008s83205</institution-id>
<institution>University of Alabama at Birmingham</institution>
</institution-wrap>
<city>Birmingham</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country country="NL">Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-12-09">
<day>09</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP108933</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-10-07">
<day>07</day>
<month>10</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-09-03">
<day>03</day>
<month>09</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.08.18.670974"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Klotzsche et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Klotzsche et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-108933-v1.pdf"/>
<abstract><p>Face perception typically occurs in three-dimensional space, where stereoscopic depth cues enrich the perception of facial features. Yet, most neurophysiological research on face processing relies on two-dimensional displays, potentially overlooking the role of stereoscopic depth information. Here, we combine immersive virtual reality (VR), electroencephalography (EEG), and eye tracking to examine the neural representation of faces under controlled manipulations of stereoscopic depth. Thirty-four participants viewed computer-generated faces with neutral, happy, angry, and surprised expressions in frontal view under monoscopic and stereoscopic viewing conditions. Using time-resolved multivariate decoding, we show that EEG signals in immersive VR conditions can reliably differentiate facial expressions. Stereoscopic depth cues elicited a distinct and decodable neural signature, confirming the sensitivity of our approach to depth-related processing. Yet, expression decoding remained robust across depth conditions, indicating that under controlled frontal viewing, the neural representation of behaviorally distinguishable facial expressions is invariant to binocular depth cues. Eye tracking showed that expression-related gaze patterns contained comparable information but did not account for neural representations, while depth information was absent in gaze patterns—consistent with dissociable representational processes. Our findings demonstrate the feasibility of EEG-based neural decoding in fully immersive VR as a tool for investigating face perception in naturalistic settings and provide new evidence for the stability of expression representations across depth variations in three-dimensional viewing conditions.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Face perception</kwd>
<kwd>depth perception</kwd>
<kwd>EEG</kwd>
<kwd>immersive virtual reality</kwd>
<kwd>emotional expressions</kwd>
<kwd>multivariate decoding</kwd>
<kwd>eye movements</kwd>
</kwd-group>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01hhn8329</institution-id>
<institution>Max Planck Society</institution>
</institution-wrap>
</funding-source>
<award-id>NEUROHUM</award-id>
</award-group>
<award-group id="funding-1a">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05hkkdn48</institution-id>
<institution>Fraunhofer Gesellschaft</institution>
</institution-wrap>
</funding-source>
<award-id>NEUROHUM</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/04pz7b180</institution-id>
<institution>Bundesministerium für Forschung, Technologie und Raumfahrt</institution>
</institution-wrap>
</funding-source>
<award-id>13GW0206</award-id>
</award-group>
<award-group id="funding-2a">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/04pz7b180</institution-id>
<institution>Bundesministerium für Forschung, Technologie und Raumfahrt</institution>
</institution-wrap>
</funding-source>
<award-id>13GW0488</award-id>
</award-group>
<award-group id="funding-2b">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/04pz7b180</institution-id>
<institution>Bundesministerium für Forschung, Technologie und Raumfahrt</institution>
</institution-wrap>
</funding-source>
<award-id>16SV9156</award-id>
</award-group>
<award-group id="funding-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/018mejw64</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>502864329</award-id>
</award-group>
<award-group id="funding-3a">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/018mejw64</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>542559580</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Slight reformulations and corrections of style and spelling mistakes; keywords updated; removal of hyphen in title; author affiliations and funding information updated; Supplemental files updated (wrong figure numbering)</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Faces are among the most relevant visual input for humans. They play a crucial role in social communication, enabling us to recognize and distinguish individuals, as well as to infer detailed semantic information about them, such as their emotional states (<xref ref-type="bibr" rid="c8">Bruce &amp; Young, 1986</xref>; <xref ref-type="bibr" rid="c43">Jack &amp; Schyns, 2015</xref>). Humans are able to detect subtle differences between faces and facial expressions, although they are composed of relatively few key features (e.g., one mouth, one nose, two eyes) and share a similar configuration. Moreover, faces and the emotions they convey are processed and evaluated more rapidly than other objects of comparable visual complexity (<xref ref-type="bibr" rid="c96">Willis &amp; Todorov, 2006</xref>). The sophisticated processing of facial stimuli is supported by a hierarchical cortical network, ranging from retinotopic areas in the early visual cortex, which process single features, to face-specific visual areas like the fusiform face area (FFA) that encode view-invariant, abstract representations of faces (<xref ref-type="bibr" rid="c11">Calder &amp; Young, 2005</xref>; <xref ref-type="bibr" rid="c20">Duchaine &amp; Yovel, 2015</xref>; <xref ref-type="bibr" rid="c28">Freiwald et al., 2016</xref>; <xref ref-type="bibr" rid="c39">Haxby et al., 2000</xref>).</p>
<p>In the real world, humans constantly process faces in complex, three-dimensional (3D) settings. However, laboratory studies of face perception usually simplify these experiences, relying on two-dimensional (2D) stimuli presented on flat screens. Such conditions omit key features of natural vision—for example, binocular depth information. Binocular disparity is a visual feature which is automatically and (seemingly) effortlessly extracted from the retinal inputs and integrated into a percept with stereoscopic depth: Each eye captures a slightly different image, which the cerebral cortex integrates—together with further visual cues such as relative size, blur, shading, perspective, texture gradients, or motion as well as with sensory signals from other modalities— to gain depth information about the shape of an object or the configuration of a scene (<xref ref-type="bibr" rid="c64">Parker, 2007</xref>; <xref ref-type="bibr" rid="c95">Welchman, 2016</xref>). During this fusion into a stereoscopic representation, coarse percepts change and refine over time (<xref ref-type="bibr" rid="c59">Menz &amp; Freeman, 2003</xref>). Among the various available depth cues, binocular information plays a special role for the subjective experience of stereoscopic depth (<xref ref-type="bibr" rid="c2">Barry, 2010</xref>; <xref ref-type="bibr" rid="c90">Vishwanath, 2014</xref>). However, binocular disparity information is neither necessary (<xref ref-type="bibr" rid="c91">Vishwanath &amp; Hibbard, 2013</xref>) nor sufficient (<xref ref-type="bibr" rid="c17">Cumming &amp; Parker, 1997</xref>) for a subjective impression of stereoscopic depth. The precise contribution of binocular disparity information for the brain’s processing of complex stimuli like faces—and, in particular, how it interacts with the representation of other visual features of the same object—is still unclear.</p>
<p>As humans often encounter faces at close range—where binocular disparity provides important cues (<xref ref-type="bibr" rid="c18">Cutting &amp; Vishton, 1995</xref>)—depth information may play a key role in face processing. Indeed, stereoscopic depth modulates the recognition and representation of faces and facial expressions: It improves participants’ ability to recognize the same person from different viewpoint angles or distances (<xref ref-type="bibr" rid="c10">Burke et al., 2007</xref>; C. H. <xref ref-type="bibr" rid="c54">Liu &amp; Ward, 2006</xref>; H. <xref ref-type="bibr" rid="c56">Liu et al., 2020</xref>). C. H. <xref ref-type="bibr" rid="c55">Liu et al. (2006)</xref> observed impaired performance when recognition conditions differed from encoding conditions in the availability of spatial depth cues (stereoscopic vs monoscopic), suggesting that neural encoding depends on whether faces are processed with or without stereoscopic depth information. <xref ref-type="bibr" rid="c36">Hakala et al. (2015)</xref> demonstrated that stereoscopically presented facial expressions evoke stronger emotional responses than their monoscopic counterparts, with natural depth cues amplifying negative reactions to angry faces. Children (3–6 years old) are faster in recognizing emotional facial expressions in stereoscopic as compared to monoscopic conditions (<xref ref-type="bibr" rid="c93">Wang et al., 2017</xref>). Collectively, these findings suggest that stereoscopic depth cues can shape natural face processing, highlighting the importance of binocular information in how humans perceive and respond to faces and their expressions.</p>
<p>Here, we used electroencephalography (EEG) in combination with immersive virtual reality (VR) and eye tracking to investigate how the human brain processes faces and facial expressions in the presence of stereoscopic depth cues. Due to its high temporal resolution, EEG is a powerful tool for studying the temporal dynamics of face perception (for reviews see <xref ref-type="bibr" rid="c9">Brunet, 2023</xref>; <xref ref-type="bibr" rid="c69">Rossion, 2014</xref>). Previous EEG research has identified several characteristic components of the event-related potential (ERP), each emerging within a specific time window and reflecting a distinct processing stage of cortical face processing: The earliest component associated with face processing, is the P1 component (80–120 ms) which has been linked to low-level visual analysis of faces (<xref ref-type="bibr" rid="c70">Rossion &amp; Caharel, 2011</xref>). It is followed by the N170 (130–200 ms), generated in the fusiform gyrus (<xref ref-type="bibr" rid="c29">Gao et al., 2019</xref>), which encodes structural face recognition and differentiates facial expressions and identities (<xref ref-type="bibr" rid="c71">Rossion &amp; Jacques, 2012</xref>). Around 250–300 ms after stimulus onset, the Early Posterior Negativity (EPN) indexes reflexive attentional orienting toward emotional contents and its selection for further processing (<xref ref-type="bibr" rid="c77">Schindler &amp; Bublatzky, 2020</xref>). Finally, the Late Positive Complex (LPC; 400–600 ms) reflects higher-order cognitive evaluation of faces and other stimuli of motivational relevance (<xref ref-type="bibr" rid="c76">Schacht &amp; Sommer, 2009</xref>; <xref ref-type="bibr" rid="c79">Schupp et al., 2006</xref>).</p>
<p>These well-established ERP components provide informative temporal markers of the sequential stages of face processing in the human brain. However, because they are typically derived by averaging over predefined electrode clusters, they may overlook more subtle, spatially distributed patterns of neural activity. Multivariate decoding—used to probe whether specific stimulus information is present in high-dimensional neurophysiological signals (e.g., <xref ref-type="bibr" rid="c45">Kamitani &amp; Tong, 2005</xref>; for a recent review, see <xref ref-type="bibr" rid="c67">Peelen &amp; Downing, 2023</xref>)—extends this approach by exploiting the full spatiotemporal structure of the EEG data and optimally weighting channels on the level of the individual participant (<xref ref-type="bibr" rid="c13">Carrasco et al., 2024</xref>; <xref ref-type="bibr" rid="c35">Grootswagers et al., 2017</xref>). By doing so, it can offer greater sensitivity, detecting stimulus-specific information embedded in distributed activity patterns—information that might not be apparent in classical ERP analyses.</p>
<p>In the following, we refer to such decodable information as <italic>representation</italic>. While this usage is controversial (see Fallon et al., 2023, for an overview), it aligns with how the term is commonly used by many neuroscientists—where <italic>representation</italic> is often approximated with <italic>decodability</italic> and reflects mutual information between a pattern of brain activity and an experimental variable of interest (<xref ref-type="bibr" rid="c40">Hebart &amp; Baker, 2018</xref>) (Successfully decoding a stimulus feature suggests that the decodable information is present in the recorded data and—given some necessary assumptions about their origin—also available in the identified physiological sources (e.g., brain regions at a given moment in time). This does not imply that the brain (e.g., other brain regions) actually makes use of this information—which would be a necessary requirement for a <italic>representation</italic> in a more strict interpretation (<xref ref-type="bibr" rid="c50">Kriegeskorte &amp; Douglas, 2019</xref>; <xref ref-type="bibr" rid="c68">Ritchie et al., 2019</xref>)). Neuroscientists use decodability as a proxy for determining <italic>where</italic> and <italic>when</italic> information about a feature is available in the brain (<xref ref-type="bibr" rid="c35">Grootswagers et al., 2017</xref>). EEG-based decoding offers high temporal resolution, enabling researchers to track the sequential stages at which such information emerges. Quantifying the contribution of individual EEG channels to decoding performance permits a coarse spatial interpretation of the underlying signals. Subsequent source reconstruction can map these patterns to their cortical generators, thereby identifying the brain regions most relevant for the representation of the decoded feature.</p>
<p>In the present study, we applied this approach to investigate facial expression representations while selectively manipulating the presence of stereoscopic depth cues. To achieve this, we used immersive VR with head mounted displays (HMDs) to deliver binocular stimulation. Immersive VR is increasingly adopted by cognitive and perceptual scientists—not only for its ability to present stereoscopic depth (<xref ref-type="bibr" rid="c14">Choi et al., 2023</xref>; <xref ref-type="bibr" rid="c19">Draschkow, 2022</xref>; <xref ref-type="bibr" rid="c85">Tarr &amp; Warren, 2002</xref>; <xref ref-type="bibr" rid="c87">Thurley, 2022</xref>) but also because the computer-generated stimulation of the (almost) entire visual field enables high levels of experimental control and facilitates various opportunities for stimulus modification. Building on these developments, cognitive neuroscientists have begun combining immersive VR with EEG to investigate neural processing of face stimuli under 3D viewing conditions (<xref ref-type="bibr" rid="c62">Nolte et al., 2024</xref>; <xref ref-type="bibr" rid="c73">Sagehorn et al., 2023</xref>; <xref ref-type="bibr" rid="c72">Sagehorn, Johnsdorf, et al., 2024</xref>; <xref ref-type="bibr" rid="c74">Sagehorn, Kisker, et al., 2024</xref>; <xref ref-type="bibr" rid="c78">Schubring et al., 2020</xref>).</p>
<p>However, combining HMDs with EEG recordings presents several challenges. Placing the HMD over an EEG cap introduces additional sources of noise, potentially obscuring signals of interest (<xref ref-type="bibr" rid="c86">Tauscher et al., 2019</xref>; <xref ref-type="bibr" rid="c94">Weber et al., 2021</xref>). Larger stimulus eccentricities due to the wider field of view compared to conventional computer screens can affect visually evoked ERP components (<xref ref-type="bibr" rid="c48">Klotzsche et al., 2023</xref>). In addition, while conventional laboratory studies often restrict eye movements to control retinotopic stimulus presentations, VR paradigms typically permit free exploration of the environment, which leads to eye movement-related artifacts and confounds in the EEG data and may require different analytical approaches (<xref ref-type="bibr" rid="c62">Nolte et al., 2024</xref>, <xref ref-type="bibr" rid="c63">2025</xref>). Finally, little is known about how the mere addition of binocular depth alters the neural processing of relevant stimuli. Overall, it remains unclear to what extent findings from conventional 2D screen-setups generalize to 3D settings that include stereoscopic depth information.</p>
<p>The present study had two main objectives. First, we addressed a methodological challenge: whether time-resolved EEG decoding can recover facial expression information in an immersive VR setup, where signal quality and noise sources differ from conventional laboratory settings. Second, we examined whether the neural representations of facial expressions are modulated by the presence of stereoscopic depth cues—examining both whether depth information alters expression-related activation patterns and whether stereoscopic depth itself is a decodable feature in the EEG. To contextualize these findings, we performed complementary source reconstruction and eye-tracking analyses to characterize neural and eye movement-related origins of both facial expression and stereoscopic depth cue representations.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>EEG Decoding</title>
<sec id="s2a1">
<title>Decodability of facial expressions</title>
<p>Information about the observed facial expression was decodable from the EEG data, indicated by a decoding performance significantly above chance-level (<xref rid="fig2" ref-type="fig">Figure 2</xref>). For multi-class decoding (distinguishing between the four facial expressions: neutral, angry, happy, and surprised), we observed a large interval with above-chance decoding performance extending from 94 ms after stimulus onset until the onset of the response panel (1,000 ms after stimulus onset). The highest decoding performance (ROC-AUC; average of individual maxima: M = 0.61, SD = 0.03, 95% CI [0.60, 0.62]) was observed for most participants at a median (Mdn) delay of 414 ms (SD = 315.81, 95% CI [306.25, 521.75]).</p>
<p><xref rid="fig2" ref-type="fig">Figure 2</xref>, Supplementary Figure S3, and Supplementary Table ST1 show the decoding results for the binary decoders which were trained to distinguish between pairs of emotions. For each contrast, we found at least one significant cluster, demonstrating above chance-level decoding also for all binary classifiers. Descriptively, different emotion pairs exhibited distinct pairwise-decoding profiles, varying in average decoding performances and temporal dynamics (e.g., the contrast between angry and neutral emotions showed higher and earlier average decodability compared to the contrast between angry and surprised). To investigate the decoding performance over time, we compared the ROC-AUC scores averaged within four time windows corresponding to the P1, N170, EPN, and LPC components commonly used in EEG research on face processing. We found a significant main effect of the time window on the ROC-AUC (F(3,96) = 22.01, p &lt; .001) which was driven by a significantly lower decoding performance in the P1-window compared to all other time windows. The remaining time windows did not differ significantly for the multiclass decoding approach. For the binary classifiers (contrasting pairs of facial expressions), we observed again a significant main effect of time window (F(3,96) = 21.13, p &lt; .001), a significant main effect of contrast (F(5,160) = 9.43, p &lt; .001), as well as a significant interaction (F(15,480) = 9.02, p &lt; .001). The P1-window yielded the lowest decoding performance across all binary decoders. Only for one out of the six contrasts (“surprised-vs-happy”), the decoding performance was significantly above chance during this time window. All other time windows did not differ in their average ROC-AUC (across all binary contrasts) and were not significantly above chance level in the P1 window. In the remaining time windows, all contrasts, except for “angry-vs-surprised” in the LPC window, performed significantly above chance level. The decoders distinguishing “angry-vs-neutral” and “angry-vs-happy” showed significantly higher decoding performance than the other binary classifiers. This difference was most pronounced in the N170 and the LPC time windows. Details are depicted in <xref rid="fig2" ref-type="fig">Figure 2</xref> and Supplementary Figure S4.</p>
</sec>
<sec id="s2a2">
<title>The role of stereoscopic depth cues</title>
<p>We did not observe different classification performances (decoding the observed facial expression) for the monoscopic and the stereoscopic viewing conditions. The multiclass decoders validated on trials with and without stereoscopic depth information differed neither in terms of the maximal decoding performance (mono: M = 0.61, SD = 0.02, 95% CI [0.60, 0.62]; stereo: M = 0.61, SD = 0.03, 95% CI [0.60, 0.62]; t(32) = -0.49, p = .627) nor for the latency of the peak decoding performance (mono: Mdn = 384.00 ms, SD = 287.71, 95% CI [285.83, 482.17]; stereo: Mdn = 524.00 ms, SD = 386.06, 95% CI [392.28, 655.72]; t(32) = -1.04, p = .305). This was confirmed by a cluster-corrected comparison of the decoding performance at each time-point which did not indicate a significant difference between the two depth conditions. In a four-by-two rmANOVA which modeled the average decoding performance as a function of the time window (four levels: P1, N170, EPN, LPC) and the depth condition (two levels: mono- and stereoscopic), only time window was a significant predictor (F(3,96) = 17.82, p &lt; .001), but neither depth condition (F(1,32) = 1.55, p = .222) nor its interaction with time window (F(3,96) = 1.00, p = .395). Also for all the binary classifiers (distinguishing pairs of emotions), we observed no significant differences between the two depth conditions, neither for peak decoding performance nor for its latency (Table ST1).</p>
<p>To scrutinize further whether any information about the presence or absence of stereoscopic depth cues was present in the EEG data, we made the depth condition itself the decoding target of a binary classifier (i.e., decoding whether the data came from a trial in the mono- or the stereoscopic condition). <xref rid="fig2" ref-type="fig">Figure 2d</xref> shows the time-resolved decoding performance of this decoder. Above-chance decodability was indicated by a long interval of significant decoding performance starting 114 ms after stimulus onset (end: 714 ms) and multiple shorter intervals at later time points in the epoch. On average, a peak decoding score of M = 0.65 (SD = 0.07, 95% CI [0.62, 0.67]) was observed for most participants at Mdn = 184 ms after stimulus onset. Descriptively, the decoding performance peaked early (peak decoding time: Mdn = 184 ms, SD = 204.17, 95% CI [114.34, 253.66]) and continuously decreased toward the end of the epoch. We observed a similar temporal profile of the decoding performance when classifying the identity of the stimulus face (rows in <xref rid="fig1" ref-type="fig">Figure 1c</xref>)—another task-irrelevant stimulus feature—from the EEG data (peak decoding score M = 0.60, SD = 0.03, 95% CI [0.59, 0.61]; peak decoding time: Mdn = 134 ms, SD = 244.62, 95% CI [50.54, 217.46]; <xref rid="fig2" ref-type="fig">Figure 2d</xref>). Classifiers trained only on trials from one of the viewing conditions (with or without stereoscopic depth information) performed equally well when tested on trials from the same or the other (“cross-decoding”) depth condition (see Supplementary Figure S5).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Experimental paradigm and decoding analysis.</title>
<p><bold>(a)</bold> In each trial of the emotion recognition task, participants fixated centrally before a face stimulus was presented for 1,000 ms. Subsequently, they selected the recognized emotional expression using the VR controller. <bold>(b)</bold> Face stimuli were presented either with (right) or without (left) stereoscopic depth information. For the stereoscopic condition, a 3D model of the face was rendered using two slightly offset virtual cameras (a “binocular VR camera rig”), providing binocular depth cues. For the monoscopic condition, the same 3D model was first rendered from a single virtual camera to produce a flat 2D texture, eliminating stereoscopic disparity. This texture was then presented to both eyes via the binocular VR camera, removing binocular depth cues while maintaining all other visual properties. <bold>(c)</bold> Overview of all stimulus combinations: three different face identities (rows) expressing four different emotional facial expressions (columns: happy, angry, surprised, neutral). Face identity was irrelevant for the emotion recognition task. Face models were originally created and evaluated by <xref ref-type="bibr" rid="c30">Gilbert et al. (2021)</xref>. <bold>(d)</bold> Schematic of the decoding approach: A multivariate linear classifier (logistic regression) was trained on successive time windows of EEG data, treating each channel as a feature. The emotional expression shown in the respective trial served as the classification label. Using a 5-fold cross-validation, the classifier was tested repeatedly on 80% of the trials and its predictions were validated on the remaining 20%. This procedure yielded decoding performance scores per time window, reflecting the amount of available stimulus information in the EEG data at the corresponding time in the epoch.</p></caption>
<graphic xlink:href="670974v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
<attrib><italic>Canon AT-1 Retro Camera</italic> (<ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6ZwNB">https://skfb.ly/6ZwNB</ext-link>) by AleixoAlonso and <italic>EyeBall</italic> (<ext-link ext-link-type="uri" xlink:href="https://skfb.ly/osJMS">https://skfb.ly/osJMS</ext-link>) by CrisLArt are licensed under Creative Commons Attribution (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</attrib>
</fig>
</sec>
<sec id="s2a3">
<title>Source reconstruction</title>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> depicts the spatial patterns (<xref ref-type="bibr" rid="c38">Haufe et al., 2014</xref>) underlying the decoders and a reconstruction of the associated cortical sources. Classification of task-relevant features (facial expression) as well as task-irrelevant features (identity, depth condition) primarily involved occipital, parietal, and temporal sources. Early after stimulus onset (P1, N170, EPN windows), the most informative sources for facial expression classification were located in the primary and early visual cortex. At later stages (LPC window), sources in the parietal cortex became more prominent contributors. Supplementary Table ST2 lists the most relevant sources for the binary classifiers, which distinguish between pairs of facial expressions. Supplementary Figure S9 visualizes the associated cortical source reconstructions. For the contrasts with the highest decoding performance (“angry-vs-neutral” and “angry-vs-happy”), primary and early visual cortices remained the most informative sources, even during the LPC time window. In contrast, the other pairwise comparisons between two emotional expressions showed more variable results, with regions along both the ventral and dorsal processing streams playing dominant roles. The most informative sources for the classifier trained to distinguish trials with and without stereoscopic depth information were located along the ventral visual processing stream and in the MT+ complex (see <xref rid="fig3" ref-type="fig">Figure 3</xref> and Supplementary Figure S8). Similarly, the decoding of face identity relied primarily on signals originating from the ventral visual processing stream during the N170 time window (Supplementary Figure S8). In contrast, during the two later time windows (EPN and LPC), identity decoding shifted to rely most on information from parietal regions (see Supplementary Table ST2 for an overview).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Time-resolved classification performance for different decoding targets and conditions.</title>
<p><bold>(a)</bold> Classification performance (mean ± 1 SEM) of the decoder distinguishing between the four facial expressions, trained separately for the mono- and stereoscopic viewing conditions, as well as on data pooled across both depth conditions. Horizontal lines at the bottom indicate clusters where decoding was significantly above chance. Shaded rectangles in the background mark relevant ERP time windows associated with face processing. <bold>(b)</bold> Selection of three (out of six) binary contrasts which underlie the multiclass classification in (a). <italic>Angry vs neutral</italic>: Highest decoding performance. <italic>Happy vs surprised</italic>: classification performance lacks an early peak and only rises later in the trial. <italic>Angry vs surprised</italic>: Lowest (but still significant) decoding performance. <bold>(c)</bold> Average performance per time window, depth condition (<italic>top</italic>; colors as in (a)), and binary contrast (<italic>bottom</italic>; *: selection and colors as in (b)). Thick bars: mean ± 1 SEM. <bold>(d)</bold> Time resolved classification performance (mean ± 1 SEM) for the task-irrelevant decoding targets: identity of the stimulus face (<italic>top</italic>) and presence vs absence of stereoscopic depth information (<italic>bottom</italic>). ROC-AUC: area under the receiver operating characteristic.</p></caption>
<graphic xlink:href="670974v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Spatial patterns and localized sources related to decoding (a) facial expressions and (b) the availability of stereoscopic depth information.</title>
<p><bold>(a)</bold> <italic>Top</italic>: Classification of the four facial expressions. Spatial patterns of the classifier (top row) and their projection onto the cortical surface using eLORETA (second and third row) for the four time windows. Colors indicate the absolute, normalized weight of each sensor or cortical parcel (warmer colors represent higher absolute weights). In the topographies, the six most informative channels (top 10%) are highlighted in yellow. Projections on the cortical surface are masked to show only the top 5% most informative parcels. Second row: Lateral view on both hemispheres. Third row: View on occipital and parietal cortices of the right hemisphere. Colored lines on the cortex mark the outlines of relevant regions (see (c)). <italic>Bottom</italic>: Time course of decoding performance for multiclass classification of the four emotional expressions (as in <xref ref-type="fig" rid="fig2">Figure 2</xref>). <bold>(b)</bold> As in (a) [in reversed order], but for the classifier trained to distinguish between trials with and without stereoscopic depth information. <bold>(c)</bold> Outlines of relevant cortical regions, following the reduced parcellation atlas by <xref ref-type="bibr" rid="c31">Glasser et al. (2016)</xref> and <xref ref-type="bibr" rid="c60">Mills (2016)</xref>. <italic>SPC: Superior Parietal Cortex; MT+: MT+ Complex and Neighboring Visual Areas; VSVC: Ventral Stream Visual Cortex; EVC: Early Visual Cortex; PVC: Primary Visual Cortex (V1)</italic>.</p></caption>
<graphic xlink:href="670974v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="670974v2_fig3a.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s2b">
<title>Eye Tracking</title>
<sec id="s2b1">
<title>Saccade and fixation patterns</title>
<p>To check if the dynamics of the EEG decoding performance can be explained by eye movements, we analyzed the concurrently recorded eye tracking data (combined gaze between left and right eye) for a subsample of participants (n=17), for whom eye tracking data were available. <xref rid="fig4" ref-type="fig">Figure 4b</xref> shows the average fixation heatmaps on the face stimuli separately for the different facial expressions. Supplementary Figure S6 shows the corresponding heatmaps separately for each of the four time windows. Participants mostly kept fixation at the center of the face. Towards later time windows (after about 200 ms), there was a notable shift of fixations towards the lower half of the face. This observation was supported by a 4x2x4 MANOVA which modeled the average fixation position as a function of facial expression, depth condition, and time window. The multivariate result was significant for the predictor time window (Wilk’s lambda = 0.93, F(6, 1022) = 6.14, p &lt; .001) but not for the other predictors or any interaction. Univariate follow-up analyses (rmANOVA and post-hoc t-tests) demonstrated that vertical fixation locations differed significantly between the LPC time window and all other time windows. No significant differences were observed among the remaining time windows, and averaged horizontal fixation positions did not differ significantly across any time window. In a generalized linear model (GLM; Poisson, log-linear) modeling the number of saccades (Pseudo R² = 0.90, Supplementary Table ST3) only time window was a significant predictor (Wald χ²(3) = 115.81, p &lt; .001). Neither facial expression (Wald χ²(3) = 2.42, p = .491) nor depth condition (Wald χ²(1) = 1.47, p = .225) or any interactions explained significant amounts of variance. The average saccade amplitude did not differ significantly between facial expressions, depth conditions, or time windows (all p-values &gt; .150). Furthermore, the model did not explain a substantial portion of the variance in the data (Gaussian, log-linear GLM; Pseudo R² = 0.03). <xref rid="fig4" ref-type="fig">Figure 4c</xref> shows the distribution of saccade directions as a function of the time window and expression. A MANOVA which modeled the average saccade direction (two-dimensional, spherical coordinates), indicated a significant main effect of time window (Wilk’s lambda = 0.96, F(6, 968) = 3.22, p = .004) and a significant interaction between time window and emotional expression (Wilk’s lambda = 0.94, F(18, 968) = 1.65, p = .04), but no significant effects for the other predictors.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Eye tracking analyses and comparison with the EEG decoding results.</title>
<p><bold>(a)</bold> <italic>Top left</italic>: Exemplary gaze trace (one trial) on a stimulus face. Saccades are plotted in red, fixations in blue. The fixation cross preceding the stimulus was displayed at (0,0). <italic>Top right</italic>: Vertical component of the same gaze trace over time. <italic>Bottom left</italic>: Horizontal gaze component over time. <italic>Bottom right</italic>: Saccade rate (saccades per second) over the time of the trial across all trials and participants. Colors like in (b) and (c). We observed a dip in saccade rate right after stimulus onset, followed by a sharp increase peaking during the EPN time window. <bold>(b)</bold> Heatmaps showing fixation distributions within the first second after stimulus onset, across all trials and participants, separated by emotional expression. <bold>(c)</bold> Circular histograms of saccade directions (polar angles relative to the preceding fixation), plotted per time window and emotional expression (colors like in (b)). Saccade counts are normalized by the length of the time window. Most saccades occurred during the time window of the EPN (an ERP component associated with reflexive attention to the stimulus), predominantly in downward (especially for angry and surprised faces) or lateral directions. <bold>(d)</bold> Decoding performance of classifiers trained on the gaze data (spherical coordinates) for different decoding targets. EEG-based decoding performance is overlaid in gray for comparison (in the subset of participants with eye tracking data). Horizontal bars at the bottom of each plot indicate time points with decoding significantly above chance level. Red bars mark significant differences in performance between eye tracking and EEG-based decoding (two-sided, cluster-corrected <italic>t</italic>-test). Note, EEG decoding results shown here are based on the subsample with eye tracking data (n=17), resulting in lower scores compared to <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>.</p></caption>
<graphic xlink:href="670974v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b2">
<title>Decodability from gaze data</title>
<p>As for the EEG data, we trained another set of time-resolved, linear classifiers to test the decodability of relevant attributes—however, now from the two-dimensional gaze data (i.e., the orientation of the combined eyes in spherical coordinates). Using cluster-corrected permutation testing, we demonstrated significant decoding performance for the classifier which successfully distinguished between the four emotional expressions. This was indicated by a cluster of significant decoding scores which started 230 ms after stimulus onset and extended to the end of the epoch (1,000 ms). The highest decoding performance (ROC AUC; M = 0.59, SD = 0.03, 95% CI [0.57, 0.60]) was observed for most participants at Mdn = 700 ms (SD = 230.04, 95% CI [590.65, 809.35]). Using the four time windows which we also used to bin the EEG data (P1, N170, EPN, LPC), we found a significant effect of the time window on the decoding performance (F(3,48) = 26.78, p &lt; .001), with significant pairwise differences between all time windows apart from P1 vs N170 and EPN vs LPC (in other words, no differences between the two early and between the two late time windows). Only for the two later time windows, EPN and LPC, the performance of the decoder was significantly above chance level.</p>
<p>We also found significant decoding performance for all binary classifiers (pairwise distinction of facial expressions) supported by cluster-corrected permutation testing against chance-level. Further, a rmANOVA comparing the average decoding performance across the binary contrasts and the four time windows revealed significant main effects for the the decoding contrast (F(5,80) = 3.12, p = .013) and the time window (F(3,48) = 29.70, p &lt; .001), but no significant interaction (F(15,240) = 1.50, p = .106). Post-hoc tests (pairwise t-tests, Bonferroni corrected) showed significant differences in the decoding performance between all time windows, apart from P1 and N170 which did not differ significantly. Later time windows yielded higher decoding performances than early time windows. The post-hoc pairwise comparisons of the contrasts did not reveal any significant differences (after Bonferroni correction).</p>
</sec>
<sec id="s2b3">
<title>Stereoscopic information</title>
<p>As for the EEG data, the classifier performance when decoding facial expressions from eye tracking data did not differ between monoscopic and stereoscopic viewing conditions. For the decoding of the facial expressions, there were no significant differences in peak performance (mono: M = 0.60, SD = 0.03, 95% CI [0.58, 0.61]; stereo: M = 0.59, SD = 0.03, 95% CI [0.57, 0.60]; t(16) = 1.88, p = .079) or timing (mono: Mdn = 741.67 ms, SD = 268.15, 95% CI [614.20, 869.14]; stereo: Mdn = 650.00 ms, SD = 301.43, 95% CI [506.71, 793.29]; t(16) = 1.11, p = .283). This finding was indirectly supported by a non-significant cluster-based permutation test contrasting the time-resolved performances for expression decoding in the mono- and the stereoscopic condition. Similarly, no binary classifier showed significant performance differences between the two depth conditions.</p>
<p>In contrast to the EEG data, a separate classifier trained to explicitly differentiate between the two depth conditions (monoscopic vs stereoscopic) based on the time-resolved eye-tracking data did not perform above chance level (see <xref rid="fig4" ref-type="fig">Figure 4d</xref>). However, a classifier trained to differentiate between the face identities achieved above chance-level decoding, indicated by a cluster of time-points with significant decoding accuracy, beginning at 242 ms after stimulus onset and continuing until the onset of the response panel (peak decoding score: M = 0.60, SD = 0.03, 95% CI [0.58, 0.61]; peak decoding time: Mdn = 550.00 ms, SD = 231.29, 95% CI [440.05, 659.95]; <xref rid="fig4" ref-type="fig">Figure 4d</xref>).</p>
</sec>
<sec id="s2b4">
<title>The relationship between EEG and eye tracking based decoding performances</title>
<p>On the group level, there was no significant correlation between the performance of the decoder trained to distinguish facial expressions based on EEG data and the one based on eye tracking data. This applied to peak decoding performance (r(15) = 0.059, p = .821), latency of the peak performance (r(15) = -0.160, p = .540), and average decoding performance across the observation window (r(15) = -0.031, p = .907). Decoding performance based on EEG data surpassed decoding based on eye tracking data early in the trial, as indicated by a significant cluster between 94 and 224 ms (see <xref rid="fig4" ref-type="fig">Figure 4d</xref>, left). For the remaining time of the observation window, there was no significant difference between EEG and eye tracking based decoding of facial expressions. By contrast, the decoders for facial <italic>identity</italic> displayed a different relationship between their performance profiles (see <xref rid="fig4" ref-type="fig">Figure 4d</xref>, middle): the EEG-based decoding again ramped up earlier than the eye tracking-based decoding (first significant cluster: 164–194 ms), but at later stages, eye tracking-based decoding outperformed EEG-based decoding (second significant cluster with opposite-signed cluster mass: 384–1,000 ms). In decoding the depth condition, the classifier trained on EEG data outperformed the classifier trained on eye tracking data (significant clusters: 114–224 ms and 244–474 ms), with the latter never reaching a performance level above chance.</p>
</sec>
</sec>
<sec id="s2c">
<title>Behavior</title>
<p>We compared the emotion ratings provided by the participants with the objective categorization of the facial expression (i.e., the FACS ratings provided by <xref ref-type="bibr" rid="c30">Gilbert et al., 2021</xref>). The results are shown in the form of confusion matrices in Supplementary Figure S1. Recognition rates differed across the emotional expressions (F(3,96) = 6.81, <italic>p</italic> &lt; .001) but were high for all expressions (correct: M = 90.73%, SD = 16.98, 95% CI [89.06, 92.41]). The lowest recognition level was found for neutral faces (correct: M = 80.32%, SD = 25.12, 95% CI [71.75, 88.89]) which were classified second most frequently as “other” (confusion rate: M = 17.39%, SD = 23.51, 95% CI [9.37, 25.41]). We did not observe a difference in recognition rate for the mono- (M = 90.96 %, SD = 16.79, 95% CI [88.10, 93.82]) and the stereoscopic depth conditions (M = 90.51 %, SD = 17.32, 95% CI [87.55, 93.46]). Accordingly, there was neither a significant main effect of depth condition (F(2, 64) = 1.85, p = .165) nor a significant interaction with the emotional expression (F(6, 192) = 1.37, p = .230).</p>
<p>To control for the experienced level of intensity, at the end of the experiment, we asked participants to rate for each of the 24 stimuli (4 emotions, 3 face identities, 2 depth conditions; <xref rid="fig1" ref-type="fig">Figure 1c</xref>) the intensity of the displayed emotional facial expression using the 9-point SAM scale for arousal (Supplementary Figure S2). We observed a significant main effect of emotion (F(3,93) = 118.95, p &lt; .001): Angry faces received the highest intensity ratings (M = 7.82, SD = 0.93, 95% CI [7.50, 8.14]), while neutral faces received the lowest (M = 2.69, SD = 1.66, 95% CI [2.11, 3.26]). Happy (M = 4.60, SD = 1.47, 95% CI [4.09, 5.11]) and surprised faces (M = 6.15, SD = 0.96, 95% CI [5.81, 6.48]) fell in between. Neither the main effect of the depth condition (F(1,31) = 0.01, p = .932) nor its interaction with the emotional expression was significant (F(3,93) = 0.34, p = .794).</p>
</sec>
</sec>
<sec id="s2d">
<title>Discussion</title>
<p>We investigated the neurocognitive processes underlying the recognition of facial expressions in immersive VR and if the presence of stereoscopic depth cues changes the neural representations of face stimuli. In each trial, we showed a computer-generated face, which displayed one out of four facial expressions. As the main measure of interest we used time-resolved, multivariate decoding from EEG data to infer how much information about the stimulus is present in the brain signals. Our main decoding target was the emotional expression shown by the face—which the participants had to indicate at the end of each trial. Facial expressions could be decoded from EEG data with robust above-chance accuracy. At the same time, the decoding performance was not influenced by the availability of stereoscopic depth information. We demonstrate that information about the presence of stereoscopic depth cues is present in the EEG signals nevertheless: With a separate classifier we could successfully distinguish mono- from stereoscopic trials. The temporal profile of this decoding differed from decoding the expressions but resembled the temporal profile of decoding the identity of the stimulus face, another task-irrelevant feature. We therefore conclude that in neurocognitive processing of face stimuli, stereoscopic depth information is treated like other task-irrelevant stimulus features and—at least as long as the behavioral task can be easily solved without integrating this information—it does not modify the (measurable) neurophysiological representation.</p>
<sec id="s2d1">
<title>Successful decoding of emotional expression</title>
<p>Our decoding results reveal that the EEG data recorded in our VR setup contained information about the emotional expression of the stimulus faces. Previous studies have demonstrated that similar linear EEG classifiers could distinguish between different facial expressions presented as grayscale, two-dimensional images on conventional computer screens (<xref ref-type="bibr" rid="c82">Smith &amp; Smith, 2019</xref>). Our results replicate and extend these findings by showing that time-resolved multivariate decoding yields comparable results when using naturalistic, 3D face stimuli in immersive VR settings. Such setups are gaining interest in human perception and cognition research for their ability to provide more naturalistic stimulation—closer to how people perceive and experience the world—thereby potentially improving the generalizability of experimental findings to real-world contexts (<xref ref-type="bibr" rid="c16">Cisek &amp; Green, 2024</xref>). Yet, VR also introduces substantial challenges compared to the traditional, more restrictive setups. For example, combining EEG with a VR headset can lower the signal-to-noise ratio by introducing artifacts from mechanical pressure on the electrodes, muscular activity caused by head movements or headset weight, or electrical noise from the device and its cables (<xref ref-type="bibr" rid="c86">Tauscher et al., 2019</xref>; <xref ref-type="bibr" rid="c94">Weber et al., 2021</xref>). Despite these challenges, our study shows that EEG signal quality in controlled VR settings is sufficient to reliably discriminate between facial expressions and identities by using linear classifiers applied to an observer’s EEG data. In this context, multivariate decoding is especially valuable, condensing high-dimensional EEG data into a one-dimensional, time-resolved, and interpretable metric—decoding performance—via optimized, data-driven weighting of EEG channels. Cross-validation ensures that the classification results are generalizable and not due to overfitting. In contrast, traditional ERP analysis rely on strong a priori assumptions (e.g., preselected electrodes) and often require extensive post-hoc corrections for multiple comparisons, yet rarely employ cross-validation. Prior studies have shown that multivariate decoding offers higher sensitivity than conventional ERP analyses (<xref ref-type="bibr" rid="c13">Carrasco et al., 2024</xref>; <xref ref-type="bibr" rid="c35">Grootswagers et al., 2017</xref>), which is particularly relevant for data prone to low signal-to-noise ratios. Future VR studies on face processing in immersive contexts may build on our findings to inform their methodological choices and anticipate effect sizes.</p>
<p>Consistent with the findings by <xref ref-type="bibr" rid="c82">Smith &amp; Smith (2019)</xref>, our EEG decoding results showed distinct patterns for task-relevant and task-irrelevant features: emotional expression (task-relevant) yielded sustained decodability after it surpassed chance level, whereas the decoding performance for task-irrelevant features (here: face identity and availability of stereoscopic depth cues) peaked early, followed by temporal decline (<xref rid="fig2" ref-type="fig">Figure 2</xref>).</p>
</sec>
<sec id="s2d2">
<title>The role of stereoscopic depth cues</title>
<p>We hypothesized that 3D viewing—by more closely approximating natural perceptual contexts— would elicit more robust and distinct neural representations, thereby enhancing decoding performance. Contrary to this prediction, we found no significant differences in decoding performance between the two conditions for either a task-relevant (expression) or a task-irrelevant (identity) decoding target. Stereoscopic information did not enhance decoding performance, alter its temporal dynamics, or affect the spatial distribution of the classifier weights. These findings were corroborated by cross-condition generalization tests (cross-decoding), where classifiers trained and tested on stereoscopic trials performed equally well when tested on monoscopic trials, and vice versa (Supplementary Figure S5). If 3D and 2D percepts relied on fundamentally different neural activation patterns, we would expect reduced generalization across conditions. However, our results provide no evidence for such a difference, suggesting that the neurophysiological activation patterns underlying facial expression categorization remain largely consistent regardless of the presence of stereoscopic depth information.</p>
<p>If the subjective percepts in the two conditions are markedly different, why do the underlying neural activation patterns associated with the decoded features show no significant differences? Importantly, our main analyses employed activation patterns optimized for decoding specific target variables (facial expression, face identity), not representing the brain’s holistic response to the stimuli. Within this scope, 3D and 2D conditions did not show differences. However, when directly decoding the presence of stereoscopic depth cues, we identified significant distinctions in the neural responses, peaking around 180 ms after stimulus onset and subsequently declining— similar to the temporal profile for decoding face identity, another task-irrelevant feature (<xref rid="fig2" ref-type="fig">Figure 2d</xref>). We thereby demonstrate that the distinction between viewing conditions with and without stereoscopic depth cues was detectable in our neurophysiological data. This adds to previous findings regarding binocular depth processing gained with different methods (from single cell recordings in non-human primates to fMRI in humans; for an overview see <xref ref-type="bibr" rid="c95">Welchman, 2016</xref>) and stimuli (from face pictures; <xref ref-type="bibr" rid="c15">Chou et al., 2021</xref>; to abstract random dot motion; <xref ref-type="bibr" rid="c64">Parker, 2007</xref>). Our study addresses a relevant research gap by demonstrating that EEG data in a VR setup with naturalistic, computer-generated stimuli can track stereoscopic depth processing. To our knowledge, this is the first study demonstrating that face stimuli differing only in stereoscopic depth cues can be differentiated through EEG decoding while investigating potential interference with the representation of other stimulus features.</p>
</sec>
<sec id="s2d3">
<title>Cortical sources</title>
<p>In an exploratory analysis, we applied source reconstruction to project the spatial patterns associated with the decoders back onto the cortical surface. For the classification of emotional expressions, primary and early visual cortices contributed most substantially, especially shortly after stimulus onset (P1, N170, and EPN windows). This dominance was particularly evident in the binary contrasts with the highest decoding performance (“angry-vs-neutral”, “angry-vs-happy”; see Supplementary Figure S9 and Supplementary Table ST2). In contrast, lower-performing binary classifiers (e.g., “angry-vs-surprised”), relied primarily on regions in lateral occipital and posterior inferotemporal cortex. This suggests that decoding was more reliable when information about the contrast was also present in primary or early visual cortices. This aligns with previous studies which found that early visual cortices, which mostly process low-level stimulus features, support effective visual input decoding (<xref ref-type="bibr" rid="c45">Kamitani &amp; Tong, 2005</xref>; <xref ref-type="bibr" rid="c57">Lützow Holm et al., 2024</xref>; <xref ref-type="bibr" rid="c97">Wilson et al., 2024</xref>). Accordingly, the contrasts with the highest decoding performance expressed the most salient visual differences between the involved faces. For example, angry faces were characterized by a widely opened mouth showing both rows of teeth, whereas neutral and happy faces had a closed mouth with no teeth visible. The decoding of these contrasts may have relied heavily on these low-level feature representations. Nevertheless, contrasts with more subtle low-level visual differences still yielded above chance-level decoding, primarily drawing on brain regions higher in the visual processing hierarchy. When two facial expressions lacked sufficient differences in their low-level visual features, decoding appeared to rely more on signals from cortical areas known to process abstract features of facial expressions. Overall, our findings indicate that also for naturalistic face stimuli the decoding-based approach does not simply exploit arbitrary features, rely solely on low-level visual features, or exclusively depend on abstract representations of the observed facial expression.</p>
<p>Running the source reconstruction separately for the mono- and stereoscopic conditions yielded only slightly different distributions across the cortical surfaces as compared to running it on the pooled data (see Supplementary Table ST2). We attribute these variations to the fact that splitting the data set leads to less robust source estimates and therefore a higher variance. Overall, we did not observe a systematic difference between the distribution of sources in the mono- as compared to the stereoscopic viewing condition, congruent with the numeric decoding results.</p>
<p>The decoder trained to explicitly categorize the depth condition (i.e., separating mono- from stereoscopic trials) mostly relied on information from areas in the ventral visual stream. Also, the decoding of the different face identities was mostly informed by areas in the ventral visual stream—in particular during the N170 time window where the decoding performance was highest. This aligns with previous findings that the N170 reflects activity of the fusiform face area (<xref ref-type="bibr" rid="c29">Gao et al., 2019</xref>) and neighbouring ventral areas which process information relevant for distinguishing different faces (<xref ref-type="bibr" rid="c21">Eimer, 2011</xref>; <xref ref-type="bibr" rid="c34">Grill-Spector et al., 2017</xref>; <xref ref-type="bibr" rid="c46">Kanwisher &amp; Yovel, 2006</xref>).</p>
<p>Our spatially resolved source reconstruction has limitations: We used a common average head model without individual anatomical brain scans, relied on the canonical locations in the 10-20 system without participant-specific registration, and faced potential spatial distortions (i.e., electrode displacement) from the VR headset worn over the EEG cap. Despite these constraints, we successfully recorded and analyzed spatially interpretable signals that meaningfully map onto the cortical surface.</p>
</sec>
<sec id="s2d4">
<title>Computer-generated faces as stimuli</title>
<p>We used computer-generated stimuli, unlike most EEG studies on human face processing which have typically relied on standardized photographs (e.g., from the Chicago Face Database; <xref ref-type="bibr" rid="c58">Ma et al., 2015</xref>). Computer-generated stimuli can be precisely tailored to the specific requirements of each study whereas photographs are less modifiable and challenging to alter without introducing visible distortions. For example, to produce gradual (i.e., parametrically adjustable) versions of facial expressions, “morphing” between versions of two existing images has been used (<xref ref-type="bibr" rid="c84">Steyvers, 1999</xref>). In recent years, more complex generative algorithms have gained momentum which allow for the creation of artificial but increasingly naturalistic face stimuli both in 2D and 3D (e.g., <xref ref-type="bibr" rid="c3">Barthel et al., 2025</xref>; <xref ref-type="bibr" rid="c26">Feng et al., 2021</xref>). Here, we used 3D models and predefined blend shape modifications to generate a set of static facial expressions. These expressions had been evaluated in 2D (i.e., rendering 2D images of the generated 3D models) by an independent sample of both experts and non-experts (<xref ref-type="bibr" rid="c30">Gilbert et al., 2021</xref>), ensuring participants could clearly distinguish them—increasing the likelihood of obtaining separable EEG signals. Our results establish that robust EEG decoding is achievable in immersive VR when using clearly distinguishable face stimuli, serving as a foundation for future studies using more ambiguous facial stimuli to explore how these affect both behavioral responses and EEG decodability.</p>
</sec>
<sec id="s2d5">
<title>Behavioral recognition</title>
<p>Participants were clearly able to recognize the displayed emotional facial expressions. We designed the stimuli by selecting facial expressions (i.e., FACS configurations) with empirically validated high recognition rates (<xref ref-type="bibr" rid="c30">Gilbert et al., 2021</xref>), minimizing ambiguities in decoding arising from subjective indecisiveness. This approach ensured that potential challenges encountered during neural decoding could be ascribed to a lack of information in the EEG signal itself (e.g., low signal-to-noise ratio), rather than to the relevant information not being encoded in the first place. While future studies using more ambiguous facial expressions may reveal additional aspects of stereoscopic information processing, here we sought to minimize the risk of uninterpretable null results due to non-significant decoding scores. A prior EEG decoding study using clearly distinguishable facial expressions achieved only modest above-chance performance (<xref ref-type="bibr" rid="c82">Smith &amp; Smith, 2019</xref>), and VR-EEG setups are likely to exacerbate noise-related reductions in effect size. We therefore prioritized maximizing decoding feasibility in VR-EEG as a foundational step for subsequent work, with a special focus on the introduction of stereoscopic depth cues.</p>
<p>Building on our results and effect sizes, future studies can now more confidently work their way towards more ambiguous facial expressions while studying the concurrent neurophysiology. We cannot exclude the possibility that for more ambiguous facial expressions, for which decoding is more difficult (both for the human observer and the EEG decoder), stereoscopic depth cues become more critical. Similarly, in computer vision-based recognition of human facial expressions, it has been shown that, particularly for low-intensity expressions, models incorporating 3D information outperform those using only 2D information (<xref ref-type="bibr" rid="c75">Savran et al., 2012</xref>).</p>
</sec>
<sec id="s2d6">
<title>Emotional intensity of the stimuli</title>
<p>Participants’ subjective ratings at the end of the experiment indicated differences in emotional intensity between the facial expressions and the different identities. Stereoscopic depth had no influence on the intensity ratings.</p>
<p>While the stimulus intensity may partially explain the EEG decoding results, for example, through arousal contagion (<xref ref-type="bibr" rid="c41">Herrando &amp; Constantinides, 2021</xref>), this alone cannot fully account for the results. While emotional arousal has widespread effects in the human brain which can be read out by similar decoding approaches (<xref ref-type="bibr" rid="c42">Hofmann et al., 2021</xref>), these are typically not localized in early visual cortices—one of the most informative brain areas in the present study. Furthermore, face identity was decodable with similar accuracy to expressions despite substantially weaker differences between the identities in terms of emotional intensity. Finally, stereoscopic viewing conditions were successfully differentiated based on the EEG, despite no differences in the emotional intensity ratings between with and without stereoscopic depth cues.</p>
<p>However, the above argumentation may hold less for later times in the trial (&gt;500 ms after stimulus onset), during which the decoding performance declined for the task-irrelevant decoding targets (identity and depth condition) while it was constant for the decoding of the facial expressions. Also, some of the most informative sources shifted from early visual cortex to parietal and ventral temporal regions, which have been found to be modulated by emotional arousal (<xref ref-type="bibr" rid="c33">Greene et al., 2014</xref>; <xref ref-type="bibr" rid="c53">Lettieri et al., 2019</xref>; <xref ref-type="bibr" rid="c92">Wade-Bohleber et al., 2020</xref>). This suggests that early decoding reflects visual feature processing, while later stages may incorporate emotional arousal effects.</p>
</sec>
<sec id="s2d7">
<title>Eye tracking results and their relation to the EEG results</title>
<p>The EEG decoding might have been influenced by eye movements (<xref ref-type="bibr" rid="c61">Mostert et al., 2018</xref>), as participants could freely explore the faces (for 1,000 ms) after initial central fixation. Analyses of fixation locations and saccade parameters revealed that the saccade rate peaked around the EPN time window, with later fixations targeting the lower face—consistent with previous findings (<xref ref-type="bibr" rid="c80">Schurgin et al., 2014</xref>). Saccade direction, especially in the later part of the trial, was modulated by the facial expression—also consistent with previous findings (<xref ref-type="bibr" rid="c12">Calvo et al., 2018</xref>; <xref ref-type="bibr" rid="c80">Schurgin et al., 2014</xref>; <xref ref-type="bibr" rid="c88">Vaidya et al., 2014</xref>). Time-resolved linear classifiers showed comparable decoding performances from eye tracking and EEG data (<xref rid="fig4" ref-type="fig">Figure 4</xref>)—with similar temporal profiles—raising the question of whether EEG decoding might, in the end, simply reflect eye movements rather than neurophysiological representations of stimulus features (see (<xref ref-type="bibr" rid="c61">Mostert et al., 2018</xref>), for a more detailed discussion of this argument and potential underlying mechanisms).</p>
<p>Importantly, multiple analyses indicate that eye movements do not fully account for the observed EEG patterns. First, if the decodable activity in the EEG data was directly caused by eye movements (e.g., due to eyeball motion or by retinal image shifts), both time series should be perfectly time-aligned or show a slight lead for eye tracking signals. Instead, we observed a different pattern: eye tracking decodability consistently lagged behind EEG signals by 100–200 ms (see <xref rid="fig4" ref-type="fig">Figure 4d</xref>). This delay cannot be attributed to temporal imprecision during data recording. To compensate for potential lag in eye tracking data recorded with the HTC Vive Pro Eye, we post-hoc aligned it with the EEG time series, by cross-correlating the eye tracking data with the electrooculogram channels. We identified and corrected a similar delay as reported by <xref ref-type="bibr" rid="c83">Stein et al. (2021)</xref>. Stimulus information was therefore decodable from EEG <italic>before</italic> the concurrent eye tracking data, making it implausible that eye movements caused the patterns we observed in the EEG. Second, if EEG decodability were merely a byproduct of the eye movements, decoding scores from both modalities should be correlated (e.g., participants with highly/late decodable eye movements should exhibit high/late EEG decoding peaks). Contrary to this assumption, we found no systematic relationship between EEG and eye tracking-based decoding metrics on the participant level (see Supplementary Figure S7). Finally, while the decoding performance curves for EEG and eye tracking look similar when classifying facial expressions, the patterns differ for the task-irrelevant decoding targets (<xref rid="fig3" ref-type="fig">Figure 3d</xref>). Most prominently, the depth condition—while well decodable from EEG—was entirely undecodable from eye tracking data. These dissociations make it unlikely that EEG decoding in our data set merely reflects eye movements. While the similarity in some dynamics suggests shared information between the modalities, the dissociations strongly indicate that EEG decoding provides complementary insights, capturing aspects of neurocognitive processing beyond what eye tracking data alone can reveal—for example, the presence or absence of stereoscopic depth information.</p>
</sec>
<sec id="s3">
<title>Conclusion</title>
<p>In sum, our results demonstrate that emotional facial expressions can be reliably decoded from EEG in immersive VR, with no clear advantage observed under conditions involving stereoscopic depth cues. While the presence of stereoscopic depth elicited distinct neural signatures, it did not affect the decodability of other, orthogonal face features—whether task-relevant or irrelevant. This suggests that, in controlled facial emotion recognition tasks using clearly distinguishable, static stimuli, the brain treats stereoscopic depth like other independent stimulus features—at least from the perspective of decodable EEG representations. Our findings establish a methodological foundation for EEG-based decoding in immersive VR and invite future research to explore how richer, more ambiguous, or dynamic face stimuli might leverage stereoscopic depth information in behaviorally and neurophysiologically meaningful ways.</p>
</sec>
</sec>
<sec id="s3a">
<title>Methods</title>
<sec id="s3a1">
<title>Participants</title>
<p>We acquired data from 34 healthy, young, female adults (age: M = 26.65, SD = 4.56, 5 left-handed). Persons wearing glasses could not participate in the study to facilitate compatibility with the eye tracker. We ensured intact stereoscopic vision in all participants using a Titmus test (<italic>Fly-S Stereo Acuity Test</italic>, Vision Assessment Corporation, Hamburg, Germany). The study was approved by the ethics committee of the Department of Psychology at the Humboldt-Universität zu Berlin and participants provided their written consent prior to participation. Participants were compensated with 12 € per hour.</p>
<p>The final sample consisted of data from 33 participants for the EEG analyses (incomplete data for 1 participant) and 17 participants for the eye tracking control analyses (due to technical failure, no eye tracking data were stored for the remaining 16 participants).</p>
</sec>
<sec id="s3a2">
<title>Setup</title>
<p>We recorded participants’ electroencephalogram (EEG) and electrooculogram (EOG) using a LiveAmp amplifier (BrainProducts GmbH, Gilching) and 60 active electrodes according to the 10-20 system in an electrode cap (actiCAP snap; BrainProducts). Four EOG electrodes were placed below the eyes and next to the outer canthi. All electrodes were referenced to electrode FCz (ground: FPz). EEG and EOG were sampled at a rate of 500 Hz and using a hardware-based low-pass filter at 131 Hz (third-order sinc filter, −3 dB cutoff). We ensured that at the beginning of the experiment, the impedances of all electrodes were below 25 kΩ.</p>
<p>During the VR part of the study, participants were seated and wore a VR headset (HTC Vive Pro Eye, HTC, Taiwan) with an integrated eye tracker (sampling rate: 120 Hz). We adjusted the headset at the beginning of the experiment to match the interpupillary distance of the participant. The VR headset was positioned on top of the EEG cap (covered by a disposable shower cap). The flat design of the actiCAP snap electrodes, combined with the cushioning of the HTC Vive Pro’s back section, facilitated an even weight distribution, minimizing excessive pressure on any individual electrode. A custom facial interface cushion with recesses at designated locations prevented pressure on the frontal EEG electrodes (Fp1/2).</p>
</sec>
<sec id="s3a3">
<title>Software</title>
<p>For the implementation of the experiment, we used the Unity game engine (v2020.3.3f1; Unity Technologies) in combination with SteamVR (Valve Corporation) and ran it on a VR-ready PC (Intel Core i9-9900K, 3.6 GHz, 32 GB RAM, NVIDIA RTX 2080Ti GPU, Windows 10). The computer was connected to the EEG amplifier via an analog port (D-SUB 25) to enable synchronization of the EEG data with the experimental events. We used the EDIA toolbox (<xref ref-type="bibr" rid="c47">Klotzsche et al., 2025</xref>)—an extension of the Unity Experiment Framework (v2.3.4; <xref ref-type="bibr" rid="c6">Brookes, 2017/2019</xref>; <xref ref-type="bibr" rid="c7">Brookes et al., 2020</xref>)—for structuring the experiment, recording events, and tracking the behavior (incl. eye movements). The eye tracker was interfaced using the SRanipal SDK and EDIA to allow for sampling the eye tracking data at 120 Hz. To synchronize the data streams (i.e., stimulus onset events, eye tracking, EEG), we used custom C# scripts to send analog triggers (EEG) and store associated timestamps (eye tracking and event data). We recorded EEG and EOG data using the BrainVision Recorder software (v1.22.0101; BrainProducts). Before and after the VR-based part of the experiment, participants filled in questionnaires implemented in SoSciSurvey (<xref ref-type="bibr" rid="c52">Leiner, 2019</xref>). Data analysis was performed using Python (v3.10.4), Scikit-learn (v1.5.1; <xref ref-type="bibr" rid="c66">Pedregosa et al., 2011</xref>), SciPy (v1.14.0; <xref ref-type="bibr" rid="c89">Virtanen et al., 2020</xref>), statsmodels (v0.14.2; <xref ref-type="bibr" rid="c81">Seabold &amp; Perktold, 2010</xref>), and NumPy (v1.26.4; <xref ref-type="bibr" rid="c37">Harris et al., 2020</xref>).</p>
</sec>
<sec id="s3a4">
<title>Stimuli</title>
<p>The stimulus set entailed computer-generated faces from three digital humans, each showing four different facial expressions (angry, happy, neutral, surprised; see <xref rid="fig1" ref-type="fig">Figure 1c</xref>), yielding 12 static face stimuli in total. We generated the faces using the open-source software makehuman and the FACSHuman plugin (<xref ref-type="bibr" rid="c30">Gilbert et al., 2021</xref>), based on material provided by the first author of the plugin. <xref ref-type="bibr" rid="c30">Gilbert et al. (2021)</xref> provide validations of 2D, grayscale versions of these stimuli. Two independent raters, professionally trained in the Facial Action Coding System (FACS; <xref ref-type="bibr" rid="c22">Ekman &amp; Friesen, 1978</xref>), had approved the adherence of the emotional expressions to the FACS criteria and their recognizability was demonstrated in a sample of naïve participants in an online experiment (<xref ref-type="bibr" rid="c30">Gilbert et al., 2021</xref>). For the present study, we selected the four emotional expressions which were best recognized and distinguished from one another in this validation experiment. We chose only female faces and we manually masked the hair and the ears of the 3D models (using the Blender software; v2.93, <xref ref-type="bibr" rid="c4">Blender Development Team, 2021</xref>) to decrease task-unrelated differences between the stimuli. Each face stimulus measured approximately 24 cm in vertical height (from the crown of the head to the base of the neck), roughly matching the dimensions of an adult human face in physical reality. We presented the faces against a gray background and at a distance of 1.37 virtual meters from the perspective of the participant, so that for the observer each face spanned approximately 10 degrees of visual angle (dva) along its vertical axis. Stimuli were presented centrally in the visual field in a way that the virtual head frontally faced the participant irrespective of the orientation of the participant’s head (i.e., in <italic>local space</italic>). Head movements, therefore, did not lead to displacements of the stimuli within the visual field. For the 3D condition, we displayed the regular three-dimensional model of the face stimuli, yielding a stereoscopic view in the VR headset. The VR software renders two slightly different viewpoints of the scene to the displays in the headset, thereby enabling stereoscopic depth cues. In the 2D condition, we replaced the 3D model by a two-dimensional “portrait shot” taken by a single cyclopean (virtual) camera and displayed as a 2D plane (canvas) in the Unity scene. The 2D image was size- and position-aligned with the (invisible) 3D head model and then rendered to the displays in the headset, yielding an impression very similar to the rendering in the 3D condition, just without stereoscopic depth information (<xref rid="fig1" ref-type="fig">Figure 1b</xref>). The percept in the 2D condition was comparable to viewing a life-size portrait picture from a distance of 1.37 m.</p>
</sec>
<sec id="s3a5">
<title>Task</title>
<p>In the main task of the experiment, participants performed an emotion recognition task (<xref rid="fig1" ref-type="fig">Figure 1a</xref>). In each trial, after a fixation period of 500 ms, we displayed for 1,000 ms a single face stimulus centrally in the visual field of the participant. After the offset of the face, the participant was presented with five response buttons arranged in a cross-like fashion centrally in the visual field (see <xref rid="fig1" ref-type="fig">Figure 1a</xref>). The buttons displayed the four facial expression categories (neutral, angry, happy, surprised) as well as the option “other”. Using a VR controller, participants indicated their response by pointing a ray toward the corresponding virtual button. The assignment of the buttons to the positions on the cross was randomized across trials. Participants could therefore only decide where to press once the buttons were visible, thus avoiding preparatory activity during the presentation of the face stimulus. The next trial started immediately after the response or automatically 5,000 ms after stimulus offset.</p>
<p>In 50% of the trials, the face stimuli were shown with stereoscopic depth information. In the remaining trials, the 2D version of the face was presented. The experimental manipulations (facial expression, identity, depth condition) were fully interleaved and pseudorandomized, ensuring a uniform distribution of all experimental conditions within each block. Participants completed six experimental blocks with 120 trials each, yielding 720 trials in total. After each block, the experiment was paused and participants could take a short break (incl. removal of the VR headset and re-calibration of the eye tracker).</p>
<p>Before the start of the first block, participants completed 24 training trials to get familiar with the task and the stimuli. In these trials, each combination of face identity, facial expression, and depth condition was shown once in a random sequence. Data from the training trials were not analyzed. After the main block of the experiment, participants performed an intensity rating of all face stimuli. To this end, all 24 combinations of face identity, facial expression, and depth condition were shown in a random sequence in the VR headset. This time, the face stimulus was visible permanently (until the participant had responded) above a 9-point self-assessment manikin (SAM) scale (<xref ref-type="bibr" rid="c5">Bradley &amp; Lang, 1994</xref>) that allowed the participant to indicate the intensity (i.e., the degree of emotional arousal) of the displayed facial expression (see Supplementary Figure S2). Participants selected the according level with the VR controller. No EEG or eye tracking data were recorded for this part.</p>
</sec>
<sec id="s3a6">
<title>EEG preprocessing</title>
<p>We processed the EEG data with MNE-Python (v1.2.3; <xref ref-type="bibr" rid="c32">Gramfort et al., 2013</xref>). First, we identified and rejected channels which were noisy throughout the experiment based on their power spectrum and visual inspection. Using ICA decomposition (extended Infomax), we then removed artifacts caused by blinks and eye movements. For fitting the ICA solution, we used a separate copy of the data which we filtered between 1 and 40 Hz (FIR filter with a Hamming window of length 1651 samples, lower/upper passband edge: 1.00/40.00 Hz, lower/upper transition bandwidth: 1.00/10.00 Hz, lower/upper −6 dB cutoff frequency: 0.50/45.00 Hz). From this copy, we extracted epochs of 1,300 ms in length, starting 300 ms before and ending 1,000 ms after the onset of the face stimuli. Epochs with particularly noisy EEG signals were identified using the autoreject software (v0.4.3; <xref ref-type="bibr" rid="c44">Jas et al., 2017</xref>) and removed before fitting the ICA weights. Based on their correlation with the bipolar EOG channels and visual inspection, we identified components related to blinks and other eye movements. Using only the remaining ICA weights (i.e., setting the weights of affected channels to zero), we cleaned a separate version of the data. This dataset was filtered between 0.1 and 40 Hz (FIR filter with Hamming window of length 16,501 samples, lower/upper passband edge: 0.10/40.00 Hz, lower/upper transition bandwidth: 0.10/10.00 Hz, lower/upper −6 dB cutoff frequency: 0.05/45.00 Hz) to preserve slow ERP components. Again, we extracted epochs of 1,300 ms in length, starting 300 ms before and ending 1,000 ms after the onset of the face stimuli. For baseline-correction, we subtracted the mean voltage during a 200 ms baseline interval (i.e., the 200 ms before stimulus onset) from each epoch. Subsequently, we used the autoreject package for local (i.e., per participant, sensor, and epoch) interpolation and trial rejection.</p>
</sec>
<sec id="s3a7">
<title>EEG decoding</title>
<p>We trained and cross-validated time-resolved linear classifiers on the EEG data to test the decodability of the facial expressions from the participant’s brain activity. A separate decoding model was validated for each participant using all available EEG channels. The samples used for training and testing the classifier were formed by averaging mini-batches of three trials from the same class. Using averaged data from mini-batches has been found to improve the performance of classifiers trained on EEG data due to suppression of noise (<xref ref-type="bibr" rid="c1">Adam et al., 2020</xref>; <xref ref-type="bibr" rid="c35">Grootswagers et al., 2017</xref>). Along the time-dimension, we downsampled the signal using a moving average of five samples (10 ms) with no overlap. For each time point, we then trained a logistic regression model (solver: liblinear, L2-regularization: λ = 1.0) on the data from all 60 EEG sensors and assessed its decoding performance by means of a stratified 5-fold cross-validation. In each fold, the model was trained on 80% of the mini-batches and tested on the remaining 20% while we kept the class proportions equal between train and test sets. Comparing the model predictions with the ground truth (FACS ratings of the facial expressions) in the test set, we calculated the area under the receiver operating characteristic (ROC-AUC) to assess the model’s decoding performance. To obtain a robust estimate, we performed this decoding procedure 50 times for each participant and time point—varying the random allocation of trials into mini-batches and randomly splitting samples into train and test sets. An overall decoding score per time point and participant was obtained by averaging the ROC-AUC scores across all repetitions.</p>
<p>We conducted cluster-based permutation tests on the group-level to determine whether the decoding performance was significantly above chance-level during the first second after stimulus onset. To this end, we calculated a one-sided paired t-test for each time point, testing if the actual decoding score was significantly higher than chance (ROC-AUC = 0.5). The cluster-based permutation procedure allowed for a correction of the number of tests (i.e., the number of time points). For a physiological interpretation of the decoding results, we calculated the <italic>patterns</italic> of the classifier (by multiplying the covariance of the EEG data with the filter weights; <xref ref-type="bibr" rid="c38">Haufe et al., 2014</xref>) for each time point.</p>
<p>To test the decodability of the facial expression (i.e., a four-class classification problem), we trained the above-mentioned model by applying a <italic>one-vs-rest</italic> (i.e., one vs the three remaining conditions) multi-class extension of the binary classifier. We further evaluated separate models for each two-sided contrast (i.e., six pairwise contrasts between the four emotional expressions) for enhanced interpretability.</p>
<p>We calculated the mean decoding score within four distinct time windows, based on canonical ERP components associated with face expression processing (P1, N170, EPN, LPC), to facilitate the interpretability of the decoding results across time. Using repeated-measures analyses of variance (rmANOVAs) and post-hoc <italic>t-</italic>tests (Bonferroni corrected), we modeled the decoding performance as a function of the <italic>time window</italic> (four levels) and the binary decoding <italic>contrast</italic> (six levels).</p>
<p>To test whether the presence of stereoscopic depth cues impacts the decodability, we performed the same decoding procedure separately with data from the mono- and stereoscopic depth conditions. We then compared the resulting decoding performances at each time window using two-sided, paired <italic>t</italic>-tests with cluster-correction. We also compared the monoscopic and the stereoscopic depth condition in terms of the peak decoding performance (highest decoding performance across all time points) and its latency using paired, two-sided <italic>t</italic>-tests. Further, we used a rmANOVA with the factors <italic>time window</italic> (P1, N170, EPN, LPC) and <italic>depth condition</italic> (mono- vs stereoscopic) to model the decoding scores. Finally, we used the same decoding procedure to make the <italic>depth condition</italic> itself the decoding target (i.e., predicting if a sample was measured under mono- or stereoscopic viewing conditions), as well as the <italic>identity</italic> of the stimulus face (three classes; see <xref rid="fig1" ref-type="fig">Figure 1c</xref>)—both task-irrelevant features of the stimulus.</p>
</sec>
<sec id="s3a8">
<title>Source reconstruction</title>
<p>We applied source reconstruction to project the spatial patterns obtained from the decoding process onto the cortical surface. Specifically, we used exact low-resolution tomography analysis (eLORETA; <xref ref-type="bibr" rid="c65">Pascual-Marqui, 2007</xref>) to localize the sources corresponding to the extracted components. The reconstruction utilized the standard average forward model from FreeSurfer (<xref ref-type="bibr" rid="c27">Fischl et al., 1999</xref>) as provided by MNE-Python, restricting the solution to dipoles with fixed orientations perpendicular to the cortical surface. Using eLORETA, we constructed spatial filters for each voxel from the leadfield matrix. The sources were then computed by multiplying the demixing matrix with the spatial patterns (<xref ref-type="bibr" rid="c38">Haufe et al., 2014</xref>) derived from the classifiers trained in EEG channel space. For multiclass classifiers, this procedure was performed separately for each underlying binary (one-vs-rest) classifier, and the normalized source time courses were averaged (per time point) to produce a single overall source time course for the multiclass decoder. To determine the most informative source within the canonical ERP time windows, we calculated the average weight of each vertex over the respective time intervals, identified the vertex with the highest average weight, and classified its corresponding brain region using the reduced parcellation atlas (23 labels per hemisphere) provided by <xref ref-type="bibr" rid="c31">Glasser et al. (2016)</xref> and <xref ref-type="bibr" rid="c60">Mills (2016)</xref>.</p>
</sec>
<sec id="s3a9">
<title>Eye tracking analyses</title>
<p>We analyzed the eye tracking data (combined gaze data for left and right eye as provided by the eye tracking SDK) in a subsample of 17 participants (due to a technical error no eye tracking data were recorded for the remaining participants). To compensate for the internal processing time of the eye tracker—specifically, the delay between recording an eye tracking sample and the moment when the corresponding sample is provided by the eye tracking SDK (<xref ref-type="bibr" rid="c83">Stein et al., 2021</xref>)—and to ensure temporal alignment of the eye tracking data with the EEG data, we first performed a cross-correlation between the eye tracking data and the EOG channels of the EEG data. This yielded a temporal offset between the two signals of on average 60 ms (<italic>SD</italic> = 3.80, range: 50–67 ms); this delay was corrected by subtracting it from the timestamps in the eye tracking samples. We performed a baseline-correction procedure on a trial-by-trial basis, in which we subtracted the mean gaze position during the 200 ms time window before the onset of the stimulus face from the gaze positions in the entire trial. Furthermore, we identified blinks using the algorithm based on the dynamics of the pupil dilation values suggested by <xref ref-type="bibr" rid="c49">Kret &amp; Sjak-Shie (2019)</xref>, which specifically looks for rapid changes in pupil size. We then linearly interpolated the gaze values during the times of identified blinks (plus a safety margin of 100 ms on each side).</p>
<p>We applied the same decoding approach as we used for the EEG data: we trained and evaluated a time-resolved linear classifier with a similar architecture (L2-regularized logistic regression; λ = 1.0; repeated randomized 5-fold cross-validation) for each participant. Here, we decoded the target variables (e.g., facial expressions, depth conditions, face identities) from the time-resolved, two-dimensional gaze direction. To this end, we converted the gaze vector (i.e., viewing direction) into spherical coordinates: phi (vertical) and theta (horizontal component). In contrast to the EEG-decoding, we did not apply downsampling nor mini-batching for the decoding from eye tracking data because, in comparison to EEG, eye tracking data are substantially less prone to measurement noise and single-trial data are more reliable.</p>
<p>In exploratory analyses, we analyzed the gaze behavior by classifying saccades and fixations on a single trial level. Saccade detection was based on a velocity-based algorithm with noise-dependent threshold (i.e., 5 SDs velocity threshold and minimum duration of 3 samples; <xref ref-type="bibr" rid="c25">Engbert et al., 2015</xref>; <xref ref-type="bibr" rid="c23">Engbert &amp; Kliegl, 2003</xref>; <xref ref-type="bibr" rid="c24">Engbert &amp; Mergenthaler, 2006</xref>). For these analyses, we excluded saccades with amplitudes smaller than 2 degrees of visual angle. This exclusion criterion was implemented due to the limitations in precision and sampling rate of the eye tracker within the VR headset, which make the classification and description of smaller saccades unreliable. For fixations, we retained only those with a duration of at least 50 milliseconds. To calculate the fixation heatmaps in <xref rid="fig4" ref-type="fig">Figure 4</xref>, we followed the recommendation provided by <xref ref-type="bibr" rid="c51">Le Meur &amp; Baccino (2013)</xref>: On the subject-level, we calculated the average spherical coordinates for each fixation, binned the fixations into spatial bins of a width and height of 0.1 dva, and convolved the resulting two-dimensional histogram (across all trials of the respective condition) with a two-dimensional gaussian kernel (<italic>SD</italic> = 1 dva for both dimensions and default parameters for the method “gaussian_kernel” from the SciPy Python module). We calculated a 4x2x4 MANOVA which modeled the average fixation position (two-dimensional, spherical coordinates) as a function of the face’s expression (neutral, angry, happy, surprised), depth condition (mono, stereo), and time window (P1, N170, EPN, LPC). We used the same model to analyze the (two-dimensional) saccade directions. Further, we applied Generalized Linear Models (GLM) to model the number of saccades (Poisson, log-linear link function) and the saccade amplitude (Gaussian, log-linear) as a function of emotional expression, time window, and depth condition.</p>
</sec>
<sec id="s3a10">
<title>Behavioral data</title>
<p>We compared participants’ behavioral categorizations of each trial’s facial expression with the objective categorizations based on the FACS values to calculate confusion matrices (Supplementary Figure S1). Due to the high level of agreement, we used the FACS categories as ground truth for the further analyses. To test for differences in the recognition performance between the mono- and the stereoscopic depth condition, we calculated a rmANOVA, modeling the recognition scores (i.e., the values on the diagonal of the confusion matrices in Supplementary Figure S1) as a function of the depth condition (two levels), the emotional expression (four levels), and their interaction. Finally, we modeled the intensity ratings from the last part of the experiment (based on the 9-point SAM scale) using a rmANOVA with the factors emotional expression and depth condition.</p>
</sec>
</sec>
</body>
<back>
<sec id="s4a1" sec-type="data-availability">
<title>Data availability</title>
<p>The data that support the findings of this study are openly available in “Edmond – the Open Research Data Repository of the Max Planck Society” at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17617/3.CQ2VXX">https://doi.org/10.17617/3.CQ2VXX</ext-link>. The code for data analysis can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/eioe/vr2f">https://github.com/eioe/vr2f</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Michael Gilbert for his support and guidance regarding the stimulus material, particularly for providing the original 3D face models from his validation study. We are also grateful to Jeroen de Mooij for his assistance and expertise in implementing the experiment in Unity. This research was supported by the cooperation between the Max Planck Society and the Fraunhofer Gesellschaft (grant: project NEUROHUM), by the German Federal Ministry of Education and Research (BMBF/BMFTR) under grants 13GW0206, 13GW0488, 16SV9156, and by the Deutsche Forschungsgemeinschaft (DFG) under grants 502864329 and 542559580. Relevant parts of this work were conducted at the Max Planck Dahlem Campus of Cognition (MPDCC) of the Max Planck Institute for Human Development, Berlin, Germany.</p>
</ack>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>CrediT statement</title>
<list list-type="simple">
<list-item><p>Felix Klotzsche: Conceptualization, Data curation, Formal Analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review &amp; editing;</p></list-item>
<list-item><p>Ammara Nasim: Conceptualization, Data curation, Investigation, Methodology, Writing – review &amp; editing;</p></list-item>
<list-item><p>Simon M. Hofmann: Conceptualization, Methodology, Software, Writing – review &amp; editing;</p></list-item>
<list-item><p>Arno Villringer: Funding Acquisition, Resources, Supervision;</p></list-item>
<list-item><p>Vadim Nikulin: Formal Analysis, Methodology, Supervision, Writing – review &amp; editing;</p></list-item>
<list-item><p>Werner Sommer: Conceptualization, Formal Analysis, Methodology, Supervision, Visualization, Writing – review &amp; editing;</p></list-item>
<list-item><p>Michael Gaebler: Conceptualization, Funding Acquisition, Resources, Supervision, Writing – original draft, Writing – review &amp; editing;</p></list-item>
</list>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary Material</label>
<media xlink:href="supplements/670974_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adam</surname>, <given-names>K. C. S.</given-names></string-name>, <string-name><surname>Vogel</surname>, <given-names>E. K.</given-names></string-name>, &amp; <string-name><surname>Awh</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Multivariate analysis reveals a generalizable human electrophysiological signature of working memory load</article-title>. <source>Psychophysiology</source>, <volume>57</volume>(<issue>12</issue>). <pub-id pub-id-type="doi">10.1111/psyp.13691</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Barry</surname>, <given-names>S. R.</given-names></string-name> (with <string-name><surname>Sacks</surname>, <given-names>O. W.</given-names></string-name></person-group>). (<year>2010</year>). <source>Fixing my gaze: A scientist’s journey into seeing in three dimensions / Susan R. Barry, foreword by Oliver Sacks</source>. <publisher-name>Basic Books</publisher-name>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Barthel</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Morgenstern</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Hinzer</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Hilsmann</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Eisert</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2025</year>). <article-title>CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis</article-title> <elocation-id>2505.17590</elocation-id>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2505.17590</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>Blender Development Team</collab></person-group>. (<year>2021</year>). <source>Blender</source> (Version <version>2.93</version>) [Computer software]. <ext-link ext-link-type="uri" xlink:href="https://www.blender.org">https://www.blender.org</ext-link></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bradley</surname>, <given-names>M. M.</given-names></string-name>, &amp; <string-name><surname>Lang</surname>, <given-names>P. J</given-names></string-name></person-group>. (<year>1994</year>). <article-title>Measuring emotion: The self-assessment manikin and the semantic differential</article-title>. <source>Journal of Behavior Therapy and Experimental Psychiatry</source>, <volume>25</volume>(<issue>1</issue>), <fpage>49</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1016/0005-7916(94)90063-9</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Brookes</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>). <source>Unity Experiment Framework</source> (Version v<version>2.3.4</version>) [Computer software]. <ext-link ext-link-type="uri" xlink:href="https://github.com/immersivecognition/unity-experiment-framework">https://github.com/immersivecognition/unity-experiment-framework</ext-link> (<comment>Original work published 2017</comment>)</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brookes</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Warburton</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Alghadier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Mon-Williams</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Mushtaq</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Studying human behavior with virtual reality: The Unity Experiment Framework</article-title>. <source>Behavior Research Methods</source>, <volume>52</volume>(<issue>2</issue>), <fpage>455</fpage>–<lpage>463</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-019-01242-0</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruce</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Young</surname>, <given-names>A</given-names></string-name></person-group>. (<year>1986</year>). <article-title>Understanding face recognition</article-title>. <source>British Journal of Psychology</source>, <volume>77</volume>(<issue>3</issue>), <fpage>305</fpage>–<lpage>327</lpage>. <pub-id pub-id-type="doi">10.1111/j.2044-8295.1986.tb02199.x</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brunet</surname>, <given-names>N. M</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Face processing and early event-related potentials: Replications and novel findings</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>17</volume>, <fpage>1268972</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2023.1268972</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burke</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Taubert</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Higman</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Are face representations viewpoint dependent? A stereo advantage for generalising across different views of faces</article-title>. <source>Vision Research</source>, <volume>47</volume>(<issue>16</issue>), <fpage>2164</fpage>–<lpage>2169</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2007.04.018</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calder</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Young</surname>, <given-names>A. W</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Understanding the recognition of facial identity and facial expression</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>6</volume>(<issue>8</issue>), <fpage>641</fpage>–<lpage>651</lpage>. <pub-id pub-id-type="doi">10.1038/nrn1724</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calvo</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Fernández-Martín</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gutiérrez-García</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Lundqvist</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Selective eye fixations on diagnostic face regions of dynamic emotional expressions: KDEF-dyn database</article-title>. <source>Scientific Reports</source>, <volume>8</volume>(<issue>1</issue>), <fpage>17039</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-35259-w</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carrasco</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Bahle</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Simmons</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Luck</surname>, <given-names>S. J</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Using multivariate pattern analysis to increase effect sizes for event-related potential analyses</article-title>. <source>Psychophysiology</source>, <volume>61</volume>(<issue>7</issue>), <fpage>e14570</fpage>. <pub-id pub-id-type="doi">10.1111/psyp.14570</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choi</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Kwon</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kaongoen</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hwang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>B. H.</given-names></string-name>, &amp; <string-name><surname>Jo</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Neural Applications Using Immersive Virtual Reality: A Review on EEG Studies</article-title>. <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source>, <volume>31</volume>, <fpage>1645</fpage>–<lpage>1658</lpage>. <pub-id pub-id-type="doi">10.1109/TNSRE.2023.3254551</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chou</surname>, <given-names>I. W. Y.</given-names></string-name>, <string-name><surname>Ban</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Chang</surname>, <given-names>D. H. F</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Modulations of depth responses in the human brain by object context: Does biological relevance matter?</article-title> <source>eNeuro</source>, <volume>8</volume>(<issue>4</issue>), <fpage>ENEURO.0039-21.2021</fpage>. <pub-id pub-id-type="doi">10.1523/ENEURO.0039-21.2021</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cisek</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Green</surname>, <given-names>A. M</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Toward a neuroscience of natural behavior</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>86</volume>, <fpage>102859</fpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2024.102859</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cumming</surname>, <given-names>B. G.</given-names></string-name>, &amp; <string-name><surname>Parker</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Responses of primary visual cortical neurons to binocular disparity without depth perception</article-title>. <source>Nature</source>, <volume>389</volume>(<issue>6648</issue>), <fpage>280</fpage>–<lpage>283</lpage>. <pub-id pub-id-type="doi">10.1038/38487</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cutting</surname>, <given-names>J. E.</given-names></string-name>, &amp; <string-name><surname>Vishton</surname>, <given-names>P. M.</given-names></string-name></person-group> (<year>1995</year>). <chapter-title>Perceiving layout and knowing distances: The integration, relative potency, and contextual use of different information about depth</chapter-title>. In <source>Perception of space and motion</source> (pp. <fpage>69</fpage>–<lpage>117</lpage>). <publisher-name>Academic Press</publisher-name>. <pub-id pub-id-type="doi">10.1016/B978-012240530-3/50005-5</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Draschkow</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Remote virtual reality as a tool for increasing external validity</article-title>. <source>Nature Reviews Psychology</source>, <fpage>1</fpage>–<lpage>2</lpage>. <pub-id pub-id-type="doi">10.1038/s44159-022-00082-8</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Yovel</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>A Revised Neural Framework for Face Processing</article-title>. <source>Annual Review of Vision Science</source>, <volume>1</volume>(Volume 1, 2015), <fpage>393</fpage>–<lpage>416</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035518</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Eimer</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2011</year>). <chapter-title>The Face-Sensitive N170 Component of the Event-Related Brain Potential</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>A. J.</given-names> <surname>Calder</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Rhodes</surname></string-name>, <string-name><given-names>M. H.</given-names> <surname>Johnson</surname></string-name>, &amp; <string-name><given-names>J. V.</given-names> <surname>Haxby</surname></string-name></person-group> (Eds.), <source>Oxford Handbook of Face Perception</source> (p. 0). <publisher-name>Oxford University Press</publisher-name>. <pub-id pub-id-type="doi">10.1093/oxfordhb/9780199559053.013.0017</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Ekman</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Friesen</surname>, <given-names>W. V.</given-names></string-name></person-group> (<year>1978</year>). <article-title>Facial action coding system</article-title>. <source>Environmental Psychology &amp; Nonverbal Behavior</source>. <ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org/doiLanding?doi=10.1037/t27734-000">https://psycnet.apa.org/doiLanding?doi=10.1037/t27734-000</ext-link>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engbert</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kliegl</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Microsaccades uncover the orientation of covert attention</article-title>. <source>Vision Research</source>, <volume>43</volume>(<issue>9</issue>), <fpage>1035</fpage>–<lpage>1045</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(03)00084-1</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engbert</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Mergenthaler</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Microsaccades are triggered by low retinal image slip</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>103</volume>(<issue>18</issue>), <fpage>7192</fpage>–<lpage>7197</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0509557103</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Engbert</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sinn</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mergenthaler</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Trukenbrod</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2015</year>). <source>Microsaccade Toolbox for R</source> (Version v<version>0.9</version>) [Computer software].</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Black</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Bolkart</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Learning an animatable detailed 3D face model from in-the-wild images</article-title>. <source>ACM Transactions on Graphics</source>, <volume>40</volume>(<issue>4</issue>), <fpage>88:1</fpage>–<lpage>88:13</lpage>. <pub-id pub-id-type="doi">10.1145/3450626.3459936</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Tootell</surname>, <given-names>R. B. H.</given-names></string-name>, &amp; <string-name><surname>Dale</surname>, <given-names>A. M</given-names></string-name></person-group>. (<year>1999</year>). <article-title>High-resolution intersubject averaging and a coordinate system for the cortical surface</article-title>. <source>Human Brain Mapping</source>, <volume>8</volume>(<issue>4</issue>), <fpage>272</fpage>–<lpage>284</lpage>. <pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:4&lt;272::AID-HBM10&gt;3.0.CO;2-4</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freiwald</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Yovel</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Face Processing Systems: From Neurons to Real-World Social Perception</article-title>. <source>Annual Review of Neuroscience</source>, <volume>39</volume>(<issue>1</issue>), <fpage>325</fpage>–<lpage>346</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013934</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Conte</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Hanayik</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The neural sources of N170: Understanding timing of activation in face-selective areas</article-title>. <source>Psychophysiology</source>, <volume>56</volume>(<issue>6</issue>), <fpage>e13336</fpage>. <pub-id pub-id-type="doi">10.1111/psyp.13336</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilbert</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Demarchi</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Urdapilleta</surname>, <given-names>I</given-names></string-name></person-group>. (<year>2021</year>). <article-title>FACSHuman, a software program for creating experimental material by modeling 3D facial expressions</article-title>. <source>Behavior Research Methods</source>, <volume>53</volume>(<issue>5</issue>), <fpage>2252</fpage>–<lpage>2272</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-021-01559-9</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Hacker</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Harwell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name></person-group> (<year>2016</year>). <article-title>A multi-modal parcellation of human cerebral cortex</article-title>. <source>Nature</source>, <volume>536</volume>(<issue>7615</issue>), <fpage>171</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1038/nature18933</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Luessi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Larson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Strohmeier</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Goj</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brooks</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Parkkonen</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Hämäläinen</surname>, <given-names>M. S</given-names></string-name></person-group>. (<year>2013</year>). <article-title>MEG and EEG Data Analysis with MNE-Python</article-title>. <source>Frontiers in Neuroscience</source>, <volume>7</volume>(<issue>267</issue>), <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greene</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Flannery</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Soto</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Distinct parietal sites mediate the influences of mood, arousal, and their interaction on human recognition memory. <italic>Cognitive, Affective</italic></article-title>, <source>&amp; Behavioral Neuroscience</source>, <volume>14</volume>(<issue>4</issue>), <fpage>1327</fpage>–<lpage>1339</lpage>. <pub-id pub-id-type="doi">10.3758/s13415-014-0266-y</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Gomez</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2017</year>). <article-title>The Functional Neuroanatomy of Human Face Perception</article-title>. <source>Annual Review of Vision Science</source>, <volume>3</volume>, <fpage>167</fpage>–<lpage>196</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-102016-061214</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wardle</surname>, <given-names>S. G.</given-names></string-name>, &amp; <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Decoding Dynamic Brain Patterns from Evoked Responses: A Tutorial on Multivariate Pattern Analysis Applied to Time Series Neuroimaging Data</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>29</volume>(<issue>4</issue>), <fpage>677</fpage>–<lpage>697</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01068</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hakala</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kätsyri</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Häkkinen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Stereoscopy Amplifies Emotions Elicited by Facial Expressions</article-title>. <source>I-Perception</source>, <volume>6</volume>(<issue>6</issue>), <fpage>2041669515615071</fpage>. <pub-id pub-id-type="doi">10.1177/2041669515615071</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Millman</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Walt</surname>, <given-names>S. J. van der</given-names></string-name>, <string-name><surname>Gommers</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Virtanen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cournapeau</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wieser</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Kern</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Picus</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hoyer</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kerkwijk</surname>, <given-names>M. H. van</given-names></string-name>, <string-name><surname>Brett</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Haldane</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Río</surname>, <given-names>J. F. del</given-names></string-name>, <string-name><surname>Wiebe</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>P.</given-names></string-name>, <etal>…</etal> <string-name><surname>Oliphant</surname>, <given-names>T. E.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Array programming with NumPy</article-title>. <source>Nature</source>, <volume>585</volume>(<issue>7825</issue>), <fpage>357</fpage>–<lpage>362</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haufe</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Meinecke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Görgen</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Dähne</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Haynes</surname>, <given-names>J.-D.</given-names></string-name>, <string-name><surname>Blankertz</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Bießmann</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2014</year>). <article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title>. <source>NeuroImage</source>, <volume>87</volume>, <fpage>96</fpage>–<lpage>110</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.067</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haxby</surname>, <given-names>J. V.</given-names></string-name>, <string-name><surname>Hoffman</surname>, <given-names>E. A.</given-names></string-name>, &amp; <string-name><surname>Gobbini</surname>, <given-names>M. I</given-names></string-name></person-group>. (<year>2000</year>). <article-title>The distributed human neural system for face perception</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>4</volume>(<issue>6</issue>), <fpage>223</fpage>–<lpage>233</lpage>. <pub-id pub-id-type="doi">10.1016/S1364-6613(00)01482-0</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebart</surname>, <given-names>M. N.</given-names></string-name>, &amp; <string-name><surname>Baker</surname>, <given-names>C. I</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Deconstructing multivariate decoding for the study of brain function</article-title>. <source>NeuroImage</source>, <volume>180</volume>, <fpage>4</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.005</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrando</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Constantinides</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Emotional Contagion: A Brief Overview and Future Directions</article-title>. <source>Frontiers in Psychology</source>, <volume>12</volume>. <pub-id pub-id-type="doi">10.3389/fpsyg.2021.712606</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hofmann</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Klotzsche</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Mariola</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nikulin</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Villringer</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Gaebler</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Decoding subjective emotional arousal from EEG during an immersive virtual reality experience</article-title>. <source>eLife</source>, <volume>10</volume>, <elocation-id>e64812</elocation-id>. <pub-id pub-id-type="doi">10.7554/eLife.64812</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jack</surname>, <given-names>R. E.</given-names></string-name>, &amp; <string-name><surname>Schyns</surname>, <given-names>P. G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>The Human Face as a Dynamic Tool for Social Communication</article-title>. <source>Current Biology: CB</source>, <volume>25</volume>(<issue>14</issue>), <fpage>R621</fpage>–<lpage>634</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2015.05.052</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Bekhti</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Raimondo</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Gramfort</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Autoreject: Automated artifact rejection for MEG and EEG data</article-title>. <source>NeuroImage</source>, <volume>159</volume>, <fpage>417</fpage>–<lpage>429</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.030</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Tong</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2005</year>). <article-title>Decoding the visual and subjective contents of the human brain</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>(<issue>5</issue>), <fpage>679</fpage>–<lpage>685</lpage>. <pub-id pub-id-type="doi">10.1038/nn1444</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Yovel</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2006</year>). <article-title>The fusiform face area: A cortical region specialized for the perception of faces</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>361</volume>(<issue>1476</issue>), <fpage>2109</fpage>–<lpage>2128</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2006.1934</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Klotzsche</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>de Mooij</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ohl</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Gaebler</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2025</year>, <month>May</month>). <source>EDIA: An open-source toolbox for virtual reality-based eye tracking research using Unity</source>. <publisher-name>Annual Meeting of the Vision Science Society</publisher-name>, <publisher-loc>St. Pete’s Beach, Florida, USA</publisher-loc>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Klotzsche</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gaebler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Villringer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sommer</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Nikulin</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Ohl</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Visual short-term memory related EEG components in a virtual reality setup</article-title> (p. <fpage>2023.01.23.525140</fpage>). <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2023.01.23.525140</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kret</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Sjak-Shie</surname>, <given-names>E. E</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Preprocessing pupil size data: Guidelines and code</article-title>. <source>Behavior Research Methods</source>, <volume>51</volume>(<issue>3</issue>), <fpage>1336</fpage>–<lpage>1342</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-018-1075-y</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Douglas</surname>, <given-names>P. K</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Interpreting encoding and decoding models</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>55</volume>, <fpage>167</fpage>–<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2019.04.002</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le Meur</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Baccino</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Methods for comparing scanpaths and saliency maps: Strengths and weaknesses</article-title>. <source>Behavior Research Methods</source>, <volume>45</volume>(<issue>1</issue>), <fpage>251</fpage>–<lpage>266</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-012-0226-9</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Leiner</surname>, <given-names>D. J.</given-names></string-name></person-group> (<year>2019</year>). <source>SoSci Survey</source> [Computer software]. <ext-link ext-link-type="uri" xlink:href="https://www.soscisurvey.de">https://www.soscisurvey.de</ext-link></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lettieri</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Handjaras</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ricciardi</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Leo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Papale</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Betta</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pietrini</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Cecchetti</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Emotionotopy in the human right temporo-parietal cortex</article-title>. <source>Nature Communications</source>, <volume>10</volume>(<issue>1</issue>), <fpage>5568</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-13599-z</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>C. H.</given-names></string-name>, &amp; <string-name><surname>Ward</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2006</year>). <article-title>The use of 3D information in face recognition</article-title>. <source>Vision Research</source>, <volume>46</volume>(<issue>6</issue>), <fpage>768</fpage>–<lpage>773</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2005.10.008</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>C. H.</given-names></string-name>, <string-name><surname>Ward</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Young</surname>, <given-names>A. W</given-names></string-name></person-group>. (<year>2006</year>). <article-title>Transfer between two- and three-dimensional representations of faces</article-title>. <source>Visual Cognition</source>, <volume>13</volume>(<issue>1</issue>), <fpage>51</fpage>–<lpage>64</lpage>. <pub-id pub-id-type="doi">10.1080/13506280500143391</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Laeng</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Czajkowski</surname>, <given-names>N. O</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Does stereopsis improve face identification? A study using a virtual reality display with integrated eye-tracking and pupillometry</article-title>. <source>Acta Psychologica</source>, <volume>210</volume>, <fpage>103142</fpage>. <pub-id pub-id-type="doi">10.1016/j.actpsy.2020.103142</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lützow Holm</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Fernández Slezak</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Tagliazucchi</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Contribution of low-level image statistics to EEG decoding of semantic content in multivariate and univariate models with feature optimization</article-title>. <source>NeuroImage</source>, <volume>293</volume>, <fpage>120626</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2024.120626</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Correll</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Wittenbrink</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2015</year>). <article-title>The Chicago face database: A free stimulus set of faces and norming data</article-title>. <source>Behavior Research Methods</source>, <volume>47</volume>(<issue>4</issue>), <fpage>1122</fpage>–<lpage>1135</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-014-0532-5</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Menz</surname>, <given-names>M. D.</given-names></string-name>, &amp; <string-name><surname>Freeman</surname>, <given-names>R. D</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Stereoscopic depth processing in the visual cortex: A coarse-to-fine mechanism</article-title>. <source>Nature Neuroscience</source>, <volume>6</volume>(<issue>1</issue>), <fpage>59</fpage>–<lpage>65</lpage>. <pub-id pub-id-type="doi">10.1038/nn986</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="data" specific-use="analyzed"><person-group person-group-type="author"><string-name><surname>Mills</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Hcp-mmp1.0 projected on fsaverage</article-title>. <source>figshare</source>. <pub-id pub-id-type="doi">10.6084/M9.FIGSHARE.3498446.V2</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Albers</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Brinkman</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Todorova</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Eye Movement-Related Confounds in Neural Decoding of Visual Working Memory Representations</article-title>. <source>eNeuro</source>, <volume>5</volume>(<issue>4</issue>), <fpage>ENEURO.0401-17.2018</fpage>. <pub-id pub-id-type="doi">10.1523/ENEURO.0401-17.2018</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Nolte</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Grasso-Cladera</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>König</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Investigating saccade-onset locked EEG signatures of face perception during free-viewing in a naturalistic virtual environment</article-title>. <source>bioRxiv</source> <pub-id pub-id-type="doi">10.1101/2024.12.12.628113</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nolte</surname>, <given-names>D.</given-names></string-name>, <string-name><given-names>Vidal</given-names> <surname>De Palol</surname></string-name>, <string-name><given-names>M.</given-names>, <surname>Keshava</surname></string-name>, <string-name><given-names>A.</given-names>, <surname>Madrid-Carvajal</surname></string-name>, <string-name><given-names>J.</given-names>, <surname>Gert</surname></string-name>, <string-name><given-names>A. L.</given-names>, <surname>von Butler</surname></string-name>, <string-name><given-names>E.-M.</given-names>, <surname>Kömürlüoğlu</surname></string-name>, <string-name><given-names>P.</given-names>, &amp; <surname>König</surname></string-name></person-group>, P. (<year>2025</year>). <article-title>Combining EEG and eye-tracking in virtual reality: Obtaining fixation-onset event-related potentials and event-related spectral perturbations</article-title>. <source>Attention, Perception &amp; Psychophysics</source>, <volume>87</volume>(<issue>1</issue>), <fpage>207</fpage>–<lpage>227</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-024-02917-3</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>2007</year>). <article-title>Binocular depth perception and the cerebral cortex</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>8</volume>(<issue>5</issue>), <fpage>379</fpage>–<lpage>391</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2131</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Pascual-Marqui</surname>, <given-names>R. D.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Discrete, 3D distributed, linear imaging methods of electric neuronal activity. Part 1: Exact, zero error localization</article-title>. <source>arXiv:0710.3341 [Math-Ph, Physics:Physics, q-Bio]</source>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/0710.3341">http://arxiv.org/abs/0710.3341</ext-link></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Michel</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Grisel</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Blondel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Prettenhofer</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Weiss</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Dubourg</surname>, <given-names>V.</given-names></string-name>, &amp; <collab>others</collab></person-group>. (<year>2011</year>). <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>Journal of Machine Learning Research</source>, <volume>12</volume>(<issue>Oct</issue>), <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, &amp; <string-name><surname>Downing</surname>, <given-names>P. E</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Testing cognitive theories with multivariate pattern analysis of neuroimaging data</article-title>. <source>Nature Human Behaviour</source>, <volume>7</volume>(<issue>9</issue>), <fpage>1430</fpage>–<lpage>1441</lpage>. <pub-id pub-id-type="doi">10.1038/s41562-023-01680-z</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ritchie</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Kaplan</surname>, <given-names>D. M.</given-names></string-name>, &amp; <string-name><surname>Klein</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Decoding the Brain: Neural Representation and the Limits of Multivariate Pattern Analysis in Cognitive Neuroscience</article-title>. <source>The British Journal for the Philosophy of Science</source>, <volume>70</volume>(<issue>2</issue>), <fpage>581</fpage>–<lpage>607</lpage>. <pub-id pub-id-type="doi">10.1093/bjps/axx023</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rossion</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Understanding face perception by means of human electrophysiology</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>18</volume>(<issue>6</issue>), <fpage>310</fpage>–<lpage>318</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2014.02.013</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Caharel</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2011</year>). <article-title>ERP evidence for the speed of face categorization in the human brain: Disentangling the contribution of low-level visual cues from face perception</article-title>. <source>Vision Research</source>, <volume>51</volume>(<issue>12</issue>), <fpage>1297</fpage>–<lpage>1311</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2011.04.003</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Jacques</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2012</year>). <chapter-title>The N170: Understanding the time course of face perception in the human brain</chapter-title>. In <person-group person-group-type="editor"><string-name><surname>Kappenman</surname> <given-names>ES</given-names></string-name> and <string-name><surname>Luck</surname> <given-names>SJ</given-names></string-name></person-group> <source>The Oxford handbook of event-related potential components</source> (pp. <fpage>115</fpage>–<lpage>141</lpage>). <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sagehorn</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Johnsdorf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kisker</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gruber</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Schöne</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Electrophysiological correlates of face and object perception: A comparative analysis of 2D laboratory and virtual reality conditions</article-title>. <source>Psychophysiology</source>, <volume>61</volume>(<issue>5</issue>), <fpage>e14519</fpage>. <pub-id pub-id-type="doi">10.1111/psyp.14519</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sagehorn</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Johnsdorf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kisker</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sylvester</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gruber</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Schöne</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Real-life relevant face perception is not captured by the N170 but reflected in later potentials: A comparison of 2D and virtual reality stimuli</article-title>. <source>Frontiers in Psychology</source>, <volume>14</volume>. <pub-id pub-id-type="doi">10.3389/fpsyg.2023.1050892</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sagehorn</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kisker</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Johnsdorf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gruber</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Schöne</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2024</year>). <article-title>A comparative analysis of face and object perception in 2D laboratory and virtual reality settings: Insights from induced oscillatory responses</article-title>. <source>Experimental Brain Research</source>. <pub-id pub-id-type="doi">10.1007/s00221-024-06935-3</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Savran</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sankur</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Taha Bilge</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Comparative evaluation of 3D vs. 2D modality for automatic detection of facial action units</article-title>. <source>Pattern Recognition</source>, <volume>45</volume>(<issue>2</issue>), <fpage>767</fpage>–<lpage>782</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2011.07.022</pub-id></mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schacht</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Sommer</surname>, <given-names>W</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Emotions in word and face processing: Early and late cortical responses</article-title>. <source>Brain and Cognition</source>, <volume>69</volume>(<issue>3</issue>), <fpage>538</fpage>–<lpage>550</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandc.2008.11.005</pub-id></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schindler</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Bublatzky</surname>, <given-names>F</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Attention and emotion: An integrative review of emotional face processing as a function of attention</article-title>. <source>Cortex</source>, <volume>130</volume>, <fpage>362</fpage>–<lpage>386</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2020.06.010</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schubring</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kraus</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Stolz</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Weiler</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Keim</surname>, <given-names>D. A.</given-names></string-name>, &amp; <string-name><surname>Schupp</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Virtual Reality Potentiates Emotion and Task Effects of Alpha/Beta Brain Oscillations</article-title>. <source>Brain Sciences</source>, <volume>10</volume>(<issue>8</issue>), <elocation-id>8</elocation-id>. <pub-id pub-id-type="doi">10.3390/brainsci10080537</pub-id></mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Schupp</surname>, <given-names>H. T.</given-names></string-name>, <string-name><surname>Flaisch</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Stockburger</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Junghöfer</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2006</year>). <chapter-title>Emotion and attention: Event-related brain potential studies</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>S.</given-names> <surname>Anders</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Ende</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Junghofer</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Kissler</surname></string-name>, &amp; <string-name><given-names>D.</given-names> <surname>Wildgruber</surname></string-name></person-group> (Eds.), <source>Progress in Brain Research</source> (Vol. <volume>156</volume>, pp. <fpage>31</fpage>–<lpage>51</lpage>). <publisher-name>Elsevier</publisher-name>. <pub-id pub-id-type="doi">10.1016/S0079-6123(06)56002-9</pub-id></mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schurgin</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Iida</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ohira</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Chiao</surname>, <given-names>J. Y.</given-names></string-name>, &amp; <string-name><surname>Franconeri</surname>, <given-names>S. L</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Eye movements during emotion recognition in faces</article-title>. <source>Journal of Vision</source>, <volume>14</volume>(<issue>13</issue>), <fpage>14</fpage>. <pub-id pub-id-type="doi">10.1167/14.13.14</pub-id></mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Seabold</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Perktold</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2010</year>). <article-title>statsmodels: Econometric and statistical modeling with python</article-title>. <conf-name>9th Python in Science Conference</conf-name>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>F. W.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>M. L</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Decoding the dynamic representation of facial expressions of emotion in explicit and incidental tasks</article-title>. <source>NeuroImage</source>, <volume>195</volume>, <fpage>261</fpage>–<lpage>271</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.065</pub-id></mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stein</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Niehorster</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Watson</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Steinicke</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Rifai</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wahl</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Lappe</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2021</year>). <article-title>A Comparison of Eye Tracking Latencies Among Several Commercial Head-Mounted Displays</article-title>. <source>I-Perception</source>, <volume>12</volume>(<issue>1</issue>), <fpage>2041669520983338</fpage>. <pub-id pub-id-type="doi">10.1177/2041669520983338</pub-id></mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steyvers</surname>, <given-names>M</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Morphing techniques for manipulating face images</article-title>. <source>Behavior Research Methods, Instruments, &amp; Computers</source>, <volume>31</volume>(<issue>2</issue>), <fpage>359</fpage>–<lpage>369</lpage>. <pub-id pub-id-type="doi">10.3758/BF03207733</pub-id></mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Warren</surname>, <given-names>W. H</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Virtual reality in behavioral neuroscience and beyond</article-title>. <source>Nature Neuroscience</source>, <volume>5</volume>(<issue>11</issue>), <fpage>1089</fpage>–<lpage>1092</lpage>. <pub-id pub-id-type="doi">10.1038/nn948</pub-id></mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Tauscher</surname>, <given-names>J.-P.</given-names></string-name>, <string-name><surname>Schottky</surname>, <given-names>F. W.</given-names></string-name>, <string-name><surname>Grogorick</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bittner</surname>, <given-names>P. M.</given-names></string-name>, <string-name><surname>Mustafa</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Magnor</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Immersive EEG: Evaluating Electroencephalography in Virtual Reality</article-title>. <conf-name>2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</conf-name>, <fpage>1794</fpage>–<lpage>1800</lpage>. <pub-id pub-id-type="doi">10.1109/VR.2019.8797858</pub-id></mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thurley</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Naturalistic neuroscience and virtual reality</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>16</volume>. <pub-id pub-id-type="doi">10.3389/fnsys.2022.896251</pub-id></mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaidya</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Jin</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Fellows</surname>, <given-names>L. K</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Eye spy: The predictive value of fixation patterns in detecting subtle and extreme emotions from faces</article-title>. <source>Cognition</source>, <volume>133</volume>(<issue>2</issue>), <fpage>443</fpage>–<lpage>456</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2014.07.004</pub-id></mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Virtanen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gommers</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Oliphant</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Haberland</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Reddy</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Cournapeau</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Burovski</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Peterson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Weckesser</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Bright</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Van Der Walt</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Brett</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Millman</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Mayorov</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>A. R. J.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kern</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Larson</surname>, <given-names>E.</given-names></string-name>, <etal>…</etal> <string-name><surname>Vázquez-Baeza</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2020</year>). <article-title>SciPy 1.0: Fundamental algorithms for scientific computing in Python</article-title>. <source>Nature Methods</source>, <volume>17</volume>(<issue>3</issue>), <fpage>261</fpage>–<lpage>272</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id></mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vishwanath</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Toward a new theory of stereopsis</article-title>. <source>Psychological Review</source>, <volume>121</volume>(<issue>2</issue>), <fpage>151</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1037/a0035233</pub-id></mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vishwanath</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Hibbard</surname>, <given-names>P. B</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Seeing in 3-D With Just One Eye</article-title>. <source>Psychological Science</source>. <pub-id pub-id-type="doi">10.1177/0956797613477867</pub-id></mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wade-Bohleber</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Thoma</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Gerber</surname>, <given-names>A. J</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Neural correlates of subjective arousal and valence in health and panic disorder</article-title>. <source>Psychiatry Research: Neuroimaging</source>, <volume>305</volume>, <fpage>111186</fpage>. <pub-id pub-id-type="doi">10.1016/j.pscychresns.2020.111186</pub-id></mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Li</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Use of 3D faces facilitates facial expression recognition in children</article-title>. <source>Scientific Reports</source>, <volume>7</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/srep45464</pub-id></mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weber</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hertweck</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Alwanni</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Fiederer</surname>, <given-names>L. D. J.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Unruh</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Fischbach</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Latoschik</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Ball</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2021</year>). <article-title>A Structured Approach to Test the Signal Quality of Electroencephalography Measurements During Use of Head-Mounted Displays for Virtual Reality Applications</article-title>. <source>Frontiers in Neuroscience</source>, <volume>15</volume>, <fpage>733673</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2021.733673</pub-id></mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Welchman</surname>, <given-names>A. E</given-names></string-name></person-group>. (<year>2016</year>). <article-title>The Human Brain in Depth: How We See in 3D</article-title>. <source>Annual Review of Vision Science</source>, <volume>2</volume>(Volume 2, 2016), <fpage>345</fpage>–<lpage>376</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114605</pub-id></mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willis</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Todorov</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2006</year>). <article-title>First Impressions: Making Up Your Mind After a 100-Ms Exposure to a Face</article-title>. <source>Psychological Science</source>, <volume>17</volume>(<issue>7</issue>), <fpage>592</fpage>–<lpage>598</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9280.2006.01750.x</pub-id></mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Xi</given-names></string-name>, <string-name><surname>Golbabaee</surname>, <given-names>Mohammad</given-names></string-name>, <string-name><surname>Proulx</surname>, <given-names>Michael J.</given-names></string-name>, &amp; and <string-name><surname>O’Neill</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Feasibility of decoding visual information from EEG</article-title>. <source>Brain-Computer Interfaces</source>, <volume>11</volume>(<issue>1–2</issue>), <fpage>33</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1080/2326263X.2023.2287719</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108933.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Meng</surname>
<given-names>Ming</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2104-4988</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/008s83205</institution-id><institution>University of Alabama at Birmingham</institution>
</institution-wrap>
<city>Birmingham</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study successfully decoded visual representations of facial expressions and stereoscopic depth information from electroencephalogram (EEG) signals recorded in an immersive virtual reality (VR) environment. The evidence is <bold>solid</bold> in demonstrating the technical feasibility of integrating state-of-the-art EEG decoding and VR with eye tracking. This work will interest neuroscience researchers, as well as engineers developing brain-machine interfaces and/or virtual reality displays.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108933.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The study by Klotzsche et al. examines whether emotional facial expressions can be decoded from EEG while participants view 3D faces in immersive VR and whether stereoscopic depth cues affect these neural representations. Participants viewed computer-generated faces (three identities, four emotions) rendered either stereoscopically or monoscopically, while performing an emotion recognition task. Time-resolved multivariate decoding revealed above-chance decodability of facial expressions from EEG. Importantly, decoding accuracy did not differ between monoscopic and stereoscopic viewing. This indicates that the neural representation of expressions is robust against stereoscopic disparity for the relevant features. However, a separate classifier could distinguish the depth condition (mono vs. stereo) from EEG, i.e., the pattern of neuronal activity differs between conditions, but not in ways relevant for the decoding of emotions. It had an early peak and a temporal profile similar to identity decoding, suggesting that early, task-irrelevant visual differences are captured neurally. Cross-decoding further demonstrated that expression decoders trained in one depth condition could generalize to the other, supporting the idea of representational invariance. Eye-tracking analyses showed that expressions and identities could be decoded from gaze patterns, but not the depth condition, and EEG- and gaze-based decoding performances were not correlated across participants. Overall, this work shows that EEG decoding in VR is feasible and sensitive, and suggests that stereoscopic cues are represented in the brain but do not influence the neural processing of facial expressions. This study addresses a relevant question with state-of-the-art experimental and data analysis techniques.</p>
<p>Strengths:</p>
<p>(1) It combines EEG, virtual reality stereoscoptic and monoscopic presentation of visual stimuli, and advanced data analysis methods to address a timely question.</p>
<p>(2) The figures are of very high quality.</p>
<p>(3) The reference list is appropriate and up to date.</p>
<p>Weaknesses:</p>
<p>(1) The introduction-results-discussion-methods order makes it hard to follow the Results without repeatedly consulting the Methods. Please introduce minimal, critical methodological context at the start of each Results subsection; reserve technical details for Methods/Supplement.</p>
<p>(2) Many Results subsections begin with a crisp question and present rich analyses, but end without a short synthesis. Please add 1-2 sentences that explicitly answer the opening question and state what the analyses demonstrate.</p>
<p>(3) The Results compellingly show that (a) expressions are decodable from EEG and (b) mono vs stereo trials are decodable from EEG; yet expression decoding is comparable across mono and stereo. It would help if you articulate why depth is neurally distinguishable while leaving expression representations unchanged. Maybe improve the discussion of the results of source localization and give a more detailed connection to what we already know about the processing of disparity.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108933.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors' main aim was to determine the extent to which the emotional expression of face images could be inferred from electrophysiological data under the viewing conditions imposed by immersive virtual reality displays. Further, given that stereoscopic depth cues can be easily manipulated in such displays, the authors wished to investigate whether successful emotion decoding was affected by the presence or absence of these depth cues, and also if the presence/absence of depth cues was itself a property of the viewing experience that could be decoded from neural data.</p>
<p>Overall, the authors use fairly standard approaches to decoding neural data to demonstrate that above-chance results (slightly above the 0.5 chance threshold for their measure of choice) are in general achievable for emotion decoding, decoding the identity of faces from neural data, and decoding the presence/absence of depth cues in an immersive virtual reality display. They further examine the contribution of specific components of the response to visual stimuli with similar outcomes.</p>
<p>Strengths:</p>
<p>The main contribution of the manuscript is methodological. Rather than shedding particular light on the neural mechanisms supporting depth processing or face perception, what is on offer is primarily a straightforward examination of an applied question. With regard to the goal of answering that applied question, I think the paper succeeds. The overall experimental design is not novel, but in this case, that is a good thing. The authors have used relatively unadorned tasks and previous approaches to applying decoding tools to EEG data to see what they can get out of the neural data collected under these viewing conditions. While I would say that there is not a great deal that is especially surprising about these results, the authors do meet the goal they set for themselves.</p>
<p>Weaknesses:</p>
<p>Some of the key weaknesses I see are points that the authors raise themselves in their discussion, particularly with regard to the generalizability of their results. In particular, the 3D faces they have employed here perhaps exhibit a somewhat limited repertoire of emotional expression and do not necessarily cover a representative gamut of emotional face appearances, such as one would encounter in naturalistic settings. Then again, part of the goal of the paper was to examine the decodability of emotional expression in a specific, non-natural viewing environment - a viewing environment in which one could reasonably expect to encounter artificial faces like these. Still, the limitations of the stimuli potentially limit the scope of the conclusions one should draw from the data. I also think that there is a great deal of room for low-level image properties to drive the decoding results for faces, which could have been addressed in a number of ways (matching power spectra, for example, or using an inverted-image control condition). The absence of such control comparisons means that it is difficult to know if this is really a result that reflects face processing or much lower-level image differences that are diagnostic of emotion or identity in this subset of images. Again, to some extent, this is potentially acceptable - if one is mostly interested in whether this result is achievable at all (by hook or by crook), then it is not so important how the goal is met. Then again, one would perhaps like to know if what has been measured here is more a reflection of spatial vision vs. face processing mechanisms.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108933.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This study investigates two main questions:</p>
<p>(1) whether brain activity recorded during immersive virtual reality can differentiate facial expressions and stereoscopic depth, and</p>
<p>(2) whether depth cues modulate facial information processing.</p>
<p>The results show that both expression and depth information can be decoded from multivariate EEG recorded in a head-mounted VR setup. However, the results show that the decoding performance of facial expressions does not benefit from depth information.</p>
<p>Strengths:</p>
<p>The study is technically strong and well executed. EEG data are of high quality despite the challenges of recording inside a head-mounted VR system. The work effectively combines stereoscopic stimulus presentation, eye-tracking to monitor gaze behavior, and time-resolved multivariate decoding techniques. Together, these elements provide an exemplary demonstration of how to collect and analyze high-quality EEG data in immersive VR environments.</p>
<p>Weaknesses:</p>
<p>The major limitation concerns the theoretical question about how stereoscopic depth modulates facial expression processing. While previous work has suggested that stereoscopic depth cues can shape natural face perception and emphasize the importance of binocular information in recognizing facial expressions (lines 95-97), the present study reports a null effect of depth. However, the stimulus configuration they used likely constrained the ability to detect any depth-related effects. All facial stimuli were static, frontal, and presented at a fixed distance. This design leads to near-ceiling behavioral performance and no behavioral effect of depth on expression recognition. It makes the null modulation of depth on expression processing unsurprising and limits the theoretical reach of the study. Adding more subtle or naturalistic features (such as various viewing angles and dynamic expressions) to the stimulus set if the authors aim to advance a strong theoretical claim about the role of binocular disparity. Or reframing the work as a technical validation of EEG decoding in this context.</p>
<p>Another issue relates to the claim that eye movements cannot explain the EEG decoding results. It is a real challenge to remove eye-movement-related artifacts and confounds, as the VR setup tends to encourage viewers to explore the environment freely. However, nearly half of the eye-tracking datasets were lost (usable in only 17 of 33 participants), which substantially weakens the evidence for EEG-gaze dissociation. Moreover, it would be almost impossible to decode facial information from only two-dimensional gaze direction, given that with 60 EEG channels, the decoding accuracy was modest (AUC ≈ 0.60). These two factors together limited the strength of the reported null correlation between neural and eye-data decoding.</p>
<p>The decoding analysis appears to use all 60 EEG channels as input features. I wonder why the authors did not examine using more spatially specific channel subsets. Facial expression and depth cues are known to preferentially engage occipito-temporal regions (e.g., N170-related sites), yet the current approach treats all sensors equally. Including all the channels may add noise and irrelevant signals to facial information decoding. Besides, using a subset of spatial-specific channels would align more directly with the subsequent source reconstruction.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.108933.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Klotzsche</surname>
<given-names>Felix</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3985-2481</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Nasim</surname>
<given-names>Ammara</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6965-9012</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Hofmann</surname>
<given-names>Simon M</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0958-501X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Villringer</surname>
<given-names>Arno</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2604-2404</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Nikulin</surname>
<given-names>Vadim</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6082-3859</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Sommer</surname>
<given-names>Werner</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5266-3445</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Gaebler</surname>
<given-names>Michael</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4442-5778</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We thank the reviewers for their thoughtful and constructive comments. We are pleased that they found the study technically strong and the integration of EEG decoding, immersive VR, and eye tracking valuable.</p>
<p>Across all three reviews, several points of clarification emerged. In our revision, we will focus on:</p>
<p>(1) Improving clarity and structure of the manuscript (Reviewer #1).</p>
<p>We will strengthen the flow between the Methods and Results subsections and include explicit concluding statements for the single results.</p>
<p>(2) Emphasize methodological scope and limitations in terms of stimulus set and generalizability (Reviewers #2 and #3).</p>
<p>We will further emphasize that a key objective was to establish, for the first time, the methodological feasibility of decoding facial features (especially emotional expressions) under VR conditions, and that our stimulus set (consisting of facial expressions that were easy to distinguish) limits (a) the task-relevance (and thus possibly the neural integration) of depth information and (b) the generalizability to less easily distinguishable settings. We appreciate the suggestion of an inverted-face control to further investigate the extent to which the decoding results were based on low-level features; however, we do not plan a follow-up experiment at this stage; instead, we will discuss this limitation more explicitly.</p>
<p>We believe these revisions will substantially strengthen the manuscript and further highlight its methodological focus.</p>
</body>
</sub-article>
</article>