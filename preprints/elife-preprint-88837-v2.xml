<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">88837</article-id>
<article-id pub-id-type="doi">10.7554/eLife.88837</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88837.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Immunology and Inflammation</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Context-Aware Amino Acid Embedding Advances Analysis of TCR-Epitope Interactions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-5976-0673</contrib-id>
<name>
<surname>Zhang</surname>
<given-names>Pengfei</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bang</surname>
<given-names>Seojin</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cai</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3528-6833</contrib-id>
<name>
<surname>Lee</surname>
<given-names>Heewook</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">†</xref>
</contrib>
<aff id="a1"><label>1</label><institution>School of Computing and Augmented Intelligence, Arizona State University</institution>, Tempe, AZ, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Biodesign Institute, Arizona State University</institution>, Tempe, AZ, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Zhao</surname>
<given-names>Siming</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Dartmouth College</institution>
</institution-wrap>
<city>Lebanon</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Choi</surname>
<given-names>Murim</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Seoul National University</institution>
</institution-wrap>
<city>Seoul</city>
<country>Republic of Korea</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="others"><label>*</label><p>Now at Google.</p></fn>
<corresp id="cor1"><label>†</label>Correspondence: <email>heewook.lee@asu.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-07-24">
<day>24</day>
<month>07</month>
<year>2023</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-04-30">
<day>30</day>
<month>04</month>
<year>2024</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP88837</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-05-21">
<day>21</day>
<month>05</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-04-14">
<day>14</day>
<month>04</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.12.536635"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2023-07-24">
<day>24</day>
<month>07</month>
<year>2023</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.88837.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.88837.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88837.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88837.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.88837.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Zhang et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Zhang et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-88837-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Accurate prediction of binding interaction between T cell receptors (TCRs) and host cells is fundamental to understanding the regulation of the adaptive immune system as well as to developing data-driven approaches for personalized immunotherapy. While several machine learning models have been developed for this prediction task, the question of how to specifically embed TCR sequences into numeric representations remains largely unexplored compared to protein sequences in general. Here, we investigate whether the embedding models designed for protein sequences, and the most widely used BLOSUM-based embedding techniques are suitable for TCR analysis. Additionally, we present our context-aware amino acid embedding models (<monospace>catELMo</monospace>) designed explicitly for TCR analysis and trained on 4M unlabeled TCR sequences with no supervision. We validate the effectiveness of <monospace>catELMo</monospace> in both supervised and unsupervised scenarios by stacking the simplest models on top of our learned embeddings. For the supervised task, we choose the binding affinity prediction problem of TCR and epitope sequences and demonstrate notably significant performance gains (up by at least 14% AUC) compared to existing embedding models as well as the state-of-the-art methods. Additionally, we also show that our learned embeddings reduce more than 93% annotation cost while achieving comparable results to the state-of-the-art methods. In TCR clustering task (unsupervised), <monospace>catELMo</monospace> identifies TCR clusters that are more homogeneous and complete about their binding epitopes. Altogether, our <monospace>catELMo</monospace> trained without any explicit supervision interprets TCR sequences better and negates the need for complex deep neural network architectures in downstream tasks.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>- Supplementary Table S1: AUC scores for the top 10 frequent epitope types (pathogens) in the testing set of epitope split.
- Supplementary Table S5: AUCs of TCR-epitope binding affinity prediction models with BLOSUM62 to embed epitope sequences.
- Supplementary Table S6: AUCs of TCR-epitope binding affinity prediction models trained on catELMo TCR embeddings and random-initialized epitope embeddings.
-Supplementary Table S7: AUCs of TCR-epitope binding affinity prediction models trained on catELMo and BLOSUM62 embeddings.
- Supplementary Figure 4: TCR clustering performance for the top 34 abundant epitopes representing 70.55% of TCRs in our collected databases.
- Section Discussion.
- Section 4.1 Data: TCR-epitope pairs for binding affinity prediction.
- Section 4.4.2 Epitope-specific TCR clustering.
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label><title>Introduction</title>
<p>T cell receptors (TCRs) play critical roles in adaptive immune systems as they enable T cells to distinguish abnormal cells from healthy cells. TCRs carry this important function by binding to antigens presented by major histocompatibility complex (MHC) and recognizing whether the antigens are self or foreign [<xref ref-type="bibr" rid="c1">1</xref>]. It is widely accepted that the third complementarity-determining region (CDR3) of the TCRβ chain is the most important in determining its binding specificity to epitope—a part of an antigen [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. The advent of publicly available databases of TCR-epitope cognate pairs opened the door to computational methods to predict the binding affinity of a given pair of TCR and epitope sequences. Computational prediction of binding affinity is important as it can drastically reduce the cost and the time needed to narrow down a set of candidate TCR targets, thereby accelerating the development of personalized immunotherapy leading to vaccine development and cancer treatment [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>]. Computational prediction is challenging primarily due to 1) many-to-many binding characteristics [<xref ref-type="bibr" rid="c6">6</xref>] and 2) the limited amount of currently available data.</p>
<p>Despite the challenges, many deep neural networks have been leveraged to predict binding affinity between TCRs and epitopes [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>]. While each model has its own strengths and weaknesses, they all suffer from poor generalizability when applied to unseen epitopes, not present in the training data [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. In order to alleviate this, we focus mainly on embedding, as embedding an amino acid sequence into a numeric representation is the very first step needed to train and run a deep neural network. Furthermore, a ‘good’ embedding has been shown to boost downstream performance even with a few numbers of downstream samples [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>].</p>
<p>BLOSUM matrices [<xref ref-type="bibr" rid="c16">16</xref>] are widely used for representing amino acids into biological-related numeric vectors in TCR analysis [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c17">17</xref>]. However, BLOSUM matrices are static embedding methods as they always map an amino acid to the same vector regardless of its context. For example, in static word embedding, the word “mouse” in phrases “a mouse in desperate search of cheese” and “to click, press and release the left mouse button” will be embedded as the same numeric representation even though it is used in different contexts. Similarly, the amino acid residue <monospace>G</monospace> appearing five times in a TCR<italic>β</italic> CDR3 sequence <monospace>CASGGTGGANTGQLYF</monospace> may play different roles in binding to antigens as each occurrence has a different position and neighboring residues. The loss of such contextual information from static embedding may inevitably compromise model performances [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>].</p>
<p>Recent successes of large language models [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>] have been prompting new research applying text embedding techniques to amino acid embedding. Large language models are generally trained on a large text corpus in a self-supervised manner where no labels are required [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. A large number of (unlabeled) protein sequences has been available via high quality and manually curated databases such as UniProt [<xref ref-type="bibr" rid="c22">22</xref>]. With the latest development of targeted sequencing assays of TCR repertoire, a large number (unlabeled) of TCR sequences has also been accessible to the public via online databases such as ImmunoSEQ [<xref ref-type="bibr" rid="c23">23</xref>]. These databases have allowed researchers to develop large-scale amino acid embedding models that can be used for various downstream tasks. Asgari et al. [<xref ref-type="bibr" rid="c24">24</xref>] first utilized Word2vec [<xref ref-type="bibr" rid="c20">20</xref>] model with 3-mers of amino acids to learn embeddings of protein sequences. By considering a 3-mer amino acids as a word and a protein sequence as a sentence, they learn amino acid representations by predicting the context of a given target 3-mer in a large corpus of surrounding ones. Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>] applied Doc2vec [<xref ref-type="bibr" rid="c21">21</xref>] models to protein sequences with different sizes of k-mers in a similar manner to Asgari et al. and showed better performance over sparse one-hot encoding. One-hot encoding produces static embeddings, like BLOSUM, which leads to the loss of positional and contextual information.</p>
<p>Later, SeqVec [<xref ref-type="bibr" rid="c26">26</xref>] and ProtTrans [<xref ref-type="bibr" rid="c27">27</xref>] experimented with dynamic protein sequence embeddings via multiple context-aware language models [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c28">28</xref>], showing advantages across multiple tasks. Note that the aforementioned amino acid embedding models were designed for protein sequence analysis. Although these models may have learned general representations of protein sequences, it does not necessarily signify their generalization performance on TCR-related downstream tasks.</p>
<p>Here, we explore strategies to develop amino acid embedding models and emphasize the importance of using “good” amino acid embeddings for a significant performance gain in TCR-related downstream tasks. It includes neural network depth, architecture, types and numbers of training samples, and parameter initialization. Based on our experimental observation, we propose <monospace>catELMo</monospace>, whose architecture is adapted from ELMo (Embeddings from Language Models [<xref ref-type="bibr" rid="c18">18</xref>]), a bi-directional context-aware language model. <monospace>catELMo</monospace> is trained on more than four million TCR sequences collected from ImmunoSEQ [<xref ref-type="bibr" rid="c23">23</xref>] in an unsupervised manner, by contextualizing amino acid inputs and predicting the next amino acid token. We compare its performance with state-of-the-art amino acid embedding methods on two TCR-related downstream tasks. In TCR-epitope binding affinity prediction application, <monospace>catELMo</monospace> significantly outperforms the state-of-the-art method by at least 14% (absolute improvement) of AUCs. We also show <monospace>catELMo</monospace> achieves an equivalent performance to the state-of-the-art method while dramatically reducing downstream training sample annotation cost (more than 93% absolute reduction). In the epitope-specific TCR clustering application, <monospace>catELMo</monospace> also achieves comparable to or better cluster results than state-of-the-art methods.</p>
</sec>
<sec id="s2">
<label>2</label><title>Results</title>
<p><monospace>catELMo</monospace> is a bi-directional amino acid embedding model that learns contextualized amino acid representations (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>), treating an amino acid as a word and a sequence as a sentence. It learns patterns of amino acid sequences with its self-supervision signal, by predicting each the next amino acid token given its previous tokens. It has been trained on 4,173,895 TCR<italic>β</italic> CDR3 sequences (52 million of amino acid tokens) from ImmunoSEQ [<xref ref-type="bibr" rid="c23">23</xref>] (<xref rid="tbl1" ref-type="table">Table 1</xref>). <monospace>catELMo</monospace> yields a real-valued representation vector for a sequence of amino acids, which can be used as input features of various downstream tasks. We evaluated <monospace>catELMo</monospace> on two different TCR-related downstream tasks, and compared its performance with existing amino acid embedding methods, namely BLOSUM62 [<xref ref-type="bibr" rid="c16">16</xref>], Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>], ProtBert [<xref ref-type="bibr" rid="c27">27</xref>], SeqVec [<xref ref-type="bibr" rid="c26">26</xref>], and TCRBert [<xref ref-type="bibr" rid="c29">29</xref>]. We also investigated various components of <monospace>catELMo</monospace> in order to account for its high performance, including the neural network architecture, layer depth and size, types of training data, and the size of downstream training data.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p><bold>Methods overview. a</bold>) <monospace>catELMo</monospace> is an ELMo-based bi-directional amino acid sequence representation model trained on TCR sequences. It takes a sequence of amino acid strings as input and predicts the right (or left) next amino acid tokens. <monospace>catELMo</monospace> consists of a charCNN layer and four bidirectional LSTM layers followed by a softmax activation. For a given TCR sequence of length L, each layer returns L vectors of length 1,024. The size of an embedded TCR sequence, therefore, is [5, L, 1024]. Global average pooling with respect to the length of TCR, L, is applied to get a representation vector with a size of 1, 024. <bold>b</bold>) TCR-epitope binding affinity prediction task. An embedding method (e.g., <monospace>catELMo</monospace>) is applied to both TCR and epitope sequences. The embedding vectors are then fed into a neural network of three linear layers for training a binding affinity prediction model. The model predicts whether a given TCR and epitope sequence bind or not. <bold>c</bold>) Epitope-specific TCR sequences clustering. The hierarchical clustering algorithm is applied to TCR embedding vectors to group TCR sequences based on their epitope-specificity.</p></caption>
<graphic xlink:href="536635v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Data Summary.</title><p>The number of unique epitopes, TCRs, and TCR-epitope pairs used for <monospace>catELMo</monospace> and downstream tasks analysis.</p></caption>
<graphic xlink:href="536635v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We briefly summarize the two downstream tasks here and refer further details to <xref rid="s4d" ref-type="sec">Section 4.4</xref>. The first downstream task is TCR-epitope binding affinity prediction (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). All embedding models compared were to embed input sequences into the identical prediction model. Each prediction model was trained on 300,016 TCR-epitope binding and non-binding pairs (1:1 ratio), embedded by each embedding model. We used a neural network with three linear layers for the prediction model, which takes a pair of TCR and epitope as input and returns a binding affinity (0–1) of the pair. The prediction performance was evaluated on testing sets each defined by two types of splitting methods [<xref ref-type="bibr" rid="c10">10</xref>], called TCR and epitope splits. The testing set of TCR split has no TCRs overlapped with training and validation sets, allowing us to measure out-of-sample TCR performance. Similarly, the testing set of epitope split has no epitopes overlapped with training and validation sets, allowing us to measure out-of-sample epitope performance. For a fair comparison, a consistent embedding method was applied to both TCR and epitope sequences within a single prediction model. The second task is epitope-specific TCR clustering that aims at grouping TCRs that bind to the same epitope (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>). We tested with TCR sequences of human and mouse species sampled from McPAS [<xref ref-type="bibr" rid="c30">30</xref>] database.</p>
<p>We applied the hierarchical clustering [<xref ref-type="bibr" rid="c31">31</xref>] and reported normalized mutual information (NMI) [<xref ref-type="bibr" rid="c32">32</xref>] and cluster purity to qualify the goodness of the clustering partition of TCR sequences.</p>
<sec id="s2a">
<label>2.1</label><title><monospace>catELMo</monospace> outperforms the existing embedding methods at discriminating binding and non-binding TCR-epitope pairs</title>
<p>We investigated the downstream performance of TCR-epitope binding affinity prediction models trained using <monospace>catELMo</monospace> embeddings. In order to compare performance across different embedding methods, we used the identical downstream model architecture for each method. The competing embedding methods compared are BLOSUM62 [<xref ref-type="bibr" rid="c16">16</xref>], Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>], ProBert [<xref ref-type="bibr" rid="c27">27</xref>], SeqVec [<xref ref-type="bibr" rid="c26">26</xref>] and TCRBert [<xref ref-type="bibr" rid="c29">29</xref>]. We observed that the prediction model using <monospace>catELMo</monospace> embeddings significantly outperformed those using existing amino acid embedding methods in both TCR (<xref rid="fig2" ref-type="fig">Figure 2a</xref>, b) and epitope (<xref rid="fig2" ref-type="fig">Figure 2d</xref>, e) split. In TCR split, where no TCRs in the testing set exist in the training and validation set, <monospace>catELMo</monospace>’s prediction performance was significantly greater than the second best method (p-value &lt; 6.28 × 10<sup>−</sup><sup>23</sup>, <xref rid="tbl2" ref-type="table">Table 2</xref>). It achieved AUC 96.04% which was 14% points higher than that of the second-highest performing method, while the rest of the methods performed worse than or similar to BLOSUM62. In epitope split, where no epitopes in the testing set exist in the training and validation set, the prediction model using <monospace>catELMo</monospace> also outperformed others with even larger performance gaps. <monospace>catELMo</monospace> significantly boosted 17% points of AUCs than the second-highest performing method (p-value &lt; 1.18 × 10<sup>−</sup><sup>7</sup>, <xref rid="tbl3" ref-type="table">Table 3</xref>). Similar performance gains from <monospace>catELMo</monospace> were also observed in other metrics such as Precision, Recall, and F1 scores (Supplementary Fig. 1).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Comparison of the amino acid embedding methods for TCR-epitope binding affinity prediction task.</title>
<p>We obtained TCR and epitope embeddings and used them as input features of binding affinity prediction model. A binding affinity prediction model is trained on each embedding method’s embedding dataset. The prediction performance comparison on <bold>a), b), c)</bold> TCR split and <bold>d), e), f)</bold> epitope split. <bold>a), d)</bold> Receiver Operating Characteristic (ROC) curve and <bold>b), e)</bold> AUC of the prediction model trained on different embedding methods. Error bars represent standard deviations over 10 trials. <bold>c), f)</bold> AUCs of the prediction model trained on different portions of downstream datasets. Error bands represent 95% confidence intervals over 10 trials.</p></caption>
<graphic xlink:href="536635v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>TCR-epitope binding affinity prediction performance of TCR split.</title><p>Average and standard deviation of 10 trials are reported. P-values are from two-sample t-tests between <monospace>catELMo</monospace> and the second best method (<underline>underlined</underline>).</p></caption>
<graphic xlink:href="536635v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><title>TCR-epitope binding affinity prediction performance of epitope split.</title><p>Average and standard deviation of 10 trials are reported. P-values are from two-sample t-tests between <monospace>catELMo</monospace> and the second best method (<underline>underlined</underline>).</p></caption>
<graphic xlink:href="536635v2_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>We also visually observed that <monospace>catELMo</monospace> aided the model to better discriminate binding and non-binding TCRs for the five most frequent epitopes that appeared in our collected TCR-epitope pairs (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). These five epitopes account for a substantial portion of our dataset, comprising 14.73% (44,292 pairs) of the total TCR-epitope pairs collected. For visualization, we performed t-SNE [<xref ref-type="bibr" rid="c35">35</xref>] on the top fifty principal components of the last latent vectors of each prediction model. Each point represents a pair of TCR-epitope, colored by epitope (lighter shade for positive binding and darker shade for negative binding). Different degrees of overlapping between positive pairs and negative ones in regard to the same epitope can be seen in the t-SNE plots. For example, most of the binding and non-binding data points from SeqVec embeddings are barely separated within each epitope group. On the other hand, the t-SNE plot of <monospace>catELMo</monospace> exhibits noticeable contrast between binding and non-binding pairs, indicating that <monospace>catELMo</monospace> aids the prediction model to distinguish labels. We also observed <monospace>catELMo</monospace> outperformed the other embedding methods in discriminating binding and non-binding TCRs for almost all individual epitopes. As shown in Supplementary Fig. 2, the prediction model using <monospace>catELMo</monospace> embeddings achieved the highest AUCs in 39 out of 40 epitopes, and the second highest AUC on an epitope (<monospace>GTSGSPIVNR</monospace>) with only 1.09% lower than the highest score. Addtionally, we observed that <monospace>catELMo</monospace> consistently outperformed other embedding methods in predicting the binding affinity between TCRs and epitopes from a diverse range of pathogens (Supplementary Table 1).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>tSNE visualization for top five frequent epitopes.</title>
<p>We visually compared the embedding models on TCR-epitope binding affinity prediction task. We conduct tSNE analysis on the top 50 principle components of the last hidden layer features of the TCR-epitope binding affinity prediction models for out-of-sample epitopes. The clearer the boundary between positive pairs (lighter shade) and negative pairs (darker shade) associated with the same epitope sequence, the better the model is at discriminating binding and non-binding pairs.</p></caption>
<graphic xlink:href="536635v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<label>2.2</label><title><monospace>catELMo</monospace> reduces a significant amount of annotation cost for achieving comparable prediction power</title>
<p>Language models trained on large corpus are known to improve downstream task performance with a smaller number of downstream training data [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c19">19</xref>]. Similarly in TCR-epitope binding, we show that <monospace>catELMo</monospace> trained entirely on unlabeled TCR sequences facilitates its downstream prediction model to achieve the same performance with a significantly smaller amount of TCR-epitope training pairs (i.e., epitope-labeled TCR sequence). We trained a binding affinity prediction model for each k% of downstream data (i.e., <monospace>catELMo</monospace> embeddings of TCR-epitope pairs) where k = 1, 2, ···, 10, 20, 30, ···, 100. The widely used BLOSUM62 embedding matrix was used as a comparison baseline under the same ks as it performs better than or is comparable to the other embedding methods.</p>
<p>A positive log-linear relationship between the number of (downstream) training data and AUCs was observed for both TCR and epitope split (<xref rid="fig2" ref-type="fig">Fig. 2c, f</xref>). The steeper slope in <monospace>catELMo</monospace> suggests that prediction models utilizing <monospace>catELMo</monospace> embeddings exhibit higher performance gain per number of training pairs compared to the BLOSUM62-based models. In TCR split, we observed that <monospace>catELMo</monospace>’s binding affinity prediction models with just 7% of the training data significantly outperform ones that use a full size of BLOSUM62 embeddings (p-value = 0.0032, <xref rid="fig2" ref-type="fig">Fig. 2c</xref>). <monospace>catELMo</monospace> with just 3%, 4%, and 6% of the downstream training data achieved similar performances to when using a full size of Yang et al., ProtBert, and SeqVec embeddings, respectively. Similarly, in epitope split, we showed <monospace>catELMo</monospace>’s prediction models with just 3% of training data achieved equivalent performance as ones built on a full size of BLOSUM62 embeddings (p-value = 0.8531, <xref rid="fig2" ref-type="fig">Fig. 2f</xref>). Compare to the other embedding methods, <monospace>catELMo</monospace> with just 1%, 2%, and 5% of the downstream training data achieved similar or better performance than when using a full size of Yang et al., ProtBert, and SeqVec embeddings, separately. Similar performance gains from <monospace>catELMo</monospace> were also observed in other metrics such as Precision, Recall, and F1 scores (Supplementary Fig. 3). Achieving accurate prediction with a small amount of training data is important for TCR analysis as obtaining the binding affinity of TCR-epitope pairs is costly.</p>
</sec>
<sec id="s2c">
<label>2.3</label><title><monospace>catELMo</monospace> allows clustering of TCR sequences with high performance</title>
<p>Clustering TCRs of similar binding profiles is important in TCR repertoire analysis as it facilitates discoveries of TCR clonotypes that are condition-specific [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>]. In order to demonstrate that <monospace>catELMo</monospace> embeddings can be used for other TCR-related downstream tasks, we performed hierarchical clustering [<xref ref-type="bibr" rid="c31">31</xref>] using each method’s embedding (<monospace>catELMo</monospace>, BLOSUM62 [<xref ref-type="bibr" rid="c16">16</xref>], Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>], ProBert [<xref ref-type="bibr" rid="c27">27</xref>], SeqVec [<xref ref-type="bibr" rid="c26">26</xref>] and TCRBert [<xref ref-type="bibr" rid="c29">29</xref>]) and evaluated the identified clusters against the ground-truth TCR groups labeled by their binding epitopes. We additionally compared our results with state-of-the-art TCR clustering methods, TCRdist [<xref ref-type="bibr" rid="c37">37</xref>] and GIANA [<xref ref-type="bibr" rid="c38">38</xref>], both of which were developed from BLOSUM62 matrix (see <xref rid="s4d2" ref-type="sec">Section 4.4.2</xref>). Normalized mutual information (<italic>NMI</italic>) [<xref ref-type="bibr" rid="c32">32</xref>] and cluster purity are used to measure the clustering quality.</p>
<p>Significant disparities in TCR binding frequencies exist across different epitopes. To construct more balanced clusters, we targeted TCR sequences bound to the top eight frequent epitopes identified in the McPAS database. We find that the cluster model for both human and mouse species built on <monospace>catELMo</monospace> embeddings maintains either the best or second best NMI and purity scores compared with ones that are computed on other embeddings (<xref rid="fig4" ref-type="fig">Fig. 4a, d</xref>). To investigate whether this observation remains true on individual species, we conducted the same clustering analysis on human and mouse species, separately. We showcase comparison for the top eight epitopes in human (<xref rid="fig4" ref-type="fig">Fig. 4b, e</xref>) and mouse (<xref rid="fig4" ref-type="fig">Fig. 4c, f</xref>) species and observe a similar pattern that clustering results with <monospace>catELMo</monospace> achieve the highest or second-highest NMI and purity scores. Since the real-world scenarios likely involve more epitopes, we conducted additional clustering experiments using the top most abundant epitopes whose combined cognate TCRs make up at least 70% of TCRs across three databases (34 epitopes). Similar performance gains were observed (Supplementary Fig. 4). Altogether, <monospace>catELMo</monospace> embedding can assist TCR clustering with no supervision while achieving similar or better performance than other state-of-the-art methods in both human and mouse species.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Comparison of the amino acid embedding methods for epitope-specific TCR clustering.</title>
<p>Hierarchical clustering is applied on the McPAS [<xref ref-type="bibr" rid="c30">30</xref>] database. We cluster TCRs binding to the top eight epitopes of <bold>a</bold>) both human and mouse species, <bold>b</bold>) only human epitopes, and <bold>c</bold>) only mouse epitopes. Larger NMI scores indicate TCR sequences that bind to the same epitope are grouped together in the same cluster and TCR sequences that do not bind to the same epitope are separated apart in different clusters.</p></caption>
<graphic xlink:href="536635v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2d">
<label>2.4</label><title>ELMo-based architecture is preferable to BERT-based architecture in TCR embedding models</title>
<p>We observed <monospace>catELMo</monospace> using ELMo-based architecture outperformed the model using embeddings of TCRBert which uses BERT (<xref rid="tbl4" ref-type="table">Table 4</xref>). The performance differences were approximately 15% AUCs in TCR split (p-value &lt; 3.86 × 10<sup>−</sup><sup>30</sup>) and 19% AUCs in epitope split (p-value &lt; 3.29 × 10<sup>−</sup><sup>8</sup>). Because TCRBert was trained on a smaller amount of TCR sequences (around 0.5 million sequences) than <monospace>catELMo</monospace>, we further compared <monospace>catELMo</monospace> with various sizes of BERT-like models trained on the same dataset as <monospace>catELMo</monospace>: BERT-Tiny-TCR, BERT-Base-TCR, and BERT-Large-TCR having a stack of 2, 12, and 30 Transformer layers respectively (see <xref rid="s4f2" ref-type="sec">Section 4.6.2</xref>). Note that BERT-Base-TCR uses the same number of Transformer layers as TCRBert. Additionally, we compared different versions of <monospace>catELMo</monospace> by varying the number of BiLSTM layers (2, 4–default, and 8, see <xref rid="s4f1" ref-type="sec">Section 4.6.1</xref>). As summarized in <xref rid="tbl4" ref-type="table">Table 4</xref>, TCR-epitope binding affinity prediction models trained on <monospace>catELMo</monospace> embeddings (AUC 96.04% and 94.70% on TCR and epitope split) consistently outperformed models trained on these Transformer-based embeddings (AUC 81.23–81.91% and 74.20–74.94% on TCR and epitope split). The performance gaps between <monospace>catELMo</monospace> and Transformer-based models (14% AUCs in TCR split and 19% AUCs in epitope split) were statistically significant (p-values &lt; 6.72 × 10<sup>−</sup><sup>26</sup> and &lt; 1.55 × 10<sup>−</sup><sup>7</sup> for TCR and epitope split respectively). We observed that TCR-epitope binding affinity prediction models trained on <monospace>catELMo</monospace>-based embeddings consistently outperformed the ones using Transformer-based embeddings (<xref rid="tbl4" ref-type="table">Table 4</xref>, 5). Even the worst- performed BiLSTM-based embedding model achieved higher AUCs than the best-performed Transformer-based embeddings at discriminating binding and non-binding TCR-epitope pairs in both TCR (p-value &lt; 2.84 × 10<sup>−</sup><sup>28</sup>) and epitope split (p-value &lt; 5.86 × 10<sup>−</sup><sup>6</sup>).</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction models built on BERT-based embedding models.</title><p>Average and standard deviation of 10 trials are reported.</p></caption>
<graphic xlink:href="536635v2_tbl4.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction models trained on different sizes of <monospace>catELMo</monospace> embeddings.</title><p>Average and standard deviation of 10 trials are reported.</p></caption>
<graphic xlink:href="536635v2_tbl5.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s2e">
<label>2.5</label><title>Within-domain transfer learning is preferable to cross-domain transfer learning in TCR analysis</title>
<p>We showed that <monospace>catELMo</monospace>, trained on TCR sequences, significantly outperformed amino acid embedding methods trained on generic protein sequences. <monospace>catELMo</monospace>-Shallow and SeqVec shared the same architecture consisting of character-level convolutional layers and a stack of two bi-directional LSTM layers but were trained on different types of training data. <monospace>catELMo</monospace>-Shallow was trained on TCR sequences (about 4 million) while SeqVec was trained on generic protein sequences (about 33 million). Although <monospace>catELMo</monospace>-Shallow was trained on a relatively smaller amount of sequences compared to SeqVec, the binding affinity prediction model built on <monospace>catELMo</monospace>-Shallow embeddings (AUC 95.67% in TCR split and 86.32% in epitope split) significantly outperformed the one built on SeqVec embeddings (AUC 81.61% in TCR split and 76.71% in epitope split) by 14.06% and 9.61% on TCR and epitope split respectively. This suggests that knowledge transfer within the same domain is preferred whenever possible in TCR analysis.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label><title>Discussion</title>
<p><monospace>catELMo</monospace> is an effective embedding model that brings substantial performance improvement in TCR-related downstream tasks. Our study emphasizes the importance of choosing the right embedding models. The embedding of amino acids into numeric vectors is the very first and crucial step that enables the training of a deep neural network. It has been previously demonstrated that a well-designed embedding can lead to significantly improved results on downstream analysis [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>]. The reported performance of <monospace>catELMo</monospace> embedding on TCR-epitope binding affinity prediction and TCR clustering tasks indicates that <monospace>catELMo</monospace> is able to learn patterns of amino acid sequences more effectively than state-of-the-art embedding methods. While all other methods compared (except BLOSUM62) leverage a large number of unlabeled amino acid sequences, only our prediction model using <monospace>catELMo</monospace> significantly outperforms widely used BLOSUM62 and other models such as netTCR [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c17">17</xref>] and ATM-TCR [<xref ref-type="bibr" rid="c10">10</xref>] trained on paired (TCR-epitope) samples only (<xref rid="tbl6" ref-type="table">Table 6</xref>). Our work suggests the need for developing sophisticated strategies to train amino acid embedding models that can enhance the performance of TCR-related downstream tasks even while requiring less amount of data and simpler prediction model structures.</p>
<table-wrap id="tbl6" orientation="portrait" position="float">
<label>Table 6.</label>
<caption><title>AUCs of TCR-epitope binding affinity prediction models comparison with state-of-the-art prediction models.</title><p>All models are trained on the same dataset. Average and standard deviation of 10 trials are reported.</p></caption>
<graphic xlink:href="536635v2_tbl6.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Two important observations made from our experiments are 1) the type of data used for training amino acid embedding models is far more important than the amount of data and 2) ELMo-based embedding models consistently perform much better than BERT-based embedding models. While previously developed amino acid embedding models such as SeqVec and ProtBert were trained on 184- and 1,690-times more amino acid tokens compared to the training data used for <monospace>catELMo</monospace>, the prediction models using SeqVec and ProtBert performed poorly compared to the model using <monospace>catELMo</monospace> (see Sections 2.1 and 2.3). SeqVec and ProtBert were trained based on generic protein sequences, whereas <monospace>catELMo</monospace> was trained on a collection of TCR sequences from pooled TCR repertoires across many samples, indicating that the use of TCR data to train embedding models is more critical than much larger amount of generic protein sequences.</p>
<p>In the field of natural language processing, Transformer-based models have been bolstered as the superior embedding model [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c19">19</xref>]. However, for TCR-related downstream tasks, <monospace>catELMo</monospace> using biLSTM layer-based design outperforms BERT using Transformer layers (see <xref rid="s2d" ref-type="sec">Section 2.4</xref>). While it is difficult to pinpoint the reasons, the bi-directional architecture to predict the next token based on its previous tokens in ELMo may mimic the interaction process of TCR and epitope sequences either from left to right or from right to left. In contrast, BERT uses Transformer encoder layers that attend tokens both on the left and right to predict a masked token, refer to as masked language modeling. As the Transformer layer can be along with the next token prediction objectives, it remains as a future work to investigate Transformer with causal language models, such as GPT-3 [<xref ref-type="bibr" rid="c14">14</xref>], for amino acid embedding. Additionally, the clear differences of TCR sequences compared to natural languages are 1) the compact vocabulary size (20 standard amino acids vs. over 170k English words) and 2) the length of peptides in TCRs being smaller than the number of words in sentences or paragraphs in natural languages. These differences may allow <monospace>catELMo</monospace> to learn sequential dependence without losing long-term memory from the left end.</p>
<p>Often in classification problems in life sciences, the difference in the number of available positive and negative data can be very large and TCR-epitope binding affinity prediction problem is no exception. In fact, the number of experimentally generated non-binding pairs are practically non-existent and obtaining experimental negative data is costly [<xref ref-type="bibr" rid="c17">17</xref>]. This requires researchers to come up with a strategy to generate negative samples and it can be non-trivial. A common practice is to sample new TCRs from repertoires and pair them with existing epitopes [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c39">39</xref>], a strategy we also used. Another approach is to randomly shuffle TCR-epitope pairs within positive binding dataset, resulting in TCRs and epitopes that are not known to bind paired together [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>]. Given the vast diversity of human TCR clonotypes, which can exceed 10<sup>15</sup> [<xref ref-type="bibr" rid="c40">40</xref>], the chance of randomly selecting a TCR that specifically recognizes a target epitope is relatively small. The prediction model consistently outperformed the other embedding methods by large margins in both TCR and epitope splits as shown in Supplementary Fig. 5. The model using <monospace>catELMo</monospace> achieves 24% and 36% higher AUCs over the second best embedding method for TCR (p-value &lt; 1.04 × 10<sup>−</sup><sup>18</sup>, Supplementary Table 2) and epitope (p-value &lt; 6.26 × 10<sup>−</sup><sup>14</sup>, Supplementary Table 3) split, respectively. Moreover, we observe that using <monospace>catELMo</monospace> embeddings, prediction models that are trained with only 2% downstream samples still statistically outperform ones that are built on a full size of BLOSUM62 embeddings in TCR split (p-value = 0.0005). Similarly, with only 1% training samples, <monospace>catELMo</monospace> reaches comparable results as BLOSUM62 with a full size of downstream samples in epitope split (p-value = 0.1438). In other words, <monospace>catELMo</monospace> dramatically reduces about 98% annotation cost (Supplementary Table 4). To mitigate potential batch effects, we generated new negative pairs using different seeds and observed consistent prediction performance across these variations. Our experiment results confirm that the embeddings from <monospace>catELMo</monospace> maintain high performance regardless of the methodology used to generate negative samples.</p>
<p>Parameter fine-tuning in neural networks is a training scheme where initial weights of the network are set to the weights of a pre-trained network. Fine-tuning has been shown to bring performance gain to the model over using random initial weights [<xref ref-type="bibr" rid="c41">41</xref>]. We investigated the possibility of performance boost of our prediction model using fine-tuned <monospace>catELMo</monospace>. Since SeqVec shares the same architecture with <monospace>catELMo</monospace>-Shallow and is trained on generic protein sequences, we used the weights of SeqVec as initial weights when fine-turning <monospace>catELMo</monospace>-Shallow. We compared the performance of binding affinity prediction models using the fine-tuned <monospace>catELMo</monospace>-Shallow and vanilla <monospace>catELMo</monospace>-Shallow (trained from scratch with random initial weights from a standard normal distribution). We observed that the performance when using fine-tuned <monospace>catELMo</monospace>-Shallow embeddings was significantly improved by approximately 2% AUCs in TCR split (p-value &lt; 4.17 × 10<sup>−</sup><sup>9</sup>) and 9% AUCs in epitope split (p-value &lt; 5.46 × 10<sup>−</sup><sup>7</sup>).</p>
<p>While epitope embeddings are a part of our prediction models, their impact on overall performance appears to be less significant compared to that of TCR embeddings. To understand the contribution of epitope embeddings, we performed additional experiments. First, we kept epitope embeddings unchanged using the widely-used BLOSUM62-based while varying different embedding methods exclusively for TCRs. The results (Supplementary Table 5) closely aligns with our previous findings (<xref rid="tbl2" ref-type="table">Tables 2</xref> and <xref rid="tbl3" ref-type="table">3</xref>), suggesting that the choice of epitope embedding method may not strongly affect the final predictive performance.</p>
<p>Furthermore, we investigated alternative embedding approaches for epitope sequences. Specifically, we replaced epitope embeddings with randomly initialized matrices containing trainable parameters, while employing <monospace>catELMo</monospace> for TCR embeddings. This setting yielded predictive performance comparable to the scenario where both TCR and epitope embeddings were <monospace>catELMo</monospace>-based (Supplementary Table 6).</p>
<p>Similarly, using BLOSUM62 for TCR embeddings and <monospace>catELMo</monospace> for epitope embeddings resulted in performance similar to when both embeddings were based on BLOSUM62. These consistent findings support the proposition that the influence of epitope embeddings may not be as significant as that of TCR embeddings (Supplementary Table 7).</p>
<p>We believe these observations may be attributed to the substantial data scale discrepancy between TCRs (more than 290k) and epitopes (less than 1k). Moreover, TCRs tend to exhibit high similarity, whereas epitopes display greater distinctiveness from one another. These features of TCRs require robust embeddings to facilitate effective separation and improve downstream performance, while epitope embeddings primarily serve as categorical encodings.</p>
<p>While TCR<italic>β</italic> CDR3 is known to be the primary determinant for TCR-epitope binding specificity, other regions such as CDR1 and CDR2 on TCR<italic>β</italic> V gene along with TCRα chain are also known to contribute to specificity in antigen recognition [<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>].There are models that can take advantage of these additional features when making predictions [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c44">44</xref>]. However, our study focuses on modeling CDR3 of TCR<italic>β</italic> chains because of the limited availability of sample data from other regions. Future work may explore strategies to incorporate these regions while mitigating the challenges of working with limited samples.</p>
</sec>
<sec id="s4">
<label>4</label><title>Methods</title>
<p>We first present data used for training the amino acid embedding models and the downstream tasks. We then review existing amino acid embedding methods and their usage on TCR-related tasks. We introduce our approach, <monospace>catELMo</monospace>, a bi-directional amino acid embedding method that computes contextual representation vectors of amino acids of a TCR (or epitope) sequence. We describe in detail how to apply <monospace>catELMo</monospace> to two different TCR-related downstream tasks. Lastly, we provide details on the experimental design, including the methods and parameters used in comparison and ablation studies.</p>
<sec id="s4a">
<label>4.1</label><title>Data</title>
<sec id="s4a1">
<title>TCRs for training <monospace>catELMo</monospace></title>
<p>We collected 5,893,249 TCR sequences from repertoires of seven projects in the ImmunoSEQ database: HIV [<xref ref-type="bibr" rid="c45">45</xref>], SARS-CoV2 [<xref ref-type="bibr" rid="c23">23</xref>], Epstein Barr Virus [<xref ref-type="bibr" rid="c46">46</xref>], Human Cytomegalovirus [<xref ref-type="bibr" rid="c47">47</xref>], Influenza A [<xref ref-type="bibr" rid="c48">48</xref>], Mycobacterium Tuberculosis [<xref ref-type="bibr" rid="c49">49</xref>], and Cancer Neoantigens [<xref ref-type="bibr" rid="c50">50</xref>]. CDR3 sequences of TCR<italic>β</italic> chains were used to train the amino acid embedding models as those are the major segment interacting with epitopes [<xref ref-type="bibr" rid="c2">2</xref>] and exist in large numbers. We excluded duplicated copies and sequences containing wildcards such as ‘*’ or ‘X’. Altogether, we obtained 4,173,895 TCR sequences (52,546,029 amino acid tokens) of which 85% were used for training and 15% were used for testing.</p>
</sec>
<sec id="s4a2">
<title>TCR-epitope pairs for binding affinity prediction</title>
<p>We collected TCR-epitope pairs known to bind each other from three publicly available databases: IEDB [<xref ref-type="bibr" rid="c34">34</xref>], VDJdb [<xref ref-type="bibr" rid="c33">33</xref>], and McPAS [<xref ref-type="bibr" rid="c30">30</xref>]. Unlike the (unlabeled) TCR dataset for <monospace>catELMo</monospace> training, each TCR is annotated with an epitope known to bind each other, which we referred to as a TCR-epitope pair. We only used pairs with human MHC class I epitopes and CDR3 sequences of the TCR<italic>β</italic> chain. We further filtered out sequences containing wildcards such as ‘*’ or ‘X’. For VDJdb, we excluded pairs with a confidence score of 0 as it means a critical aspect of sequencing or specificity validation is missing. We removed duplicated copies and merged datasets collected from the three databases. For instance, 29.85% of pairs from VDJdb overlapped with IEDB, and 55.41% of pairs from McPAS overlapped with IEDB. Altogether, we obtained 150,008 unique TCR-epitope pairs known to bind to each other having 140,675 unique TCRs and 982 unique epitopes. We then generated the same number of non-binding TCR-epitope pairs as negative samples by randomly pairing each epitope of the positive pairs with a TCR sampled from the healthy TCR repertoires of ImmunoSEQ. We note that it includes no identical TCR sequences with the TCRs used for training the embedding models. Altogether, we obtained 300,016 TCR-epitope pairs where 150,008 pairs are positive and 150,008 pairs are negative. The average length of TCRs and epitope sequences are 14.78 and 11.05, respectively. Our data collection and preprocessing procedures closely followed those outlined in our previous work [<xref ref-type="bibr" rid="c10">10</xref>].</p>
</sec>
<sec id="s4a3">
<title>TCRs for antigen-specific TCR clustering</title>
<p>We collected 9,822 unique TCR sequences of humans and mice hosts from McPAS [<xref ref-type="bibr" rid="c30">30</xref>]. Each TCR is annotated with an epitope known to bind, which is used as a ground-truth label for TCR clustering. We excluded TCR sequences that bind to neoantigen pathogens or multiple epitopes and only used CDR3 sequences of TCR<italic>β</italic> chain. We composed three subsets for different experimental purposes. The first dataset contains both human and mice TCRs. We used TCRs associated with the top eight frequent epitopes, resulting in 5,607 unique TCRs. The second dataset consists of only human TCRs, and the third dataset consists of only mouse TCRs. In a similar manner, we selected TCRs that bind to the top eight frequent epitopes. As a result, we obtained 5,528 unique TCR sequences for the second dataset and 1,322 unique TCR sequences for the third dataset.</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label><title>Amino acid embedding methods</title>
<p>In this section, we review amino acid embedding methods previously proposed. There are two categories of the existing approaches: static and context-aware embedding methods. Static embedding method represents an amino acid as a static representation vector remaining the same regardless of its context. Context-aware embedding method, however, represents an amino acid differently in accordance with its context. Context-aware embedding is also called dynamic embedding in contrast to static embedding. We explain the key ideas of various embedding methods, and introduce their usage in previous works.</p>
<sec id="s4b1">
<label>4.2.1</label><title>Static embeddings</title>
<sec id="s4b1a">
<title>BLOSUM</title>
<p>BLOSUM [<xref ref-type="bibr" rid="c16">16</xref>] is a scoring matrix where each element represents how likely an amino acid residue is to be substituted by another over evolutionary time. It has been commonly used to measure alignment scores between two protein sequences. There are various BLOSUM matrices such as BLOSUM45, BLOSUM62, and BLOSUM80 where a matrix with a higher number is used for the alignment of less divergent sequences. BLOSUM have also served as the de facto standard embedding method for various TCR analyses. For example, BLOSUM62 was used to embed TCR and epitope sequences for training deep neural network models predicting their binding affinity [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c17">17</xref>]. BLOSUM62 was also used to embed TCR sequences for antigen-specific TCR clustering and TCR repertoire clustering. GIANA [<xref ref-type="bibr" rid="c38">38</xref>] clustered TCRs based on the Euclidean distance between TCR embeddings. TCRdist [<xref ref-type="bibr" rid="c37">37</xref>] used BLOSUM62 matrix to compute the dissimilarity matrix between TCR sequences for clustering.</p>
</sec>
<sec id="s4b1b">
<title>Word2vec and Doc2vec</title>
<p>Word2vec [<xref ref-type="bibr" rid="c20">20</xref>] and Doc2vec [<xref ref-type="bibr" rid="c21">21</xref>] are a family of embedding models to learn a single linear mapping of words, which takes a one-hot word indicator vector as input and returns a real-valued word representation vector as output. There are two types of Word2vec architectures: continuous bag-of-words (CBOW) and skip-gram. CBOW predicts a word from its surrounding words in a sentence. It embeds each input word via a linear map, sums all input words’ representations, and applies a softmax layer to predict an output word. Once training is completed, the linear mapping is used to obtain a representation vector of a word. On the contrary, skip-gram predicts the surrounding words given a word while it also uses a linear mapping to obtain a representation vector. Doc2vec is a model further generalized from Word2vec, which introduces a paragraph vector representing paragraph identity as an additional input. Doc2vec also has two types of architectures: distributed memory (DM) and distributed bag-of-words (DBOW). DM predicts a word from its surrounding words and the paragraph vector, while DBOW uses the paragraph vector to predict randomly sampled context words. In a similar way, linear mapping is used to obtain a continuous representation vector of a word.</p>
<p>Several studies adapted Word2vec and Doc2vec to embed amino acid sequences [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c25">25</xref>]. ProtVec [<xref ref-type="bibr" rid="c24">24</xref>] is the first Word2vec representation model trained on a large number of amino acid sequences. Its embeddings were used for several downstream tasks such as protein family classification, disordered protein visualization, and classification. Kimothi et al. [<xref ref-type="bibr" rid="c51">51</xref>] adapted Doc2vec to embed amino acid sequences for protein sequence classification and retrieval. Yang et al. [<xref ref-type="bibr" rid="c25">25</xref>] trained Doc2vec models on 524,529 protein sequences of UniProt [<xref ref-type="bibr" rid="c22">22</xref>] database. They considered a k-mer amino acids as a word, and a protein sequence as a paragraph. They trained DM models to predict a word from w surrounding words and a paragraph with various sizes of k and w.</p>
</sec>
</sec>
<sec id="s4b2">
<label>4.2.2</label><title>Context-aware embeddings</title>
<sec id="s4b2a">
<title>ELMo</title>
<p>ELMo [<xref ref-type="bibr" rid="c18">18</xref>] is a deep context-aware word embedding model trained on a large corpus. It learns each token’s (e.g., a word) contextual representation in forward and backward directions using a stack of two bi-directional LSTM layers. Each word of a text string is first mapped into a numerical representation vector via the character-level convolutional layers. The forward (left-to-right) pass learns a token’s contextual representation depending on itself and the previous context in which it is used. The backward (left-to-right) pass learns a token’s representation depending on itself and its subsequent context.</p>
<p>ELMo is less commonly implemented for amino acid embedding than Transformer-based deep neural networks. One example is SeqVec [<xref ref-type="bibr" rid="c26">26</xref>]. It is an amino acid embedding model using ELMo’s architecture. It feeds each amino acid as a training token of size 1, and learns its contextual representation both forward and backward within a protein sequence. The data was collected from UniRef50 [<xref ref-type="bibr" rid="c52">52</xref>], which consists of 9 billion amino acid tokens and 33 million protein sequences. SeqVec was applied to several protein-related downstream tasks such as secondary structure and long intrinsic disorder prediction, and subcellular localization.</p>
</sec>
<sec id="s4b2b">
<title>BERT</title>
<p>BERT [<xref ref-type="bibr" rid="c19">19</xref>] is a large language model leveraging Transformer [<xref ref-type="bibr" rid="c53">53</xref>] layers to learn context-aware word embeddings jointly conditioned on both directions. BERT is learned for two objectives. One is the masked language model to learn contextual relationships between words in a sentence. It aims to predict the original value of masked words. The other is the next sentence prediction which aims to learn the dependency between consecutive sentences. It feeds a pair of sentences as input and predicts whether the first sentence in the pair is contextually followed by the second sentence.</p>
<p>BERT’s architecture has been used in several amino acid embedding methods [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c54">54</xref>, <xref ref-type="bibr" rid="c29">29</xref>]. They treated an amino acid residue as a word and a protein sequence as a sentence. ProtBert [<xref ref-type="bibr" rid="c27">27</xref>] was trained on 216 million protein sequences (88 billion amino acid tokens) of UniRef100 [<xref ref-type="bibr" rid="c52">52</xref>]. It was applied for several protein sequence applications such as secondary structure prediction and sub-cellular localization. ProteinBert [<xref ref-type="bibr" rid="c54">54</xref>] combined language modeling and gene ontology annotation prediction together during training. It was applied to protein secondary structure, remote homology, fluorescence and stability prediction. TCRBert [<xref ref-type="bibr" rid="c29">29</xref>] was trained on 47,040 TCR<italic>β</italic> and 4,607 TCRα sequences of PIRD [<xref ref-type="bibr" rid="c55">55</xref>] dataset and evaluated on TCR-antigen binding prediction and TCR engineering tasks.</p>
</sec>
</sec>
</sec>
<sec id="s4c">
<label>4.3</label><title>Our approach: <monospace>catELMo</monospace></title>
<p>We propose <monospace>catELMo</monospace>, a bi-directional amino acid embedding model designed for TCR analysis. <monospace>catELMo</monospace> adapts ELMo’s architecture to learn context-aware representations of amino acids. It is trained on TCR sequences, which is different from the existing amino acid embedding models such as SeqVec trained on generic protein sequences. As illustrated in <xref rid="fig1" ref-type="fig">Fig. 1a</xref>, <monospace>catELMo</monospace> is composed of a character CNN (CharCNN) [<xref ref-type="bibr" rid="c56">56</xref>] layer converting each one-hot encoded amino acid token to a continuous representation vector, a stack of four bi-directional LSTM [<xref ref-type="bibr" rid="c57">57</xref>] layers learning contextual relationship between amino acid residues, and a softmax layer predicting the next (or previous) amino acid residue.</p>
<p>Given a sequence of N amino acid tokens, (<italic>t</italic><sub>1</sub>, <italic>t</italic><sub>2</sub>, …, <italic>t<sub>N</sub></italic>), CharCNN maps each one-hot encoded amino acid token <italic>t<sub>k</sub></italic> to a latent vector c<italic><sub>k</sub></italic> through seven convolutional layers with kernel sizes ranging from 1 to 7, and the numbers of filters of 32, 32, 64, 128, 256, 512, and 512, each of which is followed by a max-pooling layer, resulting in a 1,024-dimensional vector. The output of the CharCNN, (<italic>c</italic><sub>1</sub>, <italic>c</italic><sub>2</sub>, …, <italic>c<sub>N</sub></italic>), is then fed into a stack of four bidirectional LSTM layers consisting of forward and backward passes. For the forward pass, the sequence of the CharCNN output is fed into the first forward LSTM layer followed by the second forward LSTM layer, and so on. Each LSTM cell in every forward layer has 4,096 hidden states and returns a 512-dimensional representation vector. Each output vector of the last LSTM layer is then fed into a softmax layer to predict the right next amino acid token. Residual connection is applied between the first and second layers and between the third and fourth layers to prevent gradient vanishing. Similarly, the sequence of the CharCNN output is fed into the backward pass, in which each cell returns a 512-dimensional representation vector. Unlike the forward layer, each output vector of the backward layer followed by a softmax layer aims to predict the left next amino acid token.</p>
<p>Through the forward and backward passes, <monospace>catELMo</monospace> models the joint probability of a sequence of amino acid tokens. The forward pass aims to predict the next right amino acid token given its left previous tokens, which is P (<italic>t<sub>k</sub></italic>|<italic>t</italic><sub>1</sub>, <italic>t</italic><sub>2</sub>, …, <italic>t<sub>k—</sub></italic><sub>1</sub>; <italic>θ<sub>c</sub></italic>, <italic>θ<sub>fw</sub></italic>, <italic>θ<sub>s</sub></italic>) for each k-th cell where <italic>θ<sub>c</sub></italic> indicates parameters of CharCNN, <italic>θ<sub>fw</sub></italic> indicates parameters of the forward layers, and <italic>θ<sub>s</sub></italic> indicates parameters of the softmax layer. The joint probability of all amino acid tokens for the forward pass is defined as:
<disp-formula>
<graphic xlink:href="536635v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
The backward pass aims to predict the next left amino acid token given its right previous tokens. Similarly, the joint probability of all amino acid tokens for the backward pass is defined as:
<disp-formula>
<graphic xlink:href="536635v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>θ<sub>bw</sub></italic> indicates parameters of the backward layers. During <monospace>catELMo</monospace> training, the combined log-likelihood of the forward and backward passes is jointly optimized, which is defined as:
<disp-formula>
<graphic xlink:href="536635v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Note that the forward and backward layers have their own weights (<italic>θ<sub>fw</sub></italic> and <italic>θ<sub>bw</sub></italic>). This helps to avoid information leakage that a token used to predict its right tokens in forward layers is undesirably used again to predict its own status in backward layers.</p>
<p>For each amino acid residue, <monospace>catELMo</monospace> computes five representation vectors of length 1,024: one from CharCNN and four from BiLSTM layers. Those vectors are averaged over and yield an amino acid representation vector of length 1,024. A sequence of amino acids is then represented by an element-wise average of all amino acids’ representation vectors, resulting in a representation vector of length 1,024. For example, <monospace>catELMo</monospace> computes a representation for each amino acid in a TCR sequence CASSP T SGGQET QY F as a vector of length 1,024. The sequence is then represented by averaging over 15 amino acid representation vectors, which is a vector with a length of 1,024. <monospace>catELMo</monospace> is trained up to 10 epochs with a batch size of 128 on two NVIDIA RTX 2080 GPUs. We follow the default experimental settings of ELMo unless otherwise specified.</p>
</sec>
<sec id="s4d">
<label>4.4</label><title>Downstream tasks</title>
<p>We evaluate the amino acid embedding models’ generalization performance on two downstream tasks: TCR-epitope binding affinity prediction and epitope-specific TCR clustering.</p>
<sec id="s4d1">
<label>4.4.1</label><title>TCR-epitope binding affinity prediction</title>
<p>Computational approaches that predict TCR-epitope binding affinity benefit rapid TCR screening for a target antigen and improve personalized immunotherapy. Recent computational studies [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c17">17</xref>] formulated it as a binary classification problem that predicts a binding affinity score (0–1) given a pair of TCR and epitope sequences.</p>
<p>We evaluate <monospace>catELMo</monospace> based on the prediction performance of a binding affinity prediction model trained on its embedding, and compare it with the state-of-the-art amino acid embeddings (further demonstrated in <xref rid="s4e" ref-type="sec">Section 4.5</xref>). We first obtain different types of TCR and epitope embeddings from <monospace>catELMo</monospace> and the comparison methods. To measure the generalized prediction performance of binding affinity prediction models, we split each method’s dataset into training (64%), validation (16%), and testing (20%) sets. We use two splitting strategies established in the previous work [<xref ref-type="bibr" rid="c10">10</xref>]: TCR split and epitope split. TCR split was designed to measure the models’ prediction performance on out-of-sample TCRs where no TCRs in the testing set exist in the training and validation set. Epitope split was designed to measure the models’ prediction performance on out-of-sample epitopes where no epitopes in the testing set exist in the training and validation set.</p>
<p>The downstream model architecture is the same across all embedding methods, having three linear layers where the last layer returns a binding affinity score (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). Taking <monospace>catELMo</monospace> as an example, we first obtain a <monospace>catELMo</monospace> representation of length 1,024 for each sequence. We then feed the TCR representation to a linear layer with 2,048 neurons, followed by a Sigmoid Linear Units (SiLU) activation function [<xref ref-type="bibr" rid="c58">58</xref>], batch normalization [<xref ref-type="bibr" rid="c59">59</xref>], and 0.3 rate dropout [<xref ref-type="bibr" rid="c60">60</xref>]. Similarly, we feed the epitope representation to another linear layer with 2,048 neurons, followed by the same layers. The outputs of TCR and epitope layers are then concatenated (4,096 neurons) and passed into a linear layer with 1,024 neurons, followed by a SiLU activation function, batch normalization, and 0.3 rate dropout. Finally, we append the last linear layer with a neuron followed by a sigmoid activation function to obtain the binding affinity score ranging from 0 to 1. The models are trained to minimize a binary cross-entropy loss via Adam optimizer [<xref ref-type="bibr" rid="c61">61</xref>]. We set the batch size as 32 and the learning rate as 0.001. We stop the training if either the validation loss does not decrease for 30 consecutive epochs or it iterates over 200 epochs. Finally, we compare and report AUC scores of binding affinity prediction models of the different embedding methods.</p>
</sec>
<sec id="s4d2">
<label>4.4.2</label><title>Epitope-specific TCR clustering</title>
<p>Clustering TCRs is the first and fundamental step in TCR repertoire analysis as it can potentially identify TCR clonotypes that are condition-specific [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>]. We perform hierarchical clustering [<xref ref-type="bibr" rid="c31">31</xref>] to <monospace>catELMo</monospace> and the state-of-the-art amino acid embeddings (further demonstrated in <xref rid="s4e" ref-type="sec">Section 4.5</xref>). We also obtain clusters from the existing TCR clustering approaches (TCRdist and GIANA). Both methods are developed on the BLOSUM62 matrix and apply nearest neighbor search to cluster TCR sequences. GIANA used the CDR3 of TCR<italic>β</italic> chain and V gene, while TCRdist predominantly experimented with CDR1, CDR2, and CDR3 from both TCRα and TCR<italic>β</italic> chains. For fair comparisons, we perform GIANA and TCRdist only on CDR3 <italic>β</italic> chains and with hierarchical clustering instead of the nearest neighbor search.</p>
<p>We first obtain different types of TCR embeddings from <monospace>catELMo</monospace> and the comparison methods. All embedding methods except BLOSUM62 yield the same size representation vectors regardless of TCR length. For BLOSUM62 embedding, we pad the sequences so that all sequences are mapped to the same size vectors (further demonstrated in <xref rid="s4e" ref-type="sec">Section 4.5</xref>). We then perform hierarchical clustering on TCR embeddings of each method. In detail, the clustering algorithm starts with each TCR as a cluster with size 1. It repeatedly merges the closest two clusters based on the Euclidean distance between TCR embeddings until it reaches the target number of clusters.</p>
<p>We compare the normalized mutual information (NMI) between the identified cluster and the ground-truth.</p>
<p>NMI is a harmonic mean between homogeneity and completeness. Homogeneity measures how many TCRs in a cluster bind to the same epitope, while completeness measures how many TCRs binding to the same epitope are clustered together. A higher value indicates a better clustering result. It ranges from zero to one where zero indicates no mutual information found between the identified clusters and the ground-truth clusters and one indicates a perfect correlation.</p>
</sec>
</sec>
<sec id="s4e">
<label>4.5</label><title>Comparison studies</title>
<p>We demonstrate how we implement existing amino acid embedding methods to compare with catELMo for the two TCR-related downstream tasks.</p>
<sec id="s4e1">
<title>BLOSUM62</title>
<p>Among various typs of BLOSUM matrices, we use BLOSUM62 as it has been widely used in many TCR-related models [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. We obtain embeddings by mapping each amino acid to a vector of length 24 via BLOSUM62 matrix. Since TCRs (or epitopes) have varied lengths of the sequences, we pad each sequence using IMGT [<xref ref-type="bibr" rid="c62">62</xref>] method. If a TCR sequence is shorter than the predefined length 20 (or 22 for epitopes) [<xref ref-type="bibr" rid="c10">10</xref>], we add zero-padding to the middle of the sequence. Otherwise, we remove amino acids from the middle of the sequence until it reaches the target length. For each TCR, we flatten 20 amino acid embedding vectors of length 24 into a vector of length 480. For each epitope, we flatten 22 amino acid embedding vectors of length 24 into a vector of length 528.</p>
</sec>
<sec id="s4e2">
<title>Yang et al</title>
<p>We select the 3-mer model with a window size of 5 to embed TCR and epitope sequences, which is the best combination obtained from a grid search. Each 3-mer is embedded as a numeric vector of length 64. The vectors are averaged to represent a whole sequence, resulting in a vector of length 64.</p>
</sec>
<sec id="s4e3">
<title>SeqVec and ProtBert</title>
<p>We embed each amino acid as a numeric vector of length 1,024. The vectors are element-wisely averaged to represent a whole sequence, resulting in a vector of length 1,024.</p>
</sec>
<sec id="s4e4">
<title>TCRBert</title>
<p>We embed each amino acid as a numeric vector of length 768. The vectors are element-wisely averaged to represent a whole sequence with a vector of length 768.</p>
</sec>
</sec>
<sec id="s4f">
<label>4.6</label><title>Ablation studies</title>
<p>We provide details of our experimental design and ablation studies.</p>
<sec id="s4f1">
<label>4.6.1</label><title>Depth of <monospace>catELMo</monospace></title>
<p>We investigate the effect of various depths of <monospace>catELMo</monospace> on TCR-epitope binding affinity prediction performance. We compare <monospace>catELMo</monospace> with different numbers of BiLSTM layers, specifically <monospace>catELMo</monospace>-Shallow, <monospace>catELMo</monospace>, and <monospace>catELMo</monospace>-Deep with 2, 4 and 8 layers respectively. Other hyperparameters and the training strategy remained the same as described in <xref rid="s4c" ref-type="sec">Section 4.3</xref>. For each amino acid residue, we average the output vectors of CharCNN and four (or two, eight) BiLSTM, resulting in a numerical vector of length 1,024. We then element-wisely average over all amino acids’ representations to represent a whole sequence, resulting in a numerical vector of length 1,024. Embeddings from various depths are used to train binding affinity prediction models, resulting in three sets of downstream models. All settings of the downstream models remain the same as described in <xref rid="s4d1" ref-type="sec">Section 4.4.1</xref>. The downstream models’ prediction performance is compared to investigate the optimal depth of <monospace>catELMo</monospace>.</p>
</sec>
<sec id="s4f2">
<label>4.6.2</label><title>Neural architecture of <monospace>catELMo</monospace></title>
<p>We compare <monospace>catELMo</monospace> with BERT-based amino acid embedding models using another context-aware architecture, Transformer, which has shown outstanding performance in natural language processing tasks. We train different sizes of BERT, a widely used Transformer-based model, for amino acid embedding, named BERT-Base-TCR, BERT-Tiny-TCR, and BERT-Large-TCR. Each model has 2, 12, and 30 Transformer layers and returns 768, 768, and 1024 sizes of embeddings for each amino acid token. Their objectives, however, only consist of the masked language prediction and do not include the next sentence prediction. For each TCR sequence, 15% of amino acid tokens are masked out and the model is trained to recover the masked tokens based on the remaining ones. The models are trained on the same training set as <monospace>catELMo</monospace> for 10 epochs. Other parameter settings are the same as TCRBert, which is included as one of the comparison models. All other settings remain the same as described in <xref rid="s4d1" ref-type="sec">Section 4.4.1</xref>. TCRBert and BERT-Base-TCR share the same architecture, whereas TCRBert is trained on fewer training samples (PIRD). The embedding of a whole TCR sequence is obtained by average pooling over all amino acid representations. Embeddings from each model are used to train binding affinity prediction models, resulting in three sets of downstream models. The prediction performance of the downstream prediction models is compared to evaluate the architecture of <monospace>catELMo</monospace>.</p>
</sec>
<sec id="s4f3">
<label>4.6.3</label><title>Size of downstream data</title>
<p>We investigate how much downstream data <monospace>catELMo</monospace> can save in training a binding affinity prediction model while achieving the same performance with a model trained on a full size of data. We train the same model on different portion of <monospace>catELMo</monospace> embedding dataset. In detail, we randomly select k% of binding and k% of non-binding TCR-epitope pairs from training (and validation) data (k = 1, 2, ·· ·, 10, 20, ·· ·, 100), obtain <monospace>catELMo</monospace> embeddings for those, and feed them to train TCR-epitope binding affinity prediction models. Note that the TCR-epitope binding affinity prediction models in this experiment differ only in the number of training and validation pairs, meaning that the same testing set is used for different ks. We run ten times of experiments for each k and report their average and standard deviation of AUC, recall, precision, and F1 scores. We compare their performance to those trained on a full size of the other embedding datasets. For a more detailed investigation, we also perform the same experiment on BLOSUM62 embeddings and compare it with ours.</p>
</sec>
</sec>
</sec>
<sec id="d1e1738" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1857">
<label>Supplementary Materials</label>
<media xlink:href="supplements/536635_file02.pdf"/>
</supplementary-material>
</sec>
</body>
<back>
<sec id="s5">
<title>Data availability</title>
<p>Datasets used in downstream tasks and embedding models are described in detail in <xref rid="s4a" ref-type="sec">Section 4.1</xref>. Those datasets have been combined and summarized by us and are accessible at at <ext-link ext-link-type="uri" xlink:href="https://github.com/Lee-CBG/catELMo/tree/main/datasets">https://github.com/Lee-CBG/catELMo/tree/main/datasets</ext-link>.</p>
</sec>
<sec id="s6">
<title>Code availability</title>
<p>The implementation of catELMo as well as all of our trained embedding models are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Lee-CBG/catELMo">https://github.com/Lee-CBG/catELMo</ext-link>.</p>
</sec>
<sec id="s7">
<title>Authors’ contributions</title>
<p>HL conceived the idea for the overall project. PZ, SB and HL designed the method and wrote the manuscript. PZ carried out the experiments. MC collected data and participated in project discussions.</p>
</sec>
<sec id="s8">
<title>Ethics declarations</title>
<p>The authors declare no competing interests.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><surname>Attaf</surname> <given-names>M</given-names></string-name>, <string-name><surname>Legut</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cole</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Sewell</surname> <given-names>AK</given-names></string-name>. <article-title>The T cell antigen receptor: the Swiss army knife of the immune system</article-title>. <source>Clinical &amp; Experimental Immunology</source>. <year>2015</year>;<volume>181</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>18</lpage>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><surname>Davis</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Bjorkman</surname> <given-names>PJ</given-names></string-name>. <article-title>T-cell antigen receptor genes and T-cell recognition</article-title>. <source>Nature</source>. <year>1988</year>;<volume>334</volume>(6181):<fpage>395</fpage>-402.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><surname>Krogsgaard</surname> <given-names>M</given-names></string-name>, <string-name><surname>Davis</surname> <given-names>MM</given-names></string-name>. <article-title>How T cells ‘see’ antigen</article-title>. <source>Nature Immunology</source>. <year>2005</year>;<volume>6</volume>(<issue>3</issue>):<fpage>239</fpage>–<lpage>45</lpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><surname>Sbai</surname> <given-names>H</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>A</given-names></string-name>, <string-name><surname>DeGroot</surname> <given-names>A</given-names></string-name>. <article-title>Use of T cell epitopes for vaccine development</article-title>. <source>Current drug targets-Infectious disorders</source>. <year>2001</year>;<volume>1</volume>(<issue>3</issue>):<fpage>303</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><surname>Schumacher</surname> <given-names>TN</given-names></string-name>. <article-title>T-cell-receptor gene therapy</article-title>. <source>Nature Reviews Immunology</source>. <year>2002</year>;<volume>2</volume>(<issue>7</issue>):<fpage>512</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><surname>Sewell</surname> <given-names>AK</given-names></string-name>. <article-title>Why must T cells be cross-reactive?</article-title> <source>Nature Reviews Immunology</source>. <year>2012</year>;<volume>12</volume>(<issue>9</issue>):<fpage>669</fpage>–<lpage>77</lpage>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><surname>Springer</surname> <given-names>I</given-names></string-name>, <string-name><surname>Besser</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tickotsky-Moskovitz</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dvorkin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Louzoun</surname> <given-names>Y</given-names></string-name>. <article-title>Prediction of specific TCR-peptide binding from large dictionaries of TCR-peptide pairs</article-title>. <source>Frontiers in immunology</source>. <year>2020</year>:<volume>1803</volume>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><surname>Jokinen</surname> <given-names>E</given-names></string-name>, <string-name><surname>Huuhtanen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mustjoki</surname> <given-names>S</given-names></string-name>, <string-name><surname>Heinonen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lähdesmäki</surname> <given-names>H</given-names></string-name>. <article-title>Predicting recognition between T cell receptors and epitopes with TCRGP</article-title>. <source>PLoS computational biology</source>. <year>2021</year>;<volume>17</volume>(<issue>3</issue>):<fpage>e1008814</fpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><surname>Jurtz</surname> <given-names>VI</given-names></string-name>, <string-name><surname>Jessen</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Bentzen</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Jespersen</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Mahajan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vita</surname> <given-names>R</given-names></string-name>, <etal>et al.</etal> <article-title>NetTCR: sequence-based prediction of TCR binding to peptide-MHC complexes using convolutional neural networks</article-title>. <source>BioRxiv</source>. <year>2018</year>:<volume>433706</volume>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><surname>Cai</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>P</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>H</given-names></string-name>. <article-title>ATM-TCR: TCR-epitope binding affinity prediction using a multi-head self-attention model</article-title>. <source>Frontiers in immunology</source>. <year>2022</year>;<volume>13</volume>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><surname>Weber</surname> <given-names>A</given-names></string-name>, <string-name><surname>Born</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rodriguez Martínez</surname> <given-names>M</given-names></string-name>. <article-title>TITAN: T-cell receptor specificity prediction with bimodal attention networks</article-title>. <source>Bioinformatics</source>. <year>2021</year>;<volume>37</volume>:<fpage>i237</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><surname>Lu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Jiang</surname> <given-names>P</given-names></string-name>, <string-name><surname>Xiao</surname> <given-names>X</given-names></string-name>, <etal>et al.</etal> <article-title>Deep learning-based prediction of the T cell receptor–antigen binding specificity</article-title>. <source>Nature Machine Intelligence</source>. <year>2021</year>;<volume>3</volume>(<issue>10</issue>):<fpage>864</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><surname>Moris</surname> <given-names>P</given-names></string-name>, <string-name><surname>De Pauw</surname> <given-names>J</given-names></string-name>, <string-name><surname>Postovskaya</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gielis</surname> <given-names>S</given-names></string-name>, <string-name><surname>De Neuter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bittremieux</surname> <given-names>W</given-names></string-name>, <etal>et al.</etal> <article-title>Current challenges for unseen-epitope TCR interaction prediction and a new perspective derived from image classification</article-title>. <source>Briefings in Bioinformatics</source>. <year>2021</year>;<volume>22</volume>(<issue>4</issue>):bbaa318.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><surname>Brown</surname> <given-names>T</given-names></string-name>, <string-name><surname>Mann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ryder</surname> <given-names>N</given-names></string-name>, <string-name><surname>Subbiah</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Dhariwal</surname> <given-names>P</given-names></string-name>, <etal>et al.</etal> <article-title>Language models are few-shot learners</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2020</year>;<volume>33</volume>:<fpage>1877</fpage>–<lpage>901</lpage>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><surname>Raffel</surname> <given-names>C</given-names></string-name>, <string-name><surname>Shazeer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Roberts</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Narang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Matena</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>Exploring the limits of transfer learning with a unified text-to-text transformer</article-title>. <source>Journal of Machine Learning Research</source>. <year>2020</year>;<volume>21</volume>(<issue>140</issue>):<fpage>1</fpage>–<lpage>67</lpage>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><surname>Henikoff</surname> <given-names>S</given-names></string-name>, <string-name><surname>Henikoff</surname> <given-names>JG</given-names></string-name>. <article-title>Amino acid substitution matrices from protein blocks</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1992</year>;<volume>89</volume>(<issue>22</issue>):<fpage>10915</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><surname>Montemurro</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schuster</surname> <given-names>V</given-names></string-name>, <string-name><surname>Povlsen</surname> <given-names>HR</given-names></string-name>, <string-name><surname>Bentzen</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Jurtz</surname> <given-names>V</given-names></string-name>, <string-name><surname>Chronister</surname> <given-names>WD</given-names></string-name>, <etal>et al.</etal> <article-title>NetTCR-2.0 enables accurate prediction of TCR-peptide binding by using paired TCRα and <italic>β</italic> sequence data</article-title>. <source>Communications biology</source>. <year>2021</year>;<volume>4</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><surname>Peters</surname> <given-names>ME</given-names></string-name>, <string-name><surname>Neumann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Iyyer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gardner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Deep Contextualized Word Representations</article-title>. <source>Association for Computational Linguistics</source>; <year>2018</year>. p. <fpage>2227</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K</given-names></string-name>. <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source>Association for Computational Linguistics</source>; <year>2019</year>. p. <fpage>4171</fpage>–<lpage>86</lpage>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><surname>Mikolov</surname> <given-names>T</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Corrado</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dean</surname> <given-names>J</given-names></string-name>. <article-title>Efficient Estimation of Word Representations in Vector Space. International Conference on Learning Representations</article-title>, <source>Workshop Track Proceedings</source>; <year>2013</year>..</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><surname>Le</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Mikolov</surname> <given-names>T</given-names></string-name>. <article-title>Distributed representations of sentences and documents</article-title>. <source>PMLR. International conference on machine learning</source>; <year>2014</year>. p. <fpage>1188</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><surname>Apweiler</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bairoch</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CH</given-names></string-name>, <string-name><surname>Barker</surname> <given-names>WC</given-names></string-name>, <string-name><surname>Boeckmann</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ferro</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal> <article-title>UniProt: the universal protein knowledgebase</article-title>. <source>Nucleic acids research</source>. <year>2004</year>;<volume>32</volume>:<fpage>D115</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><surname>Nolan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vignali</surname> <given-names>M</given-names></string-name>, <string-name><surname>Klinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dines</surname> <given-names>JN</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>IM</given-names></string-name>, <string-name><surname>Svejnoha</surname> <given-names>E</given-names></string-name>, <etal>et al.</etal> <article-title>A large-scale database of T-cell receptor beta (TCR<italic>β</italic>) sequences and binding associations from natural and synthetic exposure to SARS-CoV-2</article-title>. <source>Research square</source>. <year>2020</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><surname>Asgari</surname> <given-names>E</given-names></string-name>, <string-name><surname>Mofrad</surname> <given-names>MR</given-names></string-name>. <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>. <source>PloS one</source>. <year>2015</year>;<volume>10</volume>(<issue>11</issue>):<fpage>e0141287</fpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Bedbrook</surname> <given-names>CN</given-names></string-name>, <string-name><surname>Arnold</surname> <given-names>FH</given-names></string-name>. <article-title>Learned protein embeddings for machine learning</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>15</issue>):<fpage>2642</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><surname>Heinzinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Elnaggar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Dallago</surname> <given-names>C</given-names></string-name>, <string-name><surname>Nechaev</surname> <given-names>D</given-names></string-name>, <string-name><surname>Matthes</surname> <given-names>F</given-names></string-name>, <etal>et al.</etal> <article-title>Modeling aspects of the language of life through transfer-learning protein sequences</article-title>. <source>BMC bioinformatics</source>. <year>2019</year>;<volume>20</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><surname>Elnaggar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Heinzinger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dallago</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rehawi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <etal>et al.</etal> <article-title>ProtTrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2021</year>:<fpage>1</fpage>–<lpage>1</lpage>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="confproc"><string-name><surname>Lan</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gimpel</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>P</given-names></string-name>, <string-name><surname>Soricut</surname> <given-names>R</given-names></string-name>. <article-title>ALBERT: A lite BERT for self-supervised learning of language representations</article-title>. <conf-name>8th International Conference on Learning Representations, ICLR 2020</conf-name>, Addis Ababa, Ethiopia, April 26-30, 2020; <year>2020</year>..</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="other"><string-name><surname>Wu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Yost</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Daniel</surname> <given-names>B</given-names></string-name>, <string-name><surname>Belk</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Xia</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Egawa</surname> <given-names>T</given-names></string-name>, <etal>et al.</etal> <article-title>TCR-BERT: learning the grammar of T-cell receptors for flexible antigen-xbinding analyses</article-title>. <source>bioRxiv</source>. <year>2021</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><surname>Tickotsky</surname> <given-names>N</given-names></string-name>, <string-name><surname>Sagiv</surname> <given-names>T</given-names></string-name>, <string-name><surname>Prilusky</surname> <given-names>J</given-names></string-name>, <string-name><surname>Shifrut</surname> <given-names>E</given-names></string-name>, <string-name><surname>Friedman</surname> <given-names>N</given-names></string-name>. <article-title>McPAS-TCR: a manually curated catalogue of pathology-associated T cell receptor sequences</article-title>. <source>Bioinformatics</source>. <year>2017</year>;<volume>33</volume>(<issue>18</issue>):<fpage>2924</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><surname>Ward</surname> Jr <given-names>JH</given-names></string-name>. <article-title>Hierarchical grouping to optimize an objective function</article-title>. <source>Journal of the American statistical association</source>. <year>1963</year>;<volume>58</volume>(<issue>301</issue>):<fpage>236</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><surname>Strehl</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ghosh</surname> <given-names>J</given-names></string-name>. <article-title>Cluster Ensembles—A knowledge reuse framework for combining multiple partitions</article-title>. <source>J Mach Learn Res</source>. <year>2002</year>;<volume>3</volume>(<issue>Dec</issue>):<fpage>583</fpage>–<lpage>617</lpage>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><surname>Shugay</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bagaev</surname> <given-names>DV</given-names></string-name>, <string-name><surname>Zvyagin</surname> <given-names>IV</given-names></string-name>, <string-name><surname>Vroomans</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Crawford</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Dolton</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal> <article-title>VDJdb: a curated database of T-cell receptor sequences with known antigen specificity</article-title>. <source>Nucleic acids research</source>. <year>2018</year>;<volume>46</volume>(<issue>D1</issue>):<fpage>D419</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><surname>Vita</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mahajan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Overton</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Dhanda</surname> <given-names>SK</given-names></string-name>, <string-name><surname>Martini</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cantrell</surname> <given-names>JR</given-names></string-name>, <etal>et al.</etal> <article-title>The immune epitope database (IEDB): 2018 update</article-title>. <source>Nucleic acids research</source>. <year>2019</year>;<volume>47</volume>(<issue>D1</issue>):<fpage>D339</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><surname>Van der Maaten</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>G</given-names></string-name>. <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of machine learning research</source>. <year>2008</year>;<volume>9</volume>(<issue>11</issue>).</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><surname>Purtic</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pitcher</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Van Oers</surname> <given-names>NS</given-names></string-name>, <string-name><surname>Wülfing</surname> <given-names>C</given-names></string-name>. <article-title>T cell receptor (TCR) clustering in the immunological synapse integrates TCR and costimulatory signaling in selected T cells</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2005</year>;<volume>102</volume>(<issue>8</issue>):<fpage>2904</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><surname>Dash</surname> <given-names>P</given-names></string-name>, <string-name><surname>Fiore-Gartland</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Hertz</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>S</given-names></string-name>, <string-name><surname>Souquette</surname> <given-names>A</given-names></string-name>, <etal>et al.</etal> <article-title>Quantifiable predictive features define epitope-specific T cell receptor repertoires</article-title>. <source>Nature</source>. <year>2017</year>;<volume>547</volume>(7661):<fpage>89</fpage>-93.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Zhan</surname> <given-names>X</given-names></string-name>, <string-name><surname>Li</surname> <given-names>B</given-names></string-name>. <article-title>GIANA allows computationally-efficient TCR clustering and multi-disease repertoire classification by isometric transformation</article-title>. <source>Nature communications</source>. <year>2021</year>;<volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><surname>Gielis</surname> <given-names>S</given-names></string-name>, <string-name><surname>Moris</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bittremieux</surname> <given-names>W</given-names></string-name>, <string-name><surname>De Neuter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ogunjimi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Laukens</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Detection of enriched T cell epitope specificity in full T cell receptor sequence repertoires</article-title>. <source>Frontiers in immunology</source>. <year>2019</year>;<volume>10</volume>:<fpage>2820</fpage>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><string-name><surname>Lythe</surname> <given-names>G</given-names></string-name>, <string-name><surname>Callard</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Hoare</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Molina-París</surname> <given-names>C</given-names></string-name>. <article-title>How many TCR clonotypes does a body maintain?</article-title> <source>Journal of theoretical biology</source>. <year>2016</year>;<volume>389</volume>:<fpage>214</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><string-name><surname>Yosinski</surname> <given-names>J</given-names></string-name>, <string-name><surname>Clune</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bengio</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lipson</surname> <given-names>H</given-names></string-name>. <article-title>How transferable are features in deep neural networks?</article-title> <source>Advances in Neural Information Processing Systems</source>. <year>2014</year>;<volume>27</volume>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><string-name><surname>Rossjohn</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gras</surname> <given-names>S</given-names></string-name>, <string-name><surname>Miles</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Turner</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Godfrey</surname> <given-names>DI</given-names></string-name>, <string-name><surname>McCluskey</surname> <given-names>J</given-names></string-name>. <article-title>T cell antigen receptor recognition of antigen-presenting molecules</article-title>. <source>Annual review of immunology</source>. <year>2015</year>;<volume>33</volume>:<fpage>169</fpage>–<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><surname>Feng</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bond</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Ely</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Maynard</surname> <given-names>J</given-names></string-name>, <string-name><surname>Garcia</surname> <given-names>KC</given-names></string-name>. <article-title>Structural evidence for a germline-encoded T cell receptor–major histocompatibility complex interaction’codon’</article-title>. <source>Nature immunology</source>. <year>2007</year>;<volume>8</volume>(<issue>9</issue>):<fpage>975</fpage>–<lpage>83</lpage>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><string-name><surname>Springer</surname> <given-names>I</given-names></string-name>, <string-name><surname>Tickotsky</surname> <given-names>N</given-names></string-name>, <string-name><surname>Louzoun</surname> <given-names>Y</given-names></string-name>. <article-title>Contribution of T cell receptor alpha and beta CDR3, MHC typing, V and J genes to peptide binding prediction</article-title>. <source>Frontiers in immunology</source>. <year>2021</year>;<volume>12</volume>:<issue>664514</issue>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><string-name><surname>Chan</surname> <given-names>HY</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Garliss</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Kwaa</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Blankson</surname> <given-names>JN</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>KN</given-names></string-name>. <article-title>T cell receptor sequencing-based assay identifies cross-reactive recall CD8+ T cell clonotypes against autologous HIV-1 epitope variants</article-title>. <source>Frontiers in immunology</source>. <year>2020</year>;<volume>11</volume>:<issue>591</issue>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><string-name><surname>Gil</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kamga</surname> <given-names>L</given-names></string-name>, <string-name><surname>Chirravuri-Venkata</surname> <given-names>R</given-names></string-name>, <string-name><surname>Aslan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>F</given-names></string-name>, <string-name><surname>Ghersi</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>Epstein-barr virus epitope– major histocompatibility complex interaction combined with convergent recombination drives selection of diverse t cell receptor α and <italic>β</italic> repertoires</article-title>. <source>MBio</source>. <year>2020</year>;<volume>11</volume>(<issue>2</issue>):<fpage>e00250</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><string-name><surname>De Neuter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bartholomeus</surname> <given-names>E</given-names></string-name>, <string-name><surname>Elias</surname> <given-names>G</given-names></string-name>, <string-name><surname>Keersmaekers</surname> <given-names>N</given-names></string-name>, <string-name><surname>Suls</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jansens</surname> <given-names>H</given-names></string-name>, <etal>et al.</etal> <article-title>Memory CD4+ T cell receptor repertoire data mining as a tool for identifying cytomegalovirus serostatus</article-title>. <source>Genes &amp; Immunity</source>. <year>2019</year>;<volume>20</volume>(<issue>3</issue>):<fpage>255</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><surname>Herati</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Muselman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vella</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bengsch</surname> <given-names>B</given-names></string-name>, <string-name><surname>Parkhouse</surname> <given-names>K</given-names></string-name>, <string-name><surname>Del Alcazar</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>Successive annual influenza vaccination induces a recurrent oligoclonotypic memory response in circulating T follicular helper cells</article-title>. <source>Science immunology</source>. <year>2017</year>;<volume>2</volume>(<issue>8</issue>):eaag2152.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><string-name><surname>DeWitt</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Wilburn</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Sherwood</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vignali</surname> <given-names>M</given-names></string-name>, <string-name><surname>Day</surname> <given-names>CL</given-names></string-name>, <etal>et al.</etal> <article-title>A diverse lipid antigen-specific TCR repertoire is clonally expanded during active tuberculosis</article-title>. <source>The Journal of Immunology</source>. <year>2018</year>;<volume>201</volume>(<issue>3</issue>):<fpage>888</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><surname>Rajamanickam</surname> <given-names>V</given-names></string-name>, <string-name><surname>Ballesteros-Merino</surname> <given-names>C</given-names></string-name>, <string-name><surname>Samson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ross</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bernard</surname> <given-names>B</given-names></string-name>, <string-name><surname>Fox</surname> <given-names>BA</given-names></string-name>, <etal>et al..</etal> <source>Treatment-induced immune cell priming as a potential explanation for an outstanding anti-tumor response in a patient with metastatic colorectal cancer</source>; <year>2021</year>. Available from:</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="other"><string-name><surname>Kimothi</surname> <given-names>D</given-names></string-name>, <string-name><surname>Soni</surname> <given-names>A</given-names></string-name>, <string-name><surname>Biyani</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hogan</surname> <given-names>JM</given-names></string-name>. <article-title>Distributed Representations for Biological Sequence Analysis</article-title>. <source>CoRR</source>. <year>2016</year>;abs/1608.05949.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><string-name><surname>Suzek</surname> <given-names>BE</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>H</given-names></string-name>, <string-name><surname>McGarvey</surname> <given-names>P</given-names></string-name>, <string-name><surname>Mazumder</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CH</given-names></string-name>. <article-title>UniRef: comprehensive and non-redundant UniProt reference clusters</article-title>. <source>Bioinformatics</source>. <year>2007</year>;<volume>23</volume>(<issue>10</issue>):<fpage>1282</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><string-name><surname>Vaswani</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname> <given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname> <given-names>N</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gomez</surname> <given-names>AN</given-names></string-name>, <etal>et al.</etal> <article-title>Attention is all you need</article-title>. <source>Advances in neural information processing systems</source>. <year>2017</year>;<volume>30</volume>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><string-name><surname>Brandes</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ofer</surname> <given-names>D</given-names></string-name>, <string-name><surname>Peleg</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Rappoport</surname> <given-names>N</given-names></string-name>, <string-name><surname>Linial</surname> <given-names>M</given-names></string-name>. <article-title>ProteinBERT: A universal deep-learning model of protein sequence and function</article-title>. <source>Bioinformatics</source>. <year>2022</year>;<volume>38</volume>(<issue>8</issue>):<fpage>2102</fpage>–<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>W</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wei</surname> <given-names>X</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>K</given-names></string-name>, <string-name><surname>Du</surname> <given-names>W</given-names></string-name>, <etal>et al.</etal> <article-title>PIRD: pan immune repertoire database</article-title>. <source>Bioinformatics</source>. <year>2020</year>;<volume>36</volume>(<issue>3</issue>):<fpage>897</fpage>–<lpage>903</lpage>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="confproc"><string-name><surname>Kim</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Jernite</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Sontag</surname> <given-names>D</given-names></string-name>, <string-name><surname>Rush</surname> <given-names>AM</given-names></string-name>. <article-title>Character-aware neural language models</article-title>. In: <conf-name>Thirtieth AAAI conference on artificial intelligence</conf-name>; <year>2016</year>..</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><string-name><surname>Hochreiter</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schmidhuber</surname> <given-names>J</given-names></string-name>. <article-title>Long short-term memory</article-title>. <source>Neural computation</source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><string-name><surname>Elfwing</surname> <given-names>S</given-names></string-name>, <string-name><surname>Uchibe</surname> <given-names>E</given-names></string-name>, <string-name><surname>Doya</surname> <given-names>K</given-names></string-name>. <article-title>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</article-title>. <source>Neural Networks</source>. <year>2018</year>;<volume>107</volume>:<fpage>3</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><string-name><surname>Ioffe</surname> <given-names>S</given-names></string-name>, <string-name><surname>Szegedy</surname> <given-names>C</given-names></string-name>. <article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: International conference on machine learning</article-title>. <source>PMLR</source>; <year>2015</year>. p. <fpage>448</fpage>–<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="journal"><string-name><surname>Srivastava</surname> <given-names>N</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>G</given-names></string-name>, <string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>, <string-name><surname>Salakhutdinov</surname> <given-names>R</given-names></string-name>. <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>The journal of machine learning research</source>. <year>2014</year>;<volume>15</volume>(<issue>1</issue>):<fpage>1929</fpage>–<lpage>58</lpage>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Kingma</surname> <given-names>DP</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>J</given-names></string-name>. <article-title>Adam: A method for stochastic optimization</article-title>. <source>CoRR</source>. <year>2014</year>;abs/1412.6980.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><string-name><surname>Lefranc</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Pommié</surname> <given-names>C</given-names></string-name>, <string-name><surname>Kaas</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Duprat</surname> <given-names>E</given-names></string-name>, <string-name><surname>Bosc</surname> <given-names>N</given-names></string-name>, <string-name><surname>Guiraudou</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>IMGT unique numbering for immunoglobulin and T cell receptor constant domains and Ig superfamily C-like domains</article-title>. <source>Developmental &amp; Comparative Immunology</source>. <year>2005</year>;<volume>29</volume>(<issue>3</issue>):<fpage>185</fpage>–<lpage>203</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhao</surname>
<given-names>Siming</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Dartmouth College</institution>
</institution-wrap>
<city>Lebanon</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study provides an <bold>important</bold> tool for predicting binding between immune cells receptors and antigens based on protein sequence data. The analysis <bold>convincingly</bold> showed the tool's effectiveness in both supervised TCR binding prediction and unsupervised clustering, surpassing existing methods in accuracy and reducing annotation costs. This study will be of interest to immunologists and computational biologists.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this manuscript, the authors described a computational method catELMo for embedding TCR CDR3 sequences into numeric vectors using a deep-learning-based approach, ELMo. The authors applied catELMo to two applications: supervised TCR-epitope binding affinity prediction and unsupervised epitope-specific TCR clustering. In both applications, the authors showed that catELMo generated significantly better binding prediction and clustering performance than other established TCR embedding methods.</p>
<p>The authors have addressed all of my concerns except for one as following:</p>
<p>(5) GIANA's result is like</p>
<p>– ## TIME:2020-12-14 14:45:14|cmd: GIANA4.py|COVID_test/rawData/hc10s10.txt|IsometricDistance_Thr=7.0|thr_v=3.7|thr_s=3.3|exact=True|Vgene=True|ST=3</p>
<p>– ## Column Info: CDR3 aa sequence, cluster id, other information in the input file</p>
<p>
CAISDGTAASSTDTQYF 1 TRBV10-3*01 6.00384245917387e-05 0.930103216755186 COVID19:BS-EQ-0002-T1-replacement_TCRB.tsv</p>
<p>
CAISDGTAASSTDTQYF 1 TRBV10-3*01 4.34559031223066e-05 0.918135389545364 COVID19:BS-EQ-0002-T2-replacement_TCRB.tsv</p>
<p>
CANATLLQVLSTDTQYF 2 TRBV21-1*01 3.00192122958694e-05 0.878695260046097 COVID19:BS-EQ-0002-T1-replacement_TCRB.tsv</p>
<p>
CANATLLQVLSTDTQYF 2 TRBV21-1*01 1.44853010407689e-05 0.768125375525736 COVID19:BS-EQ-0002-T2-replacement_TCRB.ts</p>
<p>
...</p>
<p>as in its example file at: <ext-link ext-link-type="uri" xlink:href="https://raw.githubusercontent.com/s175573/GIANA/master/data/hc10s10--RotationEncodingBL62.txt">https://raw.githubusercontent.com/s175573/GIANA/master/data/hc10s10--RotationEncodingBL62.txt</ext-link></p>
<p>The results directly give the clustering results in the second column, and there is no direct distance metric for hierarchical clustering. Therefore, it is still not clear how the authors conducted the hierarchical clustering on GIANA's results. Did the hierarchical clustering apply to each of the original clusters on the CDR3 distances within the same original cluster?</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In the manuscript, the authors highlighted the importance of T-cell receptor (TCR) analysis and the lack of amino acid embedding methods specific to this domain. The authors proposed a novel bi-directional context-aware amino acid embedding method, catELMo, adapted from ELMo (Embeddings from Language Models), specifically designed for TCR analysis. The model is trained on TCR sequences from seven projects in the ImmunoSEQ database, instead of the generic protein sequences. They assessed the effectiveness of the proposed method in both TCR-epitope binding affinity prediction, a supervised task, and the unsupervised TCR clustering task. The results demonstrate significant performance improvements compared to existing embedding models. The authors also aimed to provide and discuss their observations on embedding model design for TCR analysis: 1) Models specifically trained on TCR sequences have better performance than models trained on general protein sequences for the TCR-related tasks; and 2) The proposed ELMo-based method outperforms TCR embedding models with BERT-based architecture. The authors also provided a comprehensive introduction and investigation of existing amino acid embedding methods. Overall, the paper is well-written and well-organized.</p>
<p>The work has originality and has potential prospects for immune response analysis and immunotherapy exploration. TCR-epitope pair binding plays a significant role in T cell regulation. Accurate prediction and analysis of TCR sequences are crucial for comprehending the biological foundations of binding mechanisms and advancing immunotherapy approaches. The proposed embedding method presents an efficient context-aware mathematical representation for TCR sequences, enabling the capture and analysis of their structural and functional characteristics. This method serves as a valuable tool for various downstream analyses and is essential for a wide range of applications.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.2.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>In this study, Zhang and colleagues proposed an ELMo-based embedding model (catELMo) for TCRβ CDR3 amino acid sequences. They showed the effectiveness of catELMo in both supervised TCR binding prediction and unsupervised clustering, surpassing existing methods in accuracy and reducing annotation costs. The study provides insights on the effect of model architectures to TCR specificity prediction and clustering tasks.</p>
<p>The authors have addressed our prior critiques of the manuscript.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.88837.2.sa4</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Pengfei</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0009-5976-0673</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Bang</surname>
<given-names>Seojin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cai</surname>
<given-names>Michael</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lee</surname>
<given-names>Heewook</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3528-6833</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>Thank you very much for the careful and positive reviews of our manuscript. We have addressed each comment in the attached revised manuscript. We describe the modifications below. To avoid confusion, we've changed supplementary figure and table captions to start with &quot;Supplement Figure&quot; and &quot;Supplementary Table,&quot; instead of &quot;Figure&quot; and &quot;Table.&quot;</p>
<p>We have modified/added:</p>
<p>●   Supplementary Table S1: AUC scores for the top 10 frequent epitope types (pathogens) in the testing set of epitope split.</p>
<p>●   Supplementary Table S5: AUCs of TCR-epitope binding affinity prediction models with BLOSUM62 to embed epitope sequences.</p>
<p>●   Supplementary Table S6: AUCs of TCR-epitope binding affinity prediction models trained on catELMo TCR embeddings and random-initialized epitope embeddings.</p>
<p>●   Supplementary Table S7: AUCs of TCR-epitope binding affinity prediction models trained on catELMo and BLOSUM62 embeddings.</p>
<p>●   Supplementary Figure 4: TCR clustering performance for the top 34 abundant epitopes representing 70.55% of TCRs in our collected databases.</p>
<p>●   Section Discussion.</p>
<p>●   Section 4.1 Data: TCR-epitope pairs for binding affinity prediction.</p>
<p>●   Section 4.4.2 Epitope-specific TCR clustering.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>In this manuscript, the authors described a computational method catELMo for embedding TCR CDR3 sequences into numeric vectors using a deep-learning-based approach, ELMo. The authors applied catELMo to two applications: supervised TCR-epitope binding affinity prediction and unsupervised epitope-specific TCR clustering. In both applications, the authors showed that catELMo generated significantly better binding prediction and clustering performance than other established TCR embedding methods. However, there are a few major concerns that need to be addressed.</p>
<p>(1) There are other TCR CDR3 embedding methods in addition to TCRBert. The authors may consider incorporating a few more methods in the evaluation, such as TESSA (PMCID: PMC7799492), DeepTCR (PMCID: PMC7952906) and the embedding method in ATM-TCR (reference 10 in the manuscript). TESSA is also the embedding method in pMTnet, which is another TCR-epitope binding prediction method and is the reference 12 mentioned in this manuscript.</p>
</disp-quote>
<p>TESSA is designed for characterizing TCR repertoires, so we initially excluded it from the comparison. Our focus was on models developed specifically for amino acid embedding rather than TCR repertoire characterization. However, to address the reviewer's inquiry, we conducted further evaluations. Since both TESSA and DeepTCR used autoencoder-based models to embed TCR sequences, we selected one used in TESSA for evaluation in our downstream prediction task, conducting ten trials in total. It achieved an average AUC of 75.69 in TCR split and 73.3 in epitope split. Notably, catELMo significantly outperformed such performance with an AUC of 96.04 in TCR split and 94.10 in epitope split.</p>
<p>Regarding the embedding method in ATM-TCR, it simply uses BLOSUM as an embedding matrix which we have already compared in Section 2.1. Furthermore, we have provided the comparison results between our prediction model trained on catELMo embeddings with the state-of-the-art prediction models such as netTCR and ATM-TCR in Table 6 of the Discussion section.</p>
<disp-quote content-type="editor-comment">
<p>(2) The TCR training data for catELMo is obtained from ImmunoSEQ platform, including SARS-CoV2, EBV, CMV, and other disease samples. Meanwhile, antigens related to these diseases and their associated TCRs are extensively annotated in databases VDJdb, IEDB and McPAS-TCR. The authors then utilized the curated TCR-epitope pairs from these databases to conduct the evaluations for eptitope binding prediction and TCR clustering. Therefore, the training data for TCR embedding may already be implicitly tuned for better representations of the TCRs used in the evaluations. This seems to be true based on Table 4, as BERT-Base-TCR outperformed TCRBert. Could catELMo be trained on PIRD as TCRBert to demonstrate catELMo's embedding for TCRs targeting unseen diseases/epitopes?</p>
</disp-quote>
<p>We would like to note that catELMo was trained exclusively on TCR sequences in an unsupervised manner, which means it has never been exposed to antigen information. We also ensured that the TCRs used in catELMo's training did not overlap with our downstream prediction data. Please refer to the section 4.1 Data where we explicitly stated, “We note that it includes no identical TCR sequences with the TCRs used for training the embedding models.”. Moreover, the performance gap (~1%) between BERT-Base-TCR and TCRBert, as observed in Table 4, is relatively small, especially when compared to the performance difference (&gt;16%) between catELMo and TCRBert.</p>
<p>To further address this concern, we conducted experiments using the same number of TCRs, 4,173,895 in total, sourced exclusively from healthy ImmunoSeq repertoires. This alternative catELMo model demonstrated a similar prediction performance (based on 10 trials) to the one reported in our paper, with an average AUC of 96.35% in TCR split and an average AUC of 94.03% in epitope split.</p>
<p>We opted not to train catELMo on the PIRD dataset for several reasons. First, approximately 7.8% of the sequences in PIRD also appear in our downstream prediction data, which could be a potential source of bias. Furthermore, PIRD encompasses sequences related to diseases such as Tuberculosis, HIV, CMV, among others, which the reviewer is concerned about.</p>
<disp-quote content-type="editor-comment">
<p>(3) In the application of TCR-epitope binding prediction, the authors mentioned that the model for embedding epitope sequences was catElMo, but how about for other methods, such as TCRBert? Do the other methods also use catELMo-embedded epitope sequences as part of the binding prediction model, or use their own model to embed the epitope sequences? Since the manuscript focuses on TCR embedding, it would be nice for other methods to be evaluated on the same epitope embedding (maybe adjusted to the same embedded vector length).</p>
<p>Furthermore, the authors found that catELMo requires less training data to achieve better performance. So one would think the other methods could not learn a reasonable epitope embedding with limited epitope data, and catELMo's better performance in binding prediction is mainly due to better epitope representation.</p>
</disp-quote>
<p>Review 1 and 3 have raised similar concerns regarding the epitope embedding approach employed in our binding affinity prediction models. We address both comments together on page 6 where we discuss the epitope embedding strategies in detail.</p>
<disp-quote content-type="editor-comment">
<p>(4) In the epitope binding prediction evaluation, the authors generated the test data using TCR-epitope pairs from VDJdb, IEDB, McPAS, which may be dominated by epitopes from CMV. Could the authors show accuracy categorized by epitope types, i.e. the accuracy for TCR-CMV pair and accuracy for TCR-SARs-CoV2 separately?</p>
</disp-quote>
<p>The categorized AUC scores have been added in Supplementary Table 7. We observed significant performance boosts from catELMo compared with other embedding models.</p>
<disp-quote content-type="editor-comment">
<p>(5) In the unsupervised TCR clustering evaluation, since GIANA and TCRdist direct outputs the clustering result, so they should not be affected by hierarchical clusters. Why did the curves of GIANA and TCRdist change in Figure 4 when relaxing the hierarchical clustering threshold?</p>
</disp-quote>
<p>For fair comparisons, we performed GIANA and TCRdist with hierarchical clustering instead of the nearest neighbor search. We have clarified it in the revised manuscript as follows.</p>
<p>“Both methods are developed on the BLOSUM62 matrix and apply nearest neighbor search to cluster TCR sequences. GIANA used the CDR3 of TCRβ chain and V gene, while TCRdist predominantly experimented with CDR1, CDR2, and CDR3 from both TCRα and TCRβ chains. For fair comparisons, we perform GIANA and TCRdist only on CDR3 β chains and with hierarchical clustering instead of the nearest neighbor search.”</p>
<disp-quote content-type="editor-comment">
<p>(6 &amp; 7) In the unsupervised TCR clustering evaluation, the authors examined the TCR related to the top eight epitopes. However, there are much more epitopes curated in VDJdb, IEDB and McPAS-TCR. In real application, the potential epitopes is also more complex than just eight epitopes. Could the authors evaluate the clustering result using all the TCR data from the databases? In addition to NMI, it is important to know how specific each TCR cluster is. Could the authors add the fraction of pure clusters in the results? Pure cluster means all the TCRs in the cluster are binding to the same epitope, and is a metric used in the method GIANA.</p>
</disp-quote>
<p>We would like to note that there is a significant disparity in TCR binding frequencies across different epitopes in current databases. For instance, the most abundant epitope (KLGGALQAK) has approximately 13k TCRs binding to it, while 836 out of 982 epitopes are associated with fewer than 100 TCRs in our dataset. Furthermore, there are 9347 TCRs having the ability to bind multiple epitopes. In order to robustly evaluate the clustering performance, we originally selected the top eight frequent epitopes from McPAS and removed TCRs binding multiple epitopes to create a more balanced dataset.</p>
<p>We acknowledge that the real-world scenario is more complex than just eight epitopes. Therefore, we conducted clustering experiments using the top most abundant epitopes whose combined cognate TCRs make up at least 70% of TCRs across three databases (34 epitopes). This is illustrated in Supplementary Figure 5. Furthermore, we extended our analysis by clustering all TCRs after filtering out those that bind to multiple epitopes, resulting in 782 unique epitopes. We found that catELMo achieved the 3rd and 2nd best performance in NMI and Purity, respectively (see Table below). These are aligned with our previous observations of the eight epitopes.</p>
<table-wrap id="sa4table1">
<label>Author response table 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-88837-sa4-table1.jpg" mimetype="image"/>
</table-wrap>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>In the manuscript, the authors highlighted the importance of T-cell receptor (TCR) analysis and the lack of amino acid embedding methods specific to this domain. The authors proposed a novel bi-directional context-aware amino acid embedding method, catELMo, adapted from ELMo (Embeddings from Language Models), specifically designed for TCR analysis. The model is trained on TCR sequences from seven projects in the ImmunoSEQ database, instead of the generic protein sequences. They assessed the effectiveness of the proposed method in both TCR-epitope binding affinity prediction, a supervised task, and the unsupervised TCR clustering task. The results demonstrate significant performance improvements compared to existing embedding models. The authors also aimed to provide and discuss their observations on embedding model design for TCR analysis: 1) Models specifically trained on TCR sequences have better performance than models trained on general protein sequences for the TCR-related tasks; and 2) The proposed ELMo-based method outperforms TCR embedding models with BERT-based architecture. The authors also provided a comprehensive introduction and investigation of existing amino acid embedding methods. Overall, the paper is well-written and well-organized.</p>
<p>The work has originality and has potential prospects for immune response analysis and immunotherapy exploration. TCR-epitope pair binding plays a significant role in T cell regulation. Accurate prediction and analysis of TCR sequences are crucial for comprehending the biological foundations of binding mechanisms and advancing immunotherapy approaches. The proposed embedding method presents an efficient context-aware mathematical representation for TCR sequences, enabling the capture and analysis of their structural and functional characteristics. This method serves as a valuable tool for various downstream analyses and is essential for a wide range of applications.
Thank you.</p>
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Here, the authors trained catElMo, a new context-aware embedding model for TCRβ CDR3 amino acid sequences for TCR-epitope specificity and clustering tasks. This method benchmarked existing work in protein and TCR language models and investigated the role that model architecture plays in the prediction performance. The major strength of this paper is comprehensively evaluating common model architectures used, which is useful for practitioners in the field. However, some key details were missing to assess whether the benchmarking study is a fair comparison between different architectures. Major comments are as follows:</p>
<list list-type="bullet">
<list-item><p>It is not clear why epitope sequences were also embedded using catELMo for the binding prediction task. Because catELMO is trained on TCRβ CDR3 sequences, it's not clear what benefit would come from this embedding. Were the other embedding models under comparison also applied to both the TCR and epitope sequences? It may be a fairer comparison if a single method is used to encode epitope sequence for all models under comparison, so that the performance reflects the quality of the TCR embedding only.</p>
</list-item></list>
</disp-quote>
<p>In our study, we indeed used the same embedding model for both TCRs and epitopes in each prediction model, ensuring a consistent approach throughout.</p>
<p>Recognizing the importance of evaluating the impact of epitope embeddings, we conducted experiments in which we used BLOSUM62 matrix to embed epitope sequences for all models. The results (Supplementary Table 5) are well aligned with the performance reported in our paper. This suggests that epitope embedding may not play as critical a role as TCR embedding in the prediction tasks. To further validate this point, we conducted two additional experiments.</p>
<p>Firstly, we used catELMo to embed TCRs while employing randomly initialized embedding matrices with trainable parameters for epitope sequences. It yielded similar prediction performance as when catELMo was used for both TCR and epitope embedding (Supplementary Table 6). Secondly, we utilized BLOSUM62 to embed TCRs but employed catELMo for epitope sequence embedding, resulting in performance comparable to using BLOSUM62 for both TCRs and epitopes (Supplementary Table 4). These experiment results confirmed the limited impact of epitope embedding on downstream performance.</p>
<p>We conjecture that these results may be attributed to the significant disparity in data scale between TCRs (~290k) and epitopes (less than 1k). Moreover, TCRs tend to exhibit high similarity, whereas epitopes display greater distinctiveness from one another. These features of TCRs require robust embeddings to facilitate effective separation and improve downstream performance, while epitope embedding primarily serves as a categorical encoding.</p>
<p>We have included a detailed discussion of these findings in the revised manuscript to provide a comprehensive understanding of the role of epitope embeddings in TCR binding prediction.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>The tSNE visualization in Figure 3 is helpful. It makes sense that the last hidden layer features separate well by binding labels for the better performing models. However, it would be useful to know if positive and negative TCRs for each epitope group also separate well in the original TCR embedding space. In other words, how much separation between these groups is due to the neural network vs just the embedding?</p>
</list-item></list>
</disp-quote>
<p>It is important to note that we used the same downstream prediction model, a simple three-linear-layer network, for all the discussed embedding methods. We believe that the separation observed in the t-SNE visualization effectively reflects the ability of our embedding model. Also, we would like to mention that it can be hard to see a clear distinction between positive and negative TCRs in the original embedding space because embedding models were not trained on positive/negative labels. Please refer to the t-SNE of the original TCR embeddings below.</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<graphic mime-subtype="jpg" xlink:href="elife-88837-sa4-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>To generate negative samples, the author randomly paired TCRs from healthy subjects to different epitopes. This could produce issues with false negatives if the epitopes used are common. Is there an estimate for how frequently there might be false negatives for those commonly occurring epitopes that most populations might also have been exposed to? Could there be a potential batch effect for the negative sampled TCR that confounds with the performance evaluation?</p>
</list-item></list>
</disp-quote>
<p>Thank you for bringing this valid and interesting point up. Generating negative samples is non-trivial since only a limited number of non-binding TCR-pairs are publicly available and experimentally validating non-binding pairs is costly [1]. Standard practices for generating negative pairs are (1) paring epitopes with healthy TCRs [2, 3], and (2) randomly shuffling existing TCR-epitope pairs [4,5]. We used both approaches (the former included in the main results, and the latter in the discussion). In both scenarios, catELMo embeddings consistently demonstrated superior performance.</p>
<p>We acknowledge the possibility of false negatives due to the finite-sized TCR database from which we randomly selected TCRs, however, we believe that the likelihood of such occurrences is low. Given the vast diversity of human TCR clonotypes, which can exceed 10^15[6], the chance of randomly selecting a TCR that specifically recognizes a target epitope is relatively small.</p>
<p>In order to investigate the batch effect, we generated new negative pairs using different seeds and observed consistent prediction performance across these variations. However, we agree that there could still be a potential batch effect for the negative samples due to potential data bias.</p>
<p>We have discussed the limitation of generative negative samples in the revised manuscript.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Most of the models being compared were trained on general proteins rather than TCR sequences. This makes their comparison to catELMO questionable since it's not clear if the improvement is due to the training data or architecture. The authors partially addressed this with BERT-based models in section 2.4. This concern would be more fully addressed if the authors also trained the Doc2vec model (Yang et al, Figure 2) on TCR sequences as baseline models instead of using the original models trained on general protein sequences. This would make clear the strength of context-aware embeddings if the performance is worse than catElmo and BERT.</p>
</list-item></list>
</disp-quote>
<p>We agree it is important to distinguish between the effects of training data and architecture on model performance.</p>
<p>In Section 2.4, as the reviewer mentioned, we compared catELMo with BERT-based models trained on the same TCR repertoire data, demonstrating that architecture plays a significant role in improving performance. Furthermore, in Section 2.5, we compared catELMo-shallow with SeqVec, which share the same architecture but were trained on different data, highlighting the importance of data on the model performance.</p>
<p>To further address the reviewer's concern, we trained a Doc2Vec model on the TCR sequences that have been used for catELMo training. We observed significantly lower prediction performance compared to catELMo, with an average AUC of 50.24% in TCR split and an average AUC of 51.02% in epitope split, making the strength of context-aware embeddings clear.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>(1) It is known that TRB CDR3, the CDR1, CDR2 on TRBV gene and the TCR alpha chain also contribute to epitope recognition, but were not modeled in catELMo. It would be nice for the authors to add this as a current limitation for catELMo in the Discussion section.</p>
</disp-quote>
<p>We have discussed the limitation in the revised manuscript.</p>
<p>“Our study focuses on modeling the TCRβ chain CDR3 region, which is known as the primary determinant of epitope binding. Other regions, such as CDR1 and CDR2 on the TRB V gene, along with the TCRα chain, may also contribute to specificity in antigen recognition. However, a limited number of available samples for those additional features can be a challenge for training embedding models. Future work may explore strategies to incorporate these regions while mitigating the challenges of working with limited samples.”</p>
<disp-quote content-type="editor-comment">
<p>(2) I tried to follow the instructions to train a binding affinity prediction model for TCR-epitope pairs, however, the cachetools=5.3.0 seems could not be found when running &quot;pip install -r requirements.txt&quot; in the conda environment bap. Is this cachetools version supported after Python 3.7 so the Python 3.6.13 suggested on the GitHub repo might not work?</p>
</disp-quote>
<p>This has been fixed. We have updated the README.md on our github page.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>The article is well-constructed and well-written, and the analysis is comprehensive.</p>
<p>The comments for minor issues that I have are as follows:</p>
<p>(1) In the Methods section, it will be clearer if the authors interpret more on how the standard deviation is calculated in all tables. How to define the '10 trials'? Are they based on different random training and test set splits?</p>
</disp-quote>
<p>‘10 trials' refers to the process of splitting the dataset into training, validation, and testing sets using different seeds for each trial. Different trials have different training, validation, and testing sets. For each trial, we trained a prediction model on its training set and measured performance on its testing set. The standard deviation was calculated from the 10 measurements, estimating model performance variation across different random splits of the data.</p>
<disp-quote content-type="editor-comment">
<p>(2) The format of AUCs and the improvement of AUCs need to be consistent, i.e., with the percent sign.</p>
</disp-quote>
<p>We have updated the format of AUCs.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>In addition to the recommendations in the public review, we had the following more minor questions and recommendations:</p>
<list list-type="bullet">
<list-item><p>Could you provide some more background on the data, such as overlaps between the databases, and how the training and validation split was performed between the three databases? Also summary statistics on the length of TCR and epitope sequence data would be helpful.</p>
</list-item></list>
</disp-quote>
<p>We have provided more details about data in our revision.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Could you comment on the runtime to train and embed using the catELMo and BERT models?</p>
</list-item></list>
</disp-quote>
<p>Our training data is TCR sequences with relatively short lengths (averaging less than 20 amino acid residues). Such characteristic significantly reduces the computational resources required compared to training large-scale language models on extensive text corpora. Leveraging standard machines equipped with two GeForce RTX 2080 GPUs, we were able to complete the training tasks within a matter of days. After training, embedding one sequence can be accomplished in a matter of seconds.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>Typos and wording:</p>
</list-item></list>
<list list-type="bullet">
<list-item><p>Table 1 first row of &quot;source&quot;: &quot;immunoSEQ&quot; instead of &quot;immuneSEQ&quot;</p>
</list-item></list>
</disp-quote>
<p>This has been corrected.</p>
<disp-quote content-type="editor-comment">
<list list-type="bullet">
<list-item><p>L23 of abstract &quot;negates the need of complex deep neural network architecture&quot; is a little confusing because ELMo itself is a deep neural network architecture. Perhaps be more specific and add that the need is for downstream tasks.</p>
</list-item></list>
</disp-quote>
<p>We have made it more specific in our abstract.</p>
<p>“...negates the need for complex deep neural network architecture in downstream tasks.”</p>
<p>References</p>
<p>(1) Montemurro, Alessandro, et al. &quot;NetTCR-2.0 enables accurate prediction of TCR-peptide binding by using paired TCRα and β sequence data.&quot; Communications biology 4.1 (2021): 1060.</p>
<p>(2) Jurtz, Vanessa Isabell, et al. &quot;NetTCR: sequence-based prediction of TCR binding to peptide-MHC complexes using convolutional neural networks.&quot; BioRxiv (2018): 433706.</p>
<p>(3) Gielis, Sofie, et al. &quot;Detection of enriched T cell epitope specificity in full T cell receptor sequence repertoires.&quot; Frontiers in immunology 10 (2019): 2820.</p>
<p>(4) Cai, Michael, et al. &quot;ATM-TCR: TCR-epitope binding affinity prediction using a multi-head self-attention model.&quot; Frontiers in Immunology 13 (2022): 893247.</p>
<p>(5) Weber, Anna, et al. &quot;TITAN: T-cell receptor specificity prediction with bimodal attention networks.&quot; Bioinformatics 37 (2021): i237-i244.</p>
<p>(6) Lythe, Grant, et al. &quot;How many TCR clonotypes does a body maintain?.&quot; Journal of theoretical biology 389 (2016): 214-224.</p>
</body>
</sub-article>
</article>