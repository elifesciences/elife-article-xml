<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">102840</article-id>
<article-id pub-id-type="doi">10.7554/eLife.102840</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.102840.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>PointTree: Automatic and accurate reconstruction of long-range axonal projections of single-neuron</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Cai</surname>
<given-names>Lin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fan</surname>
<given-names>Taiyu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Qu</surname>
<given-names>Xuzhong</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Ying</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gou</surname>
<given-names>Xianyu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ding</surname>
<given-names>Quanwei</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Feng</surname>
<given-names>Weihua</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cao</surname>
<given-names>Tingting</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lv</surname>
<given-names>Xiaohua</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>Xiuli</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Huang</surname>
<given-names>Qing</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8393-4292</contrib-id>
<name>
<surname>Quan</surname>
<given-names>Tingwei</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>quantingwei@hust.edu.cn</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zeng</surname>
<given-names>Shaoqun</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00p991c53</institution-id><institution>Britton Chance Center for Biomedical Photonics, Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology</institution></institution-wrap>, <city>Wuhan</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00p991c53</institution-id><institution>MOE Key Laboratory for Biomedical Photonics, Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology</institution></institution-wrap>, <city>Wuhan</city>, <country country="CN">China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04jcykh16</institution-id><institution>School of Computer Science &amp; Engineering, Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology</institution></institution-wrap>, <city>Wuhan</city>, <country country="CN">China</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country country="CA">Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-11-11">
<day>11</day>
<month>11</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-07-22">
<day>22</day>
<month>07</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP102840</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-09-20">
<day>20</day>
<month>09</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-09-23">
<day>23</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.23.614432"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-11-11">
<day>11</day>
<month>11</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102840.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.102840.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.102840.1.sa1">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.102840.1.sa0">Reviewer #2 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>Â© 2024, Cai et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Cai et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-102840-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Single-neuron axonal projections reveal the route map of neuron output and provide a key cue for understanding how information flows across the brain. Reconstruction of single-neuron axonal projections requires intensive manual operations in tens of terabytes of brain imaging data, and is highly time-consuming and labor-intensive. The main issue lies in the need for precise reconstruction algorithms to avoid reconstruction errors, yet current methods struggle with densely distributed axons, focusing mainly on skeleton extraction. To overcome this, we introduce a point assignment-based method that uses cylindrical point sets to accurately represent axons and a minimal information flow tree model to suppress the snowball effect of reconstruction errors. Our method successfully reconstructs single-neuron axonal projections across hundreds of GBs (Gigabytes) images with an average of 80% F1-score, while current methods only provide less than 40% F1-score reconstructions from a few hundred MBs (Megabytes) images. This huge improvement is helpful for high-throughput mapping of neuron projections.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>32471146</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Maintext(includeing Abstract, Introduction, Results, Discussion, material and Methods) are revised;Figure 4 added; author affiliations updated; Supplemental files updated.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Neuronal axons in general project to different brain regions, and their projection distribution is an essential cue for neuron type identification, neuronal circuit construction, and deeper insight into how information flows in the brain<sup><xref ref-type="bibr" rid="c1">1</xref>-<xref ref-type="bibr" rid="c4">4</xref></sup>. Advances in optical imaging and molecular labeling techniques<sup><xref ref-type="bibr" rid="c5">5</xref>-<xref ref-type="bibr" rid="c10">10</xref></sup> have allowed us to observe the entire mouse brain at single-axon resolution, and provided the database for the study of neuronal projection patterns<sup><xref ref-type="bibr" rid="c11">11</xref>-<xref ref-type="bibr" rid="c18">18</xref></sup>. However, the reconstruction of these long-range projected axons still requires extensive manual annotation in tens of TBs volumetric images<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c19">19</xref>-<xref ref-type="bibr" rid="c22">22</xref></sup>, this labor-intensive process creates a major bottleneck for high-throughput mapping of neuronal projections<sup><xref ref-type="bibr" rid="c23">23</xref></sup>.</p>
<p>The difficulties in reconstructing the long-range projections of neurons are as follows. On the one hand, while molecular labeling techniques can shed light on a very small fraction of neurons, a significant fraction of neuronal axons are still densely distributed due to the morphological complexity of neurons. The identification of densely distributed axons is considered an open problem in the field<sup><xref ref-type="bibr" rid="c23">23</xref>-<xref ref-type="bibr" rid="c25">25</xref></sup>, which still has no good solution. On the other hand, during neuron reconstruction, reconstruction errors accumulate and a single reconstruction error can result in an entire branch being connected erroneously to other neurons or missing<sup><xref ref-type="bibr" rid="c26">26</xref></sup>. Therefore, effective large-scale reconstruction of neurons requires extremely high identification accuracy of dense axons. The contradictions between these two aspects seem hard to reconcile.</p>
<p>The current neuron reconstruction frameworks focus on how to accurately extract skeletons of neurites and establish the connections between skeletons<sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c27">27</xref></sup>. The BigNeuron project<sup><xref ref-type="bibr" rid="c28">28</xref></sup> conducts a systematic evaluation of 35 automatic neuron reconstruction algorithms, all of which are based on tracing neurite skeletons and can be divided into two categories: local and global approaches. In the local approach<sup><xref ref-type="bibr" rid="c29">29</xref>-<xref ref-type="bibr" rid="c32">32</xref></sup>, the localization of the next skeleton point requires computation of the signal anisotropy of the image region near the current skeleton point. Localization errors typically occur when this image region contains other neurite signals. The global approach<sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup> first generates multiple seed points that are commonly located at the neurite centerline, and then establishes connections between these seed points for generating the neurite skeleton. This connection relies mainly on spatial location information, resulting in densely distributed neurites being connected to each other erroneously. While deep learning is widely used in neuron reconstruction<sup><xref ref-type="bibr" rid="c35">35</xref>-<xref ref-type="bibr" rid="c38">38</xref></sup>,-mainly for neuronal image segmentation and signal intensity enhancement to reduce reconstruction errors-even ideal segmentation with all neurite centers identified and their signal enhanced still exhibit significant reconstruction errors with skeleton-based methods (<xref ref-type="supplementary-material" rid="supp1">Fig. S1</xref>).</p>
<p>To address the problem of error accumulation during neuron reconstruction, it is common practice to utilize statistical information of neuron morphology, such as the angle between two neurites, to identify and remove spurious connections between the reconstructed neurites. This strategy<sup><xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup> achieves 80% reconstruction accuracy from GB-scale images under two critical constrains: (1) precise identification of neurite terminals and branch points is required for accurate angle computation and morphological analysis, and (2) somatic locations are required as critical information to remove some links between the reconstructed neurites to ensure that each cell body can be mapped to the root node of a single tree structure. However, for long-range axonal reconstruction across hundreds of GB-scale images, the strategy is not effective to eliminate the accumulation of errors due to factors such as the position of the axon at a distance from the soma, and slight morphological differences between axon junction and termination. Consequently, current long-range projection reconstruction methods are semi-automatic and require substantial human intervention<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref>,<xref ref-type="bibr" rid="c40">40</xref></sup>.</p>
<p>Here, we propose a new neuron reconstruction method called PointTree, which aims at how to assign foreground points in neuronal images to their own neurons. In the workflow, we design a constrained Gaussian clustering method to partition the foreground region of a neuronal image into a series of columnar regions whose centerline belongs to only a single neurite. This operation essentially eliminates the interference of different neurites in the dense reconstruction. In addition, each columnar region is characterized by a minimal envelope ellipsoid for constructing connections between columnar regions, which forms the neurite shapes. Based on the reconstructed shapes, we design a minimal information flow tree model to suppress the cumulative reconstruction error. Using the proposed method, we successfully achieve accurate reconstruction of long-range projections of neurons across hundreds of gigabytes of volumetric image.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>The architecture and principles of PointTree</title>
<p>In the design of PointTree, we have developed a series of optimization problems to assign foreground points in data blocks to their respective neurites. Firstly, the segment network is utilized for each data block to obtain foreground points. Subsequently, we apply a constrained Gaussian clustering method<sup><xref ref-type="bibr" rid="c41">41</xref></sup> to partition the foreground points into columnar regions and determine their geometrical parameters by solving the minimum-volume covering ellipsoids problem<sup><xref ref-type="bibr" rid="c42">42</xref></sup>. Using these geometrical parameters, we construct a 0-1 assignment problem<sup><xref ref-type="bibr" rid="c43">43</xref></sup> to establish links between these columnar regions. Finally, skeletons are extracted from these linked columnar regions to reduce data redundancy by using region growing<sup><xref ref-type="bibr" rid="c44">44</xref></sup>. The key procedures for neuron reconstruction are presented in <xref rid="fig1" ref-type="fig">Figure 1A</xref>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Summary and principle of PointTree.</title>
<p>(<bold>A</bold>) The reconstruction procedure of PointTree involves the generation, clustering, and connection of foreground points (the first row). Within this procedure, three optimization problems are designed to allocate the foreground points into their respective neurites (the second row). (<bold>B</bold>) Schematic diagram of information flow score calculation. In a neurite branch with a fixed root node (green circle), the information flow score is calculated based on the assumption that a neurite has few directional changes. The assumption determines the neurite directly connecting to the root node (red), resulting in two branch angles used to calculate the information flow score. (<bold>C</bold>) Statistical analysis of the consistency between the minimum information flow and the real situation. For 208 neurite branches, the information flow scores are calculated as ground truth according to their manually determined skeletons and root nodes. These scores are then displayed in ascending order. The root nodes of neurite branches are changed to generate both maximum and minimum information flow scores. (<bold>D</bold>) One neurite branch is decomposed into two by minimizing the total information flow scores. (<bold>E</bold>) Performance of different methods on separating closely paralleled neurites. In PointTree, a single neurite is represented by a series of ellipsoids whose centerlines are not simultaneously located within different neurites. They are connected using ellipsoid shape which results in perfect reconstruction (Left). However, skeleton-based methods fail to separate two closely paralleled neurites due to interference from other signals (Red circle in middle) or connections being interfered with by another neighboring skeleton point (Red circle in right).</p></caption>
<graphic xlink:href="614432v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In addition, PointTree employed the statistical prior information to reduce the reconstruction errors. At the branching point (node) of the neurites, it can be divided into three segments of neurite skeletons. The segment entering the node forms two angles with the other two segments exiting the node respectively. The node angle is defined as the smaller angle between the entering segment and each exiting segment (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). With node angle, we can identify the single complete neurite and its corresponding node angles. The skeleton of the neurite is generally smooth, with very few sudden directional changes and even fewer at the nodes. So, the node angles should be as small as possible. For neuronal branches, the node angles are uniquely determined when the root node is given, and the sum of the negative cosine of these node angles expressed by information flow value is small when the root node is correctly identified. This rule is defined as minimal information flow tree (MIFT).</p>
<p>In image blocks of densely distributed neurites, we used semi-automatic software<sup><xref ref-type="bibr" rid="c21">21</xref></sup> extracting 208 neuronal branches and identifying their root nodes. For each branch, we calculated their information flow values as the ground-truth information flow values (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). To validate MIFT, we looped through all possible structure of these branches by changing the root node in order to compute the maximum and minimum information flow values (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). It is evident that, for most neuronal branches (195/208), the ground-truth values of the information flow achieve the minimum value, suggesting that MIFT rule is reasonable. We utilized MIFT to modify skeleton structure and remove spurious connections between reconstructed neurites (<xref rid="fig1" ref-type="fig">Figure 1D</xref> and <xref ref-type="supplementary-material" rid="supp1">Fig.S2</xref>), both for reconstructions within individual blocks and for the fused reconstruction in adjacent blocks.</p>
<p>PointTree has the capability to separate densely distributed neurites. When dealing with two parallel neurites in close proximity to each other, their shapes can be represented by a series of columnar regions (<xref rid="fig1" ref-type="fig">The left panels of Figure 1E</xref>). We have modified the Gaussian clustering algorithm by constraining the estimated mean and covariance parameters so that the cluster shape approaches a columnar shape. Additionally, foreground points within the same cluster are connected to each other. These two features ensure that the central line in the columnar region belongs to only a single neurite, which is crucial for separating densely packed neurites. Furthermore, we utilize the minimum volume covering ellipsoid to extract shape information of the columnar regions for constructing their connections. These designs enable PointTree to successfully reconstruct packed neurites. In contrast, skeleton-based local methods rely on determining the position of the next skeleton point based on the shape anisotropy of the region. This often leads to localization errors when there are two neurite image signals within a region (<xref rid="fig1" ref-type="fig">The middle panels of Figure 1E</xref>). When it comes to skeleton-based global methods, although seed points can be located at individual neurite centers, accurately constructing connections between these seed points proves challenging due to the reliance on distance between points and susceptibility to interference from densely distributed neurites. (<xref rid="fig1" ref-type="fig">The right panels of Figure 1E</xref>).</p>
</sec>
<sec id="s2b">
<title>The merits of PointTree in dense reconstruction</title>
<p>In dense reconstruction, one of the main concerns is how well to separate densely distributed neurites which behaves as crossover and closely paralleled neurites. These neurites can be manually identified by visualization with different view angles (<xref ref-type="supplementary-material" rid="supp1">Fig.S3</xref>). We compared PointTree with several skeleton-based methods such as neuTube<sup><xref ref-type="bibr" rid="c45">45</xref></sup>, PHDF<sup><xref ref-type="bibr" rid="c46">46</xref></sup>, NGPST<sup><xref ref-type="bibr" rid="c39">39</xref></sup> and MOST<sup><xref ref-type="bibr" rid="c47">47</xref></sup> in performing this task. We manually labeled the locations where neurites are crossover or closely parallel from five 256Ã256Ã256 image blocks. For fair comparison, all methods are performed on segmented images derived from the segmentation network. <xref rid="fig2" ref-type="fig">Figure 2A</xref> illustrates the process of PointTreeâs separation of crossover and closely paralleled neurites. PointTree can successfully separate the densely distributed neurites in a range of 71.4 % and 91.7%, while these skeleton-based methods only separate 25.0% densely distributed neurites (<xref rid="fig2" ref-type="fig">Figure 2B</xref>) at most. We also present the comparison of PointTree and other methods on some reconstruction examples in which multi crossover neurites (<xref rid="fig2" ref-type="fig">Figure 2C</xref>) and closely paralleled neurites are involved. PointTree provides the perfect reconstruction while other methods fail to reconstruct these neurites.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Performance of PointTree on crossover and closely paralleled neurites.</title>
<p>(<bold>A</bold>) The reconstruction process of crossover and closely paralleled neurites. (<bold>B</bold>) Quantitative evaluation of PointTree and several skeleton-based methods on identifying closely distributed neurites. The box plots present the statistical information in which the horizontal line in the box, the lower and upper borders of the box represent the median value, the first quartile (Q1) and the third quartile (Q3) respectively. The vertical black lines indicate 1.5 Ã IQR. (<bold>C</bold>) Three reconstruction examples derived from PointTree and several skeleton-based methods.</p></caption>
<graphic xlink:href="614432v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Furthermore, we present the quantitative results derived from PointTree and five widely used skeleton-based reconstruction methods. including APP2, neuTube, NGPST, PHDF, MOST. Eight 256Ã256Ã256 image blocks that include many densely distributed neurites are of the testing dataset. All reconstruction algorithms are performed on the segmentation images of these testing dataset. We give the intuitive reconstruction comparisons (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). PointTree provides the reconstruction close to the ground truth. The skeleton-based methods generate lots of reconstruction errors and incorrectly combine multi neurites into a single branch. The quantitative reconstructions suggest that PointTree is far superior to skeleton-based methods (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). For PointTree, the average precision is above 90%, both recall and f1-score are above 85%. The skeleton-based methods cannot provide the good solution to separate the densely neurites. The f1-score of these reconstructions ranges from 30% to 40%, which indicates the ineffective reconstructions.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Comparison of reconstruction methods on image blocks containing densely distributed neurites.</title>
<p>(<bold>A</bold>) Comparison of reconstruction performance among six methods, including PointTree, NGPST, neuTube, APP2, PHDF, and MOST. Individual neurite branches are delineated in different colors. (<bold>B</bold>) Quantitative evaluation of reconstruction performance using precision, recall, and f1-score. The box plots display these three evaluation indexes (n=8). In the box, the horizontal line represents the median value. The box shows the interquartile range (IQR) from the first quartile (Q1) to the third quartile (Q3). The vertical lines indicate 1.5Ã IQR.</p></caption>
<graphic xlink:href="614432v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>Reconstruction of data with different signal-to-noise ratios</title>
<p>In the field of neuronal reconstruction, data acquired by different imaging systems often exhibit varying signal-to-noise ratio (SNR) characteristics. For some low-SNR datasets, severe noise interference makes it difficult even for human observers to accurately identify neurite structures. To systematically evaluate PointTreeâs reconstruction performance across datasets with different SNRs, we selected and analyzed data from three imaging systems: light sheet microscopy<sup><xref ref-type="bibr" rid="c48">48</xref></sup> (LSM), fluorescent micro-optical sectioning tomography<sup><xref ref-type="bibr" rid="c49">49</xref></sup> (fMOST), and high-definition fluorescent micro-optical sectioning tomography<sup><xref ref-type="bibr" rid="c50">50</xref></sup> (HD-fMOST), with SNR ranges of 2â7, 6â12, and 9â14, respectively (<xref rid="fig4" ref-type="fig">Figure 4A</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Reconstruction performance of PointTree across data with different signal-to-noise ratios.</title>
<p>(<bold>A</bold>) Data blocks from light sheet microscopy (LSM), fluorescent micro-optical sectioning tomography (fMOST) and high-definition fluorescent micro-optical sectioning tomography (HD-fMOST) are selected. SNR and corresponding reconstruction scores with PointTree are drawn with line charts. Each dataset is of sample size n =25 and each data block size of 128Ã128Ã128. (<bold>B</bold>) shows reconstruction performance of PointTree on different datasets. (<bold>C</bold>) The zoomed-in view displays the region marked by white box in the first column of (B), with 25 foreground points and 25 background points sampled respectively. The signal intensities of both the foreground points and background points are plotted in the adjacent line charts.</p></caption>
<graphic xlink:href="614432v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Experimental results demonstrate that, thanks to the powerful feature extraction capability of the deep learning network, the trained neural network achieves satisfactory segmentation performance (<xref rid="fig4" ref-type="fig">Third row in Figure 4B</xref>) even on low-SNR data (<xref rid="fig4" ref-type="fig">First two columns in Figure 4B, top row</xref>), laying a solid foundation for subsequent accurate reconstruction (<xref rid="fig4" ref-type="fig">Bottom row in Figure 4B</xref>). Quantitative analysis reveals that PointTree delivers stable reconstruction performance across all SNR levels. Specifically: for LSM data (sample size n=25, mean SNR=5.01), average precision=96.0%, recall=88.7%, and f1-score=91.0%; for fMOST data (sample size n=25, mean SNR=8.68), average precision=95.8%, recall=87.3%, and f1-score=90.0%; for HD-fMOST data (sample size n=25, mean SNR=11.4), average precision=98.1%, recall=91.0%, and f1-score=93.3% (<xref rid="fig4" ref-type="fig">Figure 4A</xref>).</p>
<p>Notably, in low-SNR LSM data, background regions contain more artifactual signals (<xref rid="fig4" ref-type="fig">First panel in Figure 4C</xref>) due to similar intensity distributions between background and foreground points. In contrast, high-SNR datasets (fMOST and HD-fMOST) exhibit cleaner background features with distinct intensity separation between background noise and neurite signals (<xref rid="fig4" ref-type="fig">Second and third panel in Figure 4C</xref>). This observation highlights the critical impact of SNR on reconstruction quality while simultaneously validating the robustness of PointTree, which is aided by the segmentation network, across diverse SNR conditions.</p>
</sec>
<sec id="s2d">
<title>Restrain error accumulation in the reconstruction</title>
<p>In order to achieve accurate axon reconstruction, it is essential to effectively suppress the snowballing accumulation of reconstruction errors. The performance of the minimal information flow tree (MIFT) in retraining the reconstruction errors is evaluated in this study. <xref rid="fig5" ref-type="fig">Figure 5A</xref> presents six 512Ã512Ã512 image blocks and their reconstructions using PointTree in the first column. The reconstruction fusing procedure is then performed on these axonal reconstructions (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). By employing MIFT to revise the reconstructions and remove false connections between axons, reasonable reconstructions are achieved. In contrast, when the same fusion procedure is conducted without MIFT to revise the reconstruction, almost all axons are incorrectly connected together (<xref rid="fig5" ref-type="fig">Bottom-right panel in Figure 5A</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Minimal information flow tree effectively restrains the accumulation of reconstruction errors.</title>
<p>(<bold>A</bold>) Reconstruction comparisons in the fusion process with MIFT and without MIFT are shown. Both image blocks and neurites reconstructions are displayed using maximum projection along the z-direction. Two fusion procedures are performed, and the final fusion reconstructions are presented in the third column. (<bold>B</bold>) The variation in reconstruction accuracy during the fusion process with MIFT and without MIFT is illustrated. Blue points represent the initial reconstruction accuracy from six image blocks, while green points and red points denote the merged reconstruction accuracy with MIFT and without MIFT, respectively. The squares represent the mean values of the evaluation indexes. (<bold>C</bold>) The skeletons of three neurite branches from the final merged reconstructions with MIFT are shown. Additionally, corresponding ground-truth reconstructions and reconstruction evaluations are also presented.</p></caption>
<graphic xlink:href="614432v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We furthermore measure the enhancement in the reconstruction accuracy achieved by MIFT (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). For the initial reconstructions from six image blocks, the average of f1-score is about 0.86. By using MIFT, the average of f1-score is above 0.8 for the reconstructions from two image blocks which are generated with the first fusion. In the second fusion (<xref rid="fig5" ref-type="fig">Top-right panel in Figure 5A</xref>), f1-score still keeps 0.79. In contrast, without MIFT, the first fusion leads to a drop of about f1-score of 0.3. After the second fusion, f1-score is less than 0.2. We also present some reconstruction examples after two fusions in <xref rid="fig5" ref-type="fig">Figure 5C</xref>, which are close to the ground truth. These results suggest that MIFT model take consideration of the proper structure of axons and thus can restrain the error communications in the reconstruction fusion process.</p>
</sec>
<sec id="s2e">
<title>Long-range axonal projections reconstruction</title>
<p>We applied PointTree for long-range axon reconstruction. The testing image block has the size of 11226Ã8791Ã1486 voxels and includes axons from eight ne urons (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). We also used GTree to manually reconstruct these neurons as the ground-truth reconstruction (<xref rid="fig6" ref-type="fig">Figure 6B</xref>). Except for the labeling of training data for segmentation network and of the axon starting points of a single neuron, the whole reconstruction process is totally automatic. The results show PointTree successfully recovered the axonal morphology of these eight neurons without manual interference (<xref rid="fig6" ref-type="fig">Figure 6C</xref> and <xref ref-type="supplementary-material" rid="video1">Movies S1 &amp; S2</xref>), and we compared these reconstructions with ground truth (<xref ref-type="supplementary-material" rid="supp1">Fig.S4</xref>). The average precision is above 85% and the average recall and f1-score are above 80% (<xref rid="fig6" ref-type="fig">Figure 6E</xref>). In addition, we presented the axons reconstructions from two image blocks (<xref rid="fig6" ref-type="fig">Figures 6C<sub>1</sub>&amp;C<sub>2</sub></xref>) which include a large number of densely distributed axons. This reconstruction performance suggests that the point assignment and the minimal information flow tree mode, as the two key strategies in PointTree, perform well in long-range axonal reconstruction.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Long-range axonal reconstruction using PointTree.</title>
<p>(<bold>A</bold>) The image block contains eight neurons in the ventral posteromedial thalamic region. The projection of these neurons includes a large number of densely distributed axons, which are enlarged in A<sub>1</sub> and A<sub>2</sub>. (<bold>B</bold>) The reconstruction of the eight neurons is achieved by annotators with semi-automatic software GTree, serving as ground-truth reconstruction to evaluate automatic algorithms. The reconstructions B<sub>1</sub> and B<sub>2</sub> correspond to the image blocks A<sub>1</sub> and A<sub>2</sub>. (<bold>C</bold>) Automatic reconstruction with PointTree results in reconstructions of the densely distributed axons, which are enlarged in C<sub>1</sub> and C<sub>2</sub>. (<bold>D</bold>) A comparison between automatic reconstruction and ground-truth reconstruction of axonal projection for one neuron is shown. Green indicates consistent reconstruction, blue indicates missed branches, and red denotes branches from other neurons. (<bold>E</bold>) Quantitative analysis of long-range projections for these neurons is presented. Statistical information is displayed in boxes, while black points represent the accuracy of the reconstructions for these neurons.</p></caption>
<graphic xlink:href="614432v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We also applied PointTree to process another 10739Ã11226Ã3921 image blocks collected with HD-fMOST system<sup><xref ref-type="bibr" rid="c50">50</xref></sup>. The high signal-to-noise ratio in this optical system results in a significantly extended dynamic range of the signal. PointTree can effectively deal with this case, and all 14 long-range projections are successfully reconstructed (<xref ref-type="supplementary-material" rid="supp1">Fig.S5</xref>). The quantitative results suggest that the average f1-score is above 90% (<xref ref-type="supplementary-material" rid="supp1">Table S1</xref>).</p>
<p>Despite the need to solve multiple large-scale optimization problems, the reconstruction speed using PointTree is generally faster than the imaging speed. For instance, in a typical scenario involving 254 image blocks with 512Ã512Ã512 voxels, the total tim e required for reconstruction is approximately 44 minutes. Even for a larger dataset comprising 821 image blocks with 512Ã512Ã512 voxels and including a significant number of sparsely distributed neurites, the total time cost amounts to about 60 minutes (<xref ref-type="supplementary-material" rid="supp1">Table S2</xref>). It should be noted that the time cost does not increase linearly as data volume increases due to the influence of neurite density on overall reconstruction time. In summary, PointTree demonstrates remarkable speed in reconstructing long-range axons (<xref ref-type="supplementary-material" rid="video3">Movie S3</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We have presented an automated method for reconstructing the long-range projections of neurons. In this study, we address the problem of mutual interference among densely distributed neurites and the cumulative error during reconstruction by designing reconstruction method based on point set assignment and the minimal information flow tree, respectively. As a result, our approach enables accurate reconstruction of long-range neuron projections from hundreds of gigabytes of data. This advance significantly enhances the efficiency of whole-brain-scale neuron reconstruction, bridging the substantial gap between factory-level generation of whole-brain-scale neuronal imaging data and tens of hours required to reconstruct one neuron.</p>
<p>Our approach is performed on image foregrounds where the segmented neurites have a fixed radius approximately equal to the total size of the three voxels. In this case, we can estimate the total number of foreground points (voxels) and set a suitable number of columnar regions for ensuring the anisotropy of each columnar region, which is based on the fact that the union of columnar regions equals to the foreground region. The anisotropy of the columnar regions will reduce the difficulty in establishing their connection. The requirement that all segmented neurites have a relatively fixed radius can be fulfilled. For all neurites, the value of their voxels decreases as these voxels deviate from the nearest centerline. The deep learning network is able to grasp this feature and segment only the neurite centerline and its neighborhood. Typically, in reconstructions of neurons whose projections are distributed over hundreds to thousands of GBs of data, less than GB-sized images with labels are needed as training data. The labeling process takes a few hours, which is negligible for semi-automatic reconstruction of all neurons in the whole volume images.</p>
<p>We propose a new reconstruction mode centered on point set assignment instead of the current reconstruction mode focused on skeleton extraction. In the current reconstruction paradigm, most deep networks are used to enhance the signal-to-noise ratio of neuronal images and do not address well the issue of signal interference during skeleton extraction. In contrast, our reconstruction approach is based on directly processing the foreground points generated by the deep learning network. With continued advances in deep learning techniques, the generality and accuracy of image segmentation will be continuously enhanced, thereby significantly boosting the application scope of our method in various scenarios. Essentially, our method can be applied to any skeleton tracking-based application scenario and effectively eliminate dense signal interference.</p>
<p>Our method still generates a few reconstruction errors. This is due to the following three aspects. First, our method directly handles image foregrounds, which leads to reconstruction errors when some neurites with weak image intensities are not identified. Second, relying solely on foreground point information and rule-based judgment methods may generate some connection errors when establishing connections between neurites. Finally, the minimal information flow treeâs fundamental assumption, that axons should be as smooth as possible does not always hold true. In fact, real axons can take quite sharp turns leading the algorithm to erroneously separate a single continuous axon into disjoint fibers (<xref ref-type="supplementary-material" rid="supp1">Fig. S7</xref>). Therefore, for the automatic reconstruction of neurons on a brain-wide scale, further work is needed to enhance the imaging intensity and incorporate soma shapes and raw image signals for neurites connection recognition.</p>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<sec id="s4a">
<title>Data collections</title>
<p>The test datasets are collected through the preparation of two kinds of samples. For one C57BL/6 male mouse, 100 nl AAV-Cre virus and 100 nl of AAV-EF1<italic>Î±</italic>-DIO-EYFP virus were injected into the VPM nucleus at the same time. 21 days later, the chemical sectioning fluorescence tomography(CSFT) system<sup><xref ref-type="bibr" rid="c49">49</xref></sup> was used to acquire imaging data (<xref rid="fig1" ref-type="fig">Figures 1</xref>-<xref ref-type="fig" rid="fig6">6</xref>), more details can be seen in the reference<sup><xref ref-type="bibr" rid="c51">51</xref></sup>. For one C57BL/6J male mouse, 100 nl of AAV-YFP was injected into the motor area. 21 days later, high-definition fluorescent micro-optical sectioning tomography (HD-fMOST) was used to acquire imaging data<sup><xref ref-type="bibr" rid="c50">50</xref></sup> (<xref ref-type="supplementary-material" rid="supp1">Fig.S5</xref>).</p>
</sec>
<sec id="s4b">
<title>Generation of foreground points</title>
<p>Our reconstruction method performs on the image foregrounds. Here, we used UNet3D<sup><xref ref-type="bibr" rid="c52">52</xref></sup> for image stacks segmentation without network structure modification. The detailed information about UNet3D can be found in the reference<sup><xref ref-type="bibr" rid="c52">52</xref></sup>. Considering the requirement that the network output, the segmented neurites, have the relatively fixed radius, we calculate the distance field of the neuriteâs skeleton as the ground-truth for supervise the network. Initially, the semi-automatic software GTree was utilized to extract the neurites skeleton and subsequently interpolate the skeleton points. The interpolation operation ensured that the distance between any skeleton point and its nearest point was less than 1 <italic>Î¼m</italic>. Subsequently, the interpolated skeleton points were used as centers to mark spherical regions with a radius of 5 voxels. These spherical regions served as candidate areas for foreground. Within these candidate areas, the distance from each point to its nearest interpolated skeleton point was calculated. Finally, the distances are mapped into Gaussian kernel distances which forms the Gaussian density map. This map normalized by maximum value leads to the distance field map to supervise UNet3D output.</p>
<p>In the training stage, Adam optimizer is used with an initial learning rate at 3e-4. The input image size is 128Ã128Ã128. Batch size is set to 1, the L1 -norm is used as loss function to train the network. We presented the reconstructions from two kinds of fMOST datasets. One is from the reference<sup><xref ref-type="bibr" rid="c51">51</xref></sup> and the other is from the reference<sup><xref ref-type="bibr" rid="c50">50</xref></sup>. Therefore, we created two sets of training data, each consisting of 20 512Ã512Ã512 image blocks (each divided into 64 image blocks of size 128Ã128Ã128). In each set, 10 image blocks contain densely distributed neurites, while the other 10 blocks contain sparsely distributed neurites. In the predicting stage, we applied the threshold operation to the distance field image. The voxels whose values are more than 0.5 are regarded as the foreground points.</p>
</sec>
<sec id="s4c">
<title>Neuron Reconstruction based on Points assignment</title>
<p>For the image stack, we allocated the foreground points to their respective neurites and established connections between neurites by constructing three optimization models: (1) the constrained Gaussian mixture model divides the foreground points into a set of points, each of which has a column shape; (2) the minimum-volume covering ellipsoids model extracts the features of the column-shaped point set; (3) the 0-1 assignment optimization model establishes connections between the column-shaped point sets, resulting in the shapes of individual neurites, and then builds connections between the reconstructed neurites.</p>
</sec>
<sec id="s4d">
<title>Constrained Gaussian mixture model</title>
<p>The three-dimensional Gaussian function exhibits an ellipsoidal shape in space, which we have utilized to approximate the columnar shape of local neurites. In this study, Gaussian distribution mixture functions with <italic>K</italic> components are employed to approximate the shape of all neurites in an image block. The component number <italic>K</italic> is obtained by point density and will be discussed later. Given the foreground points <italic>x</italic><sub><italic>1</italic></sub>, <italic>x</italic><sub><italic>2</italic></sub>, â¦,<italic>x</italic><sub><italic>n</italic></sub>, for each foreground points <italic>x</italic><sub><italic>i</italic></sub>, the probability density function <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>) is calculated as follow:
<disp-formula id="eqn1">
<graphic xlink:href="614432v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>N</italic>(<italic>x</italic><sub><italic>i</italic></sub>|<italic>Î¼</italic><sub><italic>j</italic></sub>, â <sub><italic>j</italic></sub>) is the Gaussian density function with mean value <italic>Î¼</italic><sub><italic>j</italic></sub> and covariance matrix â <sub><italic>j</italic></sub>. Weight <italic>Ï</italic> <sub><italic>j</italic></sub> is the regularization parameter. <italic>N</italic>(<italic>x</italic><sub><italic>i</italic></sub>|<italic>Î¼</italic><sub><italic>j</italic></sub>, â <sub><italic>j</italic></sub>) is given by the formula:
<disp-formula id="eqn2">
<graphic xlink:href="614432v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Based on probability density function, the conditional probability can be computed as:
<disp-formula id="eqn3">
<graphic xlink:href="614432v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>p</italic><sub><italic>i, j</italic></sub> is the conditional probability for <italic>x</italic><sub><italic>i</italic></sub> to assign to the <italic>j</italic>-th cluster. If <italic>p</italic><sub><italic>i,k</italic></sub> is the maximum value among {<italic>p</italic><sub><italic>i</italic>,1</sub>,â¦<italic>p</italic><sub><italic>i,K</italic></sub>}, the foreground point <italic>x</italic><sub><italic>i</italic></sub> will be assigned to the <italic>k</italic>-th cluster. All the points assigned to the <italic>k</italic>-th cluster form a columnar region. Considering that both the number of foreground points and component number are large, we have added some constrained conditions for Gaussian mixture model as follows:
<disp-formula id="eqn4">
<graphic xlink:href="614432v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn5">
<graphic xlink:href="614432v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<inline-formula><inline-graphic xlink:href="614432v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> refers to the fact that the total probability distribution normalizes to 1. <italic>I</italic> (Â·) represents the signal intensity from segment image, <italic>Îµ</italic><sub>0</sub> is the minimum signal intensity of foreground points and is set to 128 in the algorithm. <italic>I</italic> (<italic>Î¼</italic><sub><italic>i</italic></sub>) â¥ <italic>Îµ</italic><sub>0</sub> restrain the center of the gaussian distribution to be a foreground point. | â <sub><italic>j</italic></sub> |â¤ <italic>Îµ</italic><sub>1</sub> restrain the determinant of the covariance matrix which control the suitable number of foreground points for each columnar region. <italic>Îµ</italic><sub>1</sub> is set to cube of three times the average diameter of neurite.</p>
<p>Maximum likelihood is employed to estimate the parameters of Gaussian mixture model and the final optimization problem is formed as follow:
<disp-formula id="eqn6">
<graphic xlink:href="614432v2_eqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn7">
<graphic xlink:href="614432v2_eqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
In solving this optimization problem, we employ peak density algorithm<sup><xref ref-type="bibr" rid="c53">53</xref></sup> to compute density for each foreground points and sort them in descending order. We first select a point as seed point, and the foreground points within a radius of 5 centered on it will be excluded. Then we continue selecting seed points until all foreground points are either selected or excluded. The selected <italic>K</italic> seed points represents the initial <italic>K</italic> components. We select signal points from the median (based on density) to both sides as seed points which can decrease the situations that seed points lies in the center of a crossover or the edge of neurites, this strategy can make the generated columnar regions be more reasonable. The positions of the <italic>K</italic> seed points are set to the initial (<italic>Î¼</italic><sub>1</sub>, <italic>Î¼</italic><sub>2</sub>, â¦, <italic>Î¼</italic><sub><italic>K</italic></sub>). The initial setting of covariance matrix is the identity matrix. The constrained Gaussian mixture model was solved by EM algorithm<sup><xref ref-type="bibr" rid="c54">54</xref></sup>, the EM algorithm is divided into two steps:</p>
<p>Estep: For each point <italic>x</italic><sub><italic>i</italic></sub>, compute its probability within each gaussian distribution using the probability density function:
<disp-formula id="eqn8">
<graphic xlink:href="614432v2_eqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Mstep: Update the mean value, covariance matrices and weight vectors.
<disp-formula id="eqn9">
<graphic xlink:href="614432v2_eqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn10">
<graphic xlink:href="614432v2_eqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn11">
<graphic xlink:href="614432v2_eqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Besides, the constrained gaussian mixture model possesses additional constraints: <italic>I</italic> (<italic>Î¼</italic><sub><italic>j</italic></sub>) â¥ <italic>Îµ</italic><sub>0</sub> and | â <sub><italic>j</italic></sub> |â¤ <italic>Îµ</italic><sub>1</sub>. After finish M-step, <italic>Î¼</italic><sub><italic>j</italic></sub> with <italic>I</italic> (<italic>Î¼</italic><sub><italic>j</italic></sub>) &lt; <italic>Îµ</italic><sub>0</sub> are selected. Eigenvalue decomposition are applied on â <sub><italic>j</italic></sub> and obtain eigenvalues (<italic>Î³</italic><sub>1</sub>, <italic>Î³</italic><sub>2</sub>, <italic>Î³</italic><sub>3</sub>) in descending order and eigenvectors (<italic>v</italic><sub>1</sub>, <italic>v</italic><sub>2</sub>, <italic>v</italic><sub>3</sub>). <italic>Î¼</italic><sub><italic>j</italic></sub> is updated along <italic>v</italic><sub>1</sub> and â<italic>v</italic><sub>1</sub> to generate two new cluster with mean value and covariance matrices (<italic>u</italic> <sub><italic>j</italic>, 1</sub>, â <sub><italic>j</italic>, 1</sub>) and (<italic>u</italic> <sub><italic>j</italic>, 2</sub>, â <sub><italic>j</italic>, 2</sub>) as follow:
<disp-formula id="eqn12">
<graphic xlink:href="614432v2_eqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn13">
<graphic xlink:href="614432v2_eqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn14">
<graphic xlink:href="614432v2_eqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn15">
<graphic xlink:href="614432v2_eqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For Î£<sub><italic>j</italic></sub>&gt;<italic>Îµ</italic><sub>1</sub> it will be updated as follow:
<disp-formula id="eqn16">
<graphic xlink:href="614432v2_eqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Iteration of Estep and Mstep will continue until the <italic>k</italic>-th result {<italic>Î¼</italic><sup><italic>k</italic></sup>, â<sup><italic>k</italic></sup>} and (<italic>k-</italic>1)-th result satisfy the stopping criteria:
<disp-formula id="eqn17">
<graphic xlink:href="614432v2_eqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here the division represents element-wise division and â¥Â· â¥ denotes <inline-formula><inline-graphic xlink:href="614432v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>-norm and <bold><italic>Îµ</italic></bold> is set to 0.01.</p>
</sec>
<sec id="s4e">
<title>Shape characterization of columnar regions</title>
<p>After deriving the columnar regions through solving the constrained Gaussian mixture model, it is imperative to characterize their geometric shape (terminals and centerlines). For this purpose, we calculate the minimum-volume ellipsoids that can fully encompasses each individual columnar region. For <inline-formula><inline-graphic xlink:href="614432v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, a three-dimensional ellipsoid can be defined as follow<sup><xref ref-type="bibr" rid="c42">42</xref></sup>:
<disp-formula id="eqn18">
<graphic xlink:href="614432v2_eqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, <italic>c</italic> is the center of ellipsoid, <italic>Q</italic> represents the geometric shape, <inline-formula><inline-graphic xlink:href="614432v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> denotes the convex cone of 3Ã 3 symmetric positive definite matrices. The volume of <italic>E</italic><sub><italic>c,Q</italic></sub> is given by the formula:
<disp-formula id="eqn19">
<graphic xlink:href="614432v2_eqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, Î(Â·) is the standard gamma function of calculus, <italic>det</italic>(<italic>Q</italic>) means the determinant of matrix <italic>Q</italic>. Minimizing the volume of <italic>E</italic><sub><italic>c,Q</italic></sub> is equivalent to minimizing det(<italic>Q</italic><sup>â1/2</sup>). Therefore, for a columnar region with foreground points <italic>P</italic>{<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>,â¦<italic>x</italic><sub><italic>m</italic></sub>}, we define the target function as follow:
<disp-formula id="eqn20">
<graphic xlink:href="614432v2_eqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn21">
<graphic xlink:href="614432v2_eqn21.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn22">
<graphic xlink:href="614432v2_eqn22.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>c</italic> â <italic>CHull</italic>(<italic>P</italic><sub><italic>i</italic></sub>) restrain the solved center of ellipsoid to locate within the smallest convex hull formed by the clustering points. To solve this problem, a variable substitution <italic>A</italic> = <italic>Q</italic><sup>1/2</sup> and <italic>y</italic> = <italic>Q</italic><sup>1/2</sup><italic>c</italic> were applied to equation (20) and equation (21), the original problem <italic>P</italic>1 can be transformed into a convex optimization problem as follow:
<disp-formula id="eqn23">
<graphic xlink:href="614432v2_eqn23.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn24">
<graphic xlink:href="614432v2_eqn24.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn25">
<graphic xlink:href="614432v2_eqn25.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Through adding the logarithmic barrier function, we can obtain the following formula:
<disp-formula id="eqn26">
<graphic xlink:href="614432v2_eqn26.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn27">
<graphic xlink:href="614432v2_eqn27.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn28">
<graphic xlink:href="614432v2_eqn28.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
As <italic>Î¸</italic> varies in the interval (0, â), the solution of <italic>P</italic>3 changes. When <italic>Î¸</italic> approaches 0, the optimal solution of <italic>P</italic>3 tends to the optimal solution of <italic>P</italic>2. By adding the dual multipliers <italic>d</italic><sub><italic>i</italic></sub> which satisfies <italic>d</italic><sub><italic>i</italic></sub> Â· <italic>z</italic><sub><italic>i</italic></sub> = <italic>Î¸</italic>, the optimality conditions can be written as:
<disp-formula id="eqn29">
<graphic xlink:href="614432v2_eqn29.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn30">
<graphic xlink:href="614432v2_eqn30.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn31">
<graphic xlink:href="614432v2_eqn31.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn32">
<graphic xlink:href="614432v2_eqn32.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn33">
<graphic xlink:href="614432v2_eqn33.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
At this point, the error between the solution of the system of equations and the optimal solution of <italic>P</italic>3 is less than <italic>d</italic> <sup><italic>T</italic></sup> <italic>z</italic>. Through equation (30), the explicit expression for solving <italic>y</italic> can be obtained as follow:
<disp-formula id="eqn34">
<graphic xlink:href="614432v2_eqn34.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, <italic>X</italic> stands for a 3Ã <italic>m</italic> matrix [<italic>x</italic> | <italic>x</italic> | â¦ | <italic>x</italic><sub><italic>m</italic></sub>], <italic>e</italic> stands for vector of ones <inline-formula><inline-graphic xlink:href="614432v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <italic>d</italic> stands for<inline-formula><inline-graphic xlink:href="614432v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Substitute equation (34) into equation (29), the equation for matrix <italic>A</italic> can be obtained by:
<disp-formula id="eqn35">
<graphic xlink:href="614432v2_eqn35.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>D</italic> stands for a <italic>m</italic>Ã <italic>m</italic> diagonal matrix <italic>Diag</italic>(<italic>d</italic><sub>1</sub>, <italic>d</italic><sub>2</sub>, â¦, <italic>d</italic><sub><italic>m</italic></sub>). And the explicit expression for <italic>A</italic> is formed as
<disp-formula id="eqn36">
<graphic xlink:href="614432v2_eqn36.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
And explicit expression for <italic>y</italic> :
<disp-formula id="eqn37">
<graphic xlink:href="614432v2_eqn37.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Through substitute the above two equation to the system of equations (29)-(33), variables <italic>A</italic> and y are eliminated. The following system of equations with only variables <italic>d</italic> and <italic>z</italic> can be obtained:
<disp-formula id="eqn38">
<graphic xlink:href="614432v2_eqn38.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn39">
<graphic xlink:href="614432v2_eqn39.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn40">
<graphic xlink:href="614432v2_eqn40.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>f</italic> (<italic>d</italic>) is nonlinear function of variable <italic>d</italic> :
<disp-formula id="eqn41">
<graphic xlink:href="614432v2_eqn41.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
For a fixed barrier parameter <italic>Î¸</italic>, we employ Newtonâs method to solve the system of equations. We use â<sub><italic>d</italic></sub> <italic>f</italic> (<italic>d</italic>) to represent the Jacobian matrix of <italic>f</italic>(<italic>d</italic>). Thus, the Jacobian matrix of the system of equations can be computed as follow:
<disp-formula id="eqn42">
<graphic xlink:href="614432v2_eqn42.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
And the newtonâs direction is written as:
<disp-formula id="eqn43">
<graphic xlink:href="614432v2_eqn43.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn44">
<graphic xlink:href="614432v2_eqn44.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn45">
<graphic xlink:href="614432v2_eqn45.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
With initial (<italic>d</italic><sub>0</sub>, <italic>z</italic><sub>0</sub>), iterate with <inline-formula><inline-graphic xlink:href="614432v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, to obtain the final optimal solution, <inline-formula><inline-graphic xlink:href="614432v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> represents the newtonâs step. Detailed process can see the pseudo code as follow:</p>
<statement id="alg1">
<label>Algorithm 1</label>
<title>Compute Newtonâs direction.</title>
<p><fig id="alg1a" position="float" fig-type="figure">
<graphic xlink:href="614432v2_alg1.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<statement id="alg2">
<label>Algorithm 2</label>
<title>Process of solving P2</title>
<p><fig id="alg2a" position="float" fig-type="figure">
<graphic xlink:href="614432v2_alg2.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<p>With the solved optimal solution of (<italic>Q, c</italic>), we then check whether <italic>c</italic> is located within the convex hull of the input point set {<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>,â¦, <italic>x</italic><sub><italic>m</italic></sub>}. If it is not, constrained gaussian mixture model will be applied to partition it into two subsets and solve the minimum-volume covering ellipsoids problem again in the two subsets. Through solving the above minimum-volume covering ellipsoids problem, we can characterize the columnar regions more accurate.</p>
<p>Note that from constrained GMM, each cluster has the corresponding mean and covariance matrix of points in the cluster. These two values essentially describe the shape of the cluster. However, if these two values directly replace <italic>c</italic><sup>*</sup> and <italic>Q</italic><sup>*</sup>, the exported ellipsoid may only encompass a part of points in the cluster. For covering all points in the cluster, all elements in the covariance matrix are needed to proportionally enlarged, but the volume of the corresponding ellipsoid is not minimum. These two cases will reduce the accuracy of the connections between clusters, i.e., columnar regions. So, we introduce the minimum-volume covering ellipsoid model to extract the shape of columnar region.</p>
</sec>
<sec id="s4f">
<title>Skeleton generation using 0-1 assignment model</title>
<p>The 0-1 assignment model<sup><xref ref-type="bibr" rid="c43">43</xref></sup> can robustly and accurately establish connections between particles in live-cell imaging<sup><xref ref-type="bibr" rid="c55">55</xref></sup>. It is particularly effective in handling cases where particles are densely distributed, merged, or split. We analogize column regions to particles, and apply the 0-1 assignment model to build the connections between column regions. For the <italic>i-</italic>th columnar region, the center and the two endpoints of the longest axis of its minimum-volume covering ellipsoid are denoted by <italic>c</italic><sub><italic>i</italic></sub>, <italic>t</italic><sub><italic>i</italic>,0</sub>, <italic>t</italic><sub><italic>i</italic>,1</sub>. The direction refers to the pointing of the center point towards <italic>t</italic><sub><italic>i,k</italic></sub>, <italic>k</italic> equals to 0 or 1. According to the direction and the endpoints, we design the cost matrix for building the 0-1 assignment model.
<disp-formula id="eqn46">
<graphic xlink:href="614432v2_eqn46.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn47">
<graphic xlink:href="614432v2_eqn47.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn48">
<graphic xlink:href="614432v2_eqn48.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>D</italic> is 2<italic>n</italic>Ã2<italic>n</italic> auxiliary matrix all element of which are all set 100. Both <italic>i</italic>0 and <italic>j</italic>0 in EQ (47) are equal to 0 or 1, labeling the two endpoints of the longest axis of the ellipsoid. <italic>norm</italic>(<italic>t</italic><sub><italic>i,i</italic>0</sub>, <italic>t</italic> <sub><italic>j, j</italic>0</sub>) represents the Euclidean distance between <italic>t</italic><sub><italic>i,i</italic>0</sub> and <italic>t</italic> <sub><italic>j, j</italic>0</sub>. <italic>Î¸</italic>(<italic>t</italic><sub><italic>i,i</italic>0</sub>, <italic>t</italic> <sub><italic>j, j</italic>0</sub>) describes the angle between two ellipsoids, i.e., two columnar regions. <italic>dir</italic>(<italic>c</italic><sub><italic>i</italic></sub>, <italic>t</italic><sub><italic>i,i</italic>0</sub>) represents the line from point <italic>c</italic><sub><italic>i</italic></sub> to <italic>t</italic><sub><italic>i,i</italic>0</sub>. â¨<italic>dir</italic>(<italic>c</italic><sub><italic>i</italic></sub>, <italic>t</italic><sub><italic>i,i</italic>0</sub>), <italic>dir</italic>(<italic>c</italic><sub><italic>i</italic></sub>, <italic>t</italic> <sub><italic>j, j</italic>0</sub>) â© represents cosine angle between the two lines. The threshold of 100 in D in EQ (46) and EQ (47) is an experimental value designed to ensure that the terminal points of neurites do not connect to more than one other terminal points.</p>
<p>After set the cost matrix, the 0-1 assignment problem is defined as follow:
<disp-formula id="eqn49">
<graphic xlink:href="614432v2_eqn49.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn50">
<graphic xlink:href="614432v2_eqn50.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn51">
<graphic xlink:href="614432v2_eqn51.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>A</italic> represents the connectivity matrix between different terminals of columnar regions: if <italic>A</italic><sub><italic>i, j</italic></sub> = 1, then establish connection between terminal <italic>i</italic> and terminal <italic>j</italic>, if <italic>A</italic><sub><italic>i, j</italic></sub> = 0, then establish no connection between terminal <italic>i</italic> and terminal <italic>j</italic>. <inline-formula><inline-graphic xlink:href="614432v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and<inline-formula><inline-graphic xlink:href="614432v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> restrain each terminal establish connection with at most one other terminal. The Lapjv algorithm<sup><xref ref-type="bibr" rid="c43">43</xref></sup> is utilized to solve this optimization problem and the shapes of individual neurites in block images are formed. Furthermore, we employ the region growing method to generate skeletons from the reconstructed shape, achieving the neurites reconstruction from individual image blocks.</p>
</sec>
<sec id="s4g">
<title>Minimal information flow tree for revising the reconstruction</title>
<p>The minimal information flow tree model is designed to modify the topology of skeletons, eliminate incorrect connections, and decompose them into multiple branches. When given an input skeleton file such as the swc file<sup><xref ref-type="bibr" rid="c56">56</xref></sup>, we convert it into a binary tree structure with following steps.</p>
<p><bold>Step1</bold>: select the neurite skeleton <italic>S</italic><sub>1</sub>. <italic>S</italic><sub>1</sub> has the largest length in the neurite skeletons that connect with each other. One of its terminal nodes are recorded as the head node <italic>n</italic><sub>1</sub>.</p>
<p><bold>Step2</bold>: generate the initial tree structure. Starting at head node <italic>n</italic><sub>1</sub>, search the linking nodes along the skeleton <italic>S</italic><sub>1</sub>, denoted by <inline-formula><inline-graphic xlink:href="614432v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The topology structure is <inline-formula><inline-graphic xlink:href="614432v2_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p><bold>Step3</bold>: generate new structure induced by the linking node <inline-formula><inline-graphic xlink:href="614432v2_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is regarded as the head node and its corresponding neurite skeleton is denoted by <italic>S</italic><sub>1</sub>. Let <inline-formula><inline-graphic xlink:href="614432v2_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula> represent the linking nodes in skeleton <italic>S</italic><sub>2</sub>. The corresponding topology structure is <inline-formula><inline-graphic xlink:href="614432v2_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula>.</p>
<p><bold>Step4</bold>: repeat the operation in <bold>Step3</bold> for dealing with the linking nodes <inline-formula><inline-graphic xlink:href="614432v2_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The corresponding topology structures are added into the total tree structure. After obtaining the tree structures induced by linking nodes in <italic>S</italic><sub>1</sub>, use the operation in <bold>Step3</bold> to generate the tree structures induced by linking nodes in <italic>S</italic><sub>2</sub>. Continue in this manner until all linking nodes have been processed.</p>
<p>To gain a better understanding of the above process, we have provided a demonstration of how to generate the corresponding binary tree from the skeletons of neurites <xref ref-type="supplementary-material" rid="supp1">(Fig.S6)</xref>.</p>
<p>For the skeletons of neurites in an image block, the corresponding number of binary tree structures will be generated. We use the MIFT model to merge or split these binary structures. Suppose that an image stack contains <italic>m</italic> skeletons all of which have <italic>K</italic> nodes, denoted by <italic>n</italic><sub>1</sub>, â¦, <italic>n</italic><sub><italic>K</italic> â1</sub>, <italic>n</italic><sub><italic>K</italic></sub>. The connections among these nodes are stored in a matrix <italic>W</italic> with <italic>K</italic> Ã <italic>K</italic> elements. <italic>W</italic><sub><italic>i, j</italic></sub> = 0 indicates that there is no connection between node <italic>i</italic> and node <italic>j</italic>. <italic>W</italic><sub><italic>i, j</italic></sub> = â1indicates that <italic>j</italic> â <italic>headnode</italic> = <italic>i, W</italic><sub><italic>i, j</italic></sub> = â2 indicates that <italic>j</italic> â <italic>leftnode</italic> = <italic>i, W</italic><sub><italic>i, j</italic></sub> = â3 indicates that <italic>j</italic> â <italic>rightnode</italic> = <italic>i</italic>.</p>
<p>The information flow can be computed as follow:
<disp-formula id="eqn52">
<graphic xlink:href="614432v2_eqn52.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn53">
<graphic xlink:href="614432v2_eqn53.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here the optimization objective function in EQ (53) is called information flow. <italic>Î¸</italic>(Â·) is the angle between flow from <italic>n</italic><sub><italic>i</italic></sub> â <italic>headnode</italic> to <italic>n</italic><sub><italic>i</italic></sub> and flow from <italic>n</italic><sub><italic>i</italic></sub> to <italic>n</italic><sub><italic>i</italic></sub> â <italic>leftnode</italic>. To minimize optimization problem while ensuring that the topology matrix <italic>W</italic> does not exhibit abnormal values, we adopt the strategy of dynamic programming to update topology matrix <italic>W</italic>. Briefly, we calculate the other two possible angles <italic>Î¸</italic>(<italic>n</italic><sub><italic>i</italic></sub> â <italic>headnode, n</italic><sub><italic>i</italic></sub>, <italic>n</italic><sub><italic>i</italic></sub> â <italic>rightnode</italic>) and <italic>Î¸</italic>(<italic>n</italic><sub><italic>i</italic></sub> â <italic>leftnode, n</italic><sub><italic>i</italic></sub>, <italic>n</italic><sub><italic>i</italic></sub> â <italic>rightnode</italic>) at the first linking node <italic>n</italic><sub><italic>i</italic></sub>. The minimum information flow is selected and <italic>W</italic> is updated. Following the updated <italic>W</italic>, the next branching node is found and information flow and <italic>W</italic> is updated. The updating process iterates until all nodes are updated. The final root nodes{<italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>,â¦, <italic>r</italic><sub><italic>m</italic></sub>} are obtained (node satisfies <italic>W</italic> (<italic>r</italic><sub><italic>t</italic></sub>, <italic>i</italic>) = 0 <italic>or</italic> â1(<italic>i</italic> = 1,â¦<italic>n</italic>) is set root node). The pseudo-code for solving the optimization problem is provided below:</p>
<statement id="alg1_1">
<label>Algorithm 1</label>
<title>Generation of Minimal Information Flow Tree</title>
<p><fig id="alg1_1a" position="float" fig-type="figure">
<graphic xlink:href="614432v2_alg1_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig></p>
</statement>
<p>Please note that the model has the capability to merge binary trees. When two branches of neurites have identifiable root nodes, and one root node is in close proximity to the skeleton points on the other branch of neurites, the root node does not contribute to the calculation of information flow without fusion. However, after fusion, the root node becomes a linking node in the other branch of neurites, resulting in an additional negative information flow value. In this merging process, a threshold is required to be set. When the minimum distance between the root node of a branch of neurites and the skeleton point of the other branch of neurites is less than 8 for individual image blocks or less than 8,12,16 for fused image blocks respectively, these two branches are merged. When splitting a branch of neurites, the minimal information flow tree model is also applied to both individual and fused image blocks.</p>
</sec>
<sec id="s4h">
<title>The fusion of neurites reconstruction</title>
<p>By using MIFT model to revise the neurites reconstruction in individual image blocks, the root nodes and leaf nodes of a branch of neurites can be extracted directly. Here, we use 0-1 assignment model to merge the reconstructions between two adjacent image blocks. For two adjacent image blocks <italic>P</italic> and <italic>Q</italic>, the neurite skeleton nodes which locate near the common boundary are extracted as{<italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>, â¦<italic>p</italic><sub><italic>m</italic></sub>},{<italic>q</italic><sub>1</sub>, <italic>q</italic><sub>2</sub>, â¦<italic>q</italic><sub><italic>n</italic></sub>}and the cost matrix is constructed as follow:
<disp-formula id="eqn54">
<graphic xlink:href="614432v2_eqn54.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn55">
<graphic xlink:href="614432v2_eqn55.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>D</italic><sub><italic>m</italic>Ã<italic>m</italic></sub>, <italic>D</italic><sub><italic>n</italic>Ã<italic>n</italic></sub>, <italic>D</italic><sub><italic>n</italic>Ã<italic>m</italic></sub> are auxiliary matrix which the values are all set 20. <italic>d</italic>(<italic>p</italic><sub><italic>i</italic></sub>, <italic>q</italic><sub><italic>j</italic></sub>) represents the Euclidean distance between terminal <italic>p</italic><sub><italic>i</italic></sub> and <italic>q</italic><sub><italic>j</italic></sub>. <italic>L</italic>(<italic>p</italic><sub><italic>i</italic></sub>) and <italic>L</italic>(<italic>q</italic><sub><italic>j</italic></sub>) are fitted lines from the skeleton points near <italic>p</italic><sub><italic>i</italic></sub> and <italic>q</italic><sub><italic>j</italic></sub>. <italic>Î¸</italic>(<italic>L</italic>(<italic>p</italic><sub><italic>i</italic></sub>), <italic>L</italic>(<italic>q</italic><sub><italic>j</italic></sub>)) represents the cosine value of their angle. Thus, the 0-1 assignment problem is formed as follow:
<disp-formula id="eqn56">
<graphic xlink:href="614432v2_eqn56.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn57">
<graphic xlink:href="614432v2_eqn57.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn58">
<graphic xlink:href="614432v2_eqn58.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here <italic>A</italic> represents the connectivity relationship between nodes, if <italic>A</italic><sub><italic>i, j</italic></sub> = 1, there is connection between block <italic>P</italic> âs node <italic>i</italic> and block <italic>Q</italic> âs node <italic>j</italic>, if <italic>A</italic><sub><italic>i, j</italic></sub> = 0, there is no connection between block <italic>P</italic> âs node <italic>i</italic> and block <italic>Q</italic> âs node <italic>j</italic>. <inline-formula><inline-graphic xlink:href="614432v2_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="614432v2_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula> restrict each node connect to one other node at most. With solved matrix <italic>A</italic>, the neurite skeletons of adjacent blocks can be merged and fused skeleton structures can be obtained.</p>
</sec>
<sec id="s4i">
<title>Statistical Analysis</title>
<p>In this study, three commonly used metrics defined in<sup><xref ref-type="bibr" rid="c39">39</xref></sup> were used, including precision, recall and f1-score are computed to measure the fidelity between the reconstruction results and the ground truth. They are defined as follow:
<disp-formula id="eqn59">
<graphic xlink:href="614432v2_eqn59.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn60">
<graphic xlink:href="614432v2_eqn60.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn61">
<graphic xlink:href="614432v2_eqn61.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<italic>R</italic> represents the point set of reconstructed neurons, <italic>G</italic> represents the point set of the ground truth, | Â· | represents the number of points of a set. The three metrics are first computed on each individual neuron, and then averaged by weighting each neuron with its point number of its ground truth neuritis.</p>
<p>We also calculated the signal-to-ratio (SNR) of the data using the following method: For a given data block <italic>B</italic> and its corresponding ground-truth skeleton <italic>S</italic>, we first densify the skeleton <italic>S</italic> by using linear interpolation to ensure that the Euclidean distance between adjacent skeleton points is than 1 voxel. Next, we expand each skeleton point in the densified skeleton <italic>S</italic> <sup>â</sup> into a spherical mask with a radius of 3 voxels. The resulting region serves as the foreground <italic>mask</italic>. Finally, SNR is computed with mean intensity of foreground points and standard deviation of background points as follow:
<disp-formula id="eqn62">
<graphic xlink:href="614432v2_eqn62.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn63">
<graphic xlink:href="614432v2_eqn63.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn64">
<graphic xlink:href="614432v2_eqn64.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn65">
<graphic xlink:href="614432v2_eqn65.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<disp-formula id="eqn66">
<graphic xlink:href="614432v2_eqn66.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Here, <italic>I</italic>(<italic>x</italic>) represents the signal intensity of the voxel at position <italic>x</italic>, the SNR is calculated by <italic>Mean</italic><sub><italic>foregroud</italic></sub> and <italic>Std</italic><sub><italic>backgroud</italic></sub> by the following formula:
<disp-formula id="eqn67">
<graphic xlink:href="614432v2_eqn67.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
</sec>

</body>
<back>
<sec id="s8" sec-type="data-availability">
<title>Data and materials availability</title>
<p>The data for <xref rid="fig1" ref-type="fig">Figure 1C</xref>, <xref rid="fig2" ref-type="fig">Figure 2</xref>, <xref rid="fig3" ref-type="fig">Figure 3</xref>, <xref rid="fig4" ref-type="fig">Figure 4</xref>, <xref rid="fig5" ref-type="fig">Figure 5</xref>, <xref rid="fig6" ref-type="fig">Figure 6</xref> and <xref ref-type="supplementary-material" rid="supp1">Fig.S5</xref> is available in <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/15589145">https://zenodo.org/records/15589145</ext-link>. The raw image blocks are extremely large and can be available on request from the corresponding author. The training code of the segmentation network is available on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/FateUBW0227/Seg_Net">https://github.com/FateUBW0227/Seg_Net</ext-link>. The software of PointTree and its user guideline are available on <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/15589145">https://zenodo.org/records/15589145</ext-link>.</p></sec>
<ack>
<title>Acknowledgments</title>
<p>We thank the members of the Britton Chance Center for Biomedical Photonics for advice and help in experiments.</p>
</ack>
<sec id="d1e2698" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Funding</title>
<p>This work was supported by National Natural Science Foundation of China (32471146) and the project N20240194.</p>
</sec>
<sec id="s6">
<title>Author contributions</title>
<p>Methodology: T.Q., L.C.</p>
<p>Investigation: T.Q., L.C., T.F., X.Q., X.G., W.F.</p>
<p>Validation: L.C., T.F., Q.D, T.C, Y.Z.</p>
<p>Supervision: S.Z., T.Q., X.L., X.L., Q.H.</p>
<p>Writing: T.Q., L.C., T.F.</p>
</sec>
</sec>
<sec id="suppd1e2698" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary_materials</label>
<media xlink:href="supplements/614432_file02.pdf"/>
</supplementary-material>
<supplementary-material id="video1">
<label>Movie S1.</label>
<caption>
<title>Reconstructed long-range axonal projections and raw image data shown in <xref ref-type="fig" rid="fig6">Fig6</xref>, individual axonal projections are delineated in different colors.</title>
</caption>
<media xlink:href="video1.mp4"/>
</supplementary-material>
<supplementary-material id="video2">
<label>Movie S2.</label>
<caption>
<title>Trace one of the reconstructed projections shown in <xref ref-type="fig" rid="fig6">Fig6</xref>.</title>
</caption>
<media xlink:href="video2.mp4"/>
</supplementary-material>
<supplementary-material id="video3">
<label>Movie S3.</label>
<caption>
<title>Example run of PointTree on Windows.</title>
</caption>
<media xlink:href="video3.mp4"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parekh</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Ascoli</surname>, <given-names>G. A.</given-names></string-name></person-group> <article-title>Neuronal morphology goes digital: a research hub for cellular and system neuroscience</article-title>. <source>Neuron</source> <volume>77</volume>, <fpage>1017</fpage>â<lpage>1038</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meijering</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>Neuron tracing in perspective</article-title>. <source>Cytometry Part A</source> <volume>77</volume>, <fpage>693</fpage>â<lpage>704</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zingg</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Neural networks of the mouse neocortex</article-title>. <source>Cell</source> <volume>156</volume>, <fpage>1096</fpage>â<lpage>1111</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>Z. J.</given-names></string-name> &amp; <string-name><surname>Luo</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>It takes the world to understand the brain</article-title>. <source>Science</source> <volume>350</volume>, <fpage>42</fpage>â<lpage>44</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Panoptic imaging of transparent mice reveals whole-body neuronal projections and skullâmeninges connections</article-title>. <source>Nature neuroscience</source> <volume>22</volume>, <fpage>317</fpage>â<lpage>327</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chung</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Deisseroth</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>CLARITY for mapping the nervous system</article-title>. <source>Nature methods</source> <volume>10</volume>, <fpage>508</fpage>â<lpage>513</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Economo</surname>, <given-names>M. N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A platform for brain-wide imaging and reconstruction of individual neurons</article-title>. <source>eLife</source> <volume>5</volume>, <elocation-id>e10566</elocation-id> (<year>2016</year>). <pub-id pub-id-type="doi">10.7554/eLife.10566</pub-id></mixed-citation></ref>
<ref id="c8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>T. H.</given-names></string-name> &amp; <string-name><surname>Schnitzer</surname>, <given-names>M. J.</given-names></string-name></person-group> <article-title>Fluorescence imaging of large-scale neural ensemble dynamics</article-title>. <source>Cell</source> <volume>185</volume>, <fpage>9</fpage>â<lpage>41</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Micro-optical sectioning tomography to obtain a high-resolution atlas of the mouse brain</article-title>. <source>Science</source> <volume>330</volume>, <fpage>1404</fpage>â<lpage>1408</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Osten</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Margrie</surname>, <given-names>T. W.</given-names></string-name></person-group> <article-title>Mapping brain circuitry with a light microscope</article-title>. <source>Nature methods</source> <volume>10</volume>, <fpage>515</fpage>â<lpage>523</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>What is a cell type and how to define it?</article-title> <source>Cell</source> <volume>185</volume>, <fpage>2739</fpage>â<lpage>2755</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>High-throughput mapping of a whole rhesus monkey brain at micrometer resolution</article-title>. <source>Nature biotechnology</source> <volume>39</volume>, <fpage>1521</fpage>â<lpage>1528</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qiu</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Whole-brain spatial organization of hippocampal single-neuron projectomes</article-title>. <source>Science</source> <volume>383</volume>, <fpage>eadj9198</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Morphological diversity of single neurons in molecularly defined cell types</article-title>. <source>Nature</source> <volume>598</volume>, <fpage>174</fpage>â<lpage>181</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Single-neuron projectome of mouse prefrontal cortex</article-title>. <source>Nature Neuroscience</source> <volume>25</volume>, <fpage>515</fpage>â<lpage>529</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Foster</surname>, <given-names>N. N.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The mouse corticoâbasal gangliaâthalamic network</article-title>. <source>Nature</source> <volume>598</volume>, <fpage>188</fpage>â<lpage>194</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>MuÃ±oz-CastaÃ±eda</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cellular anatomy of the mouse primary motor cortex</article-title>. <source>Nature</source> <volume>598</volume>, <fpage>159</fpage>â<lpage>166</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>Q.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A whole-brain map of long-range inputs to GABAergic interneurons in the mouse medial prefrontal cortex</article-title>. <source>Nature neuroscience</source> <volume>22</volume>, <fpage>1357</fpage>â<lpage>1370</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Winnubst</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Reconstruction of 1,000 projection neurons reveals new cell types and organization of long-range connectivity in the mouse brain</article-title>. <source>Cell</source> <volume>179</volume>, <fpage>268</fpage>â<lpage>281.e213</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedmann</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Mapping mesoscale axonal projections in the mouse brain using a 3D convolutional network</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>117</volume>, <fpage>11068</fpage>â<lpage>11075</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>GTree: an open-source tool for dense reconstruction of brain-wide neuronal population</article-title>. <source>Neuroinformatics</source> <volume>19</volume>, <fpage>305</fpage>â<lpage>317</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>TeraVR empowers precise reconstruction of complete 3-D neuronal morphology in the whole brain</article-title>. <source>Nature communications</source> <volume>10</volume>, <fpage>3474</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Sanes</surname>, <given-names>J. R.</given-names></string-name></person-group> <article-title>Neuronal cell-type classification: challenges, opportunities and the path forward</article-title>. <source>Nature Reviews Neuroscience</source> <volume>18</volume>, <fpage>530</fpage>â<lpage>546</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Precise segmentation of densely interweaving neuron clusters using G-Cut</article-title>. <source>Nature communications</source> <volume>10</volume>, <fpage>1549</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lichtman</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Denk</surname>, <given-names>W.</given-names></string-name></person-group> <article-title>The big and the small: challenges of imaging the brainâs circuits</article-title>. <source>Science</source> <volume>334</volume>, <fpage>618</fpage>â<lpage>623</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Helmstaedter</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Cellular-resolution connectomics: challenges of dense neural circuit reconstruction</article-title>. <source>Nature methods</source> <volume>10</volume>, <fpage>501</fpage>â<lpage>507</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>BigNeuron: large-scale 3D neuron reconstruction from optical microscopy images</article-title>. <source>Neuron</source> <volume>87</volume>, <fpage>252</fpage>â<lpage>256</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manubens-Gil</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>BigNeuron: a resource to benchmark and predict performance of algorithms for automated tracing of neurons in light microscopy datasets</article-title>. <source>Nature Methods</source> <volume>20</volume>, <fpage>824</fpage>â<lpage>835</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choromanska</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>S.-F.</given-names></string-name> &amp; <string-name><surname>Yuste</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Automatic reconstruction of neural morphologies with multi-scale tracking</article-title>. <source>Frontiers in neural circuits</source> <volume>6</volume>, <fpage>25</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Brain-wide shape reconstruction of a traced neuron using the convex image segmentation method</article-title>. <source>Neuroinformatics</source> <volume>18</volume>, <fpage>199</fpage>â<lpage>218</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gonzalez-Bellido</surname>, <given-names>P. T.</given-names></string-name> &amp; <string-name><surname>Peng</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>A distance-field based automatic neuron tracing method</article-title>. <source>BMC bioinformatics</source> <volume>14</volume>, <fpage>1</fpage>â<lpage>11</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Long</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Myers</surname>, <given-names>G.</given-names></string-name></person-group> <article-title>Automatic 3D neuron tracing using all-path pruning</article-title>. <source>Bioinformatics</source> <volume>27</volume>, <fpage>i239</fpage>â<lpage>i247</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiao</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Peng</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>APP2: automatic tracing of 3D neuron morphology based on hierarchical pruning of a gray-weighted image distance-tree</article-title>. <source>Bioinformatics</source> <volume>29</volume>, <fpage>1448</fpage>â<lpage>1454</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>TÃ¼retken</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>GonzÃ¡lez</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Blum</surname>, <given-names>C.</given-names></string-name> &amp; <string-name><surname>Fua</surname>, <given-names>P.</given-names></string-name></person-group> <article-title>Automated reconstru ction of dendritic and axonal trees by global optimization with geometric priors</article-title>. <source>Neuroinformatics</source> <volume>9</volume>, <fpage>279</fpage>â<lpage>302</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Kuo</surname>, <given-names>H.-C.</given-names></string-name>, <string-name><surname>Peng</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Long</surname>, <given-names>F.</given-names></string-name></person-group> <article-title>DeepNeuron: an open deep learning toolbox for neuron tracing</article-title>. <source>Brain informatics</source> <volume>5</volume>, <fpage>1</fpage>â<lpage>9</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>Q.</given-names></string-name> &amp; <string-name><surname>Shen</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>3D neuron reconstruction in tangled neuronal image with deep networks</article-title>. <source>IEEE transactions on medical imaging</source> <volume>39</volume>, <fpage>425</fpage>â<lpage>435</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>Q.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Weakly supervised learning of 3D deep network for neuron reconstruction</article-title>. <source>Frontiers in Neuroanatomy</source> <volume>14</volume>, <fpage>38</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Ascoli</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Liu</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Neuron tracing from light microscopy images: automation, deep learning and bench testing</article-title>. <source>Bioinformatics</source> <volume>38</volume>, <fpage>5329</fpage>â<lpage>5339</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quan</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>NeuroGPS-Tree: automatic reconstruction of large-scale neuronal populations with dense neurites</article-title>. <source>Nature methods</source> <volume>13</volume>, <fpage>51</fpage>â<lpage>54</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname>, <given-names>L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Single-neuron analysis of dendrites and axons reveals the network organization in mouse prefrontal cortex</article-title>. <source>Nature Neuroscience</source> <volume>26</volume>, <fpage>1111</fpage>â<lpage>1126</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reynolds</surname>, <given-names>D. A.</given-names></string-name></person-group> <article-title>Gaussian mixture models</article-title>. <source>Encyclopedia of biometrics</source> <volume>741</volume> (<year>2009</year>).</mixed-citation></ref>
<ref id="c42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Freund</surname>, <given-names>R. M.</given-names></string-name></person-group> <article-title>Computation of minimum-volume covering ellipsoids</article-title>. <source>Operations Research</source> <volume>52</volume>, <fpage>690</fpage>â<lpage>706</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Volgenant</surname>, <given-names>A.</given-names></string-name></person-group> <article-title>Linear and semi-assignment problems: a core oriented approach</article-title>. <source>Computers &amp; Operations Research</source> <volume>23</volume>, <fpage>917</fpage>â<lpage>932</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c44"><label>44</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>R.</given-names></string-name></person-group> <article-title>Models of regional growth: past, present and future</article-title>. <source>Journal of economic surveys</source> <volume>25</volume>, <fpage>913</fpage>â<lpage>951</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Kim</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>neuTube 1.0: a new design for efficient neuron reconstruction software based on the SWC format</article-title>. <source>eneuro</source> <volume>2</volume> (<year>2015</year>).</mixed-citation></ref>
<ref id="c46"><label>46</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radojevic</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Meijering</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>Automated neuron tracing using probability hypothesis density filtering</article-title>. <source>Bioinformatics</source> <volume>33</volume>, <fpage>1073</fpage>â<lpage>1080</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>3D BrainCV: simultaneous visualization and analysis of cells and capillaries in a whole mouse brain with one-micron voxel resolution</article-title>. <source>Neuroimage</source> <volume>87</volume>, <fpage>199</fpage>â<lpage>208</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c48"><label>48</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stelzer</surname>, <given-names>E. H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Light sheet fluorescence microscopy</article-title>. <source>Nature Reviews Methods Primers</source> <volume>1</volume>, <fpage>73</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c49"><label>49</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Chemical sectioning fluorescence tomography: high-throughput, high-contrast, multicolor, whole-brain imaging at subcellular resolution</article-title>. <source>Cell Reports</source> <volume>34</volume> (<year>2021</year>).</mixed-citation></ref>
<ref id="c50"><label>50</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhong</surname>, <given-names>Q.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>High-definition imaging using line-illumination modulation microscopy</article-title>. <source>Nature methods</source> <volume>18</volume>, <fpage>309</fpage>â<lpage>315</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c51"><label>51</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Cross-streams through the ventral posteromedial thalamic nucleus to convey vibrissal information</article-title>. <source>Frontiers in Neuroanatomy</source> <volume>15</volume>, <fpage>724861</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c52"><label>52</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>ÃiÃ§ek</surname>, <given-names>Ã.</given-names></string-name>, <string-name><surname>Abdulkadir</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lienkamp</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Brox</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name></person-group> <article-title>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</article-title> in <conf-name>Medical Image Computing and Computer-Assisted InterventionâMICCAI 2016: 19th International Conference</conf-name>, <conf-loc>Athens, Greece</conf-loc>, October 17-21, <year>2016</year>, Proceedings, Part II 19. <fpage>424</fpage>â<lpage>432</lpage> (<publisher-name>Springer</publisher-name>).</mixed-citation></ref>
<ref id="c53"><label>53</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Peng</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Zhou</surname>, <given-names>Y.</given-names></string-name></person-group> <article-title>An overview on density peaks clustering</article-title>. <source>Neurocomputing</source> <volume>554</volume>, <fpage>126633</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c54"><label>54</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>McLachlan</surname>, <given-names>G. J.</given-names></string-name> &amp; <string-name><surname>Krishnan</surname>, <given-names>T.</given-names></string-name></person-group> <source>The EM algorithm and extensions</source>. (<publisher-name>John Wiley &amp; Sons</publisher-name>, <year>2007</year>).</mixed-citation></ref>
<ref id="c55"><label>55</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jaqaman</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Robust single-particle tracking in live-cell time-lapse sequences</article-title>. <source>Nature methods</source> <volume>5</volume>, <fpage>695</fpage>â<lpage>702</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c56"><label>56</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cannon</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Pyapali</surname>, <given-names>G.</given-names></string-name> &amp; <string-name><surname>Wheal</surname>, <given-names>H.</given-names></string-name></person-group> <article-title>An on-line archive of reconstructed hippocampal neurons</article-title>. <source>Journal of neuroscience methods</source> <volume>84</volume>, <fpage>49</fpage>â<lpage>54</lpage> (<year>1998</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102840.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> paper takes a novel approach to the problem of automatically reconstructing long-range axonal projections from stacks of images. The key innovation is to separate the identification of sections of an axon from the statistical rules used to constrain global structure. The authors provide <bold>compelling</bold> evidence that their method is a significant improvement over existing measures in circumstances where the labelling of axons and dendrites is relatively dense.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102840.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors introduce a novel algorithm for the automatic identification of long-range axonal projections. This is an important problem as modern high-throughput imaging techniques can produce large amounts of raw data, but identifying neuronal morphologies and connectivities requires large amounts of manual work. The algorithm works by first identifying points in three-dimensional space corresponding to parts of labelled neural projections, these are then used to identify short sections of axon using an optimisation algorithm and the prior knowledge that axonal diameters are relatively constant. Finally, a statistical model that assumes axons tend to be smooth is used to connect the sections together into complete and distinct neural trees. The authors demonstrate that their algorithm is far superior to existing techniques, especially when a dense labelling of the tissue means that neighbouring neurites interfere with the reconstruction. Despite this improvement, however, the accuracy of reconstruction remains below 90%, so manual proof-reading is still necessary to produce accurate reconstructions of axons.</p>
<p>Strengths:</p>
<p>The new algorithm combines local and global information to make a significant improvement on the state-of -the-art for automatic axonal reconstruction. The method could be applied more broadly and might have applications to reconstructions of electron microscopy data, where similar issues of high-throughput imaging and relatively slow or inaccurate reconstruction remain.</p>
<p>Weaknesses:</p>
<p>There are three weaknesses with the algorithm and manuscript.</p>
<p>(1) The best reconstruction accuracy is below 90%, which does not fully solve the problem of needing manual proof-reading.</p>
<p>(2) The 'minimum information flow tree' model the authors use to construct connected axonal trees has the potential to bias data collection. In particular, the assumption that axons should always be as smooth as possible is not always correct. This is a good rule-of-thumb for reconstructions, but real axons in many systems can take quite sharp turns and this is also seen in the data presented in the paper (Fig 1C). I would like to see explicit acknowledgement of this bias in the current manuscript and ideally a relaxation of this rule in any later versions of the algorithm.</p>
<p>(3) The writing of the manuscript is not always as clear as it could be. The manuscript would benefit from careful copy editing for language, and the Methods section in particular should be expanded to more clearly explain what each algorithm is doing. The pseudo code of the Supplemental Information could be brought into the Methods if possible as these algorithms are so fundamental to the manuscript.</p>
<p>Comments on revisions: I have no further comments or recommendations.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102840.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors have addressed my comments in this revised version of their manuscript. PointTree is an improved method for the reconstruction of neuronal anatomy that will be useful for neuroscientists.</p>
<p>In this manuscript, Cai et al. introduce PointTree, a new automated method for the reconstruction of complex neuronal projections. This method has the potential to drastically speed up the process of reconstructing complex neurites. The authors use semi-automated manual reconstruction of neurons and neurites to provide a 'ground-truth' for comparison between PointTree and other automated reconstruction methods. The reconstruction performance is evaluated for precision, recall and F1-score and positions. The performance of PointTree compared to other automated reconstruction methods is impressive based on these 3 criteria.</p>
<p>As an experimentalist, I will not comment on the computational aspects of the manuscript. Rather, I am interested in how PointTree's performance decrease in noisy samples. This is because many imaging datasets contain some level of background noise for which the human eye appears essential for accurate reconstruction of neurites. Although the samples presented in Figure 5 represent an inherent challenge for any reconstruction method, the signal to noise ratio is extremely high (also the case in all raw data images in the paper). It would be interesting to see how PointTree's performance change in increasingly noisy samples, and for the author to provide general guidance to the scientific community as to what samples might not be accurately reconstructed with PointTree.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102840.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Cai</surname>
<given-names>Lin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fan</surname>
<given-names>Taiyu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Qu</surname>
<given-names>Xuzhong</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Ying</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gou</surname>
<given-names>Xianyu</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ding</surname>
<given-names>Quanwei</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Feng</surname>
<given-names>Weihua</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cao</surname>
<given-names>Tingting</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lv</surname>
<given-names>Xiaohua</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>Xiuli</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Huang</surname>
<given-names>Qing</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Quan</surname>
<given-names>Tingwei</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8393-4292</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Zeng</surname>
<given-names>Shaoqun</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authorsâ response to the original reviews</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>The authors introduce a novel algorithm for the automatic identification of longrange axonal projections. This is an important problem as modern high-throughput imaging techniques can produce large amounts of raw data, but identifying neuronal morphologies and connectivities requires large amounts of manual work. The algorithm works by first identifying points in three-dimensional space corresponding to parts of labelled neural projections, these are then used to identify short sections of axons using an optimisation algorithm and the prior knowledge that axonal diameters are relatively constant. Finally, a statistical model that assumes axons tend to be smooth is used to connect the sections together into complete and distinct neural trees. The authors demonstrate that their algorithm is far superior to existing techniques, especially when dense labelling of the tissue means that neighbouring neurites interfere with the reconstruction. Despite this improvement, however, the accuracy of reconstruction remains below 90%, so manual proofreading is still necessary to produce accurate reconstructions of axons.</p>
<p>Strengths:</p>
<p>The new algorithm combines local and global information to make a significant improvement on the state-of-the-art for automatic axonal reconstruction. The method could be applied more broadly and might have applications to reconstructions of electron microscopy data, where similar issues of highthroughput imaging and relatively slow or inaccurate reconstruction remain.</p>
</disp-quote>
<p>We thank the reviewer for their positive comments and for taking the time to review our manuscript. We are truly grateful that the reviewer recognized the value of our method in automatically reconstructing long-range axonal projections. While we report that our method achieves reconstruction accuracy of approximately 85%, we fully acknowledge that manual proofreading is still necessary to ensure accuracy greater than 95%. We also appreciate the reviewerâs insightful suggestion regarding the potential adaptation of our algorithm for reconstructing electron microscopy (EM) data, where similar challenges in high-throughput imaging and relatively slow or inaccurate reconstruction persist. We look forward to exploring ways to integrate our method with EM data in future work.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>There are three weaknesses in the algorithm and manuscript.</p>
<p>(1) The best reconstruction accuracy is below 90%, which does not fully solve the problem of needing manual proofreading.</p>
</disp-quote>
<p>We sincerely appreciate the reviewer's valuable insights regarding reconstruction accuracy. Indeed, as illustrated in Figure S4, our current best automated reconstruction accuracy on fMOST data is still below 90%. This indicates that manual proofreading remains essential to ensure reliability.</p>
<p>For the reconstruction of long-range axonal projections, ensuring the accuracy of the reconstruction process necessitates manual revision of the automatically generated results. Existing literature has demonstrated that a higher accuracy in automatic reconstruction correlates with a reduced need for manual revisions, thereby facilitating an accelerated reconstruction process (Winnubst et al., Cell 2019; Liu et al., Nature Methods 2025).</p>
<p>As the reviewer rightly points out, achieving an accuracy exceeding 95% currently necessitates manual proofreading. Although our method does not completely eliminate this requirement, it significantly alleviates the proofreading workload by: 1) Minimizing common errors in regions with dense neuron distributions; 2) Providing more reliable initial reconstructions; and 3) Reducing the number of corrections needed during the proofreading process.</p>
<p>In the future, we will continue to enhance our reconstruction framework. As imaging systems achieve higher signal-to-noise ratios and deep learning techniques facilitate more accurate foreground detection, we anticipate that our method will attain even greater reconstruction accuracy. Furthermore, we plan to develop a software system capable of predicting potential error locations in our automated reconstruction results, thereby streamlining manual revisions. This approach distinguishes itself from existing models by obviating the need for individual traversal of the brain regions associated with each neuron reconstruction.</p>
<disp-quote content-type="editor-comment">
<p>(2) The 'minimum information flow tree' model the authors use to construct connected axonal trees has the potential to bias data collection. In particular, the assumption that axons should always be as smooth as possible is not always correct. This is a good rule-of-thumb for reconstructions, but real axons in many systems can take quite sharp turns and this is also seen in the data presented in the paper (Figure 1C). I would like to see explicit acknowledgement of this bias in the current manuscript and ideally a relaxation of this rule in any later versions of the algorithm.</p>
</disp-quote>
<p>We appreciate the reviewer's insightful opinion regarding the potential bias introduced by our minimum information flow tree model. The reviewer is absolutely correct in noting that while axon smoothness serves as a useful reconstruction heuristic, it should not be treated as an absolute constraint given that real axons can exhibit sharp turns (as shown in Figure 1C). In response to this valuable feedback, we add explicit discussion of this limitation in Discussion section as follow: âFinally, the minimal information flow treeâs fundamental assumption, that axons should be as smooth as possible does not always hold true.</p>
<p>In fact, real axons can take quite sharp turns leading the algorithm to erroneously separate a single continuous axon into disjoint neurites.â</p>
<p>In our reconstruction process, the post-processing approach partially mitigates erroneous reconstructions derived from this rule. Specifically: The minimum information flow tree will decompose such structures into two separate branches (Fig. S7A), but the decomposition node is explicitly recorded. The newly decomposed branches attempt to reconnect by searching for plausible neurites starting from their head nodes (determined by the minimum information flow tree). If no connectable neurites are found, the branch is automatically reconnected to its originally recorded decomposition node (Fig. S7B). In Fig.S7C, two reconstruction examples demonstrate the effectiveness of the post-processing approach.</p>
<p>As pointed out by the reviewers, the proposed rule for revising neuron reconstruction does not encompass all scenarios. Relaxing the constraints of this rule may lead to numerous new erroneous connections. Currently, the proposed rule is solely based on the positions of neurite centerlines and does not integrate information regarding the intensity of the original images or segmentation data. Incorporating these elements into the rule could potentially reduce reconstruction errors.</p>
<disp-quote content-type="editor-comment">
<p>(3) The writing of the manuscript is not always as clear as it could be. The manuscript would benefit from careful copy editing for language, and the Methods section in particular should be expanded to more clearly explain what each algorithm is doing. The pseudo-code of the Supplemental Information could be brought into the Methods if possible as these algorithms are so fundamental to the manuscript.</p>
</disp-quote>
<p>We sincerely thank the reviewer for these valuable suggestions to improve our manuscriptâs clarity and methodological presentation. We have implemented the following revisions:</p>
<p>(1) Language Enhancement: we have conducted rigorous internal linguistic reviews to address grammatical inaccuracies and improve textual clarity.</p>
<p>(2) Methods Expansion and Pseudo-code Integration: we have incorporated all relevant derivations from the Supplementary Materials into the Methods section, with additional explanatory text to clarify the purpose and implementation of each algorithm. All mathematical formulations have been systematically rederived with modifications to variable nomenclature, subscript/superscript notations and identified errors in the original submission. All pseudocode from Supplementary Materials has been integrated into their corresponding methods subsection.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>In this manuscript, Cai et al. introduce PointTree, a new automated method for the reconstruction of complex neuronal projections. This method has the potential to drastically speed up the process of reconstructing complex neurites. The authors use semi-automated manual reconstruction of neurons and neurites to provide a 'ground-truth' for comparison between PointTree and other automated reconstruction methods. The reconstruction performance is evaluated for precision, recall, and F1-score and positions. The performance of PointTree compared to other automated reconstruction methods is impressive based on these 3 criteria.</p>
<p>As an experimentalist, I will not comment on the computational aspects of the manuscript. Rather, I am interested in how PointTree's performance decreases in noisy samples. This is because many imaging datasets contain some level of background noise for which the human eye appears essential for the accurate reconstruction of neurites. Although the samples presented in Figure 5 represent an inherent challenge for any reconstruction method, the signal-to-noise ratio is extremely high (also the case in all raw data images in the paper). It would be interesting to see how PointTree's performance changes in increasingly noisy samples, and for the author to provide general guidance to the scientific community as to what samples might not be accurately reconstructed with PointTree.</p>
</disp-quote>
<p>We thank the reviewer for her/his time reviewing our manuscript and the interest on how PointTree perform on noisy samples. It is important to clarify that PointTree is solely responsible for the reconstruction of neurons from the foreground regions of neural images. The foreground regions of these neuronal images are obtained through a deep learning segmentation network. In cases where the image has a low signal-to-noise ratio, if the segmentation network can accurately identify the foreground areas, then PointTree will be able to accurately reconstruct neurons. In fact, existing deep learning networks have demonstrated their capability to effectively extract foreground regions from low signal-to-noise ratio images; therefore, PointTree is well-suited for processing neuronal images characterized by low signal-to-noise ratios.</p>
<p>In the revised manuscript, we conducted experiments on datasets with varying signal-to-noise ratios (SNR). The results demonstrate that Unet3D is capable of identifying the foreground regions in low-SNR images, thereby supporting the assertion that PointTree has broad applicability across diverse neuronal imaging datasets.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>It would be interesting to see how PointTree's performance changes in increasingly noisy samples, and for the author to provide general guidance to the scientific community as to what samples might not be accurately reconstructed with PointTree.</p>
</disp-quote>
<p>We extend our heartfelt gratitude to the reviewer for their insightful suggestion concerning experiments involving different noisy samples. Here are the details of the datasets used:</p>
<p>LSM dataset: Mean SNR = 5.01, with 25 samples, and a volume size of 192Ã192Ã192.</p>
<p>fMOST dataset: Mean SNR = 8.68, with 25 samples, and a volume size of 192Ã192Ã192.</p>
<p>HD-fMOST dataset: Mean SNR = 11.4, with 25 samples, and a volume size of 192Ã192Ã192.</p>
<p>The experimental results reveal that, thanks to the deep learning network's robust feature extraction capabilities, even when working with low-SNR data (as depicted in Figure 4B, first two columns of the top row), satisfactory segmentation results (Figure 4B, first two columns of the third row) were achieved. These results laid a solid foundation for subsequent accurate reconstruction.</p>
<p>PointTree demonstrated consistent mean F1-scores of 91.0%, 90.0%, and 93.3% across the three datasets, respectively. This underscores its reconstruction robustness under varying SNR conditions when supported by the segmentation network. For more in-depth information, please refer to the manuscript section titled &quot;Reconstruction of data with different signal-to-noise ratios&quot; and Figure 4.</p>
</body>
</sub-article>
</article>